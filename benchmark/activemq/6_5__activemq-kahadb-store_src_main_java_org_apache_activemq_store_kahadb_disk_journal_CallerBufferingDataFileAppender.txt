2:bb4a2f7: /**
1:bb4a2f7:  * Licensed to the Apache Software Foundation (ASF) under one or more
1:bb4a2f7:  * contributor license agreements.  See the NOTICE file distributed with
1:bb4a2f7:  * this work for additional information regarding copyright ownership.
1:bb4a2f7:  * The ASF licenses this file to You under the Apache License, Version 2.0
1:bb4a2f7:  * (the "License"); you may not use this file except in compliance with
1:bb4a2f7:  * the License.  You may obtain a copy of the License at
2:bb4a2f7:  *
1:bb4a2f7:  *      http://www.apache.org/licenses/LICENSE-2.0
1:bb4a2f7:  *
1:bb4a2f7:  * Unless required by applicable law or agreed to in writing, software
1:bb4a2f7:  * distributed under the License is distributed on an "AS IS" BASIS,
1:bb4a2f7:  * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
1:bb4a2f7:  * See the License for the specific language governing permissions and
1:bb4a2f7:  * limitations under the License.
2:bb4a2f7:  */
1:1aab71b: package org.apache.activemq.store.kahadb.disk.journal;
28:bb4a2f7: 
1:bb4a2f7: import java.io.IOException;
1:bb4a2f7: import java.util.zip.Adler32;
1:bb4a2f7: import java.util.zip.Checksum;
1:ef619b6: 
1:1aab71b: import org.apache.activemq.store.kahadb.disk.util.DataByteArrayOutputStream;
1:ef619b6: import org.apache.activemq.util.ByteSequence;
1:582af3e: import org.apache.activemq.util.RecoverableRandomAccessFile;
1:1a59827: import org.slf4j.Logger;
1:1a59827: import org.slf4j.LoggerFactory;
1:bb4a2f7: 
1:bb4a2f7: /**
1:bb4a2f7:  * An optimized writer to do batch appends to a data file. This object is thread
1:bb4a2f7:  * safe and gains throughput as you increase the number of concurrent writes it
1:bb4a2f7:  * does.
1:bb4a2f7:  * The thread calling enqueue does the file open and buffering of the data, which
1:bb4a2f7:  * reduces the round trip of the write thread.
1:ef619b6:  *
1:bb4a2f7:  */
1:89f22da: class CallerBufferingDataFileAppender extends DataFileAppender {
1:bb4a2f7: 
1:1a59827:     private static final Logger logger = LoggerFactory.getLogger(CallerBufferingDataFileAppender.class);
1:1a59827: 
1:bb4a2f7:     final DataByteArrayOutputStream cachedBuffers[] = new DataByteArrayOutputStream[] {
1:bb4a2f7:             new DataByteArrayOutputStream(maxWriteBatchSize),
1:bb4a2f7:             new DataByteArrayOutputStream(maxWriteBatchSize)
3:bb4a2f7:     };
1:384388f:     volatile byte flip = 0x1;
1:89f22da:     public class WriteBatch extends DataFileAppender.WriteBatch {
1:bb4a2f7: 
1:384388f:         DataByteArrayOutputStream buff = cachedBuffers[flip ^= 1];
1:89f22da:         private boolean forceToDisk;
1:bb4a2f7:         public WriteBatch(DataFile dataFile, int offset, Journal.WriteCommand write) throws IOException {
1:89f22da:             super(dataFile, offset);
1:bb4a2f7:             initBuffer(buff);
1:bb4a2f7:             append(write);
33:bb4a2f7:         }
1:bb4a2f7: 
1:ef619b6:         @Override
1:bb4a2f7:         public void append(Journal.WriteCommand write) throws IOException {
1:89f22da:             super.append(write);
1:bb4a2f7:             forceToDisk |= appendToBuffer(write, buff);
1:bb4a2f7:         }
1:bb4a2f7:     }
1:bb4a2f7: 
1:89f22da:     @Override
1:89f22da:     protected DataFileAppender.WriteBatch newWriteBatch(Journal.WriteCommand write, DataFile file) throws IOException {
1:89f22da:         return new WriteBatch(file, file.getLength(), write);
1:89f22da:     }
1:89f22da: 
1:bb4a2f7:     private void initBuffer(DataByteArrayOutputStream buff) throws IOException {
1:bb4a2f7:         // Write an empty batch control record.
1:bb4a2f7:         buff.reset();
1:bb4a2f7:         buff.write(Journal.BATCH_CONTROL_RECORD_HEADER);
1:bb4a2f7:         buff.writeInt(0);
1:bb4a2f7:         buff.writeLong(0);
1:bb4a2f7:     }
1:bb4a2f7: 
1:bb4a2f7:     public CallerBufferingDataFileAppender(Journal dataManager) {
1:89f22da:         super(dataManager);
1:bb4a2f7:     }
1:bb4a2f7: 
1:bb4a2f7:     /**
1:bb4a2f7:      * The async processing loop that writes to the data files and does the
1:bb4a2f7:      * force calls. Since the file sync() call is the slowest of all the
1:bb4a2f7:      * operations, this algorithm tries to 'batch' or group together several
1:bb4a2f7:      * file sync() requests into a single file sync() call. The batching is
1:bb4a2f7:      * accomplished attaching the same CountDownLatch instance to every force
1:bb4a2f7:      * request in a group.
1:bb4a2f7:      */
1:89f22da:     @Override
1:bb4a2f7:     protected void processQueue() {
1:bb4a2f7:         DataFile dataFile = null;
1:582af3e:         RecoverableRandomAccessFile file = null;
1:bb4a2f7:         WriteBatch wb = null;
5:bb4a2f7:         try {
1:bb4a2f7: 
1:bb4a2f7:             while (true) {
1:bb4a2f7: 
1:bb4a2f7:                 Object o = null;
1:bb4a2f7: 
1:bb4a2f7:                 // Block till we get a command.
3:bb4a2f7:                 synchronized (enqueueMutex) {
1:bb4a2f7:                     while (true) {
1:bb4a2f7:                         if (nextWriteBatch != null) {
1:bb4a2f7:                             o = nextWriteBatch;
1:bb4a2f7:                             nextWriteBatch = null;
3:bb4a2f7:                             break;
1:bb4a2f7:                         }
3:bb4a2f7:                         if (shutdown) {
1:bb4a2f7:                             return;
1:bb4a2f7:                         }
2:bb4a2f7:                         enqueueMutex.wait();
1:bb4a2f7:                     }
3:bb4a2f7:                     enqueueMutex.notifyAll();
1:bb4a2f7:                 }
1:bb4a2f7: 
1:bb4a2f7:                 wb = (WriteBatch)o;
1:bb4a2f7:                 if (dataFile != wb.dataFile) {
1:bb4a2f7:                     if (file != null) {
1:1a59827:                         if (periodicSync) {
1:1a59827:                             if (logger.isTraceEnabled()) {
1:1a59827:                                 logger.trace("Syning file {} on rotate", dataFile.getFile().getName());
1:1a59827:                             }
1:1a59827:                             file.sync();
1:1a59827:                         }
1:bb4a2f7:                         dataFile.closeRandomAccessFile(file);
1:bb4a2f7:                     }
1:bb4a2f7:                     dataFile = wb.dataFile;
1:bb4a2f7:                     file = dataFile.openRandomAccessFile();
1:bb4a2f7:                 }
1:bb4a2f7: 
1:bb4a2f7:                 final DataByteArrayOutputStream buff = wb.buff;
1:bb4a2f7:                 final boolean forceToDisk = wb.forceToDisk;
1:bb4a2f7: 
1:bb4a2f7:                 ByteSequence sequence = buff.toByteSequence();
1:ef619b6: 
1:ef619b6:                 // Now we can fill in the batch control record properly.
1:bb4a2f7:                 buff.reset();
1:bb4a2f7:                 buff.skip(5+Journal.BATCH_CONTROL_RECORD_MAGIC.length);
1:bb4a2f7:                 buff.writeInt(sequence.getLength()-Journal.BATCH_CONTROL_RECORD_SIZE);
1:bb4a2f7:                 if( journal.isChecksum() ) {
1:ef619b6:                     Checksum checksum = new Adler32();
1:ef619b6:                     checksum.update(sequence.getData(), sequence.getOffset()+Journal.BATCH_CONTROL_RECORD_SIZE, sequence.getLength()-Journal.BATCH_CONTROL_RECORD_SIZE);
1:ef619b6:                     buff.writeLong(checksum.getValue());
1:bb4a2f7:                 }
1:bb4a2f7: 
1:bb4a2f7:                 // Now do the 1 big write.
1:bb4a2f7:                 file.seek(wb.offset);
2:bb4a2f7:                 if (maxStat > 0) {
1:bb4a2f7:                     if (statIdx < maxStat) {
1:bb4a2f7:                         stats[statIdx++] = sequence.getLength();
4:bb4a2f7:                     } else {
1:bb4a2f7:                         long all = 0;
1:bb4a2f7:                         for (;statIdx > 0;) {
1:bb4a2f7:                             all+= stats[--statIdx];
1:bb4a2f7:                         }
1:bb4a2f7:                         System.err.println("Ave writeSize: " + all/maxStat);
1:bb4a2f7:                     }
1:bb4a2f7:                 }
1:bb4a2f7:                 file.write(sequence.getData(), sequence.getOffset(), sequence.getLength());
1:bb4a2f7:                 ReplicationTarget replicationTarget = journal.getReplicationTarget();
1:bb4a2f7:                 if( replicationTarget!=null ) {
1:ef619b6:                     replicationTarget.replicate(wb.writes.getHead().location, sequence, forceToDisk);
1:bb4a2f7:                 }
1:ef619b6: 
1:bb4a2f7:                 if (forceToDisk) {
1:ef619b6:                     file.sync();
1:bb4a2f7:                 }
1:bb4a2f7: 
1:bb4a2f7:                 Journal.WriteCommand lastWrite = wb.writes.getTail();
1:bb4a2f7:                 journal.setLastAppendLocation(lastWrite.location);
1:89f22da:                 signalDone(wb);
1:bb4a2f7: 
1:bb4a2f7: 
1:bb4a2f7:             }
1:bb4a2f7:         } catch (IOException e) {
1:bb4a2f7:             synchronized (enqueueMutex) {
1:bb4a2f7:                 firstAsyncException = e;
1:bb4a2f7:                 if (wb != null) {
1:bb4a2f7:                     wb.exception.set(e);
2:bb4a2f7:                     wb.latch.countDown();
1:bb4a2f7:                 }
1:bb4a2f7:                 if (nextWriteBatch != null) {
1:bb4a2f7:                     nextWriteBatch.exception.set(e);
1:bb4a2f7:                     nextWriteBatch.latch.countDown();
1:bb4a2f7:                 }
1:bb4a2f7:             }
4:bb4a2f7:         } catch (InterruptedException e) {
1:bb4a2f7:         } finally {
1:bb4a2f7:             try {
1:bb4a2f7:                 if (file != null) {
1:1a59827:                     if (periodicSync) {
1:1a59827:                         if (logger.isTraceEnabled()) {
1:1a59827:                             logger.trace("Syning file {} on close", dataFile.getFile().getName());
1:1a59827:                         }
1:1a59827:                         file.sync();
1:1a59827:                     }
1:bb4a2f7:                     dataFile.closeRandomAccessFile(file);
1:bb4a2f7:                 }
1:bb4a2f7:             } catch (Throwable ignore) {
1:bb4a2f7:             }
2:bb4a2f7:             shutdownDone.countDown();
1:bb4a2f7:             running = false;
1:bb4a2f7:         }
1:bb4a2f7:     }
1:bb4a2f7: 
1:bb4a2f7:     private boolean appendToBuffer(Journal.WriteCommand write, DataByteArrayOutputStream buff) throws IOException {
1:bb4a2f7:         buff.writeInt(write.location.getSize());
1:bb4a2f7:         buff.writeByte(write.location.getType());
1:bb4a2f7:         buff.write(write.data.getData(), write.data.getOffset(), write.data.getLength());
1:89f22da:         return write.sync | (syncOnComplete && write.onComplete != null);
1:bb4a2f7:     }
1:bb4a2f7: }
============================================================================
author:Christopher L. Shannon (cshannon)
-------------------------------------------------------------------------------
commit:1a59827
/////////////////////////////////////////////////////////////////////////
1: import org.slf4j.Logger;
1: import org.slf4j.LoggerFactory;
/////////////////////////////////////////////////////////////////////////
1:     private static final Logger logger = LoggerFactory.getLogger(CallerBufferingDataFileAppender.class);
1: 
/////////////////////////////////////////////////////////////////////////
1:                         if (periodicSync) {
1:                             if (logger.isTraceEnabled()) {
1:                                 logger.trace("Syning file {} on rotate", dataFile.getFile().getName());
1:                             }
1:                             file.sync();
1:                         }
/////////////////////////////////////////////////////////////////////////
1:                     if (periodicSync) {
1:                         if (logger.isTraceEnabled()) {
1:                             logger.trace("Syning file {} on close", dataFile.getFile().getName());
1:                         }
1:                         file.sync();
1:                     }
author:gtully
-------------------------------------------------------------------------------
commit:95f7262
/////////////////////////////////////////////////////////////////////////
author:Timothy Bish
-------------------------------------------------------------------------------
commit:ef619b6
/////////////////////////////////////////////////////////////////////////
1: 
1: import org.apache.activemq.util.ByteSequence;
/////////////////////////////////////////////////////////////////////////
1:  *
/////////////////////////////////////////////////////////////////////////
1:         @Override
/////////////////////////////////////////////////////////////////////////
1: 
1:                 // Now we can fill in the batch control record properly.
1:                     Checksum checksum = new Adler32();
1:                     checksum.update(sequence.getData(), sequence.getOffset()+Journal.BATCH_CONTROL_RECORD_SIZE, sequence.getLength()-Journal.BATCH_CONTROL_RECORD_SIZE);
1:                     buff.writeLong(checksum.getValue());
/////////////////////////////////////////////////////////////////////////
1:                     replicationTarget.replicate(wb.writes.getHead().location, sequence, forceToDisk);
1: 
1:                     file.sync();
commit:97b12c7
/////////////////////////////////////////////////////////////////////////
0:                     file.getFD().sync();
commit:c50b6c3
/////////////////////////////////////////////////////////////////////////
0:                 	file.getChannel().force(false);
author:Dejan Bosanac
-------------------------------------------------------------------------------
commit:582af3e
/////////////////////////////////////////////////////////////////////////
1: import org.apache.activemq.util.RecoverableRandomAccessFile;
/////////////////////////////////////////////////////////////////////////
1:         RecoverableRandomAccessFile file = null;
author:Hiram R. Chirino
-------------------------------------------------------------------------------
commit:c5cf038
commit:1aab71b
/////////////////////////////////////////////////////////////////////////
1: package org.apache.activemq.store.kahadb.disk.journal;
0: import org.apache.activemq.util.ByteSequence;
1: import org.apache.activemq.store.kahadb.disk.util.DataByteArrayOutputStream;
commit:715010a
author:Gary Tully
-------------------------------------------------------------------------------
commit:89f22da
/////////////////////////////////////////////////////////////////////////
/////////////////////////////////////////////////////////////////////////
1: class CallerBufferingDataFileAppender extends DataFileAppender {
1:     public class WriteBatch extends DataFileAppender.WriteBatch {
1:         private boolean forceToDisk;
1:             super(dataFile, offset);
1:             super.append(write);
1:     @Override
1:     protected DataFileAppender.WriteBatch newWriteBatch(Journal.WriteCommand write, DataFile file) throws IOException {
1:         return new WriteBatch(file, file.getLength(), write);
1:     }
1: 
/////////////////////////////////////////////////////////////////////////
1:         super(dataManager);
/////////////////////////////////////////////////////////////////////////
1:     @Override
/////////////////////////////////////////////////////////////////////////
1:                 signalDone(wb);
/////////////////////////////////////////////////////////////////////////
1:         return write.sync | (syncOnComplete && write.onComplete != null);
commit:384388f
/////////////////////////////////////////////////////////////////////////
1:     volatile byte flip = 0x1;
1:         DataByteArrayOutputStream buff = cachedBuffers[flip ^= 1];
commit:bb4a2f7
/////////////////////////////////////////////////////////////////////////
1: /**
1:  * Licensed to the Apache Software Foundation (ASF) under one or more
1:  * contributor license agreements.  See the NOTICE file distributed with
1:  * this work for additional information regarding copyright ownership.
1:  * The ASF licenses this file to You under the Apache License, Version 2.0
1:  * (the "License"); you may not use this file except in compliance with
1:  * the License.  You may obtain a copy of the License at
1:  *
1:  *      http://www.apache.org/licenses/LICENSE-2.0
1:  *
1:  * Unless required by applicable law or agreed to in writing, software
1:  * distributed under the License is distributed on an "AS IS" BASIS,
1:  * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
1:  * See the License for the specific language governing permissions and
1:  * limitations under the License.
1:  */
0: package org.apache.kahadb.journal;
1: 
1: import java.io.IOException;
0: import java.io.InterruptedIOException;
0: import java.io.RandomAccessFile;
0: import java.util.Map;
0: import java.util.concurrent.CountDownLatch;
0: import java.util.concurrent.atomic.AtomicInteger;
0: import java.util.concurrent.atomic.AtomicReference;
1: import java.util.zip.Adler32;
1: import java.util.zip.Checksum;
0: import org.apache.kahadb.util.ByteSequence;
0: import org.apache.kahadb.util.DataByteArrayOutputStream;
0: import org.apache.kahadb.util.LinkedNodeList;
1: 
1: /**
1:  * An optimized writer to do batch appends to a data file. This object is thread
1:  * safe and gains throughput as you increase the number of concurrent writes it
1:  * does.
1:  * The thread calling enqueue does the file open and buffering of the data, which
1:  * reduces the round trip of the write thread.
1:  * 
1:  */
0: class CallerBufferingDataFileAppender implements FileAppender {
1: 
0:     protected final Journal journal;
0:     protected final Map<Journal.WriteKey, Journal.WriteCommand> inflightWrites;
0:     protected final Object enqueueMutex = new Object() {
1:     };
0:     protected WriteBatch nextWriteBatch;
1: 
0:     protected boolean shutdown;
0:     protected IOException firstAsyncException;
0:     protected final CountDownLatch shutdownDone = new CountDownLatch(1);
0:     protected int maxWriteBatchSize;
1: 
0:     private boolean running;
0:     private Thread thread;
1: 
1:     final DataByteArrayOutputStream cachedBuffers[] = new DataByteArrayOutputStream[] {
1:             new DataByteArrayOutputStream(maxWriteBatchSize),
1:             new DataByteArrayOutputStream(maxWriteBatchSize)
1:     };
0:     AtomicInteger writeBatchInstanceCount = new AtomicInteger();
0:     public class WriteBatch {
1: 
0:         DataByteArrayOutputStream buff = cachedBuffers[writeBatchInstanceCount.getAndIncrement()%2];
0:         public final DataFile dataFile;
1: 
0:         public final LinkedNodeList<Journal.WriteCommand> writes = new LinkedNodeList<Journal.WriteCommand>();
0:         public final CountDownLatch latch = new CountDownLatch(1);
0: 		private final int offset;
0:         public int size = Journal.BATCH_CONTROL_RECORD_SIZE;
0:         public AtomicReference<IOException> exception = new AtomicReference<IOException>();
0:         public boolean forceToDisk;
1: 
1:         public WriteBatch(DataFile dataFile, int offset, Journal.WriteCommand write) throws IOException {
0:             this.dataFile = dataFile;
0: 			this.offset = offset;
0:             this.dataFile.incrementLength(Journal.BATCH_CONTROL_RECORD_SIZE);
0:             this.size=Journal.BATCH_CONTROL_RECORD_SIZE;
0:             journal.addToTotalLength(Journal.BATCH_CONTROL_RECORD_SIZE);
1:             initBuffer(buff);
1:             append(write);
1:         }
1: 
0:         public boolean canAppend(Journal.WriteCommand write) {
0:             int newSize = size + write.location.getSize();
0: 			if (newSize >= maxWriteBatchSize || offset+newSize > journal.getMaxFileLength() ) {
0:                 return false;
1:             }
0:             return true;
1:         }
1: 
1:         public void append(Journal.WriteCommand write) throws IOException {
0:             this.writes.addLast(write);
0:             write.location.setDataFileId(dataFile.getDataFileId());
0:             write.location.setOffset(offset+size);
0:             int s = write.location.getSize();
0: 			size += s;
0:             dataFile.incrementLength(s);
0:             journal.addToTotalLength(s);
1:             forceToDisk |= appendToBuffer(write, buff);
1:         }
1:     }
1: 
1:     private void initBuffer(DataByteArrayOutputStream buff) throws IOException {
1:         // Write an empty batch control record.
1:         buff.reset();
1:         buff.write(Journal.BATCH_CONTROL_RECORD_HEADER);
1:         buff.writeInt(0);
1:         buff.writeLong(0);
1:     }
1: 
1:     /**
0:      * Construct a Store writer
1:      */
1:     public CallerBufferingDataFileAppender(Journal dataManager) {
0:         this.journal = dataManager;
0:         this.inflightWrites = this.journal.getInflightWrites();
0:         this.maxWriteBatchSize = this.journal.getWriteBatchSize();
1:     }
1: 
0:     public Location storeItem(ByteSequence data, byte type, boolean sync) throws IOException {
1:     	
0:         // Write the packet our internal buffer.
0:         int size = data.getLength() + Journal.RECORD_HEAD_SPACE;
1: 
0:         final Location location = new Location();
0:         location.setSize(size);
0:         location.setType(type);
1: 
0:         Journal.WriteCommand write = new Journal.WriteCommand(location, data, sync);
1: 
0:         WriteBatch batch = enqueue(write);
0:         location.setLatch(batch.latch);
0:         if (sync) {
1:             try {
0:                 batch.latch.await();
1:             } catch (InterruptedException e) {
0:                 throw new InterruptedIOException();
1:             }
0:             IOException exception = batch.exception.get(); 
0:             if (exception != null) {
0:             	throw exception;
1:             }
1:         }	
1: 
0:         return location;
1:     }
1: 
0:     public Location storeItem(ByteSequence data, byte type, Runnable onComplete) throws IOException {
0:         // Write the packet our internal buffer.
0:         int size = data.getLength() + Journal.RECORD_HEAD_SPACE;
1: 
0:         final Location location = new Location();
0:         location.setSize(size);
0:         location.setType(type);
1: 
0:         Journal.WriteCommand write = new Journal.WriteCommand(location, data, onComplete);
1: 
0:         WriteBatch batch = enqueue(write);
1:  
0:         location.setLatch(batch.latch);
0:         return location;
1:     }
1: 
0:     private WriteBatch enqueue(Journal.WriteCommand write) throws IOException {
1:         synchronized (enqueueMutex) {
1:             if (shutdown) {
0:                 throw new IOException("Async Writter Thread Shutdown");
1:             }
1:             
0:             if (!running) {
0:                 running = true;
0:                 thread = new Thread() {
0:                     public void run() {
0:                         processQueue();
1:                     }
1:                 };
0:                 thread.setPriority(Thread.MAX_PRIORITY);
0:                 thread.setDaemon(true);
0:                 thread.setName("ActiveMQ Data File Writer");
0:                 thread.start();
0:                 firstAsyncException = null;
1:             }
1:             
0:             if (firstAsyncException != null) {
0:                 throw firstAsyncException;
1:             }
1: 
0:             while ( true ) {
0: 	            if (nextWriteBatch == null) {
0: 	            	DataFile file = journal.getCurrentWriteFile();
0: 	            	if( file.getLength() > journal.getMaxFileLength() ) {
0: 	            		file = journal.rotateWriteFile();
1: 	            	}
1: 
0: 	                nextWriteBatch = new WriteBatch(file, file.getLength(), write);
1: 	                enqueueMutex.notifyAll();
1: 	                break;
1: 	            } else {
0: 	                // Append to current batch if possible..
0: 	                if (nextWriteBatch.canAppend(write)) {
0: 	                    nextWriteBatch.append(write);
1: 	                    break;
1: 	                } else {
0: 	                    // Otherwise wait for the queuedCommand to be null
1: 	                    try {
0: 	                        while (nextWriteBatch != null) {
0: 	                            final long start = System.currentTimeMillis();
1: 	                            enqueueMutex.wait();
1: 	                            if (maxStat > 0) { 
0: 	                                System.err.println("Watiting for write to finish with full batch... millis: " + (System.currentTimeMillis() - start));
1: 	                            }
1: 	                        }
1: 	                    } catch (InterruptedException e) {
0: 	                        throw new InterruptedIOException();
1: 	                    }
1: 	                    if (shutdown) {
0: 	                        throw new IOException("Async Writter Thread Shutdown");
1: 	                    }
1: 	                }
1: 	            }
1:             }
0:             if (!write.sync) {
0:                 inflightWrites.put(new Journal.WriteKey(write.location), write);
1:             }
0:             return nextWriteBatch;
1:         }
1:     }
1: 
0:     public void close() throws IOException {
1:         synchronized (enqueueMutex) {
0:             if (!shutdown) {
0:                 shutdown = true;
0:                 if (running) {
1:                     enqueueMutex.notifyAll();
1:                 } else {
1:                     shutdownDone.countDown();
1:                 }
1:             }
1:         }
1: 
1:         try {
0:             shutdownDone.await();
1:         } catch (InterruptedException e) {
0:             throw new InterruptedIOException();
1:         }
1: 
1:     }
1: 
0:     public static final String PROPERTY_LOG_WRITE_STAT_WINDOW = "org.apache.kahadb.journal.appender.WRITE_STAT_WINDOW";
0:     public static final int maxStat = Integer.parseInt(System.getProperty(PROPERTY_LOG_WRITE_STAT_WINDOW, "0"));
0:     int statIdx = 0;
0:     int[] stats = new int[maxStat];
1:     /**
1:      * The async processing loop that writes to the data files and does the
1:      * force calls. Since the file sync() call is the slowest of all the
1:      * operations, this algorithm tries to 'batch' or group together several
1:      * file sync() requests into a single file sync() call. The batching is
1:      * accomplished attaching the same CountDownLatch instance to every force
1:      * request in a group.
1:      */
1:     protected void processQueue() {
1:         DataFile dataFile = null;
0:         RandomAccessFile file = null;
1:         WriteBatch wb = null;
1:         try {
1: 
1:             while (true) {
1: 
1:                 Object o = null;
1: 
1:                 // Block till we get a command.
1:                 synchronized (enqueueMutex) {
1:                     while (true) {
1:                         if (nextWriteBatch != null) {
1:                             o = nextWriteBatch;
1:                             nextWriteBatch = null;
1:                             break;
1:                         }
1:                         if (shutdown) {
1:                             return;
1:                         }
1:                         enqueueMutex.wait();
1:                     }
1:                     enqueueMutex.notifyAll();
1:                 }
1: 
1:                 wb = (WriteBatch)o;
1:                 if (dataFile != wb.dataFile) {
1:                     if (file != null) {
0:                         file.setLength(dataFile.getLength());
1:                         dataFile.closeRandomAccessFile(file);
1:                     }
1:                     dataFile = wb.dataFile;
1:                     file = dataFile.openRandomAccessFile();
0:                     if( file.length() < journal.preferedFileLength ) {
0:                         file.setLength(journal.preferedFileLength);
1:                     }
1:                 }
1: 
1:                 final DataByteArrayOutputStream buff = wb.buff;
1:                 final boolean forceToDisk = wb.forceToDisk;
1: 
1:                 ByteSequence sequence = buff.toByteSequence();
1:                 
0:                 // Now we can fill in the batch control record properly. 
1:                 buff.reset();
1:                 buff.skip(5+Journal.BATCH_CONTROL_RECORD_MAGIC.length);
1:                 buff.writeInt(sequence.getLength()-Journal.BATCH_CONTROL_RECORD_SIZE);
1:                 if( journal.isChecksum() ) {
0: 	                Checksum checksum = new Adler32();
0: 	                checksum.update(sequence.getData(), sequence.getOffset()+Journal.BATCH_CONTROL_RECORD_SIZE, sequence.getLength()-Journal.BATCH_CONTROL_RECORD_SIZE);
0: 	                buff.writeLong(checksum.getValue());
1:                 }
1: 
1:                 // Now do the 1 big write.
1:                 file.seek(wb.offset);
1:                 if (maxStat > 0) {
1:                     if (statIdx < maxStat) {
1:                         stats[statIdx++] = sequence.getLength();
1:                     } else {
1:                         long all = 0;
1:                         for (;statIdx > 0;) {
1:                             all+= stats[--statIdx];
1:                         }
1:                         System.err.println("Ave writeSize: " + all/maxStat);
1:                     }
1:                 }
1:                 file.write(sequence.getData(), sequence.getOffset(), sequence.getLength());
1:                 ReplicationTarget replicationTarget = journal.getReplicationTarget();
1:                 if( replicationTarget!=null ) {
0:                 	replicationTarget.replicate(wb.writes.getHead().location, sequence, forceToDisk);
1:                 }
1:                 
1:                 if (forceToDisk) {
0:                     file.getFD().sync();
1:                 }
1: 
1:                 Journal.WriteCommand lastWrite = wb.writes.getTail();
1:                 journal.setLastAppendLocation(lastWrite.location);
1: 
0:                 // Now that the data is on disk, remove the writes from the in
0:                 // flight
0:                 // cache.
0:                 Journal.WriteCommand write = wb.writes.getHead();
0:                 while (write != null) {
0:                     if (!write.sync) {
0:                         inflightWrites.remove(new Journal.WriteKey(write.location));
1:                     }
0:                     if (write.onComplete != null) {
1:                         try {
0:                             write.onComplete.run();
0:                         } catch (Throwable e) {
0:                             e.printStackTrace();
1:                         }
1:                     }
0:                     write = write.getNext();
1:                 }
1: 
0:                 // Signal any waiting threads that the write is on disk.
1:                 wb.latch.countDown();
1:             }
1:         } catch (IOException e) {
1:             synchronized (enqueueMutex) {
1:                 firstAsyncException = e;
1:                 if (wb != null) {
1:                     wb.exception.set(e);
1:                     wb.latch.countDown();
1:                 }
1:                 if (nextWriteBatch != null) {
1:                     nextWriteBatch.exception.set(e);
1:                     nextWriteBatch.latch.countDown();
1:                 }
1:             }
1:         } catch (InterruptedException e) {
1:         } finally {
1:             try {
1:                 if (file != null) {
1:                     dataFile.closeRandomAccessFile(file);
1:                 }
1:             } catch (Throwable ignore) {
1:             }
1:             shutdownDone.countDown();
1:             running = false;
1:         }
1:     }
1: 
1:     private boolean appendToBuffer(Journal.WriteCommand write, DataByteArrayOutputStream buff) throws IOException {
1:         buff.writeInt(write.location.getSize());
1:         buff.writeByte(write.location.getType());
1:         buff.write(write.data.getData(), write.data.getOffset(), write.data.getLength());
0:         return write.sync | write.onComplete != null;
1:     }
1: }
============================================================================