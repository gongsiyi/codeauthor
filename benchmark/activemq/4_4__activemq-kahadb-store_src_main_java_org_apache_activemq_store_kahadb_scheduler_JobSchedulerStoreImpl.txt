1:0484af1: /**
1:0484af1:  * Licensed to the Apache Software Foundation (ASF) under one or more
1:0484af1:  * contributor license agreements.  See the NOTICE file distributed with
1:0484af1:  * this work for additional information regarding copyright ownership.
1:0484af1:  * The ASF licenses this file to You under the Apache License, Version 2.0
1:0484af1:  * (the "License"); you may not use this file except in compliance with
1:0484af1:  * the License.  You may obtain a copy of the License at
1:0484af1:  *
1:0484af1:  *      http://www.apache.org/licenses/LICENSE-2.0
1:0484af1:  *
1:0484af1:  * Unless required by applicable law or agreed to in writing, software
1:0484af1:  * distributed under the License is distributed on an "AS IS" BASIS,
1:0484af1:  * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
1:0484af1:  * See the License for the specific language governing permissions and
1:0484af1:  * limitations under the License.
1:0484af1:  */
1:0484af1: package org.apache.activemq.store.kahadb.scheduler;
1:f21992e: 
1:0484af1: import java.io.DataInput;
1:0484af1: import java.io.DataOutput;
1:0484af1: import java.io.File;
1:74846bb: import java.io.FilenameFilter;
1:0484af1: import java.io.IOException;
1:0484af1: import java.util.ArrayList;
1:74846bb: import java.util.Collection;
1:0484af1: import java.util.HashMap;
1:0484af1: import java.util.HashSet;
1:0484af1: import java.util.Iterator;
1:0484af1: import java.util.List;
1:0484af1: import java.util.Map;
1:0484af1: import java.util.Map.Entry;
1:0484af1: import java.util.Set;
1:74846bb: import java.util.TreeSet;
1:74846bb: import java.util.UUID;
1:f21992e: 
1:f21992e: import org.apache.activemq.broker.scheduler.JobScheduler;
1:f21992e: import org.apache.activemq.broker.scheduler.JobSchedulerStore;
1:74846bb: import org.apache.activemq.protobuf.Buffer;
1:74846bb: import org.apache.activemq.store.kahadb.AbstractKahaDBStore;
1:74846bb: import org.apache.activemq.store.kahadb.JournalCommand;
1:74846bb: import org.apache.activemq.store.kahadb.KahaDBMetaData;
1:74846bb: import org.apache.activemq.store.kahadb.Visitor;
1:74846bb: import org.apache.activemq.store.kahadb.data.KahaAddScheduledJobCommand;
1:74846bb: import org.apache.activemq.store.kahadb.data.KahaDestroySchedulerCommand;
1:74846bb: import org.apache.activemq.store.kahadb.data.KahaRemoveScheduledJobCommand;
1:74846bb: import org.apache.activemq.store.kahadb.data.KahaRemoveScheduledJobsCommand;
1:74846bb: import org.apache.activemq.store.kahadb.data.KahaRescheduleJobCommand;
1:74846bb: import org.apache.activemq.store.kahadb.data.KahaTraceCommand;
1:74846bb: import org.apache.activemq.store.kahadb.disk.index.BTreeVisitor;
1:74846bb: import org.apache.activemq.store.kahadb.disk.journal.DataFile;
1:f21992e: import org.apache.activemq.store.kahadb.disk.journal.Location;
1:f21992e: import org.apache.activemq.store.kahadb.disk.page.Page;
1:f21992e: import org.apache.activemq.store.kahadb.disk.page.PageFile;
1:f21992e: import org.apache.activemq.store.kahadb.disk.page.Transaction;
1:f21992e: import org.apache.activemq.store.kahadb.disk.util.VariableMarshaller;
1:74846bb: import org.apache.activemq.store.kahadb.scheduler.legacy.LegacyStoreReplayer;
1:f21992e: import org.apache.activemq.util.ByteSequence;
1:f21992e: import org.apache.activemq.util.IOHelper;
1:f21992e: import org.slf4j.Logger;
1:f21992e: import org.slf4j.LoggerFactory;
1:f21992e: 
1:74846bb: public class JobSchedulerStoreImpl extends AbstractKahaDBStore implements JobSchedulerStore {
36:0484af1: 
1:74846bb:     private static final Logger LOG = LoggerFactory.getLogger(JobSchedulerStoreImpl.class);
1:0484af1: 
1:74846bb:     private JobSchedulerKahaDBMetaData metaData = new JobSchedulerKahaDBMetaData(this);
1:74846bb:     private final MetaDataMarshaller metaDataMarshaller = new MetaDataMarshaller(this);
1:74846bb:     private final Map<String, JobSchedulerImpl> schedulers = new HashMap<String, JobSchedulerImpl>();
1:74846bb:     private File legacyStoreArchiveDirectory;
1:0484af1: 
1:74846bb:     /**
1:74846bb:      * The Scheduler Token is used to identify base revisions of the Scheduler store.  A store
1:74846bb:      * based on the initial scheduler design will not have this tag in it's meta-data and will
1:74846bb:      * indicate an update is needed.  Later versions of the scheduler can also change this value
1:74846bb:      * to indicate incompatible store bases which require complete meta-data and journal rewrites
1:74846bb:      * instead of simpler meta-data updates.
1:74846bb:      */
1:74846bb:     static final UUID SCHEDULER_STORE_TOKEN = UUID.fromString("57ed642b-1ee3-47b3-be6d-b7297d500409");
1:0484af1: 
1:74846bb:     /**
1:74846bb:      * The default scheduler store version.  All new store instance will be given this version and
1:74846bb:      * earlier versions will be updated to this version.
1:74846bb:      */
1:74846bb:     static final int CURRENT_VERSION = 1;
1:0484af1: 
1:74846bb:     @Override
1:74846bb:     public JobScheduler getJobScheduler(final String name) throws Exception {
1:74846bb:         this.indexLock.writeLock().lock();
1:74846bb:         try {
1:74846bb:             JobSchedulerImpl result = this.schedulers.get(name);
1:74846bb:             if (result == null) {
1:74846bb:                 final JobSchedulerImpl js = new JobSchedulerImpl(this);
1:74846bb:                 js.setName(name);
1:74846bb:                 getPageFile().tx().execute(new Transaction.Closure<IOException>() {
1:74846bb:                     @Override
1:74846bb:                     public void execute(Transaction tx) throws IOException {
1:74846bb:                         js.createIndexes(tx);
1:74846bb:                         js.load(tx);
1:74846bb:                         metaData.getJobSchedulers().put(tx, name, js);
1:74846bb:                     }
1:74846bb:                 });
1:74846bb:                 result = js;
1:74846bb:                 this.schedulers.put(name, js);
1:74846bb:                 if (isStarted()) {
1:74846bb:                     result.start();
1:74846bb:                 }
1:74846bb:                 this.pageFile.flush();
54:0484af1:             }
1:74846bb:             return result;
1:74846bb:         } finally {
1:74846bb:             this.indexLock.writeLock().unlock();
1:0484af1:         }
1:0484af1:     }
1:f21992e: 
1:74846bb:     @Override
1:74846bb:     public boolean removeJobScheduler(final String name) throws Exception {
1:74846bb:         boolean result = false;
1:74846bb: 
1:74846bb:         this.indexLock.writeLock().lock();
1:74846bb:         try {
1:74846bb:             final JobSchedulerImpl js = this.schedulers.remove(name);
1:74846bb:             result = js != null;
1:74846bb:             if (result) {
1:74846bb:                 js.stop();
1:74846bb:                 getPageFile().tx().execute(new Transaction.Closure<IOException>() {
1:74846bb:                     @Override
1:74846bb:                     public void execute(Transaction tx) throws IOException {
1:74846bb:                         metaData.getJobSchedulers().remove(tx, name);
1:74846bb:                         js.removeAll(tx);
1:74846bb:                     }
1:74846bb:                 });
1:74846bb:             }
1:74846bb:         } finally {
1:74846bb:             this.indexLock.writeLock().unlock();
1:74846bb:         }
1:74846bb:         return result;
1:74846bb:     }
1:74846bb: 
1:74846bb:     /**
1:74846bb:      * Sets the directory where the legacy scheduler store files are archived before an
1:74846bb:      * update attempt is made.  Both the legacy index files and the journal files are moved
1:74846bb:      * to this folder prior to an upgrade attempt.
1:74846bb:      *
1:74846bb:      * @param directory
1:74846bb:      *      The directory to move the legacy Scheduler Store files to.
1:74846bb:      */
1:74846bb:     public void setLegacyStoreArchiveDirectory(File directory) {
1:74846bb:         this.legacyStoreArchiveDirectory = directory;
1:74846bb:     }
1:74846bb: 
1:74846bb:     /**
1:74846bb:      * Gets the directory where the legacy Scheduler Store files will be archived if the
1:74846bb:      * broker is started and an existing Job Scheduler Store from an old version is detected.
1:74846bb:      *
1:74846bb:      * @return the directory where scheduler store legacy files are archived on upgrade.
1:74846bb:      */
1:74846bb:     public File getLegacyStoreArchiveDirectory() {
1:74846bb:         if (this.legacyStoreArchiveDirectory == null) {
1:74846bb:             this.legacyStoreArchiveDirectory = new File(getDirectory(), "legacySchedulerStore");
1:74846bb:         }
1:74846bb: 
1:74846bb:         return this.legacyStoreArchiveDirectory.getAbsoluteFile();
1:74846bb:     }
1:74846bb: 
1:74846bb:     @Override
1:74846bb:     public void load() throws IOException {
1:74846bb:         if (opened.compareAndSet(false, true)) {
1:74846bb:             getJournal().start();
1:74846bb:             try {
1:74846bb:                 loadPageFile();
1:74846bb:             } catch (UnknownStoreVersionException ex) {
1:74846bb:                 LOG.info("Can't start until store update is performed.");
1:74846bb:                 upgradeFromLegacy();
1:74846bb:                 // Restart with the updated store
1:74846bb:                 getJournal().start();
1:74846bb:                 loadPageFile();
1:74846bb:                 LOG.info("Update from legacy Scheduler store completed successfully.");
1:74846bb:             } catch (Throwable t) {
1:74846bb:                 LOG.warn("Index corrupted. Recovering the index through journal replay. Cause: {}", t.toString());
1:74846bb:                 LOG.debug("Index load failure", t);
1:74846bb: 
1:74846bb:                 // try to recover index
1:74846bb:                 try {
1:74846bb:                     pageFile.unload();
1:74846bb:                 } catch (Exception ignore) {
1:74846bb:                 }
1:74846bb:                 if (isArchiveCorruptedIndex()) {
1:74846bb:                     pageFile.archive();
1:74846bb:                 } else {
1:74846bb:                     pageFile.delete();
1:74846bb:                 }
1:74846bb:                 metaData = new JobSchedulerKahaDBMetaData(this);
1:74846bb:                 pageFile = null;
1:74846bb:                 loadPageFile();
1:74846bb:             }
1:74846bb:             startCheckpoint();
1:74846bb:             recover();
1:74846bb:         }
1:74846bb:         LOG.info("{} started.", this);
1:74846bb:     }
1:74846bb: 
1:74846bb:     @Override
1:74846bb:     public void unload() throws IOException {
1:74846bb:         if (opened.compareAndSet(true, false)) {
1:74846bb:             for (JobSchedulerImpl js : this.schedulers.values()) {
1:74846bb:                 try {
1:74846bb:                     js.stop();
1:74846bb:                 } catch (Exception e) {
1:74846bb:                     throw new IOException(e);
1:74846bb:                 }
1:74846bb:             }
1:74846bb:             this.indexLock.writeLock().lock();
1:74846bb:             try {
1:74846bb:                 if (pageFile != null && pageFile.isLoaded()) {
1:74846bb:                     metaData.setState(KahaDBMetaData.CLOSED_STATE);
1:74846bb: 
1:74846bb:                     if (metaData.getPage() != null) {
1:74846bb:                         pageFile.tx().execute(new Transaction.Closure<IOException>() {
1:74846bb:                             @Override
1:74846bb:                             public void execute(Transaction tx) throws IOException {
1:74846bb:                                 tx.store(metaData.getPage(), metaDataMarshaller, true);
1:74846bb:                             }
1:74846bb:                         });
1:74846bb:                     }
1:74846bb:                 }
1:74846bb:             } finally {
1:74846bb:                 this.indexLock.writeLock().unlock();
1:74846bb:             }
1:74846bb: 
1:74846bb:             checkpointLock.writeLock().lock();
1:74846bb:             try {
1:74846bb:                 if (metaData.getPage() != null) {
1:74846bb:                     checkpointUpdate(true);
1:74846bb:                 }
1:74846bb:             } finally {
1:74846bb:                 checkpointLock.writeLock().unlock();
1:74846bb:             }
1:74846bb:             synchronized (checkpointThreadLock) {
1:74846bb:                 if (checkpointThread != null) {
1:74846bb:                     try {
1:74846bb:                         checkpointThread.join();
1:74846bb:                         checkpointThread = null;
1:74846bb:                     } catch (InterruptedException e) {
1:74846bb:                     }
1:74846bb:                 }
1:74846bb:             }
1:74846bb: 
1:74846bb:             if (pageFile != null) {
1:74846bb:                 pageFile.unload();
1:74846bb:                 pageFile = null;
1:74846bb:             }
1:74846bb:             if (this.journal != null) {
1:74846bb:                 journal.close();
1:74846bb:                 journal = null;
1:74846bb:             }
1:74846bb: 
1:74846bb:             metaData = new JobSchedulerKahaDBMetaData(this);
1:74846bb:         }
1:74846bb:         LOG.info("{} stopped.", this);
1:74846bb:     }
1:74846bb: 
1:74846bb:     private void loadPageFile() throws IOException {
1:74846bb:         this.indexLock.writeLock().lock();
1:74846bb:         try {
1:74846bb:             final PageFile pageFile = getPageFile();
1:74846bb:             pageFile.load();
1:74846bb:             pageFile.tx().execute(new Transaction.Closure<IOException>() {
1:74846bb:                 @Override
1:74846bb:                 public void execute(Transaction tx) throws IOException {
1:74846bb:                     if (pageFile.getPageCount() == 0) {
1:74846bb:                         Page<JobSchedulerKahaDBMetaData> page = tx.allocate();
1:74846bb:                         assert page.getPageId() == 0;
1:74846bb:                         page.set(metaData);
1:74846bb:                         metaData.setPage(page);
1:74846bb:                         metaData.setState(KahaDBMetaData.CLOSED_STATE);
1:74846bb:                         metaData.initialize(tx);
1:74846bb:                         tx.store(metaData.getPage(), metaDataMarshaller, true);
1:74846bb:                     } else {
1:74846bb:                         Page<JobSchedulerKahaDBMetaData> page = null;
1:74846bb:                         page = tx.load(0, metaDataMarshaller);
1:74846bb:                         metaData = page.get();
1:74846bb:                         metaData.setPage(page);
1:74846bb:                     }
1:74846bb:                     metaData.load(tx);
1:74846bb:                     metaData.loadScheduler(tx, schedulers);
1:74846bb:                     for (JobSchedulerImpl js : schedulers.values()) {
1:74846bb:                         try {
1:74846bb:                             js.start();
1:74846bb:                         } catch (Exception e) {
1:74846bb:                             JobSchedulerStoreImpl.LOG.error("Failed to load " + js.getName(), e);
1:74846bb:                         }
1:74846bb:                     }
1:74846bb:                 }
1:74846bb:             });
1:74846bb: 
1:74846bb:             pageFile.flush();
1:74846bb:         } finally {
1:74846bb:             this.indexLock.writeLock().unlock();
1:74846bb:         }
1:74846bb:     }
1:74846bb: 
1:74846bb:     private void upgradeFromLegacy() throws IOException {
1:74846bb: 
1:74846bb:         journal.close();
1:74846bb:         journal = null;
1:74846bb:         try {
1:74846bb:             pageFile.unload();
1:74846bb:             pageFile = null;
1:74846bb:         } catch (Exception ignore) {}
1:74846bb: 
1:74846bb:         File storeDir = getDirectory().getAbsoluteFile();
1:74846bb:         File storeArchiveDir = getLegacyStoreArchiveDirectory();
1:74846bb: 
1:74846bb:         LOG.info("Attempting to move old store files from {} to {}", storeDir, storeArchiveDir);
1:74846bb: 
1:74846bb:         // Move only the known store files, locks and other items left in place.
1:74846bb:         IOHelper.moveFiles(storeDir, storeArchiveDir, new FilenameFilter() {
1:74846bb: 
1:74846bb:             @Override
1:74846bb:             public boolean accept(File dir, String name) {
1:74846bb:                 if (name.endsWith(".data") || name.endsWith(".redo") || name.endsWith(".log")) {
1:74846bb:                     return true;
1:74846bb:                 }
1:74846bb:                 return false;
1:74846bb:             }
1:74846bb:         });
1:74846bb: 
1:74846bb:         // We reset everything to clean state, then we can read from the old
1:74846bb:         // scheduler store and replay the scheduled jobs into this one as adds.
1:74846bb:         getJournal().start();
1:74846bb:         metaData = new JobSchedulerKahaDBMetaData(this);
1:74846bb:         pageFile = null;
1:74846bb:         loadPageFile();
1:74846bb: 
1:74846bb:         LegacyStoreReplayer replayer = new LegacyStoreReplayer(getLegacyStoreArchiveDirectory());
1:74846bb:         replayer.load();
1:74846bb:         replayer.startReplay(this);
1:74846bb: 
1:74846bb:         // Cleanup after replay and store what we've done.
1:74846bb:         pageFile.tx().execute(new Transaction.Closure<IOException>() {
1:74846bb:             @Override
1:74846bb:             public void execute(Transaction tx) throws IOException {
1:74846bb:                 tx.store(metaData.getPage(), metaDataMarshaller, true);
1:74846bb:             }
1:74846bb:         });
1:74846bb: 
1:74846bb:         checkpointUpdate(true);
1:74846bb:         getJournal().close();
1:74846bb:         getPageFile().unload();
1:74846bb:     }
1:74846bb: 
1:74846bb:     @Override
1:74846bb:     protected void checkpointUpdate(Transaction tx, boolean cleanup) throws IOException {
1:74846bb:         LOG.debug("Job Scheduler Store Checkpoint started.");
1:74846bb: 
1:74846bb:         // reflect last update exclusive of current checkpoint
1:74846bb:         Location lastUpdate = metaData.getLastUpdateLocation();
1:74846bb:         metaData.setState(KahaDBMetaData.OPEN_STATE);
1:74846bb:         tx.store(metaData.getPage(), metaDataMarshaller, true);
1:74846bb:         pageFile.flush();
1:74846bb: 
1:74846bb:         if (cleanup) {
1:74846bb:             final TreeSet<Integer> completeFileSet = new TreeSet<Integer>(journal.getFileMap().keySet());
1:74846bb:             final TreeSet<Integer> gcCandidateSet = new TreeSet<Integer>(completeFileSet);
1:74846bb: 
1:74846bb:             LOG.trace("Last update: {}, full gc candidates set: {}", lastUpdate, gcCandidateSet);
1:74846bb: 
1:74846bb:             if (lastUpdate != null) {
1:74846bb:                 gcCandidateSet.remove(lastUpdate.getDataFileId());
1:74846bb:             }
1:74846bb: 
1:74846bb:             this.metaData.getJournalRC().visit(tx, new BTreeVisitor<Integer, Integer>() {
1:74846bb: 
1:74846bb:                 @Override
1:74846bb:                 public void visit(List<Integer> keys, List<Integer> values) {
1:74846bb:                     for (Integer key : keys) {
1:74846bb:                         if (gcCandidateSet.remove(key)) {
1:74846bb:                             LOG.trace("Removed referenced file: {} from GC set", key);
1:74846bb:                         }
1:74846bb:                     }
1:74846bb:                 }
1:74846bb: 
1:74846bb:                 @Override
1:74846bb:                 public boolean isInterestedInKeysBetween(Integer first, Integer second) {
1:74846bb:                     return true;
1:74846bb:                 }
1:74846bb:             });
1:74846bb: 
1:74846bb:             LOG.trace("gc candidates after reference check: {}", gcCandidateSet);
1:74846bb: 
1:74846bb:             // If there are GC candidates then check the remove command location to see
1:74846bb:             // if any of them can go or if they must stay in order to ensure proper recover.
1:74846bb:             //
1:74846bb:             // A log containing any remove commands must be kept until all the logs with the
1:74846bb:             // add commands for all the removed jobs have been dropped.
1:74846bb:             if (!gcCandidateSet.isEmpty()) {
1:74846bb:                 Iterator<Entry<Integer, List<Integer>>> removals = metaData.getRemoveLocationTracker().iterator(tx);
1:74846bb:                 List<Integer> orphans = new ArrayList<Integer>();
1:74846bb:                 while (removals.hasNext()) {
1:8c4b5f4:                     boolean orphanedRemove = true;
1:74846bb:                     Entry<Integer, List<Integer>> entry = removals.next();
1:74846bb: 
1:74846bb:                     // If this log is not a GC candidate then there's no need to do a check to rule it out
1:74846bb:                     if (gcCandidateSet.contains(entry.getKey())) {
1:74846bb:                         for (Integer addLocation : entry.getValue()) {
1:74846bb:                             if (completeFileSet.contains(addLocation)) {
1:8c4b5f4:                                 LOG.trace("A remove in log {} has an add still in existance in {}.", entry.getKey(), addLocation);
1:8c4b5f4:                                 orphanedRemove = false;
1:74846bb:                                 break;
1:74846bb:                             }
1:74846bb:                         }
1:74846bb: 
1:74846bb:                         // If it's not orphaned than we can't remove it, otherwise we
1:74846bb:                         // stop tracking it it's log will get deleted on the next check.
1:8c4b5f4:                         if (!orphanedRemove) {
1:74846bb:                             gcCandidateSet.remove(entry.getKey());
1:74846bb:                         } else {
1:74846bb:                             LOG.trace("All removes in log {} are orphaned, file can be GC'd", entry.getKey());
1:74846bb:                             orphans.add(entry.getKey());
1:74846bb:                         }
1:74846bb:                     }
1:74846bb:                 }
1:74846bb: 
1:74846bb:                 // Drop all orphaned removes from the tracker.
1:74846bb:                 for (Integer orphan : orphans) {
1:74846bb:                     metaData.getRemoveLocationTracker().remove(tx, orphan);
1:74846bb:                 }
1:74846bb:             }
1:74846bb: 
1:74846bb:             LOG.trace("gc candidates after removals check: {}", gcCandidateSet);
1:74846bb:             if (!gcCandidateSet.isEmpty()) {
1:74846bb:                 if (LOG.isDebugEnabled()) {
1:74846bb:                     LOG.debug("Cleanup removing the data files: " + gcCandidateSet);
1:74846bb:                 }
1:74846bb:                 journal.removeDataFiles(gcCandidateSet);
1:74846bb:             }
1:74846bb:         }
1:74846bb: 
1:74846bb:         LOG.debug("Job Scheduler Store Checkpoint complete.");
1:74846bb:     }
1:74846bb: 
1:74846bb:     /**
1:74846bb:      * Adds a reference for the journal log file pointed to by the given Location value.
1:74846bb:      *
1:74846bb:      * To prevent log files in the journal that still contain valid data that needs to be
1:74846bb:      * kept in order to allow for recovery the logs must have active references.  Each Job
1:74846bb:      * scheduler should ensure that the logs are accurately referenced.
1:74846bb:      *
1:74846bb:      * @param tx
1:74846bb:      *      The TX under which the update is to be performed.
1:74846bb:      * @param location
1:74846bb:      *      The location value to update the reference count of.
1:74846bb:      *
1:74846bb:      * @throws IOException if an error occurs while updating the journal references table.
1:74846bb:      */
1:74846bb:     protected void incrementJournalCount(Transaction tx, Location location) throws IOException {
1:74846bb:         int logId = location.getDataFileId();
1:74846bb:         Integer val = metaData.getJournalRC().get(tx, logId);
1:74846bb:         int refCount = val != null ? val.intValue() + 1 : 1;
1:74846bb:         metaData.getJournalRC().put(tx, logId, refCount);
1:74846bb:     }
1:74846bb: 
1:74846bb:     /**
1:74846bb:      * Removes one reference for the Journal log file indicated in the given Location value.
1:74846bb:      *
1:74846bb:      * The references are used to track which log files cannot be GC'd.  When the reference count
1:74846bb:      * on a log file reaches zero the file id is removed from the tracker and the log will be
1:74846bb:      * removed on the next check point update.
1:74846bb:      *
1:74846bb:      * @param tx
1:74846bb:      *      The TX under which the update is to be performed.
1:74846bb:      * @param location
1:74846bb:      *      The location value to update the reference count of.
1:74846bb:      *
1:74846bb:      * @throws IOException if an error occurs while updating the journal references table.
1:74846bb:      */
1:74846bb:     protected void decrementJournalCount(Transaction tx, Location location) throws IOException {
1:74846bb:         int logId = location.getDataFileId();
1:74846bb:         Integer refCount = metaData.getJournalRC().get(tx, logId);
1:74846bb:         if (refCount != null) {
1:74846bb:             int refCountValue = refCount;
1:74846bb:             refCountValue--;
1:74846bb:             if (refCountValue <= 0) {
1:74846bb:                 metaData.getJournalRC().remove(tx, logId);
1:74846bb:             } else {
1:74846bb:                 metaData.getJournalRC().put(tx, logId, refCountValue);
1:74846bb:             }
1:74846bb:         }
1:74846bb:     }
1:74846bb: 
1:74846bb:     /**
1:74846bb:      * Updates the Job removal tracking index with the location of a remove command and the
1:74846bb:      * original JobLocation entry.
1:74846bb:      *
1:74846bb:      * The JobLocation holds the locations in the logs where the add and update commands for
1:74846bb:      * a job stored.  The log file containing the remove command can only be discarded after
1:74846bb:      * both the add and latest update log files have also been discarded.
1:74846bb:      *
1:74846bb:      * @param tx
1:74846bb:      *      The TX under which the update is to be performed.
1:74846bb:      * @param location
1:74846bb:      *      The location value to reference a remove command.
1:74846bb:      * @param removedJob
1:74846bb:      *      The original JobLocation instance that holds the add and update locations
1:74846bb:      *
1:74846bb:      * @throws IOException if an error occurs while updating the remove location tracker.
1:74846bb:      */
1:74846bb:     protected void referenceRemovedLocation(Transaction tx, Location location, JobLocation removedJob) throws IOException {
1:74846bb:         int logId = location.getDataFileId();
1:74846bb:         List<Integer> removed = this.metaData.getRemoveLocationTracker().get(tx, logId);
1:74846bb:         if (removed == null) {
1:74846bb:             removed = new ArrayList<Integer>();
1:74846bb:         }
1:74846bb:         removed.add(removedJob.getLocation().getDataFileId());
1:74846bb:         this.metaData.getRemoveLocationTracker().put(tx, logId, removed);
1:74846bb:     }
1:74846bb: 
1:74846bb:     /**
1:74846bb:      * Retrieve the scheduled Job's byte blob from the journal.
1:74846bb:      *
1:74846bb:      * @param location
1:74846bb:      *      The location of the KahaAddScheduledJobCommand that originated the Job.
1:74846bb:      *
1:74846bb:      * @return a ByteSequence containing the payload of the scheduled Job.
1:74846bb:      *
1:74846bb:      * @throws IOException if an error occurs while reading the payload value.
1:74846bb:      */
1:74846bb:     protected ByteSequence getPayload(Location location) throws IOException {
1:74846bb:         KahaAddScheduledJobCommand job = (KahaAddScheduledJobCommand) this.load(location);
1:74846bb:         Buffer payload = job.getPayload();
1:74846bb:         return new ByteSequence(payload.getData(), payload.getOffset(), payload.getLength());
1:74846bb:     }
1:74846bb: 
1:74846bb:     public void readLockIndex() {
1:74846bb:         this.indexLock.readLock().lock();
1:74846bb:     }
1:74846bb: 
1:74846bb:     public void readUnlockIndex() {
1:74846bb:         this.indexLock.readLock().unlock();
1:74846bb:     }
1:74846bb: 
1:74846bb:     public void writeLockIndex() {
1:74846bb:         this.indexLock.writeLock().lock();
1:74846bb:     }
1:74846bb: 
1:74846bb:     public void writeUnlockIndex() {
1:74846bb:         this.indexLock.writeLock().unlock();
1:74846bb:     }
1:74846bb: 
1:74846bb:     @Override
1:74846bb:     public String toString() {
1:74846bb:         return "JobSchedulerStore: " + getDirectory();
1:74846bb:     }
1:74846bb: 
1:74846bb:     @Override
1:74846bb:     protected String getPageFileName() {
1:74846bb:         return "scheduleDB";
1:74846bb:     }
1:74846bb: 
1:74846bb:     @Override
1:74846bb:     protected File getDefaultDataDirectory() {
1:74846bb:         return new File(IOHelper.getDefaultDataDirectory(), "delayedDB");
1:74846bb:     }
1:74846bb: 
1:74846bb:     private class MetaDataMarshaller extends VariableMarshaller<JobSchedulerKahaDBMetaData> {
1:74846bb: 
3:0484af1:         private final JobSchedulerStoreImpl store;
1:11ae61f: 
1:0484af1:         MetaDataMarshaller(JobSchedulerStoreImpl store) {
3:0484af1:             this.store = store;
1:11ae61f:         }
1:11ae61f: 
1:11ae61f:         @Override
1:74846bb:         public JobSchedulerKahaDBMetaData readPayload(DataInput dataIn) throws IOException {
1:74846bb:             JobSchedulerKahaDBMetaData rc = new JobSchedulerKahaDBMetaData(store);
1:0484af1:             rc.read(dataIn);
1:0484af1:             return rc;
1:11ae61f:         }
1:44b25cb: 
1:11ae61f:         @Override
1:74846bb:         public void writePayload(JobSchedulerKahaDBMetaData object, DataOutput dataOut) throws IOException {
1:0484af1:             object.write(dataOut);
1:44b25cb:         }
1:44b25cb:     }
1:44b25cb: 
1:74846bb:     /**
1:74846bb:      * Called during index recovery to rebuild the index from the last known good location.  For
1:74846bb:      * entries that occur before the last known good position we just ignore then and move on.
1:74846bb:      *
1:74846bb:      * @param command
1:74846bb:      *        the command read from the Journal which should be used to update the index.
1:74846bb:      * @param location
1:74846bb:      *        the location in the index where the command was read.
1:74846bb:      * @param inDoubtlocation
1:74846bb:      *        the location in the index known to be the last time the index was valid.
1:74846bb:      *
1:74846bb:      * @throws IOException if an error occurs while recovering the index.
1:74846bb:      */
1:74846bb:     protected void doRecover(JournalCommand<?> data, final Location location, final Location inDoubtlocation) throws IOException {
1:74846bb:         if (inDoubtlocation != null && location.compareTo(inDoubtlocation) >= 0) {
1:74846bb:             process(data, location);
1:f21992e:         }
1:0484af1:     }
1:f21992e: 
1:74846bb:     /**
1:74846bb:      * Called during recovery to allow the store to rebuild from scratch.
1:74846bb:      *
1:74846bb:      * @param data
1:74846bb:      *      The command to process, which was read from the Journal.
1:74846bb:      * @param location
1:74846bb:      *      The location of the command in the Journal.
1:74846bb:      *
1:74846bb:      * @throws IOException if an error occurs during command processing.
1:74846bb:      */
1:44b25cb:     @Override
1:74846bb:     protected void process(JournalCommand<?> data, final Location location) throws IOException {
1:74846bb:         data.visit(new Visitor() {
1:74846bb:             @Override
1:74846bb:             public void visit(final KahaAddScheduledJobCommand command) throws IOException {
1:74846bb:                 final JobSchedulerImpl scheduler;
1:f21992e: 
1:74846bb:                 indexLock.writeLock().lock();
1:74846bb:                 try {
4:0484af1:                     try {
1:74846bb:                         scheduler = (JobSchedulerImpl) getJobScheduler(command.getScheduler());
1:0484af1:                     } catch (Exception e) {
1:74846bb:                         throw new IOException(e);
1:74846bb:                     }
1:74846bb:                     getPageFile().tx().execute(new Transaction.Closure<IOException>() {
1:74846bb:                         @Override
1:74846bb:                         public void execute(Transaction tx) throws IOException {
1:74846bb:                             scheduler.process(tx, command, location);
1:74846bb:                         }
1:74846bb:                     });
1:74846bb: 
1:74846bb:                     processLocation(location);
1:74846bb:                 } finally {
1:74846bb:                     indexLock.writeLock().unlock();
1:74846bb:                 }
1:74846bb:             }
1:74846bb: 
1:74846bb:             @Override
1:74846bb:             public void visit(final KahaRemoveScheduledJobCommand command) throws IOException {
1:74846bb:                 final JobSchedulerImpl scheduler;
1:74846bb: 
1:74846bb:                 indexLock.writeLock().lock();
1:74846bb:                 try {
1:74846bb:                     try {
1:74846bb:                         scheduler = (JobSchedulerImpl) getJobScheduler(command.getScheduler());
1:74846bb:                     } catch (Exception e) {
1:74846bb:                         throw new IOException(e);
1:74846bb:                     }
1:74846bb:                     getPageFile().tx().execute(new Transaction.Closure<IOException>() {
1:74846bb:                         @Override
1:74846bb:                         public void execute(Transaction tx) throws IOException {
1:74846bb:                             scheduler.process(tx, command, location);
1:74846bb:                         }
1:74846bb:                     });
1:74846bb: 
1:74846bb:                     processLocation(location);
1:74846bb:                 } finally {
1:74846bb:                     indexLock.writeLock().unlock();
1:74846bb:                 }
1:74846bb:             }
1:74846bb: 
1:74846bb:             @Override
1:74846bb:             public void visit(final KahaRemoveScheduledJobsCommand command) throws IOException {
1:74846bb:                 final JobSchedulerImpl scheduler;
1:74846bb: 
1:74846bb:                 indexLock.writeLock().lock();
1:74846bb:                 try {
1:74846bb:                     try {
1:74846bb:                         scheduler = (JobSchedulerImpl) getJobScheduler(command.getScheduler());
1:74846bb:                     } catch (Exception e) {
1:74846bb:                         throw new IOException(e);
1:74846bb:                     }
1:74846bb:                     getPageFile().tx().execute(new Transaction.Closure<IOException>() {
1:74846bb:                         @Override
1:74846bb:                         public void execute(Transaction tx) throws IOException {
1:74846bb:                             scheduler.process(tx, command, location);
1:74846bb:                         }
1:74846bb:                     });
1:74846bb: 
1:74846bb:                     processLocation(location);
1:74846bb:                 } finally {
1:74846bb:                     indexLock.writeLock().unlock();
1:74846bb:                 }
1:74846bb:             }
1:74846bb: 
1:74846bb:             @Override
1:74846bb:             public void visit(final KahaRescheduleJobCommand command) throws IOException {
1:74846bb:                 final JobSchedulerImpl scheduler;
1:74846bb: 
1:74846bb:                 indexLock.writeLock().lock();
1:74846bb:                 try {
1:74846bb:                     try {
1:74846bb:                         scheduler = (JobSchedulerImpl) getJobScheduler(command.getScheduler());
1:74846bb:                     } catch (Exception e) {
1:74846bb:                         throw new IOException(e);
1:74846bb:                     }
1:74846bb:                     getPageFile().tx().execute(new Transaction.Closure<IOException>() {
1:74846bb:                         @Override
1:74846bb:                         public void execute(Transaction tx) throws IOException {
1:74846bb:                             scheduler.process(tx, command, location);
1:74846bb:                         }
1:74846bb:                     });
1:74846bb: 
1:74846bb:                     processLocation(location);
1:74846bb:                 } finally {
1:74846bb:                     indexLock.writeLock().unlock();
1:74846bb:                 }
1:74846bb:             }
1:74846bb: 
1:44b25cb:             @Override
1:74846bb:             public void visit(final KahaDestroySchedulerCommand command) {
1:74846bb:                 try {
1:74846bb:                     removeJobScheduler(command.getScheduler());
1:74846bb:                 } catch (Exception e) {
1:74846bb:                     LOG.warn("Failed to remove scheduler: {}", command.getScheduler());
1:74846bb:                 }
1:74846bb: 
1:74846bb:                 processLocation(location);
1:74846bb:             }
1:74846bb: 
1:74846bb:             @Override
1:74846bb:             public void visit(KahaTraceCommand command) {
1:74846bb:                 processLocation(location);
1:74846bb:             }
1:74846bb:         });
1:74846bb:     }
1:74846bb: 
1:74846bb:     protected void processLocation(final Location location) {
1:74846bb:         indexLock.writeLock().lock();
1:74846bb:         try {
1:74846bb:             this.metaData.setLastUpdateLocation(location);
1:74846bb:         } finally {
1:74846bb:             indexLock.writeLock().unlock();
1:74846bb:         }
1:74846bb:     }
1:74846bb: 
1:74846bb:     /**
1:74846bb:      * We recover from the Journal logs as needed to restore the index.
1:74846bb:      *
1:74846bb:      * @throws IllegalStateException
1:74846bb:      * @throws IOException
1:74846bb:      */
1:74846bb:     private void recover() throws IllegalStateException, IOException {
1:74846bb:         this.indexLock.writeLock().lock();
1:74846bb:         try {
1:74846bb:             long start = System.currentTimeMillis();
1:74846bb:             Location lastIndoubtPosition = getRecoveryPosition();
1:74846bb:             Location recoveryPosition = lastIndoubtPosition;
1:74846bb: 
1:74846bb:             if (recoveryPosition != null) {
1:74846bb:                 int redoCounter = 0;
1:0a21c5f:                 LOG.info("Recovering from the scheduled job journal @" + recoveryPosition);
1:74846bb:                 while (recoveryPosition != null) {
1:0a21c5f:                     try {
1:74846bb:                         JournalCommand<?> message = load(recoveryPosition);
1:74846bb:                         metaData.setLastUpdateLocation(recoveryPosition);
1:74846bb:                         doRecover(message, recoveryPosition, lastIndoubtPosition);
1:74846bb:                         redoCounter++;
1:0a21c5f:                     } catch (IOException failedRecovery) {
1:0a21c5f:                         if (isIgnoreMissingJournalfiles()) {
1:0a21c5f:                             LOG.debug("Failed to recover data at position:" + recoveryPosition, failedRecovery);
1:0a21c5f:                             // track this dud location
1:0a21c5f:                             journal.corruptRecoveryLocation(recoveryPosition);
1:0a21c5f:                         } else {
1:0a21c5f:                             throw new IOException("Failed to recover data at position:" + recoveryPosition, failedRecovery);
1:0a21c5f:                         }
1:0a21c5f:                     }
1:74846bb:                     recoveryPosition = journal.getNextLocation(recoveryPosition);
1:74846bb:                      if (LOG.isInfoEnabled() && redoCounter % 100000 == 0) {
1:74846bb:                          LOG.info("@ {}, {} entries recovered ..", recoveryPosition, redoCounter);
1:74846bb:                      }
1:74846bb:                 }
1:74846bb:                 long end = System.currentTimeMillis();
1:74846bb:                 LOG.info("Recovery replayed {} operations from the journal in {} seconds.",
1:74846bb:                          redoCounter, ((end - start) / 1000.0f));
1:74846bb:             }
1:74846bb: 
1:74846bb:             // We may have to undo some index updates.
1:74846bb:             pageFile.tx().execute(new Transaction.Closure<IOException>() {
1:74846bb:                 @Override
1:74846bb:                 public void execute(Transaction tx) throws IOException {
1:74846bb:                     recoverIndex(tx);
1:74846bb:                 }
1:74846bb:             });
1:74846bb: 
1:74846bb:         } finally {
1:74846bb:             this.indexLock.writeLock().unlock();
1:74846bb:         }
1:74846bb:     }
1:74846bb: 
1:74846bb:     private Location getRecoveryPosition() throws IOException {
1:74846bb:         // This loads the first position and we completely rebuild the index if we
1:74846bb:         // do not override it with some known recovery start location.
1:74846bb:         Location result = null;
1:74846bb: 
1:74846bb:         if (!isForceRecoverIndex()) {
1:74846bb:             if (metaData.getLastUpdateLocation() != null) {
1:74846bb:                 result = metaData.getLastUpdateLocation();
1:74846bb:             }
1:74846bb:         }
1:74846bb: 
1:74846bb:         return journal.getNextLocation(result);
1:74846bb:     }
1:74846bb: 
1:74846bb:     private void recoverIndex(Transaction tx) throws IOException {
1:74846bb:         long start = System.currentTimeMillis();
1:74846bb: 
1:74846bb:         // It is possible index updates got applied before the journal updates..
1:74846bb:         // in that case we need to removed references to Jobs that are not in the journal
1:74846bb:         final Location lastAppendLocation = journal.getLastAppendLocation();
1:74846bb:         long undoCounter = 0;
1:74846bb: 
1:74846bb:         // Go through all the jobs in each scheduler and check if any are added after
1:74846bb:         // the last appended location and remove those.  For now we ignore the update
1:74846bb:         // location since the scheduled job will update itself after the next fire and
1:74846bb:         // a new update will replace any existing update.
1:74846bb:         for (Iterator<Map.Entry<String, JobSchedulerImpl>> i = metaData.getJobSchedulers().iterator(tx); i.hasNext();) {
1:74846bb:             Map.Entry<String, JobSchedulerImpl> entry = i.next();
1:74846bb:             JobSchedulerImpl scheduler = entry.getValue();
1:74846bb: 
1:74846bb:             List<JobLocation> jobs = scheduler.getAllScheduledJobs(tx);
1:74846bb:             for (JobLocation job : jobs) {
1:74846bb:                 if (job.getLocation().compareTo(lastAppendLocation) >= 0) {
1:74846bb:                     if (scheduler.removeJobAtTime(tx, job.getJobId(), job.getNextTime())) {
1:74846bb:                         LOG.trace("Removed Job past last appened in the journal: {}", job.getJobId());
1:74846bb:                         undoCounter++;
1:0484af1:                     }
1:0484af1:                 }
1:0484af1:             }
1:0484af1:         }
1:f21992e: 
1:74846bb:         if (undoCounter > 0) {
1:74846bb:             // The rolled back operations are basically in flight journal writes.  To avoid getting
1:74846bb:             // these the end user should do sync writes to the journal.
1:74846bb:             long end = System.currentTimeMillis();
1:74846bb:             LOG.info("Rolled back {} messages from the index in {} seconds.", undoCounter, ((end - start) / 1000.0f));
1:74846bb:             undoCounter = 0;
1:74846bb:         }
1:0484af1: 
1:74846bb:         // Now we check for missing and corrupt journal files.
1:74846bb: 
1:74846bb:         // 1. Collect the set of all referenced journal files based on the Location of the
1:74846bb:         //    the scheduled jobs and the marked last update field.
1:74846bb:         HashSet<Integer> missingJournalFiles = new HashSet<Integer>();
1:74846bb:         for (Iterator<Map.Entry<String, JobSchedulerImpl>> i = metaData.getJobSchedulers().iterator(tx); i.hasNext();) {
1:74846bb:             Map.Entry<String, JobSchedulerImpl> entry = i.next();
1:74846bb:             JobSchedulerImpl scheduler = entry.getValue();
1:74846bb: 
1:74846bb:             List<JobLocation> jobs = scheduler.getAllScheduledJobs(tx);
1:74846bb:             for (JobLocation job : jobs) {
1:74846bb:                 missingJournalFiles.add(job.getLocation().getDataFileId());
1:74846bb:                 if (job.getLastUpdate() != null) {
1:74846bb:                     missingJournalFiles.add(job.getLastUpdate().getDataFileId());
1:74846bb:                 }
1:74846bb:             }
1:74846bb:         }
1:74846bb: 
1:74846bb:         // 2. Remove from that set all known data file Id's in the journal and what's left
1:74846bb:         //    is the missing set which will soon also contain the corrupted set.
1:74846bb:         missingJournalFiles.removeAll(journal.getFileMap().keySet());
1:74846bb:         if (!missingJournalFiles.isEmpty()) {
1:74846bb:             LOG.info("Some journal files are missing: {}", missingJournalFiles);
1:74846bb:         }
1:74846bb: 
1:74846bb:         // 3. Now check all references in the journal logs for corruption and add any
1:74846bb:         //    corrupt journal files to the missing set.
1:74846bb:         HashSet<Location> corruptedLocations = new HashSet<Location>();
1:74846bb: 
1:74846bb:         if (isCheckForCorruptJournalFiles()) {
1:74846bb:             Collection<DataFile> dataFiles = journal.getFileMap().values();
1:74846bb:             for (DataFile dataFile : dataFiles) {
1:74846bb:                 int id = dataFile.getDataFileId();
1:74846bb:                 for (long offset : dataFile.getCorruptedBlocks()) {
1:74846bb:                     corruptedLocations.add(new Location(id, (int) offset));
1:74846bb:                 }
1:74846bb:             }
1:74846bb: 
1:74846bb:             if (!corruptedLocations.isEmpty()) {
1:74846bb:                 LOG.debug("Found some corrupted data blocks in the journal: {}", corruptedLocations.size());
1:74846bb:             }
1:74846bb:         }
1:74846bb: 
1:74846bb:         // 4. Now we either fail or we remove all references to missing or corrupt journal
1:74846bb:         //    files from the various JobSchedulerImpl instances.  We only remove the Job if
1:74846bb:         //    the initial Add operation is missing when the ignore option is set, the updates
1:74846bb:         //    could be lost but that's price you pay when ignoring the missing logs.
1:74846bb:         if (!missingJournalFiles.isEmpty() || !corruptedLocations.isEmpty()) {
1:74846bb:             if (!isIgnoreMissingJournalfiles()) {
1:74846bb:                 throw new IOException("Detected missing/corrupt journal files.");
1:74846bb:             }
1:74846bb: 
1:74846bb:             // Remove all Jobs that reference an Location that is either missing or corrupt.
1:74846bb:             undoCounter = removeJobsInMissingOrCorruptJounralFiles(tx, missingJournalFiles, corruptedLocations);
1:74846bb: 
1:74846bb:             // Clean up the Journal Reference count Map.
1:74846bb:             removeJournalRCForMissingFiles(tx, missingJournalFiles);
1:74846bb:         }
1:74846bb: 
1:74846bb:         if (undoCounter > 0) {
1:74846bb:             long end = System.currentTimeMillis();
1:74846bb:             LOG.info("Detected missing/corrupt journal files.  Dropped {} jobs from the " +
1:74846bb:                      "index in {} seconds.", undoCounter, ((end - start) / 1000.0f));
1:0484af1:         }
1:0484af1:     }
1:0484af1: 
1:74846bb:     private void removeJournalRCForMissingFiles(Transaction tx, Set<Integer> missing) throws IOException {
1:74846bb:         List<Integer> matches = new ArrayList<Integer>();
1:74846bb: 
1:74846bb:         Iterator<Entry<Integer, Integer>> references = metaData.getJournalRC().iterator(tx);
1:74846bb:         while (references.hasNext()) {
1:74846bb:             int dataFileId = references.next().getKey();
1:74846bb:             if (missing.contains(dataFileId)) {
1:74846bb:                 matches.add(dataFileId);
1:74846bb:             }
1:74846bb:         }
1:74846bb: 
1:74846bb:         for (Integer match : matches) {
1:74846bb:             metaData.getJournalRC().remove(tx, match);
1:74846bb:         }
1:0484af1:     }
1:0484af1: 
1:74846bb:     private int removeJobsInMissingOrCorruptJounralFiles(Transaction tx, Set<Integer> missing, Set<Location> corrupted) throws IOException {
1:74846bb:         int removed = 0;
1:0484af1: 
1:74846bb:         // Remove Jobs that reference missing or corrupt files.
1:74846bb:         // Remove Reference counts to missing or corrupt files.
1:74846bb:         // Remove and remove command markers to missing or corrupt files.
1:74846bb:         for (Iterator<Map.Entry<String, JobSchedulerImpl>> i = metaData.getJobSchedulers().iterator(tx); i.hasNext();) {
1:74846bb:             Map.Entry<String, JobSchedulerImpl> entry = i.next();
1:74846bb:             JobSchedulerImpl scheduler = entry.getValue();
1:0484af1: 
1:74846bb:             List<JobLocation> jobs = scheduler.getAllScheduledJobs(tx);
1:74846bb:             for (JobLocation job : jobs) {
1:0484af1: 
1:74846bb:                 // Remove all jobs in missing log files.
1:74846bb:                 if (missing.contains(job.getLocation().getDataFileId())) {
1:74846bb:                     scheduler.removeJobAtTime(tx, job.getJobId(), job.getNextTime());
1:74846bb:                     removed++;
1:74846bb:                     continue;
1:74846bb:                 }
1:0484af1: 
1:74846bb:                 // Remove all jobs in corrupted parts of log files.
1:74846bb:                 if (corrupted.contains(job.getLocation())) {
1:74846bb:                     scheduler.removeJobAtTime(tx, job.getJobId(), job.getNextTime());
1:74846bb:                     removed++;
1:74846bb:                 }
1:74846bb:             }
1:74846bb:         }
1:0484af1: 
1:74846bb:         return removed;
1:0484af1:     }
1:0484af1: }
============================================================================
author:Timothy Bish
-------------------------------------------------------------------------------
commit:8c4b5f4
/////////////////////////////////////////////////////////////////////////
1:                     boolean orphanedRemove = true;
1:                                 LOG.trace("A remove in log {} has an add still in existance in {}.", entry.getKey(), addLocation);
1:                                 orphanedRemove = false;
1:                         if (!orphanedRemove) {
commit:74846bb
/////////////////////////////////////////////////////////////////////////
1: import java.io.FilenameFilter;
1: import java.util.Collection;
/////////////////////////////////////////////////////////////////////////
1: import java.util.TreeSet;
1: import java.util.UUID;
1: import org.apache.activemq.protobuf.Buffer;
1: import org.apache.activemq.store.kahadb.AbstractKahaDBStore;
1: import org.apache.activemq.store.kahadb.JournalCommand;
1: import org.apache.activemq.store.kahadb.KahaDBMetaData;
1: import org.apache.activemq.store.kahadb.Visitor;
1: import org.apache.activemq.store.kahadb.data.KahaAddScheduledJobCommand;
1: import org.apache.activemq.store.kahadb.data.KahaDestroySchedulerCommand;
1: import org.apache.activemq.store.kahadb.data.KahaRemoveScheduledJobCommand;
1: import org.apache.activemq.store.kahadb.data.KahaRemoveScheduledJobsCommand;
1: import org.apache.activemq.store.kahadb.data.KahaRescheduleJobCommand;
1: import org.apache.activemq.store.kahadb.data.KahaTraceCommand;
1: import org.apache.activemq.store.kahadb.disk.index.BTreeVisitor;
1: import org.apache.activemq.store.kahadb.disk.journal.DataFile;
1: import org.apache.activemq.store.kahadb.scheduler.legacy.LegacyStoreReplayer;
1: public class JobSchedulerStoreImpl extends AbstractKahaDBStore implements JobSchedulerStore {
1:     private static final Logger LOG = LoggerFactory.getLogger(JobSchedulerStoreImpl.class);
1:     private JobSchedulerKahaDBMetaData metaData = new JobSchedulerKahaDBMetaData(this);
1:     private final MetaDataMarshaller metaDataMarshaller = new MetaDataMarshaller(this);
1:     private final Map<String, JobSchedulerImpl> schedulers = new HashMap<String, JobSchedulerImpl>();
1:     private File legacyStoreArchiveDirectory;
1:     /**
1:      * The Scheduler Token is used to identify base revisions of the Scheduler store.  A store
1:      * based on the initial scheduler design will not have this tag in it's meta-data and will
1:      * indicate an update is needed.  Later versions of the scheduler can also change this value
1:      * to indicate incompatible store bases which require complete meta-data and journal rewrites
1:      * instead of simpler meta-data updates.
1:      */
1:     static final UUID SCHEDULER_STORE_TOKEN = UUID.fromString("57ed642b-1ee3-47b3-be6d-b7297d500409");
1:     /**
1:      * The default scheduler store version.  All new store instance will be given this version and
1:      * earlier versions will be updated to this version.
1:      */
1:     static final int CURRENT_VERSION = 1;
1:     @Override
1:     public JobScheduler getJobScheduler(final String name) throws Exception {
1:         this.indexLock.writeLock().lock();
1:         try {
1:             JobSchedulerImpl result = this.schedulers.get(name);
1:             if (result == null) {
1:                 final JobSchedulerImpl js = new JobSchedulerImpl(this);
1:                 js.setName(name);
1:                 getPageFile().tx().execute(new Transaction.Closure<IOException>() {
1:                     @Override
1:                     public void execute(Transaction tx) throws IOException {
1:                         js.createIndexes(tx);
1:                         js.load(tx);
1:                         metaData.getJobSchedulers().put(tx, name, js);
1:                     }
1:                 });
1:                 result = js;
1:                 this.schedulers.put(name, js);
1:                 if (isStarted()) {
1:                     result.start();
1:                 }
1:                 this.pageFile.flush();
1:             return result;
1:         } finally {
1:             this.indexLock.writeLock().unlock();
1:     @Override
1:     public boolean removeJobScheduler(final String name) throws Exception {
1:         boolean result = false;
1: 
1:         this.indexLock.writeLock().lock();
1:         try {
1:             final JobSchedulerImpl js = this.schedulers.remove(name);
1:             result = js != null;
1:             if (result) {
1:                 js.stop();
1:                 getPageFile().tx().execute(new Transaction.Closure<IOException>() {
1:                     @Override
1:                     public void execute(Transaction tx) throws IOException {
1:                         metaData.getJobSchedulers().remove(tx, name);
1:                         js.removeAll(tx);
1:                     }
1:                 });
1:             }
1:         } finally {
1:             this.indexLock.writeLock().unlock();
1:         }
1:         return result;
1:     }
1: 
1:     /**
1:      * Sets the directory where the legacy scheduler store files are archived before an
1:      * update attempt is made.  Both the legacy index files and the journal files are moved
1:      * to this folder prior to an upgrade attempt.
1:      *
1:      * @param directory
1:      *      The directory to move the legacy Scheduler Store files to.
1:      */
1:     public void setLegacyStoreArchiveDirectory(File directory) {
1:         this.legacyStoreArchiveDirectory = directory;
1:     }
1: 
1:     /**
1:      * Gets the directory where the legacy Scheduler Store files will be archived if the
1:      * broker is started and an existing Job Scheduler Store from an old version is detected.
1:      *
1:      * @return the directory where scheduler store legacy files are archived on upgrade.
1:      */
1:     public File getLegacyStoreArchiveDirectory() {
1:         if (this.legacyStoreArchiveDirectory == null) {
1:             this.legacyStoreArchiveDirectory = new File(getDirectory(), "legacySchedulerStore");
1:         }
1: 
1:         return this.legacyStoreArchiveDirectory.getAbsoluteFile();
1:     }
1: 
1:     @Override
1:     public void load() throws IOException {
1:         if (opened.compareAndSet(false, true)) {
1:             getJournal().start();
1:             try {
1:                 loadPageFile();
1:             } catch (UnknownStoreVersionException ex) {
1:                 LOG.info("Can't start until store update is performed.");
1:                 upgradeFromLegacy();
1:                 // Restart with the updated store
1:                 getJournal().start();
1:                 loadPageFile();
1:                 LOG.info("Update from legacy Scheduler store completed successfully.");
1:             } catch (Throwable t) {
1:                 LOG.warn("Index corrupted. Recovering the index through journal replay. Cause: {}", t.toString());
1:                 LOG.debug("Index load failure", t);
1: 
1:                 // try to recover index
1:                 try {
1:                     pageFile.unload();
1:                 } catch (Exception ignore) {
1:                 }
1:                 if (isArchiveCorruptedIndex()) {
1:                     pageFile.archive();
1:                 } else {
1:                     pageFile.delete();
1:                 }
1:                 metaData = new JobSchedulerKahaDBMetaData(this);
1:                 pageFile = null;
1:                 loadPageFile();
1:             }
1:             startCheckpoint();
1:             recover();
1:         }
1:         LOG.info("{} started.", this);
1:     }
1: 
1:     @Override
1:     public void unload() throws IOException {
1:         if (opened.compareAndSet(true, false)) {
1:             for (JobSchedulerImpl js : this.schedulers.values()) {
1:                 try {
1:                     js.stop();
1:                 } catch (Exception e) {
1:                     throw new IOException(e);
1:                 }
1:             }
1:             this.indexLock.writeLock().lock();
1:             try {
1:                 if (pageFile != null && pageFile.isLoaded()) {
1:                     metaData.setState(KahaDBMetaData.CLOSED_STATE);
1: 
1:                     if (metaData.getPage() != null) {
1:                         pageFile.tx().execute(new Transaction.Closure<IOException>() {
1:                             @Override
1:                             public void execute(Transaction tx) throws IOException {
1:                                 tx.store(metaData.getPage(), metaDataMarshaller, true);
1:                             }
1:                         });
1:                     }
1:                 }
1:             } finally {
1:                 this.indexLock.writeLock().unlock();
1:             }
1: 
1:             checkpointLock.writeLock().lock();
1:             try {
1:                 if (metaData.getPage() != null) {
1:                     checkpointUpdate(true);
1:                 }
1:             } finally {
1:                 checkpointLock.writeLock().unlock();
1:             }
1:             synchronized (checkpointThreadLock) {
1:                 if (checkpointThread != null) {
1:                     try {
1:                         checkpointThread.join();
1:                         checkpointThread = null;
1:                     } catch (InterruptedException e) {
1:                     }
1:                 }
1:             }
1: 
1:             if (pageFile != null) {
1:                 pageFile.unload();
1:                 pageFile = null;
1:             }
1:             if (this.journal != null) {
1:                 journal.close();
1:                 journal = null;
1:             }
1: 
1:             metaData = new JobSchedulerKahaDBMetaData(this);
1:         }
1:         LOG.info("{} stopped.", this);
1:     }
1: 
1:     private void loadPageFile() throws IOException {
1:         this.indexLock.writeLock().lock();
1:         try {
1:             final PageFile pageFile = getPageFile();
1:             pageFile.load();
1:             pageFile.tx().execute(new Transaction.Closure<IOException>() {
1:                 @Override
1:                 public void execute(Transaction tx) throws IOException {
1:                     if (pageFile.getPageCount() == 0) {
1:                         Page<JobSchedulerKahaDBMetaData> page = tx.allocate();
1:                         assert page.getPageId() == 0;
1:                         page.set(metaData);
1:                         metaData.setPage(page);
1:                         metaData.setState(KahaDBMetaData.CLOSED_STATE);
1:                         metaData.initialize(tx);
1:                         tx.store(metaData.getPage(), metaDataMarshaller, true);
1:                     } else {
1:                         Page<JobSchedulerKahaDBMetaData> page = null;
1:                         page = tx.load(0, metaDataMarshaller);
1:                         metaData = page.get();
1:                         metaData.setPage(page);
1:                     }
1:                     metaData.load(tx);
1:                     metaData.loadScheduler(tx, schedulers);
1:                     for (JobSchedulerImpl js : schedulers.values()) {
1:                         try {
1:                             js.start();
1:                         } catch (Exception e) {
1:                             JobSchedulerStoreImpl.LOG.error("Failed to load " + js.getName(), e);
1:                         }
1:                     }
1:                 }
1:             });
1: 
1:             pageFile.flush();
1:         } finally {
1:             this.indexLock.writeLock().unlock();
1:         }
1:     }
1: 
1:     private void upgradeFromLegacy() throws IOException {
1: 
1:         journal.close();
1:         journal = null;
1:         try {
1:             pageFile.unload();
1:             pageFile = null;
1:         } catch (Exception ignore) {}
1: 
1:         File storeDir = getDirectory().getAbsoluteFile();
1:         File storeArchiveDir = getLegacyStoreArchiveDirectory();
1: 
1:         LOG.info("Attempting to move old store files from {} to {}", storeDir, storeArchiveDir);
1: 
1:         // Move only the known store files, locks and other items left in place.
1:         IOHelper.moveFiles(storeDir, storeArchiveDir, new FilenameFilter() {
1: 
1:             @Override
1:             public boolean accept(File dir, String name) {
1:                 if (name.endsWith(".data") || name.endsWith(".redo") || name.endsWith(".log")) {
1:                     return true;
1:                 }
1:                 return false;
1:             }
1:         });
1: 
1:         // We reset everything to clean state, then we can read from the old
1:         // scheduler store and replay the scheduled jobs into this one as adds.
1:         getJournal().start();
1:         metaData = new JobSchedulerKahaDBMetaData(this);
1:         pageFile = null;
1:         loadPageFile();
1: 
1:         LegacyStoreReplayer replayer = new LegacyStoreReplayer(getLegacyStoreArchiveDirectory());
1:         replayer.load();
1:         replayer.startReplay(this);
1: 
1:         // Cleanup after replay and store what we've done.
1:         pageFile.tx().execute(new Transaction.Closure<IOException>() {
1:             @Override
1:             public void execute(Transaction tx) throws IOException {
1:                 tx.store(metaData.getPage(), metaDataMarshaller, true);
1:             }
1:         });
1: 
1:         checkpointUpdate(true);
1:         getJournal().close();
1:         getPageFile().unload();
1:     }
1: 
1:     @Override
1:     protected void checkpointUpdate(Transaction tx, boolean cleanup) throws IOException {
1:         LOG.debug("Job Scheduler Store Checkpoint started.");
1: 
1:         // reflect last update exclusive of current checkpoint
1:         Location lastUpdate = metaData.getLastUpdateLocation();
1:         metaData.setState(KahaDBMetaData.OPEN_STATE);
1:         tx.store(metaData.getPage(), metaDataMarshaller, true);
1:         pageFile.flush();
1: 
1:         if (cleanup) {
1:             final TreeSet<Integer> completeFileSet = new TreeSet<Integer>(journal.getFileMap().keySet());
1:             final TreeSet<Integer> gcCandidateSet = new TreeSet<Integer>(completeFileSet);
1: 
1:             LOG.trace("Last update: {}, full gc candidates set: {}", lastUpdate, gcCandidateSet);
1: 
1:             if (lastUpdate != null) {
1:                 gcCandidateSet.remove(lastUpdate.getDataFileId());
1:             }
1: 
1:             this.metaData.getJournalRC().visit(tx, new BTreeVisitor<Integer, Integer>() {
1: 
1:                 @Override
1:                 public void visit(List<Integer> keys, List<Integer> values) {
1:                     for (Integer key : keys) {
1:                         if (gcCandidateSet.remove(key)) {
1:                             LOG.trace("Removed referenced file: {} from GC set", key);
1:                         }
1:                     }
1:                 }
1: 
1:                 @Override
1:                 public boolean isInterestedInKeysBetween(Integer first, Integer second) {
1:                     return true;
1:                 }
1:             });
1: 
1:             LOG.trace("gc candidates after reference check: {}", gcCandidateSet);
1: 
1:             // If there are GC candidates then check the remove command location to see
1:             // if any of them can go or if they must stay in order to ensure proper recover.
1:             //
1:             // A log containing any remove commands must be kept until all the logs with the
1:             // add commands for all the removed jobs have been dropped.
1:             if (!gcCandidateSet.isEmpty()) {
1:                 Iterator<Entry<Integer, List<Integer>>> removals = metaData.getRemoveLocationTracker().iterator(tx);
1:                 List<Integer> orphans = new ArrayList<Integer>();
1:                 while (removals.hasNext()) {
0:                     boolean orphanedRemve = true;
1:                     Entry<Integer, List<Integer>> entry = removals.next();
1: 
1:                     // If this log is not a GC candidate then there's no need to do a check to rule it out
1:                     if (gcCandidateSet.contains(entry.getKey())) {
1:                         for (Integer addLocation : entry.getValue()) {
1:                             if (completeFileSet.contains(addLocation)) {
0:                                 orphanedRemve = false;
1:                                 break;
1:                             }
1:                         }
1: 
1:                         // If it's not orphaned than we can't remove it, otherwise we
1:                         // stop tracking it it's log will get deleted on the next check.
0:                         if (!orphanedRemve) {
0:                             LOG.trace("A remove in log {} has an add still in existance.", entry.getKey());
1:                             gcCandidateSet.remove(entry.getKey());
1:                         } else {
1:                             LOG.trace("All removes in log {} are orphaned, file can be GC'd", entry.getKey());
1:                             orphans.add(entry.getKey());
1:                         }
1:                     }
1:                 }
1: 
1:                 // Drop all orphaned removes from the tracker.
1:                 for (Integer orphan : orphans) {
1:                     metaData.getRemoveLocationTracker().remove(tx, orphan);
1:                 }
1:             }
1: 
1:             LOG.trace("gc candidates after removals check: {}", gcCandidateSet);
1:             if (!gcCandidateSet.isEmpty()) {
1:                 if (LOG.isDebugEnabled()) {
1:                     LOG.debug("Cleanup removing the data files: " + gcCandidateSet);
1:                 }
1:                 journal.removeDataFiles(gcCandidateSet);
1:             }
1:         }
1: 
1:         LOG.debug("Job Scheduler Store Checkpoint complete.");
1:     }
1: 
1:     /**
1:      * Adds a reference for the journal log file pointed to by the given Location value.
1:      *
1:      * To prevent log files in the journal that still contain valid data that needs to be
1:      * kept in order to allow for recovery the logs must have active references.  Each Job
1:      * scheduler should ensure that the logs are accurately referenced.
1:      *
1:      * @param tx
1:      *      The TX under which the update is to be performed.
1:      * @param location
1:      *      The location value to update the reference count of.
1:      *
1:      * @throws IOException if an error occurs while updating the journal references table.
1:      */
1:     protected void incrementJournalCount(Transaction tx, Location location) throws IOException {
1:         int logId = location.getDataFileId();
1:         Integer val = metaData.getJournalRC().get(tx, logId);
1:         int refCount = val != null ? val.intValue() + 1 : 1;
1:         metaData.getJournalRC().put(tx, logId, refCount);
1:     }
1: 
1:     /**
1:      * Removes one reference for the Journal log file indicated in the given Location value.
1:      *
1:      * The references are used to track which log files cannot be GC'd.  When the reference count
1:      * on a log file reaches zero the file id is removed from the tracker and the log will be
1:      * removed on the next check point update.
1:      *
1:      * @param tx
1:      *      The TX under which the update is to be performed.
1:      * @param location
1:      *      The location value to update the reference count of.
1:      *
1:      * @throws IOException if an error occurs while updating the journal references table.
1:      */
1:     protected void decrementJournalCount(Transaction tx, Location location) throws IOException {
1:         int logId = location.getDataFileId();
1:         Integer refCount = metaData.getJournalRC().get(tx, logId);
1:         if (refCount != null) {
1:             int refCountValue = refCount;
1:             refCountValue--;
1:             if (refCountValue <= 0) {
1:                 metaData.getJournalRC().remove(tx, logId);
1:             } else {
1:                 metaData.getJournalRC().put(tx, logId, refCountValue);
1:             }
1:         }
1:     }
1: 
1:     /**
1:      * Updates the Job removal tracking index with the location of a remove command and the
1:      * original JobLocation entry.
1:      *
1:      * The JobLocation holds the locations in the logs where the add and update commands for
1:      * a job stored.  The log file containing the remove command can only be discarded after
1:      * both the add and latest update log files have also been discarded.
1:      *
1:      * @param tx
1:      *      The TX under which the update is to be performed.
1:      * @param location
1:      *      The location value to reference a remove command.
1:      * @param removedJob
1:      *      The original JobLocation instance that holds the add and update locations
1:      *
1:      * @throws IOException if an error occurs while updating the remove location tracker.
1:      */
1:     protected void referenceRemovedLocation(Transaction tx, Location location, JobLocation removedJob) throws IOException {
1:         int logId = location.getDataFileId();
1:         List<Integer> removed = this.metaData.getRemoveLocationTracker().get(tx, logId);
1:         if (removed == null) {
1:             removed = new ArrayList<Integer>();
1:         }
1:         removed.add(removedJob.getLocation().getDataFileId());
1:         this.metaData.getRemoveLocationTracker().put(tx, logId, removed);
1:     }
1: 
1:     /**
1:      * Retrieve the scheduled Job's byte blob from the journal.
1:      *
1:      * @param location
1:      *      The location of the KahaAddScheduledJobCommand that originated the Job.
1:      *
1:      * @return a ByteSequence containing the payload of the scheduled Job.
1:      *
1:      * @throws IOException if an error occurs while reading the payload value.
1:      */
1:     protected ByteSequence getPayload(Location location) throws IOException {
1:         KahaAddScheduledJobCommand job = (KahaAddScheduledJobCommand) this.load(location);
1:         Buffer payload = job.getPayload();
1:         return new ByteSequence(payload.getData(), payload.getOffset(), payload.getLength());
1:     }
1: 
1:     public void readLockIndex() {
1:         this.indexLock.readLock().lock();
1:     }
1: 
1:     public void readUnlockIndex() {
1:         this.indexLock.readLock().unlock();
1:     }
1: 
1:     public void writeLockIndex() {
1:         this.indexLock.writeLock().lock();
1:     }
1: 
1:     public void writeUnlockIndex() {
1:         this.indexLock.writeLock().unlock();
1:     }
1: 
1:     @Override
1:     public String toString() {
1:         return "JobSchedulerStore: " + getDirectory();
1:     }
1: 
1:     @Override
1:     protected String getPageFileName() {
1:         return "scheduleDB";
1:     }
1: 
1:     @Override
1:     protected File getDefaultDataDirectory() {
1:         return new File(IOHelper.getDefaultDataDirectory(), "delayedDB");
1:     }
1: 
1:     private class MetaDataMarshaller extends VariableMarshaller<JobSchedulerKahaDBMetaData> {
1: 
/////////////////////////////////////////////////////////////////////////
1:         public JobSchedulerKahaDBMetaData readPayload(DataInput dataIn) throws IOException {
1:             JobSchedulerKahaDBMetaData rc = new JobSchedulerKahaDBMetaData(store);
1:         public void writePayload(JobSchedulerKahaDBMetaData object, DataOutput dataOut) throws IOException {
1:     /**
1:      * Called during index recovery to rebuild the index from the last known good location.  For
1:      * entries that occur before the last known good position we just ignore then and move on.
1:      *
1:      * @param command
1:      *        the command read from the Journal which should be used to update the index.
1:      * @param location
1:      *        the location in the index where the command was read.
1:      * @param inDoubtlocation
1:      *        the location in the index known to be the last time the index was valid.
1:      *
1:      * @throws IOException if an error occurs while recovering the index.
1:      */
1:     protected void doRecover(JournalCommand<?> data, final Location location, final Location inDoubtlocation) throws IOException {
1:         if (inDoubtlocation != null && location.compareTo(inDoubtlocation) >= 0) {
1:             process(data, location);
1:     /**
1:      * Called during recovery to allow the store to rebuild from scratch.
1:      *
1:      * @param data
1:      *      The command to process, which was read from the Journal.
1:      * @param location
1:      *      The location of the command in the Journal.
1:      *
1:      * @throws IOException if an error occurs during command processing.
1:      */
1:     protected void process(JournalCommand<?> data, final Location location) throws IOException {
1:         data.visit(new Visitor() {
1:             public void visit(final KahaAddScheduledJobCommand command) throws IOException {
1:                 final JobSchedulerImpl scheduler;
1:                 indexLock.writeLock().lock();
1:                 try {
1:                         scheduler = (JobSchedulerImpl) getJobScheduler(command.getScheduler());
1:                         throw new IOException(e);
1:                     }
1:                     getPageFile().tx().execute(new Transaction.Closure<IOException>() {
1:                         @Override
1:                         public void execute(Transaction tx) throws IOException {
1:                             scheduler.process(tx, command, location);
1:                         }
1:                     });
1: 
1:                     processLocation(location);
1:                 } finally {
1:                     indexLock.writeLock().unlock();
1:                 }
1:             }
1: 
1:             @Override
1:             public void visit(final KahaRemoveScheduledJobCommand command) throws IOException {
1:                 final JobSchedulerImpl scheduler;
1: 
1:                 indexLock.writeLock().lock();
1:                 try {
1:                     try {
1:                         scheduler = (JobSchedulerImpl) getJobScheduler(command.getScheduler());
1:                     } catch (Exception e) {
1:                         throw new IOException(e);
1:                     }
1:                     getPageFile().tx().execute(new Transaction.Closure<IOException>() {
1:                         @Override
1:                         public void execute(Transaction tx) throws IOException {
1:                             scheduler.process(tx, command, location);
1:                         }
1:                     });
1: 
1:                     processLocation(location);
1:                 } finally {
1:                     indexLock.writeLock().unlock();
1:                 }
1:             }
1: 
1:             @Override
1:             public void visit(final KahaRemoveScheduledJobsCommand command) throws IOException {
1:                 final JobSchedulerImpl scheduler;
1: 
1:                 indexLock.writeLock().lock();
1:                 try {
1:                     try {
1:                         scheduler = (JobSchedulerImpl) getJobScheduler(command.getScheduler());
1:                     } catch (Exception e) {
1:                         throw new IOException(e);
1:                     }
1:                     getPageFile().tx().execute(new Transaction.Closure<IOException>() {
1:                         @Override
1:                         public void execute(Transaction tx) throws IOException {
1:                             scheduler.process(tx, command, location);
1:                         }
1:                     });
1: 
1:                     processLocation(location);
1:                 } finally {
1:                     indexLock.writeLock().unlock();
1:                 }
1:             }
1: 
1:             @Override
1:             public void visit(final KahaRescheduleJobCommand command) throws IOException {
1:                 final JobSchedulerImpl scheduler;
1: 
1:                 indexLock.writeLock().lock();
1:                 try {
1:                     try {
1:                         scheduler = (JobSchedulerImpl) getJobScheduler(command.getScheduler());
1:                     } catch (Exception e) {
1:                         throw new IOException(e);
1:                     }
1:                     getPageFile().tx().execute(new Transaction.Closure<IOException>() {
1:                         @Override
1:                         public void execute(Transaction tx) throws IOException {
1:                             scheduler.process(tx, command, location);
1:                         }
1:                     });
1: 
1:                     processLocation(location);
1:                 } finally {
1:                     indexLock.writeLock().unlock();
1:                 }
1:             }
1: 
1:             @Override
1:             public void visit(final KahaDestroySchedulerCommand command) {
1:                 try {
1:                     removeJobScheduler(command.getScheduler());
1:                 } catch (Exception e) {
1:                     LOG.warn("Failed to remove scheduler: {}", command.getScheduler());
1:                 }
1: 
1:                 processLocation(location);
1:             }
1: 
1:             @Override
1:             public void visit(KahaTraceCommand command) {
1:                 processLocation(location);
1:             }
1:         });
1:     }
1: 
1:     protected void processLocation(final Location location) {
1:         indexLock.writeLock().lock();
1:         try {
1:             this.metaData.setLastUpdateLocation(location);
1:         } finally {
1:             indexLock.writeLock().unlock();
1:         }
1:     }
1: 
1:     /**
1:      * We recover from the Journal logs as needed to restore the index.
1:      *
1:      * @throws IllegalStateException
1:      * @throws IOException
1:      */
1:     private void recover() throws IllegalStateException, IOException {
1:         this.indexLock.writeLock().lock();
1:         try {
1:             long start = System.currentTimeMillis();
1:             Location lastIndoubtPosition = getRecoveryPosition();
1:             Location recoveryPosition = lastIndoubtPosition;
1: 
1:             if (recoveryPosition != null) {
1:                 int redoCounter = 0;
0:                 LOG.info("Recovering from the journal ...");
1:                 while (recoveryPosition != null) {
1:                     JournalCommand<?> message = load(recoveryPosition);
1:                     metaData.setLastUpdateLocation(recoveryPosition);
1:                     doRecover(message, recoveryPosition, lastIndoubtPosition);
1:                     redoCounter++;
1:                     recoveryPosition = journal.getNextLocation(recoveryPosition);
1:                      if (LOG.isInfoEnabled() && redoCounter % 100000 == 0) {
1:                          LOG.info("@ {}, {} entries recovered ..", recoveryPosition, redoCounter);
1:                      }
1:                 }
1:                 long end = System.currentTimeMillis();
1:                 LOG.info("Recovery replayed {} operations from the journal in {} seconds.",
1:                          redoCounter, ((end - start) / 1000.0f));
1:             }
1: 
1:             // We may have to undo some index updates.
1:             pageFile.tx().execute(new Transaction.Closure<IOException>() {
1:                 @Override
1:                 public void execute(Transaction tx) throws IOException {
1:                     recoverIndex(tx);
1:                 }
1:             });
1: 
1:         } finally {
1:             this.indexLock.writeLock().unlock();
1:         }
1:     }
1: 
1:     private Location getRecoveryPosition() throws IOException {
1:         // This loads the first position and we completely rebuild the index if we
1:         // do not override it with some known recovery start location.
1:         Location result = null;
1: 
1:         if (!isForceRecoverIndex()) {
1:             if (metaData.getLastUpdateLocation() != null) {
1:                 result = metaData.getLastUpdateLocation();
1:             }
1:         }
1: 
1:         return journal.getNextLocation(result);
1:     }
1: 
1:     private void recoverIndex(Transaction tx) throws IOException {
1:         long start = System.currentTimeMillis();
1: 
1:         // It is possible index updates got applied before the journal updates..
1:         // in that case we need to removed references to Jobs that are not in the journal
1:         final Location lastAppendLocation = journal.getLastAppendLocation();
1:         long undoCounter = 0;
1: 
1:         // Go through all the jobs in each scheduler and check if any are added after
1:         // the last appended location and remove those.  For now we ignore the update
1:         // location since the scheduled job will update itself after the next fire and
1:         // a new update will replace any existing update.
1:         for (Iterator<Map.Entry<String, JobSchedulerImpl>> i = metaData.getJobSchedulers().iterator(tx); i.hasNext();) {
1:             Map.Entry<String, JobSchedulerImpl> entry = i.next();
1:             JobSchedulerImpl scheduler = entry.getValue();
1: 
1:             List<JobLocation> jobs = scheduler.getAllScheduledJobs(tx);
1:             for (JobLocation job : jobs) {
1:                 if (job.getLocation().compareTo(lastAppendLocation) >= 0) {
1:                     if (scheduler.removeJobAtTime(tx, job.getJobId(), job.getNextTime())) {
1:                         LOG.trace("Removed Job past last appened in the journal: {}", job.getJobId());
1:                         undoCounter++;
1:         if (undoCounter > 0) {
1:             // The rolled back operations are basically in flight journal writes.  To avoid getting
1:             // these the end user should do sync writes to the journal.
1:             long end = System.currentTimeMillis();
1:             LOG.info("Rolled back {} messages from the index in {} seconds.", undoCounter, ((end - start) / 1000.0f));
1:             undoCounter = 0;
1:         }
1:         // Now we check for missing and corrupt journal files.
1: 
1:         // 1. Collect the set of all referenced journal files based on the Location of the
1:         //    the scheduled jobs and the marked last update field.
1:         HashSet<Integer> missingJournalFiles = new HashSet<Integer>();
1:         for (Iterator<Map.Entry<String, JobSchedulerImpl>> i = metaData.getJobSchedulers().iterator(tx); i.hasNext();) {
1:             Map.Entry<String, JobSchedulerImpl> entry = i.next();
1:             JobSchedulerImpl scheduler = entry.getValue();
1: 
1:             List<JobLocation> jobs = scheduler.getAllScheduledJobs(tx);
1:             for (JobLocation job : jobs) {
1:                 missingJournalFiles.add(job.getLocation().getDataFileId());
1:                 if (job.getLastUpdate() != null) {
1:                     missingJournalFiles.add(job.getLastUpdate().getDataFileId());
1:                 }
1:             }
1:         }
1: 
1:         // 2. Remove from that set all known data file Id's in the journal and what's left
1:         //    is the missing set which will soon also contain the corrupted set.
1:         missingJournalFiles.removeAll(journal.getFileMap().keySet());
1:         if (!missingJournalFiles.isEmpty()) {
1:             LOG.info("Some journal files are missing: {}", missingJournalFiles);
1:         }
1: 
1:         // 3. Now check all references in the journal logs for corruption and add any
1:         //    corrupt journal files to the missing set.
1:         HashSet<Location> corruptedLocations = new HashSet<Location>();
1: 
1:         if (isCheckForCorruptJournalFiles()) {
1:             Collection<DataFile> dataFiles = journal.getFileMap().values();
1:             for (DataFile dataFile : dataFiles) {
1:                 int id = dataFile.getDataFileId();
1:                 for (long offset : dataFile.getCorruptedBlocks()) {
1:                     corruptedLocations.add(new Location(id, (int) offset));
1:                 }
1:             }
1: 
1:             if (!corruptedLocations.isEmpty()) {
1:                 LOG.debug("Found some corrupted data blocks in the journal: {}", corruptedLocations.size());
1:             }
1:         }
1: 
1:         // 4. Now we either fail or we remove all references to missing or corrupt journal
1:         //    files from the various JobSchedulerImpl instances.  We only remove the Job if
1:         //    the initial Add operation is missing when the ignore option is set, the updates
1:         //    could be lost but that's price you pay when ignoring the missing logs.
1:         if (!missingJournalFiles.isEmpty() || !corruptedLocations.isEmpty()) {
1:             if (!isIgnoreMissingJournalfiles()) {
1:                 throw new IOException("Detected missing/corrupt journal files.");
1:             }
1: 
1:             // Remove all Jobs that reference an Location that is either missing or corrupt.
1:             undoCounter = removeJobsInMissingOrCorruptJounralFiles(tx, missingJournalFiles, corruptedLocations);
1: 
1:             // Clean up the Journal Reference count Map.
1:             removeJournalRCForMissingFiles(tx, missingJournalFiles);
1:         }
1: 
1:         if (undoCounter > 0) {
1:             long end = System.currentTimeMillis();
1:             LOG.info("Detected missing/corrupt journal files.  Dropped {} jobs from the " +
1:                      "index in {} seconds.", undoCounter, ((end - start) / 1000.0f));
1:     private void removeJournalRCForMissingFiles(Transaction tx, Set<Integer> missing) throws IOException {
1:         List<Integer> matches = new ArrayList<Integer>();
1: 
1:         Iterator<Entry<Integer, Integer>> references = metaData.getJournalRC().iterator(tx);
1:         while (references.hasNext()) {
1:             int dataFileId = references.next().getKey();
1:             if (missing.contains(dataFileId)) {
1:                 matches.add(dataFileId);
1:             }
1:         }
1: 
1:         for (Integer match : matches) {
1:             metaData.getJournalRC().remove(tx, match);
1:         }
1:     private int removeJobsInMissingOrCorruptJounralFiles(Transaction tx, Set<Integer> missing, Set<Location> corrupted) throws IOException {
1:         int removed = 0;
1:         // Remove Jobs that reference missing or corrupt files.
1:         // Remove Reference counts to missing or corrupt files.
1:         // Remove and remove command markers to missing or corrupt files.
1:         for (Iterator<Map.Entry<String, JobSchedulerImpl>> i = metaData.getJobSchedulers().iterator(tx); i.hasNext();) {
1:             Map.Entry<String, JobSchedulerImpl> entry = i.next();
1:             JobSchedulerImpl scheduler = entry.getValue();
1:             List<JobLocation> jobs = scheduler.getAllScheduledJobs(tx);
1:             for (JobLocation job : jobs) {
1:                 // Remove all jobs in missing log files.
1:                 if (missing.contains(job.getLocation().getDataFileId())) {
1:                     scheduler.removeJobAtTime(tx, job.getJobId(), job.getNextTime());
1:                     removed++;
1:                     continue;
1:                 }
1:                 // Remove all jobs in corrupted parts of log files.
1:                 if (corrupted.contains(job.getLocation())) {
1:                     scheduler.removeJobAtTime(tx, job.getJobId(), job.getNextTime());
1:                     removed++;
1:                 }
1:             }
1:         }
1:         return removed;
commit:11ae61f
/////////////////////////////////////////////////////////////////////////
0: import org.apache.activemq.broker.LockableServiceSupport;
0: import org.apache.activemq.broker.Locker;
0: import org.apache.activemq.store.SharedFileLocker;
/////////////////////////////////////////////////////////////////////////
0: public class JobSchedulerStoreImpl extends LockableServiceSupport implements JobSchedulerStore {
/////////////////////////////////////////////////////////////////////////
/////////////////////////////////////////////////////////////////////////
/////////////////////////////////////////////////////////////////////////
/////////////////////////////////////////////////////////////////////////
/////////////////////////////////////////////////////////////////////////
1: 
1:     @Override
0:     public Locker createDefaultLocker() throws IOException {
0:         SharedFileLocker locker = new SharedFileLocker();
0:         locker.setDirectory(this.getDirectory());
0:         return locker;
1:     }
1: 
1:     @Override
0:     public void init() throws Exception {
1:     }
commit:7db0fe6
/////////////////////////////////////////////////////////////////////////
/////////////////////////////////////////////////////////////////////////
0: import org.apache.activemq.util.ServiceSupport;
0: public class JobSchedulerStoreImpl extends ServiceSupport implements JobSchedulerStore {
/////////////////////////////////////////////////////////////////////////
0:     LockFile lockFile;
/////////////////////////////////////////////////////////////////////////
commit:44b25cb
/////////////////////////////////////////////////////////////////////////
0: import org.apache.activemq.broker.LockableServiceSupport;
0: import org.apache.activemq.broker.Locker;
0: import org.apache.activemq.store.SharedFileLocker;
/////////////////////////////////////////////////////////////////////////
0: public class JobSchedulerStoreImpl extends LockableServiceSupport implements JobSchedulerStore {
/////////////////////////////////////////////////////////////////////////
/////////////////////////////////////////////////////////////////////////
1: 
1:     @Override
0:     public Locker createDefaultLocker() throws IOException {
0:         SharedFileLocker locker = new SharedFileLocker();
0:         locker.configure(this);
0:         return locker;
1:     }
1: 
1:     @Override
0:     public void init() throws Exception {
1:     }
author:gtully
-------------------------------------------------------------------------------
commit:0a21c5f
/////////////////////////////////////////////////////////////////////////
1:                 LOG.info("Recovering from the scheduled job journal @" + recoveryPosition);
1:                     try {
0:                         JournalCommand<?> message = load(recoveryPosition);
0:                         metaData.setLastUpdateLocation(recoveryPosition);
0:                         doRecover(message, recoveryPosition, lastIndoubtPosition);
0:                         redoCounter++;
1:                     } catch (IOException failedRecovery) {
1:                         if (isIgnoreMissingJournalfiles()) {
1:                             LOG.debug("Failed to recover data at position:" + recoveryPosition, failedRecovery);
1:                             // track this dud location
1:                             journal.corruptRecoveryLocation(recoveryPosition);
1:                         } else {
1:                             throw new IOException("Failed to recover data at position:" + recoveryPosition, failedRecovery);
1:                         }
1:                     }
author:Timothy A. Bish
-------------------------------------------------------------------------------
commit:85bb229
/////////////////////////////////////////////////////////////////////////
commit:f21992e
/////////////////////////////////////////////////////////////////////////
/////////////////////////////////////////////////////////////////////////
0: import java.util.concurrent.atomic.AtomicLong;
1: 
1: import org.apache.activemq.broker.scheduler.JobScheduler;
1: import org.apache.activemq.broker.scheduler.JobSchedulerStore;
0: import org.apache.activemq.store.kahadb.disk.index.BTreeIndex;
0: import org.apache.activemq.store.kahadb.disk.journal.Journal;
1: import org.apache.activemq.store.kahadb.disk.journal.Location;
1: import org.apache.activemq.store.kahadb.disk.page.Page;
1: import org.apache.activemq.store.kahadb.disk.page.PageFile;
1: import org.apache.activemq.store.kahadb.disk.page.Transaction;
0: import org.apache.activemq.store.kahadb.disk.util.IntegerMarshaller;
0: import org.apache.activemq.store.kahadb.disk.util.StringMarshaller;
1: import org.apache.activemq.store.kahadb.disk.util.VariableMarshaller;
1: import org.apache.activemq.util.ByteSequence;
1: import org.apache.activemq.util.IOHelper;
0: import org.apache.activemq.util.LockFile;
0: import org.apache.activemq.util.ServiceStopper;
0: import org.apache.activemq.util.ServiceSupport;
1: import org.slf4j.Logger;
1: import org.slf4j.LoggerFactory;
/////////////////////////////////////////////////////////////////////////
0:     protected AtomicLong journalSize = new AtomicLong(0);
/////////////////////////////////////////////////////////////////////////
1: 
/////////////////////////////////////////////////////////////////////////
/////////////////////////////////////////////////////////////////////////
1: 
0:         @Override
0:         @Override
0:         @Override
/////////////////////////////////////////////////////////////////////////
0:         @Override
/////////////////////////////////////////////////////////////////////////
1: 
1: 
0:         @Override
0:         @Override
/////////////////////////////////////////////////////////////////////////
1: 
0:         if (!isStarted()) {
0:             return journalSize.get() + pageFile.getDiskSize();
/////////////////////////////////////////////////////////////////////////
0:                 @Override
/////////////////////////////////////////////////////////////////////////
0:                 @Override
/////////////////////////////////////////////////////////////////////////
0:         this.journal.setSizeAccumulator(this.journalSize);
0:             @Override
/////////////////////////////////////////////////////////////////////////
0:                 for (JobSchedulerImpl js : schedulers.values()) {
0:                         JobSchedulerStoreImpl.LOG.error("Failed to load " + js.getName(), e);
1:                 }
1: 
/////////////////////////////////////////////////////////////////////////
/////////////////////////////////////////////////////////////////////////
/////////////////////////////////////////////////////////////////////////
/////////////////////////////////////////////////////////////////////////
0:                         LOG.info("Database " + lockFileName + " is locked... waiting " + (DATABASE_LOCKED_WAIT_DELAY / 1000)
0:                             + " seconds for the database to be unlocked. Reason: " + e);
/////////////////////////////////////////////////////////////////////////
author:Hiram R. Chirino
-------------------------------------------------------------------------------
commit:c5cf038
commit:0484af1
/////////////////////////////////////////////////////////////////////////
1: /**
1:  * Licensed to the Apache Software Foundation (ASF) under one or more
1:  * contributor license agreements.  See the NOTICE file distributed with
1:  * this work for additional information regarding copyright ownership.
1:  * The ASF licenses this file to You under the Apache License, Version 2.0
1:  * (the "License"); you may not use this file except in compliance with
1:  * the License.  You may obtain a copy of the License at
1:  *
1:  *      http://www.apache.org/licenses/LICENSE-2.0
1:  *
1:  * Unless required by applicable law or agreed to in writing, software
1:  * distributed under the License is distributed on an "AS IS" BASIS,
1:  * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
1:  * See the License for the specific language governing permissions and
1:  * limitations under the License.
1:  */
1: package org.apache.activemq.store.kahadb.scheduler;
1: 
0: import org.apache.activemq.broker.scheduler.JobScheduler;
0: import org.apache.activemq.broker.scheduler.JobSchedulerStore;
0: import org.apache.activemq.util.IOHelper;
0: import org.apache.activemq.util.ServiceStopper;
0: import org.apache.activemq.util.ServiceSupport;
0: import org.apache.activemq.store.kahadb.disk.index.BTreeIndex;
0: import org.apache.activemq.store.kahadb.disk.journal.Journal;
0: import org.apache.activemq.store.kahadb.disk.journal.Location;
0: import org.apache.activemq.store.kahadb.disk.page.Page;
0: import org.apache.activemq.store.kahadb.disk.page.PageFile;
0: import org.apache.activemq.store.kahadb.disk.page.Transaction;
0: import org.apache.activemq.util.ByteSequence;
0: import org.apache.activemq.store.kahadb.disk.util.IntegerMarshaller;
0: import org.apache.activemq.util.LockFile;
0: import org.apache.activemq.store.kahadb.disk.util.StringMarshaller;
0: import org.apache.activemq.store.kahadb.disk.util.VariableMarshaller;
0: import org.slf4j.Logger;
0: import org.slf4j.LoggerFactory;
1: 
1: import java.io.DataInput;
1: import java.io.DataOutput;
1: import java.io.File;
1: import java.io.IOException;
1: import java.util.ArrayList;
1: import java.util.HashMap;
1: import java.util.HashSet;
1: import java.util.Iterator;
1: import java.util.List;
1: import java.util.Map;
1: import java.util.Map.Entry;
1: import java.util.Set;
1: 
0: public class JobSchedulerStoreImpl extends ServiceSupport implements JobSchedulerStore {
0:     static final Logger LOG = LoggerFactory.getLogger(JobSchedulerStoreImpl.class);
0:     private static final int DATABASE_LOCKED_WAIT_DELAY = 10 * 1000;
1: 
0:     public static final int CLOSED_STATE = 1;
0:     public static final int OPEN_STATE = 2;
1: 
0:     private File directory;
0:     PageFile pageFile;
0:     private Journal journal;
0:     private LockFile lockFile;
0:     private boolean failIfDatabaseIsLocked;
0:     private int journalMaxFileLength = Journal.DEFAULT_MAX_FILE_LENGTH;
0:     private int journalMaxWriteBatchSize = Journal.DEFAULT_MAX_WRITE_BATCH_SIZE;
0:     private boolean enableIndexWriteAsync = false;
0:     // private int indexWriteBatchSize = PageFile.DEFAULT_WRITE_BATCH_SIZE;
0:     MetaData metaData = new MetaData(this);
0:     final MetaDataMarshaller metaDataMarshaller = new MetaDataMarshaller(this);
0:     Map<String, JobSchedulerImpl> schedulers = new HashMap<String, JobSchedulerImpl>();
1: 
0:     protected class MetaData {
0:         protected MetaData(JobSchedulerStoreImpl store) {
1:             this.store = store;
1:         }
1:         private final JobSchedulerStoreImpl store;
0:         Page<MetaData> page;
0:         BTreeIndex<Integer, Integer> journalRC;
0:         BTreeIndex<String, JobSchedulerImpl> storedSchedulers;
1: 
0:         void createIndexes(Transaction tx) throws IOException {
0:             this.storedSchedulers = new BTreeIndex<String, JobSchedulerImpl>(pageFile, tx.allocate().getPageId());
0:             this.journalRC = new BTreeIndex<Integer, Integer>(pageFile, tx.allocate().getPageId());
1:         }
1: 
0:         void load(Transaction tx) throws IOException {
0:             this.storedSchedulers.setKeyMarshaller(StringMarshaller.INSTANCE);
0:             this.storedSchedulers.setValueMarshaller(new JobSchedulerMarshaller(this.store));
0:             this.storedSchedulers.load(tx);
0:             this.journalRC.setKeyMarshaller(IntegerMarshaller.INSTANCE);
0:             this.journalRC.setValueMarshaller(IntegerMarshaller.INSTANCE);
0:             this.journalRC.load(tx);
1:         }
1: 
0:         void loadScheduler(Transaction tx, Map<String, JobSchedulerImpl> schedulers) throws IOException {
0:             for (Iterator<Entry<String, JobSchedulerImpl>> i = this.storedSchedulers.iterator(tx); i.hasNext();) {
0:                 Entry<String, JobSchedulerImpl> entry = i.next();
0:                 entry.getValue().load(tx);
0:                 schedulers.put(entry.getKey(), entry.getValue());
1:             }
1:         }
1: 
0:         public void read(DataInput is) throws IOException {
0:             this.storedSchedulers = new BTreeIndex<String, JobSchedulerImpl>(pageFile, is.readLong());
0:             this.storedSchedulers.setKeyMarshaller(StringMarshaller.INSTANCE);
0:             this.storedSchedulers.setValueMarshaller(new JobSchedulerMarshaller(this.store));
0:             this.journalRC = new BTreeIndex<Integer, Integer>(pageFile, is.readLong());
0:             this.journalRC.setKeyMarshaller(IntegerMarshaller.INSTANCE);
0:             this.journalRC.setValueMarshaller(IntegerMarshaller.INSTANCE);
1:         }
1: 
0:         public void write(DataOutput os) throws IOException {
0:             os.writeLong(this.storedSchedulers.getPageId());
0:             os.writeLong(this.journalRC.getPageId());
1: 
1:         }
1:     }
1: 
0:     class MetaDataMarshaller extends VariableMarshaller<MetaData> {
1:         private final JobSchedulerStoreImpl store;
1: 
1:         MetaDataMarshaller(JobSchedulerStoreImpl store) {
1:             this.store = store;
1:         }
0:         public MetaData readPayload(DataInput dataIn) throws IOException {
0:             MetaData rc = new MetaData(this.store);
1:             rc.read(dataIn);
1:             return rc;
1:         }
1: 
0:         public void writePayload(MetaData object, DataOutput dataOut) throws IOException {
1:             object.write(dataOut);
1:         }
1:     }
1: 
0:     class ValueMarshaller extends VariableMarshaller<List<JobLocation>> {
0:         public List<JobLocation> readPayload(DataInput dataIn) throws IOException {
0:             List<JobLocation> result = new ArrayList<JobLocation>();
0:             int size = dataIn.readInt();
0:             for (int i = 0; i < size; i++) {
0:                 JobLocation jobLocation = new JobLocation();
0:                 jobLocation.readExternal(dataIn);
0:                 result.add(jobLocation);
1:             }
0:             return result;
1:         }
1: 
0:         public void writePayload(List<JobLocation> value, DataOutput dataOut) throws IOException {
0:             dataOut.writeInt(value.size());
0:             for (JobLocation jobLocation : value) {
0:                 jobLocation.writeExternal(dataOut);
1:             }
1:         }
1:     }
1: 
0:     class JobSchedulerMarshaller extends VariableMarshaller<JobSchedulerImpl> {
1:         private final JobSchedulerStoreImpl store;
0:         JobSchedulerMarshaller(JobSchedulerStoreImpl store) {
1:             this.store = store;
1:         }
0:         public JobSchedulerImpl readPayload(DataInput dataIn) throws IOException {
0:             JobSchedulerImpl result = new JobSchedulerImpl(this.store);
0:             result.read(dataIn);
0:             return result;
1:         }
1: 
0:         public void writePayload(JobSchedulerImpl js, DataOutput dataOut) throws IOException {
0:             js.write(dataOut);
1:         }
1:     }
1: 
0:     @Override
0:     public File getDirectory() {
0:         return directory;
1:     }
1: 
0:     @Override
0:     public void setDirectory(File directory) {
0:         this.directory = directory;
1:     }
1:     
0:     @Override
0:     public long size() {
0:         if ( !isStarted() ) {
0:             return 0;
1:         }
1:         try {
0:             return journal.getDiskSize() + pageFile.getDiskSize();
0:         } catch (IOException e) {
0:             throw new RuntimeException(e);
1:         }
1:     }
1: 
0:     @Override
0:     public JobScheduler getJobScheduler(final String name) throws Exception {
0:         JobSchedulerImpl result = this.schedulers.get(name);
0:         if (result == null) {
0:             final JobSchedulerImpl js = new JobSchedulerImpl(this);
0:             js.setName(name);
0:             getPageFile().tx().execute(new Transaction.Closure<IOException>() {
0:                 public void execute(Transaction tx) throws IOException {
0:                     js.createIndexes(tx);
0:                     js.load(tx);
0:                     metaData.storedSchedulers.put(tx, name, js);
1:                 }
0:             });
0:             result = js;
0:             this.schedulers.put(name, js);
0:             if (isStarted()) {
0:                 result.start();
1:             }
0:             this.pageFile.flush();
1:         }
0:         return result;
1:     }
1: 
0:     @Override
0:     synchronized public boolean removeJobScheduler(final String name) throws Exception {
0:         boolean result = false;
0:         final JobSchedulerImpl js = this.schedulers.remove(name);
0:         result = js != null;
0:         if (result) {
0:             js.stop();
0:             getPageFile().tx().execute(new Transaction.Closure<IOException>() {
0:                 public void execute(Transaction tx) throws IOException {
0:                     metaData.storedSchedulers.remove(tx, name);
0:                     js.destroy(tx);
1:                 }
0:             });
1:         }
0:         return result;
1:     }
1: 
0:     @Override
0:     protected synchronized void doStart() throws Exception {
0:         if (this.directory == null) {
0:             this.directory = new File(IOHelper.getDefaultDataDirectory() + File.pathSeparator + "delayedDB");
1:         }
0:         IOHelper.mkdirs(this.directory);
0:         lock();
0:         this.journal = new Journal();
0:         this.journal.setDirectory(directory);
0:         this.journal.setMaxFileLength(getJournalMaxFileLength());
0:         this.journal.setWriteBatchSize(getJournalMaxWriteBatchSize());
0:         this.journal.start();
0:         this.pageFile = new PageFile(directory, "scheduleDB");
0:         this.pageFile.setWriteBatchSize(1);
0:         this.pageFile.load();
1: 
0:         this.pageFile.tx().execute(new Transaction.Closure<IOException>() {
0:             public void execute(Transaction tx) throws IOException {
0:                 if (pageFile.getPageCount() == 0) {
0:                     Page<MetaData> page = tx.allocate();
0:                     assert page.getPageId() == 0;
0:                     page.set(metaData);
0:                     metaData.page = page;
0:                     metaData.createIndexes(tx);
0:                     tx.store(metaData.page, metaDataMarshaller, true);
1: 
0:                 } else {
0:                     Page<MetaData> page = tx.load(0, metaDataMarshaller);
0:                     metaData = page.get();
0:                     metaData.page = page;
1:                 }
0:                 metaData.load(tx);
0:                 metaData.loadScheduler(tx, schedulers);
0:                 for (JobSchedulerImpl js :schedulers.values()) {
1:                     try {
0:                         js.start();
1:                     } catch (Exception e) {
0:                         JobSchedulerStoreImpl.LOG.error("Failed to load " + js.getName(),e);
1:                     }
1:                }
1:             }
0:         });
1: 
0:         this.pageFile.flush();
0:         LOG.info(this + " started");
1:     }
1:     
0:     @Override
0:     protected synchronized void doStop(ServiceStopper stopper) throws Exception {
0:         for (JobSchedulerImpl js : this.schedulers.values()) {
0:             js.stop();
1:         }
0:         if (this.pageFile != null) {
0:             this.pageFile.unload();
1:         }
0:         if (this.journal != null) {
0:             journal.close();
1:         }
0:         if (this.lockFile != null) {
0:             this.lockFile.unlock();
1:         }
0:         this.lockFile = null;
0:         LOG.info(this + " stopped");
1: 
1:     }
1: 
0:     synchronized void incrementJournalCount(Transaction tx, Location location) throws IOException {
0:         int logId = location.getDataFileId();
0:         Integer val = this.metaData.journalRC.get(tx, logId);
0:         int refCount = val != null ? val.intValue() + 1 : 1;
0:         this.metaData.journalRC.put(tx, logId, refCount);
1: 
1:     }
1: 
0:     synchronized void decrementJournalCount(Transaction tx, Location location) throws IOException {
0:         int logId = location.getDataFileId();
0:         int refCount = this.metaData.journalRC.get(tx, logId);
0:         refCount--;
0:         if (refCount <= 0) {
0:             this.metaData.journalRC.remove(tx, logId);
0:             Set<Integer> set = new HashSet<Integer>();
0:             set.add(logId);
0:             this.journal.removeDataFiles(set);
0:         } else {
0:             this.metaData.journalRC.put(tx, logId, refCount);
1:         }
1: 
1:     }
1: 
0:     synchronized ByteSequence getPayload(Location location) throws IllegalStateException, IOException {
0:         ByteSequence result = null;
0:         result = this.journal.read(location);
0:         return result;
1:     }
1: 
0:     synchronized Location write(ByteSequence payload, boolean sync) throws IllegalStateException, IOException {
0:         return this.journal.write(payload, sync);
1:     }
1: 
0:     private void lock() throws IOException {
0:         if (lockFile == null) {
0:             File lockFileName = new File(directory, "lock");
0:             lockFile = new LockFile(lockFileName, true);
0:             if (failIfDatabaseIsLocked) {
0:                 lockFile.lock();
0:             } else {
0:                 while (true) {
1:                     try {
0:                         lockFile.lock();
0:                         break;
0:                     } catch (IOException e) {
0:                         LOG.info("Database " + lockFileName + " is locked... waiting "
0:                                 + (DATABASE_LOCKED_WAIT_DELAY / 1000)
0:                                 + " seconds for the database to be unlocked. Reason: " + e);
1:                         try {
0:                             Thread.sleep(DATABASE_LOCKED_WAIT_DELAY);
0:                         } catch (InterruptedException e1) {
1:                         }
1:                     }
1:                 }
1:             }
1:         }
1:     }
1: 
0:     PageFile getPageFile() {
0:         this.pageFile.isLoaded();
0:         return this.pageFile;
1:     }
1: 
0:     public boolean isFailIfDatabaseIsLocked() {
0:         return failIfDatabaseIsLocked;
1:     }
1: 
0:     public void setFailIfDatabaseIsLocked(boolean failIfDatabaseIsLocked) {
0:         this.failIfDatabaseIsLocked = failIfDatabaseIsLocked;
1:     }
1: 
0:     public int getJournalMaxFileLength() {
0:         return journalMaxFileLength;
1:     }
1: 
0:     public void setJournalMaxFileLength(int journalMaxFileLength) {
0:         this.journalMaxFileLength = journalMaxFileLength;
1:     }
1: 
0:     public int getJournalMaxWriteBatchSize() {
0:         return journalMaxWriteBatchSize;
1:     }
1: 
0:     public void setJournalMaxWriteBatchSize(int journalMaxWriteBatchSize) {
0:         this.journalMaxWriteBatchSize = journalMaxWriteBatchSize;
1:     }
1: 
0:     public boolean isEnableIndexWriteAsync() {
0:         return enableIndexWriteAsync;
1:     }
1: 
0:     public void setEnableIndexWriteAsync(boolean enableIndexWriteAsync) {
0:         this.enableIndexWriteAsync = enableIndexWriteAsync;
1:     }
1: 
0:     @Override
0:     public String toString() {
0:         return "JobSchedulerStore:" + this.directory;
1:     }
1: 
1: }
============================================================================