1:2052239: /**
1:2052239:  * Licensed to the Apache Software Foundation (ASF) under one or more
1:2052239:  * contributor license agreements.  See the NOTICE file distributed with
1:2052239:  * this work for additional information regarding copyright ownership.
1:2052239:  * The ASF licenses this file to You under the Apache License, Version 2.0
1:2052239:  * (the "License"); you may not use this file except in compliance with
1:2052239:  * the License.  You may obtain a copy of the License at
1:2052239:  *
1:2052239:  *      http://www.apache.org/licenses/LICENSE-2.0
1:2052239:  *
1:2052239:  * Unless required by applicable law or agreed to in writing, software
1:2052239:  * distributed under the License is distributed on an "AS IS" BASIS,
1:2052239:  * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
1:2052239:  * See the License for the specific language governing permissions and
1:2052239:  * limitations under the License.
1:2052239:  */
1:2052239: package org.apache.activemq.store.kahadb;
1:2052239: 
1:2052239: import org.apache.activemq.ActiveMQConnectionFactory;
1:2052239: import org.apache.activemq.broker.BrokerService;
1:2052239: import org.apache.activemq.broker.region.policy.PolicyEntry;
1:2052239: import org.apache.activemq.broker.region.policy.PolicyMap;
1:2052239: import org.apache.activemq.command.ActiveMQQueue;
1:2052239: import org.apache.activemq.store.kahadb.disk.journal.DataFile;
1:2052239: import org.junit.After;
1:2052239: import org.junit.Test;
1:2052239: import org.slf4j.Logger;
1:2052239: import org.slf4j.LoggerFactory;
1:2052239: 
1:2052239: import javax.jms.Connection;
1:2052239: import javax.jms.Destination;
1:2052239: import javax.jms.Message;
1:2052239: import javax.jms.MessageProducer;
1:2052239: import javax.jms.Session;
1:2052239: import java.io.File;
1:2052239: import java.io.IOException;
1:2052239: import java.security.Permission;
1:2052239: import java.util.Collection;
1:2052239: import java.util.concurrent.CountDownLatch;
1:2052239: import java.util.concurrent.TimeUnit;
1:2052239: import java.util.concurrent.atomic.AtomicInteger;
1:2052239: 
1:2052239: import static org.apache.activemq.store.kahadb.JournalCorruptionEofIndexRecoveryTest.drain;
1:2052239: import static org.apache.activemq.store.kahadb.disk.journal.Journal.DEFAULT_ARCHIVE_DIRECTORY;
1:2052239: import static org.junit.Assert.*;
1:2052239: 
1:2052239: public class JournalArchiveTest {
1:2052239: 
1:2052239:     private static final Logger LOG = LoggerFactory.getLogger(JournalArchiveTest.class);
1:2052239: 
1:2052239:     private final String KAHADB_DIRECTORY = "target/activemq-data/";
1:2052239:     private final String payload = new String(new byte[1024]);
1:2052239: 
1:2052239:     private BrokerService broker = null;
1:2052239:     private final Destination destination = new ActiveMQQueue("Test");
1:2052239:     private KahaDBPersistenceAdapter adapter;
1:2052239: 
1:2052239:     protected void startBroker() throws Exception {
1:2052239:         doStartBroker(true);
1:2052239:     }
1:2052239: 
1:2052239:     protected void restartBroker() throws Exception {
1:2052239:         if (broker != null) {
1:2052239:             broker.stop();
1:2052239:             broker.waitUntilStopped();
1:2052239:         }
1:2052239: 
1:2052239:         doStartBroker(false);
1:2052239:     }
1:2052239: 
1:2052239:     private void doStartBroker(boolean delete) throws Exception {
1:2052239:         doCreateBroker(delete);
1:2052239:         LOG.info("Starting broker..");
1:2052239:         broker.start();
1:2052239:     }
1:2052239: 
1:2052239:     private void doCreateBroker(boolean delete) throws Exception {
1:2052239: 
1:2052239:         broker = new BrokerService();
1:2052239:         broker.setDeleteAllMessagesOnStartup(delete);
1:2052239:         broker.setPersistent(true);
1:2052239:         broker.setUseJmx(true);
1:2052239:         broker.setDataDirectory(KAHADB_DIRECTORY);
1:2052239: 
1:2052239:         PolicyMap policyMap = new PolicyMap();
1:2052239:         PolicyEntry policyEntry = new PolicyEntry();
1:2052239:         policyEntry.setUseCache(false);
1:2052239:         policyMap.setDefaultEntry(policyEntry);
1:2052239:         broker.setDestinationPolicy(policyMap);
1:2052239: 
1:2052239:         configurePersistence(broker);
1:2052239:     }
1:2052239: 
1:2052239:     protected void configurePersistence(BrokerService brokerService) throws Exception {
1:2052239:         adapter = (KahaDBPersistenceAdapter) brokerService.getPersistenceAdapter();
1:2052239: 
1:2052239:         // ensure there are a bunch of data files but multiple entries in each
1:2052239:         adapter.setJournalMaxFileLength(1024 * 20);
1:2052239: 
1:2052239:         // speed up the test case, checkpoint an cleanup early and often
1:2052239:         adapter.setCheckpointInterval(2000);
1:2052239:         adapter.setCleanupInterval(2000);
1:2052239: 
1:2052239:         adapter.setCheckForCorruptJournalFiles(true);
1:2052239: 
1:2052239:         adapter.setArchiveDataLogs(true);
1:2052239:     }
1:2052239: 
1:2052239:     @After
1:2052239:     public void tearDown() throws Exception {
1:2052239:         if (broker != null) {
1:2052239:             broker.stop();
1:2052239:             broker.waitUntilStopped();
1:2052239:         }
1:2052239:     }
1:2052239: 
1:2052239: 
1:2052239:     @Test
1:2052239:     public void testRecoveryOnArchiveFailure() throws Exception {
1:2052239:         final AtomicInteger atomicInteger = new AtomicInteger();
1:2052239: 
1:2052239:         System.setSecurityManager(new SecurityManager() {
1:2052239:             public void checkPermission(Permission perm) {}
1:2052239:             public void checkPermission(Permission perm, Object context) {}
1:2052239: 
1:2052239:             public void checkWrite(String file) {
1:2052239:                 if (file.contains(DEFAULT_ARCHIVE_DIRECTORY) && atomicInteger.incrementAndGet() > 4) {
1:2052239:                     throw new SecurityException("No Perms to write to archive times:" + atomicInteger.get());
1:2052239:                 }
1:2052239:             }
1:2052239:         });
1:2052239:         startBroker();
1:2052239: 
1:2052239:         int sent = produceMessagesToConsumeMultipleDataFiles(50);
1:2052239: 
1:2052239:         int numFilesAfterSend = getNumberOfJournalFiles();
1:2052239:         LOG.info("Num journal files: " + numFilesAfterSend);
1:2052239: 
1:2052239:         assertTrue("more than x files: " + numFilesAfterSend, numFilesAfterSend > 4);
1:2052239: 
1:2052239:         final CountDownLatch gotShutdown = new CountDownLatch(1);
1:2052239:         broker.addShutdownHook(new Runnable() {
1:2052239:             @Override
1:2052239:             public void run() {
1:2052239:                 gotShutdown.countDown();
1:2052239:             }
1:2052239:         });
1:2052239: 
1:2052239:         int received = tryConsume(destination, sent);
1:2052239:         assertEquals("all message received", sent, received);
1:2052239:         assertTrue("broker got shutdown on page in error", gotShutdown.await(10, TimeUnit.SECONDS));
1:2052239: 
1:2052239:         // no restrictions
1:2052239:         System.setSecurityManager(null);
1:2052239: 
1:2052239:         int numFilesAfterRestart = 0;
1:2052239:         try {
1:2052239:             // ensure we can restart after failure to archive
1:2052239:             doStartBroker(false);
1:2052239:             numFilesAfterRestart = getNumberOfJournalFiles();
1:2052239:             LOG.info("Num journal files before gc: " + numFilesAfterRestart);
1:2052239: 
1:2052239:             // force gc
1:2052239:             ((KahaDBPersistenceAdapter) broker.getPersistenceAdapter()).getStore().checkpoint(true);
1:2052239: 
1:2052239:         } catch (Exception error) {
1:2052239:             LOG.error("Failed to restart!", error);
1:2052239:             fail("Failed to restart after failure to archive");
1:2052239:         }
1:2052239:         int numFilesAfterGC = getNumberOfJournalFiles();
1:2052239:         LOG.info("Num journal files after restart nd gc: " + numFilesAfterGC);
1:2052239:         assertTrue("Gc has happened", numFilesAfterGC < numFilesAfterRestart);
1:2052239:         assertTrue("Gc has worked", numFilesAfterGC < 4);
1:2052239: 
1:2052239:         File archiveDirectory = ((KahaDBPersistenceAdapter) broker.getPersistenceAdapter()).getStore().getJournal().getDirectoryArchive();
1:2052239:         assertEquals("verify files in archive dir", numFilesAfterSend, archiveDirectory.listFiles().length);
1:2052239:     }
1:2052239: 
1:2052239: 
1:2052239:     private int getNumberOfJournalFiles() throws IOException {
1:2052239:         Collection<DataFile> files = ((KahaDBPersistenceAdapter) broker.getPersistenceAdapter()).getStore().getJournal().getFileMap().values();
1:2052239:         int reality = 0;
1:2052239:         for (DataFile file : files) {
1:2052239:             if (file != null) {
1:2052239:                 reality++;
1:2052239:             }
1:2052239:         }
1:2052239:         return reality;
1:2052239:     }
1:2052239: 
1:2052239:     private int produceMessages(Destination destination, int numToSend) throws Exception {
1:2052239:         int sent = 0;
1:2052239:         Connection connection = new ActiveMQConnectionFactory(broker.getVmConnectorURI()).createConnection();
1:2052239:         connection.start();
1:2052239:         try {
1:2052239:             Session session = connection.createSession(false, Session.AUTO_ACKNOWLEDGE);
1:2052239:             MessageProducer producer = session.createProducer(destination);
1:2052239:             for (int i = 0; i < numToSend; i++) {
1:2052239:                 producer.send(createMessage(session, i));
1:2052239:                 sent++;
1:2052239:             }
1:2052239:         } finally {
1:2052239:             connection.close();
1:2052239:         }
1:2052239: 
1:2052239:         return sent;
1:2052239:     }
1:2052239: 
1:2052239:     private int tryConsume(Destination destination, int numToGet) throws Exception {
1:2052239:         ActiveMQConnectionFactory cf = new ActiveMQConnectionFactory(broker.getVmConnectorURI());
1:2052239:         return  drain(cf, destination, numToGet);
1:2052239:     }
1:2052239: 
1:2052239:     private int produceMessagesToConsumeMultipleDataFiles(int numToSend) throws Exception {
1:2052239:         return produceMessages(destination, numToSend);
1:2052239:     }
1:2052239: 
1:2052239:     private Message createMessage(Session session, int i) throws Exception {
1:2052239:         return session.createTextMessage(payload + "::" + i);
1:2052239:     }
1:2052239: }
============================================================================
author:gtully
-------------------------------------------------------------------------------
commit:2052239
/////////////////////////////////////////////////////////////////////////
1: /**
1:  * Licensed to the Apache Software Foundation (ASF) under one or more
1:  * contributor license agreements.  See the NOTICE file distributed with
1:  * this work for additional information regarding copyright ownership.
1:  * The ASF licenses this file to You under the Apache License, Version 2.0
1:  * (the "License"); you may not use this file except in compliance with
1:  * the License.  You may obtain a copy of the License at
1:  *
1:  *      http://www.apache.org/licenses/LICENSE-2.0
1:  *
1:  * Unless required by applicable law or agreed to in writing, software
1:  * distributed under the License is distributed on an "AS IS" BASIS,
1:  * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
1:  * See the License for the specific language governing permissions and
1:  * limitations under the License.
1:  */
1: package org.apache.activemq.store.kahadb;
1: 
1: import org.apache.activemq.ActiveMQConnectionFactory;
1: import org.apache.activemq.broker.BrokerService;
1: import org.apache.activemq.broker.region.policy.PolicyEntry;
1: import org.apache.activemq.broker.region.policy.PolicyMap;
1: import org.apache.activemq.command.ActiveMQQueue;
1: import org.apache.activemq.store.kahadb.disk.journal.DataFile;
1: import org.junit.After;
1: import org.junit.Test;
1: import org.slf4j.Logger;
1: import org.slf4j.LoggerFactory;
1: 
1: import javax.jms.Connection;
1: import javax.jms.Destination;
1: import javax.jms.Message;
1: import javax.jms.MessageProducer;
1: import javax.jms.Session;
1: import java.io.File;
1: import java.io.IOException;
1: import java.security.Permission;
1: import java.util.Collection;
1: import java.util.concurrent.CountDownLatch;
1: import java.util.concurrent.TimeUnit;
1: import java.util.concurrent.atomic.AtomicInteger;
1: 
1: import static org.apache.activemq.store.kahadb.JournalCorruptionEofIndexRecoveryTest.drain;
1: import static org.apache.activemq.store.kahadb.disk.journal.Journal.DEFAULT_ARCHIVE_DIRECTORY;
1: import static org.junit.Assert.*;
1: 
1: public class JournalArchiveTest {
1: 
1:     private static final Logger LOG = LoggerFactory.getLogger(JournalArchiveTest.class);
1: 
1:     private final String KAHADB_DIRECTORY = "target/activemq-data/";
1:     private final String payload = new String(new byte[1024]);
1: 
1:     private BrokerService broker = null;
1:     private final Destination destination = new ActiveMQQueue("Test");
1:     private KahaDBPersistenceAdapter adapter;
1: 
1:     protected void startBroker() throws Exception {
1:         doStartBroker(true);
1:     }
1: 
1:     protected void restartBroker() throws Exception {
1:         if (broker != null) {
1:             broker.stop();
1:             broker.waitUntilStopped();
1:         }
1: 
1:         doStartBroker(false);
1:     }
1: 
1:     private void doStartBroker(boolean delete) throws Exception {
1:         doCreateBroker(delete);
1:         LOG.info("Starting broker..");
1:         broker.start();
1:     }
1: 
1:     private void doCreateBroker(boolean delete) throws Exception {
1: 
1:         broker = new BrokerService();
1:         broker.setDeleteAllMessagesOnStartup(delete);
1:         broker.setPersistent(true);
1:         broker.setUseJmx(true);
1:         broker.setDataDirectory(KAHADB_DIRECTORY);
1: 
1:         PolicyMap policyMap = new PolicyMap();
1:         PolicyEntry policyEntry = new PolicyEntry();
1:         policyEntry.setUseCache(false);
1:         policyMap.setDefaultEntry(policyEntry);
1:         broker.setDestinationPolicy(policyMap);
1: 
1:         configurePersistence(broker);
1:     }
1: 
1:     protected void configurePersistence(BrokerService brokerService) throws Exception {
1:         adapter = (KahaDBPersistenceAdapter) brokerService.getPersistenceAdapter();
1: 
1:         // ensure there are a bunch of data files but multiple entries in each
1:         adapter.setJournalMaxFileLength(1024 * 20);
1: 
1:         // speed up the test case, checkpoint an cleanup early and often
1:         adapter.setCheckpointInterval(2000);
1:         adapter.setCleanupInterval(2000);
1: 
1:         adapter.setCheckForCorruptJournalFiles(true);
1: 
1:         adapter.setArchiveDataLogs(true);
1:     }
1: 
1:     @After
1:     public void tearDown() throws Exception {
1:         if (broker != null) {
1:             broker.stop();
1:             broker.waitUntilStopped();
1:         }
1:     }
1: 
1: 
1:     @Test
1:     public void testRecoveryOnArchiveFailure() throws Exception {
1:         final AtomicInteger atomicInteger = new AtomicInteger();
1: 
1:         System.setSecurityManager(new SecurityManager() {
1:             public void checkPermission(Permission perm) {}
1:             public void checkPermission(Permission perm, Object context) {}
1: 
1:             public void checkWrite(String file) {
1:                 if (file.contains(DEFAULT_ARCHIVE_DIRECTORY) && atomicInteger.incrementAndGet() > 4) {
1:                     throw new SecurityException("No Perms to write to archive times:" + atomicInteger.get());
1:                 }
1:             }
1:         });
1:         startBroker();
1: 
1:         int sent = produceMessagesToConsumeMultipleDataFiles(50);
1: 
1:         int numFilesAfterSend = getNumberOfJournalFiles();
1:         LOG.info("Num journal files: " + numFilesAfterSend);
1: 
1:         assertTrue("more than x files: " + numFilesAfterSend, numFilesAfterSend > 4);
1: 
1:         final CountDownLatch gotShutdown = new CountDownLatch(1);
1:         broker.addShutdownHook(new Runnable() {
1:             @Override
1:             public void run() {
1:                 gotShutdown.countDown();
1:             }
1:         });
1: 
1:         int received = tryConsume(destination, sent);
1:         assertEquals("all message received", sent, received);
1:         assertTrue("broker got shutdown on page in error", gotShutdown.await(10, TimeUnit.SECONDS));
1: 
1:         // no restrictions
1:         System.setSecurityManager(null);
1: 
1:         int numFilesAfterRestart = 0;
1:         try {
1:             // ensure we can restart after failure to archive
1:             doStartBroker(false);
1:             numFilesAfterRestart = getNumberOfJournalFiles();
1:             LOG.info("Num journal files before gc: " + numFilesAfterRestart);
1: 
1:             // force gc
1:             ((KahaDBPersistenceAdapter) broker.getPersistenceAdapter()).getStore().checkpoint(true);
1: 
1:         } catch (Exception error) {
1:             LOG.error("Failed to restart!", error);
1:             fail("Failed to restart after failure to archive");
1:         }
1:         int numFilesAfterGC = getNumberOfJournalFiles();
1:         LOG.info("Num journal files after restart nd gc: " + numFilesAfterGC);
1:         assertTrue("Gc has happened", numFilesAfterGC < numFilesAfterRestart);
1:         assertTrue("Gc has worked", numFilesAfterGC < 4);
1: 
1:         File archiveDirectory = ((KahaDBPersistenceAdapter) broker.getPersistenceAdapter()).getStore().getJournal().getDirectoryArchive();
1:         assertEquals("verify files in archive dir", numFilesAfterSend, archiveDirectory.listFiles().length);
1:     }
1: 
1: 
1:     private int getNumberOfJournalFiles() throws IOException {
1:         Collection<DataFile> files = ((KahaDBPersistenceAdapter) broker.getPersistenceAdapter()).getStore().getJournal().getFileMap().values();
1:         int reality = 0;
1:         for (DataFile file : files) {
1:             if (file != null) {
1:                 reality++;
1:             }
1:         }
1:         return reality;
1:     }
1: 
1:     private int produceMessages(Destination destination, int numToSend) throws Exception {
1:         int sent = 0;
1:         Connection connection = new ActiveMQConnectionFactory(broker.getVmConnectorURI()).createConnection();
1:         connection.start();
1:         try {
1:             Session session = connection.createSession(false, Session.AUTO_ACKNOWLEDGE);
1:             MessageProducer producer = session.createProducer(destination);
1:             for (int i = 0; i < numToSend; i++) {
1:                 producer.send(createMessage(session, i));
1:                 sent++;
1:             }
1:         } finally {
1:             connection.close();
1:         }
1: 
1:         return sent;
1:     }
1: 
1:     private int tryConsume(Destination destination, int numToGet) throws Exception {
1:         ActiveMQConnectionFactory cf = new ActiveMQConnectionFactory(broker.getVmConnectorURI());
1:         return  drain(cf, destination, numToGet);
1:     }
1: 
1:     private int produceMessagesToConsumeMultipleDataFiles(int numToSend) throws Exception {
1:         return produceMessages(destination, numToSend);
1:     }
1: 
1:     private Message createMessage(Session session, int i) throws Exception {
1:         return session.createTextMessage(payload + "::" + i);
1:     }
1: }
============================================================================