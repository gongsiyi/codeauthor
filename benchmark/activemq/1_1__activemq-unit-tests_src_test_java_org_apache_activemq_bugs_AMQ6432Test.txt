1:9b64e18: /**
1:9b64e18:  * Licensed to the Apache Software Foundation (ASF) under one or more
1:9b64e18:  * contributor license agreements.  See the NOTICE file distributed with
1:9b64e18:  * this work for additional information regarding copyright ownership.
1:9b64e18:  * The ASF licenses this file to You under the Apache License, Version 2.0
1:9b64e18:  * (the "License"); you may not use this file except in compliance with
1:9b64e18:  * the License.  You may obtain a copy of the License at
1:9b64e18:  *
1:9b64e18:  *      http://www.apache.org/licenses/LICENSE-2.0
1:9b64e18:  *
1:9b64e18:  * Unless required by applicable law or agreed to in writing, software
1:9b64e18:  * distributed under the License is distributed on an "AS IS" BASIS,
1:9b64e18:  * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
1:9b64e18:  * See the License for the specific language governing permissions and
1:9b64e18:  * limitations under the License.
1:9b64e18:  */
1:9b64e18: package org.apache.activemq.bugs;
1:9b64e18: 
1:9b64e18: import org.apache.activemq.ActiveMQConnectionFactory;
1:9b64e18: import org.apache.activemq.broker.BrokerService;
1:9b64e18: import org.apache.activemq.broker.jmx.QueueViewMBean;
1:9b64e18: import org.apache.activemq.command.ActiveMQQueue;
1:9b64e18: import org.apache.activemq.store.kahadb.KahaDBPersistenceAdapter;
1:9b64e18: import org.apache.activemq.store.kahadb.MessageDatabase;
1:9b64e18: import org.apache.activemq.util.DefaultTestAppender;
1:9b64e18: import org.apache.activemq.util.Wait;
1:9b64e18: import org.apache.log4j.Appender;
1:9b64e18: import org.apache.log4j.Level;
1:9b64e18: import org.apache.log4j.spi.LoggingEvent;
1:9b64e18: import org.junit.After;
1:9b64e18: import org.junit.Before;
1:9b64e18: import org.junit.Test;
1:9b64e18: import org.slf4j.Logger;
1:9b64e18: import org.slf4j.LoggerFactory;
1:9b64e18: 
1:9b64e18: import javax.jms.BytesMessage;
1:9b64e18: import javax.jms.Connection;
1:9b64e18: import javax.jms.DeliveryMode;
1:9b64e18: import javax.jms.Destination;
1:9b64e18: import javax.jms.MessageConsumer;
1:9b64e18: import javax.jms.MessageProducer;
1:9b64e18: import javax.jms.Session;
1:9b64e18: import javax.management.ObjectName;
1:9b64e18: import java.io.File;
1:9b64e18: import java.io.FilenameFilter;
1:9b64e18: import java.util.Arrays;
1:9b64e18: import java.util.concurrent.ExecutorService;
1:9b64e18: import java.util.concurrent.Executors;
1:9b64e18: import java.util.concurrent.TimeUnit;
1:9b64e18: import java.util.concurrent.atomic.AtomicBoolean;
1:9b64e18: 
1:9b64e18: import static org.junit.Assert.assertFalse;
1:9b64e18: import static org.junit.Assert.assertTrue;
1:9b64e18: 
1:9b64e18: public class AMQ6432Test {
1:9b64e18:     private static final Logger LOG = LoggerFactory.getLogger(AMQ6432Test.class);
1:9b64e18: 
1:9b64e18:     private static final String QUEUE_NAME = "test.queue";
1:9b64e18:     private BrokerService broker;
1:9b64e18: 
1:9b64e18:     @Before
1:9b64e18:     public void setup() throws Exception {
1:9b64e18: 
1:9b64e18:         broker = new BrokerService();
1:9b64e18:         broker.setDeleteAllMessagesOnStartup(true);
1:9b64e18:         broker.setPersistent(true);
1:9b64e18: 
1:9b64e18:         KahaDBPersistenceAdapter kahaDB = new KahaDBPersistenceAdapter();
1:9b64e18:         kahaDB.setJournalMaxFileLength(256 * 1024);
1:9b64e18:         kahaDB.setCleanupInterval(500);
1:9b64e18:         kahaDB.setCompactAcksAfterNoGC(1);
1:9b64e18:         kahaDB.setCompactAcksIgnoresStoreGrowth(true);
1:9b64e18:         broker.setPersistenceAdapter(kahaDB);
1:9b64e18: 
1:9b64e18: 
1:9b64e18:         broker.start();
1:9b64e18:         broker.waitUntilStarted();
1:9b64e18:     }
1:9b64e18: 
1:9b64e18:     @After
1:9b64e18:     public void tearDown() throws Exception {
1:9b64e18:         broker.stop();
1:9b64e18:     }
1:9b64e18: 
1:9b64e18:     @Test
1:9b64e18:     public void testTransactedStoreUsageSuspendResume() throws Exception {
1:9b64e18: 
1:9b64e18:         org.apache.log4j.Logger log4jLogger =
1:9b64e18:                 org.apache.log4j.Logger.getLogger(MessageDatabase.class);
1:9b64e18:         final AtomicBoolean failed = new AtomicBoolean(false);
1:9b64e18: 
1:9b64e18:         final File journalDataDir = ((KahaDBPersistenceAdapter) broker.getPersistenceAdapter()).getStore().getJournal().getDirectory();
1:9b64e18: 
1:9b64e18:         Appender appender = new DefaultTestAppender() {
1:9b64e18:             @Override
1:9b64e18:             public void doAppend(LoggingEvent event) {
1:9b64e18:                 if (event.getLevel().equals(Level.WARN) && event.getMessage().toString().startsWith("Failed to load next journal")) {
1:9b64e18:                     LOG.info("received unexpected log message: " + event.getMessage());
1:9b64e18:                     failed.set(true);
1:9b64e18:                 }
1:9b64e18:             }
1:9b64e18:         };
1:9b64e18:         log4jLogger.addAppender(appender);
1:9b64e18:         try {
1:9b64e18: 
1:9b64e18:             ExecutorService sendExecutor = Executors.newSingleThreadExecutor();
1:9b64e18:             sendExecutor.execute(new Runnable() {
1:9b64e18:                 @Override
1:9b64e18:                 public void run() {
1:9b64e18:                     try {
1:9b64e18:                         sendReceive(10000);
1:9b64e18:                     } catch (Exception ignored) {
1:9b64e18:                     }
1:9b64e18:                 }
1:9b64e18:             });
1:9b64e18: 
1:9b64e18:             sendExecutor.execute(new Runnable() {
1:9b64e18:                 @Override
1:9b64e18:                 public void run() {
1:9b64e18:                     try {
1:9b64e18:                         sendLargeAndPurge(5000);
1:9b64e18:                     } catch (Exception ignored) {
1:9b64e18:                     }
1:9b64e18:                 }
1:9b64e18:             });
1:9b64e18: 
1:9b64e18:             sendExecutor.shutdown();
1:9b64e18:             sendExecutor.awaitTermination(10, TimeUnit.MINUTES);
1:9b64e18: 
1:9b64e18: 
1:9b64e18:             // need to let a few gc cycles to complete then there will be 2 files in the mix and acks will move
1:9b64e18:             TimeUnit.SECONDS.sleep(2);
1:9b64e18: 
1:9b64e18:             assertTrue("gc worked ok", Wait.waitFor(new Wait.Condition() {
1:9b64e18:                 @Override
1:9b64e18:                 public boolean isSatisified() throws Exception {
1:9b64e18:                     return ((KahaDBPersistenceAdapter) broker.getPersistenceAdapter()).getStore().getJournal().getFileMap().size() < 3;
1:9b64e18:                 }
1:9b64e18:             }));
1:9b64e18: 
1:9b64e18:         } finally {
1:9b64e18:             log4jLogger.removeAppender(appender);
1:9b64e18:         }
1:9b64e18:         assertFalse("failed on unexpected log event", failed.get());
1:9b64e18: 
1:9b64e18:         sendReceive(500);
1:9b64e18: 
1:9b64e18:         assertTrue("gc worked ok", Wait.waitFor(new Wait.Condition() {
1:9b64e18:             @Override
1:9b64e18:             public boolean isSatisified() throws Exception {
1:9b64e18:                 return ((KahaDBPersistenceAdapter) broker.getPersistenceAdapter()).getStore().getJournal().getFileMap().size() < 2;
1:9b64e18:             }
1:9b64e18:         }));
1:9b64e18: 
1:9b64e18:         // file actually gone!
1:9b64e18:         LOG.info("Files: " + Arrays.asList(journalDataDir.listFiles()));
1:9b64e18:         assertTrue("Minimum data files in the mix", journalDataDir.listFiles(new FilenameFilter() {
1:9b64e18:             @Override
1:9b64e18:             public boolean accept(File dir, String name) {
1:9b64e18:                 return name.startsWith("db-");
1:9b64e18:             }
1:9b64e18:         }).length == 1);
1:9b64e18: 
1:9b64e18: 
1:9b64e18:     }
1:9b64e18: 
1:9b64e18:     private void sendReceive(int max) throws Exception {
1:9b64e18:         ActiveMQConnectionFactory factory = new ActiveMQConnectionFactory("vm://localhost");
1:9b64e18:         factory.setAlwaysSyncSend(true);
1:9b64e18:         Connection connection = factory.createConnection();
1:9b64e18:         connection.start();
1:9b64e18:         Session session = connection.createSession(false, Session.AUTO_ACKNOWLEDGE);
1:9b64e18:         Destination queue = session.createQueue(QUEUE_NAME+max);
1:9b64e18:         MessageProducer producer = session.createProducer(null);
1:9b64e18: 
1:9b64e18:         producer.setDeliveryMode(DeliveryMode.PERSISTENT);
1:9b64e18:         BytesMessage message = session.createBytesMessage();
1:9b64e18:         message.writeBytes(new byte[10]);
1:9b64e18: 
1:9b64e18:         MessageConsumer consumer = session.createConsumer(queue);
1:9b64e18: 
1:9b64e18:         for (int i=0; i<max; i++) {
1:9b64e18:             producer.send(queue, message);
1:9b64e18:             consumer.receive(4000);
1:9b64e18:         }
1:9b64e18:         connection.close();
1:9b64e18:     }
1:9b64e18: 
1:9b64e18:     private void sendLargeAndPurge(int max) throws Exception {
1:9b64e18:         ActiveMQConnectionFactory factory = new ActiveMQConnectionFactory("vm://localhost");
1:9b64e18:         factory.setAlwaysSyncSend(true);
1:9b64e18:         Connection connection = factory.createConnection();
1:9b64e18:         connection.start();
1:9b64e18:         Session session = connection.createSession(false, Session.AUTO_ACKNOWLEDGE);
1:9b64e18:         ActiveMQQueue toPurge = new ActiveMQQueue(QUEUE_NAME + "-to-purge-" + max);
1:9b64e18:         MessageProducer producer = session.createProducer(null);
1:9b64e18: 
1:9b64e18:         producer.setDeliveryMode(DeliveryMode.PERSISTENT);
1:9b64e18:         BytesMessage message = session.createBytesMessage();
1:9b64e18:         message.writeBytes(new byte[1024]);
1:9b64e18: 
1:9b64e18:         for (int i = 0; i < max; i++) {
1:9b64e18:             producer.send(toPurge, message);
1:9b64e18:         }
1:9b64e18: 
1:9b64e18:         connection.close();
1:9b64e18: 
1:9b64e18:         TimeUnit.SECONDS.sleep(1);
1:9b64e18: 
1:9b64e18:         ObjectName queueViewMBeanName =
1:9b64e18:                 new ObjectName("org.apache.activemq:type=Broker,brokerName=localhost,destinationType=Queue,destinationName="
1:9b64e18:                         + toPurge.getQueueName());
1:9b64e18:         QueueViewMBean proxy = (QueueViewMBean) broker.getManagementContext()
1:9b64e18:                 .newProxyInstance(queueViewMBeanName,
1:9b64e18:                         QueueViewMBean.class, true);
1:9b64e18: 
1:9b64e18:         proxy.purge();
1:9b64e18:     }
1:9b64e18: }
============================================================================
author:gtully
-------------------------------------------------------------------------------
commit:9b64e18
/////////////////////////////////////////////////////////////////////////
1: /**
1:  * Licensed to the Apache Software Foundation (ASF) under one or more
1:  * contributor license agreements.  See the NOTICE file distributed with
1:  * this work for additional information regarding copyright ownership.
1:  * The ASF licenses this file to You under the Apache License, Version 2.0
1:  * (the "License"); you may not use this file except in compliance with
1:  * the License.  You may obtain a copy of the License at
1:  *
1:  *      http://www.apache.org/licenses/LICENSE-2.0
1:  *
1:  * Unless required by applicable law or agreed to in writing, software
1:  * distributed under the License is distributed on an "AS IS" BASIS,
1:  * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
1:  * See the License for the specific language governing permissions and
1:  * limitations under the License.
1:  */
1: package org.apache.activemq.bugs;
1: 
1: import org.apache.activemq.ActiveMQConnectionFactory;
1: import org.apache.activemq.broker.BrokerService;
1: import org.apache.activemq.broker.jmx.QueueViewMBean;
1: import org.apache.activemq.command.ActiveMQQueue;
1: import org.apache.activemq.store.kahadb.KahaDBPersistenceAdapter;
1: import org.apache.activemq.store.kahadb.MessageDatabase;
1: import org.apache.activemq.util.DefaultTestAppender;
1: import org.apache.activemq.util.Wait;
1: import org.apache.log4j.Appender;
1: import org.apache.log4j.Level;
1: import org.apache.log4j.spi.LoggingEvent;
1: import org.junit.After;
1: import org.junit.Before;
1: import org.junit.Test;
1: import org.slf4j.Logger;
1: import org.slf4j.LoggerFactory;
1: 
1: import javax.jms.BytesMessage;
1: import javax.jms.Connection;
1: import javax.jms.DeliveryMode;
1: import javax.jms.Destination;
1: import javax.jms.MessageConsumer;
1: import javax.jms.MessageProducer;
1: import javax.jms.Session;
1: import javax.management.ObjectName;
1: import java.io.File;
1: import java.io.FilenameFilter;
1: import java.util.Arrays;
1: import java.util.concurrent.ExecutorService;
1: import java.util.concurrent.Executors;
1: import java.util.concurrent.TimeUnit;
1: import java.util.concurrent.atomic.AtomicBoolean;
1: 
1: import static org.junit.Assert.assertFalse;
1: import static org.junit.Assert.assertTrue;
1: 
1: public class AMQ6432Test {
1:     private static final Logger LOG = LoggerFactory.getLogger(AMQ6432Test.class);
1: 
1:     private static final String QUEUE_NAME = "test.queue";
1:     private BrokerService broker;
1: 
1:     @Before
1:     public void setup() throws Exception {
1: 
1:         broker = new BrokerService();
1:         broker.setDeleteAllMessagesOnStartup(true);
1:         broker.setPersistent(true);
1: 
1:         KahaDBPersistenceAdapter kahaDB = new KahaDBPersistenceAdapter();
1:         kahaDB.setJournalMaxFileLength(256 * 1024);
1:         kahaDB.setCleanupInterval(500);
1:         kahaDB.setCompactAcksAfterNoGC(1);
1:         kahaDB.setCompactAcksIgnoresStoreGrowth(true);
1:         broker.setPersistenceAdapter(kahaDB);
1: 
1: 
1:         broker.start();
1:         broker.waitUntilStarted();
1:     }
1: 
1:     @After
1:     public void tearDown() throws Exception {
1:         broker.stop();
1:     }
1: 
1:     @Test
1:     public void testTransactedStoreUsageSuspendResume() throws Exception {
1: 
1:         org.apache.log4j.Logger log4jLogger =
1:                 org.apache.log4j.Logger.getLogger(MessageDatabase.class);
1:         final AtomicBoolean failed = new AtomicBoolean(false);
1: 
1:         final File journalDataDir = ((KahaDBPersistenceAdapter) broker.getPersistenceAdapter()).getStore().getJournal().getDirectory();
1: 
1:         Appender appender = new DefaultTestAppender() {
1:             @Override
1:             public void doAppend(LoggingEvent event) {
1:                 if (event.getLevel().equals(Level.WARN) && event.getMessage().toString().startsWith("Failed to load next journal")) {
1:                     LOG.info("received unexpected log message: " + event.getMessage());
1:                     failed.set(true);
1:                 }
1:             }
1:         };
1:         log4jLogger.addAppender(appender);
1:         try {
1: 
1:             ExecutorService sendExecutor = Executors.newSingleThreadExecutor();
1:             sendExecutor.execute(new Runnable() {
1:                 @Override
1:                 public void run() {
1:                     try {
1:                         sendReceive(10000);
1:                     } catch (Exception ignored) {
1:                     }
1:                 }
1:             });
1: 
1:             sendExecutor.execute(new Runnable() {
1:                 @Override
1:                 public void run() {
1:                     try {
1:                         sendLargeAndPurge(5000);
1:                     } catch (Exception ignored) {
1:                     }
1:                 }
1:             });
1: 
1:             sendExecutor.shutdown();
1:             sendExecutor.awaitTermination(10, TimeUnit.MINUTES);
1: 
1: 
1:             // need to let a few gc cycles to complete then there will be 2 files in the mix and acks will move
1:             TimeUnit.SECONDS.sleep(2);
1: 
1:             assertTrue("gc worked ok", Wait.waitFor(new Wait.Condition() {
1:                 @Override
1:                 public boolean isSatisified() throws Exception {
1:                     return ((KahaDBPersistenceAdapter) broker.getPersistenceAdapter()).getStore().getJournal().getFileMap().size() < 3;
1:                 }
1:             }));
1: 
1:         } finally {
1:             log4jLogger.removeAppender(appender);
1:         }
1:         assertFalse("failed on unexpected log event", failed.get());
1: 
1:         sendReceive(500);
1: 
1:         assertTrue("gc worked ok", Wait.waitFor(new Wait.Condition() {
1:             @Override
1:             public boolean isSatisified() throws Exception {
1:                 return ((KahaDBPersistenceAdapter) broker.getPersistenceAdapter()).getStore().getJournal().getFileMap().size() < 2;
1:             }
1:         }));
1: 
1:         // file actually gone!
1:         LOG.info("Files: " + Arrays.asList(journalDataDir.listFiles()));
1:         assertTrue("Minimum data files in the mix", journalDataDir.listFiles(new FilenameFilter() {
1:             @Override
1:             public boolean accept(File dir, String name) {
1:                 return name.startsWith("db-");
1:             }
1:         }).length == 1);
1: 
1: 
1:     }
1: 
1:     private void sendReceive(int max) throws Exception {
1:         ActiveMQConnectionFactory factory = new ActiveMQConnectionFactory("vm://localhost");
1:         factory.setAlwaysSyncSend(true);
1:         Connection connection = factory.createConnection();
1:         connection.start();
1:         Session session = connection.createSession(false, Session.AUTO_ACKNOWLEDGE);
1:         Destination queue = session.createQueue(QUEUE_NAME+max);
1:         MessageProducer producer = session.createProducer(null);
1: 
1:         producer.setDeliveryMode(DeliveryMode.PERSISTENT);
1:         BytesMessage message = session.createBytesMessage();
1:         message.writeBytes(new byte[10]);
1: 
1:         MessageConsumer consumer = session.createConsumer(queue);
1: 
1:         for (int i=0; i<max; i++) {
1:             producer.send(queue, message);
1:             consumer.receive(4000);
1:         }
1:         connection.close();
1:     }
1: 
1:     private void sendLargeAndPurge(int max) throws Exception {
1:         ActiveMQConnectionFactory factory = new ActiveMQConnectionFactory("vm://localhost");
1:         factory.setAlwaysSyncSend(true);
1:         Connection connection = factory.createConnection();
1:         connection.start();
1:         Session session = connection.createSession(false, Session.AUTO_ACKNOWLEDGE);
1:         ActiveMQQueue toPurge = new ActiveMQQueue(QUEUE_NAME + "-to-purge-" + max);
1:         MessageProducer producer = session.createProducer(null);
1: 
1:         producer.setDeliveryMode(DeliveryMode.PERSISTENT);
1:         BytesMessage message = session.createBytesMessage();
1:         message.writeBytes(new byte[1024]);
1: 
1:         for (int i = 0; i < max; i++) {
1:             producer.send(toPurge, message);
1:         }
1: 
1:         connection.close();
1: 
1:         TimeUnit.SECONDS.sleep(1);
1: 
1:         ObjectName queueViewMBeanName =
1:                 new ObjectName("org.apache.activemq:type=Broker,brokerName=localhost,destinationType=Queue,destinationName="
1:                         + toPurge.getQueueName());
1:         QueueViewMBean proxy = (QueueViewMBean) broker.getManagementContext()
1:                 .newProxyInstance(queueViewMBeanName,
1:                         QueueViewMBean.class, true);
1: 
1:         proxy.purge();
1:     }
1: }
============================================================================