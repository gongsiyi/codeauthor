1:73db4d2: /**
1:73db4d2:  * Licensed to the Apache Software Foundation (ASF) under one or more
1:73db4d2:  * contributor license agreements.  See the NOTICE file distributed with
1:73db4d2:  * this work for additional information regarding copyright ownership.
1:73db4d2:  * The ASF licenses this file to You under the Apache License, Version 2.0
1:73db4d2:  * (the "License"); you may not use this file except in compliance with
1:73db4d2:  * the License.  You may obtain a copy of the License at
1:73db4d2:  *
1:73db4d2:  *      http://www.apache.org/licenses/LICENSE-2.0
1:73db4d2:  *
1:73db4d2:  * Unless required by applicable law or agreed to in writing, software
1:73db4d2:  * distributed under the License is distributed on an "AS IS" BASIS,
1:73db4d2:  * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
1:73db4d2:  * See the License for the specific language governing permissions and
1:73db4d2:  * limitations under the License.
1:73db4d2:  */
1:73db4d2: package org.apache.activemq.store.kahadb;
16:73db4d2: 
1:73db4d2: import static org.junit.Assert.assertEquals;
1:73db4d2: import static org.junit.Assert.assertTrue;
1:5db5f3e: import static org.junit.Assert.fail;
1:5db5f3e: 
1:73db4d2: import java.io.File;
1:73db4d2: import java.io.IOException;
1:73db4d2: import java.util.ArrayList;
1:73db4d2: import java.util.Collection;
1:507d40a: import java.util.Iterator;
1:507d40a: import java.util.Map.Entry;
1:8c3ef6c: import java.util.concurrent.atomic.AtomicBoolean;
1:73db4d2: 
1:73db4d2: import javax.jms.Connection;
1:822e2be: import javax.jms.ConnectionFactory;
1:73db4d2: import javax.jms.Destination;
1:6b8e743: import javax.jms.JMSException;
1:73db4d2: import javax.jms.Message;
1:73db4d2: import javax.jms.MessageConsumer;
1:73db4d2: import javax.jms.MessageProducer;
1:73db4d2: import javax.jms.Session;
1:73db4d2: 
1:73db4d2: import org.apache.activemq.ActiveMQConnectionFactory;
1:73db4d2: import org.apache.activemq.broker.BrokerService;
1:73db4d2: import org.apache.activemq.command.ActiveMQQueue;
1:507d40a: import org.apache.activemq.store.kahadb.MessageDatabase.MessageKeys;
1:507d40a: import org.apache.activemq.store.kahadb.MessageDatabase.StoredDestination;
1:73db4d2: import org.apache.activemq.store.kahadb.disk.journal.DataFile;
1:73db4d2: import org.apache.activemq.store.kahadb.disk.journal.Journal;
1:822e2be: import org.apache.activemq.store.kahadb.disk.journal.Location;
1:507d40a: import org.apache.activemq.store.kahadb.disk.page.Transaction;
1:73db4d2: import org.apache.activemq.util.ByteSequence;
1:8c3ef6c: import org.apache.activemq.util.DefaultTestAppender;
1:73db4d2: import org.apache.activemq.util.IOHelper;
1:73db4d2: import org.apache.activemq.util.RecoverableRandomAccessFile;
1:8c3ef6c: import org.apache.log4j.Level;
1:8c3ef6c: import org.apache.log4j.spi.LoggingEvent;
1:73db4d2: import org.junit.After;
1:5db5f3e: import org.junit.Before;
1:73db4d2: import org.junit.Test;
1:73db4d2: import org.slf4j.Logger;
1:73db4d2: import org.slf4j.LoggerFactory;
1:73db4d2: 
1:73db4d2: 
1:73db4d2: public class JournalCorruptionEofIndexRecoveryTest {
1:73db4d2: 
1:73db4d2:     private static final Logger LOG = LoggerFactory.getLogger(JournalCorruptionEofIndexRecoveryTest.class);
1:73db4d2: 
1:13044de:     private ActiveMQConnectionFactory cf = null;
1:13044de:     private BrokerService broker = null;
1:73db4d2:     private String connectionUri;
1:73db4d2:     private KahaDBPersistenceAdapter adapter;
1:5db5f3e:     private boolean ignoreMissingJournalFiles = false;
1:8c218ee:     private int journalMaxBatchSize;
1:73db4d2: 
1:73db4d2:     private final Destination destination = new ActiveMQQueue("Test");
1:13044de:     private final String KAHADB_DIRECTORY = "target/activemq-data/";
1:13044de:     private final String payload = new String(new byte[1024]);
1:8c218ee:     File brokerDataDir = null;
1:73db4d2: 
1:73db4d2:     protected void startBroker() throws Exception {
1:73db4d2:         doStartBroker(true, false);
1:73db4d2:     }
1:73db4d2: 
1:73db4d2:     protected void restartBroker(boolean whackIndex) throws Exception {
1:73db4d2:         restartBroker(whackIndex, false);
1:73db4d2:     }
1:73db4d2: 
1:73db4d2:     protected void restartBroker(boolean whackIndex, boolean forceRecoverIndex) throws Exception {
1:73db4d2:         if (broker != null) {
1:73db4d2:             broker.stop();
1:73db4d2:             broker.waitUntilStopped();
1:73db4d2:         }
1:73db4d2: 
1:73db4d2:         if (whackIndex) {
1:8c218ee:             File indexToDelete = new File(brokerDataDir, "db.data");
1:73db4d2:             LOG.info("Whacking index: " + indexToDelete);
1:73db4d2:             indexToDelete.delete();
1:73db4d2:         }
1:73db4d2: 
1:73db4d2:         doStartBroker(false, forceRecoverIndex);
1:73db4d2:     }
1:73db4d2: 
1:73db4d2: 
1:73db4d2:     private void doStartBroker(boolean delete, boolean forceRecoverIndex) throws Exception {
1:73db4d2:         broker = new BrokerService();
1:13044de:         broker.setDataDirectory(KAHADB_DIRECTORY);
1:73db4d2: 
1:73db4d2:         if (delete) {
1:73db4d2:             IOHelper.deleteChildren(broker.getPersistenceAdapter().getDirectory());
1:73db4d2:             IOHelper.delete(broker.getPersistenceAdapter().getDirectory());
1:73db4d2:         }
1:73db4d2: 
1:73db4d2:         broker.setPersistent(true);
1:73db4d2:         broker.setUseJmx(true);
1:73db4d2:         broker.addConnector("tcp://localhost:0");
1:73db4d2: 
1:73db4d2:         configurePersistence(broker, forceRecoverIndex);
1:73db4d2: 
1:73db4d2:         connectionUri = "vm://localhost?create=false";
1:73db4d2:         cf = new ActiveMQConnectionFactory(connectionUri);
1:73db4d2: 
1:73db4d2:         broker.start();
1:8c218ee:         brokerDataDir = broker.getPersistenceAdapter().getDirectory();
1:73db4d2:         LOG.info("Starting broker..");
1:73db4d2:     }
1:73db4d2: 
1:73db4d2:     protected void configurePersistence(BrokerService brokerService, boolean forceRecoverIndex) throws Exception {
1:73db4d2:         adapter = (KahaDBPersistenceAdapter) brokerService.getPersistenceAdapter();
1:73db4d2: 
1:73db4d2:         adapter.setForceRecoverIndex(forceRecoverIndex);
1:73db4d2: 
1:73db4d2:         // ensure there are a bunch of data files but multiple entries in each
1:73db4d2:         adapter.setJournalMaxFileLength(1024 * 20);
1:73db4d2: 
1:8c218ee:         adapter.setJournalMaxWriteBatchSize(journalMaxBatchSize);
1:8c218ee: 
1:73db4d2:         // speed up the test case, checkpoint an cleanup early and often
1:73db4d2:         adapter.setCheckpointInterval(5000);
1:73db4d2:         adapter.setCleanupInterval(5000);
1:73db4d2: 
1:73db4d2:         adapter.setCheckForCorruptJournalFiles(true);
1:5db5f3e:         adapter.setIgnoreMissingJournalfiles(ignoreMissingJournalFiles);
1:4a82118: 
1:62bdbb0:         adapter.setPreallocationStrategy(Journal.PreallocationStrategy.ZEROS.name());
1:62bdbb0:         adapter.setPreallocationScope(Journal.PreallocationScope.ENTIRE_JOURNAL_ASYNC.name());
1:73db4d2:     }
1:73db4d2: 
1:73db4d2:     @After
1:73db4d2:     public void tearDown() throws Exception {
1:73db4d2:         if (broker != null) {
1:73db4d2:             broker.stop();
1:73db4d2:             broker.waitUntilStopped();
1:73db4d2:         }
1:73db4d2:     }
1:73db4d2: 
1:5db5f3e:     @Before
1:5db5f3e:     public void reset() throws Exception {
1:5db5f3e:         ignoreMissingJournalFiles = true;
1:8c218ee:         journalMaxBatchSize = Journal.DEFAULT_MAX_WRITE_BATCH_SIZE;
1:5db5f3e:     }
1:5db5f3e: 
1:5db5f3e:     @Test
1:5db5f3e:     public void testNoRestartOnCorruptJournal() throws Exception {
1:5db5f3e:         ignoreMissingJournalFiles = false;
1:5db5f3e: 
1:5db5f3e:         startBroker();
1:8c3ef6c: 
1:5db5f3e:         produceMessagesToConsumeMultipleDataFiles(50);
1:5db5f3e: 
1:5db5f3e:         int numFiles = getNumberOfJournalFiles();
1:5db5f3e: 
1:5db5f3e:         assertTrue("more than x files: " + numFiles, numFiles > 2);
1:5db5f3e: 
1:5db5f3e:         corruptBatchEndEof(3);
1:5db5f3e: 
1:5db5f3e:         try {
1:5db5f3e:             restartBroker(true);
1:5db5f3e:             fail("Expect failure to start with corrupt journal");
1:5db5f3e:         } catch (Exception expected) {
1:5db5f3e:         }
1:5db5f3e:     }
1:5db5f3e: 
1:4a82118:     @Test
1:73db4d2:     public void testRecoveryAfterCorruptionEof() throws Exception {
1:73db4d2:         startBroker();
1:5db5f3e: 
1:73db4d2:         produceMessagesToConsumeMultipleDataFiles(50);
1:73db4d2: 
1:73db4d2:         int numFiles = getNumberOfJournalFiles();
1:73db4d2: 
1:73db4d2:         assertTrue("more than x files: " + numFiles, numFiles > 2);
1:73db4d2: 
1:73db4d2:         corruptBatchEndEof(3);
1:73db4d2: 
1:73db4d2:         restartBroker(false);
1:73db4d2: 
1:73db4d2:         assertEquals("missing one message", 49, broker.getAdminView().getTotalMessageCount());
1:73db4d2:         assertEquals("Drain", 49, drainQueue(49));
1:73db4d2:     }
1:73db4d2: 
1:73db4d2:     @Test
1:8c3ef6c:     public void testRecoveryAfterCorruptionMetadataLocation() throws Exception {
1:8c3ef6c:         startBroker();
1:8c3ef6c: 
1:8c3ef6c:         produceMessagesToConsumeMultipleDataFiles(50);
1:8c3ef6c: 
1:8c3ef6c:         int numFiles = getNumberOfJournalFiles();
1:8c3ef6c: 
1:8c3ef6c:         assertTrue("more than x files: " + numFiles, numFiles > 2);
1:73db4d2: 
1:8c3ef6c:         broker.getPersistenceAdapter().checkpoint(true);
1:8c3ef6c:         Location location = ((KahaDBPersistenceAdapter) broker.getPersistenceAdapter()).getStore().getMetadata().producerSequenceIdTrackerLocation;
1:8c3ef6c: 
1:8c3ef6c:         DataFile dataFile = ((KahaDBPersistenceAdapter) broker.getPersistenceAdapter()).getStore().getJournal().getFileMap().get(Integer.valueOf(location.getDataFileId()));
1:8c3ef6c:         RecoverableRandomAccessFile randomAccessFile = dataFile.openRandomAccessFile();
1:8c3ef6c:         randomAccessFile.seek(location.getOffset());
1:8c3ef6c:         randomAccessFile.writeInt(Integer.MAX_VALUE);
1:8c3ef6c:         randomAccessFile.getChannel().force(true);
1:8c3ef6c: 
1:8c3ef6c:         ((KahaDBPersistenceAdapter) broker.getPersistenceAdapter()).getStore().getJournal().close();
1:8c3ef6c:         try {
1:8c3ef6c:             broker.stop();
1:8c3ef6c:             broker.waitUntilStopped();
1:8c3ef6c:         } catch (Exception expected) {
1:8c3ef6c:         } finally {
1:8c3ef6c:             broker = null;
1:8c3ef6c:         }
1:8c3ef6c: 
1:8c3ef6c:         AtomicBoolean trappedExpectedLogMessage = new AtomicBoolean(false);
1:8c3ef6c:         DefaultTestAppender appender = new DefaultTestAppender() {
1:8c3ef6c:             @Override
1:8c3ef6c:             public void doAppend(LoggingEvent event) {
1:8c3ef6c:                 if (event.getLevel() == Level.WARN
1:8c3ef6c:                         && event.getRenderedMessage().contains("Cannot recover message audit")
1:8c3ef6c:                         && event.getThrowableInformation().getThrowable().getLocalizedMessage().contains("Invalid location size")) {
1:8c3ef6c:                     trappedExpectedLogMessage.set(true);
1:8c3ef6c:                 }
1:8c3ef6c:             }
1:8c3ef6c:         };
1:8c3ef6c:         org.apache.log4j.Logger.getRootLogger().addAppender(appender);
1:8c3ef6c: 
1:8c3ef6c: 
1:8c3ef6c:         try {
1:8c3ef6c:             restartBroker(false);
1:8c3ef6c:         } finally {
1:8c3ef6c:             org.apache.log4j.Logger.getRootLogger().removeAppender(appender);
1:8c3ef6c:         }
1:8c3ef6c: 
1:8c3ef6c:         assertEquals("no missing message", 50, broker.getAdminView().getTotalMessageCount());
1:8c3ef6c:         assertTrue("Did replay records on invalid location size", trappedExpectedLogMessage.get());
1:8c3ef6c:     }
1:8c3ef6c: 
1:8c3ef6c:     @Test
1:73db4d2:     public void testRecoveryAfterCorruptionCheckSum() throws Exception {
1:73db4d2:         startBroker();
1:73db4d2: 
1:73db4d2:         produceMessagesToConsumeMultipleDataFiles(4);
1:73db4d2: 
1:73db4d2:         corruptBatchCheckSumSplash(1);
1:73db4d2: 
1:73db4d2:         restartBroker(true);
1:73db4d2: 
1:73db4d2:         assertEquals("missing one message", 3, broker.getAdminView().getTotalMessageCount());
1:73db4d2:         assertEquals("Drain", 3, drainQueue(4));
1:73db4d2:     }
1:73db4d2: 
1:73db4d2:     @Test
1:73db4d2:     public void testRecoveryAfterCorruptionCheckSumExistingIndex() throws Exception {
1:73db4d2:         startBroker();
1:73db4d2: 
1:73db4d2:         produceMessagesToConsumeMultipleDataFiles(4);
1:73db4d2: 
1:73db4d2:         corruptBatchCheckSumSplash(1);
1:73db4d2: 
1:73db4d2:         restartBroker(false);
1:73db4d2: 
1:73db4d2:         assertEquals("unnoticed", 4, broker.getAdminView().getTotalMessageCount());
1:73db4d2:         assertEquals("Drain", 0, drainQueue(4));
1:73db4d2: 
1:73db4d2:         // force recover index and loose one message
1:73db4d2:         restartBroker(false, true);
1:73db4d2: 
1:73db4d2:         assertEquals("missing one index recreation", 3, broker.getAdminView().getTotalMessageCount());
1:73db4d2:         assertEquals("Drain", 3, drainQueue(4));
1:73db4d2:     }
1:73db4d2: 
1:73db4d2:     @Test
1:4a82118:     public void testRecoverIndex() throws Exception {
1:4a82118:         startBroker();
1:4a82118: 
1:4a82118:         final int numToSend = 4;
1:4a82118:         produceMessagesToConsumeMultipleDataFiles(numToSend);
1:4a82118: 
1:4a82118:         // force journal replay by whacking the index
1:4a82118:         restartBroker(false, true);
1:4a82118: 
1:4a82118:         assertEquals("Drain", numToSend, drainQueue(numToSend));
1:4a82118:     }
1:8c218ee: 
1:8c218ee:     @Test
1:8c218ee:     public void testRecoverIndexWithSmallBatch() throws Exception {
1:8c218ee:         journalMaxBatchSize = 2 * 1024;
1:8c218ee:         startBroker();
1:8c218ee: 
1:8c218ee:         final int numToSend = 4;
1:8c218ee:         produceMessagesToConsumeMultipleDataFiles(numToSend);
1:8c218ee: 
1:8c218ee:         // force journal replay by whacking the index
1:8c218ee:         restartBroker(false, true);
1:8c218ee: 
1:8c218ee:         assertEquals("Drain", numToSend, drainQueue(numToSend));
1:8c218ee:     }
1:822e2be: 
1:4a82118: 
1:822e2be:     @Test
1:822e2be:     public void testRecoveryAfterProducerAuditLocationCorrupt() throws Exception {
1:822e2be:         doTestRecoveryAfterLocationCorrupt(false);
1:822e2be:     }
1:822e2be: 
1:822e2be:     @Test
1:822e2be:     public void testRecoveryAfterAckMapLocationCorrupt() throws Exception {
1:822e2be:         doTestRecoveryAfterLocationCorrupt(true);
1:822e2be:     }
1:822e2be: 
1:822e2be:     private void doTestRecoveryAfterLocationCorrupt(boolean aOrB) throws Exception {
1:822e2be:         startBroker();
1:822e2be: 
1:822e2be:         produceMessagesToConsumeMultipleDataFiles(50);
1:822e2be: 
1:822e2be:         int numFiles = getNumberOfJournalFiles();
1:822e2be: 
1:822e2be:         assertTrue("more than x files: " + numFiles, numFiles > 4);
1:822e2be: 
1:822e2be:         KahaDBStore store = ((KahaDBPersistenceAdapter) broker.getPersistenceAdapter()).getStore();
1:822e2be:         store.checkpointCleanup(true);
1:822e2be:         Location toCorrupt = aOrB ? store.getMetadata().ackMessageFileMapLocation : store.getMetadata().producerSequenceIdTrackerLocation;
1:822e2be:         corruptLocation(toCorrupt);
1:822e2be: 
1:822e2be:         restartBroker(false, false);
1:822e2be: 
1:822e2be:         assertEquals("missing no message", 50, broker.getAdminView().getTotalMessageCount());
1:822e2be:         assertEquals("Drain", 50, drainQueue(50));
1:822e2be:     }
1:822e2be: 
1:822e2be:     private void corruptLocation(Location toCorrupt) throws IOException {
1:822e2be: 
1:822e2be:         DataFile dataFile = ((KahaDBPersistenceAdapter) broker.getPersistenceAdapter()).getStore().getJournal().getFileMap().get(new Integer(toCorrupt.getDataFileId()));
1:822e2be: 
1:822e2be:         RecoverableRandomAccessFile randomAccessFile = dataFile.openRandomAccessFile();
1:822e2be: 
1:822e2be:         randomAccessFile.seek(toCorrupt.getOffset());
1:822e2be:         randomAccessFile.writeInt(3);
1:822e2be:         dataFile.closeRandomAccessFile(randomAccessFile);
1:822e2be:     }
1:822e2be: 
1:73db4d2:     private void corruptBatchCheckSumSplash(int id) throws Exception{
2:73db4d2:         Collection<DataFile> files =
2:73db4d2:                 ((KahaDBPersistenceAdapter) broker.getPersistenceAdapter()).getStore().getJournal().getFileMap().values();
1:73db4d2:         DataFile dataFile = (DataFile) files.toArray()[0];
1:73db4d2:         RecoverableRandomAccessFile randomAccessFile = dataFile.openRandomAccessFile();
1:4a82118: 
1:73db4d2:         ArrayList<Integer> batchPositions = findBatch(randomAccessFile, Integer.MAX_VALUE);
1:73db4d2:         LOG.info("Batch positions: " + batchPositions);
1:73db4d2:         int pos = batchPositions.get(1);
1:73db4d2:         LOG.info("corrupting checksum and size (to push it past eof) of batch record at:" + id + "-" + pos);
1:73db4d2:         randomAccessFile.seek(pos + Journal.BATCH_CONTROL_RECORD_HEADER.length + 4);
1:73db4d2:         // whack the batch control record checksum
1:73db4d2:         randomAccessFile.writeLong(0l);
1:73db4d2: 
1:73db4d2:         // mod the data size in the location header so reading blows
1:73db4d2:         randomAccessFile.seek(pos + Journal.BATCH_CONTROL_RECORD_SIZE);
1:73db4d2:         int size = randomAccessFile.readInt();
1:73db4d2:         byte type = randomAccessFile.readByte();
1:73db4d2: 
1:73db4d2:         LOG.info("Read: size:" + size + ", type:" + type);
1:73db4d2: 
1:73db4d2:         randomAccessFile.seek(pos + Journal.BATCH_CONTROL_RECORD_SIZE);
1:73db4d2:         size -= 1;
1:73db4d2:         LOG.info("rewrite incorrect location size @:" + (pos + Journal.BATCH_CONTROL_RECORD_SIZE) + " as: " + size);
1:73db4d2:         randomAccessFile.writeInt(size);
1:507d40a:         corruptOrderIndex(id, size);
1:73db4d2: 
1:73db4d2:         randomAccessFile.getChannel().force(true);
1:62bdbb0:         dataFile.closeRandomAccessFile(randomAccessFile);
1:73db4d2:     }
1:73db4d2: 
1:73db4d2:     private void corruptBatchEndEof(int id) throws Exception{
1:73db4d2:         Collection<DataFile> files =
1:73db4d2:                 ((KahaDBPersistenceAdapter) broker.getPersistenceAdapter()).getStore().getJournal().getFileMap().values();
1:73db4d2:         DataFile dataFile = (DataFile) files.toArray()[id];
1:73db4d2:         RecoverableRandomAccessFile randomAccessFile = dataFile.openRandomAccessFile();
1:73db4d2: 
1:73db4d2:         ArrayList<Integer> batchPositions = findBatch(randomAccessFile, Integer.MAX_VALUE);
1:73db4d2:         int pos = batchPositions.get(batchPositions.size() - 3);
1:73db4d2:         LOG.info("corrupting checksum and size (to push it past eof) of batch record at:" + id + "-" + pos);
1:73db4d2:         randomAccessFile.seek(pos + Journal.BATCH_CONTROL_RECORD_HEADER.length);
1:73db4d2:         randomAccessFile.writeInt(31 * 1024 * 1024);
1:73db4d2:         randomAccessFile.writeLong(0l);
1:73db4d2:         randomAccessFile.getChannel().force(true);
1:73db4d2:     }
1:73db4d2: 
1:507d40a:     private void corruptOrderIndex(final int num, final int size) throws Exception {
1:507d40a:         //This is because of AMQ-6097, now that the MessageOrderIndex stores the size in the Location,
1:507d40a:         //we need to corrupt that value as well
1:507d40a:         final KahaDBStore kahaDbStore = (KahaDBStore) ((KahaDBPersistenceAdapter) broker.getPersistenceAdapter()).getStore();
1:507d40a:         kahaDbStore.indexLock.writeLock().lock();
1:507d40a:         try {
1:507d40a:             kahaDbStore.pageFile.tx().execute(new Transaction.Closure<IOException>() {
1:507d40a:                 @Override
1:507d40a:                 public void execute(Transaction tx) throws IOException {
1:507d40a:                     StoredDestination sd = kahaDbStore.getStoredDestination(kahaDbStore.convert(
1:507d40a:                             (ActiveMQQueue)destination), tx);
1:507d40a:                     int i = 1;
1:507d40a:                     for (Iterator<Entry<Long, MessageKeys>> iterator = sd.orderIndex.iterator(tx); iterator.hasNext();) {
1:507d40a:                         Entry<Long, MessageKeys> entry = iterator.next();
1:507d40a:                         if (i == num) {
1:507d40a:                             //change the size value to the wrong size
1:507d40a:                             sd.orderIndex.get(tx, entry.getKey());
1:507d40a:                             MessageKeys messageKeys = entry.getValue();
1:507d40a:                             messageKeys.location.setSize(size);
1:507d40a:                             sd.orderIndex.put(tx, sd.orderIndex.lastGetPriority(), entry.getKey(), messageKeys);
1:507d40a:                             break;
1:507d40a:                         }
1:507d40a:                         i++;
1:507d40a:                     }
1:507d40a:                 }
1:507d40a:             });
1:507d40a:         } finally {
1:507d40a:             kahaDbStore.indexLock.writeLock().unlock();
1:507d40a:         }
1:507d40a:     }
1:73db4d2: 
1:73db4d2:     private ArrayList<Integer> findBatch(RecoverableRandomAccessFile randomAccessFile, int where) throws IOException {
1:73db4d2:         final ArrayList<Integer> batchPositions = new ArrayList<Integer>();
1:73db4d2:         final ByteSequence header = new ByteSequence(Journal.BATCH_CONTROL_RECORD_HEADER);
1:73db4d2:         byte data[] = new byte[1024 * 20];
1:73db4d2: 
1:73db4d2:         ByteSequence bs = new ByteSequence(data, 0, randomAccessFile.read(data, 0, data.length));
1:73db4d2: 
1:73db4d2:         int pos = 0;
1:73db4d2:         for (int i = 0; i < where; i++) {
1:73db4d2:             int found = bs.indexOf(header, pos);
1:73db4d2:             if (found == -1) {
1:73db4d2:                 break;
1:73db4d2:             }
1:73db4d2:             batchPositions.add(found);
1:73db4d2:             pos = found + Journal.BATCH_CONTROL_RECORD_HEADER.length - 1;
1:73db4d2:         }
1:73db4d2: 
1:73db4d2:         return batchPositions;
1:73db4d2:     }
1:73db4d2: 
1:73db4d2:     private int getNumberOfJournalFiles() throws IOException {
1:13044de:         Collection<DataFile> files = ((KahaDBPersistenceAdapter) broker.getPersistenceAdapter()).getStore().getJournal().getFileMap().values();
1:73db4d2:         int reality = 0;
1:73db4d2:         for (DataFile file : files) {
1:73db4d2:             if (file != null) {
1:73db4d2:                 reality++;
1:73db4d2:             }
1:73db4d2:         }
1:73db4d2:         return reality;
1:73db4d2:     }
1:73db4d2: 
1:73db4d2:     private int produceMessages(Destination destination, int numToSend) throws Exception {
1:73db4d2:         int sent = 0;
1:13044de:         Connection connection = new ActiveMQConnectionFactory(broker.getTransportConnectors().get(0).getConnectUri()).createConnection();
1:73db4d2:         connection.start();
1:73db4d2:         try {
1:73db4d2:             Session session = connection.createSession(false, Session.AUTO_ACKNOWLEDGE);
1:73db4d2:             MessageProducer producer = session.createProducer(destination);
1:73db4d2:             for (int i = 0; i < numToSend; i++) {
1:73db4d2:                 producer.send(createMessage(session, i));
1:73db4d2:                 sent++;
1:73db4d2:             }
1:73db4d2:         } finally {
1:6b8e743:             connection.close();
1:73db4d2:         }
1:73db4d2: 
1:73db4d2:         return sent;
1:73db4d2:     }
1:73db4d2: 
1:73db4d2:     private int produceMessagesToConsumeMultipleDataFiles(int numToSend) throws Exception {
1:73db4d2:         return produceMessages(destination, numToSend);
1:73db4d2:     }
1:73db4d2: 
1:73db4d2:     private Message createMessage(Session session, int i) throws Exception {
1:73db4d2:         return session.createTextMessage(payload + "::" + i);
1:73db4d2:     }
1:73db4d2: 
1:73db4d2:     private int drainQueue(int max) throws Exception {
1:822e2be:         return drain(cf, destination, max);
1:822e2be:     }
1:822e2be: 
1:822e2be:     public static int drain(ConnectionFactory cf, Destination destination, int max) throws Exception {
1:73db4d2:         Connection connection = cf.createConnection();
1:73db4d2:         connection.start();
1:73db4d2:         Session session = connection.createSession(false, Session.AUTO_ACKNOWLEDGE);
1:822e2be:         MessageConsumer consumer = null;
1:73db4d2:         int count = 0;
1:6b8e743:         try {
1:822e2be:             consumer = session.createConsumer(destination);
1:822e2be:             while (count < max && consumer.receive(4000) != null) {
1:6b8e743:                 count++;
1:6b8e743:             }
1:6b8e743:         } catch (JMSException ok) {
1:6b8e743:         } finally {
1:822e2be:             if (consumer != null) {
1:822e2be:                 try {
1:822e2be:                     consumer.close();
1:822e2be:                 } catch (JMSException ok) {}
1:822e2be:             }
1:822e2be:             try {
1:822e2be:                 connection.close();
1:822e2be:             } catch (JMSException ok) {}
1:73db4d2:         }
1:73db4d2:         return count;
1:73db4d2:     }
1:73db4d2: }
============================================================================
author:gtully
-------------------------------------------------------------------------------
commit:8c3ef6c
/////////////////////////////////////////////////////////////////////////
1: import java.util.concurrent.atomic.AtomicBoolean;
/////////////////////////////////////////////////////////////////////////
1: import org.apache.activemq.util.DefaultTestAppender;
1: import org.apache.log4j.Level;
1: import org.apache.log4j.spi.LoggingEvent;
/////////////////////////////////////////////////////////////////////////
1:     public void testRecoveryAfterCorruptionMetadataLocation() throws Exception {
1:         startBroker();
1: 
1:         produceMessagesToConsumeMultipleDataFiles(50);
1: 
1:         int numFiles = getNumberOfJournalFiles();
1: 
1:         assertTrue("more than x files: " + numFiles, numFiles > 2);
1: 
1:         broker.getPersistenceAdapter().checkpoint(true);
1:         Location location = ((KahaDBPersistenceAdapter) broker.getPersistenceAdapter()).getStore().getMetadata().producerSequenceIdTrackerLocation;
1: 
1:         DataFile dataFile = ((KahaDBPersistenceAdapter) broker.getPersistenceAdapter()).getStore().getJournal().getFileMap().get(Integer.valueOf(location.getDataFileId()));
1:         RecoverableRandomAccessFile randomAccessFile = dataFile.openRandomAccessFile();
1:         randomAccessFile.seek(location.getOffset());
1:         randomAccessFile.writeInt(Integer.MAX_VALUE);
1:         randomAccessFile.getChannel().force(true);
1: 
1:         ((KahaDBPersistenceAdapter) broker.getPersistenceAdapter()).getStore().getJournal().close();
1:         try {
1:             broker.stop();
1:             broker.waitUntilStopped();
1:         } catch (Exception expected) {
1:         } finally {
1:             broker = null;
1:         }
1: 
1:         AtomicBoolean trappedExpectedLogMessage = new AtomicBoolean(false);
1:         DefaultTestAppender appender = new DefaultTestAppender() {
1:             @Override
1:             public void doAppend(LoggingEvent event) {
1:                 if (event.getLevel() == Level.WARN
1:                         && event.getRenderedMessage().contains("Cannot recover message audit")
1:                         && event.getThrowableInformation().getThrowable().getLocalizedMessage().contains("Invalid location size")) {
1:                     trappedExpectedLogMessage.set(true);
1:                 }
1:             }
1:         };
1:         org.apache.log4j.Logger.getRootLogger().addAppender(appender);
1: 
1: 
1:         try {
1:             restartBroker(false);
1:         } finally {
1:             org.apache.log4j.Logger.getRootLogger().removeAppender(appender);
1:         }
1: 
1:         assertEquals("no missing message", 50, broker.getAdminView().getTotalMessageCount());
1:         assertTrue("Did replay records on invalid location size", trappedExpectedLogMessage.get());
1:     }
1: 
1:     @Test
commit:8c218ee
/////////////////////////////////////////////////////////////////////////
1:     private int journalMaxBatchSize;
1:     File brokerDataDir = null;
/////////////////////////////////////////////////////////////////////////
1:             File indexToDelete = new File(brokerDataDir, "db.data");
/////////////////////////////////////////////////////////////////////////
1:         brokerDataDir = broker.getPersistenceAdapter().getDirectory();
/////////////////////////////////////////////////////////////////////////
1:         adapter.setJournalMaxWriteBatchSize(journalMaxBatchSize);
1: 
/////////////////////////////////////////////////////////////////////////
1:         journalMaxBatchSize = Journal.DEFAULT_MAX_WRITE_BATCH_SIZE;
/////////////////////////////////////////////////////////////////////////
1:     @Test
1:     public void testRecoverIndexWithSmallBatch() throws Exception {
1:         journalMaxBatchSize = 2 * 1024;
1:         startBroker();
1: 
1:         final int numToSend = 4;
1:         produceMessagesToConsumeMultipleDataFiles(numToSend);
1: 
1:         // force journal replay by whacking the index
1:         restartBroker(false, true);
1: 
1:         assertEquals("Drain", numToSend, drainQueue(numToSend));
1:     }
1: 
commit:822e2be
/////////////////////////////////////////////////////////////////////////
1: import javax.jms.ConnectionFactory;
/////////////////////////////////////////////////////////////////////////
1: import org.apache.activemq.store.kahadb.disk.journal.Location;
/////////////////////////////////////////////////////////////////////////
1: 
1:     @Test
1:     public void testRecoveryAfterProducerAuditLocationCorrupt() throws Exception {
1:         doTestRecoveryAfterLocationCorrupt(false);
1:     }
1: 
1:     @Test
1:     public void testRecoveryAfterAckMapLocationCorrupt() throws Exception {
1:         doTestRecoveryAfterLocationCorrupt(true);
1:     }
1: 
1:     private void doTestRecoveryAfterLocationCorrupt(boolean aOrB) throws Exception {
1:         startBroker();
1: 
1:         produceMessagesToConsumeMultipleDataFiles(50);
1: 
1:         int numFiles = getNumberOfJournalFiles();
1: 
1:         assertTrue("more than x files: " + numFiles, numFiles > 4);
1: 
1:         KahaDBStore store = ((KahaDBPersistenceAdapter) broker.getPersistenceAdapter()).getStore();
1:         store.checkpointCleanup(true);
1:         Location toCorrupt = aOrB ? store.getMetadata().ackMessageFileMapLocation : store.getMetadata().producerSequenceIdTrackerLocation;
1:         corruptLocation(toCorrupt);
1: 
1:         restartBroker(false, false);
1: 
1:         assertEquals("missing no message", 50, broker.getAdminView().getTotalMessageCount());
1:         assertEquals("Drain", 50, drainQueue(50));
1:     }
1: 
1:     private void corruptLocation(Location toCorrupt) throws IOException {
1: 
1:         DataFile dataFile = ((KahaDBPersistenceAdapter) broker.getPersistenceAdapter()).getStore().getJournal().getFileMap().get(new Integer(toCorrupt.getDataFileId()));
1: 
1:         RecoverableRandomAccessFile randomAccessFile = dataFile.openRandomAccessFile();
1: 
1:         randomAccessFile.seek(toCorrupt.getOffset());
1:         randomAccessFile.writeInt(3);
1:         dataFile.closeRandomAccessFile(randomAccessFile);
1:     }
1: 
/////////////////////////////////////////////////////////////////////////
1:         return drain(cf, destination, max);
1:     }
1: 
1:     public static int drain(ConnectionFactory cf, Destination destination, int max) throws Exception {
1:         MessageConsumer consumer = null;
1:             consumer = session.createConsumer(destination);
1:             while (count < max && consumer.receive(4000) != null) {
1:             if (consumer != null) {
1:                 try {
1:                     consumer.close();
1:                 } catch (JMSException ok) {}
1:             }
1:             try {
1:                 connection.close();
1:             } catch (JMSException ok) {}
commit:6b8e743
/////////////////////////////////////////////////////////////////////////
1: import javax.jms.JMSException;
/////////////////////////////////////////////////////////////////////////
1:         try {
0:             while (count < max && consumer.receive(5000) != null) {
1:                 count++;
1:             }
1:         } catch (JMSException ok) {
1:         } finally {
0:             consumer.close();
1:             connection.close();
commit:62bdbb0
/////////////////////////////////////////////////////////////////////////
1:         adapter.setPreallocationStrategy(Journal.PreallocationStrategy.ZEROS.name());
1:         adapter.setPreallocationScope(Journal.PreallocationScope.ENTIRE_JOURNAL_ASYNC.name());
/////////////////////////////////////////////////////////////////////////
1:         dataFile.closeRandomAccessFile(randomAccessFile);
commit:5db5f3e
/////////////////////////////////////////////////////////////////////////
1: import static org.junit.Assert.fail;
1: 
/////////////////////////////////////////////////////////////////////////
1: import org.junit.Before;
/////////////////////////////////////////////////////////////////////////
1:     private boolean ignoreMissingJournalFiles = false;
/////////////////////////////////////////////////////////////////////////
1:         adapter.setIgnoreMissingJournalfiles(ignoreMissingJournalFiles);
/////////////////////////////////////////////////////////////////////////
1:     @Before
1:     public void reset() throws Exception {
1:         ignoreMissingJournalFiles = true;
1:     }
1: 
1:     @Test
1:     public void testNoRestartOnCorruptJournal() throws Exception {
1:         ignoreMissingJournalFiles = false;
1: 
1:         startBroker();
1: 
1:         produceMessagesToConsumeMultipleDataFiles(50);
1: 
1:         int numFiles = getNumberOfJournalFiles();
1: 
1:         assertTrue("more than x files: " + numFiles, numFiles > 2);
1: 
1:         corruptBatchEndEof(3);
1: 
1:         try {
1:             restartBroker(true);
1:             fail("Expect failure to start with corrupt journal");
1:         } catch (Exception expected) {
1:         }
1:     }
1: 
commit:4a82118
/////////////////////////////////////////////////////////////////////////
0:         adapter.setPreallocationStrategy("zeros");
0:         adapter.setPreallocationScope("entire_journal");
1: 
/////////////////////////////////////////////////////////////////////////
1:     @Test
1:     public void testRecoverIndex() throws Exception {
1:         startBroker();
1: 
1:         final int numToSend = 4;
1:         produceMessagesToConsumeMultipleDataFiles(numToSend);
1: 
1:         // force journal replay by whacking the index
1:         restartBroker(false, true);
1: 
1:         assertEquals("Drain", numToSend, drainQueue(numToSend));
1: 
1:     }
1: 
commit:73db4d2
/////////////////////////////////////////////////////////////////////////
1: /**
1:  * Licensed to the Apache Software Foundation (ASF) under one or more
1:  * contributor license agreements.  See the NOTICE file distributed with
1:  * this work for additional information regarding copyright ownership.
1:  * The ASF licenses this file to You under the Apache License, Version 2.0
1:  * (the "License"); you may not use this file except in compliance with
1:  * the License.  You may obtain a copy of the License at
1:  *
1:  *      http://www.apache.org/licenses/LICENSE-2.0
1:  *
1:  * Unless required by applicable law or agreed to in writing, software
1:  * distributed under the License is distributed on an "AS IS" BASIS,
1:  * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
1:  * See the License for the specific language governing permissions and
1:  * limitations under the License.
1:  */
1: package org.apache.activemq.store.kahadb;
1: 
1: import java.io.File;
1: import java.io.IOException;
1: import java.util.ArrayList;
1: import java.util.Collection;
1: import javax.jms.Connection;
1: import javax.jms.Destination;
1: import javax.jms.Message;
1: import javax.jms.MessageConsumer;
1: import javax.jms.MessageProducer;
1: import javax.jms.Session;
1: import org.apache.activemq.ActiveMQConnectionFactory;
1: import org.apache.activemq.broker.BrokerService;
1: import org.apache.activemq.command.ActiveMQQueue;
1: import org.apache.activemq.store.kahadb.disk.journal.DataFile;
1: import org.apache.activemq.store.kahadb.disk.journal.Journal;
1: import org.apache.activemq.util.ByteSequence;
1: import org.apache.activemq.util.IOHelper;
1: import org.apache.activemq.util.RecoverableRandomAccessFile;
1: import org.junit.After;
1: import org.junit.Test;
1: import org.slf4j.Logger;
1: import org.slf4j.LoggerFactory;
1: 
1: 
1: import static org.junit.Assert.assertEquals;
1: import static org.junit.Assert.assertTrue;
1: 
1: 
1: public class JournalCorruptionEofIndexRecoveryTest {
1: 
1:     private static final Logger LOG = LoggerFactory.getLogger(JournalCorruptionEofIndexRecoveryTest.class);
1: 
0:     ActiveMQConnectionFactory cf = null;
0:     BrokerService broker = null;
1:     private final Destination destination = new ActiveMQQueue("Test");
1:     private String connectionUri;
1:     private KahaDBPersistenceAdapter adapter;
1: 
1: 
1:     protected void startBroker() throws Exception {
1:         doStartBroker(true, false);
1:     }
1: 
1: 
1:     protected void restartBroker(boolean whackIndex) throws Exception {
1:         restartBroker(whackIndex, false);
1:     }
1: 
1:     protected void restartBroker(boolean whackIndex, boolean forceRecoverIndex) throws Exception {
0:         File dataDir = broker.getPersistenceAdapter().getDirectory();
1:         if (broker != null) {
1:             broker.stop();
1:             broker.waitUntilStopped();
1:         }
1: 
1:         if (whackIndex) {
0:             File indexToDelete = new File(dataDir, "db.data");
1:             LOG.info("Whacking index: " + indexToDelete);
1:             indexToDelete.delete();
1:         }
1: 
1:         doStartBroker(false, forceRecoverIndex);
1:     }
1: 
1: 
1:     private void doStartBroker(boolean delete, boolean forceRecoverIndex) throws Exception {
1:         broker = new BrokerService();
1:         if (delete) {
1:             IOHelper.deleteChildren(broker.getPersistenceAdapter().getDirectory());
1:             IOHelper.delete(broker.getPersistenceAdapter().getDirectory());
1:         }
1: 
1:         broker.setPersistent(true);
1:         broker.setUseJmx(true);
1:         broker.addConnector("tcp://localhost:0");
1: 
1:         configurePersistence(broker, forceRecoverIndex);
1: 
1:         connectionUri = "vm://localhost?create=false";
1:         cf = new ActiveMQConnectionFactory(connectionUri);
1: 
1:         broker.start();
1:         LOG.info("Starting broker..");
1:     }
1: 
1:     protected void configurePersistence(BrokerService brokerService, boolean forceRecoverIndex) throws Exception {
1:         adapter = (KahaDBPersistenceAdapter) brokerService.getPersistenceAdapter();
1: 
1:         adapter.setForceRecoverIndex(forceRecoverIndex);
1: 
1:         // ensure there are a bunch of data files but multiple entries in each
1:         adapter.setJournalMaxFileLength(1024 * 20);
1: 
1:         // speed up the test case, checkpoint an cleanup early and often
1:         adapter.setCheckpointInterval(5000);
1:         adapter.setCleanupInterval(5000);
1: 
1:         adapter.setCheckForCorruptJournalFiles(true);
0:         adapter.setIgnoreMissingJournalfiles(true);
1: 
1:     }
1: 
1:     @After
1:     public void tearDown() throws Exception {
1:         if (broker != null) {
1:             broker.stop();
1:             broker.waitUntilStopped();
1:         }
1:     }
1: 
1: 
1:     @Test
1:     public void testRecoveryAfterCorruptionEof() throws Exception {
1:         startBroker();
1: 
1:         produceMessagesToConsumeMultipleDataFiles(50);
1: 
1:         int numFiles = getNumberOfJournalFiles();
1: 
1:         assertTrue("more than x files: " + numFiles, numFiles > 2);
1: 
1:         corruptBatchEndEof(3);
1: 
1:         restartBroker(false);
1: 
1:         assertEquals("missing one message", 49, broker.getAdminView().getTotalMessageCount());
1: 
1:         assertEquals("Drain", 49, drainQueue(49));
1: 
1:     }
1: 
1:     @Test
1:     public void testRecoveryAfterCorruptionCheckSum() throws Exception {
1:         startBroker();
1: 
1:         produceMessagesToConsumeMultipleDataFiles(4);
1: 
1:         corruptBatchCheckSumSplash(1);
1: 
1:         restartBroker(true);
1: 
1:         assertEquals("missing one message", 3, broker.getAdminView().getTotalMessageCount());
1: 
1:         assertEquals("Drain", 3, drainQueue(4));
1: 
1:     }
1: 
1:     @Test
1:     public void testRecoveryAfterCorruptionCheckSumExistingIndex() throws Exception {
1:         startBroker();
1: 
1:         produceMessagesToConsumeMultipleDataFiles(4);
1: 
1:         corruptBatchCheckSumSplash(1);
1: 
1:         restartBroker(false);
1: 
1:         assertEquals("unnoticed", 4, broker.getAdminView().getTotalMessageCount());
1: 
1:         assertEquals("Drain", 0, drainQueue(4));
1: 
1:         // force recover index and loose one message
1:         restartBroker(false, true);
1: 
1:         assertEquals("missing one index recreation", 3, broker.getAdminView().getTotalMessageCount());
1: 
1:         assertEquals("Drain", 3, drainQueue(4));
1: 
1:     }
1: 
1:     private void corruptBatchCheckSumSplash(int id) throws Exception{
1:         Collection<DataFile> files =
1:                 ((KahaDBPersistenceAdapter) broker.getPersistenceAdapter()).getStore().getJournal().getFileMap().values();
1:         DataFile dataFile = (DataFile) files.toArray()[0];
1:         RecoverableRandomAccessFile randomAccessFile = dataFile.openRandomAccessFile();
1: 
1:         ArrayList<Integer> batchPositions = findBatch(randomAccessFile, Integer.MAX_VALUE);
1:         LOG.info("Batch positions: " + batchPositions);
1:         int pos = batchPositions.get(1);
1:         LOG.info("corrupting checksum and size (to push it past eof) of batch record at:" + id + "-" + pos);
1:         randomAccessFile.seek(pos + Journal.BATCH_CONTROL_RECORD_HEADER.length + 4);
1:         // whack the batch control record checksum
1:         randomAccessFile.writeLong(0l);
1: 
1:         // mod the data size in the location header so reading blows
1:         randomAccessFile.seek(pos + Journal.BATCH_CONTROL_RECORD_SIZE);
1:         int size = randomAccessFile.readInt();
1:         byte type = randomAccessFile.readByte();
1: 
1:         LOG.info("Read: size:" + size + ", type:" + type);
1: 
1:         randomAccessFile.seek(pos + Journal.BATCH_CONTROL_RECORD_SIZE);
1:         size -= 1;
1:         LOG.info("rewrite incorrect location size @:" + (pos + Journal.BATCH_CONTROL_RECORD_SIZE) + " as: " + size);
1:         randomAccessFile.writeInt(size);
1: 
1:         randomAccessFile.getChannel().force(true);
1: 
1:     }
1: 
1:     private void corruptBatchEndEof(int id) throws Exception{
1:         Collection<DataFile> files =
1:                 ((KahaDBPersistenceAdapter) broker.getPersistenceAdapter()).getStore().getJournal().getFileMap().values();
1:         DataFile dataFile = (DataFile) files.toArray()[id];
1:         RecoverableRandomAccessFile randomAccessFile = dataFile.openRandomAccessFile();
1: 
1:         ArrayList<Integer> batchPositions = findBatch(randomAccessFile, Integer.MAX_VALUE);
1:         int pos = batchPositions.get(batchPositions.size() - 3);
1:         LOG.info("corrupting checksum and size (to push it past eof) of batch record at:" + id + "-" + pos);
1:         randomAccessFile.seek(pos + Journal.BATCH_CONTROL_RECORD_HEADER.length);
1:         randomAccessFile.writeInt(31 * 1024 * 1024);
1:         randomAccessFile.writeLong(0l);
1:         randomAccessFile.getChannel().force(true);
1: 
1:     }
1: 
1:     private ArrayList<Integer> findBatch(RecoverableRandomAccessFile randomAccessFile, int where) throws IOException {
1:         final ArrayList<Integer> batchPositions = new ArrayList<Integer>();
1:         final ByteSequence header = new ByteSequence(Journal.BATCH_CONTROL_RECORD_HEADER);
1:         byte data[] = new byte[1024 * 20];
1: 
1:         ByteSequence bs = new ByteSequence(data, 0, randomAccessFile.read(data, 0, data.length));
1: 
1:         int pos = 0;
1:         for (int i = 0; i < where; i++) {
1:             int found = bs.indexOf(header, pos);
1:             if (found == -1) {
1:                 break;
1:             }
1:             batchPositions.add(found);
1:             pos = found + Journal.BATCH_CONTROL_RECORD_HEADER.length - 1;
1:         }
1: 
1:         return batchPositions;
1:     }
1: 
1: 
1:     private int getNumberOfJournalFiles() throws IOException {
1: 
1:         Collection<DataFile> files =
1:                 ((KahaDBPersistenceAdapter) broker.getPersistenceAdapter()).getStore().getJournal().getFileMap().values();
1:         int reality = 0;
1:         for (DataFile file : files) {
1:             if (file != null) {
1:                 reality++;
1:             }
1:         }
1:         return reality;
1:     }
1: 
1: 
1:     private int produceMessages(Destination destination, int numToSend) throws Exception {
1:         int sent = 0;
0:         Connection connection = new ActiveMQConnectionFactory(
0:                 broker.getTransportConnectors().get(0).getConnectUri()).createConnection();
1:         connection.start();
1:         try {
1:             Session session = connection.createSession(false, Session.AUTO_ACKNOWLEDGE);
1:             MessageProducer producer = session.createProducer(destination);
1:             for (int i = 0; i < numToSend; i++) {
1:                 producer.send(createMessage(session, i));
1:                 sent++;
1:             }
1:         } finally {
0:             connection.close();
1:         }
1: 
1:         return sent;
1:     }
1: 
1:     private int produceMessagesToConsumeMultipleDataFiles(int numToSend) throws Exception {
1:         return produceMessages(destination, numToSend);
1:     }
1: 
0:     final String payload = new String(new byte[1024]);
1: 
1:     private Message createMessage(Session session, int i) throws Exception {
1:         return session.createTextMessage(payload + "::" + i);
1:     }
1: 
1:     private int drainQueue(int max) throws Exception {
1:         Connection connection = cf.createConnection();
1:         connection.start();
1:         Session session = connection.createSession(false, Session.AUTO_ACKNOWLEDGE);
0:         MessageConsumer consumer = session.createConsumer(destination);
1:         int count = 0;
0:         while (count < max && consumer.receive(5000) != null) {
0:             count++;
1:         }
0:         consumer.close();
0:         connection.close();
1:         return count;
1:     }
1: }
author:Christopher L. Shannon (cshannon)
-------------------------------------------------------------------------------
commit:507d40a
/////////////////////////////////////////////////////////////////////////
1: import java.util.Iterator;
1: import java.util.Map.Entry;
/////////////////////////////////////////////////////////////////////////
1: import org.apache.activemq.store.kahadb.MessageDatabase.MessageKeys;
1: import org.apache.activemq.store.kahadb.MessageDatabase.StoredDestination;
1: import org.apache.activemq.store.kahadb.disk.page.Transaction;
/////////////////////////////////////////////////////////////////////////
1:         corruptOrderIndex(id, size);
/////////////////////////////////////////////////////////////////////////
1:     private void corruptOrderIndex(final int num, final int size) throws Exception {
1:         //This is because of AMQ-6097, now that the MessageOrderIndex stores the size in the Location,
1:         //we need to corrupt that value as well
1:         final KahaDBStore kahaDbStore = (KahaDBStore) ((KahaDBPersistenceAdapter) broker.getPersistenceAdapter()).getStore();
1:         kahaDbStore.indexLock.writeLock().lock();
1:         try {
1:             kahaDbStore.pageFile.tx().execute(new Transaction.Closure<IOException>() {
1:                 @Override
1:                 public void execute(Transaction tx) throws IOException {
1:                     StoredDestination sd = kahaDbStore.getStoredDestination(kahaDbStore.convert(
1:                             (ActiveMQQueue)destination), tx);
1:                     int i = 1;
1:                     for (Iterator<Entry<Long, MessageKeys>> iterator = sd.orderIndex.iterator(tx); iterator.hasNext();) {
1:                         Entry<Long, MessageKeys> entry = iterator.next();
1:                         if (i == num) {
1:                             //change the size value to the wrong size
1:                             sd.orderIndex.get(tx, entry.getKey());
1:                             MessageKeys messageKeys = entry.getValue();
1:                             messageKeys.location.setSize(size);
1:                             sd.orderIndex.put(tx, sd.orderIndex.lastGetPriority(), entry.getKey(), messageKeys);
1:                             break;
1:                         }
1:                         i++;
1:                     }
1:                 }
1:             });
1:         } finally {
1:             kahaDbStore.indexLock.writeLock().unlock();
1:         }
1:     }
0: 
author:Timothy Bish
-------------------------------------------------------------------------------
commit:13044de
/////////////////////////////////////////////////////////////////////////
0: import static org.junit.Assert.assertEquals;
0: import static org.junit.Assert.assertTrue;
0: 
0: 
0: 
/////////////////////////////////////////////////////////////////////////
1:     private ActiveMQConnectionFactory cf = null;
1:     private BrokerService broker = null;
0:     private final Destination destination = new ActiveMQQueue("Test");
1:     private final String KAHADB_DIRECTORY = "target/activemq-data/";
1:     private final String payload = new String(new byte[1024]);
/////////////////////////////////////////////////////////////////////////
1:         broker.setDataDirectory(KAHADB_DIRECTORY);
0: 
/////////////////////////////////////////////////////////////////////////
/////////////////////////////////////////////////////////////////////////
/////////////////////////////////////////////////////////////////////////
/////////////////////////////////////////////////////////////////////////
/////////////////////////////////////////////////////////////////////////
/////////////////////////////////////////////////////////////////////////
/////////////////////////////////////////////////////////////////////////
/////////////////////////////////////////////////////////////////////////
/////////////////////////////////////////////////////////////////////////
1:         Collection<DataFile> files = ((KahaDBPersistenceAdapter) broker.getPersistenceAdapter()).getStore().getJournal().getFileMap().values();
/////////////////////////////////////////////////////////////////////////
1:         Connection connection = new ActiveMQConnectionFactory(broker.getTransportConnectors().get(0).getConnectUri()).createConnection();
/////////////////////////////////////////////////////////////////////////
============================================================================