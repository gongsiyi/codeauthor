1:ca5e41b: /**
1:ca5e41b:  * Licensed to the Apache Software Foundation (ASF) under one or more
1:ca5e41b:  * contributor license agreements.  See the NOTICE file distributed with
1:ca5e41b:  * this work for additional information regarding copyright ownership.
1:ca5e41b:  * The ASF licenses this file to You under the Apache License, Version 2.0
1:ca5e41b:  * (the "License"); you may not use this file except in compliance with
1:ca5e41b:  * the License.  You may obtain a copy of the License at
1:ca5e41b:  *
1:ca5e41b:  *      http://www.apache.org/licenses/LICENSE-2.0
1:ca5e41b:  *
1:ca5e41b:  * Unless required by applicable law or agreed to in writing, software
1:ca5e41b:  * distributed under the License is distributed on an "AS IS" BASIS,
1:ca5e41b:  * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
1:ca5e41b:  * See the License for the specific language governing permissions and
1:ca5e41b:  * limitations under the License.
1:ca5e41b:  */
1:ca5e41b: package org.apache.activemq.store.kahadb;
1:ca5e41b: 
1:ca5e41b: import org.apache.activemq.ActiveMQConnectionFactory;
1:ca5e41b: import org.apache.activemq.broker.BrokerService;
1:ca5e41b: import org.apache.activemq.broker.region.policy.PolicyEntry;
1:ca5e41b: import org.apache.activemq.broker.region.policy.PolicyMap;
1:ca5e41b: import org.apache.activemq.command.ActiveMQQueue;
1:ca5e41b: import org.apache.activemq.store.kahadb.disk.journal.DataFile;
1:ca5e41b: import org.apache.activemq.store.kahadb.disk.journal.Location;
1:ca5e41b: import org.apache.activemq.store.kahadb.disk.page.Page;
1:ca5e41b: import org.apache.activemq.store.kahadb.disk.page.PageFile;
1:ca5e41b: import org.apache.activemq.store.kahadb.disk.page.Transaction;
1:ca5e41b: import org.junit.After;
1:ca5e41b: import org.junit.Ignore;
1:ca5e41b: import org.junit.Test;
1:ca5e41b: import org.slf4j.Logger;
1:ca5e41b: import org.slf4j.LoggerFactory;
1:ca5e41b: 
1:ca5e41b: import javax.jms.Connection;
1:ca5e41b: import javax.jms.Destination;
1:ca5e41b: import javax.jms.Message;
1:ca5e41b: import javax.jms.MessageProducer;
1:ca5e41b: import javax.jms.Session;
1:ca5e41b: import java.io.IOException;
1:ca5e41b: import java.util.Collection;
1:ca5e41b: 
1:ca5e41b: import static org.apache.activemq.store.kahadb.JournalCorruptionEofIndexRecoveryTest.drain;
1:ca5e41b: import static org.junit.Assert.assertEquals;
1:ca5e41b: import static org.junit.Assert.assertTrue;
1:ca5e41b: 
1:ca5e41b: public class JournalMetaDataCheckpointTest {
1:ca5e41b: 
1:ca5e41b:     private static final Logger LOG = LoggerFactory.getLogger(JournalMetaDataCheckpointTest.class);
1:ca5e41b: 
1:ca5e41b:     private final String KAHADB_DIRECTORY = "target/activemq-data/";
1:ca5e41b:     private final String payload = new String(new byte[1024]);
1:ca5e41b: 
1:ca5e41b:     private BrokerService broker = null;
1:ca5e41b:     private final Destination destination = new ActiveMQQueue("Test");
1:ca5e41b:     private KahaDBPersistenceAdapter adapter;
1:ca5e41b: 
1:ca5e41b:     protected void startBroker() throws Exception {
1:ca5e41b:         doStartBroker(true);
1:ca5e41b:     }
1:ca5e41b: 
1:ca5e41b:     protected void restartBroker() throws Exception {
1:ca5e41b:         if (broker != null) {
1:ca5e41b:             broker.stop();
1:ca5e41b:             broker.waitUntilStopped();
1:ca5e41b:         }
1:ca5e41b: 
1:ca5e41b:         doStartBroker(false);
1:ca5e41b:     }
1:ca5e41b: 
1:ca5e41b:     private void doStartBroker(boolean delete) throws Exception {
1:ca5e41b:         doCreateBroker(delete);
1:ca5e41b:         LOG.info("Starting broker..");
1:ca5e41b:         broker.start();
1:ca5e41b:     }
1:ca5e41b: 
1:ca5e41b:     private void doCreateBroker(boolean delete) throws Exception {
1:ca5e41b: 
1:ca5e41b:         broker = new BrokerService();
1:ca5e41b:         broker.setDeleteAllMessagesOnStartup(delete);
1:ca5e41b:         broker.setPersistent(true);
1:ca5e41b:         broker.setUseJmx(true);
1:ca5e41b:         broker.setDataDirectory(KAHADB_DIRECTORY);
1:ca5e41b: 
1:ca5e41b:         PolicyMap policyMap = new PolicyMap();
1:ca5e41b:         PolicyEntry policyEntry = new PolicyEntry();
1:ca5e41b:         policyEntry.setUseCache(false);
1:ca5e41b:         policyMap.setDefaultEntry(policyEntry);
1:ca5e41b:         broker.setDestinationPolicy(policyMap);
1:ca5e41b: 
1:ca5e41b:         configurePersistence(broker);
1:ca5e41b:     }
1:ca5e41b: 
1:ca5e41b:     protected void configurePersistence(BrokerService brokerService) throws Exception {
1:ca5e41b:         adapter = (KahaDBPersistenceAdapter) brokerService.getPersistenceAdapter();
1:ca5e41b: 
1:ca5e41b:         // ensure there are a bunch of data files but multiple entries in each
1:ca5e41b:         adapter.setJournalMaxFileLength(1024 * 20);
1:ca5e41b: 
1:ca5e41b:         // manual cleanup
1:ca5e41b:         adapter.setCheckpointInterval(0);
1:ca5e41b:         adapter.setCleanupInterval(0);
1:ca5e41b: 
1:ca5e41b:         adapter.setCheckForCorruptJournalFiles(true);
1:ca5e41b: 
1:ca5e41b:     }
1:ca5e41b: 
1:ca5e41b:     @After
1:ca5e41b:     public void tearDown() throws Exception {
1:ca5e41b:         if (broker != null) {
1:ca5e41b:             broker.stop();
1:ca5e41b:             broker.waitUntilStopped();
1:ca5e41b:         }
1:ca5e41b:     }
1:ca5e41b: 
1:ca5e41b:     @Test
1:ca5e41b:     public void testRecoveryOnDeleteFailureMetaDataOk() throws Exception {
1:ca5e41b:         startBroker();
1:ca5e41b: 
1:ca5e41b:         int sent = produceMessagesToConsumeMultipleDataFiles(50);
1:ca5e41b: 
1:ca5e41b:         int numFilesAfterSend = getNumberOfJournalFiles();
1:ca5e41b:         LOG.info("Sent {}, Num journal files: {} ", sent, numFilesAfterSend);
1:ca5e41b: 
1:ca5e41b:         assertTrue("more than x files: " + numFilesAfterSend, numFilesAfterSend > 4);
1:ca5e41b: 
1:ca5e41b: 
1:ca5e41b:         int received = tryConsume(destination, sent/2);
1:ca5e41b:         assertEquals("all message received", sent/2, received);
1:ca5e41b: 
1:ca5e41b: 
1:ca5e41b:         int numFilesAfterRestart = getNumberOfJournalFiles();
1:ca5e41b:         LOG.info("Num journal files before gc: " + numFilesAfterRestart);
1:ca5e41b: 
1:ca5e41b:         // force gc
1:ca5e41b:         ((KahaDBPersistenceAdapter) broker.getPersistenceAdapter()).getStore().checkpoint(true);
1:ca5e41b: 
1:ca5e41b:         int numFilesAfterGC = getNumberOfJournalFiles();
1:ca5e41b:         assertEquals("all message received", sent/2, received);
1:ca5e41b:         LOG.info("Num journal files after restart nd gc: " + numFilesAfterGC);
1:ca5e41b:         assertTrue("Gc has happened", numFilesAfterGC < numFilesAfterRestart);
1:ca5e41b: 
1:ca5e41b:         // verify metadata is correct on disk
1:ca5e41b:         final MessageDatabase.Metadata[] fromDiskMetaData = new MessageDatabase.Metadata[1];
1:af03ad4:         final KahaDBStore messageStore = ((KahaDBPersistenceAdapter) broker.getPersistenceAdapter()).getStore();
1:ca5e41b: 
1:ca5e41b:         // need to avoid cache and in-progress writes of existing pageFile
1:ca5e41b:         PageFile fromDiskPageFile = new PageFile(messageStore.getIndexDirectory(), "db");
1:ca5e41b:         fromDiskPageFile.setEnablePageCaching(false);
1:ca5e41b:         fromDiskPageFile.setEnableRecoveryFile(false);
1:ca5e41b:         fromDiskPageFile.load();
1:ca5e41b:         fromDiskPageFile.tx().execute(new Transaction.Closure<IOException>() {
1:ca5e41b:             @Override
1:ca5e41b:             public void execute(Transaction tx) throws IOException {
1:ca5e41b:                     Page<MessageDatabase.Metadata> page = tx.load(0, messageStore.metadataMarshaller);
1:ca5e41b:                 fromDiskMetaData[0] = page.get();
1:ca5e41b:             }
1:ca5e41b:         });
1:ca5e41b: 
1:ca5e41b:         assertEquals("location is uptodate", messageStore.getMetadata().ackMessageFileMapLocation, fromDiskMetaData[0].ackMessageFileMapLocation);
1:ca5e41b:     }
1:ca5e41b: 
1:ca5e41b:     @Ignore("needs work")
1:ca5e41b:     public void testAckMessageFileMapSyncOnModOnly() throws Exception {
1:ca5e41b:         startBroker();
1:ca5e41b:         // force gc
1:ca5e41b:         ((KahaDBPersistenceAdapter) broker.getPersistenceAdapter()).getStore().checkpoint(true);
1:ca5e41b: 
2:ca5e41b:         KahaDBStore messageStore = ((KahaDBPersistenceAdapter) broker.getPersistenceAdapter()).getStore();
1:ca5e41b: 
1:ca5e41b:         Location ackMessageFileModLoc =  messageStore.getMetadata().ackMessageFileMapLocation;
1:ca5e41b:         // force gc
1:ca5e41b:         ((KahaDBPersistenceAdapter) broker.getPersistenceAdapter()).getStore().checkpoint(true);
1:ca5e41b: 
1:ca5e41b:         assertEquals("location is not changed on no modification", ackMessageFileModLoc, messageStore.getMetadata().ackMessageFileMapLocation);
1:ca5e41b:     }
1:ca5e41b: 
1:ca5e41b:     private int getNumberOfJournalFiles() throws IOException {
1:ca5e41b:         Collection<DataFile> files = ((KahaDBPersistenceAdapter) broker.getPersistenceAdapter()).getStore().getJournal().getFileMap().values();
1:ca5e41b:         int reality = 0;
1:ca5e41b:         for (DataFile file : files) {
1:ca5e41b:             if (file != null) {
1:ca5e41b:                 reality++;
1:ca5e41b:             }
1:ca5e41b:         }
1:ca5e41b:         return reality;
1:ca5e41b:     }
1:ca5e41b: 
1:ca5e41b:     private int produceMessages(Destination destination, int numToSend) throws Exception {
1:ca5e41b:         int sent = 0;
1:ca5e41b:         Connection connection = new ActiveMQConnectionFactory(broker.getVmConnectorURI()).createConnection();
1:ca5e41b:         connection.start();
1:ca5e41b:         try {
1:ca5e41b:             Session session = connection.createSession(false, Session.AUTO_ACKNOWLEDGE);
1:ca5e41b:             MessageProducer producer = session.createProducer(destination);
1:ca5e41b:             for (int i = 0; i < numToSend; i++) {
1:ca5e41b:                 producer.send(createMessage(session, i));
1:ca5e41b:                 sent++;
1:ca5e41b:             }
1:ca5e41b:         } finally {
1:ca5e41b:             connection.close();
1:ca5e41b:         }
1:ca5e41b: 
1:ca5e41b:         return sent;
1:ca5e41b:     }
1:ca5e41b: 
1:ca5e41b:     private int tryConsume(Destination destination, int numToGet) throws Exception {
1:ca5e41b:         ActiveMQConnectionFactory cf = new ActiveMQConnectionFactory(broker.getVmConnectorURI());
1:ca5e41b:         return  drain(cf, destination, numToGet);
1:ca5e41b:     }
1:ca5e41b: 
1:ca5e41b:     private int produceMessagesToConsumeMultipleDataFiles(int numToSend) throws Exception {
1:ca5e41b:         return produceMessages(destination, numToSend);
1:ca5e41b:     }
1:ca5e41b: 
1:ca5e41b:     private Message createMessage(Session session, int i) throws Exception {
1:ca5e41b:         return session.createTextMessage(payload + "::" + i);
1:ca5e41b:     }
1:ca5e41b: }
============================================================================
author:gtully
-------------------------------------------------------------------------------
commit:af03ad4
/////////////////////////////////////////////////////////////////////////
1:         final KahaDBStore messageStore = ((KahaDBPersistenceAdapter) broker.getPersistenceAdapter()).getStore();
commit:ca5e41b
/////////////////////////////////////////////////////////////////////////
1: /**
1:  * Licensed to the Apache Software Foundation (ASF) under one or more
1:  * contributor license agreements.  See the NOTICE file distributed with
1:  * this work for additional information regarding copyright ownership.
1:  * The ASF licenses this file to You under the Apache License, Version 2.0
1:  * (the "License"); you may not use this file except in compliance with
1:  * the License.  You may obtain a copy of the License at
1:  *
1:  *      http://www.apache.org/licenses/LICENSE-2.0
1:  *
1:  * Unless required by applicable law or agreed to in writing, software
1:  * distributed under the License is distributed on an "AS IS" BASIS,
1:  * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
1:  * See the License for the specific language governing permissions and
1:  * limitations under the License.
1:  */
1: package org.apache.activemq.store.kahadb;
1: 
1: import org.apache.activemq.ActiveMQConnectionFactory;
1: import org.apache.activemq.broker.BrokerService;
1: import org.apache.activemq.broker.region.policy.PolicyEntry;
1: import org.apache.activemq.broker.region.policy.PolicyMap;
1: import org.apache.activemq.command.ActiveMQQueue;
1: import org.apache.activemq.store.kahadb.disk.journal.DataFile;
1: import org.apache.activemq.store.kahadb.disk.journal.Location;
1: import org.apache.activemq.store.kahadb.disk.page.Page;
1: import org.apache.activemq.store.kahadb.disk.page.PageFile;
1: import org.apache.activemq.store.kahadb.disk.page.Transaction;
1: import org.junit.After;
1: import org.junit.Ignore;
1: import org.junit.Test;
1: import org.slf4j.Logger;
1: import org.slf4j.LoggerFactory;
1: 
1: import javax.jms.Connection;
1: import javax.jms.Destination;
1: import javax.jms.Message;
1: import javax.jms.MessageProducer;
1: import javax.jms.Session;
1: import java.io.IOException;
1: import java.util.Collection;
1: 
1: import static org.apache.activemq.store.kahadb.JournalCorruptionEofIndexRecoveryTest.drain;
1: import static org.junit.Assert.assertEquals;
1: import static org.junit.Assert.assertTrue;
1: 
1: public class JournalMetaDataCheckpointTest {
1: 
1:     private static final Logger LOG = LoggerFactory.getLogger(JournalMetaDataCheckpointTest.class);
1: 
1:     private final String KAHADB_DIRECTORY = "target/activemq-data/";
1:     private final String payload = new String(new byte[1024]);
1: 
1:     private BrokerService broker = null;
1:     private final Destination destination = new ActiveMQQueue("Test");
1:     private KahaDBPersistenceAdapter adapter;
1: 
1:     protected void startBroker() throws Exception {
1:         doStartBroker(true);
1:     }
1: 
1:     protected void restartBroker() throws Exception {
1:         if (broker != null) {
1:             broker.stop();
1:             broker.waitUntilStopped();
1:         }
1: 
1:         doStartBroker(false);
1:     }
1: 
1:     private void doStartBroker(boolean delete) throws Exception {
1:         doCreateBroker(delete);
1:         LOG.info("Starting broker..");
1:         broker.start();
1:     }
1: 
1:     private void doCreateBroker(boolean delete) throws Exception {
1: 
1:         broker = new BrokerService();
1:         broker.setDeleteAllMessagesOnStartup(delete);
1:         broker.setPersistent(true);
1:         broker.setUseJmx(true);
1:         broker.setDataDirectory(KAHADB_DIRECTORY);
1: 
1:         PolicyMap policyMap = new PolicyMap();
1:         PolicyEntry policyEntry = new PolicyEntry();
1:         policyEntry.setUseCache(false);
1:         policyMap.setDefaultEntry(policyEntry);
1:         broker.setDestinationPolicy(policyMap);
1: 
1:         configurePersistence(broker);
1:     }
1: 
1:     protected void configurePersistence(BrokerService brokerService) throws Exception {
1:         adapter = (KahaDBPersistenceAdapter) brokerService.getPersistenceAdapter();
1: 
1:         // ensure there are a bunch of data files but multiple entries in each
1:         adapter.setJournalMaxFileLength(1024 * 20);
1: 
1:         // manual cleanup
1:         adapter.setCheckpointInterval(0);
1:         adapter.setCleanupInterval(0);
1: 
1:         adapter.setCheckForCorruptJournalFiles(true);
1: 
1:     }
1: 
1:     @After
1:     public void tearDown() throws Exception {
1:         if (broker != null) {
1:             broker.stop();
1:             broker.waitUntilStopped();
1:         }
1:     }
1: 
1:     @Test
1:     public void testRecoveryOnDeleteFailureMetaDataOk() throws Exception {
1:         startBroker();
1: 
1:         int sent = produceMessagesToConsumeMultipleDataFiles(50);
1: 
1:         int numFilesAfterSend = getNumberOfJournalFiles();
1:         LOG.info("Sent {}, Num journal files: {} ", sent, numFilesAfterSend);
1: 
1:         assertTrue("more than x files: " + numFilesAfterSend, numFilesAfterSend > 4);
1: 
1: 
1:         int received = tryConsume(destination, sent/2);
1:         assertEquals("all message received", sent/2, received);
1: 
1: 
1:         int numFilesAfterRestart = getNumberOfJournalFiles();
1:         LOG.info("Num journal files before gc: " + numFilesAfterRestart);
1: 
1:         // force gc
1:         ((KahaDBPersistenceAdapter) broker.getPersistenceAdapter()).getStore().checkpoint(true);
1: 
1:         int numFilesAfterGC = getNumberOfJournalFiles();
1:         assertEquals("all message received", sent/2, received);
1:         LOG.info("Num journal files after restart nd gc: " + numFilesAfterGC);
1:         assertTrue("Gc has happened", numFilesAfterGC < numFilesAfterRestart);
1: 
1:         // verify metadata is correct on disk
1:         final MessageDatabase.Metadata[] fromDiskMetaData = new MessageDatabase.Metadata[1];
1:         KahaDBStore messageStore = ((KahaDBPersistenceAdapter) broker.getPersistenceAdapter()).getStore();
1: 
1:         // need to avoid cache and in-progress writes of existing pageFile
1:         PageFile fromDiskPageFile = new PageFile(messageStore.getIndexDirectory(), "db");
1:         fromDiskPageFile.setEnablePageCaching(false);
1:         fromDiskPageFile.setEnableRecoveryFile(false);
1:         fromDiskPageFile.load();
1:         fromDiskPageFile.tx().execute(new Transaction.Closure<IOException>() {
1:             @Override
1:             public void execute(Transaction tx) throws IOException {
1:                     Page<MessageDatabase.Metadata> page = tx.load(0, messageStore.metadataMarshaller);
1:                 fromDiskMetaData[0] = page.get();
1:             }
1:         });
1: 
1:         assertEquals("location is uptodate", messageStore.getMetadata().ackMessageFileMapLocation, fromDiskMetaData[0].ackMessageFileMapLocation);
1:     }
1: 
1:     @Ignore("needs work")
1:     public void testAckMessageFileMapSyncOnModOnly() throws Exception {
1:         startBroker();
1:         // force gc
1:         ((KahaDBPersistenceAdapter) broker.getPersistenceAdapter()).getStore().checkpoint(true);
1: 
1:         KahaDBStore messageStore = ((KahaDBPersistenceAdapter) broker.getPersistenceAdapter()).getStore();
1: 
1:         Location ackMessageFileModLoc =  messageStore.getMetadata().ackMessageFileMapLocation;
1:         // force gc
1:         ((KahaDBPersistenceAdapter) broker.getPersistenceAdapter()).getStore().checkpoint(true);
1: 
1:         assertEquals("location is not changed on no modification", ackMessageFileModLoc, messageStore.getMetadata().ackMessageFileMapLocation);
1:     }
1: 
1:     private int getNumberOfJournalFiles() throws IOException {
1:         Collection<DataFile> files = ((KahaDBPersistenceAdapter) broker.getPersistenceAdapter()).getStore().getJournal().getFileMap().values();
1:         int reality = 0;
1:         for (DataFile file : files) {
1:             if (file != null) {
1:                 reality++;
1:             }
1:         }
1:         return reality;
1:     }
1: 
1:     private int produceMessages(Destination destination, int numToSend) throws Exception {
1:         int sent = 0;
1:         Connection connection = new ActiveMQConnectionFactory(broker.getVmConnectorURI()).createConnection();
1:         connection.start();
1:         try {
1:             Session session = connection.createSession(false, Session.AUTO_ACKNOWLEDGE);
1:             MessageProducer producer = session.createProducer(destination);
1:             for (int i = 0; i < numToSend; i++) {
1:                 producer.send(createMessage(session, i));
1:                 sent++;
1:             }
1:         } finally {
1:             connection.close();
1:         }
1: 
1:         return sent;
1:     }
1: 
1:     private int tryConsume(Destination destination, int numToGet) throws Exception {
1:         ActiveMQConnectionFactory cf = new ActiveMQConnectionFactory(broker.getVmConnectorURI());
1:         return  drain(cf, destination, numToGet);
1:     }
1: 
1:     private int produceMessagesToConsumeMultipleDataFiles(int numToSend) throws Exception {
1:         return produceMessages(destination, numToSend);
1:     }
1: 
1:     private Message createMessage(Session session, int i) throws Exception {
1:         return session.createTextMessage(payload + "::" + i);
1:     }
1: }
============================================================================