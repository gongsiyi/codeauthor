1:f2517c0: /**
1:f2517c0:  * Licensed to the Apache Software Foundation (ASF) under one or more
1:f2517c0:  * contributor license agreements.  See the NOTICE file distributed with
1:f2517c0:  * this work for additional information regarding copyright ownership.
1:f2517c0:  * The ASF licenses this file to You under the Apache License, Version 2.0
1:f2517c0:  * (the "License"); you may not use this file except in compliance with
1:f2517c0:  * the License.  You may obtain a copy of the License at
1:f2517c0:  *
1:f2517c0:  *      http://www.apache.org/licenses/LICENSE-2.0
1:f2517c0:  *
1:f2517c0:  * Unless required by applicable law or agreed to in writing, software
1:f2517c0:  * distributed under the License is distributed on an "AS IS" BASIS,
1:f2517c0:  * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
1:f2517c0:  * See the License for the specific language governing permissions and
1:f2517c0:  * limitations under the License.
1:f2517c0:  */
1:f2517c0: package org.apache.activemq.bugs;
3:f2517c0: 
1:3bf9d0c: import static org.junit.Assert.assertEquals;
1:b0a1bd8: import static org.junit.Assert.assertNull;
1:3bf9d0c: import static org.junit.Assert.assertNotNull;
1:3bf9d0c: import static org.junit.Assert.assertTrue;
1:5db5f3e: import static org.junit.Assert.fail;
1:3bf9d0c: 
1:5db5f3e: import java.io.File;
1:3bf9d0c: import java.io.IOException;
1:3bf9d0c: import java.util.Collection;
1:3bf9d0c: import java.util.concurrent.TimeUnit;
1:f2517c0: 
1:f2517c0: import javax.jms.Connection;
1:f2517c0: import javax.jms.Destination;
1:f2517c0: import javax.jms.JMSException;
1:f2517c0: import javax.jms.Message;
1:f2517c0: import javax.jms.MessageConsumer;
1:f2517c0: import javax.jms.MessageProducer;
1:3bf9d0c: import javax.jms.Queue;
1:f2517c0: import javax.jms.Session;
1:3bf9d0c: import javax.jms.Topic;
1:f2517c0: 
1:3bf9d0c: import org.apache.activemq.ActiveMQConnectionFactory;
1:3bf9d0c: import org.apache.activemq.ActiveMQSession;
1:3bf9d0c: import org.apache.activemq.broker.BrokerService;
1:3bf9d0c: import org.apache.activemq.command.ActiveMQQueue;
1:3bf9d0c: import org.apache.activemq.command.ActiveMQTopic;
1:d52e591: import org.apache.activemq.leveldb.LevelDBStore;
1:d52e591: import org.apache.activemq.store.PersistenceAdapter;
1:3bf9d0c: import org.apache.activemq.store.kahadb.KahaDBPersistenceAdapter;
1:3bf9d0c: import org.apache.activemq.store.kahadb.disk.journal.DataFile;
1:4f43a21: import org.apache.activemq.store.kahadb.disk.journal.Journal;
1:3bf9d0c: import org.apache.activemq.util.Wait;
1:3bf9d0c: import org.junit.After;
1:3bf9d0c: import org.junit.Test;
1:3bf9d0c: import org.slf4j.Logger;
1:3bf9d0c: import org.slf4j.LoggerFactory;
1:f2517c0: 
1:f2517c0: public class AMQ2832Test {
1:f2517c0: 
1:8bf987b:     private static final Logger LOG = LoggerFactory.getLogger(AMQ2832Test.class);
1:f2517c0: 
1:f2517c0:     BrokerService broker = null;
1:3bf9d0c:     private ActiveMQConnectionFactory cf;
1:f2517c0:     private final Destination destination = new ActiveMQQueue("AMQ2832Test");
1:3bf9d0c:     private String connectionUri;
1:f2517c0: 
1:3bf9d0c:     protected void startBroker() throws Exception {
1:3bf9d0c:         doStartBroker(true, false);
1:3bf9d0c:     }
1:3bf9d0c: 
1:3bf9d0c:     protected void restartBroker() throws Exception {
1:3bf9d0c:         if (broker != null) {
1:3bf9d0c:             broker.stop();
1:3bf9d0c:             broker.waitUntilStopped();
1:3bf9d0c:         }
1:3bf9d0c:         doStartBroker(false, false);
1:3bf9d0c:     }
1:3bf9d0c: 
1:3bf9d0c:     protected void recoverBroker() throws Exception {
1:3bf9d0c:         if (broker != null) {
1:3bf9d0c:             broker.stop();
1:3bf9d0c:             broker.waitUntilStopped();
1:3bf9d0c:         }
1:3bf9d0c:         doStartBroker(false, true);
1:3bf9d0c:     }
1:3bf9d0c: 
1:3bf9d0c:     private void doStartBroker(boolean delete, boolean recover) throws Exception {
1:f2517c0:         broker = new BrokerService();
1:f2517c0:         broker.setDeleteAllMessagesOnStartup(delete);
1:f2517c0:         broker.setPersistent(true);
1:3bf9d0c:         broker.setUseJmx(true);
1:f2517c0:         broker.addConnector("tcp://localhost:0");
1:f2517c0: 
1:3bf9d0c:         configurePersistence(broker, recover);
1:3bf9d0c: 
1:3bf9d0c:         connectionUri = "vm://localhost?create=false";
1:3bf9d0c:         cf = new ActiveMQConnectionFactory(connectionUri);
1:f2517c0: 
1:f2517c0:         broker.start();
1:f2517c0:         LOG.info("Starting broker..");
2:f2517c0:     }
1:f2517c0: 
1:3bf9d0c:     protected void configurePersistence(BrokerService brokerService, boolean recover) throws Exception {
1:f2517c0:         KahaDBPersistenceAdapter adapter = (KahaDBPersistenceAdapter) brokerService.getPersistenceAdapter();
1:f2517c0: 
1:f2517c0:         // ensure there are a bunch of data files but multiple entries in each
1:f2517c0:         adapter.setJournalMaxFileLength(1024 * 20);
1:f2517c0: 
1:f2517c0:         // speed up the test case, checkpoint an cleanup early and often
1:3bf9d0c:         adapter.setCheckpointInterval(5000);
1:3bf9d0c:         adapter.setCleanupInterval(5000);
1:65cef69:         adapter.setPreallocationScope(Journal.PreallocationScope.ENTIRE_JOURNAL.name());
1:f2517c0: 
1:3bf9d0c:         if (recover) {
1:f2517c0:             adapter.setForceRecoverIndex(true);
1:3bf9d0c:         }
1:f2517c0:     }
1:f2517c0: 
1:3bf9d0c:     @After
1:3bf9d0c:     public void tearDown() throws Exception {
1:3bf9d0c:         if (broker != null) {
1:3bf9d0c:             broker.stop();
1:3bf9d0c:             broker.waitUntilStopped();
1:3bf9d0c:         }
1:f2517c0:     }
1:f2517c0: 
1:b0a1bd8:    /**
1:b0a1bd8:     * Scenario:
1:b0a1bd8:     * db-1.log has an unacknowledged message,
1:b0a1bd8:     * db-2.log contains acks for the messages from db-1.log,
1:b0a1bd8:     * db-3.log contains acks for the messages from db-2.log
1:b0a1bd8:     *
1:b0a1bd8:     * Expected behavior: since db-1.log is blocked, db-2.log and db-3.log should not be removed during the cleanup.
1:b0a1bd8:     * Current situation on 5.10.0, 5.10.1 is that db-3.log is removed causing all messages from db-2.log, whose acks were in db-3.log, to be replayed.
1:b0a1bd8:     *
1:b0a1bd8:     * @throws Exception
1:b0a1bd8:     */
1:f2517c0:     @Test
1:b0a1bd8:     public void testAckChain() throws Exception {
1:5db5f3e:         startBroker();
1:b0a1bd8: 
1:5db5f3e:         makeAckChain();
1:b0a1bd8: 
1:5db5f3e:         broker.stop();
1:5db5f3e:         broker.waitUntilStopped();
1:b0a1bd8: 
1:5db5f3e:         recoverBroker();
1:f2517c0: 
1:5db5f3e:         StagedConsumer consumer = new StagedConsumer();
1:5db5f3e:         Message message = consumer.receive(1);
1:5db5f3e:         assertNotNull("One message stays unacked from db-1.log", message);
1:5db5f3e:         message.acknowledge();
1:5db5f3e:         message = consumer.receive(1);
1:5db5f3e:         assertNull("There should not be any unconsumed messages any more", message);
1:5db5f3e:         consumer.close();
1:5db5f3e:     }
1:f2517c0: 
1:5db5f3e:     private void makeAckChain() throws Exception {
1:5db5f3e:         StagedConsumer consumer = new StagedConsumer();
1:5db5f3e:         // file #1
1:5db5f3e:         produceMessagesToConsumeMultipleDataFiles(5);
1:5db5f3e:         // acknowledge first 2 messages and leave the 3rd one unacknowledged blocking db-1.log
1:5db5f3e:         consumer.receive(3);
1:b0a1bd8: 
1:5db5f3e:         // send messages by consuming and acknowledging every message right after sent in order to get KahadbAdd and Remove command to be saved together
1:5db5f3e:         // this is necessary in order to get KahaAddMessageCommand to be saved in one db file and the corresponding KahaRemoveMessageCommand in the next one
1:5db5f3e:         produceAndConsumeImmediately(20, consumer);
1:5db5f3e:         consumer.receive(2).acknowledge(); // consume and ack the last 2 unconsumed
1:5db5f3e: 
1:5db5f3e:         // now we have 3 files written and started with #4
1:5db5f3e:         consumer.close();
1:5db5f3e:     }
1:5db5f3e: 
1:5db5f3e:     @Test
1:5db5f3e:     public void testNoRestartOnMissingAckDataFile() throws Exception {
1:5db5f3e:         startBroker();
1:5db5f3e: 
1:5db5f3e:         // reuse scenario from previous test
1:5db5f3e:         makeAckChain();
1:5db5f3e: 
1:5db5f3e:         File dataDir = broker.getPersistenceAdapter().getDirectory();
1:5db5f3e:         broker.stop();
1:5db5f3e:         broker.waitUntilStopped();
1:5db5f3e: 
1:5db5f3e:         File secondLastDataFile = new File(dataDir, "db-3.log");
1:5db5f3e:         LOG.info("Whacking data file with acks: " + secondLastDataFile);
1:5db5f3e:         secondLastDataFile.delete();
1:5db5f3e: 
1:5db5f3e:         try {
1:5db5f3e:             doStartBroker(false, false);
1:5db5f3e:             fail("Expect failure to start with corrupt journal");
1:5db5f3e:         } catch (IOException expected) {
1:5db5f3e:         }
1:5db5f3e:     }
1:5db5f3e: 
1:b0a1bd8: 
1:b0a1bd8:    private void produceAndConsumeImmediately(int numOfMsgs, StagedConsumer consumer) throws Exception {
1:b0a1bd8:       for (int i = 0; i < numOfMsgs; i++) {
1:b0a1bd8:          produceMessagesToConsumeMultipleDataFiles(1);
1:b0a1bd8:          consumer.receive(1).acknowledge();
1:b0a1bd8:       }
1:b0a1bd8:    }
1:b0a1bd8: 
1:b0a1bd8:    @Test
1:f2517c0:     public void testAckRemovedMessageReplayedAfterRecovery() throws Exception {
1:b0a1bd8: 
1:b0a1bd8:         startBroker();
1:f2517c0: 
1:b0a1bd8:         StagedConsumer consumer = new StagedConsumer();
1:f2517c0:         int numMessagesAvailable = produceMessagesToConsumeMultipleDataFiles(20);
1:f2517c0:         // this will block the reclaiming of one data file
1:f2517c0:         Message firstUnacked = consumer.receive(10);
1:f2517c0:         LOG.info("first unacked: " + firstUnacked.getJMSMessageID());
1:f2517c0:         Message secondUnacked = consumer.receive(1);
1:f2517c0:         LOG.info("second unacked: " + secondUnacked.getJMSMessageID());
1:f2517c0:         numMessagesAvailable -= 11;
1:f2517c0: 
1:f2517c0:         numMessagesAvailable += produceMessagesToConsumeMultipleDataFiles(10);
1:f2517c0:         // ensure ack is another data file
1:f2517c0:         LOG.info("Acking firstUnacked: " + firstUnacked.getJMSMessageID());
1:f2517c0:         firstUnacked.acknowledge();
1:f2517c0: 
1:f2517c0:         numMessagesAvailable += produceMessagesToConsumeMultipleDataFiles(10);
1:f2517c0: 
1:f2517c0:         consumer.receive(numMessagesAvailable).acknowledge();
1:f2517c0: 
1:f2517c0:         // second unacked should keep first data file available but journal with the first ack
1:f2517c0:         // may get whacked
1:b0a1bd8:         consumer.close();
1:f2517c0: 
1:b0a1bd8:         broker.stop();
1:b0a1bd8:         broker.waitUntilStopped();
1:f2517c0: 
1:b0a1bd8:         recoverBroker();
1:f2517c0: 
1:b0a1bd8:         consumer = new StagedConsumer();
1:f2517c0:         // need to force recovery?
1:b0a1bd8: 
1:f2517c0:         Message msg = consumer.receive(1, 5);
1:f2517c0:         assertNotNull("One messages left after recovery", msg);
1:f2517c0:         msg.acknowledge();
1:f2517c0: 
1:f2517c0:         // should be no more messages
1:f2517c0:         msg = consumer.receive(1, 5);
1:f2517c0:         assertEquals("Only one messages left after recovery: " + msg, null, msg);
1:b0a1bd8:         consumer.close();
1:b0a1bd8:     }
1:3bf9d0c: 
1:3bf9d0c:     @Test
1:3bf9d0c:     public void testAlternateLossScenario() throws Exception {
1:3bf9d0c: 
1:3bf9d0c:         startBroker();
1:d52e591:         PersistenceAdapter pa  = broker.getPersistenceAdapter();
1:d52e591:         if (pa instanceof LevelDBStore) {
1:d52e591:             return;
1:d52e591:         }
1:f2517c0: 
1:3bf9d0c:         ActiveMQQueue queue = new ActiveMQQueue("MyQueue");
1:3bf9d0c:         ActiveMQQueue disposable = new ActiveMQQueue("MyDisposableQueue");
1:3bf9d0c:         ActiveMQTopic topic = new ActiveMQTopic("MyDurableTopic");
1:3bf9d0c: 
1:3bf9d0c:         // This ensure that data file 1 never goes away.
1:3bf9d0c:         createInactiveDurableSub(topic);
1:3bf9d0c:         assertEquals(1, getNumberOfJournalFiles());
1:3bf9d0c: 
1:3bf9d0c:         // One Queue Message that will be acked in another data file.
1:3bf9d0c:         produceMessages(queue, 1);
1:3bf9d0c:         assertEquals(1, getNumberOfJournalFiles());
1:3bf9d0c: 
1:3bf9d0c:         // Add some messages to consume space
1:3bf9d0c:         produceMessages(disposable, 50);
1:3bf9d0c: 
1:3bf9d0c:         int dataFilesCount = getNumberOfJournalFiles();
1:3bf9d0c:         assertTrue(dataFilesCount > 1);
1:3bf9d0c: 
1:3bf9d0c:         // Create an ack for the single message on this queue
1:3bf9d0c:         drainQueue(queue);
1:3bf9d0c: 
1:3bf9d0c:         // Add some more messages to consume space beyond tha data file with the ack
1:3bf9d0c:         produceMessages(disposable, 50);
1:3bf9d0c: 
1:3bf9d0c:         assertTrue(dataFilesCount < getNumberOfJournalFiles());
1:3bf9d0c:         dataFilesCount = getNumberOfJournalFiles();
1:3bf9d0c: 
1:3bf9d0c:         restartBroker();
1:3bf9d0c: 
1:3bf9d0c:         // Clear out all queue data
1:3bf9d0c:         broker.getAdminView().removeQueue(disposable.getQueueName());
1:3bf9d0c: 
1:3bf9d0c:         // Once this becomes true our ack could be lost.
1:3bf9d0c:         assertTrue("Less than three journal file expected, was " + getNumberOfJournalFiles(), Wait.waitFor(new Wait.Condition() {
1:3bf9d0c:             @Override
1:3bf9d0c:             public boolean isSatisified() throws Exception {
1:65cef69:                 return getNumberOfJournalFiles() <= 3;
1:3bf9d0c:             }
1:3bf9d0c:         }, TimeUnit.MINUTES.toMillis(3)));
1:3bf9d0c: 
1:3bf9d0c:         // Recover and the Message should not be replayed but if the old MessageAck is lost
1:3bf9d0c:         // then it could be.
1:3bf9d0c:         recoverBroker();
1:3bf9d0c: 
1:3bf9d0c:         assertTrue(drainQueue(queue) == 0);
1:3bf9d0c:     }
1:3bf9d0c: 
1:3bf9d0c:     private int getNumberOfJournalFiles() throws IOException {
1:d52e591: 
1:3bf9d0c:         Collection<DataFile> files =
1:3bf9d0c:             ((KahaDBPersistenceAdapter) broker.getPersistenceAdapter()).getStore().getJournal().getFileMap().values();
1:4f43a21:         LOG.info("Data files: " + files);
1:3bf9d0c:         int reality = 0;
1:3bf9d0c:         for (DataFile file : files) {
1:3bf9d0c:             if (file != null) {
1:3bf9d0c:                 reality++;
1:3bf9d0c:             }
1:3bf9d0c:         }
1:3bf9d0c: 
1:3bf9d0c:         return reality;
1:3bf9d0c:     }
1:3bf9d0c: 
1:3bf9d0c:     private void createInactiveDurableSub(Topic topic) throws Exception {
1:3bf9d0c:         Connection connection = cf.createConnection();
1:3bf9d0c:         connection.setClientID("Inactive");
1:3bf9d0c:         Session session = connection.createSession(false, Session.AUTO_ACKNOWLEDGE);
1:3bf9d0c:         MessageConsumer consumer = session.createDurableSubscriber(topic, "Inactive");
1:3bf9d0c:         consumer.close();
1:3bf9d0c:         connection.close();
1:3bf9d0c:         produceMessages(topic, 1);
1:3bf9d0c:     }
1:3bf9d0c: 
1:3bf9d0c:     private int drainQueue(Queue queue) throws Exception {
1:3bf9d0c:         Connection connection = cf.createConnection();
1:3bf9d0c:         connection.setClientID("Inactive");
1:3bf9d0c:         connection.start();
1:3bf9d0c:         Session session = connection.createSession(false, Session.AUTO_ACKNOWLEDGE);
1:3bf9d0c:         MessageConsumer consumer = session.createConsumer(queue);
1:3bf9d0c:         int count = 0;
1:3bf9d0c:         while (consumer.receive(5000) != null) {
1:3bf9d0c:             count++;
1:3bf9d0c:         }
1:3bf9d0c:         consumer.close();
1:3bf9d0c:         connection.close();
1:3bf9d0c:         return count;
1:3bf9d0c:     }
1:3bf9d0c: 
1:3bf9d0c:     private int produceMessages(Destination destination, int numToSend) throws Exception {
1:f2517c0:         int sent = 0;
1:f2517c0:         Connection connection = new ActiveMQConnectionFactory(
1:f2517c0:                 broker.getTransportConnectors().get(0).getConnectUri()).createConnection();
1:f2517c0:         connection.start();
1:f2517c0:         try {
1:f2517c0:             Session session = connection.createSession(false, Session.AUTO_ACKNOWLEDGE);
1:f2517c0:             MessageProducer producer = session.createProducer(destination);
1:f2517c0:             for (int i = 0; i < numToSend; i++) {
1:f2517c0:                 producer.send(createMessage(session, i));
1:f2517c0:                 sent++;
1:f2517c0:             }
1:f2517c0:         } finally {
1:f2517c0:             connection.close();
1:f2517c0:         }
1:3bf9d0c: 
1:f2517c0:         return sent;
1:f2517c0:     }
1:f2517c0: 
1:3bf9d0c:     private int produceMessagesToConsumeMultipleDataFiles(int numToSend) throws Exception {
1:3bf9d0c:         return produceMessages(destination, numToSend);
1:3bf9d0c:     }
1:3bf9d0c: 
1:f2517c0:     final String payload = new String(new byte[1024]);
1:f2517c0: 
1:f2517c0:     private Message createMessage(Session session, int i) throws Exception {
1:f2517c0:         return session.createTextMessage(payload + "::" + i);
1:f2517c0:     }
1:f2517c0: 
1:f2517c0:     private class StagedConsumer {
1:f2517c0:         Connection connection;
1:f2517c0:         MessageConsumer consumer;
1:f2517c0: 
1:f2517c0:         StagedConsumer() throws Exception {
1:f2517c0:             connection = new ActiveMQConnectionFactory("failover://" +
1:f2517c0:                     broker.getTransportConnectors().get(0).getConnectUri().toString()).createConnection();
1:f2517c0:             connection.start();
1:f2517c0:             consumer = connection.createSession(false, ActiveMQSession.INDIVIDUAL_ACKNOWLEDGE).createConsumer(destination);
1:f2517c0:         }
1:f2517c0: 
1:f2517c0:         public Message receive(int numToReceive) throws Exception {
1:f2517c0:             return receive(numToReceive, 2);
1:f2517c0:         }
1:f2517c0: 
1:f2517c0:         public Message receive(int numToReceive, int timeoutInSeconds) throws Exception {
1:f2517c0:             Message msg = null;
1:f2517c0:             for (; numToReceive > 0; numToReceive--) {
1:f2517c0: 
1:f2517c0:                 do  {
1:f2517c0:                     msg = consumer.receive(1*1000);
1:f2517c0:                 } while (msg == null && --timeoutInSeconds > 0);
1:f2517c0: 
1:f2517c0:                 if (numToReceive > 1) {
1:f2517c0:                     msg.acknowledge();
1:f2517c0:                 }
1:f2517c0: 
1:f2517c0:                 if (msg != null) {
1:f2517c0:                     LOG.debug("received: " + msg.getJMSMessageID());
1:f2517c0:                 }
1:f2517c0:             }
1:f2517c0:             // last message, unacked
1:f2517c0:             return msg;
1:f2517c0:         }
1:f2517c0: 
1:f2517c0:         void close() throws JMSException {
3:f2517c0:             consumer.close();
1:f2517c0:             connection.close();
1:f2517c0:         }
1:f2517c0:     }
1:f2517c0: }
============================================================================
author:gtully
-------------------------------------------------------------------------------
commit:65cef69
/////////////////////////////////////////////////////////////////////////
1:         adapter.setPreallocationScope(Journal.PreallocationScope.ENTIRE_JOURNAL.name());
/////////////////////////////////////////////////////////////////////////
1:                 return getNumberOfJournalFiles() <= 3;
commit:4f43a21
/////////////////////////////////////////////////////////////////////////
1: import org.apache.activemq.store.kahadb.disk.journal.Journal;
/////////////////////////////////////////////////////////////////////////
0:                 return getNumberOfJournalFiles() <= 4;
/////////////////////////////////////////////////////////////////////////
1:         LOG.info("Data files: " + files);
commit:5db5f3e
/////////////////////////////////////////////////////////////////////////
1: import static org.junit.Assert.fail;
1: import java.io.File;
/////////////////////////////////////////////////////////////////////////
1:         startBroker();
1:         makeAckChain();
1:         broker.stop();
1:         broker.waitUntilStopped();
1:         recoverBroker();
1:         StagedConsumer consumer = new StagedConsumer();
1:         Message message = consumer.receive(1);
1:         assertNotNull("One message stays unacked from db-1.log", message);
1:         message.acknowledge();
1:         message = consumer.receive(1);
1:         assertNull("There should not be any unconsumed messages any more", message);
1:         consumer.close();
1:     }
1:     private void makeAckChain() throws Exception {
1:         StagedConsumer consumer = new StagedConsumer();
1:         // file #1
1:         produceMessagesToConsumeMultipleDataFiles(5);
1:         // acknowledge first 2 messages and leave the 3rd one unacknowledged blocking db-1.log
1:         consumer.receive(3);
1:         // send messages by consuming and acknowledging every message right after sent in order to get KahadbAdd and Remove command to be saved together
1:         // this is necessary in order to get KahaAddMessageCommand to be saved in one db file and the corresponding KahaRemoveMessageCommand in the next one
1:         produceAndConsumeImmediately(20, consumer);
1:         consumer.receive(2).acknowledge(); // consume and ack the last 2 unconsumed
1: 
1:         // now we have 3 files written and started with #4
1:         consumer.close();
1:     }
1: 
1:     @Test
1:     public void testNoRestartOnMissingAckDataFile() throws Exception {
1:         startBroker();
1: 
1:         // reuse scenario from previous test
1:         makeAckChain();
1: 
1:         File dataDir = broker.getPersistenceAdapter().getDirectory();
1:         broker.stop();
1:         broker.waitUntilStopped();
1: 
1:         File secondLastDataFile = new File(dataDir, "db-3.log");
1:         LOG.info("Whacking data file with acks: " + secondLastDataFile);
1:         secondLastDataFile.delete();
1: 
1:         try {
1:             doStartBroker(false, false);
1:             fail("Expect failure to start with corrupt journal");
1:         } catch (IOException expected) {
1:         }
1:     }
1: 
commit:b0a1bd8
/////////////////////////////////////////////////////////////////////////
1: import static org.junit.Assert.assertNull;
/////////////////////////////////////////////////////////////////////////
1:    /**
1:     * Scenario:
1:     * db-1.log has an unacknowledged message,
1:     * db-2.log contains acks for the messages from db-1.log,
1:     * db-3.log contains acks for the messages from db-2.log
1:     *
1:     * Expected behavior: since db-1.log is blocked, db-2.log and db-3.log should not be removed during the cleanup.
1:     * Current situation on 5.10.0, 5.10.1 is that db-3.log is removed causing all messages from db-2.log, whose acks were in db-3.log, to be replayed.
1:     *
1:     * @throws Exception
1:     */
1:     public void testAckChain() throws Exception {
1:        startBroker();
1: 
1:        StagedConsumer consumer = new StagedConsumer();
0:        // file #1
0:        produceMessagesToConsumeMultipleDataFiles(5);
0:        // acknowledge first 2 messages and leave the 3rd one unacknowledged blocking db-1.log
0:        consumer.receive(3);
1: 
0:        // send messages by consuming and acknowledging every message right after sent in order to get KahadbAdd and Remove command to be saved together
0:        // this is necessary in order to get KahaAddMessageCommand to be saved in one db file and the corresponding KahaRemoveMessageCommand in the next one
0:        produceAndConsumeImmediately(20, consumer);
0:        consumer.receive(2).acknowledge(); // consume and ack the last 2 unconsumed
1: 
0:        // now we have 3 files written and started with #4
1:        consumer.close();
1: 
1:        broker.stop();
1:        broker.waitUntilStopped();
1: 
1:        recoverBroker();
1: 
1:        consumer = new StagedConsumer();
0:        Message message = consumer.receive(1);
0:        assertNotNull("One message stays unacked from db-1.log", message);
0:        message.acknowledge();
0:        message = consumer.receive(1);
0:        assertNull("There should not be any unconsumed messages any more", message);
1:        consumer.close();
1:    }
1: 
1:    private void produceAndConsumeImmediately(int numOfMsgs, StagedConsumer consumer) throws Exception {
1:       for (int i = 0; i < numOfMsgs; i++) {
1:          produceMessagesToConsumeMultipleDataFiles(1);
1:          consumer.receive(1).acknowledge();
1:       }
1:    }
1: 
1:    @Test
author:Timothy A. Bish
-------------------------------------------------------------------------------
commit:d52e591
/////////////////////////////////////////////////////////////////////////
1: import org.apache.activemq.leveldb.LevelDBStore;
1: import org.apache.activemq.store.PersistenceAdapter;
/////////////////////////////////////////////////////////////////////////
1:         PersistenceAdapter pa  = broker.getPersistenceAdapter();
1:         if (pa instanceof LevelDBStore) {
1:             return;
1:         }
/////////////////////////////////////////////////////////////////////////
1: 
commit:3bf9d0c
/////////////////////////////////////////////////////////////////////////
1: import static org.junit.Assert.assertEquals;
1: import static org.junit.Assert.assertNotNull;
1: import static org.junit.Assert.assertTrue;
1: 
1: import java.io.IOException;
1: import java.util.Collection;
1: import java.util.concurrent.TimeUnit;
/////////////////////////////////////////////////////////////////////////
1: import javax.jms.Queue;
1: import javax.jms.Topic;
1: import org.apache.activemq.ActiveMQConnectionFactory;
1: import org.apache.activemq.ActiveMQSession;
1: import org.apache.activemq.broker.BrokerService;
1: import org.apache.activemq.command.ActiveMQQueue;
1: import org.apache.activemq.command.ActiveMQTopic;
1: import org.apache.activemq.store.kahadb.KahaDBPersistenceAdapter;
1: import org.apache.activemq.store.kahadb.disk.journal.DataFile;
1: import org.apache.activemq.util.Wait;
1: import org.junit.After;
1: import org.junit.Test;
1: import org.slf4j.Logger;
1: import org.slf4j.LoggerFactory;
1:     private ActiveMQConnectionFactory cf;
1:     private String connectionUri;
1:     protected void startBroker() throws Exception {
1:         doStartBroker(true, false);
1:     }
1: 
1:     protected void restartBroker() throws Exception {
1:         if (broker != null) {
1:             broker.stop();
1:             broker.waitUntilStopped();
1:         }
1:         doStartBroker(false, false);
1:     }
1: 
1:     protected void recoverBroker() throws Exception {
1:         if (broker != null) {
1:             broker.stop();
1:             broker.waitUntilStopped();
1:         }
1:         doStartBroker(false, true);
1:     }
1: 
1:     private void doStartBroker(boolean delete, boolean recover) throws Exception {
1:         broker.setUseJmx(true);
1:         configurePersistence(broker, recover);
1: 
1:         connectionUri = "vm://localhost?create=false";
1:         cf = new ActiveMQConnectionFactory(connectionUri);
1:     protected void configurePersistence(BrokerService brokerService, boolean recover) throws Exception {
1:         adapter.setCheckpointInterval(5000);
1:         adapter.setCleanupInterval(5000);
1:         if (recover) {
1:     }
1:     @After
1:     public void tearDown() throws Exception {
1:         if (broker != null) {
1:             broker.stop();
1:             broker.waitUntilStopped();
1:         }
1:         startBroker();
/////////////////////////////////////////////////////////////////////////
1:         recoverBroker();
0:         consumer = new StagedConsumer();
/////////////////////////////////////////////////////////////////////////
1:     @Test
1:     public void testAlternateLossScenario() throws Exception {
1: 
0:         startBroker();
1: 
1:         ActiveMQQueue queue = new ActiveMQQueue("MyQueue");
1:         ActiveMQQueue disposable = new ActiveMQQueue("MyDisposableQueue");
1:         ActiveMQTopic topic = new ActiveMQTopic("MyDurableTopic");
1: 
1:         // This ensure that data file 1 never goes away.
1:         createInactiveDurableSub(topic);
1:         assertEquals(1, getNumberOfJournalFiles());
1: 
1:         // One Queue Message that will be acked in another data file.
1:         produceMessages(queue, 1);
1:         assertEquals(1, getNumberOfJournalFiles());
1: 
1:         // Add some messages to consume space
1:         produceMessages(disposable, 50);
1: 
1:         int dataFilesCount = getNumberOfJournalFiles();
1:         assertTrue(dataFilesCount > 1);
1: 
1:         // Create an ack for the single message on this queue
1:         drainQueue(queue);
1: 
1:         // Add some more messages to consume space beyond tha data file with the ack
1:         produceMessages(disposable, 50);
1: 
1:         assertTrue(dataFilesCount < getNumberOfJournalFiles());
1:         dataFilesCount = getNumberOfJournalFiles();
1: 
1:         restartBroker();
1: 
1:         // Clear out all queue data
1:         broker.getAdminView().removeQueue(disposable.getQueueName());
1: 
1:         // Once this becomes true our ack could be lost.
1:         assertTrue("Less than three journal file expected, was " + getNumberOfJournalFiles(), Wait.waitFor(new Wait.Condition() {
1:             @Override
1:             public boolean isSatisified() throws Exception {
0:                 return getNumberOfJournalFiles() <= 3;
1:             }
1:         }, TimeUnit.MINUTES.toMillis(3)));
1: 
1:         // Recover and the Message should not be replayed but if the old MessageAck is lost
1:         // then it could be.
0:         recoverBroker();
1: 
1:         assertTrue(drainQueue(queue) == 0);
1:     }
1: 
1:     private int getNumberOfJournalFiles() throws IOException {
1:         Collection<DataFile> files =
1:             ((KahaDBPersistenceAdapter) broker.getPersistenceAdapter()).getStore().getJournal().getFileMap().values();
1:         int reality = 0;
1:         for (DataFile file : files) {
1:             if (file != null) {
1:                 reality++;
1:             }
1:         }
1: 
1:         return reality;
1:     }
1: 
1:     private void createInactiveDurableSub(Topic topic) throws Exception {
1:         Connection connection = cf.createConnection();
1:         connection.setClientID("Inactive");
1:         Session session = connection.createSession(false, Session.AUTO_ACKNOWLEDGE);
1:         MessageConsumer consumer = session.createDurableSubscriber(topic, "Inactive");
1:         consumer.close();
1:         connection.close();
1:         produceMessages(topic, 1);
1:     }
1: 
1:     private int drainQueue(Queue queue) throws Exception {
1:         Connection connection = cf.createConnection();
1:         connection.setClientID("Inactive");
1:         connection.start();
1:         Session session = connection.createSession(false, Session.AUTO_ACKNOWLEDGE);
1:         MessageConsumer consumer = session.createConsumer(queue);
1:         int count = 0;
1:         while (consumer.receive(5000) != null) {
1:             count++;
1:         }
1:         consumer.close();
1:         connection.close();
1:         return count;
1:     }
1: 
1:     private int produceMessages(Destination destination, int numToSend) throws Exception {
/////////////////////////////////////////////////////////////////////////
1: 
1:     private int produceMessagesToConsumeMultipleDataFiles(int numToSend) throws Exception {
1:         return produceMessages(destination, numToSend);
1:     }
1: 
commit:ef24cc9
author:Bosanac Dejan
-------------------------------------------------------------------------------
commit:8bf987b
/////////////////////////////////////////////////////////////////////////
0: import org.slf4j.Logger;
0: import org.slf4j.LoggerFactory;
/////////////////////////////////////////////////////////////////////////
1:     private static final Logger LOG = LoggerFactory.getLogger(AMQ2832Test.class);
author:Gary Tully
-------------------------------------------------------------------------------
commit:f2517c0
/////////////////////////////////////////////////////////////////////////
1: /**
1:  * Licensed to the Apache Software Foundation (ASF) under one or more
1:  * contributor license agreements.  See the NOTICE file distributed with
1:  * this work for additional information regarding copyright ownership.
1:  * The ASF licenses this file to You under the Apache License, Version 2.0
1:  * (the "License"); you may not use this file except in compliance with
1:  * the License.  You may obtain a copy of the License at
1:  *
1:  *      http://www.apache.org/licenses/LICENSE-2.0
1:  *
1:  * Unless required by applicable law or agreed to in writing, software
1:  * distributed under the License is distributed on an "AS IS" BASIS,
1:  * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
1:  * See the License for the specific language governing permissions and
1:  * limitations under the License.
1:  */
1: package org.apache.activemq.bugs;
1: 
0: import org.apache.activemq.ActiveMQConnectionFactory;
0: import org.apache.activemq.ActiveMQSession;
0: import org.apache.activemq.broker.BrokerService;
0: import org.apache.activemq.command.ActiveMQQueue;
0: import org.apache.activemq.store.kahadb.KahaDBPersistenceAdapter;
0: import org.apache.commons.logging.Log;
0: import org.apache.commons.logging.LogFactory;
0: import org.junit.Test;
1: 
1: import javax.jms.Connection;
1: import javax.jms.Destination;
1: import javax.jms.JMSException;
1: import javax.jms.Message;
1: import javax.jms.MessageConsumer;
1: import javax.jms.MessageProducer;
1: import javax.jms.Session;
1: 
0: import static org.junit.Assert.assertEquals;
0: import static org.junit.Assert.assertNotNull;
1: 
1: public class AMQ2832Test {
1: 
0:     private static final Log LOG = LogFactory.getLog(AMQ2832Test.class);
1: 
1:     BrokerService broker = null;
1:     private final Destination destination = new ActiveMQQueue("AMQ2832Test");
1: 
0:     protected void startBroker(boolean delete) throws Exception {
1:         broker = new BrokerService();
1:         broker.setDeleteAllMessagesOnStartup(delete);
1:         broker.setPersistent(true);
0:         broker.setUseJmx(false);
1:         broker.addConnector("tcp://localhost:0");
1: 
0:         configurePersistence(broker, delete);
1: 
1:         broker.start();
1:         LOG.info("Starting broker..");
1:     }
1: 
0:     protected void configurePersistence(BrokerService brokerService, boolean deleteAllOnStart) throws Exception {
1:         KahaDBPersistenceAdapter adapter = (KahaDBPersistenceAdapter) brokerService.getPersistenceAdapter();
1: 
1:         // ensure there are a bunch of data files but multiple entries in each
1:         adapter.setJournalMaxFileLength(1024 * 20);
1: 
1:         // speed up the test case, checkpoint an cleanup early and often
0:         adapter.setCheckpointInterval(500);
0:         adapter.setCleanupInterval(500);
1: 
0:         if (!deleteAllOnStart) {
1:             adapter.setForceRecoverIndex(true);
1:         }
1: 
1:     }
1: 
1:     @Test
1:     public void testAckRemovedMessageReplayedAfterRecovery() throws Exception {
1: 
0:         startBroker(true);
1: 
0:         StagedConsumer consumer = new StagedConsumer();
1:         int numMessagesAvailable = produceMessagesToConsumeMultipleDataFiles(20);
1:         // this will block the reclaiming of one data file
1:         Message firstUnacked = consumer.receive(10);
1:         LOG.info("first unacked: " + firstUnacked.getJMSMessageID());
1:         Message secondUnacked = consumer.receive(1);
1:         LOG.info("second unacked: " + secondUnacked.getJMSMessageID());
1:         numMessagesAvailable -= 11;
1: 
1:         numMessagesAvailable += produceMessagesToConsumeMultipleDataFiles(10);
1:         // ensure ack is another data file
1:         LOG.info("Acking firstUnacked: " + firstUnacked.getJMSMessageID());
1:         firstUnacked.acknowledge();
1: 
1:         numMessagesAvailable += produceMessagesToConsumeMultipleDataFiles(10);
1: 
1:         consumer.receive(numMessagesAvailable).acknowledge();
1: 
1:         // second unacked should keep first data file available but journal with the first ack
1:         // may get whacked
1:         consumer.close();
1: 
0:         broker.stop();
0:         broker.waitUntilStopped();
1: 
0:         startBroker(false);
1: 
0:         consumer = new StagedConsumer();     
1:         // need to force recovery?
1: 
1:         Message msg = consumer.receive(1, 5);
1:         assertNotNull("One messages left after recovery", msg);
1:         msg.acknowledge();
1: 
1:         // should be no more messages
1:         msg = consumer.receive(1, 5);
1:         assertEquals("Only one messages left after recovery: " + msg, null, msg);
1:         consumer.close();
1: 
1:     }
1: 
0:     private int produceMessagesToConsumeMultipleDataFiles(int numToSend) throws Exception {
1:         int sent = 0;
1:         Connection connection = new ActiveMQConnectionFactory(
1:                 broker.getTransportConnectors().get(0).getConnectUri()).createConnection();
1:         connection.start();
1:         try {
1:             Session session = connection.createSession(false, Session.AUTO_ACKNOWLEDGE);
1:             MessageProducer producer = session.createProducer(destination);
1:             for (int i = 0; i < numToSend; i++) {
1:                 producer.send(createMessage(session, i));
1:                 sent++;
1:             }
1:         } finally {
1:             connection.close();
1:         }
1:         
1:         return sent;
1:     }
1: 
1:     final String payload = new String(new byte[1024]);
1: 
1:     private Message createMessage(Session session, int i) throws Exception {
1:         return session.createTextMessage(payload + "::" + i);
1:     }
1: 
1:     private class StagedConsumer {
1:         Connection connection;
1:         MessageConsumer consumer;
1: 
1:         StagedConsumer() throws Exception {
1:             connection = new ActiveMQConnectionFactory("failover://" +
1:                     broker.getTransportConnectors().get(0).getConnectUri().toString()).createConnection();
1:             connection.start();
1:             consumer = connection.createSession(false, ActiveMQSession.INDIVIDUAL_ACKNOWLEDGE).createConsumer(destination);
1:         }
1: 
1:         public Message receive(int numToReceive) throws Exception {
1:             return receive(numToReceive, 2);
1:         }
1: 
1:         public Message receive(int numToReceive, int timeoutInSeconds) throws Exception {
1:             Message msg = null;
1:             for (; numToReceive > 0; numToReceive--) {
1: 
1:                 do  {
1:                     msg = consumer.receive(1*1000);
1:                 } while (msg == null && --timeoutInSeconds > 0);
1: 
1:                 if (numToReceive > 1) {
1:                     msg.acknowledge();
1:                 }
1: 
1:                 if (msg != null) {
1:                     LOG.debug("received: " + msg.getJMSMessageID());
1:                 }
1:             }
1:             // last message, unacked
1:             return msg;
1:         }
1: 
1:         void close() throws JMSException {
1:             consumer.close();
1:             connection.close();
1:         }
1:     }
1: }
============================================================================