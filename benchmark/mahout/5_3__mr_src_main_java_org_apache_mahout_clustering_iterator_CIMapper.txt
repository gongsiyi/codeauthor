1:1cdd095: /*
1:1cdd095:  * Licensed to the Apache Software Foundation (ASF) under one or more
1:1cdd095:  * contributor license agreements.  See the NOTICE file distributed with
1:1cdd095:  * this work for additional information regarding copyright ownership.
1:1cdd095:  * The ASF licenses this file to You under the Apache License, Version 2.0
1:1cdd095:  * (the "License"); you may not use this file except in compliance with
1:1cdd095:  * the License.  You may obtain a copy of the License at
1:1cdd095:  *
1:1cdd095:  *     http://www.apache.org/licenses/LICENSE-2.0
1:1cdd095:  *
1:1cdd095:  * Unless required by applicable law or agreed to in writing, software
1:1cdd095:  * distributed under the License is distributed on an "AS IS" BASIS,
1:1cdd095:  * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
1:1cdd095:  * See the License for the specific language governing permissions and
1:1cdd095:  * limitations under the License.
1:1cdd095:  */
1:1cdd095: 
1:8d102ea: package org.apache.mahout.clustering.iterator;
1:529b114: 
1:dc637e8: import java.io.IOException;
1:dc637e8: import java.util.List;
1:529b114: 
1:590ffed: import org.apache.hadoop.conf.Configuration;
1:2153bb9: import org.apache.hadoop.fs.Path;
1:dc637e8: import org.apache.hadoop.io.IntWritable;
1:dc637e8: import org.apache.hadoop.io.WritableComparable;
1:dc637e8: import org.apache.hadoop.mapreduce.Mapper;
1:8d102ea: import org.apache.mahout.clustering.Cluster;
1:8d102ea: import org.apache.mahout.clustering.classify.ClusterClassifier;
1:dc637e8: import org.apache.mahout.math.Vector;
1:dc637e8: import org.apache.mahout.math.Vector.Element;
1:dc637e8: import org.apache.mahout.math.VectorWritable;
1:529b114: 
1:529b114: public class CIMapper extends Mapper<WritableComparable<?>,VectorWritable,IntWritable,ClusterWritable> {
1:529b114:   
1:dc637e8:   private ClusterClassifier classifier;
1:dc637e8:   private ClusteringPolicy policy;
1:229aeff: 
1:dc637e8:   @Override
1:2153bb9:   protected void setup(Context context) throws IOException, InterruptedException {
1:590ffed:     Configuration conf = context.getConfiguration();
1:590ffed:     String priorClustersPath = conf.get(ClusterIterator.PRIOR_PATH_KEY);
1:76e80dc:     classifier = new ClusterClassifier();
1:590ffed:     classifier.readFromSeqFiles(conf, new Path(priorClustersPath));
1:76e80dc:     policy = classifier.getPolicy();
1:1ac30a4:     policy.update(classifier);
1:dc637e8:     super.setup(context);
1:dc637e8:   }
1:229aeff: 
1:dc637e8:   @Override
1:2153bb9:   protected void map(WritableComparable<?> key, VectorWritable value, Context context) throws IOException,
1:2153bb9:       InterruptedException {
1:dc637e8:     Vector probabilities = classifier.classify(value.get());
1:dc637e8:     Vector selections = policy.select(probabilities);
1:dc62944:     for (Element el : selections.nonZeroes()) {
1:dc637e8:       classifier.train(el.index(), value.get(), el.get());
1:dc637e8:     }
1:dc637e8:   }
1:229aeff: 
1:dc637e8:   @Override
1:2153bb9:   protected void cleanup(Context context) throws IOException, InterruptedException {
1:dc637e8:     List<Cluster> clusters = classifier.getModels();
1:2153bb9:     ClusterWritable cw = new ClusterWritable();
1:dc637e8:     for (int index = 0; index < clusters.size(); index++) {
1:2153bb9:       cw.setValue(clusters.get(index));
1:2153bb9:       context.write(new IntWritable(index), cw);
1:dc637e8:     }
1:dc637e8:     super.cleanup(context);
1:dc637e8:   }
1:529b114:   
1:dc637e8: }
============================================================================
author:pferrel
-------------------------------------------------------------------------------
commit:b988c49
author:frankscholten
-------------------------------------------------------------------------------
commit:1a42d85
author:Sean R. Owen
-------------------------------------------------------------------------------
commit:1cdd095
/////////////////////////////////////////////////////////////////////////
1: /*
1:  * Licensed to the Apache Software Foundation (ASF) under one or more
1:  * contributor license agreements.  See the NOTICE file distributed with
1:  * this work for additional information regarding copyright ownership.
1:  * The ASF licenses this file to You under the Apache License, Version 2.0
1:  * (the "License"); you may not use this file except in compliance with
1:  * the License.  You may obtain a copy of the License at
1:  *
1:  *     http://www.apache.org/licenses/LICENSE-2.0
1:  *
1:  * Unless required by applicable law or agreed to in writing, software
1:  * distributed under the License is distributed on an "AS IS" BASIS,
1:  * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
1:  * See the License for the specific language governing permissions and
1:  * limitations under the License.
1:  */
1: 
commit:229aeff
/////////////////////////////////////////////////////////////////////////
1: 
/////////////////////////////////////////////////////////////////////////
1: 
/////////////////////////////////////////////////////////////////////////
1: 
author:Jacob Alexander Mannix
-------------------------------------------------------------------------------
commit:dc62944
/////////////////////////////////////////////////////////////////////////
/////////////////////////////////////////////////////////////////////////
1:     for (Element el : selections.nonZeroes()) {
author:Jeff Eastman
-------------------------------------------------------------------------------
commit:590ffed
/////////////////////////////////////////////////////////////////////////
1: import org.apache.hadoop.conf.Configuration;
/////////////////////////////////////////////////////////////////////////
1:     Configuration conf = context.getConfiguration();
1:     String priorClustersPath = conf.get(ClusterIterator.PRIOR_PATH_KEY);
1:     classifier.readFromSeqFiles(conf, new Path(priorClustersPath));
commit:8d102ea
/////////////////////////////////////////////////////////////////////////
1: package org.apache.mahout.clustering.iterator;
/////////////////////////////////////////////////////////////////////////
1: import org.apache.mahout.clustering.Cluster;
1: import org.apache.mahout.clustering.classify.ClusterClassifier;
commit:76e80dc
/////////////////////////////////////////////////////////////////////////
1:     classifier = new ClusterClassifier();
0:     classifier.readFromSeqFiles(new Path(priorClustersPath));
1:     policy = classifier.getPolicy();
commit:1ac30a4
/////////////////////////////////////////////////////////////////////////
1:     policy.update(classifier);
commit:a28cf1d
/////////////////////////////////////////////////////////////////////////
0:     String policyPath = context.getConfiguration().get(ClusterIterator.POLICY_PATH_KEY);
0:     policy = ClusterIterator.readPolicy(new Path(policyPath));
commit:529b114
/////////////////////////////////////////////////////////////////////////
1: public class CIMapper extends Mapper<WritableComparable<?>,VectorWritable,IntWritable,ClusterWritable> {
1:   
1:   
0:    * @see
0:    * org.apache.hadoop.mapreduce.Mapper#setup(org.apache.hadoop.mapreduce.Mapper
0:    * .Context)
/////////////////////////////////////////////////////////////////////////
1:   
0:    * @see org.apache.hadoop.mapreduce.Mapper#map(java.lang.Object,
0:    * java.lang.Object, org.apache.hadoop.mapreduce.Mapper.Context)
/////////////////////////////////////////////////////////////////////////
1:   
0:    * @see
0:    * org.apache.hadoop.mapreduce.Mapper#cleanup(org.apache.hadoop.mapreduce.
0:    * Mapper.Context)
/////////////////////////////////////////////////////////////////////////
1:   
commit:2153bb9
/////////////////////////////////////////////////////////////////////////
1: import org.apache.hadoop.fs.Path;
/////////////////////////////////////////////////////////////////////////
0:     Mapper<WritableComparable<?>,VectorWritable,IntWritable,ClusterWritable> {
0: 
0: 
0:    * @see org.apache.hadoop.mapreduce.Mapper#setup(org.apache.hadoop.mapreduce.Mapper .Context)
1:   protected void setup(Context context) throws IOException, InterruptedException {
0:     String priorClustersPath = context.getConfiguration().get(ClusterIterator.PRIOR_PATH_KEY);
0:     classifier = ClusterIterator.readClassifier(new Path(priorClustersPath));
0: 
0:    * @see org.apache.hadoop.mapreduce.Mapper#map(java.lang.Object, java.lang.Object,
0:    * org.apache.hadoop.mapreduce.Mapper.Context)
1:   protected void map(WritableComparable<?> key, VectorWritable value, Context context) throws IOException,
1:       InterruptedException {
/////////////////////////////////////////////////////////////////////////
0: 
0:    * @see org.apache.hadoop.mapreduce.Mapper#cleanup(org.apache.hadoop.mapreduce. Mapper.Context)
1:   protected void cleanup(Context context) throws IOException, InterruptedException {
1:     ClusterWritable cw = new ClusterWritable();
1:       cw.setValue(clusters.get(index));
1:       context.write(new IntWritable(index), cw);
0: 
commit:dc637e8
/////////////////////////////////////////////////////////////////////////
0: package org.apache.mahout.clustering;
0: 
1: import java.io.IOException;
0: import java.util.Iterator;
1: import java.util.List;
0: 
1: import org.apache.hadoop.io.IntWritable;
1: import org.apache.hadoop.io.WritableComparable;
1: import org.apache.hadoop.mapreduce.Mapper;
1: import org.apache.mahout.math.Vector;
1: import org.apache.mahout.math.Vector.Element;
1: import org.apache.mahout.math.VectorWritable;
0: 
0: public class CIMapper extends
0:     Mapper<WritableComparable<?>,VectorWritable,IntWritable,Cluster> {
0:   
1:   private ClusterClassifier classifier;
1:   private ClusteringPolicy policy;
0:   
0:   /*
0:    * (non-Javadoc)
0:    * 
0:    * @see
0:    * org.apache.hadoop.mapreduce.Mapper#setup(org.apache.hadoop.mapreduce.Mapper
0:    * .Context)
0:    */
1:   @Override
0:   protected void setup(Context context) throws IOException,
0:       InterruptedException {
0:     List<Cluster> models = null;
0:     classifier = new ClusterClassifier(models);
0:     policy = new KMeansClusteringPolicy();
1:     super.setup(context);
1:   }
0:   
0:   /*
0:    * (non-Javadoc)
0:    * 
0:    * @see org.apache.hadoop.mapreduce.Mapper#map(java.lang.Object,
0:    * java.lang.Object, org.apache.hadoop.mapreduce.Mapper.Context)
0:    */
1:   @Override
0:   protected void map(WritableComparable<?> key, VectorWritable value,
0:       Context context) throws IOException, InterruptedException {
1:     Vector probabilities = classifier.classify(value.get());
1:     Vector selections = policy.select(probabilities);
0:     for (Iterator<Element> it = selections.iterateNonZero(); it.hasNext();) {
0:       Element el = it.next();
1:       classifier.train(el.index(), value.get(), el.get());
1:     }
1:   }
0:   
0:   /*
0:    * (non-Javadoc)
0:    * 
0:    * @see
0:    * org.apache.hadoop.mapreduce.Mapper#cleanup(org.apache.hadoop.mapreduce.
0:    * Mapper.Context)
0:    */
1:   @Override
0:   protected void cleanup(Context context) throws IOException,
0:       InterruptedException {
1:     List<Cluster> clusters = classifier.getModels();
1:     for (int index = 0; index < clusters.size(); index++) {
0:       context.write(new IntWritable(index), clusters.get(index));
1:     }
1:     super.cleanup(context);
1:   }
0:   
1: }
============================================================================