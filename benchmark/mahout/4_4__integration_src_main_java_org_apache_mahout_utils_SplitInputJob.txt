5:e6a308b: /**
1:e6a308b:  * Licensed to the Apache Software Foundation (ASF) under one or more
1:e6a308b:  * contributor license agreements.  See the NOTICE file distributed with
1:e6a308b:  * this work for additional information regarding copyright ownership.
1:e6a308b:  * The ASF licenses this file to You under the Apache License, Version 2.0
1:e6a308b:  * (the "License"); you may not use this file except in compliance with
1:e6a308b:  * the License.  You may obtain a copy of the License at
1:e6a308b:  *
1:e6a308b:  *     http://www.apache.org/licenses/LICENSE-2.0
1:e6a308b:  *
1:e6a308b:  * Unless required by applicable law or agreed to in writing, software
1:e6a308b:  * distributed under the License is distributed on an "AS IS" BASIS,
1:e6a308b:  * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
1:e6a308b:  * See the License for the specific language governing permissions and
1:e6a308b:  * limitations under the License.
5:e6a308b:  */
1:e6a308b: 
1:e6a308b: package org.apache.mahout.utils;
1:e6a308b: 
1:e6a308b: import java.io.IOException;
1:bdb1c48: import java.io.Serializable;
1:e6a308b: import java.util.Random;
1:e6a308b: 
1:e6a308b: import org.apache.hadoop.conf.Configuration;
1:e6a308b: import org.apache.hadoop.fs.FileSystem;
1:e6a308b: import org.apache.hadoop.fs.Path;
1:e6a308b: import org.apache.hadoop.io.Writable;
1:e6a308b: import org.apache.hadoop.io.WritableComparable;
1:e6a308b: import org.apache.hadoop.io.WritableComparator;
1:e6a308b: import org.apache.hadoop.mapreduce.Job;
1:e6a308b: import org.apache.hadoop.mapreduce.Mapper;
1:e6a308b: import org.apache.hadoop.mapreduce.Reducer;
1:e6a308b: import org.apache.hadoop.mapreduce.lib.input.FileInputFormat;
1:e6a308b: import org.apache.hadoop.mapreduce.lib.input.SequenceFileInputFormat;
1:e6a308b: import org.apache.hadoop.mapreduce.lib.output.FileOutputFormat;
1:9f036af: import org.apache.hadoop.mapreduce.lib.output.MultipleOutputs;
1:e6a308b: import org.apache.hadoop.mapreduce.lib.output.SequenceFileOutputFormat;
1:e6a308b: import org.apache.mahout.common.Pair;
1:e6a308b: import org.apache.mahout.common.RandomUtils;
1:e6a308b: import org.apache.mahout.common.iterator.sequencefile.PathFilters;
1:e6a308b: import org.apache.mahout.common.iterator.sequencefile.PathType;
1:e6a308b: import org.apache.mahout.common.iterator.sequencefile.SequenceFileDirIterator;
1:e6a308b: 
1:e6a308b: /**
1:e6a308b:  * Class which implements a map reduce version of SplitInput.
1:e6a308b:  * This class takes a SequenceFile input, e.g. a set of training data
1:e6a308b:  * for a learning algorithm, downsamples it, applies a random
1:e6a308b:  * permutation and splits it into test and training sets
1:e6a308b:  */
1:822a5e1: public final class SplitInputJob {
1:e6a308b: 
1:9f036af:   private static final String DOWNSAMPLING_FACTOR = "SplitInputJob.downsamplingFactor";
1:9f036af:   private static final String RANDOM_SELECTION_PCT = "SplitInputJob.randomSelectionPct";
1:e6a308b:   private static final String TRAINING_TAG = "training";
1:e6a308b:   private static final String TEST_TAG = "test";
1:822a5e1: 
1:9f036af:   private SplitInputJob() {}
1:e6a308b: 
1:e6a308b:   /**
1:e6a308b:    * Run job to downsample, randomly permute and split data into test and
1:e6a308b:    * training sets. This job takes a SequenceFile as input and outputs two
1:e6a308b:    * SequenceFiles test-r-00000 and training-r-00000 which contain the test and
1:e6a308b:    * training sets respectively
1:e6a308b:    *
1:e6a308b:    * @param initialConf
1:85f9ece:    *          Initial configuration
1:e6a308b:    * @param inputPath
1:e6a308b:    *          path to input data SequenceFile
1:e6a308b:    * @param outputPath
1:e6a308b:    *          path for output data SequenceFiles
1:e6a308b:    * @param keepPct
1:e6a308b:    *          percentage of key value pairs in input to keep. The rest are
1:e6a308b:    *          discarded
1:e6a308b:    * @param randomSelectionPercent
1:e6a308b:    *          percentage of key value pairs to allocate to test set. Remainder
1:e6a308b:    *          are allocated to training set
1:e6a308b:    */
1:e6a308b:   @SuppressWarnings("rawtypes")
1:e6a308b:   public static void run(Configuration initialConf, Path inputPath,
1:e6a308b:       Path outputPath, int keepPct, float randomSelectionPercent)
1:6d16230:     throws IOException, ClassNotFoundException, InterruptedException {
1:e6a308b: 
1:822a5e1:     int downsamplingFactor = (int) (100.0 / keepPct);
1:e6a308b:     initialConf.setInt(DOWNSAMPLING_FACTOR, downsamplingFactor);
1:e6a308b:     initialConf.setFloat(RANDOM_SELECTION_PCT, randomSelectionPercent);
1:e6a308b: 
1:e6a308b:     // Determine class of keys and values
1:e6a308b:     FileSystem fs = FileSystem.get(initialConf);
1:e6a308b: 
1:e6a308b:     SequenceFileDirIterator<? extends WritableComparable, Writable> iterator =
1:87c15be:         new SequenceFileDirIterator<>(inputPath,
1:e6a308b:             PathType.LIST, PathFilters.partFilter(), null, false, fs.getConf());
1:822a5e1:     Class<? extends WritableComparable> keyClass;
1:822a5e1:     Class<? extends Writable> valueClass;
1:e6a308b:     if (iterator.hasNext()) {
1:e6a308b:       Pair<? extends WritableComparable, Writable> pair = iterator.next();
1:e6a308b:       keyClass = pair.getFirst().getClass();
1:e6a308b:       valueClass = pair.getSecond().getClass();
1:e6a308b:     } else {
1:822a5e1:       throw new IllegalStateException("Couldn't determine class of the input values");
1:822a5e1:     }
1:e6a308b: 
1:9f036af:     Job job = new Job(new Configuration(initialConf));
1:9f036af: 
1:9f036af:     MultipleOutputs.addNamedOutput(job, TRAINING_TAG, SequenceFileOutputFormat.class, keyClass, valueClass);
1:9f036af:     MultipleOutputs.addNamedOutput(job, TEST_TAG, SequenceFileOutputFormat.class, keyClass, valueClass);
1:1250c23:     job.setJarByClass(SplitInputJob.class);
1:e6a308b:     FileInputFormat.addInputPath(job, inputPath);
1:e6a308b:     FileOutputFormat.setOutputPath(job, outputPath);
1:e6a308b:     job.setNumReduceTasks(1);
1:e6a308b:     job.setInputFormatClass(SequenceFileInputFormat.class);
1:e6a308b:     job.setOutputFormatClass(SequenceFileOutputFormat.class);
1:e6a308b:     job.setMapperClass(SplitInputMapper.class);
1:e6a308b:     job.setReducerClass(SplitInputReducer.class);
1:e6a308b:     job.setSortComparatorClass(SplitInputComparator.class);
1:e6a308b:     job.setOutputKeyClass(keyClass);
1:e6a308b:     job.setOutputValueClass(valueClass);
1:822a5e1:     job.submit();
1:7c2b664:     boolean succeeded = job.waitForCompletion(true);
1:7c2b664:     if (!succeeded) {
1:7c2b664:       throw new IllegalStateException("Job failed!");
1:7c2b664:     }
3:e6a308b:   }
1:e6a308b: 
1:9f036af:   /** Mapper which downsamples the input by downsamplingFactor */
1:e6a308b:   public static class SplitInputMapper extends
1:e6a308b:       Mapper<WritableComparable<?>, Writable, WritableComparable<?>, Writable> {
1:e6a308b: 
1:e6a308b:     private int downsamplingFactor;
1:e6a308b: 
1:e6a308b:     @Override
1:9f036af:     public void setup(Context ctx) {
1:9f036af:       downsamplingFactor = ctx.getConfiguration().getInt(DOWNSAMPLING_FACTOR, 1);
1:e6a308b:     }
1:e6a308b: 
1:9f036af:     /** Only run map() for one out of every downsampleFactor inputs */
1:e6a308b:     @Override
1:e6a308b:     public void run(Context context) throws IOException, InterruptedException {
1:e6a308b:       setup(context);
1:229aeff:       int i = 0;
1:229aeff:       while (context.nextKeyValue()) {
1:e6a308b:         if (i % downsamplingFactor == 0) {
1:e6a308b:           map(context.getCurrentKey(), context.getCurrentValue(), context);
1:e6a308b:         }
1:229aeff:         i++;
1:e6a308b:       }
1:e6a308b:       cleanup(context);
1:e6a308b:     }
1:e6a308b: 
1:e6a308b:   }
1:e6a308b: 
1:9f036af:   /** Reducer which uses MultipleOutputs to randomly allocate key value pairs between test and training outputs */
1:e6a308b:   public static class SplitInputReducer extends
1:e6a308b:       Reducer<WritableComparable<?>, Writable, WritableComparable<?>, Writable> {
1:e6a308b: 
1:e6a308b:     private MultipleOutputs multipleOutputs;
1:e6a308b:     private final Random rnd = RandomUtils.getRandom();
1:e6a308b:     private float randomSelectionPercent;
1:e6a308b: 
1:e6a308b:     @Override
1:9f036af:     protected void setup(Context ctx) throws IOException {
1:9f036af:       randomSelectionPercent = ctx.getConfiguration().getFloat(RANDOM_SELECTION_PCT, 0);
1:9f036af:       multipleOutputs = new MultipleOutputs(ctx);
1:e6a308b:     }
1:e6a308b: 
1:e6a308b:     /**
1:e6a308b:      * Randomly allocate key value pairs between test and training sets.
1:e6a308b:      * randomSelectionPercent of the pairs will go to the test set.
1:e6a308b:      */
1:e6a308b:     @Override
1:e6a308b:     protected void reduce(WritableComparable<?> key, Iterable<Writable> values,
1:e6a308b:         Context context) throws IOException, InterruptedException {
1:e6a308b:       for (Writable value : values) {
1:e6a308b:         if (rnd.nextInt(100) < randomSelectionPercent) {
1:9f036af:           multipleOutputs.write(TEST_TAG, key, value);
1:e6a308b:         } else {
1:9f036af:           multipleOutputs.write(TRAINING_TAG, key, value);
1:e6a308b:         }
1:e6a308b:       }
1:e6a308b: 
1:e6a308b:     }
1:e6a308b: 
1:e6a308b:     @Override
1:e6a308b:     protected void cleanup(Context context) throws IOException {
1:9f036af:       try {
1:9f036af:         multipleOutputs.close();
1:9f036af:       } catch (InterruptedException e) {
1:9f036af:         throw new IOException(e);
1:9f036af:       }
1:e6a308b:     }
1:e6a308b: 
1:e6a308b:   }
1:e6a308b: 
1:9f036af:   /** Randomly permute key value pairs */
1:bdb1c48:   public static class SplitInputComparator extends WritableComparator implements Serializable {
1:e6a308b: 
1:e6a308b:     private final Random rnd = RandomUtils.getRandom();
1:e6a308b: 
1:e6a308b:     protected SplitInputComparator() {
1:e6a308b:       super(WritableComparable.class);
1:e6a308b:     }
1:e6a308b: 
1:e6a308b:     @Override
1:e6a308b:     public int compare(byte[] b1, int s1, int l1, byte[] b2, int s2, int l2) {
1:e6a308b:       if (rnd.nextBoolean()) {
1:e6a308b:         return 1;
1:e6a308b:       } else {
1:e6a308b:         return -1;
1:e6a308b:       }
1:e6a308b:     }
1:e6a308b:   }
1:e6a308b: 
1:e6a308b: }
============================================================================
author:Suneel Marthi
-------------------------------------------------------------------------------
commit:85f9ece
/////////////////////////////////////////////////////////////////////////
1:    *          Initial configuration
commit:87c15be
/////////////////////////////////////////////////////////////////////////
1:         new SequenceFileDirIterator<>(inputPath,
author:Sebastian Schelter
-------------------------------------------------------------------------------
commit:9f036af
/////////////////////////////////////////////////////////////////////////
1: import org.apache.hadoop.mapreduce.lib.output.MultipleOutputs;
/////////////////////////////////////////////////////////////////////////
/////////////////////////////////////////////////////////////////////////
1:   private static final String DOWNSAMPLING_FACTOR = "SplitInputJob.downsamplingFactor";
1:   private static final String RANDOM_SELECTION_PCT = "SplitInputJob.randomSelectionPct";
1:   private SplitInputJob() {}
/////////////////////////////////////////////////////////////////////////
1:     Job job = new Job(new Configuration(initialConf));
1: 
1:     MultipleOutputs.addNamedOutput(job, TRAINING_TAG, SequenceFileOutputFormat.class, keyClass, valueClass);
1:     MultipleOutputs.addNamedOutput(job, TEST_TAG, SequenceFileOutputFormat.class, keyClass, valueClass);
/////////////////////////////////////////////////////////////////////////
1:   /** Mapper which downsamples the input by downsamplingFactor */
1:     public void setup(Context ctx) {
1:       downsamplingFactor = ctx.getConfiguration().getInt(DOWNSAMPLING_FACTOR, 1);
1:     /** Only run map() for one out of every downsampleFactor inputs */
/////////////////////////////////////////////////////////////////////////
1:   /** Reducer which uses MultipleOutputs to randomly allocate key value pairs between test and training outputs */
1:     protected void setup(Context ctx) throws IOException {
1:       randomSelectionPercent = ctx.getConfiguration().getFloat(RANDOM_SELECTION_PCT, 0);
1:       multipleOutputs = new MultipleOutputs(ctx);
/////////////////////////////////////////////////////////////////////////
1:           multipleOutputs.write(TEST_TAG, key, value);
1:           multipleOutputs.write(TRAINING_TAG, key, value);
/////////////////////////////////////////////////////////////////////////
1:       try {
1:         multipleOutputs.close();
1:       } catch (InterruptedException e) {
1:         throw new IOException(e);
1:       }
1:   /** Randomly permute key value pairs */
commit:bdb1c48
/////////////////////////////////////////////////////////////////////////
1: import java.io.Serializable;
/////////////////////////////////////////////////////////////////////////
1:   public static class SplitInputComparator extends WritableComparator implements Serializable {
commit:6d16230
/////////////////////////////////////////////////////////////////////////
1:     throws IOException, ClassNotFoundException, InterruptedException {
commit:1250c23
/////////////////////////////////////////////////////////////////////////
1:     job.setJarByClass(SplitInputJob.class);
author:Sean R. Owen
-------------------------------------------------------------------------------
commit:229aeff
/////////////////////////////////////////////////////////////////////////
1:       int i = 0;
1:       while (context.nextKeyValue()) {
1:         i++;
commit:7c2b664
/////////////////////////////////////////////////////////////////////////
1:     boolean succeeded = job.waitForCompletion(true);
1:     if (!succeeded) {
1:       throw new IllegalStateException("Job failed!");
1:     }
commit:822a5e1
/////////////////////////////////////////////////////////////////////////
1: public final class SplitInputJob {
/////////////////////////////////////////////////////////////////////////
0:   private SplitInputJob() {
1:   }
1: 
/////////////////////////////////////////////////////////////////////////
0:       throws IOException, ClassNotFoundException, InterruptedException {
1:     int downsamplingFactor = (int) (100.0 / keepPct);
/////////////////////////////////////////////////////////////////////////
1:     Class<? extends WritableComparable> keyClass;
1:     Class<? extends Writable> valueClass;
1:       throw new IllegalStateException("Couldn't determine class of the input values");
/////////////////////////////////////////////////////////////////////////
1:     job.submit();
0:     job.waitForCompletion(true);
author:Grant Ingersoll
-------------------------------------------------------------------------------
commit:e6a308b
/////////////////////////////////////////////////////////////////////////
1: /**
1:  * Licensed to the Apache Software Foundation (ASF) under one or more
1:  * contributor license agreements.  See the NOTICE file distributed with
1:  * this work for additional information regarding copyright ownership.
1:  * The ASF licenses this file to You under the Apache License, Version 2.0
1:  * (the "License"); you may not use this file except in compliance with
1:  * the License.  You may obtain a copy of the License at
1:  *
1:  *     http://www.apache.org/licenses/LICENSE-2.0
1:  *
1:  * Unless required by applicable law or agreed to in writing, software
1:  * distributed under the License is distributed on an "AS IS" BASIS,
1:  * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
1:  * See the License for the specific language governing permissions and
1:  * limitations under the License.
1:  */
1: 
1: package org.apache.mahout.utils;
1: 
1: import java.io.IOException;
1: import java.util.Random;
1: 
1: import org.apache.hadoop.conf.Configuration;
1: import org.apache.hadoop.fs.FileSystem;
1: import org.apache.hadoop.fs.Path;
1: import org.apache.hadoop.io.Writable;
1: import org.apache.hadoop.io.WritableComparable;
1: import org.apache.hadoop.io.WritableComparator;
0: import org.apache.hadoop.mapred.JobConf;
0: import org.apache.hadoop.mapred.OutputCollector;
0: import org.apache.hadoop.mapred.lib.MultipleOutputs;
1: import org.apache.hadoop.mapreduce.Job;
1: import org.apache.hadoop.mapreduce.Mapper;
1: import org.apache.hadoop.mapreduce.Reducer;
1: import org.apache.hadoop.mapreduce.lib.input.FileInputFormat;
1: import org.apache.hadoop.mapreduce.lib.input.SequenceFileInputFormat;
1: import org.apache.hadoop.mapreduce.lib.output.FileOutputFormat;
1: import org.apache.hadoop.mapreduce.lib.output.SequenceFileOutputFormat;
1: import org.apache.mahout.common.Pair;
1: import org.apache.mahout.common.RandomUtils;
1: import org.apache.mahout.common.iterator.sequencefile.PathFilters;
1: import org.apache.mahout.common.iterator.sequencefile.PathType;
1: import org.apache.mahout.common.iterator.sequencefile.SequenceFileDirIterator;
1: 
0: @SuppressWarnings("deprecation")
1: /**
1:  * Class which implements a map reduce version of SplitInput.
1:  * This class takes a SequenceFile input, e.g. a set of training data
1:  * for a learning algorithm, downsamples it, applies a random
1:  * permutation and splits it into test and training sets
1:  */
0: public class SplitInputJob {
1: 
0:   private static final String DOWNSAMPLING_FACTOR =
0:       "SplitInputJob.downsamplingFactor";
0:   private static final String RANDOM_SELECTION_PCT =
0:       "SplitInputJob.randomSelectionPct";
1:   private static final String TRAINING_TAG = "training";
1:   private static final String TEST_TAG = "test";
1: 
1:   /**
1:    * Run job to downsample, randomly permute and split data into test and
1:    * training sets. This job takes a SequenceFile as input and outputs two
1:    * SequenceFiles test-r-00000 and training-r-00000 which contain the test and
1:    * training sets respectively
1:    *
1:    * @param initialConf
1:    * @param inputPath
1:    *          path to input data SequenceFile
1:    * @param outputPath
1:    *          path for output data SequenceFiles
1:    * @param keepPct
1:    *          percentage of key value pairs in input to keep. The rest are
1:    *          discarded
1:    * @param randomSelectionPercent
1:    *          percentage of key value pairs to allocate to test set. Remainder
1:    *          are allocated to training set
1:    */
1:   @SuppressWarnings("rawtypes")
1:   public static void run(Configuration initialConf, Path inputPath,
1:       Path outputPath, int keepPct, float randomSelectionPercent)
0:       throws IOException {
1: 
0:     int downsamplingFactor = (int) (100.0f / keepPct);
1:     initialConf.setInt(DOWNSAMPLING_FACTOR, downsamplingFactor);
1:     initialConf.setFloat(RANDOM_SELECTION_PCT, randomSelectionPercent);
1: 
1:     // Determine class of keys and values
1:     FileSystem fs = FileSystem.get(initialConf);
1: 
1:     SequenceFileDirIterator<? extends WritableComparable, Writable> iterator =
0:         new SequenceFileDirIterator<WritableComparable, Writable>(inputPath,
1:             PathType.LIST, PathFilters.partFilter(), null, false, fs.getConf());
0:     Class<? extends WritableComparable> keyClass = null;
0:     Class<? extends Writable> valueClass = null;
1:     if (iterator.hasNext()) {
1:       Pair<? extends WritableComparable, Writable> pair = iterator.next();
1:       keyClass = pair.getFirst().getClass();
1:       valueClass = pair.getSecond().getClass();
1:     } else {
0:       throw new RuntimeException("Couldn't determine class of the input values");
1:     }
0:     // Use old API for multiple outputs
0:     JobConf oldApiJob = new JobConf(initialConf);
0:     MultipleOutputs.addNamedOutput(oldApiJob, TRAINING_TAG,
0:         org.apache.hadoop.mapred.SequenceFileOutputFormat.class,
0:         keyClass, valueClass);
0:     MultipleOutputs.addNamedOutput(oldApiJob, TEST_TAG,
0:         org.apache.hadoop.mapred.SequenceFileOutputFormat.class,
0:         keyClass, valueClass);
1: 
0:     // Setup job with new API
0:     Job job = new Job(oldApiJob);
1:     FileInputFormat.addInputPath(job, inputPath);
1:     FileOutputFormat.setOutputPath(job, outputPath);
1:     job.setNumReduceTasks(1);
1:     job.setInputFormatClass(SequenceFileInputFormat.class);
1:     job.setOutputFormatClass(SequenceFileOutputFormat.class);
1:     job.setMapperClass(SplitInputMapper.class);
1:     job.setReducerClass(SplitInputReducer.class);
1:     job.setSortComparatorClass(SplitInputComparator.class);
1:     job.setOutputKeyClass(keyClass);
1:     job.setOutputValueClass(valueClass);
0:     try {
0:       job.submit();
0:       job.waitForCompletion(true);
0:     } catch (InterruptedException e) {
0:       e.printStackTrace();
0:     } catch (ClassNotFoundException e) {
0:       e.printStackTrace();
1:     }
1:   }
1: 
1:   /**
0:    * Mapper which downsamples the input by downsamplingFactor
1:    */
1:   public static class SplitInputMapper extends
1:       Mapper<WritableComparable<?>, Writable, WritableComparable<?>, Writable> {
1: 
1:     private int downsamplingFactor;
1: 
1:     @Override
0:     public void setup(Context context) {
0:       downsamplingFactor =
0:           context.getConfiguration().getInt(DOWNSAMPLING_FACTOR, 1);
1:     }
1: 
1:     /**
0:      * Only run map() for one out of every downsampleFactor inputs
1:      */
1:     @Override
1:     public void run(Context context) throws IOException, InterruptedException {
1:       setup(context);
0:       for (int i = 0; context.nextKeyValue(); i++) {
1:         if (i % downsamplingFactor == 0) {
1:           map(context.getCurrentKey(), context.getCurrentValue(), context);
1:         }
1:       }
1:       cleanup(context);
1:     }
1: 
1:   }
1: 
1:   /**
0:    * Reducer which uses MultipleOutputs to randomly allocate key value pairs
0:    * between test and training outputs
1:    */
1:   public static class SplitInputReducer extends
1:       Reducer<WritableComparable<?>, Writable, WritableComparable<?>, Writable> {
1: 
1:     private MultipleOutputs multipleOutputs;
0:     private OutputCollector<WritableComparable<?>, Writable> trainingCollector = null;
0:     private OutputCollector<WritableComparable<?>, Writable> testCollector = null;
1:     private final Random rnd = RandomUtils.getRandom();
1:     private float randomSelectionPercent;
1: 
0:     @SuppressWarnings("unchecked")
1:     @Override
0:     protected void setup(Context context) throws IOException {
0:       randomSelectionPercent =
0:           context.getConfiguration().getFloat(RANDOM_SELECTION_PCT, 0);
0:       multipleOutputs =
0:           new MultipleOutputs(new JobConf(context.getConfiguration()));
0:       trainingCollector = multipleOutputs.getCollector(TRAINING_TAG, null);
0:       testCollector = multipleOutputs.getCollector(TEST_TAG, null);
1:     }
1: 
1:     /**
1:      * Randomly allocate key value pairs between test and training sets.
1:      * randomSelectionPercent of the pairs will go to the test set.
1:      */
1:     @Override
1:     protected void reduce(WritableComparable<?> key, Iterable<Writable> values,
1:         Context context) throws IOException, InterruptedException {
1:       for (Writable value : values) {
1:         if (rnd.nextInt(100) < randomSelectionPercent) {
0:           testCollector.collect(key, value);
1:         } else {
0:           trainingCollector.collect(key, value);
1:         }
1:       }
1: 
1:     }
1: 
1:     @Override
1:     protected void cleanup(Context context) throws IOException {
0:       multipleOutputs.close();
1:     }
1: 
1:   }
1: 
1:   /**
0:    * Randomly permute key value pairs
1:    */
0:   public static class SplitInputComparator extends WritableComparator {
1: 
1:     private final Random rnd = RandomUtils.getRandom();
1: 
1:     protected SplitInputComparator() {
1:       super(WritableComparable.class);
1:     }
1: 
1:     @Override
1:     public int compare(byte[] b1, int s1, int l1, byte[] b2, int s2, int l2) {
1:       if (rnd.nextBoolean()) {
1:         return 1;
1:       } else {
1:         return -1;
1:       }
1:     }
1:   }
1: 
1: }
============================================================================