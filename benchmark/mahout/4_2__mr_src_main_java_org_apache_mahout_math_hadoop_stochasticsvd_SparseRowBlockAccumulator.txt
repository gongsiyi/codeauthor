1:1499411: /*
1:1499411:  * Licensed to the Apache Software Foundation (ASF) under one or more
1:1499411:  * contributor license agreements.  See the NOTICE file distributed with
1:1499411:  * this work for additional information regarding copyright ownership.
1:1499411:  * The ASF licenses this file to You under the Apache License, Version 2.0
1:1499411:  * (the "License"); you may not use this file except in compliance with
1:1499411:  * the License.  You may obtain a copy of the License at
1:1499411:  *
1:1499411:  *     http://www.apache.org/licenses/LICENSE-2.0
1:1499411:  *
1:1499411:  * Unless required by applicable law or agreed to in writing, software
1:1499411:  * distributed under the License is distributed on an "AS IS" BASIS,
1:1499411:  * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
1:1499411:  * See the License for the specific language governing permissions and
1:1499411:  * limitations under the License.
1:1499411:  */
1:1499411: 
1:ffc7fab: package org.apache.mahout.math.hadoop.stochasticsvd;
1:ffc7fab: 
1:ffc7fab: import java.io.Closeable;
1:ffc7fab: import java.io.IOException;
1:ffc7fab: 
1:ffc7fab: import org.apache.hadoop.io.LongWritable;
1:ffc7fab: import org.apache.hadoop.mapred.OutputCollector;
1:ffc7fab: import org.apache.mahout.math.Vector;
1:ffc7fab: 
1:ffc7fab: /**
1:ffc7fab:  * Aggregate incoming rows into blocks based on the row number (long). Rows can
1:ffc7fab:  * be sparse (meaning they come perhaps in big intervals) and don't even have to
1:ffc7fab:  * come in any order, but they should be coming in proximity, so when we output
1:ffc7fab:  * block key, we hopefully aggregate more than one row by then.
1:ffc7fab:  * <P>
1:ffc7fab:  * 
1:ffc7fab:  * If block is sufficiently large to fit all rows that mapper may produce, it
1:ffc7fab:  * will not even ever hit a spill at all as we would already be plussing
1:ffc7fab:  * efficiently in the mapper.
1:ffc7fab:  * <P>
1:ffc7fab:  * 
1:ffc7fab:  * Also, for sparse inputs it will also be working especially well if transposed
1:ffc7fab:  * columns of the left side matrix and corresponding rows of the right side
1:ffc7fab:  * matrix experience sparsity in same elements.
1:ffc7fab:  * <P>
1:ffc7fab:  * 
1:ffc7fab:  */
1:ffc7fab: public class SparseRowBlockAccumulator implements
1:ffc7fab:     OutputCollector<Long, Vector>, Closeable {
1:ffc7fab: 
1:1499411:   private final int height;
1:1499411:   private final OutputCollector<LongWritable, SparseRowBlockWritable> delegate;
1:ffc7fab:   private long currentBlockNum = -1;
1:ffc7fab:   private SparseRowBlockWritable block;
1:1499411:   private final LongWritable blockKeyW = new LongWritable();
1:ffc7fab: 
1:ffc7fab:   public SparseRowBlockAccumulator(int height,
1:ffc7fab:                                    OutputCollector<LongWritable, SparseRowBlockWritable> delegate) {
1:ffc7fab:     this.height = height;
1:ffc7fab:     this.delegate = delegate;
1:ffc7fab:   }
1:ffc7fab: 
1:ffc7fab:   private void flushBlock() throws IOException {
1:1499411:     if (block == null || block.getNumRows() == 0) {
1:ffc7fab:       return;
1:1499411:     }
1:ffc7fab:     blockKeyW.set(currentBlockNum);
1:ffc7fab:     delegate.collect(blockKeyW, block);
1:ffc7fab:     block.clear();
1:ffc7fab:   }
1:ffc7fab: 
1:ffc7fab:   @Override
1:ffc7fab:   public void collect(Long rowIndex, Vector v) throws IOException {
1:ffc7fab: 
1:ffc7fab:     long blockKey = rowIndex / height;
1:ffc7fab: 
1:ffc7fab:     if (blockKey != currentBlockNum) {
1:ffc7fab:       flushBlock();
1:1499411:       if (block == null) {
1:ffc7fab:         block = new SparseRowBlockWritable(100);
1:1499411:       }
1:ffc7fab:       currentBlockNum = blockKey;
1:ffc7fab:     }
1:ffc7fab: 
1:ffc7fab:     block.plusRow((int) (rowIndex % height), v);
1:ffc7fab:   }
1:ffc7fab: 
1:ffc7fab:   @Override
1:ffc7fab:   public void close() throws IOException {
1:ffc7fab:     flushBlock();
1:ffc7fab:   }
1:ffc7fab: 
1:ffc7fab: }
============================================================================
author:pferrel
-------------------------------------------------------------------------------
commit:b988c49
author:frankscholten
-------------------------------------------------------------------------------
commit:1a42d85
author:Sean R. Owen
-------------------------------------------------------------------------------
commit:1499411
/////////////////////////////////////////////////////////////////////////
1: /*
1:  * Licensed to the Apache Software Foundation (ASF) under one or more
1:  * contributor license agreements.  See the NOTICE file distributed with
1:  * this work for additional information regarding copyright ownership.
1:  * The ASF licenses this file to You under the Apache License, Version 2.0
1:  * (the "License"); you may not use this file except in compliance with
1:  * the License.  You may obtain a copy of the License at
1:  *
1:  *     http://www.apache.org/licenses/LICENSE-2.0
1:  *
1:  * Unless required by applicable law or agreed to in writing, software
1:  * distributed under the License is distributed on an "AS IS" BASIS,
1:  * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
1:  * See the License for the specific language governing permissions and
1:  * limitations under the License.
1:  */
1: 
/////////////////////////////////////////////////////////////////////////
1:   private final int height;
1:   private final OutputCollector<LongWritable, SparseRowBlockWritable> delegate;
1:   private final LongWritable blockKeyW = new LongWritable();
1:     if (block == null || block.getNumRows() == 0) {
1:     }
/////////////////////////////////////////////////////////////////////////
1:       if (block == null) {
1:       }
author:Dmitriy Lyubimov
-------------------------------------------------------------------------------
commit:ffc7fab
/////////////////////////////////////////////////////////////////////////
1: package org.apache.mahout.math.hadoop.stochasticsvd;
1: 
1: import java.io.Closeable;
1: import java.io.IOException;
1: 
1: import org.apache.hadoop.io.LongWritable;
1: import org.apache.hadoop.mapred.OutputCollector;
1: import org.apache.mahout.math.Vector;
1: 
1: /**
1:  * Aggregate incoming rows into blocks based on the row number (long). Rows can
1:  * be sparse (meaning they come perhaps in big intervals) and don't even have to
1:  * come in any order, but they should be coming in proximity, so when we output
1:  * block key, we hopefully aggregate more than one row by then.
1:  * <P>
1:  * 
1:  * If block is sufficiently large to fit all rows that mapper may produce, it
1:  * will not even ever hit a spill at all as we would already be plussing
1:  * efficiently in the mapper.
1:  * <P>
1:  * 
1:  * Also, for sparse inputs it will also be working especially well if transposed
1:  * columns of the left side matrix and corresponding rows of the right side
1:  * matrix experience sparsity in same elements.
1:  * <P>
1:  * 
1:  */
1: public class SparseRowBlockAccumulator implements
1:     OutputCollector<Long, Vector>, Closeable {
1: 
0:   private int height;
0:   private OutputCollector<LongWritable, SparseRowBlockWritable> delegate;
1:   private long currentBlockNum = -1;
1:   private SparseRowBlockWritable block;
0:   private LongWritable blockKeyW = new LongWritable();
1: 
1:   public SparseRowBlockAccumulator(int height,
1:                                    OutputCollector<LongWritable, SparseRowBlockWritable> delegate) {
0:     super();
1:     this.height = height;
1:     this.delegate = delegate;
1:   }
1: 
1:   private void flushBlock() throws IOException {
0:     if (block == null || block.getNumRows() == 0)
1:       return;
1:     blockKeyW.set(currentBlockNum);
1:     delegate.collect(blockKeyW, block);
1:     block.clear();
1:   }
1: 
1:   @Override
1:   public void collect(Long rowIndex, Vector v) throws IOException {
1: 
1:     long blockKey = rowIndex / height;
1: 
1:     if (blockKey != currentBlockNum) {
1:       flushBlock();
0:       if (block == null)
1:         block = new SparseRowBlockWritable(100);
1:       currentBlockNum = blockKey;
1:     }
1: 
1:     block.plusRow((int) (rowIndex % height), v);
1:   }
1: 
1:   @Override
1:   public void close() throws IOException {
1:     flushBlock();
1:   }
1: 
1: }
============================================================================