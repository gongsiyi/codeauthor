1:c439f53: /**
1:c439f53:  * Licensed to the Apache Software Foundation (ASF) under one or more
1:c439f53:  * contributor license agreements.  See the NOTICE file distributed with
1:c439f53:  * this work for additional information regarding copyright ownership.
1:c439f53:  * The ASF licenses this file to You under the Apache License, Version 2.0
1:c439f53:  * (the "License"); you may not use this file except in compliance with
1:c439f53:  * the License.  You may obtain a copy of the License at
2:c439f53:  *
1:c439f53:  *     http://www.apache.org/licenses/LICENSE-2.0
1:c439f53:  *
1:c439f53:  * Unless required by applicable law or agreed to in writing, software
1:c439f53:  * distributed under the License is distributed on an "AS IS" BASIS,
1:c439f53:  * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
1:c439f53:  * See the License for the specific language governing permissions and
1:c439f53:  * limitations under the License.
1:c439f53:  */
1:c439f53: 
1:c439f53: package org.apache.mahout.cf.taste.example.kddcup.track1.svd;
1:c439f53: 
1:471cb81: import org.apache.mahout.cf.taste.common.Refreshable;
1:c439f53: import org.apache.mahout.cf.taste.common.TasteException;
1:c439f53: import org.apache.mahout.cf.taste.impl.common.FastByIDMap;
1:c439f53: import org.apache.mahout.cf.taste.impl.common.FullRunningAverage;
1:c439f53: import org.apache.mahout.cf.taste.impl.common.LongPrimitiveIterator;
1:c439f53: import org.apache.mahout.cf.taste.impl.common.RunningAverage;
1:c439f53: import org.apache.mahout.cf.taste.impl.recommender.svd.Factorization;
1:c439f53: import org.apache.mahout.cf.taste.impl.recommender.svd.Factorizer;
1:c439f53: import org.apache.mahout.cf.taste.model.DataModel;
1:c439f53: import org.apache.mahout.cf.taste.model.Preference;
1:c439f53: import org.apache.mahout.common.RandomUtils;
1:c439f53: import org.slf4j.Logger;
1:c439f53: import org.slf4j.LoggerFactory;
1:c439f53: 
1:471cb81: import java.util.Collection;
1:c439f53: import java.util.Random;
1:c439f53: 
1:c439f53: /**
1:39fe224:  * {@link Factorizer} based on Simon Funk's famous article <a href="http://sifter.org/~simon/journal/20061211.html">
1:39fe224:  * "Netflix Update: Try this at home"</a>.
1:c439f53:  *
1:39fe224:  * Attempts to be as memory efficient as possible, only iterating once through the
1:39fe224:  * {@link FactorizablePreferences} or {@link DataModel} while copying everything to primitive arrays.
1:39fe224:  * Learning works in place on these datastructures after that.
1:c439f53:  */
1:c439f53: public class ParallelArraysSGDFactorizer implements Factorizer {
1:c439f53: 
1:c439f53:   public static final double DEFAULT_LEARNING_RATE = 0.005;
1:c439f53:   public static final double DEFAULT_PREVENT_OVERFITTING = 0.02;
1:c439f53:   public static final double DEFAULT_RANDOM_NOISE = 0.005;
1:c439f53: 
1:c439f53:   private final int numFeatures;
1:c439f53:   private final int numIterations;
1:c439f53:   private final float minPreference;
1:c439f53:   private final float maxPreference;
1:c439f53: 
1:c439f53:   private final Random random;
1:c439f53:   private final double learningRate;
1:c439f53:   private final double preventOverfitting;
1:c439f53: 
1:c439f53:   private final FastByIDMap<Integer> userIDMapping;
1:c439f53:   private final FastByIDMap<Integer> itemIDMapping;
1:c439f53: 
1:c439f53:   private final double[][] userFeatures;
1:c439f53:   private final double[][] itemFeatures;
1:c439f53: 
1:c439f53:   private final int[] userIndexes;
1:c439f53:   private final int[] itemIndexes;
1:c439f53:   private final float[] values;
1:c439f53: 
1:c439f53:   private final double defaultValue;
1:c439f53:   private final double interval;
1:c439f53:   private final double[] cachedEstimates;
1:c439f53: 
1:c439f53: 
1:c439f53:   private static final Logger log = LoggerFactory.getLogger(ParallelArraysSGDFactorizer.class);
1:c439f53: 
1:c439f53:   public ParallelArraysSGDFactorizer(DataModel dataModel, int numFeatures, int numIterations) {
1:c439f53:     this(new DataModelFactorizablePreferences(dataModel), numFeatures, numIterations, DEFAULT_LEARNING_RATE,
1:c439f53:         DEFAULT_PREVENT_OVERFITTING, DEFAULT_RANDOM_NOISE);
1:c439f53:   }
1:c439f53: 
1:c439f53:   public ParallelArraysSGDFactorizer(DataModel dataModel, int numFeatures, int numIterations, double learningRate,
1:c439f53:                                      double preventOverfitting, double randomNoise) {
1:c439f53:     this(new DataModelFactorizablePreferences(dataModel), numFeatures, numIterations, learningRate, preventOverfitting,
1:c439f53:         randomNoise);
1:c439f53:   }
1:c439f53: 
1:c439f53:   public ParallelArraysSGDFactorizer(FactorizablePreferences factorizablePrefs, int numFeatures, int numIterations) {
1:c439f53:     this(factorizablePrefs, numFeatures, numIterations, DEFAULT_LEARNING_RATE, DEFAULT_PREVENT_OVERFITTING,
1:c439f53:         DEFAULT_RANDOM_NOISE);
1:c439f53:   }
1:c439f53: 
1:c439f53:   public ParallelArraysSGDFactorizer(FactorizablePreferences factorizablePreferences, int numFeatures,
1:c439f53:       int numIterations, double learningRate, double preventOverfitting, double randomNoise) {
1:c439f53: 
1:c439f53:     this.numFeatures = numFeatures;
1:c439f53:     this.numIterations = numIterations;
1:c439f53:     minPreference = factorizablePreferences.getMinPreference();
1:c439f53:     maxPreference = factorizablePreferences.getMaxPreference();
1:c439f53: 
1:c439f53:     this.random = RandomUtils.getRandom();
1:c439f53:     this.learningRate = learningRate;
1:c439f53:     this.preventOverfitting = preventOverfitting;
1:c439f53: 
1:c439f53:     int numUsers = factorizablePreferences.numUsers();
1:c439f53:     int numItems = factorizablePreferences.numItems();
1:c439f53:     int numPrefs = factorizablePreferences.numPreferences();
1:c439f53: 
1:c439f53:     log.info("Mapping {} users...", numUsers);
1:87c15be:     userIDMapping = new FastByIDMap<>(numUsers);
1:c439f53:     int index = 0;
1:c439f53:     LongPrimitiveIterator userIterator = factorizablePreferences.getUserIDs();
1:c439f53:     while (userIterator.hasNext()) {
1:c439f53:       userIDMapping.put(userIterator.nextLong(), index++);
1:c439f53:     }
1:c439f53: 
1:c439f53:     log.info("Mapping {} items", numItems);
1:87c15be:     itemIDMapping = new FastByIDMap<>(numItems);
1:c439f53:     index = 0;
1:c439f53:     LongPrimitiveIterator itemIterator = factorizablePreferences.getItemIDs();
1:c439f53:     while (itemIterator.hasNext()) {
1:c439f53:       itemIDMapping.put(itemIterator.nextLong(), index++);
1:c439f53:     }
1:c439f53: 
1:c439f53:     this.userIndexes = new int[numPrefs];
1:c439f53:     this.itemIndexes = new int[numPrefs];
1:c439f53:     this.values = new float[numPrefs];
1:c439f53:     this.cachedEstimates = new double[numPrefs];
1:c439f53: 
1:c439f53:     index = 0;
1:c439f53:     log.info("Loading {} preferences into memory", numPrefs);
1:c439f53:     RunningAverage average = new FullRunningAverage();
1:c439f53:     for (Preference preference : factorizablePreferences.getPreferences()) {
1:c439f53:       userIndexes[index] = userIDMapping.get(preference.getUserID());
1:c439f53:       itemIndexes[index] = itemIDMapping.get(preference.getItemID());
1:c439f53:       values[index] = preference.getValue();
1:c439f53:       cachedEstimates[index] = 0;
1:c439f53: 
1:c439f53:       average.addDatum(preference.getValue());
1:c439f53: 
1:c439f53:       index++;
1:c439f53:       if (index % 1000000 == 0) {
1:c439f53:         log.info("Processed {} preferences", index);
1:c439f53:       }
1:c439f53:     }
1:c439f53:     log.info("Processed {} preferences, done.", index);
1:c439f53: 
1:c439f53:     double averagePreference = average.getAverage();
1:c439f53:     log.info("Average preference value is {}", averagePreference);
1:c439f53: 
1:c439f53:     double prefInterval = factorizablePreferences.getMaxPreference() - factorizablePreferences.getMinPreference();
1:39fe224:     defaultValue = Math.sqrt((averagePreference - prefInterval * 0.1) / numFeatures);
1:39fe224:     interval = prefInterval * 0.1 / numFeatures;
1:c439f53: 
1:c439f53:     userFeatures = new double[numUsers][numFeatures];
1:c439f53:     itemFeatures = new double[numItems][numFeatures];
1:c439f53: 
1:c439f53:     log.info("Initializing feature vectors...");
1:c439f53:     for (int feature = 0; feature < numFeatures; feature++) {
1:c439f53:       for (int userIndex = 0; userIndex < numUsers; userIndex++) {
1:c439f53:         userFeatures[userIndex][feature] = defaultValue + (random.nextDouble() - 0.5) * interval * randomNoise;
1:c439f53:       }
1:c439f53:       for (int itemIndex = 0; itemIndex < numItems; itemIndex++) {
1:c439f53:         itemFeatures[itemIndex][feature] = defaultValue + (random.nextDouble() - 0.5) * interval * randomNoise;
1:c439f53:       }
1:c439f53:     }
1:c439f53:   }
1:c439f53: 
1:c439f53:   @Override
1:c439f53:   public Factorization factorize() throws TasteException {
1:c439f53:     for (int feature = 0; feature < numFeatures; feature++) {
1:c439f53:       log.info("Shuffling preferences...");
1:c439f53:       shufflePreferences();
1:6d16230:       log.info("Starting training of feature {} ...", feature);
1:c439f53:       for (int currentIteration = 0; currentIteration < numIterations; currentIteration++) {
1:39fe224:         if (currentIteration == numIterations - 1) {
1:c439f53:           double rmse = trainingIterationWithRmse(feature);
1:c439f53:           log.info("Finished training feature {} with RMSE {}", feature, rmse);
1:39fe224:         } else {
1:39fe224:           trainingIteration(feature);
1:c439f53:         }
1:c439f53:       }
1:c439f53:       if (feature < numFeatures - 1) {
1:c439f53:         log.info("Updating cache...");
1:c439f53:         for (int index = 0; index < userIndexes.length; index++) {
1:c439f53:           cachedEstimates[index] = estimate(userIndexes[index], itemIndexes[index], feature, cachedEstimates[index],
1:c439f53:               false);
1:c439f53:         }
1:c439f53:       }
1:c439f53:     }
1:c439f53:     log.info("Factorization done");
1:c439f53:     return new Factorization(userIDMapping, itemIDMapping, userFeatures, itemFeatures);
1:c439f53:   }
1:c439f53: 
1:c439f53:   private void trainingIteration(int feature) {
1:c439f53:     for (int index = 0; index < userIndexes.length; index++) {
1:c439f53:       train(userIndexes[index], itemIndexes[index], feature, values[index], cachedEstimates[index]);
1:c439f53:     }
1:c439f53:   }
1:c439f53: 
1:c439f53:   private double trainingIterationWithRmse(int feature) {
1:4ca6b86:     double rmse = 0.0;
1:c439f53:     for (int index = 0; index < userIndexes.length; index++) {
1:c439f53:       double error = train(userIndexes[index], itemIndexes[index], feature, values[index], cachedEstimates[index]);
1:39fe224:       rmse += error * error;
1:c439f53:     }
1:4ca6b86:     return Math.sqrt(rmse / userIndexes.length);
1:c439f53:   }
1:c439f53: 
1:c439f53:   private double estimate(int userIndex, int itemIndex, int feature, double cachedEstimate, boolean trailing) {
1:c439f53:     double sum = cachedEstimate;
1:c439f53:     sum += userFeatures[userIndex][feature] * itemFeatures[itemIndex][feature];
1:c439f53:     if (trailing) {
1:39fe224:       sum += (numFeatures - feature - 1) * (defaultValue + interval) * (defaultValue + interval);
1:c439f53:       if (sum > maxPreference) {
1:c439f53:         sum = maxPreference;
1:c439f53:       } else if (sum < minPreference) {
1:c439f53:         sum = minPreference;
1:c439f53:       }
1:c439f53:     }
1:c439f53:     return sum;
1:c439f53:   }
1:c439f53: 
1:c439f53:   public double train(int userIndex, int itemIndex, int feature, double original, double cachedEstimate) {
1:c439f53:     double error = original - estimate(userIndex, itemIndex, feature, cachedEstimate, true);
1:c439f53:     double[] userVector = userFeatures[userIndex];
1:c439f53:     double[] itemVector = itemFeatures[itemIndex];
1:c439f53: 
1:c439f53:     userVector[feature] += learningRate * (error * itemVector[feature] - preventOverfitting * userVector[feature]);
1:c439f53:     itemVector[feature] += learningRate * (error * userVector[feature] - preventOverfitting * itemVector[feature]);
1:c439f53: 
1:c439f53:     return error;
1:c439f53:   }
1:c439f53: 
1:c439f53:   protected void shufflePreferences() {
1:c439f53:     /* Durstenfeld shuffle */
1:c439f53:     for (int currentPos = userIndexes.length - 1; currentPos > 0; currentPos--) {
1:c439f53:       int swapPos = random.nextInt(currentPos + 1);
1:c439f53:       swapPreferences(currentPos, swapPos);
1:c439f53:     }
1:c439f53:   }
1:c439f53: 
1:c439f53:   private void swapPreferences(int posA, int posB) {
1:c439f53:     int tmpUserIndex = userIndexes[posA];
1:c439f53:     int tmpItemIndex = itemIndexes[posA];
1:c439f53:     float tmpValue = values[posA];
1:c439f53:     double tmpEstimate = cachedEstimates[posA];
1:c439f53: 
1:c439f53:     userIndexes[posA] = userIndexes[posB];
1:c439f53:     itemIndexes[posA] = itemIndexes[posB];
1:c439f53:     values[posA] = values[posB];
1:c439f53:     cachedEstimates[posA] = cachedEstimates[posB];
1:c439f53: 
1:c439f53:     userIndexes[posB] = tmpUserIndex;
1:c439f53:     itemIndexes[posB] = tmpItemIndex;
1:c439f53:     values[posB] = tmpValue;
1:c439f53:     cachedEstimates[posB] = tmpEstimate;
1:c439f53:   }
1:471cb81: 
1:471cb81:   @Override
1:471cb81:   public void refresh(Collection<Refreshable> alreadyRefreshed) {
1:471cb81:     // do nothing
1:471cb81:   }
1:471cb81: 
1:c439f53: }
============================================================================
author:Suneel Marthi
-------------------------------------------------------------------------------
commit:87c15be
/////////////////////////////////////////////////////////////////////////
1:     userIDMapping = new FastByIDMap<>(numUsers);
/////////////////////////////////////////////////////////////////////////
1:     itemIDMapping = new FastByIDMap<>(numItems);
author:Sean R. Owen
-------------------------------------------------------------------------------
commit:4ca6b86
/////////////////////////////////////////////////////////////////////////
1:     double rmse = 0.0;
1:     return Math.sqrt(rmse / userIndexes.length);
commit:39fe224
/////////////////////////////////////////////////////////////////////////
1:  * {@link Factorizer} based on Simon Funk's famous article <a href="http://sifter.org/~simon/journal/20061211.html">
1:  * "Netflix Update: Try this at home"</a>.
1:  * Attempts to be as memory efficient as possible, only iterating once through the
1:  * {@link FactorizablePreferences} or {@link DataModel} while copying everything to primitive arrays.
1:  * Learning works in place on these datastructures after that.
/////////////////////////////////////////////////////////////////////////
1:     defaultValue = Math.sqrt((averagePreference - prefInterval * 0.1) / numFeatures);
1:     interval = prefInterval * 0.1 / numFeatures;
/////////////////////////////////////////////////////////////////////////
1:         if (currentIteration == numIterations - 1) {
1:         } else {
1:           trainingIteration(feature);
/////////////////////////////////////////////////////////////////////////
1:       rmse += error * error;
/////////////////////////////////////////////////////////////////////////
1:       sum += (numFeatures - feature - 1) * (defaultValue + interval) * (defaultValue + interval);
commit:471cb81
/////////////////////////////////////////////////////////////////////////
1: import org.apache.mahout.cf.taste.common.Refreshable;
/////////////////////////////////////////////////////////////////////////
1: import java.util.Collection;
/////////////////////////////////////////////////////////////////////////
1: 
1:   @Override
1:   public void refresh(Collection<Refreshable> alreadyRefreshed) {
1:     // do nothing
1:   }
1: 
author:Sebastian Schelter
-------------------------------------------------------------------------------
commit:6d16230
/////////////////////////////////////////////////////////////////////////
1:       log.info("Starting training of feature {} ...", feature);
commit:c439f53
/////////////////////////////////////////////////////////////////////////
1: /**
1:  * Licensed to the Apache Software Foundation (ASF) under one or more
1:  * contributor license agreements.  See the NOTICE file distributed with
1:  * this work for additional information regarding copyright ownership.
1:  * The ASF licenses this file to You under the Apache License, Version 2.0
1:  * (the "License"); you may not use this file except in compliance with
1:  * the License.  You may obtain a copy of the License at
1:  *
1:  *     http://www.apache.org/licenses/LICENSE-2.0
1:  *
1:  * Unless required by applicable law or agreed to in writing, software
1:  * distributed under the License is distributed on an "AS IS" BASIS,
1:  * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
1:  * See the License for the specific language governing permissions and
1:  * limitations under the License.
1:  */
1: 
1: package org.apache.mahout.cf.taste.example.kddcup.track1.svd;
1: 
1: import org.apache.mahout.cf.taste.common.TasteException;
1: import org.apache.mahout.cf.taste.impl.common.FastByIDMap;
1: import org.apache.mahout.cf.taste.impl.common.FullRunningAverage;
1: import org.apache.mahout.cf.taste.impl.common.LongPrimitiveIterator;
1: import org.apache.mahout.cf.taste.impl.common.RunningAverage;
1: import org.apache.mahout.cf.taste.impl.recommender.svd.Factorization;
1: import org.apache.mahout.cf.taste.impl.recommender.svd.Factorizer;
1: import org.apache.mahout.cf.taste.model.DataModel;
1: import org.apache.mahout.cf.taste.model.Preference;
1: import org.apache.mahout.common.RandomUtils;
1: import org.slf4j.Logger;
1: import org.slf4j.LoggerFactory;
1: 
1: import java.util.Random;
1: 
1: /**
0:  * {@link org.apache.mahout.cf.taste.impl.recommender.svd.Factorizer} based on Simon Funk's famous article "Netflix Update: Try this at home"
0:  * {@see http://sifter.org/~simon/journal/20061211.html}.
1:  *
0:  * Attempts to be as memory efficient as possible, only iterating once through the {@link FactorizablePreferences} or {@link DataModel} while
0:  * copying everything to primitive arrays. Learning works in place on these datastructures after that.
1:  *
1:  */
1: public class ParallelArraysSGDFactorizer implements Factorizer {
1: 
1:   public static final double DEFAULT_LEARNING_RATE = 0.005;
1:   public static final double DEFAULT_PREVENT_OVERFITTING = 0.02;
1:   public static final double DEFAULT_RANDOM_NOISE = 0.005;
1: 
1:   private final int numFeatures;
1:   private final int numIterations;
1:   private final float minPreference;
1:   private final float maxPreference;
1: 
1:   private final Random random;
1:   private final double learningRate;
1:   private final double preventOverfitting;
1: 
1:   private final FastByIDMap<Integer> userIDMapping;
1:   private final FastByIDMap<Integer> itemIDMapping;
1: 
1:   private final double[][] userFeatures;
1:   private final double[][] itemFeatures;
1: 
1:   private final int[] userIndexes;
1:   private final int[] itemIndexes;
1:   private final float[] values;
1: 
1:   private final double defaultValue;
1:   private final double interval;
1:   private final double[] cachedEstimates;
1: 
1: 
1:   private static final Logger log = LoggerFactory.getLogger(ParallelArraysSGDFactorizer.class);
1: 
1:   public ParallelArraysSGDFactorizer(DataModel dataModel, int numFeatures, int numIterations) {
1:     this(new DataModelFactorizablePreferences(dataModel), numFeatures, numIterations, DEFAULT_LEARNING_RATE,
1:         DEFAULT_PREVENT_OVERFITTING, DEFAULT_RANDOM_NOISE);
1:   }
1: 
1:   public ParallelArraysSGDFactorizer(DataModel dataModel, int numFeatures, int numIterations, double learningRate,
1:                                      double preventOverfitting, double randomNoise) {
1:     this(new DataModelFactorizablePreferences(dataModel), numFeatures, numIterations, learningRate, preventOverfitting,
1:         randomNoise);
1:   }
1: 
1:   public ParallelArraysSGDFactorizer(FactorizablePreferences factorizablePrefs, int numFeatures, int numIterations) {
1:     this(factorizablePrefs, numFeatures, numIterations, DEFAULT_LEARNING_RATE, DEFAULT_PREVENT_OVERFITTING,
1:         DEFAULT_RANDOM_NOISE);
1:   }
1: 
1:   public ParallelArraysSGDFactorizer(FactorizablePreferences factorizablePreferences, int numFeatures,
1:       int numIterations, double learningRate, double preventOverfitting, double randomNoise) {
1: 
1:     this.numFeatures = numFeatures;
1:     this.numIterations = numIterations;
1:     minPreference = factorizablePreferences.getMinPreference();
1:     maxPreference = factorizablePreferences.getMaxPreference();
1: 
1:     this.random = RandomUtils.getRandom();
1:     this.learningRate = learningRate;
1:     this.preventOverfitting = preventOverfitting;
1: 
1:     int numUsers = factorizablePreferences.numUsers();
1:     int numItems = factorizablePreferences.numItems();
1:     int numPrefs = factorizablePreferences.numPreferences();
1: 
1:     log.info("Mapping {} users...", numUsers);
0:     userIDMapping = new FastByIDMap<Integer>(numUsers);
1:     int index = 0;
1:     LongPrimitiveIterator userIterator = factorizablePreferences.getUserIDs();
1:     while (userIterator.hasNext()) {
1:       userIDMapping.put(userIterator.nextLong(), index++);
1:     }
1: 
1:     log.info("Mapping {} items", numItems);
0:     itemIDMapping = new FastByIDMap<Integer>(numItems);
1:     index = 0;
1:     LongPrimitiveIterator itemIterator = factorizablePreferences.getItemIDs();
1:     while (itemIterator.hasNext()) {
1:       itemIDMapping.put(itemIterator.nextLong(), index++);
1:     }
1: 
1:     this.userIndexes = new int[numPrefs];
1:     this.itemIndexes = new int[numPrefs];
1:     this.values = new float[numPrefs];
1:     this.cachedEstimates = new double[numPrefs];
1: 
1:     index = 0;
1:     log.info("Loading {} preferences into memory", numPrefs);
1:     RunningAverage average = new FullRunningAverage();
1:     for (Preference preference : factorizablePreferences.getPreferences()) {
1:       userIndexes[index] = userIDMapping.get(preference.getUserID());
1:       itemIndexes[index] = itemIDMapping.get(preference.getItemID());
1:       values[index] = preference.getValue();
1:       cachedEstimates[index] = 0;
1: 
1:       average.addDatum(preference.getValue());
1: 
1:       index++;
1:       if (index % 1000000 == 0) {
1:         log.info("Processed {} preferences", index);
1:       }
1:     }
1:     log.info("Processed {} preferences, done.", index);
1: 
1:     double averagePreference = average.getAverage();
1:     log.info("Average preference value is {}", averagePreference);
1: 
1:     double prefInterval = factorizablePreferences.getMaxPreference() - factorizablePreferences.getMinPreference();
0:     defaultValue = Math.sqrt((averagePreference - (prefInterval * 0.1)) / numFeatures);
0:     interval = (prefInterval * 0.1) / numFeatures;
1: 
1:     userFeatures = new double[numUsers][numFeatures];
1:     itemFeatures = new double[numItems][numFeatures];
1: 
1:     log.info("Initializing feature vectors...");
1:     for (int feature = 0; feature < numFeatures; feature++) {
1:       for (int userIndex = 0; userIndex < numUsers; userIndex++) {
1:         userFeatures[userIndex][feature] = defaultValue + (random.nextDouble() - 0.5) * interval * randomNoise;
1:       }
1:       for (int itemIndex = 0; itemIndex < numItems; itemIndex++) {
1:         itemFeatures[itemIndex][feature] = defaultValue + (random.nextDouble() - 0.5) * interval * randomNoise;
1:       }
1:     }
1:   }
1: 
1:   @Override
1:   public Factorization factorize() throws TasteException {
1:     for (int feature = 0; feature < numFeatures; feature++) {
1:       log.info("Shuffling preferences...");
1:       shufflePreferences();
0:      log.info("Starting training of feature {} ...", feature);
1:       for (int currentIteration = 0; currentIteration < numIterations; currentIteration++) {
0:         if (currentIteration != (numIterations - 1)) {
0:           trainingIteration(feature);
0:         } else {
1:           double rmse = trainingIterationWithRmse(feature);
1:           log.info("Finished training feature {} with RMSE {}", feature, rmse);
1:         }
1:       }
1:       if (feature < numFeatures - 1) {
1:         log.info("Updating cache...");
1:         for (int index = 0; index < userIndexes.length; index++) {
1:           cachedEstimates[index] = estimate(userIndexes[index], itemIndexes[index], feature, cachedEstimates[index],
1:               false);
1:         }
1:       }
1:     }
1:     log.info("Factorization done");
1:     return new Factorization(userIDMapping, itemIDMapping, userFeatures, itemFeatures);
1:   }
1: 
1:   private void trainingIteration(int feature) {
1:     for (int index = 0; index < userIndexes.length; index++) {
1:       train(userIndexes[index], itemIndexes[index], feature, values[index], cachedEstimates[index]);
1:     }
1:   }
1: 
1:   private double trainingIterationWithRmse(int feature) {
0:     double rmse = 0;
1:     for (int index = 0; index < userIndexes.length; index++) {
1:       double error = train(userIndexes[index], itemIndexes[index], feature, values[index], cachedEstimates[index]);
0:       rmse += (error * error);
1:     }
0:     return Math.sqrt(rmse / (double) userIndexes.length);
1:   }
1: 
1:   private double estimate(int userIndex, int itemIndex, int feature, double cachedEstimate, boolean trailing) {
1:     double sum = cachedEstimate;
1:     sum += userFeatures[userIndex][feature] * itemFeatures[itemIndex][feature];
1:     if (trailing) {
0:       sum += (numFeatures - feature - 1) * ((defaultValue + interval) * (defaultValue + interval));
1:       if (sum > maxPreference) {
1:         sum = maxPreference;
1:       } else if (sum < minPreference) {
1:         sum = minPreference;
1:       }
1:     }
1:     return sum;
1:   }
1: 
1:   public double train(int userIndex, int itemIndex, int feature, double original, double cachedEstimate) {
1:     double error = original - estimate(userIndex, itemIndex, feature, cachedEstimate, true);
1:     double[] userVector = userFeatures[userIndex];
1:     double[] itemVector = itemFeatures[itemIndex];
1: 
1:     userVector[feature] += learningRate * (error * itemVector[feature] - preventOverfitting * userVector[feature]);
1:     itemVector[feature] += learningRate * (error * userVector[feature] - preventOverfitting * itemVector[feature]);
1: 
1:     return error;
1:   }
1: 
1:   protected void shufflePreferences() {
1:     /* Durstenfeld shuffle */
1:     for (int currentPos = userIndexes.length - 1; currentPos > 0; currentPos--) {
1:       int swapPos = random.nextInt(currentPos + 1);
1:       swapPreferences(currentPos, swapPos);
1:     }
1:   }
1: 
1:   private void swapPreferences(int posA, int posB) {
1:     int tmpUserIndex = userIndexes[posA];
1:     int tmpItemIndex = itemIndexes[posA];
1:     float tmpValue = values[posA];
1:     double tmpEstimate = cachedEstimates[posA];
1: 
1:     userIndexes[posA] = userIndexes[posB];
1:     itemIndexes[posA] = itemIndexes[posB];
1:     values[posA] = values[posB];
1:     cachedEstimates[posA] = cachedEstimates[posB];
1: 
1:     userIndexes[posB] = tmpUserIndex;
1:     itemIndexes[posB] = tmpItemIndex;
1:     values[posB] = tmpValue;
1:     cachedEstimates[posB] = tmpEstimate;
1:   }
1: }
============================================================================