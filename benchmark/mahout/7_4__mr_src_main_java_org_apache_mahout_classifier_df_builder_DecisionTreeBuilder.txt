1:e9cc323: /**
1:e9cc323:  * Licensed to the Apache Software Foundation (ASF) under one or more
1:e9cc323:  * contributor license agreements.  See the NOTICE file distributed with
1:e9cc323:  * this work for additional information regarding copyright ownership.
1:e9cc323:  * The ASF licenses this file to You under the Apache License, Version 2.0
1:e9cc323:  * (the "License"); you may not use this file except in compliance with
1:e9cc323:  * the License.  You may obtain a copy of the License at
3:e9cc323:  *
1:e9cc323:  *     http://www.apache.org/licenses/LICENSE-2.0
1:e9cc323:  *
1:e9cc323:  * Unless required by applicable law or agreed to in writing, software
1:e9cc323:  * distributed under the License is distributed on an "AS IS" BASIS,
1:e9cc323:  * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
1:e9cc323:  * See the License for the specific language governing permissions and
1:e9cc323:  * limitations under the License.
1:e9cc323:  */
22:e9cc323: 
1:e9cc323: package org.apache.mahout.classifier.df.builder;
1:e9cc323: 
1:e9cc323: import org.apache.mahout.classifier.df.data.Data;
1:e9cc323: import org.apache.mahout.classifier.df.data.Dataset;
1:e9cc323: import org.apache.mahout.classifier.df.data.Instance;
1:e9cc323: import org.apache.mahout.classifier.df.data.conditions.Condition;
1:e9cc323: import org.apache.mahout.classifier.df.node.CategoricalNode;
1:e9cc323: import org.apache.mahout.classifier.df.node.Leaf;
1:e9cc323: import org.apache.mahout.classifier.df.node.Node;
1:e9cc323: import org.apache.mahout.classifier.df.node.NumericalNode;
1:e9cc323: import org.apache.mahout.classifier.df.split.IgSplit;
1:e9cc323: import org.apache.mahout.classifier.df.split.OptIgSplit;
1:e9cc323: import org.apache.mahout.classifier.df.split.RegressionSplit;
1:e9cc323: import org.apache.mahout.classifier.df.split.Split;
1:e9cc323: import org.slf4j.Logger;
1:e9cc323: import org.slf4j.LoggerFactory;
1:e9cc323: 
1:564c3e1: import java.util.Collection;
1:85f9ece: import java.util.HashSet;
1:d6aba1a: import java.util.Random;
1:e9cc323: 
1:e9cc323: /**
1:e9cc323:  * Builds a classification tree or regression tree<br>
1:e9cc323:  * A classification tree is built when the criterion variable is the categorical attribute.<br>
1:e9cc323:  * A regression tree is built when the criterion variable is the numerical attribute.
1:e9cc323:  */
1:1ffa3a4: @Deprecated
1:e9cc323: public class DecisionTreeBuilder implements TreeBuilder {
1:d6aba1a: 
1:e9cc323:   private static final Logger log = LoggerFactory.getLogger(DecisionTreeBuilder.class);
1:e9cc323: 
1:e9cc323:   private static final int[] NO_ATTRIBUTES = new int[0];
1:564c3e1:   private static final double EPSILON = 1.0e-6;
1:e9cc323: 
1:d6aba1a:   /**
1:d6aba1a:    * indicates which CATEGORICAL attributes have already been selected in the parent nodes
1:d6aba1a:    */
1:e9cc323:   private boolean[] selected;
1:d6aba1a:   /**
1:d6aba1a:    * number of attributes to select randomly at each node
1:d6aba1a:    */
1:d6aba1a:   private int m;
1:d6aba1a:   /**
1:d6aba1a:    * IgSplit implementation
1:d6aba1a:    */
1:e9cc323:   private IgSplit igSplit;
1:d6aba1a:   /**
1:d6aba1a:    * tree is complemented
1:d6aba1a:    */
1:e9cc323:   private boolean complemented = true;
1:d6aba1a:   /**
1:d6aba1a:    * minimum number for split
1:d6aba1a:    */
1:564c3e1:   private double minSplitNum = 2.0;
1:d6aba1a:   /**
1:d6aba1a:    * minimum proportion of the total variance for split
1:d6aba1a:    */
1:564c3e1:   private double minVarianceProportion = 1.0e-3;
1:d6aba1a:   /**
1:d6aba1a:    * full set data
1:d6aba1a:    */
1:e9cc323:   private Data fullSet;
1:d6aba1a:   /**
1:d6aba1a:    * minimum variance for split
1:d6aba1a:    */
1:e9cc323:   private double minVariance = Double.NaN;
1:d6aba1a: 
1:e9cc323:   public void setM(int m) {
1:e9cc323:     this.m = m;
1:e9cc323:   }
1:d6aba1a: 
1:e9cc323:   public void setIgSplit(IgSplit igSplit) {
1:e9cc323:     this.igSplit = igSplit;
1:e9cc323:   }
1:e9cc323: 
1:e9cc323:   public void setComplemented(boolean complemented) {
1:e9cc323:     this.complemented = complemented;
1:e9cc323:   }
1:d6aba1a: 
1:e9cc323:   public void setMinSplitNum(int minSplitNum) {
1:e9cc323:     this.minSplitNum = minSplitNum;
1:e9cc323:   }
1:e9cc323: 
1:e9cc323:   public void setMinVarianceProportion(double minVarianceProportion) {
1:e9cc323:     this.minVarianceProportion = minVarianceProportion;
1:e9cc323:   }
1:d6aba1a: 
1:e9cc323:   @Override
1:e9cc323:   public Node build(Random rng, Data data) {
1:e9cc323:     if (selected == null) {
1:e9cc323:       selected = new boolean[data.getDataset().nbAttributes()];
1:e9cc323:       selected[data.getDataset().getLabelId()] = true; // never select the label
1:e9cc323:     }
1:e9cc323:     if (m == 0) {
1:e9cc323:       // set default m
1:564c3e1:       double e = data.getDataset().nbAttributes() - 1;
1:e9cc323:       if (data.getDataset().isNumerical(data.getDataset().getLabelId())) {
1:e9cc323:         // regression
1:e9cc323:         m = (int) Math.ceil(e / 3.0);
1:e9cc323:       } else {
1:e9cc323:         // classification
1:e9cc323:         m = (int) Math.ceil(Math.sqrt(e));
1:e9cc323:       }
1:e9cc323:     }
1:e9cc323: 
1:e9cc323:     if (data.isEmpty()) {
1:1608f61:       return new Leaf(Double.NaN);
1:e9cc323:     }
1:e9cc323: 
1:564c3e1:     double sum = 0.0;
1:e9cc323:     if (data.getDataset().isNumerical(data.getDataset().getLabelId())) {
1:e9cc323:       // regression
1:e9cc323:       // sum and sum squared of a label is computed
1:564c3e1:       double sumSquared = 0.0;
1:e9cc323:       for (int i = 0; i < data.size(); i++) {
1:e9cc323:         double label = data.getDataset().getLabel(data.get(i));
1:e9cc323:         sum += label;
1:e9cc323:         sumSquared += label * label;
1:e9cc323:       }
1:e9cc323: 
1:e9cc323:       // computes the variance
1:e9cc323:       double var = sumSquared - (sum * sum) / data.size();
1:e9cc323: 
1:e9cc323:       // computes the minimum variance
1:e9cc323:       if (Double.compare(minVariance, Double.NaN) == 0) {
1:e9cc323:         minVariance = var / data.size() * minVarianceProportion;
1:e9cc323:         log.debug("minVariance:{}", minVariance);
1:e9cc323:       }
1:d6aba1a: 
1:e9cc323:       // variance is compared with minimum variance
1:e9cc323:       if ((var / data.size()) < minVariance) {
1:10c535c:         log.debug("variance({}) < minVariance({}) Leaf({})", var / data.size(), minVariance, sum / data.size());
1:e9cc323:         return new Leaf(sum / data.size());
1:e9cc323:       }
1:e9cc323:     } else {
1:e9cc323:       // classification
1:e9cc323:       if (isIdentical(data)) {
1:e9cc323:         return new Leaf(data.majorityLabel(rng));
1:e9cc323:       }
1:e9cc323:       if (data.identicalLabel()) {
1:e9cc323:         return new Leaf(data.getDataset().getLabel(data.get(0)));
1:e9cc323:       }
1:e9cc323:     }
1:e9cc323: 
1:e9cc323:     // store full set data
1:e9cc323:     if (fullSet == null) {
1:e9cc323:       fullSet = data;
1:e9cc323:     }
1:d6aba1a: 
1:e9cc323:     int[] attributes = randomAttributes(rng, selected, m);
1:e9cc323:     if (attributes == null || attributes.length == 0) {
1:e9cc323:       // we tried all the attributes and could not split the data anymore
1:e9cc323:       double label;
1:e9cc323:       if (data.getDataset().isNumerical(data.getDataset().getLabelId())) {
1:e9cc323:         // regression
1:e9cc323:         label = sum / data.size();
1:e9cc323:       } else {
1:e9cc323:         // classification
1:e9cc323:         label = data.majorityLabel(rng);
1:e9cc323:       }
1:e9cc323:       log.warn("attribute which can be selected is not found Leaf({})", label);
1:e9cc323:       return new Leaf(label);
1:e9cc323:     }
1:e9cc323: 
1:e9cc323:     if (igSplit == null) {
1:e9cc323:       if (data.getDataset().isNumerical(data.getDataset().getLabelId())) {
1:e9cc323:         // regression
1:e9cc323:         igSplit = new RegressionSplit();
1:e9cc323:       } else {
1:e9cc323:         // classification
1:e9cc323:         igSplit = new OptIgSplit();
1:e9cc323:       }
1:e9cc323:     }
1:e9cc323: 
1:e9cc323:     // find the best split
1:e9cc323:     Split best = null;
1:e9cc323:     for (int attr : attributes) {
1:e9cc323:       Split split = igSplit.computeSplit(data, attr);
1:e9cc323:       if (best == null || best.getIg() < split.getIg()) {
1:e9cc323:         best = split;
1:e9cc323:       }
1:e9cc323:     }
1:e9cc323: 
1:e9cc323:     // information gain is near to zero.
1:e9cc323:     if (best.getIg() < EPSILON) {
1:e9cc323:       double label;
1:e9cc323:       if (data.getDataset().isNumerical(data.getDataset().getLabelId())) {
1:e9cc323:         label = sum / data.size();
1:e9cc323:       } else {
1:e9cc323:         label = data.majorityLabel(rng);
1:e9cc323:       }
1:e9cc323:       log.debug("ig is near to zero Leaf({})", label);
1:e9cc323:       return new Leaf(label);
1:e9cc323:     }
1:e9cc323: 
1:4ca6b86:     log.debug("best split attr:{}, split:{}, ig:{}", best.getAttr(), best.getSplit(), best.getIg());
1:e9cc323: 
1:e9cc323:     boolean alreadySelected = selected[best.getAttr()];
1:e9cc323:     if (alreadySelected) {
1:e9cc323:       // attribute already selected
1:e9cc323:       log.warn("attribute {} already selected in a parent node", best.getAttr());
1:e9cc323:     }
1:d6aba1a: 
1:e9cc323:     Node childNode;
1:e9cc323:     if (data.getDataset().isNumerical(best.getAttr())) {
1:e9cc323:       boolean[] temp = null;
1:e9cc323: 
1:e9cc323:       Data loSubset = data.subset(Condition.lesser(best.getAttr(), best.getSplit()));
1:e9cc323:       Data hiSubset = data.subset(Condition.greaterOrEquals(best.getAttr(), best.getSplit()));
1:e9cc323: 
1:e9cc323:       if (loSubset.isEmpty() || hiSubset.isEmpty()) {
1:e9cc323:         // the selected attribute did not change the data, avoid using it in the child notes
1:e9cc323:         selected[best.getAttr()] = true;
1:e9cc323:       } else {
1:e9cc323:         // the data changed, so we can unselect all previousely selected NUMERICAL attributes
1:e9cc323:         temp = selected;
1:e9cc323:         selected = cloneCategoricalAttributes(data.getDataset(), selected);
1:e9cc323:       }
1:d6aba1a: 
1:e9cc323:       // size of the subset is less than the minSpitNum
1:e9cc323:       if (loSubset.size() < minSplitNum || hiSubset.size() < minSplitNum) {
1:e9cc323:         // branch is not split
1:e9cc323:         double label;
1:e9cc323:         if (data.getDataset().isNumerical(data.getDataset().getLabelId())) {
1:e9cc323:           label = sum / data.size();
1:e9cc323:         } else {
1:e9cc323:           label = data.majorityLabel(rng);
1:e9cc323:         }
1:e9cc323:         log.debug("branch is not split Leaf({})", label);
1:e9cc323:         return new Leaf(label);
1:e9cc323:       }
1:e9cc323: 
1:e9cc323:       Node loChild = build(rng, loSubset);
1:e9cc323:       Node hiChild = build(rng, hiSubset);
1:e9cc323: 
1:e9cc323:       // restore the selection state of the attributes
1:e9cc323:       if (temp != null) {
1:e9cc323:         selected = temp;
1:e9cc323:       } else {
1:e9cc323:         selected[best.getAttr()] = alreadySelected;
1:e9cc323:       }
1:d6aba1a: 
1:e9cc323:       childNode = new NumericalNode(best.getAttr(), best.getSplit(), loChild, hiChild);
1:e9cc323:     } else { // CATEGORICAL attribute
1:e9cc323:       double[] values = data.values(best.getAttr());
1:e9cc323: 
1:e9cc323:       // tree is complemented
1:564c3e1:       Collection<Double> subsetValues = null;
1:e9cc323:       if (complemented) {
1:85f9ece:         subsetValues = new HashSet<>();
1:e9cc323:         for (double value : values) {
1:e9cc323:           subsetValues.add(value);
1:e9cc323:         }
1:e9cc323:         values = fullSet.values(best.getAttr());
1:e9cc323:       }
1:e9cc323: 
1:e9cc323:       int cnt = 0;
1:e9cc323:       Data[] subsets = new Data[values.length];
1:e9cc323:       for (int index = 0; index < values.length; index++) {
1:e9cc323:         if (complemented && !subsetValues.contains(values[index])) {
1:e9cc323:           continue;
1:e9cc323:         }
1:e9cc323:         subsets[index] = data.subset(Condition.equals(best.getAttr(), values[index]));
1:e9cc323:         if (subsets[index].size() >= minSplitNum) {
1:e9cc323:           cnt++;
1:e9cc323:         }
1:e9cc323:       }
1:e9cc323: 
1:e9cc323:       // size of the subset is less than the minSpitNum
1:e9cc323:       if (cnt < 2) {
1:e9cc323:         // branch is not split
1:e9cc323:         double label;
1:e9cc323:         if (data.getDataset().isNumerical(data.getDataset().getLabelId())) {
1:e9cc323:           label = sum / data.size();
1:e9cc323:         } else {
1:e9cc323:           label = data.majorityLabel(rng);
1:e9cc323:         }
1:e9cc323:         log.debug("branch is not split Leaf({})", label);
1:e9cc323:         return new Leaf(label);
1:e9cc323:       }
1:e9cc323: 
1:e9cc323:       selected[best.getAttr()] = true;
1:e9cc323: 
1:e9cc323:       Node[] children = new Node[values.length];
1:e9cc323:       for (int index = 0; index < values.length; index++) {
1:e9cc323:         if (complemented && (subsetValues == null || !subsetValues.contains(values[index]))) {
1:e9cc323:           // tree is complemented
1:e9cc323:           double label;
1:e9cc323:           if (data.getDataset().isNumerical(data.getDataset().getLabelId())) {
1:e9cc323:             label = sum / data.size();
1:e9cc323:           } else {
1:e9cc323:             label = data.majorityLabel(rng);
1:e9cc323:           }
1:e9cc323:           log.debug("complemented Leaf({})", label);
1:e9cc323:           children[index] = new Leaf(label);
1:e9cc323:           continue;
1:e9cc323:         }
1:e9cc323:         children[index] = build(rng, subsets[index]);
1:e9cc323:       }
1:e9cc323: 
1:e9cc323:       selected[best.getAttr()] = alreadySelected;
1:e9cc323: 
1:e9cc323:       childNode = new CategoricalNode(best.getAttr(), values, children);
1:e9cc323:     }
1:d6aba1a: 
1:e9cc323:     return childNode;
1:e9cc323:   }
1:d6aba1a: 
1:e9cc323:   /**
1:e9cc323:    * checks if all the vectors have identical attribute values. Ignore selected attributes.
1:d6aba1a:    *
1:e9cc323:    * @return true is all the vectors are identical or the data is empty<br>
1:e9cc323:    *         false otherwise
1:e9cc323:    */
1:e9cc323:   private boolean isIdentical(Data data) {
1:e9cc323:     if (data.isEmpty()) {
1:e9cc323:       return true;
1:e9cc323:     }
1:d6aba1a: 
1:e9cc323:     Instance instance = data.get(0);
1:e9cc323:     for (int attr = 0; attr < selected.length; attr++) {
1:e9cc323:       if (selected[attr]) {
1:e9cc323:         continue;
1:e9cc323:       }
1:d6aba1a: 
1:e9cc323:       for (int index = 1; index < data.size(); index++) {
1:e9cc323:         if (data.get(index).get(attr) != instance.get(attr)) {
1:e9cc323:           return false;
1:e9cc323:         }
1:e9cc323:       }
1:e9cc323:     }
1:d6aba1a: 
1:e9cc323:     return true;
1:e9cc323:   }
1:e9cc323: 
1:e9cc323:   /**
1:e9cc323:    * Make a copy of the selection state of the attributes, unselect all numerical attributes
1:d6aba1a:    *
1:e9cc323:    * @param selected selection state to clone
1:e9cc323:    * @return cloned selection state
1:e9cc323:    */
1:e9cc323:   private static boolean[] cloneCategoricalAttributes(Dataset dataset, boolean[] selected) {
1:e9cc323:     boolean[] cloned = new boolean[selected.length];
1:e9cc323: 
1:e9cc323:     for (int i = 0; i < selected.length; i++) {
1:e9cc323:       cloned[i] = !dataset.isNumerical(i) && selected[i];
1:e9cc323:     }
1:e9cc323:     cloned[dataset.getLabelId()] = true;
1:e9cc323: 
1:e9cc323:     return cloned;
1:e9cc323:   }
1:e9cc323: 
1:e9cc323:   /**
1:e9cc323:    * Randomly selects m attributes to consider for split, excludes IGNORED and LABEL attributes
1:d6aba1a:    *
1:d6aba1a:    * @param rng      random-numbers generator
1:d6aba1a:    * @param selected attributes' state (selected or not)
1:d6aba1a:    * @param m        number of attributes to choose
1:e9cc323:    * @return list of selected attributes' indices, or null if all attributes have already been selected
1:e9cc323:    */
1:e9cc323:   private static int[] randomAttributes(Random rng, boolean[] selected, int m) {
1:e9cc323:     int nbNonSelected = 0; // number of non selected attributes
1:e9cc323:     for (boolean sel : selected) {
1:e9cc323:       if (!sel) {
1:e9cc323:         nbNonSelected++;
1:e9cc323:       }
1:e9cc323:     }
1:d6aba1a: 
1:e9cc323:     if (nbNonSelected == 0) {
1:e9cc323:       log.warn("All attributes are selected !");
1:e9cc323:       return NO_ATTRIBUTES;
1:e9cc323:     }
1:d6aba1a: 
1:e9cc323:     int[] result;
1:e9cc323:     if (nbNonSelected <= m) {
1:e9cc323:       // return all non selected attributes
1:e9cc323:       result = new int[nbNonSelected];
1:e9cc323:       int index = 0;
1:e9cc323:       for (int attr = 0; attr < selected.length; attr++) {
1:e9cc323:         if (!selected[attr]) {
1:e9cc323:           result[index++] = attr;
1:e9cc323:         }
1:e9cc323:       }
1:e9cc323:     } else {
1:e9cc323:       result = new int[m];
1:e9cc323:       for (int index = 0; index < m; index++) {
1:e9cc323:         // randomly choose a "non selected" attribute
1:e9cc323:         int rind;
1:e9cc323:         do {
1:e9cc323:           rind = rng.nextInt(selected.length);
1:e9cc323:         } while (selected[rind]);
1:d6aba1a: 
1:e9cc323:         result[index] = rind;
1:e9cc323:         selected[rind] = true; // temporarily set the chosen attribute to be selected
1:e9cc323:       }
1:d6aba1a: 
1:e9cc323:       // the chosen attributes are not yet selected
1:e9cc323:       for (int attr : result) {
1:e9cc323:         selected[attr] = false;
1:e9cc323:       }
1:e9cc323:     }
1:d6aba1a: 
1:e9cc323:     return result;
1:e9cc323:   }
1:e9cc323: }
============================================================================
author:smarthi
-------------------------------------------------------------------------------
commit:1ffa3a4
/////////////////////////////////////////////////////////////////////////
1: @Deprecated
author:Suneel Marthi
-------------------------------------------------------------------------------
commit:85f9ece
/////////////////////////////////////////////////////////////////////////
/////////////////////////////////////////////////////////////////////////
1: import java.util.HashSet;
/////////////////////////////////////////////////////////////////////////
1:         subsetValues = new HashSet<>();
author:pferrel
-------------------------------------------------------------------------------
commit:b988c49
author:frankscholten
-------------------------------------------------------------------------------
commit:1a42d85
author:Sean R. Owen
-------------------------------------------------------------------------------
commit:4ca6b86
/////////////////////////////////////////////////////////////////////////
1:     log.debug("best split attr:{}, split:{}, ig:{}", best.getAttr(), best.getSplit(), best.getIg());
commit:10c535c
/////////////////////////////////////////////////////////////////////////
1:         log.debug("variance({}) < minVariance({}) Leaf({})", var / data.size(), minVariance, sum / data.size());
/////////////////////////////////////////////////////////////////////////
0:     log.debug("best split attr:{}, split:{}, ig:{}", best.getIg(), best.getAttr(), best.getSplit(), best.getIg());
commit:1608f61
/////////////////////////////////////////////////////////////////////////
1:       return new Leaf(Double.NaN);
commit:564c3e1
/////////////////////////////////////////////////////////////////////////
1: import java.util.Collection;
/////////////////////////////////////////////////////////////////////////
1:   private static final double EPSILON = 1.0e-6;
/////////////////////////////////////////////////////////////////////////
1:   private double minSplitNum = 2.0;
1:   private double minVarianceProportion = 1.0e-3;
/////////////////////////////////////////////////////////////////////////
1:       double e = data.getDataset().nbAttributes() - 1;
/////////////////////////////////////////////////////////////////////////
1:     double sum = 0.0;
1:       double sumSquared = 0.0;
/////////////////////////////////////////////////////////////////////////
0:             (sum / data.size()) + ')');
/////////////////////////////////////////////////////////////////////////
1:       Collection<Double> subsetValues = null;
author:tcp
-------------------------------------------------------------------------------
commit:44459bd
/////////////////////////////////////////////////////////////////////////
0:     log.debug("best split attr:" + best.getAttr() + ", split:" + best.getSplit() + ", ig:" 
0:         + best.getIg());
author:Abdel Hakim Deneche
-------------------------------------------------------------------------------
commit:d6aba1a
/////////////////////////////////////////////////////////////////////////
0: import com.google.common.collect.Sets;
/////////////////////////////////////////////////////////////////////////
0: import java.util.HashSet;
1: import java.util.Random;
/////////////////////////////////////////////////////////////////////////
1: 
1:   /**
1:    * indicates which CATEGORICAL attributes have already been selected in the parent nodes
1:    */
1:   /**
1:    * number of attributes to select randomly at each node
1:    */
1:   private int m;
1:   /**
1:    * IgSplit implementation
1:    */
1:   /**
1:    * tree is complemented
1:    */
1:   /**
1:    * minimum number for split
1:    */
1:   /**
1:    * minimum proportion of the total variance for split
1:    */
1:   /**
1:    * full set data
1:    */
1:   /**
1:    * minimum variance for split
1:    */
1: 
1: 
/////////////////////////////////////////////////////////////////////////
1: 
/////////////////////////////////////////////////////////////////////////
1: 
/////////////////////////////////////////////////////////////////////////
1: 
0:         log.debug("variance(" + (var / data.size()) + ") < minVariance(" + minVariance + ") Leaf(" +
0:             (sum / data.size()) + ")");
/////////////////////////////////////////////////////////////////////////
1: 
/////////////////////////////////////////////////////////////////////////
0:         best.getIg());
1: 
/////////////////////////////////////////////////////////////////////////
1: 
/////////////////////////////////////////////////////////////////////////
1: 
1: 
1: 
1:    *
/////////////////////////////////////////////////////////////////////////
1: 
1: 
1: 
1:    *
/////////////////////////////////////////////////////////////////////////
1:    *
1:    * @param rng      random-numbers generator
1:    * @param selected attributes' state (selected or not)
1:    * @param m        number of attributes to choose
/////////////////////////////////////////////////////////////////////////
1: 
1: 
/////////////////////////////////////////////////////////////////////////
1: 
1: 
1: 
commit:e9cc323
/////////////////////////////////////////////////////////////////////////
1: /**
1:  * Licensed to the Apache Software Foundation (ASF) under one or more
1:  * contributor license agreements.  See the NOTICE file distributed with
1:  * this work for additional information regarding copyright ownership.
1:  * The ASF licenses this file to You under the Apache License, Version 2.0
1:  * (the "License"); you may not use this file except in compliance with
1:  * the License.  You may obtain a copy of the License at
1:  *
1:  *     http://www.apache.org/licenses/LICENSE-2.0
1:  *
1:  * Unless required by applicable law or agreed to in writing, software
1:  * distributed under the License is distributed on an "AS IS" BASIS,
1:  * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
1:  * See the License for the specific language governing permissions and
1:  * limitations under the License.
1:  */
1: 
1: package org.apache.mahout.classifier.df.builder;
1: 
0: import java.util.HashSet;
0: import java.util.Random;
1: 
1: import org.apache.mahout.classifier.df.data.Data;
1: import org.apache.mahout.classifier.df.data.Dataset;
1: import org.apache.mahout.classifier.df.data.Instance;
1: import org.apache.mahout.classifier.df.data.conditions.Condition;
1: import org.apache.mahout.classifier.df.node.CategoricalNode;
1: import org.apache.mahout.classifier.df.node.Leaf;
1: import org.apache.mahout.classifier.df.node.Node;
1: import org.apache.mahout.classifier.df.node.NumericalNode;
1: import org.apache.mahout.classifier.df.split.IgSplit;
1: import org.apache.mahout.classifier.df.split.OptIgSplit;
1: import org.apache.mahout.classifier.df.split.RegressionSplit;
1: import org.apache.mahout.classifier.df.split.Split;
1: import org.slf4j.Logger;
1: import org.slf4j.LoggerFactory;
1: 
0: import com.google.common.collect.Sets;
1: 
1: /**
1:  * Builds a classification tree or regression tree<br>
1:  * A classification tree is built when the criterion variable is the categorical attribute.<br>
1:  * A regression tree is built when the criterion variable is the numerical attribute.
1:  */
1: public class DecisionTreeBuilder implements TreeBuilder {
1:   
1:   private static final Logger log = LoggerFactory.getLogger(DecisionTreeBuilder.class);
1: 
1:   private static final int[] NO_ATTRIBUTES = new int[0];
0:   private static final double EPSILON = 1e-6;
1: 
0:   /** indicates which CATEGORICAL attributes have already been selected in the parent nodes */
1:   private boolean[] selected;
0:   /** number of attributes to select randomly at each node */
0:   private int m = 0;
0:   /** IgSplit implementation */
1:   private IgSplit igSplit;
0:   /** tree is complemented */
1:   private boolean complemented = true;
0:   /** minimum number for split */
0:   private double minSplitNum = 2;
0:   /** minimum proportion of the total variance for split */
0:   private double minVarianceProportion = 1e-3;
0:   /** full set data */
1:   private Data fullSet;
0:   /** minimum variance for split */
1:   private double minVariance = Double.NaN;
1:   
1:   public void setM(int m) {
1:     this.m = m;
1:   }
1:   
1:   public void setIgSplit(IgSplit igSplit) {
1:     this.igSplit = igSplit;
1:   }
1: 
1:   public void setComplemented(boolean complemented) {
1:     this.complemented = complemented;
1:   }
1:   
1:   public void setMinSplitNum(int minSplitNum) {
1:     this.minSplitNum = minSplitNum;
1:   }
1: 
1:   public void setMinVarianceProportion(double minVarianceProportion) {
1:     this.minVarianceProportion = minVarianceProportion;
1:   }
1:   
1:   @Override
1:   public Node build(Random rng, Data data) {
1:     if (selected == null) {
1:       selected = new boolean[data.getDataset().nbAttributes()];
1:       selected[data.getDataset().getLabelId()] = true; // never select the label
1:     }
1:     if (m == 0) {
1:       // set default m
0:       double e = (data.getDataset().nbAttributes() - 1);
1:       if (data.getDataset().isNumerical(data.getDataset().getLabelId())) {
1:         // regression
1:         m = (int) Math.ceil(e / 3.0);
1:       } else {
1:         // classification
1:         m = (int) Math.ceil(Math.sqrt(e));
1:       }
1:     }
1: 
1:     if (data.isEmpty()) {
0:       return new Leaf(-1);
1:     }
1: 
0:     double sum = 0;
0:     double sumSquared = 0;
1:     if (data.getDataset().isNumerical(data.getDataset().getLabelId())) {
1:       // regression
1:       // sum and sum squared of a label is computed
1:       for (int i = 0; i < data.size(); i++) {
1:         double label = data.getDataset().getLabel(data.get(i));
1:         sum += label;
1:         sumSquared += label * label;
1:       }
1: 
1:       // computes the variance
1:       double var = sumSquared - (sum * sum) / data.size();
1: 
1:       // computes the minimum variance
1:       if (Double.compare(minVariance, Double.NaN) == 0) {
1:         minVariance = var / data.size() * minVarianceProportion;
1:         log.debug("minVariance:{}", minVariance);
1:       }
1:       
1:       // variance is compared with minimum variance
1:       if ((var / data.size()) < minVariance) {
0:         log.debug("variance(" + (var /data.size()) + ") < minVariance(" + minVariance + ") Leaf(" +
0:           (sum / data.size()) + ")");
1:         return new Leaf(sum / data.size());
1:       }
1:     } else {
1:       // classification
1:       if (isIdentical(data)) {
1:         return new Leaf(data.majorityLabel(rng));
1:       }
1:       if (data.identicalLabel()) {
1:         return new Leaf(data.getDataset().getLabel(data.get(0)));
1:       }
1:     }
1: 
1:     // store full set data
1:     if (fullSet == null) {
1:       fullSet = data;
1:     }
1:     
1:     int[] attributes = randomAttributes(rng, selected, m);
1:     if (attributes == null || attributes.length == 0) {
1:       // we tried all the attributes and could not split the data anymore
1:       double label;
1:       if (data.getDataset().isNumerical(data.getDataset().getLabelId())) {
1:         // regression
1:         label = sum / data.size();
1:       } else {
1:         // classification
1:         label = data.majorityLabel(rng);
1:       }
1:       log.warn("attribute which can be selected is not found Leaf({})", label);
1:       return new Leaf(label);
1:     }
1: 
1:     if (igSplit == null) {
1:       if (data.getDataset().isNumerical(data.getDataset().getLabelId())) {
1:         // regression
1:         igSplit = new RegressionSplit();
1:       } else {
1:         // classification
1:         igSplit = new OptIgSplit();
1:       }
1:     }
1: 
1:     // find the best split
1:     Split best = null;
1:     for (int attr : attributes) {
1:       Split split = igSplit.computeSplit(data, attr);
1:       if (best == null || best.getIg() < split.getIg()) {
1:         best = split;
1:       }
1:     }
1: 
1:     // information gain is near to zero.
1:     if (best.getIg() < EPSILON) {
1:       double label;
1:       if (data.getDataset().isNumerical(data.getDataset().getLabelId())) {
1:         label = sum / data.size();
1:       } else {
1:         label = data.majorityLabel(rng);
1:       }
1:       log.debug("ig is near to zero Leaf({})", label);
1:       return new Leaf(label);
1:     }
1: 
0:     log.debug("best split attr:" + best.getAttr() + ", split:" + best.getSplit() + ", ig:" +
0:       best.getIg());
1: 
1:     boolean alreadySelected = selected[best.getAttr()];
1:     if (alreadySelected) {
1:       // attribute already selected
1:       log.warn("attribute {} already selected in a parent node", best.getAttr());
1:     }
1:     
1:     Node childNode;
1:     if (data.getDataset().isNumerical(best.getAttr())) {
1:       boolean[] temp = null;
1: 
1:       Data loSubset = data.subset(Condition.lesser(best.getAttr(), best.getSplit()));
1:       Data hiSubset = data.subset(Condition.greaterOrEquals(best.getAttr(), best.getSplit()));
1: 
1:       if (loSubset.isEmpty() || hiSubset.isEmpty()) {
1:         // the selected attribute did not change the data, avoid using it in the child notes
1:         selected[best.getAttr()] = true;
1:       } else {
1:         // the data changed, so we can unselect all previousely selected NUMERICAL attributes
1:         temp = selected;
1:         selected = cloneCategoricalAttributes(data.getDataset(), selected);
1:       }
1: 
1:       // size of the subset is less than the minSpitNum
1:       if (loSubset.size() < minSplitNum || hiSubset.size() < minSplitNum) {
1:         // branch is not split
1:         double label;
1:         if (data.getDataset().isNumerical(data.getDataset().getLabelId())) {
1:           label = sum / data.size();
1:         } else {
1:           label = data.majorityLabel(rng);
1:         }
1:         log.debug("branch is not split Leaf({})", label);
1:         return new Leaf(label);
1:       }
1: 
1:       Node loChild = build(rng, loSubset);
1:       Node hiChild = build(rng, hiSubset);
1: 
1:       // restore the selection state of the attributes
1:       if (temp != null) {
1:         selected = temp;
1:       } else {
1:         selected[best.getAttr()] = alreadySelected;
1:       }
1: 
1:       childNode = new NumericalNode(best.getAttr(), best.getSplit(), loChild, hiChild);
1:     } else { // CATEGORICAL attribute
1:       double[] values = data.values(best.getAttr());
1: 
1:       // tree is complemented
0:       HashSet<Double> subsetValues = null;
1:       if (complemented) {
0:         subsetValues = Sets.newHashSet();
1:         for (double value : values) {
1:           subsetValues.add(value);
1:         }
1:         values = fullSet.values(best.getAttr());
1:       }
1: 
1:       int cnt = 0;
1:       Data[] subsets = new Data[values.length];
1:       for (int index = 0; index < values.length; index++) {
1:         if (complemented && !subsetValues.contains(values[index])) {
1:           continue;
1:         }
1:         subsets[index] = data.subset(Condition.equals(best.getAttr(), values[index]));
1:         if (subsets[index].size() >= minSplitNum) {
1:           cnt++;
1:         }
1:       }
1: 
1:       // size of the subset is less than the minSpitNum
1:       if (cnt < 2) {
1:         // branch is not split
1:         double label;
1:         if (data.getDataset().isNumerical(data.getDataset().getLabelId())) {
1:           label = sum / data.size();
1:         } else {
1:           label = data.majorityLabel(rng);
1:         }
1:         log.debug("branch is not split Leaf({})", label);
1:         return new Leaf(label);
1:       }
1: 
1:       selected[best.getAttr()] = true;
1:       
1:       Node[] children = new Node[values.length];
1:       for (int index = 0; index < values.length; index++) {
1:         if (complemented && (subsetValues == null || !subsetValues.contains(values[index]))) {
1:           // tree is complemented
1:           double label;
1:           if (data.getDataset().isNumerical(data.getDataset().getLabelId())) {
1:             label = sum / data.size();
1:           } else {
1:             label = data.majorityLabel(rng);
1:           }
1:           log.debug("complemented Leaf({})", label);
1:           children[index] = new Leaf(label);
1:           continue;
1:         }
1:         children[index] = build(rng, subsets[index]);
1:       }
1: 
1:       selected[best.getAttr()] = alreadySelected;
1:       
1:       childNode = new CategoricalNode(best.getAttr(), values, children);
1:     }
1:     
1:     return childNode;
1:   }
1:   
1:   /**
1:    * checks if all the vectors have identical attribute values. Ignore selected attributes.
1:    * 
1:    * @return true is all the vectors are identical or the data is empty<br>
1:    *         false otherwise
1:    */
1:   private boolean isIdentical(Data data) {
1:     if (data.isEmpty()) {
1:       return true;
1:     }
1:     
1:     Instance instance = data.get(0);
1:     for (int attr = 0; attr < selected.length; attr++) {
1:       if (selected[attr]) {
1:         continue;
1:       }
1:       
1:       for (int index = 1; index < data.size(); index++) {
1:         if (data.get(index).get(attr) != instance.get(attr)) {
1:           return false;
1:         }
1:       }
1:     }
1:     
1:     return true;
1:   }
1: 
1:   /**
1:    * Make a copy of the selection state of the attributes, unselect all numerical attributes
1:    * @param selected selection state to clone
1:    * @return cloned selection state
1:    */
1:   private static boolean[] cloneCategoricalAttributes(Dataset dataset, boolean[] selected) {
1:     boolean[] cloned = new boolean[selected.length];
1: 
1:     for (int i = 0; i < selected.length; i++) {
1:       cloned[i] = !dataset.isNumerical(i) && selected[i];
1:     }
1:     cloned[dataset.getLabelId()] = true;
1: 
1:     return cloned;
1:   }
1: 
1:   /**
1:    * Randomly selects m attributes to consider for split, excludes IGNORED and LABEL attributes
1:    * 
0:    * @param rng
0:    *          random-numbers generator
0:    * @param selected
0:    *          attributes' state (selected or not)
0:    * @param m
0:    *          number of attributes to choose
1:    * @return list of selected attributes' indices, or null if all attributes have already been selected
1:    */
1:   private static int[] randomAttributes(Random rng, boolean[] selected, int m) {
1:     int nbNonSelected = 0; // number of non selected attributes
1:     for (boolean sel : selected) {
1:       if (!sel) {
1:         nbNonSelected++;
1:       }
1:     }
1:     
1:     if (nbNonSelected == 0) {
1:       log.warn("All attributes are selected !");
1:       return NO_ATTRIBUTES;
1:     }
1:     
1:     int[] result;
1:     if (nbNonSelected <= m) {
1:       // return all non selected attributes
1:       result = new int[nbNonSelected];
1:       int index = 0;
1:       for (int attr = 0; attr < selected.length; attr++) {
1:         if (!selected[attr]) {
1:           result[index++] = attr;
1:         }
1:       }
1:     } else {
1:       result = new int[m];
1:       for (int index = 0; index < m; index++) {
1:         // randomly choose a "non selected" attribute
1:         int rind;
1:         do {
1:           rind = rng.nextInt(selected.length);
1:         } while (selected[rind]);
1:         
1:         result[index] = rind;
1:         selected[rind] = true; // temporarily set the chosen attribute to be selected
1:       }
1:       
1:       // the chosen attributes are not yet selected
1:       for (int attr : result) {
1:         selected[attr] = false;
1:       }
1:     }
1:     
1:     return result;
1:   }
1: }
============================================================================