1:9a15cb8: /*
1:9a15cb8:  * Licensed to the Apache Software Foundation (ASF) under one or more
1:9a15cb8:  * contributor license agreements.  See the NOTICE file distributed with
1:9a15cb8:  * this work for additional information regarding copyright ownership.
1:9a15cb8:  * The ASF licenses this file to You under the Apache License, Version 2.0
1:9a15cb8:  * (the "License"); you may not use this file except in compliance with
1:9a15cb8:  * the License.  You may obtain a copy of the License at
1:9a15cb8:  *
1:9a15cb8:  *     http://www.apache.org/licenses/LICENSE-2.0
1:9a15cb8:  *
1:9a15cb8:  * Unless required by applicable law or agreed to in writing, software
1:9a15cb8:  * distributed under the License is distributed on an "AS IS" BASIS,
1:9a15cb8:  * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
1:9a15cb8:  * See the License for the specific language governing permissions and
1:9a15cb8:  * limitations under the License.
1:9a15cb8:  */
1:9a15cb8: 
1:9a15cb8: package org.apache.mahout.text.wikipedia;
1:9a15cb8: 
1:9a15cb8: import java.io.File;
1:9a15cb8: import java.io.IOException;
1:85f9ece: import java.util.HashSet;
1:9a15cb8: import java.util.Locale;
1:9a15cb8: import java.util.Set;
1:9a15cb8: 
1:9a15cb8: import org.apache.commons.cli2.CommandLine;
1:9a15cb8: import org.apache.commons.cli2.Group;
1:9a15cb8: import org.apache.commons.cli2.Option;
1:9a15cb8: import org.apache.commons.cli2.OptionException;
1:9a15cb8: import org.apache.commons.cli2.builder.ArgumentBuilder;
1:9a15cb8: import org.apache.commons.cli2.builder.DefaultOptionBuilder;
1:9a15cb8: import org.apache.commons.cli2.builder.GroupBuilder;
1:9a15cb8: import org.apache.commons.cli2.commandline.Parser;
1:9a15cb8: import org.apache.hadoop.conf.Configuration;
1:9a15cb8: import org.apache.hadoop.fs.Path;
1:9a15cb8: import org.apache.hadoop.io.DefaultStringifier;
1:4ca6b86: import org.apache.hadoop.io.Stringifier;
1:9a15cb8: import org.apache.hadoop.io.Text;
1:9a15cb8: import org.apache.hadoop.mapreduce.Job;
1:9a15cb8: import org.apache.hadoop.mapreduce.lib.input.FileInputFormat;
1:9a15cb8: import org.apache.hadoop.mapreduce.lib.output.FileOutputFormat;
1:9a15cb8: import org.apache.hadoop.mapreduce.lib.output.TextOutputFormat;
1:9a15cb8: import org.apache.hadoop.util.GenericsUtil;
1:9a15cb8: import org.apache.lucene.analysis.Analyzer;
1:9a15cb8: import org.apache.mahout.common.ClassUtils;
1:9a15cb8: import org.apache.mahout.common.CommandLineUtil;
1:39b8c2d: import org.apache.mahout.common.HadoopUtil;
1:9a15cb8: import org.apache.mahout.common.commandline.DefaultOptionCreator;
1:9a15cb8: import org.apache.mahout.common.iterator.FileLineIterable;
1:9a15cb8: import org.slf4j.Logger;
1:9a15cb8: import org.slf4j.LoggerFactory;
1:9a15cb8: 
1:9a15cb8: /**
1:9a15cb8:  * Create and run the Wikipedia Dataset Creator.
1:9a15cb8:  */
1:9a15cb8: public final class WikipediaDatasetCreatorDriver {
1:9a15cb8:   private static final Logger log = LoggerFactory.getLogger(WikipediaDatasetCreatorDriver.class);
1:9a15cb8:   
1:9a15cb8:   private WikipediaDatasetCreatorDriver() { }
1:9a15cb8:   
1:9a15cb8:   /**
1:9a15cb8:    * Takes in two arguments:
1:9a15cb8:    * <ol>
1:9a15cb8:    * <li>The input {@link org.apache.hadoop.fs.Path} where the input documents live</li>
1:9a15cb8:    * <li>The output {@link org.apache.hadoop.fs.Path} where to write the classifier as a
1:9a15cb8:    * {@link org.apache.hadoop.io.SequenceFile}</li>
1:9a15cb8:    * </ol>
1:9a15cb8:    */
1:9a15cb8:   public static void main(String[] args) throws IOException, InterruptedException {
1:9a15cb8:     DefaultOptionBuilder obuilder = new DefaultOptionBuilder();
1:9a15cb8:     ArgumentBuilder abuilder = new ArgumentBuilder();
1:9a15cb8:     GroupBuilder gbuilder = new GroupBuilder();
1:9a15cb8:     
1:9a15cb8:     Option dirInputPathOpt = DefaultOptionCreator.inputOption().create();
1:9a15cb8:     
1:9a15cb8:     Option dirOutputPathOpt = DefaultOptionCreator.outputOption().create();
1:9a15cb8:     
1:9a15cb8:     Option categoriesOpt = obuilder.withLongName("categories").withRequired(true).withArgument(
1:9a15cb8:       abuilder.withName("categories").withMinimum(1).withMaximum(1).create()).withDescription(
1:9a15cb8:       "Location of the categories file.  One entry per line. "
1:9a15cb8:           + "Will be used to make a string match in Wikipedia Category field").withShortName("c").create();
1:9a15cb8:     
1:9a15cb8:     Option exactMatchOpt = obuilder.withLongName("exactMatch").withDescription(
1:9a15cb8:       "If set, then the category name must exactly match the "
1:9a15cb8:           + "entry in the categories file. Default is false").withShortName("e").create();
1:9a15cb8:     Option analyzerOpt = obuilder.withLongName("analyzer").withRequired(false).withArgument(
1:9a15cb8:       abuilder.withName("analyzer").withMinimum(1).withMaximum(1).create()).withDescription(
1:9a15cb8:       "The analyzer to use, must have a no argument constructor").withShortName("a").create();
1:9a15cb8:     Option helpOpt = DefaultOptionCreator.helpOption();
1:9a15cb8:     
1:9a15cb8:     Group group = gbuilder.withName("Options").withOption(categoriesOpt).withOption(dirInputPathOpt)
1:9a15cb8:         .withOption(dirOutputPathOpt).withOption(exactMatchOpt).withOption(analyzerOpt).withOption(helpOpt)
1:9a15cb8:         .create();
1:9a15cb8:     
1:9a15cb8:     Parser parser = new Parser();
1:9a15cb8:     parser.setGroup(group);
1:9a15cb8:     try {
1:9a15cb8:       CommandLine cmdLine = parser.parse(args);
1:9a15cb8:       if (cmdLine.hasOption(helpOpt)) {
1:9a15cb8:         CommandLineUtil.printHelp(group);
1:9a15cb8:         return;
2:9a15cb8:       }
1:9a15cb8:       
1:9a15cb8:       String inputPath = (String) cmdLine.getValue(dirInputPathOpt);
1:9a15cb8:       String outputPath = (String) cmdLine.getValue(dirOutputPathOpt);
1:9a15cb8:       String catFile = (String) cmdLine.getValue(categoriesOpt);
1:9a15cb8:       Class<? extends Analyzer> analyzerClass = WikipediaAnalyzer.class;
1:9a15cb8:       if (cmdLine.hasOption(analyzerOpt)) {
1:9a15cb8:         String className = cmdLine.getValue(analyzerOpt).toString();
1:9a15cb8:         analyzerClass = Class.forName(className).asSubclass(Analyzer.class);
1:9a15cb8:         // try instantiating it, b/c there isn't any point in setting it if
1:9a15cb8:         // you can't instantiate it
1:9a15cb8:         ClassUtils.instantiateAs(analyzerClass, Analyzer.class);
1:9a15cb8:       }
1:9a15cb8:       runJob(inputPath, outputPath, catFile, cmdLine.hasOption(exactMatchOpt),
1:9a15cb8:         analyzerClass);
1:9a15cb8:     } catch (OptionException e) {
1:9a15cb8:       log.error("Exception", e);
1:9a15cb8:       CommandLineUtil.printHelp(group);
1:9a15cb8:     } catch (ClassNotFoundException e) {
1:9a15cb8:       log.error("Exception", e);
1:9a15cb8:       CommandLineUtil.printHelp(group);
1:9a15cb8:     }
1:9a15cb8:   }
1:9a15cb8:   
1:9a15cb8:   /**
1:9a15cb8:    * Run the job
1:9a15cb8:    * 
1:9a15cb8:    * @param input
1:9a15cb8:    *          the input pathname String
1:9a15cb8:    * @param output
1:9a15cb8:    *          the output pathname String
1:9a15cb8:    * @param catFile
1:9a15cb8:    *          the file containing the Wikipedia categories
1:9a15cb8:    * @param exactMatchOnly
1:9a15cb8:    *          if true, then the Wikipedia category must match exactly instead of simply containing the
1:9a15cb8:    *          category string
1:9a15cb8:    */
1:9a15cb8:   public static void runJob(String input,
1:9a15cb8:                             String output,
1:9a15cb8:                             String catFile,
1:9a15cb8:                             boolean exactMatchOnly,
1:9a15cb8:                             Class<? extends Analyzer> analyzerClass)
1:9a15cb8:     throws IOException, InterruptedException, ClassNotFoundException {
1:9a15cb8:     Configuration conf = new Configuration();
1:9a15cb8:     conf.set("key.value.separator.in.input.line", " ");
1:9a15cb8:     conf.set("xmlinput.start", "<page>");
1:9a15cb8:     conf.set("xmlinput.end", "</page>");
1:9a15cb8:     conf.setBoolean("exact.match.only", exactMatchOnly);
1:9a15cb8:     conf.set("analyzer.class", analyzerClass.getName());
1:9a15cb8:     conf.set("io.serializations",
1:9a15cb8:              "org.apache.hadoop.io.serializer.JavaSerialization,"
1:9a15cb8:              + "org.apache.hadoop.io.serializer.WritableSerialization");
1:9a15cb8:     // Dont ever forget this. People should keep track of how hadoop conf
1:9a15cb8:     // parameters can make or break a piece of code
1:9a15cb8:     
1:85f9ece:     Set<String> categories = new HashSet<>();
1:9a15cb8:     for (String line : new FileLineIterable(new File(catFile))) {
1:9a15cb8:       categories.add(line.trim().toLowerCase(Locale.ENGLISH));
1:9a15cb8:     }
1:9a15cb8:     
1:4ca6b86:     Stringifier<Set<String>> setStringifier =
1:02ff22f:         new DefaultStringifier<>(conf, GenericsUtil.getClass(categories));
1:9a15cb8:     
1:9a15cb8:     String categoriesStr = setStringifier.toString(categories);
1:9a15cb8:     
1:9a15cb8:     conf.set("wikipedia.categories", categoriesStr);
1:9a15cb8:     
1:9a15cb8:     Job job = new Job(conf);
1:8396a27:     log.info("Input: {} Out: {} Categories: {}", input, output, catFile);
1:9a15cb8:     job.setJarByClass(WikipediaDatasetCreatorDriver.class);
1:9a15cb8:     job.setOutputKeyClass(Text.class);
1:9a15cb8:     job.setOutputValueClass(Text.class);
1:9a15cb8:     job.setMapperClass(WikipediaDatasetCreatorMapper.class);
1:9a15cb8:     //TODO: job.setNumMapTasks(100);
1:9a15cb8:     job.setInputFormatClass(XmlInputFormat.class);
1:9a15cb8:     job.setReducerClass(WikipediaDatasetCreatorReducer.class);
1:9a15cb8:     job.setOutputFormatClass(TextOutputFormat.class);
1:9a15cb8:     
1:9a15cb8:     FileInputFormat.setInputPaths(job, new Path(input));
1:9a15cb8:     Path outPath = new Path(output);
1:9a15cb8:     FileOutputFormat.setOutputPath(job, outPath);
1:9a15cb8:     HadoopUtil.delete(conf, outPath);
1:9a15cb8:     
1:9a15cb8:     boolean succeeded = job.waitForCompletion(true);
1:9a15cb8:     if (!succeeded) {
1:9a15cb8:       throw new IllegalStateException("Job failed!");
1:9a15cb8:     }
1:9a15cb8:   }
1:9a15cb8: }
============================================================================
author:Karl Richter
-------------------------------------------------------------------------------
commit:02ff22f
/////////////////////////////////////////////////////////////////////////
1:         new DefaultStringifier<>(conf, GenericsUtil.getClass(categories));
author:Suneel Marthi
-------------------------------------------------------------------------------
commit:85f9ece
/////////////////////////////////////////////////////////////////////////
1: import java.util.HashSet;
/////////////////////////////////////////////////////////////////////////
1:     Set<String> categories = new HashSet<>();
author:Sean R. Owen
-------------------------------------------------------------------------------
commit:4ca6b86
/////////////////////////////////////////////////////////////////////////
1: import org.apache.hadoop.io.Stringifier;
/////////////////////////////////////////////////////////////////////////
1:     Stringifier<Set<String>> setStringifier =
commit:8396a27
/////////////////////////////////////////////////////////////////////////
1:     log.info("Input: {} Out: {} Categories: {}", input, output, catFile);
author:Ted Dunning
-------------------------------------------------------------------------------
commit:402e296
/////////////////////////////////////////////////////////////////////////
author:Sebastian Schelter
-------------------------------------------------------------------------------
commit:210b265
/////////////////////////////////////////////////////////////////////////
0: import com.google.common.collect.Sets;
/////////////////////////////////////////////////////////////////////////
0:     Set<String> categories = Sets.newHashSet();
author:Robin Anil
-------------------------------------------------------------------------------
commit:39b8c2d
/////////////////////////////////////////////////////////////////////////
1: import org.apache.mahout.common.HadoopUtil;
commit:9a15cb8
/////////////////////////////////////////////////////////////////////////
1: /*
1:  * Licensed to the Apache Software Foundation (ASF) under one or more
1:  * contributor license agreements.  See the NOTICE file distributed with
1:  * this work for additional information regarding copyright ownership.
1:  * The ASF licenses this file to You under the Apache License, Version 2.0
1:  * (the "License"); you may not use this file except in compliance with
1:  * the License.  You may obtain a copy of the License at
1:  *
1:  *     http://www.apache.org/licenses/LICENSE-2.0
1:  *
1:  * Unless required by applicable law or agreed to in writing, software
1:  * distributed under the License is distributed on an "AS IS" BASIS,
1:  * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
1:  * See the License for the specific language governing permissions and
1:  * limitations under the License.
1:  */
1: 
1: package org.apache.mahout.text.wikipedia;
1: 
1: import java.io.File;
1: import java.io.IOException;
0: import java.util.HashSet;
1: import java.util.Locale;
1: import java.util.Set;
1: 
1: import org.apache.commons.cli2.CommandLine;
1: import org.apache.commons.cli2.Group;
1: import org.apache.commons.cli2.Option;
1: import org.apache.commons.cli2.OptionException;
1: import org.apache.commons.cli2.builder.ArgumentBuilder;
1: import org.apache.commons.cli2.builder.DefaultOptionBuilder;
1: import org.apache.commons.cli2.builder.GroupBuilder;
1: import org.apache.commons.cli2.commandline.Parser;
1: import org.apache.hadoop.conf.Configuration;
1: import org.apache.hadoop.fs.Path;
1: import org.apache.hadoop.io.DefaultStringifier;
1: import org.apache.hadoop.io.Text;
1: import org.apache.hadoop.mapreduce.Job;
1: import org.apache.hadoop.mapreduce.lib.input.FileInputFormat;
1: import org.apache.hadoop.mapreduce.lib.output.FileOutputFormat;
1: import org.apache.hadoop.mapreduce.lib.output.TextOutputFormat;
1: import org.apache.hadoop.util.GenericsUtil;
1: import org.apache.lucene.analysis.Analyzer;
0: import org.apache.mahout.analysis.WikipediaAnalyzer;
1: import org.apache.mahout.common.ClassUtils;
1: import org.apache.mahout.common.CommandLineUtil;
1: import org.apache.mahout.common.commandline.DefaultOptionCreator;
1: import org.apache.mahout.common.iterator.FileLineIterable;
0: import org.apache.mahout.common.HadoopUtil;
1: import org.slf4j.Logger;
1: import org.slf4j.LoggerFactory;
1: 
1: /**
1:  * Create and run the Wikipedia Dataset Creator.
1:  */
1: public final class WikipediaDatasetCreatorDriver {
1:   private static final Logger log = LoggerFactory.getLogger(WikipediaDatasetCreatorDriver.class);
1:   
1:   private WikipediaDatasetCreatorDriver() { }
1:   
1:   /**
1:    * Takes in two arguments:
1:    * <ol>
1:    * <li>The input {@link org.apache.hadoop.fs.Path} where the input documents live</li>
1:    * <li>The output {@link org.apache.hadoop.fs.Path} where to write the classifier as a
1:    * {@link org.apache.hadoop.io.SequenceFile}</li>
1:    * </ol>
1:    */
1:   public static void main(String[] args) throws IOException, InterruptedException {
1:     DefaultOptionBuilder obuilder = new DefaultOptionBuilder();
1:     ArgumentBuilder abuilder = new ArgumentBuilder();
1:     GroupBuilder gbuilder = new GroupBuilder();
1:     
1:     Option dirInputPathOpt = DefaultOptionCreator.inputOption().create();
1:     
1:     Option dirOutputPathOpt = DefaultOptionCreator.outputOption().create();
1:     
1:     Option categoriesOpt = obuilder.withLongName("categories").withRequired(true).withArgument(
1:       abuilder.withName("categories").withMinimum(1).withMaximum(1).create()).withDescription(
1:       "Location of the categories file.  One entry per line. "
1:           + "Will be used to make a string match in Wikipedia Category field").withShortName("c").create();
1:     
1:     Option exactMatchOpt = obuilder.withLongName("exactMatch").withDescription(
1:       "If set, then the category name must exactly match the "
1:           + "entry in the categories file. Default is false").withShortName("e").create();
1:     Option analyzerOpt = obuilder.withLongName("analyzer").withRequired(false).withArgument(
1:       abuilder.withName("analyzer").withMinimum(1).withMaximum(1).create()).withDescription(
1:       "The analyzer to use, must have a no argument constructor").withShortName("a").create();
1:     Option helpOpt = DefaultOptionCreator.helpOption();
1:     
1:     Group group = gbuilder.withName("Options").withOption(categoriesOpt).withOption(dirInputPathOpt)
1:         .withOption(dirOutputPathOpt).withOption(exactMatchOpt).withOption(analyzerOpt).withOption(helpOpt)
1:         .create();
1:     
1:     Parser parser = new Parser();
1:     parser.setGroup(group);
1:     try {
1:       CommandLine cmdLine = parser.parse(args);
1:       if (cmdLine.hasOption(helpOpt)) {
1:         CommandLineUtil.printHelp(group);
1:         return;
1:       }
1:       
1:       String inputPath = (String) cmdLine.getValue(dirInputPathOpt);
1:       String outputPath = (String) cmdLine.getValue(dirOutputPathOpt);
1:       String catFile = (String) cmdLine.getValue(categoriesOpt);
1:       Class<? extends Analyzer> analyzerClass = WikipediaAnalyzer.class;
1:       if (cmdLine.hasOption(analyzerOpt)) {
1:         String className = cmdLine.getValue(analyzerOpt).toString();
1:         analyzerClass = Class.forName(className).asSubclass(Analyzer.class);
1:         // try instantiating it, b/c there isn't any point in setting it if
1:         // you can't instantiate it
1:         ClassUtils.instantiateAs(analyzerClass, Analyzer.class);
1:       }
1:       runJob(inputPath, outputPath, catFile, cmdLine.hasOption(exactMatchOpt),
1:         analyzerClass);
1:     } catch (OptionException e) {
1:       log.error("Exception", e);
1:       CommandLineUtil.printHelp(group);
1:     } catch (ClassNotFoundException e) {
1:       log.error("Exception", e);
1:       CommandLineUtil.printHelp(group);
1:     }
1:   }
1:   
1:   /**
1:    * Run the job
1:    * 
1:    * @param input
1:    *          the input pathname String
1:    * @param output
1:    *          the output pathname String
1:    * @param catFile
1:    *          the file containing the Wikipedia categories
1:    * @param exactMatchOnly
1:    *          if true, then the Wikipedia category must match exactly instead of simply containing the
1:    *          category string
1:    */
1:   public static void runJob(String input,
1:                             String output,
1:                             String catFile,
1:                             boolean exactMatchOnly,
1:                             Class<? extends Analyzer> analyzerClass)
1:     throws IOException, InterruptedException, ClassNotFoundException {
1:     Configuration conf = new Configuration();
1:     conf.set("key.value.separator.in.input.line", " ");
1:     conf.set("xmlinput.start", "<page>");
1:     conf.set("xmlinput.end", "</page>");
1:     conf.setBoolean("exact.match.only", exactMatchOnly);
1:     conf.set("analyzer.class", analyzerClass.getName());
1:     conf.set("io.serializations",
1:              "org.apache.hadoop.io.serializer.JavaSerialization,"
1:              + "org.apache.hadoop.io.serializer.WritableSerialization");
1:     // Dont ever forget this. People should keep track of how hadoop conf
1:     // parameters can make or break a piece of code
1:     
0:     Set<String> categories = new HashSet<String>();
1:     for (String line : new FileLineIterable(new File(catFile))) {
1:       categories.add(line.trim().toLowerCase(Locale.ENGLISH));
1:     }
1:     
0:     DefaultStringifier<Set<String>> setStringifier =
0:         new DefaultStringifier<Set<String>>(conf, GenericsUtil.getClass(categories));
1:     
1:     String categoriesStr = setStringifier.toString(categories);
1:     
1:     conf.set("wikipedia.categories", categoriesStr);
1:     
1:     Job job = new Job(conf);
0:     if (log.isInfoEnabled()) {
0:       log.info("Input: {} Out: {} Categories: {}", new Object[] {input, output, catFile});
1:     }
1:     job.setJarByClass(WikipediaDatasetCreatorDriver.class);
1:     job.setOutputKeyClass(Text.class);
1:     job.setOutputValueClass(Text.class);
1:     job.setMapperClass(WikipediaDatasetCreatorMapper.class);
1:     //TODO: job.setNumMapTasks(100);
1:     job.setInputFormatClass(XmlInputFormat.class);
1:     job.setReducerClass(WikipediaDatasetCreatorReducer.class);
1:     job.setOutputFormatClass(TextOutputFormat.class);
1:     
1:     FileInputFormat.setInputPaths(job, new Path(input));
1:     Path outPath = new Path(output);
1:     FileOutputFormat.setOutputPath(job, outPath);
1:     HadoopUtil.delete(conf, outPath);
1:     
1:     boolean succeeded = job.waitForCompletion(true);
1:     if (!succeeded) {
1:       throw new IllegalStateException("Job failed!");
1:     }
1:   }
1: }
============================================================================