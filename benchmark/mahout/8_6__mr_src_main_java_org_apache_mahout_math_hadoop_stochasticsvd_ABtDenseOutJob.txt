1:5a2250c: /**
1:5a2250c:  * Licensed to the Apache Software Foundation (ASF) under one or more
1:5a2250c:  * contributor license agreements.  See the NOTICE file distributed with
1:5a2250c:  * this work for additional information regarding copyright ownership.
1:5a2250c:  * The ASF licenses this file to You under the Apache License, Version 2.0
1:5a2250c:  * (the "License"); you may not use this file except in compliance with
1:5a2250c:  * the License.  You may obtain a copy of the License at
2:5a2250c:  *
1:5a2250c:  *     http://www.apache.org/licenses/LICENSE-2.0
1:5a2250c:  *
1:5a2250c:  * Unless required by applicable law or agreed to in writing, software
1:5a2250c:  * distributed under the License is distributed on an "AS IS" BASIS,
1:5a2250c:  * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
1:5a2250c:  * See the License for the specific language governing permissions and
1:5a2250c:  * limitations under the License.
2:5a2250c:  */
14:5a2250c: 
1:5a2250c: package org.apache.mahout.math.hadoop.stochasticsvd;
1:5214b1d: 
1:5a2250c: import java.io.Closeable;
1:5a2250c: import java.io.IOException;
1:5a2250c: import java.text.NumberFormat;
1:5a2250c: import java.util.ArrayDeque;
1:5a2250c: import java.util.Arrays;
1:5a2250c: import java.util.Deque;
1:5a2250c: import java.util.Iterator;
1:5a2250c: import java.util.regex.Matcher;
1:5a2250c: 
1:58cc1ae: import com.google.common.collect.Lists;
1:67a531e: import org.apache.commons.lang3.Validate;
1:5a2250c: import org.apache.hadoop.conf.Configuration;
1:8bac914: import org.apache.hadoop.filecache.DistributedCache;
1:8bac914: import org.apache.hadoop.fs.FileStatus;
1:5a2250c: import org.apache.hadoop.fs.FileSystem;
1:5a2250c: import org.apache.hadoop.fs.Path;
1:5a2250c: import org.apache.hadoop.io.IntWritable;
1:5a2250c: import org.apache.hadoop.io.SequenceFile;
1:5a2250c: import org.apache.hadoop.io.SequenceFile.CompressionType;
1:5a2250c: import org.apache.hadoop.io.Writable;
1:5a2250c: import org.apache.hadoop.mapred.JobConf;
1:5a2250c: import org.apache.hadoop.mapred.OutputCollector;
1:5a2250c: import org.apache.hadoop.mapreduce.Job;
1:5a2250c: import org.apache.hadoop.mapreduce.Mapper;
1:5a2250c: import org.apache.hadoop.mapreduce.Reducer;
1:5a2250c: import org.apache.hadoop.mapreduce.lib.input.FileInputFormat;
1:5a2250c: import org.apache.hadoop.mapreduce.lib.input.SequenceFileInputFormat;
1:5a2250c: import org.apache.hadoop.mapreduce.lib.output.FileOutputFormat;
1:5a2250c: import org.apache.hadoop.mapreduce.lib.output.SequenceFileOutputFormat;
1:6d9179e: import org.apache.mahout.common.HadoopUtil;
1:5a2250c: import org.apache.mahout.common.IOUtils;
1:5a2250c: import org.apache.mahout.common.Pair;
1:5a2250c: import org.apache.mahout.common.iterator.sequencefile.PathType;
1:5a2250c: import org.apache.mahout.common.iterator.sequencefile.SequenceFileDirIterator;
1:5a2250c: import org.apache.mahout.math.DenseVector;
1:5a2250c: import org.apache.mahout.math.SequentialAccessSparseVector;
1:5a2250c: import org.apache.mahout.math.Vector;
1:5a2250c: import org.apache.mahout.math.VectorWritable;
1:175701c: import org.apache.mahout.math.function.Functions;
1:5a2250c: import org.apache.mahout.math.hadoop.stochasticsvd.qr.QRFirstStep;
1:5a2250c: 
1:5a2250c: /**
1:5a2250c:  * Computes ABt products, then first step of QR which is pushed down to the
1:5a2250c:  * reducer.
1:5a2250c:  */
1:5a2250c: @SuppressWarnings("deprecation")
1:10c535c: public final class ABtDenseOutJob {
1:5a2250c: 
1:5a2250c:   public static final String PROP_BT_PATH = "ssvd.Bt.path";
1:8bac914:   public static final String PROP_BT_BROADCAST = "ssvd.Bt.broadcast";
1:175701c:   public static final String PROP_SB_PATH = "ssvdpca.sb.path";
1:175701c:   public static final String PROP_SQ_PATH = "ssvdpca.sq.path";
1:175701c:   public static final String PROP_XI_PATH = "ssvdpca.xi.path";
1:5a2250c: 
1:5a2250c:   private ABtDenseOutJob() {
7:5a2250c:   }
1:5a2250c: 
1:5a2250c:   /**
1:5a2250c:    * So, here, i preload A block into memory.
1:5a2250c:    * <P>
1:5a2250c:    * 
1:5a2250c:    * A sparse matrix seems to be ideal for that but there are two reasons why i
1:5a2250c:    * am not using it:
1:5a2250c:    * <UL>
1:5a2250c:    * <LI>1) I don't know the full block height. so i may need to reallocate it
1:5a2250c:    * from time to time. Although this probably not a showstopper.
1:5a2250c:    * <LI>2) I found that RandomAccessSparseVectors seem to take much more memory
1:5a2250c:    * than the SequentialAccessSparseVectors.
1:5a2250c:    * </UL>
1:5a2250c:    * <P>
1:5a2250c:    * 
1:5a2250c:    */
1:5a2250c:   public static class ABtMapper
2:5a2250c:       extends
1:5a2250c:       Mapper<Writable, VectorWritable, SplitPartitionedWritable, DenseBlockWritable> {
1:5a2250c: 
1:5a2250c:     private SplitPartitionedWritable outKey;
1:87c15be:     private final Deque<Closeable> closeables = new ArrayDeque<>();
1:5a2250c:     private SequenceFileDirIterator<IntWritable, VectorWritable> btInput;
1:5a2250c:     private Vector[] aCols;
1:5a2250c:     private double[][] yiCols;
1:5a2250c:     private int aRowCount;
1:5a2250c:     private int kp;
1:5a2250c:     private int blockHeight;
1:8bac914:     private boolean distributedBt;
1:8bac914:     private Path[] btLocalPath;
1:8bac914:     private Configuration localFsConfig;
1:175701c:     /*
1:175701c:      * xi and s_q are PCA-related corrections, per MAHOUT-817
1:175701c:      */
1:175701c:     protected Vector xi;
1:175701c:     protected Vector sq;
1:5a2250c: 
1:5a2250c:     @Override
1:5a2250c:     protected void map(Writable key, VectorWritable value, Context context)
1:5a2250c:       throws IOException, InterruptedException {
1:5a2250c: 
1:5a2250c:       Vector vec = value.get();
1:5a2250c: 
1:5a2250c:       int vecSize = vec.size();
1:5a2250c:       if (aCols == null) {
1:5a2250c:         aCols = new Vector[vecSize];
1:5a2250c:       } else if (aCols.length < vecSize) {
1:5a2250c:         aCols = Arrays.copyOf(aCols, vecSize);
1:5a2250c:       }
1:5a2250c: 
1:5a2250c:       if (vec.isDense()) {
1:5a2250c:         for (int i = 0; i < vecSize; i++) {
1:5a2250c:           extendAColIfNeeded(i, aRowCount + 1);
1:5a2250c:           aCols[i].setQuick(aRowCount, vec.getQuick(i));
1:5a2250c:         }
1:5a2250c:       } else if (vec.size() > 0) {
1:dc62944:         for (Vector.Element vecEl : vec.nonZeroes()) {
1:5a2250c:           int i = vecEl.index();
1:5a2250c:           extendAColIfNeeded(i, aRowCount + 1);
1:5a2250c:           aCols[i].setQuick(aRowCount, vecEl.get());
1:5a2250c:         }
1:5a2250c:       }
1:5a2250c:       aRowCount++;
1:5a2250c:     }
1:5a2250c: 
1:5a2250c:     private void extendAColIfNeeded(int col, int rowCount) {
1:5a2250c:       if (aCols[col] == null) {
1:5a2250c:         aCols[col] =
1:5a2250c:           new SequentialAccessSparseVector(rowCount < blockHeight ? blockHeight
1:5a2250c:               : rowCount, 1);
1:5a2250c:       } else if (aCols[col].size() < rowCount) {
1:5a2250c:         Vector newVec =
1:5a2250c:           new SequentialAccessSparseVector(rowCount + blockHeight,
1:8bac914:                                            aCols[col].getNumNondefaultElements() << 1);
1:5a2250c:         newVec.viewPart(0, aCols[col].size()).assign(aCols[col]);
1:5a2250c:         aCols[col] = newVec;
1:5a2250c:       }
1:5a2250c:     }
1:5a2250c: 
1:5a2250c:     @Override
1:5a2250c:     protected void cleanup(Context context) throws IOException,
2:5a2250c:       InterruptedException {
1:5a2250c:       try {
1:5a2250c: 
1:5a2250c:         yiCols = new double[kp][];
1:5a2250c: 
1:5a2250c:         for (int i = 0; i < kp; i++) {
1:5a2250c:           yiCols[i] = new double[Math.min(aRowCount, blockHeight)];
1:5a2250c:         }
1:5a2250c: 
1:564c3e1:         int numPasses = (aRowCount - 1) / blockHeight + 1;
1:5a2250c: 
1:5a2250c:         String propBtPathStr = context.getConfiguration().get(PROP_BT_PATH);
1:5a2250c:         Validate.notNull(propBtPathStr, "Bt input is not set");
1:564c3e1:         Path btPath = new Path(propBtPathStr);
1:564c3e1:         DenseBlockWritable dbw = new DenseBlockWritable();
1:5a2250c: 
1:5214b1d:         /*
1:5a2250c:          * so it turns out that it may be much more efficient to do a few
1:5a2250c:          * independent passes over Bt accumulating the entire block in memory
1:5a2250c:          * than pass huge amount of blocks out to combiner. so we aim of course
1:5a2250c:          * to fit entire s x (k+p) dense block in memory where s is the number
1:5a2250c:          * of A rows in this split. If A is much sparser than (k+p) avg # of
1:5a2250c:          * elements per row then the block may exceed the split size. if this
1:5a2250c:          * happens, and if the given blockHeight is not high enough to
1:5a2250c:          * accomodate this (because of memory constraints), then we start
1:5a2250c:          * splitting s into several passes. since computation is cpu-bound
1:5a2250c:          * anyway, it should be o.k. for supersparse inputs. (as ok it can be
1:5a2250c:          * that projection is thicker than the original anyway, why would one
1:5a2250c:          * use that many k+p then).
1:5214b1d:          */
1:564c3e1:         int lastRowIndex = -1;
1:5a2250c:         for (int pass = 0; pass < numPasses; pass++) {
1:5214b1d: 
1:8bac914:           if (distributedBt) {
1:8bac914: 
1:8bac914:             btInput =
1:87c15be:               new SequenceFileDirIterator<>(btLocalPath, true, localFsConfig);
1:8bac914: 
1:8bac914:           } else {
1:8bac914: 
1:8bac914:             btInput =
1:87c15be:               new SequenceFileDirIterator<>(btPath, PathType.GLOB, null, null, true, context.getConfiguration());
1:8bac914:           }
1:5a2250c:           closeables.addFirst(btInput);
1:8bac914:           Validate.isTrue(btInput.hasNext(), "Empty B' input!");
1:8bac914: 
1:5a2250c:           int aRowBegin = pass * blockHeight;
1:5a2250c:           int bh = Math.min(blockHeight, aRowCount - aRowBegin);
1:8bac914: 
2:5a2250c:           /*
1:5a2250c:            * check if we need to trim block allocation
1:5a2250c:            */
1:5a2250c:           if (pass > 0) {
1:564c3e1:             if (bh == blockHeight) {
1:564c3e1:               for (int i = 0; i < kp; i++) {
1:5a2250c:                 Arrays.fill(yiCols[i], 0.0);
1:564c3e1:               }
1:564c3e1:             } else {
1:564c3e1: 
1:564c3e1:               for (int i = 0; i < kp; i++) {
1:564c3e1:                 yiCols[i] = null;
1:564c3e1:               }
1:564c3e1:               for (int i = 0; i < kp; i++) {
1:564c3e1:                 yiCols[i] = new double[bh];
1:564c3e1:               }
1:5214b1d:             }
1:5214b1d:           }
1:5214b1d: 
1:5a2250c:           while (btInput.hasNext()) {
1:5a2250c:             Pair<IntWritable, VectorWritable> btRec = btInput.next();
1:5a2250c:             int btIndex = btRec.getFirst().get();
1:5a2250c:             Vector btVec = btRec.getSecond().get();
1:5a2250c:             Vector aCol;
1:5a2250c:             if (btIndex > aCols.length || (aCol = aCols[btIndex]) == null
1:5a2250c:                 || aCol.size() == 0) {
1:5214b1d: 
1:5a2250c:               /* 100% zero A column in the block, skip it as sparse */
1:5a2250c:               continue;
1:5214b1d:             }
1:5a2250c:             int j = -1;
1:dc62944:             for (Vector.Element aEl : aCol.nonZeroes()) {
1:5a2250c:               j = aEl.index();
1:5214b1d: 
1:5a2250c:               /*
1:5a2250c:                * now we compute only swathes between aRowBegin..aRowBegin+bh
1:5a2250c:                * exclusive. it seems like a deficiency but in fact i think it
1:5a2250c:                * will balance itself out: either A is dense and then we
1:5a2250c:                * shouldn't have more than one pass and therefore filter
1:5a2250c:                * conditions will never kick in. Or, the only situation where we
1:5a2250c:                * can't fit Y_i block in memory is when A input is much sparser
1:5a2250c:                * than k+p per row. But if this is the case, then we'd be looking
1:8b194c8:                * at very few elements without engaging them in any operations so
1:5a2250c:                * even then it should be ok.
1:5a2250c:                */
1:564c3e1:               if (j < aRowBegin) {
1:5a2250c:                 continue;
1:564c3e1:               }
1:564c3e1:               if (j >= aRowBegin + bh) {
1:5a2250c:                 break;
1:564c3e1:               }
1:5214b1d: 
1:5a2250c:               /*
1:5a2250c:                * assume btVec is dense
1:5a2250c:                */
1:175701c:               if (xi != null) {
1:175701c:                 /*
1:175701c:                  * MAHOUT-817: PCA correction for B'. I rewrite the whole
1:175701c:                  * computation loop so i don't have to check if PCA correction
1:175701c:                  * is needed at individual element level. It looks bulkier this
1:175701c:                  * way but perhaps less wasteful on cpu.
1:175701c:                  */
1:175701c:                 for (int s = 0; s < kp; s++) {
1:175701c:                   // code defensively against shortened xi
1:175701c:                   double xii = xi.size() > btIndex ? xi.get(btIndex) : 0.0;
1:175701c:                   yiCols[s][j - aRowBegin] +=
1:175701c:                     aEl.get() * (btVec.getQuick(s) - xii * sq.get(s));
1:175701c:                 }
1:175701c:               } else {
1:175701c:                 /*
1:175701c:                  * no PCA correction
1:175701c:                  */
1:175701c:                 for (int s = 0; s < kp; s++) {
1:175701c:                   yiCols[s][j - aRowBegin] += aEl.get() * btVec.getQuick(s);
1:175701c:                 }
1:5a2250c:               }
1:5214b1d: 
1:5a2250c:             }
1:5a2250c:             if (lastRowIndex < j) {
1:5a2250c:               lastRowIndex = j;
1:5a2250c:             }
1:5a2250c:           }
1:5a2250c: 
1:5a2250c:           /*
1:5a2250c:            * so now we have stuff in yi
1:5a2250c:            */
1:5a2250c:           dbw.setBlock(yiCols);
1:5a2250c:           outKey.setTaskItemOrdinal(pass);
1:5a2250c:           context.write(outKey, dbw);
1:5a2250c: 
1:5a2250c:           closeables.remove(btInput);
1:5a2250c:           btInput.close();
1:5a2250c:         }
1:5a2250c: 
1:5a2250c:       } finally {
1:5a2250c:         IOUtils.close(closeables);
1:5a2250c:       }
1:5a2250c:     }
1:5a2250c: 
1:5a2250c:     @Override
1:564c3e1:     protected void setup(Context context) throws IOException,
1:5a2250c:       InterruptedException {
1:5a2250c: 
1:175701c:       Configuration conf = context.getConfiguration();
1:175701c:       int k = Integer.parseInt(conf.get(QRFirstStep.PROP_K));
1:175701c:       int p = Integer.parseInt(conf.get(QRFirstStep.PROP_P));
1:5a2250c:       kp = k + p;
1:5a2250c: 
1:5a2250c:       outKey = new SplitPartitionedWritable(context);
1:5a2250c: 
1:175701c:       blockHeight = conf.getInt(BtJob.PROP_OUTER_PROD_BLOCK_HEIGHT, -1);
1:175701c:       distributedBt = conf.get(PROP_BT_BROADCAST) != null;
1:8bac914:       if (distributedBt) {
1:6d9179e:         btLocalPath = HadoopUtil.getCachedFiles(conf);
1:8bac914:         localFsConfig = new Configuration();
1:8bac914:         localFsConfig.set("fs.default.name", "file:///");
1:8bac914:       }
1:5a2250c: 
1:175701c:       /*
1:175701c:        * PCA -related corrections (MAHOUT-817)
1:175701c:        */
1:175701c:       String xiPathStr = conf.get(PROP_XI_PATH);
1:175701c:       if (xiPathStr != null) {
1:175701c:         xi = SSVDHelper.loadAndSumUpVectors(new Path(xiPathStr), conf);
1:175701c:         sq =
1:175701c:           SSVDHelper.loadAndSumUpVectors(new Path(conf.get(PROP_SQ_PATH)), conf);
1:175701c:       }
1:175701c: 
1:5a2250c:     }
1:5a2250c:   }
1:5a2250c: 
1:5a2250c:   /**
1:5a2250c:    * QR first step pushed down to reducer.
1:5a2250c:    * 
1:5a2250c:    */
1:5a2250c:   public static class QRReducer
1:6d16230:     extends Reducer<SplitPartitionedWritable, DenseBlockWritable, SplitPartitionedWritable, VectorWritable> {
1:5a2250c: 
1:5a2250c:     /*
1:5a2250c:      * HACK: partition number formats in hadoop, copied. this may stop working
1:5a2250c:      * if it gets out of sync with newer hadoop version. But unfortunately rules
1:5a2250c:      * of forming output file names are not sufficiently exposed so we need to
1:5a2250c:      * hack it if we write the same split output from either mapper or reducer.
1:5a2250c:      * alternatively, we probably can replace it by our own output file naming
1:5a2250c:      * management completely and bypass MultipleOutputs entirely.
1:5a2250c:      */
1:5a2250c: 
1:8bac914:     private static final NumberFormat NUMBER_FORMAT =
1:8bac914:       NumberFormat.getInstance();
1:5a2250c:     static {
1:5a2250c:       NUMBER_FORMAT.setMinimumIntegerDigits(5);
1:5a2250c:       NUMBER_FORMAT.setGroupingUsed(false);
1:5a2250c:     }
1:5a2250c: 
1:58cc1ae:     private final Deque<Closeable> closeables = Lists.newLinkedList();
1:5a2250c: 
1:5a2250c:     protected int blockHeight;
1:5a2250c: 
1:5a2250c:     protected int lastTaskId = -1;
1:5a2250c: 
1:5a2250c:     protected OutputCollector<Writable, DenseBlockWritable> qhatCollector;
1:5a2250c:     protected OutputCollector<Writable, VectorWritable> rhatCollector;
1:5a2250c:     protected QRFirstStep qr;
1:5a2250c:     protected Vector yiRow;
1:175701c:     protected Vector sb;
1:5a2250c: 
1:5a2250c:     @Override
1:5a2250c:     protected void setup(Context context) throws IOException,
1:5a2250c:       InterruptedException {
1:175701c:       Configuration conf = context.getConfiguration();
1:175701c:       blockHeight = conf.getInt(BtJob.PROP_OUTER_PROD_BLOCK_HEIGHT, -1);
1:175701c:       String sbPathStr = conf.get(PROP_SB_PATH);
1:5a2250c: 
1:175701c:       /*
1:175701c:        * PCA -related corrections (MAHOUT-817)
1:175701c:        */
1:175701c:       if (sbPathStr != null) {
1:175701c:         sb = SSVDHelper.loadAndSumUpVectors(new Path(sbPathStr), conf);
1:175701c:       }
1:5a2250c:     }
1:5a2250c: 
1:5a2250c:     protected void setupBlock(Context context, SplitPartitionedWritable spw)
1:5a2250c:       throws InterruptedException, IOException {
1:5a2250c:       IOUtils.close(closeables);
1:5a2250c:       qhatCollector =
1:5a2250c:         createOutputCollector(QJob.OUTPUT_QHAT,
1:5a2250c:                               spw,
1:5a2250c:                               context,
1:5a2250c:                               DenseBlockWritable.class);
1:5a2250c:       rhatCollector =
1:5a2250c:         createOutputCollector(QJob.OUTPUT_RHAT,
1:5a2250c:                               spw,
1:5a2250c:                               context,
1:5a2250c:                               VectorWritable.class);
1:5a2250c:       qr =
1:5a2250c:         new QRFirstStep(context.getConfiguration(),
1:5a2250c:                         qhatCollector,
1:5a2250c:                         rhatCollector);
1:5a2250c:       closeables.addFirst(qr);
1:5a2250c:       lastTaskId = spw.getTaskId();
1:5a2250c: 
1:5a2250c:     }
1:5a2250c: 
1:5a2250c:     @Override
1:5a2250c:     protected void reduce(SplitPartitionedWritable key,
1:5a2250c:                           Iterable<DenseBlockWritable> values,
1:5a2250c:                           Context context) throws IOException,
1:5a2250c:       InterruptedException {
1:5a2250c: 
1:5a2250c:       if (key.getTaskId() != lastTaskId) {
1:5a2250c:         setupBlock(context, key);
1:5a2250c:       }
1:5a2250c: 
1:5a2250c:       Iterator<DenseBlockWritable> iter = values.iterator();
1:5a2250c:       DenseBlockWritable dbw = iter.next();
1:5a2250c:       double[][] yiCols = dbw.getBlock();
1:5a2250c:       if (iter.hasNext()) {
1:5a2250c:         throw new IOException("Unexpected extra Y_i block in reducer input.");
1:5a2250c:       }
1:5a2250c: 
1:5a2250c:       long blockBase = key.getTaskItemOrdinal() * blockHeight;
1:5a2250c:       int bh = yiCols[0].length;
1:5a2250c:       if (yiRow == null) {
1:5a2250c:         yiRow = new DenseVector(yiCols.length);
1:5a2250c:       }
1:5a2250c: 
1:5a2250c:       for (int k = 0; k < bh; k++) {
1:564c3e1:         for (int j = 0; j < yiCols.length; j++) {
1:5a2250c:           yiRow.setQuick(j, yiCols[j][k]);
1:564c3e1:         }
1:5a2250c: 
1:5a2250c:         key.setTaskItemOrdinal(blockBase + k);
1:175701c: 
1:175701c:         // pca offset correction if any
1:175701c:         if (sb != null) {
1:175701c:           yiRow.assign(sb, Functions.MINUS);
1:175701c:         }
1:175701c: 
1:5a2250c:         qr.collect(key, yiRow);
1:5a2250c:       }
1:5a2250c: 
1:5a2250c:     }
1:5a2250c: 
1:5a2250c:     private Path getSplitFilePath(String name,
1:5a2250c:                                   SplitPartitionedWritable spw,
1:5a2250c:                                   Context context) throws InterruptedException,
1:5a2250c:       IOException {
1:5a2250c:       String uniqueFileName = FileOutputFormat.getUniqueFile(context, name, "");
1:5a2250c:       uniqueFileName = uniqueFileName.replaceFirst("-r-", "-m-");
1:5a2250c:       uniqueFileName =
1:8bac914:         uniqueFileName.replaceFirst("\\d+$",
1:8bac914:                                     Matcher.quoteReplacement(NUMBER_FORMAT.format(spw.getTaskId())));
1:5a2250c:       return new Path(FileOutputFormat.getWorkOutputPath(context),
1:5a2250c:                       uniqueFileName);
1:5a2250c:     }
1:5a2250c: 
1:5a2250c:     /**
1:5a2250c:      * key doesn't matter here, only value does. key always gets substituted by
1:5a2250c:      * SPW.
1:5a2250c:      * 
1:5a2250c:      * @param <K>
1:5a2250c:      *          bogus
1:5a2250c:      */
1:6d16230:     private <K, V> OutputCollector<K, V> createOutputCollector(String name,
1:5a2250c:                               final SplitPartitionedWritable spw,
1:5a2250c:                               Context ctx,
1:6d16230:                               Class<V> valueClass) throws IOException, InterruptedException {
1:5a2250c:       Path outputPath = getSplitFilePath(name, spw, ctx);
1:5a2250c:       final SequenceFile.Writer w =
1:1de8cec:         SequenceFile.createWriter(FileSystem.get(outputPath.toUri(), ctx.getConfiguration()),
1:5a2250c:                                   ctx.getConfiguration(),
1:5a2250c:                                   outputPath,
1:5a2250c:                                   SplitPartitionedWritable.class,
1:5a2250c:                                   valueClass);
1:5a2250c:       closeables.addFirst(w);
1:5a2250c:       return new OutputCollector<K, V>() {
1:5a2250c:         @Override
1:5a2250c:         public void collect(K key, V val) throws IOException {
1:5a2250c:           w.append(spw, val);
1:5a2250c:         }
1:5a2250c:       };
1:5a2250c:     }
1:5a2250c: 
1:5a2250c:     @Override
1:5a2250c:     protected void cleanup(Context context) throws IOException,
1:5a2250c:       InterruptedException {
1:5a2250c: 
1:5a2250c:       IOUtils.close(closeables);
1:5a2250c:     }
1:5a2250c: 
1:5a2250c:   }
1:5a2250c: 
1:5a2250c:   public static void run(Configuration conf,
1:5a2250c:                          Path[] inputAPaths,
1:5a2250c:                          Path inputBtGlob,
1:175701c:                          Path xiPath,
1:175701c:                          Path sqPath,
1:175701c:                          Path sbPath,
1:5a2250c:                          Path outputPath,
1:5a2250c:                          int aBlockRows,
1:5a2250c:                          int minSplitSize,
1:5a2250c:                          int k,
1:5a2250c:                          int p,
1:5a2250c:                          int outerProdBlockHeight,
1:8bac914:                          int numReduceTasks,
1:8bac914:                          boolean broadcastBInput)
1:8bac914:     throws ClassNotFoundException, InterruptedException, IOException {
1:5a2250c: 
1:5a2250c:     JobConf oldApiJob = new JobConf(conf);
1:5a2250c: 
1:5a2250c:     Job job = new Job(oldApiJob);
1:5a2250c:     job.setJobName("ABt-job");
1:5a2250c:     job.setJarByClass(ABtDenseOutJob.class);
1:5a2250c: 
1:5a2250c:     job.setInputFormatClass(SequenceFileInputFormat.class);
1:5a2250c:     FileInputFormat.setInputPaths(job, inputAPaths);
1:5a2250c:     if (minSplitSize > 0) {
1:5a2250c:       FileInputFormat.setMinInputSplitSize(job, minSplitSize);
1:5a2250c:     }
1:5a2250c: 
1:5a2250c:     FileOutputFormat.setOutputPath(job, outputPath);
1:5a2250c: 
1:5a2250c:     SequenceFileOutputFormat.setOutputCompressionType(job,
1:5a2250c:                                                       CompressionType.BLOCK);
1:5a2250c: 
1:5a2250c:     job.setMapOutputKeyClass(SplitPartitionedWritable.class);
1:5a2250c:     job.setMapOutputValueClass(DenseBlockWritable.class);
1:5a2250c: 
1:5a2250c:     job.setOutputKeyClass(SplitPartitionedWritable.class);
1:5a2250c:     job.setOutputValueClass(VectorWritable.class);
1:5a2250c: 
1:5a2250c:     job.setMapperClass(ABtMapper.class);
1:5a2250c:     job.setReducerClass(QRReducer.class);
1:5a2250c: 
1:5a2250c:     job.getConfiguration().setInt(QJob.PROP_AROWBLOCK_SIZE, aBlockRows);
1:5a2250c:     job.getConfiguration().setInt(BtJob.PROP_OUTER_PROD_BLOCK_HEIGHT,
1:5a2250c:                                   outerProdBlockHeight);
1:5a2250c:     job.getConfiguration().setInt(QRFirstStep.PROP_K, k);
1:5a2250c:     job.getConfiguration().setInt(QRFirstStep.PROP_P, p);
1:5a2250c:     job.getConfiguration().set(PROP_BT_PATH, inputBtGlob.toString());
1:5a2250c: 
1:175701c:     /*
1:175701c:      * PCA-related options, MAHOUT-817
1:175701c:      */
1:175701c:     if (xiPath != null) {
1:175701c:       job.getConfiguration().set(PROP_XI_PATH, xiPath.toString());
1:175701c:       job.getConfiguration().set(PROP_SB_PATH, sbPath.toString());
1:175701c:       job.getConfiguration().set(PROP_SQ_PATH, sqPath.toString());
1:175701c:     }
1:175701c: 
1:5a2250c:     job.setNumReduceTasks(numReduceTasks);
1:5214b1d: 
1:8bac914:     // broadcast Bt files if required.
1:8bac914:     if (broadcastBInput) {
1:8bac914:       job.getConfiguration().set(PROP_BT_BROADCAST, "y");
1:8bac914: 
1:1de8cec:       FileSystem fs = FileSystem.get(inputBtGlob.toUri(), conf);
1:8bac914:       FileStatus[] fstats = fs.globStatus(inputBtGlob);
1:8bac914:       if (fstats != null) {
1:8bac914:         for (FileStatus fstat : fstats) {
1:8bac914:           /*
1:8bac914:            * new api is not enabled yet in our dependencies at this time, still
1:8bac914:            * using deprecated one
1:8bac914:            */
1:8bac914:           DistributedCache.addCacheFile(fstat.getPath().toUri(),
1:8bac914:                                         job.getConfiguration());
1:8bac914:         }
1:8bac914:       }
1:8bac914:     }
1:8bac914: 
1:5a2250c:     job.submit();
1:5a2250c:     job.waitForCompletion(false);
1:5214b1d: 
1:5a2250c:     if (!job.isSuccessful()) {
1:5a2250c:       throw new IOException("ABt job unsuccessful.");
1:5214b1d:     }
1:5a2250c: 
1:5214b1d:   }
1:5a2250c: 
1:5214b1d: }
============================================================================
author:pferrel
-------------------------------------------------------------------------------
commit:b988c49
author:Suneel Marthi
-------------------------------------------------------------------------------
commit:87c15be
/////////////////////////////////////////////////////////////////////////
1:     private final Deque<Closeable> closeables = new ArrayDeque<>();
/////////////////////////////////////////////////////////////////////////
1:               new SequenceFileDirIterator<>(btLocalPath, true, localFsConfig);
1:               new SequenceFileDirIterator<>(btPath, PathType.GLOB, null, null, true, context.getConfiguration());
/////////////////////////////////////////////////////////////////////////
author:frankscholten
-------------------------------------------------------------------------------
commit:1a42d85
author:Sean R. Owen
-------------------------------------------------------------------------------
commit:335a993
/////////////////////////////////////////////////////////////////////////
commit:10c535c
/////////////////////////////////////////////////////////////////////////
1: public final class ABtDenseOutJob {
commit:1de8cec
/////////////////////////////////////////////////////////////////////////
/////////////////////////////////////////////////////////////////////////
1:         SequenceFile.createWriter(FileSystem.get(outputPath.toUri(), ctx.getConfiguration()),
/////////////////////////////////////////////////////////////////////////
1:       FileSystem fs = FileSystem.get(inputBtGlob.toUri(), conf);
commit:564c3e1
/////////////////////////////////////////////////////////////////////////
/////////////////////////////////////////////////////////////////////////
1:         int numPasses = (aRowCount - 1) / blockHeight + 1;
1:         Path btPath = new Path(propBtPathStr);
1:         DenseBlockWritable dbw = new DenseBlockWritable();
/////////////////////////////////////////////////////////////////////////
1:         int lastRowIndex = -1;
/////////////////////////////////////////////////////////////////////////
1:             if (bh == blockHeight) {
1:               for (int i = 0; i < kp; i++) {
1:               }
1:             } else {
1: 
1:               for (int i = 0; i < kp; i++) {
1:                 yiCols[i] = null;
1:               }
1:               for (int i = 0; i < kp; i++) {
1:                 yiCols[i] = new double[bh];
1:               }
/////////////////////////////////////////////////////////////////////////
1:               if (j < aRowBegin) {
1:               }
1:               if (j >= aRowBegin + bh) {
1:               }
/////////////////////////////////////////////////////////////////////////
1:     protected void setup(Context context) throws IOException,
/////////////////////////////////////////////////////////////////////////
1:         for (int j = 0; j < yiCols.length; j++) {
1:         }
author:Sebastian Schelter
-------------------------------------------------------------------------------
commit:8b194c8
/////////////////////////////////////////////////////////////////////////
1:                * at very few elements without engaging them in any operations so
commit:58cc1ae
/////////////////////////////////////////////////////////////////////////
1: import com.google.common.collect.Lists;
/////////////////////////////////////////////////////////////////////////
0:                * at very few elements without engaging them in any OPERATIONS so
/////////////////////////////////////////////////////////////////////////
1:     private final Deque<Closeable> closeables = Lists.newLinkedList();
commit:6d9179e
/////////////////////////////////////////////////////////////////////////
1: import org.apache.mahout.common.HadoopUtil;
/////////////////////////////////////////////////////////////////////////
1:         btLocalPath = HadoopUtil.getCachedFiles(conf);
commit:6d16230
/////////////////////////////////////////////////////////////////////////
1:     extends Reducer<SplitPartitionedWritable, DenseBlockWritable, SplitPartitionedWritable, VectorWritable> {
/////////////////////////////////////////////////////////////////////////
1:     private <K, V> OutputCollector<K, V> createOutputCollector(String name,
1:                               Class<V> valueClass) throws IOException, InterruptedException {
author:Jacob Alexander Mannix
-------------------------------------------------------------------------------
commit:dc62944
/////////////////////////////////////////////////////////////////////////
1:         for (Vector.Element vecEl : vec.nonZeroes()) {
/////////////////////////////////////////////////////////////////////////
1:             for (Vector.Element aEl : aCol.nonZeroes()) {
author:smarthi
-------------------------------------------------------------------------------
commit:67a531e
/////////////////////////////////////////////////////////////////////////
1: import org.apache.commons.lang3.Validate;
author:Dmitriy Lyubimov
-------------------------------------------------------------------------------
commit:175701c
/////////////////////////////////////////////////////////////////////////
1: import org.apache.mahout.math.function.Functions;
/////////////////////////////////////////////////////////////////////////
1:   public static final String PROP_SB_PATH = "ssvdpca.sb.path";
1:   public static final String PROP_SQ_PATH = "ssvdpca.sq.path";
1:   public static final String PROP_XI_PATH = "ssvdpca.xi.path";
/////////////////////////////////////////////////////////////////////////
1:     /*
1:      * xi and s_q are PCA-related corrections, per MAHOUT-817
1:      */
1:     protected Vector xi;
1:     protected Vector sq;
/////////////////////////////////////////////////////////////////////////
1:               if (xi != null) {
1:                 /*
1:                  * MAHOUT-817: PCA correction for B'. I rewrite the whole
1:                  * computation loop so i don't have to check if PCA correction
1:                  * is needed at individual element level. It looks bulkier this
1:                  * way but perhaps less wasteful on cpu.
1:                  */
1:                 for (int s = 0; s < kp; s++) {
1:                   // code defensively against shortened xi
1:                   double xii = xi.size() > btIndex ? xi.get(btIndex) : 0.0;
1:                   yiCols[s][j - aRowBegin] +=
1:                     aEl.get() * (btVec.getQuick(s) - xii * sq.get(s));
1:                 }
1:               } else {
1:                 /*
1:                  * no PCA correction
1:                  */
1:                 for (int s = 0; s < kp; s++) {
1:                   yiCols[s][j - aRowBegin] += aEl.get() * btVec.getQuick(s);
1:                 }
/////////////////////////////////////////////////////////////////////////
1:       Configuration conf = context.getConfiguration();
1:       int k = Integer.parseInt(conf.get(QRFirstStep.PROP_K));
1:       int p = Integer.parseInt(conf.get(QRFirstStep.PROP_P));
1:       blockHeight = conf.getInt(BtJob.PROP_OUTER_PROD_BLOCK_HEIGHT, -1);
1:       distributedBt = conf.get(PROP_BT_BROADCAST) != null;
0:         btLocalPath = DistributedCache.getLocalCacheFiles(conf);
1:       /*
1:        * PCA -related corrections (MAHOUT-817)
1:        */
1:       String xiPathStr = conf.get(PROP_XI_PATH);
1:       if (xiPathStr != null) {
1:         xi = SSVDHelper.loadAndSumUpVectors(new Path(xiPathStr), conf);
1:         sq =
1:           SSVDHelper.loadAndSumUpVectors(new Path(conf.get(PROP_SQ_PATH)), conf);
1:       }
1: 
/////////////////////////////////////////////////////////////////////////
1:     protected Vector sb;
1:       Configuration conf = context.getConfiguration();
1:       blockHeight = conf.getInt(BtJob.PROP_OUTER_PROD_BLOCK_HEIGHT, -1);
1:       String sbPathStr = conf.get(PROP_SB_PATH);
1:       /*
1:        * PCA -related corrections (MAHOUT-817)
1:        */
1:       if (sbPathStr != null) {
1:         sb = SSVDHelper.loadAndSumUpVectors(new Path(sbPathStr), conf);
1:       }
/////////////////////////////////////////////////////////////////////////
1: 
1:         // pca offset correction if any
1:         if (sb != null) {
1:           yiRow.assign(sb, Functions.MINUS);
1:         }
1: 
/////////////////////////////////////////////////////////////////////////
1:                          Path xiPath,
1:                          Path sqPath,
1:                          Path sbPath,
/////////////////////////////////////////////////////////////////////////
1:     /*
1:      * PCA-related options, MAHOUT-817
1:      */
1:     if (xiPath != null) {
1:       job.getConfiguration().set(PROP_XI_PATH, xiPath.toString());
1:       job.getConfiguration().set(PROP_SB_PATH, sbPath.toString());
1:       job.getConfiguration().set(PROP_SQ_PATH, sqPath.toString());
1:     }
1: 
commit:8bac914
/////////////////////////////////////////////////////////////////////////
1: import org.apache.hadoop.filecache.DistributedCache;
1: import org.apache.hadoop.fs.FileStatus;
/////////////////////////////////////////////////////////////////////////
1:   public static final String PROP_BT_BROADCAST = "ssvd.Bt.broadcast";
/////////////////////////////////////////////////////////////////////////
1:     private boolean distributedBt;
1:     private Path[] btLocalPath;
1:     private Configuration localFsConfig;
/////////////////////////////////////////////////////////////////////////
0:         for (Iterator<Vector.Element> vecIter = vec.iterateNonZero(); vecIter.hasNext();) {
/////////////////////////////////////////////////////////////////////////
1:                                            aCols[col].getNumNondefaultElements() << 1);
/////////////////////////////////////////////////////////////////////////
1:           if (distributedBt) {
1: 
1:             btInput =
0:               new SequenceFileDirIterator<IntWritable, VectorWritable>(btLocalPath,
0:                                                                        null,
0:                                                                        true,
0:                                                                        localFsConfig);
1: 
1:           } else {
1: 
1:             btInput =
0:               new SequenceFileDirIterator<IntWritable, VectorWritable>(btPath,
0:                                                                        PathType.GLOB,
0:                                                                        null,
0:                                                                        null,
0:                                                                        true,
0:                                                                        context.getConfiguration());
1:           }
1:           Validate.isTrue(btInput.hasNext(), "Empty B' input!");
/////////////////////////////////////////////////////////////////////////
0:             for (Iterator<Vector.Element> aColIter = aCol.iterateNonZero(); aColIter.hasNext();) {
/////////////////////////////////////////////////////////////////////////
0:       distributedBt = context.getConfiguration().get(PROP_BT_BROADCAST) != null;
1:       if (distributedBt) {
1: 
0:         btLocalPath =
0:           DistributedCache.getLocalCacheFiles(context.getConfiguration());
1: 
1:         localFsConfig = new Configuration();
1:         localFsConfig.set("fs.default.name", "file:///");
1:       }
/////////////////////////////////////////////////////////////////////////
1:     private static final NumberFormat NUMBER_FORMAT =
1:       NumberFormat.getInstance();
/////////////////////////////////////////////////////////////////////////
1:         uniqueFileName.replaceFirst("\\d+$",
1:                                     Matcher.quoteReplacement(NUMBER_FORMAT.format(spw.getTaskId())));
/////////////////////////////////////////////////////////////////////////
1:                          int numReduceTasks,
1:                          boolean broadcastBInput)
1:     throws ClassNotFoundException, InterruptedException, IOException {
/////////////////////////////////////////////////////////////////////////
1:     // broadcast Bt files if required.
1:     if (broadcastBInput) {
1:       job.getConfiguration().set(PROP_BT_BROADCAST, "y");
1: 
0:       FileSystem fs = FileSystem.get(conf);
1:       FileStatus[] fstats = fs.globStatus(inputBtGlob);
1:       if (fstats != null) {
1:         for (FileStatus fstat : fstats) {
1:           /*
1:            * new api is not enabled yet in our dependencies at this time, still
1:            * using deprecated one
1:            */
1:           DistributedCache.addCacheFile(fstat.getPath().toUri(),
1:                                         job.getConfiguration());
1:         }
1:       }
1:     }
1: 
commit:ebeade9
/////////////////////////////////////////////////////////////////////////
/////////////////////////////////////////////////////////////////////////
/////////////////////////////////////////////////////////////////////////
/////////////////////////////////////////////////////////////////////////
0:         for (Iterator<Vector.Element> vecIter = vec.iterateNonZero(); vecIter
0:           .hasNext();) {
/////////////////////////////////////////////////////////////////////////
0:                                            aCols[col]
0:                                              .getNumNondefaultElements() << 1);
/////////////////////////////////////////////////////////////////////////
0:           btInput =
0:             new SequenceFileDirIterator<IntWritable, VectorWritable>(btPath,
0:                                                                      PathType.GLOB,
0:                                                                      null,
0:                                                                      null,
0:                                                                      true,
0:                                                                      context
0:                                                                        .getConfiguration());
/////////////////////////////////////////////////////////////////////////
0:             for (Iterator<Vector.Element> aColIter = aCol.iterateNonZero(); aColIter
0:               .hasNext();) {
/////////////////////////////////////////////////////////////////////////
/////////////////////////////////////////////////////////////////////////
0:     private static final NumberFormat NUMBER_FORMAT = NumberFormat
0:       .getInstance();
/////////////////////////////////////////////////////////////////////////
0:         uniqueFileName.replaceFirst("\\d+$", Matcher
0:           .quoteReplacement(NUMBER_FORMAT.format(spw.getTaskId())));
/////////////////////////////////////////////////////////////////////////
0:                          int numReduceTasks) throws ClassNotFoundException,
0:     InterruptedException, IOException {
/////////////////////////////////////////////////////////////////////////
commit:5214b1d
/////////////////////////////////////////////////////////////////////////
0: import org.apache.hadoop.filecache.DistributedCache;
0: import org.apache.hadoop.fs.FileStatus;
0: import org.apache.hadoop.fs.LocalFileSystem;
/////////////////////////////////////////////////////////////////////////
0:   public static final String PROP_BT_BROADCAST = "ssvd.Bt.broadcast";
/////////////////////////////////////////////////////////////////////////
0:     private boolean distributedBt;
0:     private Path btLocalPath;
/////////////////////////////////////////////////////////////////////////
0:         for (Iterator<Vector.Element> vecIter = vec.iterateNonZero(); vecIter.hasNext();) {
/////////////////////////////////////////////////////////////////////////
0:                                            aCols[col].getNumNondefaultElements() << 1);
/////////////////////////////////////////////////////////////////////////
0:           if (distributedBt) {
1: 
0:             btInput =
0:               new SequenceFileDirIterator<IntWritable, VectorWritable>(btLocalPath,
0:                                                                        PathType.LIST,
0:                                                                        null,
0:                                                                        null,
0:                                                                        true,
0:                                                                        new Configuration());
1: 
0:           } else {
1: 
0:             btInput =
0:               new SequenceFileDirIterator<IntWritable, VectorWritable>(btPath,
0:                                                                        PathType.GLOB,
0:                                                                        null,
0:                                                                        null,
0:                                                                        true,
0:                                                                        context.getConfiguration());
1:           }
/////////////////////////////////////////////////////////////////////////
0:             for (Iterator<Vector.Element> aColIter = aCol.iterateNonZero(); aColIter.hasNext();) {
/////////////////////////////////////////////////////////////////////////
0:       distributedBt = context.getConfiguration().get(PROP_BT_BROADCAST) != null;
0:       if (distributedBt) {
1: 
0:         Path[] btFiles =
0:           DistributedCache.getLocalCacheFiles(context.getConfiguration());
1: 
0:         String btLocalPathStr = "";
0:         Validate.notNull(btFiles,
0:                          "BT input comes empty from distributed cache.");
1: 
0:         for (Path btFile : btFiles) {
1: 
0:           if (btLocalPathStr.length() > 0)
0:             btLocalPathStr += Path.SEPARATOR_CHAR;
0:           btLocalPathStr += btFile;
1:         }
0:         btLocalPath = new Path(btLocalPathStr);
1:       }
/////////////////////////////////////////////////////////////////////////
0:     private static final NumberFormat NUMBER_FORMAT =
0:       NumberFormat.getInstance();
/////////////////////////////////////////////////////////////////////////
0:         uniqueFileName.replaceFirst("\\d+$",
0:                                     Matcher.quoteReplacement(NUMBER_FORMAT.format(spw.getTaskId())));
/////////////////////////////////////////////////////////////////////////
0:                          int numReduceTasks,
0:                          boolean broadcastBInput)
0:     throws ClassNotFoundException, InterruptedException, IOException {
/////////////////////////////////////////////////////////////////////////
0:     // broadcast Bt files if required.
0:     if (broadcastBInput) {
0:       job.getConfiguration().set(PROP_BT_BROADCAST, "y");
1: 
0:       FileSystem fs = FileSystem.get(conf);
0:       FileStatus[] fstats = fs.globStatus(inputBtGlob);
0:       if (fstats != null) {
0:         for (FileStatus fstat : fstats) {
1:           /*
0:            * new api is not enabled yet in our dependencies at this time, still
0:            * using deprecated one
1:            */
0:           DistributedCache.addCacheFile(fstat.getPath().toUri(),
0:                                         job.getConfiguration());
1:         }
1:       }
1:     }
1: 
commit:5a2250c
/////////////////////////////////////////////////////////////////////////
1: /**
1:  * Licensed to the Apache Software Foundation (ASF) under one or more
1:  * contributor license agreements.  See the NOTICE file distributed with
1:  * this work for additional information regarding copyright ownership.
1:  * The ASF licenses this file to You under the Apache License, Version 2.0
1:  * (the "License"); you may not use this file except in compliance with
1:  * the License.  You may obtain a copy of the License at
1:  *
1:  *     http://www.apache.org/licenses/LICENSE-2.0
1:  *
1:  * Unless required by applicable law or agreed to in writing, software
1:  * distributed under the License is distributed on an "AS IS" BASIS,
1:  * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
1:  * See the License for the specific language governing permissions and
1:  * limitations under the License.
1:  */
1: 
1: package org.apache.mahout.math.hadoop.stochasticsvd;
1: 
1: import java.io.Closeable;
1: import java.io.IOException;
1: import java.text.NumberFormat;
1: import java.util.ArrayDeque;
1: import java.util.Arrays;
1: import java.util.Deque;
1: import java.util.Iterator;
0: import java.util.LinkedList;
1: import java.util.regex.Matcher;
1: 
0: import org.apache.commons.lang.Validate;
1: import org.apache.hadoop.conf.Configuration;
1: import org.apache.hadoop.fs.FileSystem;
1: import org.apache.hadoop.fs.Path;
1: import org.apache.hadoop.io.IntWritable;
1: import org.apache.hadoop.io.SequenceFile;
1: import org.apache.hadoop.io.SequenceFile.CompressionType;
1: import org.apache.hadoop.io.Writable;
1: import org.apache.hadoop.mapred.JobConf;
1: import org.apache.hadoop.mapred.OutputCollector;
1: import org.apache.hadoop.mapreduce.Job;
1: import org.apache.hadoop.mapreduce.Mapper;
1: import org.apache.hadoop.mapreduce.Reducer;
1: import org.apache.hadoop.mapreduce.lib.input.FileInputFormat;
1: import org.apache.hadoop.mapreduce.lib.input.SequenceFileInputFormat;
1: import org.apache.hadoop.mapreduce.lib.output.FileOutputFormat;
1: import org.apache.hadoop.mapreduce.lib.output.SequenceFileOutputFormat;
1: import org.apache.mahout.common.IOUtils;
1: import org.apache.mahout.common.Pair;
1: import org.apache.mahout.common.iterator.sequencefile.PathType;
1: import org.apache.mahout.common.iterator.sequencefile.SequenceFileDirIterator;
1: import org.apache.mahout.math.DenseVector;
1: import org.apache.mahout.math.SequentialAccessSparseVector;
1: import org.apache.mahout.math.Vector;
1: import org.apache.mahout.math.VectorWritable;
1: import org.apache.mahout.math.hadoop.stochasticsvd.qr.QRFirstStep;
1: 
1: /**
1:  * Computes ABt products, then first step of QR which is pushed down to the
1:  * reducer.
1:  * 
1:  */
1: @SuppressWarnings("deprecation")
0: public class ABtDenseOutJob {
1: 
1:   public static final String PROP_BT_PATH = "ssvd.Bt.path";
1: 
1:   private ABtDenseOutJob() {
1:   }
1: 
1:   /**
1:    * So, here, i preload A block into memory.
1:    * <P>
1:    * 
1:    * A sparse matrix seems to be ideal for that but there are two reasons why i
1:    * am not using it:
1:    * <UL>
1:    * <LI>1) I don't know the full block height. so i may need to reallocate it
1:    * from time to time. Although this probably not a showstopper.
1:    * <LI>2) I found that RandomAccessSparseVectors seem to take much more memory
1:    * than the SequentialAccessSparseVectors.
1:    * </UL>
1:    * <P>
1:    * 
1:    */
1:   public static class ABtMapper
1:       extends
1:       Mapper<Writable, VectorWritable, SplitPartitionedWritable, DenseBlockWritable> {
1: 
1:     private SplitPartitionedWritable outKey;
0:     private final Deque<Closeable> closeables = new ArrayDeque<Closeable>();
1:     private SequenceFileDirIterator<IntWritable, VectorWritable> btInput;
1:     private Vector[] aCols;
1:     private double[][] yiCols;
1:     private int aRowCount;
1:     private int kp;
1:     private int blockHeight;
1: 
1:     @Override
1:     protected void map(Writable key, VectorWritable value, Context context)
1:       throws IOException, InterruptedException {
1: 
1:       Vector vec = value.get();
1: 
1:       int vecSize = vec.size();
1:       if (aCols == null) {
1:         aCols = new Vector[vecSize];
1:       } else if (aCols.length < vecSize) {
1:         aCols = Arrays.copyOf(aCols, vecSize);
1:       }
1: 
1:       if (vec.isDense()) {
1:         for (int i = 0; i < vecSize; i++) {
1:           extendAColIfNeeded(i, aRowCount + 1);
1:           aCols[i].setQuick(aRowCount, vec.getQuick(i));
1:         }
1:       } else if (vec.size() > 0) {
0:         for (Iterator<Vector.Element> vecIter = vec.iterateNonZero(); vecIter
0:           .hasNext();) {
0:           Vector.Element vecEl = vecIter.next();
1:           int i = vecEl.index();
1:           extendAColIfNeeded(i, aRowCount + 1);
1:           aCols[i].setQuick(aRowCount, vecEl.get());
1:         }
1:       }
1:       aRowCount++;
1:     }
1: 
1:     private void extendAColIfNeeded(int col, int rowCount) {
1:       if (aCols[col] == null) {
1:         aCols[col] =
1:           new SequentialAccessSparseVector(rowCount < blockHeight ? blockHeight
1:               : rowCount, 1);
1:       } else if (aCols[col].size() < rowCount) {
1:         Vector newVec =
1:           new SequentialAccessSparseVector(rowCount + blockHeight,
0:                                            aCols[col]
0:                                              .getNumNondefaultElements() << 1);
1:         newVec.viewPart(0, aCols[col].size()).assign(aCols[col]);
1:         aCols[col] = newVec;
1:       }
1:     }
1: 
1:     @Override
1:     protected void cleanup(Context context) throws IOException,
1:       InterruptedException {
1:       try {
1: 
0:         int lastRowIndex = -1;
1: 
1:         yiCols = new double[kp][];
1: 
1:         for (int i = 0; i < kp; i++) {
1:           yiCols[i] = new double[Math.min(aRowCount, blockHeight)];
1:         }
1: 
0:         final int numPasses = (aRowCount - 1) / blockHeight + 1;
1: 
1:         String propBtPathStr = context.getConfiguration().get(PROP_BT_PATH);
1:         Validate.notNull(propBtPathStr, "Bt input is not set");
0:         final Path btPath = new Path(propBtPathStr);
0:         final DenseBlockWritable dbw = new DenseBlockWritable();
1: 
1:         /*
1:          * so it turns out that it may be much more efficient to do a few
1:          * independent passes over Bt accumulating the entire block in memory
1:          * than pass huge amount of blocks out to combiner. so we aim of course
1:          * to fit entire s x (k+p) dense block in memory where s is the number
1:          * of A rows in this split. If A is much sparser than (k+p) avg # of
1:          * elements per row then the block may exceed the split size. if this
1:          * happens, and if the given blockHeight is not high enough to
1:          * accomodate this (because of memory constraints), then we start
1:          * splitting s into several passes. since computation is cpu-bound
1:          * anyway, it should be o.k. for supersparse inputs. (as ok it can be
1:          * that projection is thicker than the original anyway, why would one
1:          * use that many k+p then).
1:          */
1:         for (int pass = 0; pass < numPasses; pass++) {
1: 
0:           btInput =
0:             new SequenceFileDirIterator<IntWritable, VectorWritable>(btPath,
0:                                                                      PathType.GLOB,
0:                                                                      null,
0:                                                                      null,
0:                                                                      true,
0:                                                                      context
0:                                                                        .getConfiguration());
1:           closeables.addFirst(btInput);
1: 
1:           int aRowBegin = pass * blockHeight;
1:           int bh = Math.min(blockHeight, aRowCount - aRowBegin);
1: 
1:           /*
1:            * check if we need to trim block allocation
1:            */
1:           if (pass > 0) {
0:             if (bh != blockHeight) {
1: 
0:               for (int i = 0; i < kp; i++)
0:                 yiCols[i] = null;
0:               for (int i = 0; i < kp; i++)
0:                 yiCols[i] = new double[bh];
0:             } else {
0:               for (int i = 0; i < kp; i++)
1:                 Arrays.fill(yiCols[i], 0.0);
1:             }
1:           }
1: 
1:           while (btInput.hasNext()) {
1:             Pair<IntWritable, VectorWritable> btRec = btInput.next();
1:             int btIndex = btRec.getFirst().get();
1:             Vector btVec = btRec.getSecond().get();
1:             Vector aCol;
1:             if (btIndex > aCols.length || (aCol = aCols[btIndex]) == null
1:                 || aCol.size() == 0) {
1: 
1:               /* 100% zero A column in the block, skip it as sparse */
1:               continue;
1:             }
1:             int j = -1;
0:             for (Iterator<Vector.Element> aColIter = aCol.iterateNonZero(); aColIter
0:               .hasNext();) {
0:               Vector.Element aEl = aColIter.next();
1:               j = aEl.index();
1: 
1:               /*
1:                * now we compute only swathes between aRowBegin..aRowBegin+bh
1:                * exclusive. it seems like a deficiency but in fact i think it
1:                * will balance itself out: either A is dense and then we
1:                * shouldn't have more than one pass and therefore filter
1:                * conditions will never kick in. Or, the only situation where we
1:                * can't fit Y_i block in memory is when A input is much sparser
1:                * than k+p per row. But if this is the case, then we'd be looking
0:                * at very few elements without engaging them in any operations so
1:                * even then it should be ok.
1:                */
0:               if (j < aRowBegin)
1:                 continue;
0:               else if (j >= aRowBegin + bh)
1:                 break;
1: 
1:               /*
1:                * assume btVec is dense
1:                */
0:               for (int s = 0; s < kp; s++) {
0:                 yiCols[s][j - aRowBegin] += aEl.get() * btVec.getQuick(s);
1:               }
1: 
1:             }
1:             if (lastRowIndex < j) {
1:               lastRowIndex = j;
1:             }
1:           }
1: 
1:           /*
1:            * so now we have stuff in yi
1:            */
1:           dbw.setBlock(yiCols);
1:           outKey.setTaskItemOrdinal(pass);
1:           context.write(outKey, dbw);
1: 
1:           closeables.remove(btInput);
1:           btInput.close();
1:         }
1: 
1:       } finally {
1:         IOUtils.close(closeables);
1:       }
1:     }
1: 
1:     @Override
0:     protected void setup(final Context context) throws IOException,
1:       InterruptedException {
1: 
0:       int k =
0:         Integer.parseInt(context.getConfiguration().get(QRFirstStep.PROP_K));
0:       int p =
0:         Integer.parseInt(context.getConfiguration().get(QRFirstStep.PROP_P));
1:       kp = k + p;
1: 
1:       outKey = new SplitPartitionedWritable(context);
1: 
0:       blockHeight =
0:         context.getConfiguration().getInt(BtJob.PROP_OUTER_PROD_BLOCK_HEIGHT,
0:                                           -1);
1: 
1:     }
1:   }
1: 
1:   /**
1:    * QR first step pushed down to reducer.
1:    * 
1:    */
1:   public static class QRReducer
1:       extends
0:       Reducer<SplitPartitionedWritable, DenseBlockWritable, SplitPartitionedWritable, VectorWritable> {
1: 
1:     /*
1:      * HACK: partition number formats in hadoop, copied. this may stop working
1:      * if it gets out of sync with newer hadoop version. But unfortunately rules
1:      * of forming output file names are not sufficiently exposed so we need to
1:      * hack it if we write the same split output from either mapper or reducer.
1:      * alternatively, we probably can replace it by our own output file naming
1:      * management completely and bypass MultipleOutputs entirely.
1:      */
1: 
0:     private static final NumberFormat NUMBER_FORMAT = NumberFormat
0:       .getInstance();
1:     static {
1:       NUMBER_FORMAT.setMinimumIntegerDigits(5);
1:       NUMBER_FORMAT.setGroupingUsed(false);
1:     }
1: 
0:     private final Deque<Closeable> closeables = new LinkedList<Closeable>();
1: 
1:     protected int blockHeight;
1: 
0:     protected int accumSize;
1:     protected int lastTaskId = -1;
1: 
1:     protected OutputCollector<Writable, DenseBlockWritable> qhatCollector;
1:     protected OutputCollector<Writable, VectorWritable> rhatCollector;
1:     protected QRFirstStep qr;
1:     protected Vector yiRow;
1: 
1:     @Override
1:     protected void setup(Context context) throws IOException,
1:       InterruptedException {
0:       blockHeight =
0:         context.getConfiguration().getInt(BtJob.PROP_OUTER_PROD_BLOCK_HEIGHT,
0:                                           -1);
1: 
1:     }
1: 
1:     protected void setupBlock(Context context, SplitPartitionedWritable spw)
1:       throws InterruptedException, IOException {
1:       IOUtils.close(closeables);
1:       qhatCollector =
1:         createOutputCollector(QJob.OUTPUT_QHAT,
1:                               spw,
1:                               context,
1:                               DenseBlockWritable.class);
1:       rhatCollector =
1:         createOutputCollector(QJob.OUTPUT_RHAT,
1:                               spw,
1:                               context,
1:                               VectorWritable.class);
1:       qr =
1:         new QRFirstStep(context.getConfiguration(),
1:                         qhatCollector,
1:                         rhatCollector);
1:       closeables.addFirst(qr);
1:       lastTaskId = spw.getTaskId();
1: 
1:     }
1: 
1:     @Override
1:     protected void reduce(SplitPartitionedWritable key,
1:                           Iterable<DenseBlockWritable> values,
1:                           Context context) throws IOException,
1:       InterruptedException {
1: 
1:       if (key.getTaskId() != lastTaskId) {
1:         setupBlock(context, key);
1:       }
1: 
1:       Iterator<DenseBlockWritable> iter = values.iterator();
1:       DenseBlockWritable dbw = iter.next();
1:       double[][] yiCols = dbw.getBlock();
1:       if (iter.hasNext()) {
1:         throw new IOException("Unexpected extra Y_i block in reducer input.");
1:       }
1: 
1:       long blockBase = key.getTaskItemOrdinal() * blockHeight;
1:       int bh = yiCols[0].length;
1:       if (yiRow == null) {
1:         yiRow = new DenseVector(yiCols.length);
1:       }
1: 
1:       for (int k = 0; k < bh; k++) {
0:         for (int j = 0; j < yiCols.length; j++)
1:           yiRow.setQuick(j, yiCols[j][k]);
1: 
1:         key.setTaskItemOrdinal(blockBase + k);
1:         qr.collect(key, yiRow);
1:       }
1: 
1:     }
1: 
1:     private Path getSplitFilePath(String name,
1:                                   SplitPartitionedWritable spw,
1:                                   Context context) throws InterruptedException,
1:       IOException {
1:       String uniqueFileName = FileOutputFormat.getUniqueFile(context, name, "");
1:       uniqueFileName = uniqueFileName.replaceFirst("-r-", "-m-");
1:       uniqueFileName =
0:         uniqueFileName.replaceFirst("\\d+$", Matcher
0:           .quoteReplacement(NUMBER_FORMAT.format(spw.getTaskId())));
1:       return new Path(FileOutputFormat.getWorkOutputPath(context),
1:                       uniqueFileName);
1:     }
1: 
1:     /**
1:      * key doesn't matter here, only value does. key always gets substituted by
1:      * SPW.
1:      * 
1:      * @param <K>
1:      *          bogus
0:      * @param <V>
0:      * @param name
0:      * @param spw
0:      * @param ctx
0:      * @param valueClass
0:      * @return
0:      * @throws IOException
0:      * @throws InterruptedException
1:      */
0:     private <K, V> OutputCollector<K, V>
0:         createOutputCollector(String name,
1:                               final SplitPartitionedWritable spw,
1:                               Context ctx,
0:                               Class<V> valueClass) throws IOException,
1:           InterruptedException {
1:       Path outputPath = getSplitFilePath(name, spw, ctx);
1:       final SequenceFile.Writer w =
0:         SequenceFile.createWriter(FileSystem.get(ctx.getConfiguration()),
1:                                   ctx.getConfiguration(),
1:                                   outputPath,
1:                                   SplitPartitionedWritable.class,
1:                                   valueClass);
1:       closeables.addFirst(w);
1:       return new OutputCollector<K, V>() {
1:         @Override
1:         public void collect(K key, V val) throws IOException {
1:           w.append(spw, val);
1:         }
1:       };
1:     }
1: 
1:     @Override
1:     protected void cleanup(Context context) throws IOException,
1:       InterruptedException {
1: 
1:       IOUtils.close(closeables);
1:     }
1: 
1:   }
1: 
1:   public static void run(Configuration conf,
1:                          Path[] inputAPaths,
1:                          Path inputBtGlob,
1:                          Path outputPath,
1:                          int aBlockRows,
1:                          int minSplitSize,
1:                          int k,
1:                          int p,
1:                          int outerProdBlockHeight,
0:                          int numReduceTasks) throws ClassNotFoundException,
0:     InterruptedException, IOException {
1: 
1:     JobConf oldApiJob = new JobConf(conf);
1: 
1:     Job job = new Job(oldApiJob);
1:     job.setJobName("ABt-job");
1:     job.setJarByClass(ABtDenseOutJob.class);
1: 
1:     job.setInputFormatClass(SequenceFileInputFormat.class);
1:     FileInputFormat.setInputPaths(job, inputAPaths);
1:     if (minSplitSize > 0) {
1:       FileInputFormat.setMinInputSplitSize(job, minSplitSize);
1:     }
1: 
1:     FileOutputFormat.setOutputPath(job, outputPath);
1: 
1:     SequenceFileOutputFormat.setOutputCompressionType(job,
1:                                                       CompressionType.BLOCK);
1: 
1:     job.setMapOutputKeyClass(SplitPartitionedWritable.class);
1:     job.setMapOutputValueClass(DenseBlockWritable.class);
1: 
1:     job.setOutputKeyClass(SplitPartitionedWritable.class);
1:     job.setOutputValueClass(VectorWritable.class);
1: 
1:     job.setMapperClass(ABtMapper.class);
1:     job.setReducerClass(QRReducer.class);
1: 
1:     job.getConfiguration().setInt(QJob.PROP_AROWBLOCK_SIZE, aBlockRows);
1:     job.getConfiguration().setInt(BtJob.PROP_OUTER_PROD_BLOCK_HEIGHT,
1:                                   outerProdBlockHeight);
1:     job.getConfiguration().setInt(QRFirstStep.PROP_K, k);
1:     job.getConfiguration().setInt(QRFirstStep.PROP_P, p);
1:     job.getConfiguration().set(PROP_BT_PATH, inputBtGlob.toString());
1: 
1:     job.setNumReduceTasks(numReduceTasks);
1: 
1:     job.submit();
1:     job.waitForCompletion(false);
1: 
1:     if (!job.isSuccessful()) {
1:       throw new IOException("ABt job unsuccessful.");
1:     }
1: 
1:   }
1: 
1: }
============================================================================