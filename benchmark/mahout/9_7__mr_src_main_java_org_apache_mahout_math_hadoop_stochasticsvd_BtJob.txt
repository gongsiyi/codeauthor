1:151de0d: /**
1:151de0d:  * Licensed to the Apache Software Foundation (ASF) under one or more
1:151de0d:  * contributor license agreements.  See the NOTICE file distributed with
1:151de0d:  * this work for additional information regarding copyright ownership.
1:151de0d:  * The ASF licenses this file to You under the Apache License, Version 2.0
1:151de0d:  * (the "License"); you may not use this file except in compliance with
1:151de0d:  * the License.  You may obtain a copy of the License at
1:8bac914:  *
1:151de0d:  *     http://www.apache.org/licenses/LICENSE-2.0
3:ffc7fab:  *
1:151de0d:  * Unless required by applicable law or agreed to in writing, software
1:151de0d:  * distributed under the License is distributed on an "AS IS" BASIS,
1:151de0d:  * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
1:151de0d:  * See the License for the specific language governing permissions and
1:151de0d:  * limitations under the License.
1:151de0d:  */
22:151de0d: 
1:151de0d: package org.apache.mahout.math.hadoop.stochasticsvd;
1:151de0d: 
1:67a531e: import org.apache.commons.lang3.Validate;
1:151de0d: import org.apache.hadoop.conf.Configuration;
1:8bac914: import org.apache.hadoop.filecache.DistributedCache;
1:8bac914: import org.apache.hadoop.fs.FileStatus;
1:8bac914: import org.apache.hadoop.fs.FileSystem;
1:151de0d: import org.apache.hadoop.fs.Path;
1:151de0d: import org.apache.hadoop.io.IntWritable;
1:ffc7fab: import org.apache.hadoop.io.LongWritable;
1:151de0d: import org.apache.hadoop.io.SequenceFile.CompressionType;
1:151de0d: import org.apache.hadoop.io.Writable;
1:151de0d: import org.apache.hadoop.io.compress.DefaultCodec;
1:151de0d: import org.apache.hadoop.mapred.JobConf;
1:ffc7fab: import org.apache.hadoop.mapred.OutputCollector;
1:151de0d: import org.apache.hadoop.mapred.lib.MultipleOutputs;
1:151de0d: import org.apache.hadoop.mapreduce.Job;
1:151de0d: import org.apache.hadoop.mapreduce.Mapper;
1:151de0d: import org.apache.hadoop.mapreduce.Reducer;
1:151de0d: import org.apache.hadoop.mapreduce.lib.input.FileInputFormat;
1:151de0d: import org.apache.hadoop.mapreduce.lib.input.SequenceFileInputFormat;
1:151de0d: import org.apache.hadoop.mapreduce.lib.output.FileOutputFormat;
1:151de0d: import org.apache.hadoop.mapreduce.lib.output.SequenceFileOutputFormat;
1:6d9179e: import org.apache.mahout.common.HadoopUtil;
1:ffc7fab: import org.apache.mahout.common.IOUtils;
1:ffc7fab: import org.apache.mahout.common.iterator.sequencefile.PathType;
1:ffc7fab: import org.apache.mahout.common.iterator.sequencefile.SequenceFileDirValueIterator;
1:a13b4b7: import org.apache.mahout.common.iterator.sequencefile.SequenceFileValueIterator;
1:151de0d: import org.apache.mahout.math.DenseVector;
1:478cad9: import org.apache.mahout.math.NamedVector;
1:f43adfe: import org.apache.mahout.math.UpperTriangular;
1:151de0d: import org.apache.mahout.math.Vector;
1:151de0d: import org.apache.mahout.math.VectorWritable;
1:175701c: import org.apache.mahout.math.function.Functions;
1:175701c: import org.apache.mahout.math.function.PlusMult;
1:ffc7fab: import org.apache.mahout.math.hadoop.stochasticsvd.qr.QRLastStep;
1:151de0d: 
1:f43adfe: import java.io.Closeable;
1:f43adfe: import java.io.IOException;
1:f43adfe: import java.util.ArrayDeque;
1:f43adfe: import java.util.Deque;
1:f43adfe: 
1:151de0d: /**
1:ffc7fab:  * Bt job. For details, see working notes in MAHOUT-376.
1:f43adfe:  * <p/>
1:f43adfe:  * <p/>
1:ffc7fab:  * Uses hadoop deprecated API wherever new api has not been updated
1:ffc7fab:  * (MAHOUT-593), hence @SuppressWarning("deprecation").
1:f43adfe:  * <p/>
1:f43adfe:  * <p/>
1:ffc7fab:  * This job outputs either Bt in its standard output, or upper triangular
1:ffc7fab:  * matrices representing BBt partial sums if that's requested . If the latter
1:ffc7fab:  * mode is enabled, then we accumulate BBt outer product sums in upper
1:ffc7fab:  * triangular accumulator and output it at the end of the job, thus saving space
1:ffc7fab:  * and BBt job.
1:f43adfe:  * <p/>
1:f43adfe:  * <p/>
1:ffc7fab:  * This job also outputs Q and Bt and optionally BBt. Bt is output to standard
1:ffc7fab:  * job output (part-*) and Q and BBt use named multiple outputs.
1:f43adfe:  * <p/>
1:f43adfe:  * <p/>
1:151de0d:  */
1:b479bd2: @SuppressWarnings("deprecation")
1:3218e95: public final class BtJob {
1:151de0d: 
1:151de0d:   public static final String OUTPUT_Q = "Q";
1:151de0d:   public static final String OUTPUT_BT = "part";
1:ffc7fab:   public static final String OUTPUT_BBT = "bbt";
1:175701c:   public static final String OUTPUT_SQ = "sq";
1:175701c:   public static final String OUTPUT_SB = "sb";
1:175701c: 
1:151de0d:   public static final String PROP_QJOB_PATH = "ssvd.QJob.path";
1:ffc7fab:   public static final String PROP_OUPTUT_BBT_PRODUCTS =
1:ffc7fab:     "ssvd.BtJob.outputBBtProducts";
1:ffc7fab:   public static final String PROP_OUTER_PROD_BLOCK_HEIGHT =
1:ffc7fab:     "ssvd.outerProdBlockHeight";
1:8bac914:   public static final String PROP_RHAT_BROADCAST = "ssvd.rhat.broadcast";
1:175701c:   public static final String PROP_XI_PATH = "ssvdpca.xi.path";
1:478cad9:   public static final String PROP_NV = "ssvd.nv";
1:175701c: 
1:a13b4b7:   private BtJob() {
1:151de0d:   }
1:ffc7fab: 
1:ffc7fab:   public static class BtMapper extends
1:f43adfe:     Mapper<Writable, VectorWritable, LongWritable, SparseRowBlockWritable> {
1:151de0d: 
1:ffc7fab:     private QRLastStep qr;
1:87c15be:     private final Deque<Closeable> closeables = new ArrayDeque<>();
1:ffc7fab: 
1:151de0d:     private int blockNum;
1:151de0d:     private MultipleOutputs outputs;
1:a13b4b7:     private final VectorWritable qRowValue = new VectorWritable();
1:ffc7fab:     private Vector btRow;
1:ffc7fab:     private SparseRowBlockAccumulator btCollector;
1:ffc7fab:     private Context mapContext;
1:478cad9:     private boolean nv;
1:151de0d: 
1:175701c:     // pca stuff
1:175701c:     private Vector sqAccum;
1:175701c:     private boolean computeSq;
1:151de0d: 
1:ffc7fab:     /**
1:ffc7fab:      * We maintain A and QtHat inputs partitioned the same way, so we
1:ffc7fab:      * essentially are performing map-side merge here of A and QtHats except
1:ffc7fab:      * QtHat is stored not row-wise but block-wise.
1:ffc7fab:      */
1:ffc7fab:     @Override
1:ffc7fab:     protected void map(Writable key, VectorWritable value, Context context)
1:ffc7fab:       throws IOException, InterruptedException {
1:ffc7fab: 
1:ffc7fab:       mapContext = context;
1:151de0d:       // output Bt outer products
1:151de0d:       Vector aRow = value.get();
1:b479bd2: 
1:ffc7fab:       Vector qRow = qr.next();
1:ffc7fab:       int kp = qRow.size();
1:ffc7fab: 
1:ffc7fab:       // make sure Qs are inheriting A row labels.
1:478cad9:       outputQRow(key, qRow, aRow);
1:ffc7fab: 
1:175701c:       // MAHOUT-817
1:175701c:       if (computeSq) {
1:175701c:         if (sqAccum == null) {
1:175701c:           sqAccum = new DenseVector(kp);
1:175701c:         }
1:175701c:         sqAccum.assign(qRow, Functions.PLUS);
1:175701c:       }
1:175701c: 
1:ffc7fab:       if (btRow == null) {
1:ffc7fab:         btRow = new DenseVector(kp);
1:ffc7fab:       }
1:ffc7fab: 
1:b479bd2:       if (!aRow.isDense()) {
1:dc62944:         for (Vector.Element el : aRow.nonZeroes()) {
1:a13b4b7:           double mul = el.get();
1:a13b4b7:           for (int j = 0; j < kp; j++) {
1:b0ac9f5:             btRow.setQuick(j, mul * qRow.getQuick(j));
1:b0ac9f5:           }
1:ffc7fab:           btCollector.collect((long) el.index(), btRow);
1:151de0d:         }
1:b479bd2:       } else {
1:b0ac9f5:         int n = aRow.size();
1:b0ac9f5:         for (int i = 0; i < n; i++) {
1:b0ac9f5:           double mul = aRow.getQuick(i);
1:a13b4b7:           for (int j = 0; j < kp; j++) {
1:b0ac9f5:             btRow.setQuick(j, mul * qRow.getQuick(j));
1:151de0d:           }
1:ffc7fab:           btCollector.collect((long) i, btRow);
1:151de0d:         }
1:151de0d:       }
1:1499411:     }
1:151de0d: 
3:151de0d:     @Override
1:151de0d:     protected void setup(Context context) throws IOException,
1:ffc7fab:       InterruptedException {
1:151de0d:       super.setup(context);
1:151de0d: 
1:175701c:       Configuration conf = context.getConfiguration();
1:175701c: 
1:175701c:       Path qJobPath = new Path(conf.get(PROP_QJOB_PATH));
1:151de0d: 
1:5a2250c:       /*
1:5a2250c:        * actually this is kind of dangerous because this routine thinks we need
1:5a2250c:        * to create file name for our current job and this will use -m- so it's
1:5a2250c:        * just serendipity we are calling it from the mapper too as the QJob did.
1:5a2250c:        */
1:ffc7fab:       Path qInputPath =
1:ffc7fab:         new Path(qJobPath, FileOutputFormat.getUniqueFile(context,
1:ffc7fab:                                                           QJob.OUTPUT_QHAT,
1:ffc7fab:                                                           ""));
1:151de0d:       blockNum = context.getTaskAttemptID().getTaskID().getId();
1:151de0d: 
1:ffc7fab:       SequenceFileValueIterator<DenseBlockWritable> qhatInput =
1:87c15be:         new SequenceFileValueIterator<>(qInputPath,
2:ffc7fab:                                                           true,
1:175701c:                                                           conf);
1:ffc7fab:       closeables.addFirst(qhatInput);
1:151de0d: 
1:5a2250c:       /*
1:5a2250c:        * read all r files _in order of task ids_, i.e. partitions (aka group
1:8bac914:        * nums).
1:175701c:        *
1:8bac914:        * Note: if broadcast option is used, this comes from distributed cache
1:8bac914:        * files rather than hdfs path.
1:5a2250c:        */
1:151de0d: 
1:8bac914:       SequenceFileDirValueIterator<VectorWritable> rhatInput;
1:8bac914: 
1:175701c:       boolean distributedRHat = conf.get(PROP_RHAT_BROADCAST) != null;
1:8bac914:       if (distributedRHat) {
1:8bac914: 
1:6d9179e:         Path[] rFiles = HadoopUtil.getCachedFiles(conf);
1:8bac914: 
1:8bac914:         Validate.notNull(rFiles,
1:8bac914:                          "no RHat files in distributed cache job definition");
1:25d59aa:         //TODO: this probably can be replaced w/ local fs makeQualified
1:175701c:         Configuration lconf = new Configuration();
1:175701c:         lconf.set("fs.default.name", "file:///");
1:8bac914: 
1:8bac914:         rhatInput =
1:87c15be:           new SequenceFileDirValueIterator<>(rFiles,
1:175701c:                                                            SSVDHelper.PARTITION_COMPARATOR,
1:8bac914:                                                            true,
1:175701c:                                                            lconf);
1:8bac914: 
1:8bac914:       } else {
1:8bac914:         Path rPath = new Path(qJobPath, QJob.OUTPUT_RHAT + "-*");
1:8bac914:         rhatInput =
1:87c15be:           new SequenceFileDirValueIterator<>(rPath,
1:8bac914:                                                            PathType.GLOB,
1:8bac914:                                                            null,
1:175701c:                                                            SSVDHelper.PARTITION_COMPARATOR,
1:8bac914:                                                            true,
1:175701c:                                                            conf);
1:8bac914:       }
1:8bac914: 
1:8bac914:       Validate.isTrue(rhatInput.hasNext(), "Empty R-hat input!");
1:8bac914: 
1:ffc7fab:       closeables.addFirst(rhatInput);
1:175701c:       outputs = new MultipleOutputs(new JobConf(conf));
1:8bac914:       closeables.addFirst(new IOUtils.MultipleOutputsCloseableAdapter(outputs));
1:ffc7fab: 
1:ffc7fab:       qr = new QRLastStep(qhatInput, rhatInput, blockNum);
1:ffc7fab:       closeables.addFirst(qr);
1:5a2250c:       /*
1:5a2250c:        * it's so happens that current QRLastStep's implementation preloads R
1:5a2250c:        * sequence into memory in the constructor so it's ok to close rhat input
1:5a2250c:        * now.
1:5a2250c:        */
1:ffc7fab:       if (!rhatInput.hasNext()) {
1:ffc7fab:         closeables.remove(rhatInput);
1:ffc7fab:         rhatInput.close();
1:ffc7fab:       }
1:ffc7fab: 
1:ffc7fab:       OutputCollector<LongWritable, SparseRowBlockWritable> btBlockCollector =
1:ffc7fab:         new OutputCollector<LongWritable, SparseRowBlockWritable>() {
1:ffc7fab: 
1:ffc7fab:           @Override
1:ffc7fab:           public void collect(LongWritable blockKey,
1:ffc7fab:                               SparseRowBlockWritable block) throws IOException {
1:ffc7fab:             try {
1:ffc7fab:               mapContext.write(blockKey, block);
1:ffc7fab:             } catch (InterruptedException exc) {
1:ffc7fab:               throw new IOException("Interrupted.", exc);
1:ffc7fab:             }
1:ffc7fab:           }
1:ffc7fab:         };
1:ffc7fab: 
1:ffc7fab:       btCollector =
1:175701c:         new SparseRowBlockAccumulator(conf.getInt(PROP_OUTER_PROD_BLOCK_HEIGHT,
1:175701c:                                                   -1), btBlockCollector);
1:ffc7fab:       closeables.addFirst(btCollector);
1:175701c: 
1:175701c:       // MAHOUT-817
1:8b6a26a:       computeSq = conf.get(PROP_XI_PATH) != null;
1:175701c: 
1:478cad9:       // MAHOUT-1067
1:478cad9:       nv = conf.getBoolean(PROP_NV, false);
1:478cad9: 
1:175701c:     }
1:ffc7fab: 
1:175701c:     @Override
1:175701c:     protected void cleanup(Context context) throws IOException,
1:175701c:       InterruptedException {
1:175701c:       try {
1:175701c:         if (sqAccum != null) {
1:175701c:           /*
1:175701c:            * hack: we will output sq partial sums with index -1 for summation.
1:175701c:            */
1:175701c:           SparseRowBlockWritable sbrw = new SparseRowBlockWritable(1);
1:175701c:           sbrw.plusRow(0, sqAccum);
1:175701c:           LongWritable lw = new LongWritable(-1);
1:175701c:           context.write(lw, sbrw);
1:175701c:         }
1:175701c:       } finally {
1:175701c:         IOUtils.close(closeables);
1:175701c:       }
1:175701c:     }
1:175701c: 
1:175701c:     @SuppressWarnings("unchecked")
1:6d16230:     private void outputQRow(Writable key, Vector qRow, Vector aRow) throws IOException {
1:478cad9:       if (nv && (aRow instanceof NamedVector)) {
1:478cad9:         qRowValue.set(new NamedVector(qRow, ((NamedVector) aRow).getName()));
1:478cad9:       } else {
1:478cad9:         qRowValue.set(qRow);
1:478cad9:       }
1:478cad9:       outputs.getCollector(OUTPUT_Q, null).collect(key, qRowValue);
1:b479bd2:     }
1:151de0d:   }
1:b479bd2: 
1:ffc7fab:   public static class OuterProductCombiner
1:f43adfe:     extends
1:f43adfe:     Reducer<Writable, SparseRowBlockWritable, Writable, SparseRowBlockWritable> {
1:151de0d: 
1:ffc7fab:     protected final SparseRowBlockWritable accum = new SparseRowBlockWritable();
1:87c15be:     protected final Deque<Closeable> closeables = new ArrayDeque<>();
1:ffc7fab:     protected int blockHeight;
1:ffc7fab: 
1:ffc7fab:     @Override
1:ffc7fab:     protected void setup(Context context) throws IOException,
1:ffc7fab:       InterruptedException {
2:ffc7fab:       blockHeight =
2:ffc7fab:         context.getConfiguration().getInt(PROP_OUTER_PROD_BLOCK_HEIGHT, -1);
1:ffc7fab:     }
1:151de0d: 
1:151de0d:     @Override
1:ffc7fab:     protected void reduce(Writable key,
1:ffc7fab:                           Iterable<SparseRowBlockWritable> values,
1:ffc7fab:                           Context context) throws IOException,
1:ffc7fab:       InterruptedException {
1:1499411:       for (SparseRowBlockWritable bw : values) {
1:ffc7fab:         accum.plusBlock(bw);
1:151de0d:       }
1:ffc7fab:       context.write(key, accum);
1:ffc7fab:       accum.clear();
1:ffc7fab:     }
1:ffc7fab: 
1:ffc7fab:     @Override
1:ffc7fab:     protected void cleanup(Context context) throws IOException,
1:ffc7fab:       InterruptedException {
1:ffc7fab: 
1:ffc7fab:       IOUtils.close(closeables);
1:ffc7fab:     }
1:ffc7fab:   }
1:ffc7fab: 
1:ffc7fab:   public static class OuterProductReducer
1:f43adfe:     extends
1:f43adfe:     Reducer<LongWritable, SparseRowBlockWritable, IntWritable, VectorWritable> {
1:ffc7fab: 
1:ffc7fab:     protected final SparseRowBlockWritable accum = new SparseRowBlockWritable();
1:87c15be:     protected final Deque<Closeable> closeables = new ArrayDeque<>();
1:ffc7fab: 
1:ffc7fab:     protected int blockHeight;
1:ffc7fab:     private boolean outputBBt;
1:ffc7fab:     private UpperTriangular mBBt;
1:ffc7fab:     private MultipleOutputs outputs;
1:ffc7fab:     private final IntWritable btKey = new IntWritable();
1:ffc7fab:     private final VectorWritable btValue = new VectorWritable();
1:ffc7fab: 
1:175701c:     // MAHOUT-817
1:175701c:     private Vector xi;
1:175701c:     private final PlusMult pmult = new PlusMult(0);
1:175701c:     private Vector sbAccum;
1:175701c: 
1:ffc7fab:     @Override
1:ffc7fab:     protected void setup(Context context) throws IOException,
1:ffc7fab:       InterruptedException {
1:ffc7fab: 
1:175701c:       Configuration conf = context.getConfiguration();
1:175701c:       blockHeight = conf.getInt(PROP_OUTER_PROD_BLOCK_HEIGHT, -1);
1:ffc7fab: 
1:175701c:       outputBBt = conf.getBoolean(PROP_OUPTUT_BBT_PRODUCTS, false);
1:ffc7fab: 
1:ffc7fab:       if (outputBBt) {
1:175701c:         int k = conf.getInt(QJob.PROP_K, -1);
1:175701c:         int p = conf.getInt(QJob.PROP_P, -1);
1:ffc7fab: 
1:ffc7fab:         Validate.isTrue(k > 0, "invalid k parameter");
1:b8a553b:         Validate.isTrue(p >= 0, "invalid p parameter");
1:ffc7fab:         mBBt = new UpperTriangular(k + p);
1:175701c: 
1:ffc7fab:       }
1:ffc7fab: 
1:175701c:       String xiPathStr = conf.get(PROP_XI_PATH);
1:175701c:       if (xiPathStr != null) {
1:175701c:         xi = SSVDHelper.loadAndSumUpVectors(new Path(xiPathStr), conf);
1:03e6875:         if (xi == null) {
1:03e6875:           throw new IOException(String.format("unable to load mean path xi from %s.",
1:03e6875:                                               xiPathStr));
1:03e6875:         }
1:175701c:       }
1:175701c: 
1:175701c:       if (outputBBt || xi != null) {
1:175701c:         outputs = new MultipleOutputs(new JobConf(conf));
1:175701c:         closeables.addFirst(new IOUtils.MultipleOutputsCloseableAdapter(outputs));
1:175701c:       }
1:175701c: 
1:ffc7fab:     }
1:ffc7fab: 
1:ffc7fab:     @Override
1:ffc7fab:     protected void reduce(LongWritable key,
1:ffc7fab:                           Iterable<SparseRowBlockWritable> values,
1:ffc7fab:                           Context context) throws IOException,
1:ffc7fab:       InterruptedException {
1:ffc7fab: 
1:ffc7fab:       accum.clear();
1:1499411:       for (SparseRowBlockWritable bw : values) {
1:ffc7fab:         accum.plusBlock(bw);
1:151de0d:       }
1:ffc7fab: 
1:175701c:       // MAHOUT-817:
1:175701c:       if (key.get() == -1L) {
1:175701c: 
1:175701c:         Vector sq = accum.getRows()[0];
1:175701c: 
1:175701c:         @SuppressWarnings("unchecked")
1:175701c:         OutputCollector<IntWritable, VectorWritable> sqOut =
1:175701c:           outputs.getCollector(OUTPUT_SQ, null);
1:175701c: 
1:175701c:         sqOut.collect(new IntWritable(0), new VectorWritable(sq));
1:175701c:         return;
1:175701c:       }
1:175701c: 
1:5a2250c:       /*
1:5a2250c:        * at this point, sum of rows should be in accum, so we just generate
1:5a2250c:        * outer self product of it and add to BBt accumulator.
1:5a2250c:        */
1:ffc7fab: 
1:ffc7fab:       for (int k = 0; k < accum.getNumRows(); k++) {
1:ffc7fab:         Vector btRow = accum.getRows()[k];
1:ffc7fab:         btKey.set((int) (key.get() * blockHeight + accum.getRowIndices()[k]));
1:ffc7fab:         btValue.set(btRow);
1:ffc7fab:         context.write(btKey, btValue);
1:ffc7fab: 
1:ffc7fab:         if (outputBBt) {
1:ffc7fab:           int kp = mBBt.numRows();
1:ffc7fab:           // accumulate partial BBt sum
1:ffc7fab:           for (int i = 0; i < kp; i++) {
1:ffc7fab:             double vi = btRow.get(i);
1:ffc7fab:             if (vi != 0.0) {
1:ffc7fab:               for (int j = i; j < kp; j++) {
1:ffc7fab:                 double vj = btRow.get(j);
1:1499411:                 if (vj != 0.0) {
1:ffc7fab:                   mBBt.setQuick(i, j, mBBt.getQuick(i, j) + vi * vj);
1:ffc7fab:                 }
1:ffc7fab:               }
1:ffc7fab:             }
1:ffc7fab:           }
1:ffc7fab:         }
1:ffc7fab: 
1:175701c:         // MAHOUT-817
1:175701c:         if (xi != null) {
1:175701c:           // code defensively against shortened xi
1:175701c:           int btIndex = btKey.get();
1:175701c:           double xii = xi.size() > btIndex ? xi.getQuick(btIndex) : 0.0;
1:175701c:           // compute s_b
1:175701c:           pmult.setMultiplicator(xii);
1:229aeff:           if (sbAccum == null) {
1:175701c:             sbAccum = new DenseVector(btRow.size());
1:ffc7fab:           }
1:175701c:           sbAccum.assign(btRow, pmult);
1:175701c:         }
1:175701c: 
1:b0ac9f5:       }
1:151de0d:     }
1:ffc7fab: 
1:ffc7fab:     @Override
2:ffc7fab:     protected void cleanup(Context context) throws IOException,
1:ffc7fab:       InterruptedException {
1:ffc7fab: 
1:ffc7fab:       // if we output BBt instead of Bt then we need to do it.
1:ffc7fab:       try {
1:ffc7fab:         if (outputBBt) {
1:ffc7fab: 
1:ffc7fab:           @SuppressWarnings("unchecked")
1:ffc7fab:           OutputCollector<Writable, Writable> collector =
1:ffc7fab:             outputs.getCollector(OUTPUT_BBT, null);
1:ffc7fab: 
1:8bac914:           collector.collect(new IntWritable(),
1:8bac914:                             new VectorWritable(new DenseVector(mBBt.getData())));
1:ffc7fab:         }
1:175701c: 
1:175701c:         // MAHOUT-817
1:175701c:         if (sbAccum != null) {
1:175701c:           @SuppressWarnings("unchecked")
1:175701c:           OutputCollector<IntWritable, VectorWritable> collector =
1:175701c:             outputs.getCollector(OUTPUT_SB, null);
1:175701c: 
1:175701c:           collector.collect(new IntWritable(), new VectorWritable(sbAccum));
1:175701c: 
1:175701c:         }
1:ffc7fab:       } finally {
2:ffc7fab:         IOUtils.close(closeables);
1:151de0d:       }
1:b479bd2: 
1:151de0d:     }
1:229aeff:   }
1:151de0d: 
1:a13b4b7:   public static void run(Configuration conf,
1:a13b4b7:                          Path[] inputPathA,
1:a13b4b7:                          Path inputPathQJob,
1:175701c:                          Path xiPath,
1:a13b4b7:                          Path outputPath,
1:a13b4b7:                          int minSplitSize,
1:a13b4b7:                          int k,
1:a13b4b7:                          int p,
1:ffc7fab:                          int btBlockHeight,
1:a13b4b7:                          int numReduceTasks,
1:8bac914:                          boolean broadcast,
1:ffc7fab:                          Class<? extends Writable> labelClass,
1:ffc7fab:                          boolean outputBBtProducts)
1:ffc7fab:     throws ClassNotFoundException, InterruptedException, IOException {
1:151de0d: 
1:151de0d:     JobConf oldApiJob = new JobConf(conf);
1:ffc7fab: 
1:8bac914:     MultipleOutputs.addNamedOutput(oldApiJob,
1:8bac914:                                    OUTPUT_Q,
1:8bac914:                                    org.apache.hadoop.mapred.SequenceFileOutputFormat.class,
1:8bac914:                                    labelClass,
1:8bac914:                                    VectorWritable.class);
1:ffc7fab: 
1:ffc7fab:     if (outputBBtProducts) {
1:8bac914:       MultipleOutputs.addNamedOutput(oldApiJob,
1:8bac914:                                      OUTPUT_BBT,
1:8bac914:                                      org.apache.hadoop.mapred.SequenceFileOutputFormat.class,
1:8bac914:                                      IntWritable.class,
1:8bac914:                                      VectorWritable.class);
1:478cad9:       /*
1:478cad9:        * MAHOUT-1067: if we are asked to output BBT products then named vector
1:478cad9:        * names should be propagated to Q too so that UJob could pick them up
1:478cad9:        * from there.
1:478cad9:        */
1:478cad9:       oldApiJob.setBoolean(PROP_NV, true);
1:ffc7fab:     }
1:175701c:     if (xiPath != null) {
1:175701c:       // compute pca -related stuff as well
1:175701c:       MultipleOutputs.addNamedOutput(oldApiJob,
1:175701c:                                      OUTPUT_SQ,
1:175701c:                                      org.apache.hadoop.mapred.SequenceFileOutputFormat.class,
1:175701c:                                      IntWritable.class,
1:175701c:                                      VectorWritable.class);
1:175701c:       MultipleOutputs.addNamedOutput(oldApiJob,
1:175701c:                                      OUTPUT_SB,
1:175701c:                                      org.apache.hadoop.mapred.SequenceFileOutputFormat.class,
1:175701c:                                      IntWritable.class,
1:175701c:                                      VectorWritable.class);
1:175701c:     }
1:ffc7fab: 
1:5a2250c:     /*
1:5a2250c:      * HACK: we use old api multiple outputs since they are not available in the
1:5a2250c:      * new api of either 0.20.2 or 0.20.203 but wrap it into a new api job so we
1:5a2250c:      * can use new api interfaces.
1:5a2250c:      */
1:151de0d: 
1:151de0d:     Job job = new Job(oldApiJob);
1:151de0d:     job.setJobName("Bt-job");
1:151de0d:     job.setJarByClass(BtJob.class);
1:151de0d: 
1:151de0d:     job.setInputFormatClass(SequenceFileInputFormat.class);
1:151de0d:     job.setOutputFormatClass(SequenceFileOutputFormat.class);
1:151de0d:     FileInputFormat.setInputPaths(job, inputPathA);
1:a13b4b7:     if (minSplitSize > 0) {
1:a13b4b7:       FileInputFormat.setMinInputSplitSize(job, minSplitSize);
1:1499411:     }
1:151de0d:     FileOutputFormat.setOutputPath(job, outputPath);
1:151de0d: 
1:5a2250c:     // WARN: tight hadoop integration here:
1:151de0d:     job.getConfiguration().set("mapreduce.output.basename", OUTPUT_BT);
1:5a2250c: 
1:a13b4b7:     FileOutputFormat.setOutputCompressorClass(job, DefaultCodec.class);
1:ffc7fab:     SequenceFileOutputFormat.setOutputCompressionType(job,
1:ffc7fab:                                                       CompressionType.BLOCK);
1:151de0d: 
1:ffc7fab:     job.setMapOutputKeyClass(LongWritable.class);
1:ffc7fab:     job.setMapOutputValueClass(SparseRowBlockWritable.class);
1:151de0d: 
1:151de0d:     job.setOutputKeyClass(IntWritable.class);
1:151de0d:     job.setOutputValueClass(VectorWritable.class);
1:151de0d: 
1:151de0d:     job.setMapperClass(BtMapper.class);
1:ffc7fab:     job.setCombinerClass(OuterProductCombiner.class);
1:151de0d:     job.setReducerClass(OuterProductReducer.class);
1:151de0d: 
1:151de0d:     job.getConfiguration().setInt(QJob.PROP_K, k);
1:151de0d:     job.getConfiguration().setInt(QJob.PROP_P, p);
1:151de0d:     job.getConfiguration().set(PROP_QJOB_PATH, inputPathQJob.toString());
1:ffc7fab:     job.getConfiguration().setBoolean(PROP_OUPTUT_BBT_PRODUCTS,
1:ffc7fab:                                       outputBBtProducts);
1:ffc7fab:     job.getConfiguration().setInt(PROP_OUTER_PROD_BLOCK_HEIGHT, btBlockHeight);
1:151de0d: 
1:151de0d:     job.setNumReduceTasks(numReduceTasks);
1:151de0d: 
1:8bac914:     /*
1:175701c:      * PCA-related options, MAHOUT-817
1:175701c:      */
1:175701c:     if (xiPath != null) {
1:175701c:       job.getConfiguration().set(PROP_XI_PATH, xiPath.toString());
1:175701c:     }
1:175701c: 
1:175701c:     /*
1:8bac914:      * we can broadhast Rhat files since all of them are reuqired by each job,
1:8bac914:      * but not Q files which correspond to splits of A (so each split of A will
1:8bac914:      * require only particular Q file, each time different one).
1:8bac914:      */
1:8bac914: 
1:8bac914:     if (broadcast) {
1:8bac914:       job.getConfiguration().set(PROP_RHAT_BROADCAST, "y");
1:8bac914: 
1:1de8cec:       FileSystem fs = FileSystem.get(inputPathQJob.toUri(), conf);
1:8bac914:       FileStatus[] fstats =
1:8bac914:         fs.globStatus(new Path(inputPathQJob, QJob.OUTPUT_RHAT + "-*"));
1:8bac914:       if (fstats != null) {
1:8bac914:         for (FileStatus fstat : fstats) {
1:8bac914:           /*
1:8bac914:            * new api is not enabled yet in our dependencies at this time, still
1:8bac914:            * using deprecated one
1:8bac914:            */
1:8bac914:           DistributedCache.addCacheFile(fstat.getPath().toUri(),
1:8bac914:                                         job.getConfiguration());
1:8bac914:         }
1:8bac914:       }
1:8bac914:     }
1:8bac914: 
1:151de0d:     job.submit();
1:151de0d:     job.waitForCompletion(false);
1:151de0d: 
1:a13b4b7:     if (!job.isSuccessful()) {
1:151de0d:       throw new IOException("Bt job unsuccessful.");
1:151de0d:     }
1:151de0d:   }
1:1499411: }
============================================================================
author:pferrel
-------------------------------------------------------------------------------
commit:b988c49
author:Suneel Marthi
-------------------------------------------------------------------------------
commit:87c15be
/////////////////////////////////////////////////////////////////////////
/////////////////////////////////////////////////////////////////////////
1:     private final Deque<Closeable> closeables = new ArrayDeque<>();
/////////////////////////////////////////////////////////////////////////
1:         new SequenceFileValueIterator<>(qInputPath,
/////////////////////////////////////////////////////////////////////////
1:           new SequenceFileDirValueIterator<>(rFiles,
/////////////////////////////////////////////////////////////////////////
1:           new SequenceFileDirValueIterator<>(rPath,
/////////////////////////////////////////////////////////////////////////
1:     protected final Deque<Closeable> closeables = new ArrayDeque<>();
/////////////////////////////////////////////////////////////////////////
1:     protected final Deque<Closeable> closeables = new ArrayDeque<>();
author:frankscholten
-------------------------------------------------------------------------------
commit:1a42d85
author:Dmitriy Lyubimov
-------------------------------------------------------------------------------
commit:f43adfe
/////////////////////////////////////////////////////////////////////////
/////////////////////////////////////////////////////////////////////////
1: import org.apache.mahout.math.UpperTriangular;
1: import java.io.Closeable;
1: import java.io.IOException;
1: import java.util.ArrayDeque;
1: import java.util.Deque;
1: 
1:  * <p/>
1:  * <p/>
1:  * <p/>
1:  * <p/>
1:  * <p/>
1:  * <p/>
1:  * <p/>
1:  * <p/>
/////////////////////////////////////////////////////////////////////////
1:     Mapper<Writable, VectorWritable, LongWritable, SparseRowBlockWritable> {
/////////////////////////////////////////////////////////////////////////
1:     extends
1:     Reducer<Writable, SparseRowBlockWritable, Writable, SparseRowBlockWritable> {
/////////////////////////////////////////////////////////////////////////
1:     extends
1:     Reducer<LongWritable, SparseRowBlockWritable, IntWritable, VectorWritable> {
commit:03e6875
/////////////////////////////////////////////////////////////////////////
1:         if (xi == null) {
1:           throw new IOException(String.format("unable to load mean path xi from %s.",
1:                                               xiPathStr));
1:         }
commit:478cad9
/////////////////////////////////////////////////////////////////////////
1: import org.apache.mahout.math.NamedVector;
/////////////////////////////////////////////////////////////////////////
1:   public static final String PROP_NV = "ssvd.nv";
/////////////////////////////////////////////////////////////////////////
1:     private boolean nv;
/////////////////////////////////////////////////////////////////////////
1:       outputQRow(key, qRow, aRow);
/////////////////////////////////////////////////////////////////////////
1:       // MAHOUT-1067
1:       nv = conf.getBoolean(PROP_NV, false);
1: 
/////////////////////////////////////////////////////////////////////////
0:     private void outputQRow(Writable key, Vector qRow, Vector aRow ) throws IOException {
1:       if (nv && (aRow instanceof NamedVector)) {
1:         qRowValue.set(new NamedVector(qRow, ((NamedVector) aRow).getName()));
1:       } else {
1:         qRowValue.set(qRow);
1:       }
1:       outputs.getCollector(OUTPUT_Q, null).collect(key, qRowValue);
/////////////////////////////////////////////////////////////////////////
1:       /*
1:        * MAHOUT-1067: if we are asked to output BBT products then named vector
1:        * names should be propagated to Q too so that UJob could pick them up
1:        * from there.
1:        */
1:       oldApiJob.setBoolean(PROP_NV, true);
commit:175701c
/////////////////////////////////////////////////////////////////////////
1: import org.apache.mahout.math.function.Functions;
1: import org.apache.mahout.math.function.PlusMult;
/////////////////////////////////////////////////////////////////////////
1:   public static final String OUTPUT_SQ = "sq";
1:   public static final String OUTPUT_SB = "sb";
1: 
/////////////////////////////////////////////////////////////////////////
1:   public static final String PROP_XI_PATH = "ssvdpca.xi.path";
1: 
/////////////////////////////////////////////////////////////////////////
1:     // pca stuff
1:     private Vector sqAccum;
1:     private boolean computeSq;
/////////////////////////////////////////////////////////////////////////
1:       // MAHOUT-817
1:       if (computeSq) {
1:         if (sqAccum == null) {
1:           sqAccum = new DenseVector(kp);
1:         }
1:         sqAccum.assign(qRow, Functions.PLUS);
1:       }
1: 
/////////////////////////////////////////////////////////////////////////
1:       Configuration conf = context.getConfiguration();
1: 
1:       Path qJobPath = new Path(conf.get(PROP_QJOB_PATH));
/////////////////////////////////////////////////////////////////////////
1:                                                           conf);
1:        *
1:       boolean distributedRHat = conf.get(PROP_RHAT_BROADCAST) != null;
0:         Path[] rFiles = DistributedCache.getLocalCacheFiles(conf);
1:         Configuration lconf = new Configuration();
1:         lconf.set("fs.default.name", "file:///");
1:                                                            SSVDHelper.PARTITION_COMPARATOR,
1:                                                            lconf);
/////////////////////////////////////////////////////////////////////////
1:                                                            SSVDHelper.PARTITION_COMPARATOR,
1:                                                            conf);
1:       outputs = new MultipleOutputs(new JobConf(conf));
/////////////////////////////////////////////////////////////////////////
1:         new SparseRowBlockAccumulator(conf.getInt(PROP_OUTER_PROD_BLOCK_HEIGHT,
1:                                                   -1), btBlockCollector);
1:       // MAHOUT-817
0:       computeSq = (conf.get(PROP_XI_PATH) != null);
1: 
1:     }
1: 
1:     @Override
1:     protected void cleanup(Context context) throws IOException,
1:       InterruptedException {
1:       try {
1:         if (sqAccum != null) {
1:           /*
1:            * hack: we will output sq partial sums with index -1 for summation.
1:            */
1:           SparseRowBlockWritable sbrw = new SparseRowBlockWritable(1);
1:           sbrw.plusRow(0, sqAccum);
1:           LongWritable lw = new LongWritable(-1);
1:           context.write(lw, sbrw);
1:         }
1:       } finally {
1:         IOUtils.close(closeables);
1:       }
1:     }
1: 
1:     @SuppressWarnings("unchecked")
0:     private void outputQRow(Writable key, Writable value) throws IOException {
0:       outputs.getCollector(OUTPUT_Q, null).collect(key, value);
/////////////////////////////////////////////////////////////////////////
1:     // MAHOUT-817
1:     private Vector xi;
1:     private final PlusMult pmult = new PlusMult(0);
1:     private Vector sbAccum;
1: 
1:       Configuration conf = context.getConfiguration();
1:       blockHeight = conf.getInt(PROP_OUTER_PROD_BLOCK_HEIGHT, -1);
1:       outputBBt = conf.getBoolean(PROP_OUPTUT_BBT_PRODUCTS, false);
1:         int k = conf.getInt(QJob.PROP_K, -1);
1:         int p = conf.getInt(QJob.PROP_P, -1);
1: 
1:       String xiPathStr = conf.get(PROP_XI_PATH);
1:       if (xiPathStr != null) {
1:         xi = SSVDHelper.loadAndSumUpVectors(new Path(xiPathStr), conf);
1:       }
1: 
1:       if (outputBBt || xi != null) {
1:         outputs = new MultipleOutputs(new JobConf(conf));
1:         closeables.addFirst(new IOUtils.MultipleOutputsCloseableAdapter(outputs));
1:       }
1: 
/////////////////////////////////////////////////////////////////////////
1:       // MAHOUT-817:
1:       if (key.get() == -1L) {
1: 
1:         Vector sq = accum.getRows()[0];
1: 
1:         @SuppressWarnings("unchecked")
1:         OutputCollector<IntWritable, VectorWritable> sqOut =
1:           outputs.getCollector(OUTPUT_SQ, null);
1: 
1:         sqOut.collect(new IntWritable(0), new VectorWritable(sq));
1:         return;
1:       }
1: 
/////////////////////////////////////////////////////////////////////////
1:         // MAHOUT-817
1:         if (xi != null) {
1:           // code defensively against shortened xi
1:           int btIndex = btKey.get();
1:           double xii = xi.size() > btIndex ? xi.getQuick(btIndex) : 0.0;
1:           // compute s_b
1:           pmult.setMultiplicator(xii);
0:           if (sbAccum == null)
1:             sbAccum = new DenseVector(btRow.size());
1:           sbAccum.assign(btRow, pmult);
1:         }
1: 
/////////////////////////////////////////////////////////////////////////
1: 
1:         // MAHOUT-817
1:         if (sbAccum != null) {
1:           @SuppressWarnings("unchecked")
1:           OutputCollector<IntWritable, VectorWritable> collector =
1:             outputs.getCollector(OUTPUT_SB, null);
1: 
1:           collector.collect(new IntWritable(), new VectorWritable(sbAccum));
1: 
1:         }
1:                          Path xiPath,
/////////////////////////////////////////////////////////////////////////
1:     if (xiPath != null) {
1:       // compute pca -related stuff as well
1:       MultipleOutputs.addNamedOutput(oldApiJob,
1:                                      OUTPUT_SQ,
1:                                      org.apache.hadoop.mapred.SequenceFileOutputFormat.class,
1:                                      IntWritable.class,
1:                                      VectorWritable.class);
1:       MultipleOutputs.addNamedOutput(oldApiJob,
1:                                      OUTPUT_SB,
1:                                      org.apache.hadoop.mapred.SequenceFileOutputFormat.class,
1:                                      IntWritable.class,
1:                                      VectorWritable.class);
1:     }
/////////////////////////////////////////////////////////////////////////
1:      * PCA-related options, MAHOUT-817
1:      */
1:     if (xiPath != null) {
1:       job.getConfiguration().set(PROP_XI_PATH, xiPath.toString());
1:     }
1: 
1:     /*
commit:8bac914
/////////////////////////////////////////////////////////////////////////
1: import org.apache.hadoop.filecache.DistributedCache;
1: import org.apache.hadoop.fs.FileStatus;
1: import org.apache.hadoop.fs.FileSystem;
/////////////////////////////////////////////////////////////////////////
1:   public static final String PROP_RHAT_BROADCAST = "ssvd.rhat.broadcast";
/////////////////////////////////////////////////////////////////////////
0:         for (Iterator<Vector.Element> iter = aRow.iterateNonZero(); iter.hasNext();) {
/////////////////////////////////////////////////////////////////////////
0:                                                           context.getConfiguration());
1:        * nums).
1:        * 
1:        * Note: if broadcast option is used, this comes from distributed cache
1:        * files rather than hdfs path.
1:       SequenceFileDirValueIterator<VectorWritable> rhatInput;
0:       boolean distributedRHat =
0:         context.getConfiguration().get(PROP_RHAT_BROADCAST) != null;
1:       if (distributedRHat) {
1: 
0:         Path[] rFiles =
0:           DistributedCache.getLocalCacheFiles(context.getConfiguration());
1: 
1:         Validate.notNull(rFiles,
1:                          "no RHat files in distributed cache job definition");
1: 
0:         Configuration conf = new Configuration();
0:         conf.set("fs.default.name", "file:///");
1: 
1:         rhatInput =
0:           new SequenceFileDirValueIterator<VectorWritable>(rFiles,
0:                                                            SSVDSolver.PARTITION_COMPARATOR,
1:                                                            true,
0:                                                            conf);
1: 
1:       } else {
1:         Path rPath = new Path(qJobPath, QJob.OUTPUT_RHAT + "-*");
1:         rhatInput =
0:           new SequenceFileDirValueIterator<VectorWritable>(rPath,
1:                                                            PathType.GLOB,
1:                                                            null,
0:                                                            SSVDSolver.PARTITION_COMPARATOR,
1:                                                            true,
0:                                                            context.getConfiguration());
1:       }
1: 
1:       Validate.isTrue(rhatInput.hasNext(), "Empty R-hat input!");
1: 
/////////////////////////////////////////////////////////////////////////
0:                                              .getInt(PROP_OUTER_PROD_BLOCK_HEIGHT,
0:                                                      -1),
0:                                       btBlockCollector);
/////////////////////////////////////////////////////////////////////////
1:         closeables.addFirst(new IOUtils.MultipleOutputsCloseableAdapter(outputs));
/////////////////////////////////////////////////////////////////////////
1:           collector.collect(new IntWritable(),
1:                             new VectorWritable(new DenseVector(mBBt.getData())));
/////////////////////////////////////////////////////////////////////////
1:                          boolean broadcast,
1:     MultipleOutputs.addNamedOutput(oldApiJob,
1:                                    OUTPUT_Q,
1:                                    org.apache.hadoop.mapred.SequenceFileOutputFormat.class,
1:                                    labelClass,
1:                                    VectorWritable.class);
1:       MultipleOutputs.addNamedOutput(oldApiJob,
1:                                      OUTPUT_BBT,
1:                                      org.apache.hadoop.mapred.SequenceFileOutputFormat.class,
1:                                      IntWritable.class,
1:                                      VectorWritable.class);
/////////////////////////////////////////////////////////////////////////
1:     /*
1:      * we can broadhast Rhat files since all of them are reuqired by each job,
1:      * but not Q files which correspond to splits of A (so each split of A will
1:      * require only particular Q file, each time different one).
1:      */
1: 
1:     if (broadcast) {
1:       job.getConfiguration().set(PROP_RHAT_BROADCAST, "y");
1: 
0:       FileSystem fs = FileSystem.get(conf);
1:       FileStatus[] fstats =
1:         fs.globStatus(new Path(inputPathQJob, QJob.OUTPUT_RHAT + "-*"));
1:       if (fstats != null) {
1:         for (FileStatus fstat : fstats) {
1:           /*
1:            * new api is not enabled yet in our dependencies at this time, still
1:            * using deprecated one
1:            */
1:           DistributedCache.addCacheFile(fstat.getPath().toUri(),
1:                                         job.getConfiguration());
1:         }
1:       }
1:     }
1: 
commit:5a2250c
/////////////////////////////////////////////////////////////////////////
/////////////////////////////////////////////////////////////////////////
/////////////////////////////////////////////////////////////////////////
1:       /*
1:        * actually this is kind of dangerous because this routine thinks we need
1:        * to create file name for our current job and this will use -m- so it's
1:        * just serendipity we are calling it from the mapper too as the QJob did.
1:        */
/////////////////////////////////////////////////////////////////////////
1:       /*
1:        * read all r files _in order of task ids_, i.e. partitions (aka group
0:        * nums)
1:        */
/////////////////////////////////////////////////////////////////////////
1:       /*
1:        * it's so happens that current QRLastStep's implementation preloads R
1:        * sequence into memory in the constructor so it's ok to close rhat input
1:        * now.
1:        */
/////////////////////////////////////////////////////////////////////////
/////////////////////////////////////////////////////////////////////////
1:       /*
1:        * at this point, sum of rows should be in accum, so we just generate
1:        * outer self product of it and add to BBt accumulator.
1:        */
/////////////////////////////////////////////////////////////////////////
1:     /*
1:      * HACK: we use old api multiple outputs since they are not available in the
1:      * new api of either 0.20.2 or 0.20.203 but wrap it into a new api job so we
1:      * can use new api interfaces.
1:      */
/////////////////////////////////////////////////////////////////////////
1:     // WARN: tight hadoop integration here:
1: 
/////////////////////////////////////////////////////////////////////////
/////////////////////////////////////////////////////////////////////////
commit:b8a553b
/////////////////////////////////////////////////////////////////////////
1:         Validate.isTrue(p >= 0, "invalid p parameter");
commit:ffc7fab
/////////////////////////////////////////////////////////////////////////
0: import java.io.Closeable;
0: import java.util.ArrayDeque;
0: import java.util.Deque;
0: import org.apache.commons.lang.Validate;
1: import org.apache.hadoop.io.LongWritable;
1: import org.apache.hadoop.mapred.OutputCollector;
/////////////////////////////////////////////////////////////////////////
1: import org.apache.mahout.common.IOUtils;
1: import org.apache.mahout.common.iterator.sequencefile.PathType;
1: import org.apache.mahout.common.iterator.sequencefile.SequenceFileDirValueIterator;
1: import org.apache.mahout.math.hadoop.stochasticsvd.qr.QRLastStep;
1:  * Bt job. For details, see working notes in MAHOUT-376.
0:  * <P>
1:  * Uses hadoop deprecated API wherever new api has not been updated
1:  * (MAHOUT-593), hence @SuppressWarning("deprecation").
0:  * <P>
1:  * 
1:  * This job outputs either Bt in its standard output, or upper triangular
1:  * matrices representing BBt partial sums if that's requested . If the latter
1:  * mode is enabled, then we accumulate BBt outer product sums in upper
1:  * triangular accumulator and output it at the end of the job, thus saving space
1:  * and BBt job.
0:  * <P>
1:  * 
1:  * This job also outputs Q and Bt and optionally BBt. Bt is output to standard
1:  * job output (part-*) and Q and BBt use named multiple outputs.
1:  * 
0:  * <P>
/////////////////////////////////////////////////////////////////////////
1:   public static final String OUTPUT_BBT = "bbt";
1:   public static final String PROP_OUPTUT_BBT_PRODUCTS =
1:     "ssvd.BtJob.outputBBtProducts";
1:   public static final String PROP_OUTER_PROD_BLOCK_HEIGHT =
1:     "ssvd.outerProdBlockHeight";
1: 
0:   static final double SPARSE_ZEROS_PCT_THRESHOLD = 0.1;
1:   public static class BtMapper extends
0:       Mapper<Writable, VectorWritable, LongWritable, SparseRowBlockWritable> {
1:     private QRLastStep qr;
0:     private Deque<Closeable> closeables = new ArrayDeque<Closeable>();
1: 
1:     private Vector btRow;
1:     private SparseRowBlockAccumulator btCollector;
1:     private Context mapContext;
1:     protected void cleanup(Context context) throws IOException,
1:       InterruptedException {
1:       IOUtils.close(closeables);
1:     /**
1:      * We maintain A and QtHat inputs partitioned the same way, so we
1:      * essentially are performing map-side merge here of A and QtHats except
1:      * QtHat is stored not row-wise but block-wise.
1:      */
1:     @Override
1:     protected void map(Writable key, VectorWritable value, Context context)
1:       throws IOException, InterruptedException {
1: 
1:       mapContext = context;
1:       Vector qRow = qr.next();
1:       int kp = qRow.size();
0:       qRowValue.set(qRow);
1: 
1:       // make sure Qs are inheriting A row labels.
0:       outputQRow(key, qRowValue);
1: 
1:       if (btRow == null) {
1:         btRow = new DenseVector(kp);
1:       }
1: 
0:         for (Iterator<Vector.Element> iter = aRow.iterateNonZero(); iter
0:           .hasNext();) {
0:           Vector.Element el = iter.next();
0:           // btKey.set(el.index());
0:           // context.write(btKey, btValue);
1:           btCollector.collect((long) el.index(), btRow);
/////////////////////////////////////////////////////////////////////////
0:           // btKey.set(i);
0:           // context.write(btKey, btValue);
1:           btCollector.collect((long) i, btRow);
0:     protected void setup(final Context context) throws IOException,
1:       InterruptedException {
1:       Path qInputPath =
1:         new Path(qJobPath, FileOutputFormat.getUniqueFile(context,
1:                                                           QJob.OUTPUT_QHAT,
1:                                                           ""));
1:       SequenceFileValueIterator<DenseBlockWritable> qhatInput =
0:         new SequenceFileValueIterator<DenseBlockWritable>(qInputPath,
1:                                                           true,
0:                                                           context
0:                                                             .getConfiguration());
1:       closeables.addFirst(qhatInput);
0:       // read all r files _in order of task ids_, i.e. partitions (aka group
0:       // nums)
0:       Path rPath = new Path(qJobPath, QJob.OUTPUT_RHAT + "-*");
0:       SequenceFileDirValueIterator<VectorWritable> rhatInput =
0:         new SequenceFileDirValueIterator<VectorWritable>(rPath,
0:                                                          PathType.GLOB,
0:                                                          null,
0:                                                          SSVDSolver.PARTITION_COMPARATOR,
1:                                                          true,
0:                                                          context
0:                                                            .getConfiguration());
1:       closeables.addFirst(rhatInput);
0:       closeables.addFirst(new IOUtils.MultipleOutputsCloseableAdapter(outputs));
1: 
1:       qr = new QRLastStep(qhatInput, rhatInput, blockNum);
1:       closeables.addFirst(qr);
0:       // it's so happens that current QRLastStep's implementation
0:       // preloads R sequence into memory in the constructor
0:       // so it's ok to close rhat input now.
1:       if (!rhatInput.hasNext()) {
1:         closeables.remove(rhatInput);
1:         rhatInput.close();
1:       }
1: 
1:       OutputCollector<LongWritable, SparseRowBlockWritable> btBlockCollector =
1:         new OutputCollector<LongWritable, SparseRowBlockWritable>() {
1: 
1:           @Override
1:           public void collect(LongWritable blockKey,
1:                               SparseRowBlockWritable block) throws IOException {
1:             try {
1:               mapContext.write(blockKey, block);
1:             } catch (InterruptedException exc) {
1:               throw new IOException("Interrupted.", exc);
1:             }
1:           }
1:         };
1: 
1:       btCollector =
0:         new SparseRowBlockAccumulator(context.getConfiguration()
0:           .getInt(PROP_OUTER_PROD_BLOCK_HEIGHT, -1), btBlockCollector);
1:       closeables.addFirst(btCollector);
1: 
1:   public static class OuterProductCombiner
0:       extends
0:       Reducer<Writable, SparseRowBlockWritable, Writable, SparseRowBlockWritable> {
0:     // protected final VectorWritable outValue = new VectorWritable();
1:     protected final SparseRowBlockWritable accum = new SparseRowBlockWritable();
0:     protected final Deque<Closeable> closeables = new ArrayDeque<Closeable>();
1:     protected int blockHeight;
1:     protected void setup(Context context) throws IOException,
1:       blockHeight =
1:         context.getConfiguration().getInt(PROP_OUTER_PROD_BLOCK_HEIGHT, -1);
1:     }
1:     @Override
1:     protected void reduce(Writable key,
1:                           Iterable<SparseRowBlockWritable> values,
1:                           Context context) throws IOException,
1:       InterruptedException {
0:       for (SparseRowBlockWritable bw : values)
1:         accum.plusBlock(bw);
1:       context.write(key, accum);
1:       accum.clear();
1:     }
1: 
1:     @Override
1:     protected void cleanup(Context context) throws IOException,
1:       InterruptedException {
1: 
1:       IOUtils.close(closeables);
1:     }
1:   }
1: 
1:   public static class OuterProductReducer
0:       extends
0:       Reducer<LongWritable, SparseRowBlockWritable, IntWritable, VectorWritable> {
1: 
1:     protected final SparseRowBlockWritable accum = new SparseRowBlockWritable();
0:     protected final Deque<Closeable> closeables = new ArrayDeque<Closeable>();
1: 
1:     protected int blockHeight;
1:     private boolean outputBBt;
1:     private UpperTriangular mBBt;
1:     private MultipleOutputs outputs;
1:     private final IntWritable btKey = new IntWritable();
1:     private final VectorWritable btValue = new VectorWritable();
1: 
1:     @Override
1:     protected void setup(Context context) throws IOException,
1:       InterruptedException {
1: 
1:       blockHeight =
1:         context.getConfiguration().getInt(PROP_OUTER_PROD_BLOCK_HEIGHT, -1);
1: 
0:       outputBBt =
0:         context.getConfiguration().getBoolean(PROP_OUPTUT_BBT_PRODUCTS, false);
1: 
1:       if (outputBBt) {
0:         int k = context.getConfiguration().getInt(QJob.PROP_K, -1);
0:         int p = context.getConfiguration().getInt(QJob.PROP_P, -1);
1: 
1:         Validate.isTrue(k > 0, "invalid k parameter");
0:         Validate.isTrue(p > 0, "invalid p parameter");
1:         mBBt = new UpperTriangular(k + p);
1: 
0:         outputs = new MultipleOutputs(new JobConf(context.getConfiguration()));
0:         closeables
0:           .addFirst(new IOUtils.MultipleOutputsCloseableAdapter(outputs));
1: 
1:       }
1:     }
1: 
1:     @Override
1:     protected void reduce(LongWritable key,
1:                           Iterable<SparseRowBlockWritable> values,
1:                           Context context) throws IOException,
1:       InterruptedException {
1: 
1:       accum.clear();
0:       for (SparseRowBlockWritable bw : values)
1:         accum.plusBlock(bw);
1: 
0:       // at this point, sum of rows should be in accum,
0:       // so we just generate outer self product of it and add to
0:       // BBt accumulator.
1: 
1:       for (int k = 0; k < accum.getNumRows(); k++) {
1:         Vector btRow = accum.getRows()[k];
1:         btKey.set((int) (key.get() * blockHeight + accum.getRowIndices()[k]));
1:         btValue.set(btRow);
1:         context.write(btKey, btValue);
1: 
1:         if (outputBBt) {
1:           int kp = mBBt.numRows();
1:           // accumulate partial BBt sum
1:           for (int i = 0; i < kp; i++) {
1:             double vi = btRow.get(i);
1:             if (vi != 0.0) {
1:               for (int j = i; j < kp; j++) {
1:                 double vj = btRow.get(j);
0:                 if (vj != 0.0)
1:                   mBBt.setQuick(i, j, mBBt.getQuick(i, j) + vi * vj);
1:               }
1:             }
1:           }
1:         }
1: 
1:       }
1:     }
1: 
1:     @Override
1:     protected void cleanup(Context context) throws IOException,
1:       InterruptedException {
1: 
1:       // if we output BBt instead of Bt then we need to do it.
1:       try {
1:         if (outputBBt) {
1: 
1:           @SuppressWarnings("unchecked")
1:           OutputCollector<Writable, Writable> collector =
1:             outputs.getCollector(OUTPUT_BBT, null);
1: 
0:           collector
0:             .collect(new IntWritable(),
0:                      new VectorWritable(new DenseVector(mBBt.getData())));
1:         }
1:       } finally {
1:         IOUtils.close(closeables);
/////////////////////////////////////////////////////////////////////////
1:                          int btBlockHeight,
1:                          Class<? extends Writable> labelClass,
1:                          boolean outputBBtProducts)
1:     throws ClassNotFoundException, InterruptedException, IOException {
1: 
0:     MultipleOutputs
0:       .addNamedOutput(oldApiJob,
0:                       OUTPUT_Q,
0:                       org.apache.hadoop.mapred.SequenceFileOutputFormat.class,
0:                       labelClass,
0:                       VectorWritable.class);
1: 
1:     if (outputBBtProducts) {
0:       MultipleOutputs
0:         .addNamedOutput(oldApiJob,
0:                         OUTPUT_BBT,
0:                         org.apache.hadoop.mapred.SequenceFileOutputFormat.class,
0:                         IntWritable.class,
0:                         VectorWritable.class);
1:     }
1: 
0:     // hack: we use old api multiple outputs
0:     // since they are not available in the new api of
0:     // either 0.20.2 or 0.20.203 but wrap it into a new api
0:     // job so we can use new api interfaces.
/////////////////////////////////////////////////////////////////////////
0:     // FileOutputFormat.setCompressOutput(job, true);
1:     SequenceFileOutputFormat.setOutputCompressionType(job,
1:                                                       CompressionType.BLOCK);
1:     job.setMapOutputKeyClass(LongWritable.class);
1:     job.setMapOutputValueClass(SparseRowBlockWritable.class);
1:     job.setCombinerClass(OuterProductCombiner.class);
/////////////////////////////////////////////////////////////////////////
1:     job.getConfiguration().setBoolean(PROP_OUPTUT_BBT_PRODUCTS,
1:                                       outputBBtProducts);
1:     job.getConfiguration().setInt(PROP_OUTER_PROD_BLOCK_HEIGHT, btBlockHeight);
/////////////////////////////////////////////////////////////////////////
commit:b479bd2
/////////////////////////////////////////////////////////////////////////
0:  * Bt job. For details, see working notes in MAHOUT-376. <P>
0:  * 
0:  * Uses hadoop deprecated API wherever new api has not been updated (MAHOUT-593), 
0:  * hence @SuppressWarning("deprecation"). <P>
0:  * 
1: @SuppressWarnings("deprecation")
/////////////////////////////////////////////////////////////////////////
1: 
0:     // private int qCount; // debug
/////////////////////////////////////////////////////////////////////////
0:       mQt = GivensThinSolver.computeQtHat(v.getBlock(), blockNum == 0 ? 0 : 1,
0:           new CopyConstructorIterator<UpperTriangular>(mRs.iterator()));
/////////////////////////////////////////////////////////////////////////
0:       // qCount++;
/////////////////////////////////////////////////////////////////////////
0:     @SuppressWarnings("unchecked")
0:     private void outputQRow(Writable key, Writable value) throws IOException {
0:       outputs.getCollector(OUTPUT_Q, null).collect(key, value);
1:     }
1:     
/////////////////////////////////////////////////////////////////////////
0:                                // reverse
1:       
0:       outputQRow(key,qRowValue);
1:       if (!aRow.isDense()) {
/////////////////////////////////////////////////////////////////////////
1:       } else {
/////////////////////////////////////////////////////////////////////////
0:         SequenceFileValueIterator<VectorWritable> iterator = new SequenceFileValueIterator<VectorWritable>(
0:             fstat.getPath(), true, context.getConfiguration());
/////////////////////////////////////////////////////////////////////////
0:           GivensThinSolver.mergeR(mRs.get(0), new UpperTriangular(rValue.get()));
/////////////////////////////////////////////////////////////////////////
0:     protected void reduce(IntWritable key, Iterable<VectorWritable> values, Context ctx) throws IOException,
0:       InterruptedException {
/////////////////////////////////////////////////////////////////////////
0:                          Class<? extends Writable> labelClass) throws ClassNotFoundException, InterruptedException,
0:     IOException {
0:     MultipleOutputs.addNamedOutput(oldApiJob, OUTPUT_Q, org.apache.hadoop.mapred.SequenceFileOutputFormat.class,
0:         labelClass, VectorWritable.class);
/////////////////////////////////////////////////////////////////////////
0:     SequenceFileOutputFormat.setOutputCompressionType(job, CompressionType.BLOCK);
commit:b0ac9f5
/////////////////////////////////////////////////////////////////////////
0: import org.apache.mahout.math.RandomAccessSparseVector;
0: import org.apache.mahout.math.SequentialAccessSparseVector;
/////////////////////////////////////////////////////////////////////////
0:       if ( (aRow instanceof SequentialAccessSparseVector) ||
0:           (aRow instanceof RandomAccessSparseVector )) {
0:         for ( Vector.Element el:aRow ) { 
0:           double mul=el.get();
0:           for ( int j =0; j < kp; j++ ) 
1:             btRow.setQuick(j, mul * qRow.getQuick(j));
0:           btKey.set(el.index());
0:           context.write(btKey, btValue);
1:         }
0:       } else { 
1:         int n = aRow.size();
1:         for (int i = 0; i < n; i++) {
1:           double mul = aRow.getQuick(i);
0:           for (int j = 0; j < kp; j++)
1:             btRow.setQuick(j, mul * qRow.getQuick(j));
0:           btKey.set(i);
0:           context.write(btKey, btValue);
1:         }
commit:151de0d
/////////////////////////////////////////////////////////////////////////
1: /**
1:  * Licensed to the Apache Software Foundation (ASF) under one or more
1:  * contributor license agreements.  See the NOTICE file distributed with
1:  * this work for additional information regarding copyright ownership.
1:  * The ASF licenses this file to You under the Apache License, Version 2.0
1:  * (the "License"); you may not use this file except in compliance with
1:  * the License.  You may obtain a copy of the License at
0:  *
1:  *     http://www.apache.org/licenses/LICENSE-2.0
0:  *
1:  * Unless required by applicable law or agreed to in writing, software
1:  * distributed under the License is distributed on an "AS IS" BASIS,
1:  * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
1:  * See the License for the specific language governing permissions and
1:  * limitations under the License.
1:  */
1: 
1: package org.apache.mahout.math.hadoop.stochasticsvd;
1: 
0: import java.io.IOException;
0: import java.util.ArrayList;
0: import java.util.Arrays;
0: import java.util.Iterator;
0: import java.util.List;
1: 
1: import org.apache.hadoop.conf.Configuration;
0: import org.apache.hadoop.fs.FileStatus;
0: import org.apache.hadoop.fs.FileSystem;
1: import org.apache.hadoop.fs.Path;
1: import org.apache.hadoop.io.IntWritable;
0: import org.apache.hadoop.io.SequenceFile;
1: import org.apache.hadoop.io.SequenceFile.CompressionType;
1: import org.apache.hadoop.io.Writable;
1: import org.apache.hadoop.io.compress.DefaultCodec;
1: import org.apache.hadoop.mapred.JobConf;
1: import org.apache.hadoop.mapred.lib.MultipleOutputs;
1: import org.apache.hadoop.mapreduce.Job;
1: import org.apache.hadoop.mapreduce.Mapper;
1: import org.apache.hadoop.mapreduce.Reducer;
1: import org.apache.hadoop.mapreduce.lib.input.FileInputFormat;
1: import org.apache.hadoop.mapreduce.lib.input.SequenceFileInputFormat;
1: import org.apache.hadoop.mapreduce.lib.output.FileOutputFormat;
1: import org.apache.hadoop.mapreduce.lib.output.SequenceFileOutputFormat;
1: import org.apache.mahout.math.DenseVector;
1: import org.apache.mahout.math.Vector;
1: import org.apache.mahout.math.VectorWritable;
0: import org.apache.mahout.math.hadoop.stochasticsvd.QJob.QJobKeyWritable;
1: 
1: /**
0:  * Bt job. For details, see working notes in MAHOUT-376. 
0:  *
1:  */
0: @SuppressWarnings("deprecation")
0: public class BtJob {
1: 
1:   public static final String OUTPUT_Q = "Q";
1:   public static final String OUTPUT_BT = "part";
1:   public static final String PROP_QJOB_PATH = "ssvd.QJob.path";
1: 
0:   public static class BtMapper extends
0:       Mapper<Writable, VectorWritable, IntWritable, VectorWritable> {
1: 
0:     private SequenceFile.Reader qInput;
0:     private List<UpperTriangular> mRs = new ArrayList<UpperTriangular>();
1:     private int blockNum;
0:     private double[][] mQt;
0:     private int cnt;
0:     private int r;
1:     private MultipleOutputs outputs;
0:     private IntWritable btKey = new IntWritable();
0:     private VectorWritable btValue = new VectorWritable();
0:     private int kp;
0:     private VectorWritable qRowValue = new VectorWritable();
0:     private int qCount; // debug
1: 
0:     void loadNextQt(Context ctx) throws IOException, InterruptedException {
0:       QJobKeyWritable key = new QJobKeyWritable();
0:       DenseBlockWritable v = new DenseBlockWritable();
1: 
0:       boolean more = qInput.next(key, v);
0:       assert more;
1: 
0:       mQt = GivensThinSolver.computeQtHat(v.getBlock(), blockNum == 0 ? 0
0:           : 1, new GivensThinSolver.DeepCopyUTIterator(mRs.iterator()));
0:       r = mQt[0].length;
0:       kp = mQt.length;
0:       if (btValue.get() == null)
0:         btValue.set(new DenseVector(kp));
0:       if (qRowValue.get() == null)
0:         qRowValue.set(new DenseVector(kp));
1: 
0:       // also output QHat -- although we don't know the A labels there. Is it
0:       // important?
0:       // DenseVector qRow = new DenseVector(m_kp);
0:       // IntWritable oKey = new IntWritable();
0:       // VectorWritable oV = new VectorWritable();
0:       //
0:       // for ( int i = m_r-1; i>=0; i-- ) {
0:       // for ( int j= 0; j < m_kp; j++ )
0:       // qRow.setQuick(j, m_qt[j][i]);
0:       // oKey.set((m_blockNum<<20)+m_r-i-1);
0:       // oV.set(qRow);
0:       // // so the block #s range is thus 0..2048, and number of rows per block
0:       // is 0..2^20.
0:       // // since we are not really sending it out to sort (it is a 'side
0:       // file'),
0:       // // it doesn't matter if it overflows.
0:       // m_outputs.write( OUTPUT_Q, oKey, oV);
0:       // }
0:       qCount++;
1:     }
1: 
1:     @Override
0:     protected void cleanup(Context context) throws IOException,
0:         InterruptedException {
1: 
0:       if (qInput != null)
0:         qInput.close();
0:       if (outputs != null)
0:         outputs.close();
0:       super.cleanup(context);
1:     }
1: 
1:     @Override
0:     @SuppressWarnings ({"unchecked"})
0:     protected void map(Writable key, VectorWritable value, Context context)
0:         throws IOException, InterruptedException {
0:       if (mQt != null && cnt++ == r)
0:         mQt = null;
0:       if (mQt == null) {
0:         loadNextQt(context);
0:         cnt = 1;
1:       }
1: 
1:       // output Bt outer products
1:       Vector aRow = value.get();
0:       int qRowIndex = r - cnt; // because QHats are initially stored in
0:                                    // reverse
0:       Vector qRow = qRowValue.get();
0:       for (int j = 0; j < kp; j++)
0:         qRow.setQuick(j, mQt[j][qRowIndex]);
1: 
0:       outputs.getCollector(OUTPUT_Q, null).collect(key, qRowValue); // make
0:                                                                         // sure
0:                                                                         // Qs
0:                                                                         // are
0:                                                                         // inheriting
0:                                                                         // A row
0:                                                                         // labels.
1: 
0:       int n = aRow.size();
0:       Vector btRow = btValue.get();
0:       for (int i = 0; i < n; i++) {
0:         double mul = aRow.getQuick(i);
0:         for (int j = 0; j < kp; j++)
0:           btRow.setQuick(j, mul * qRow.getQuick(j));
0:         btKey.set(i);
0:         context.write(btKey, btValue);
1:       }
1: 
1:     }
1: 
1:     @Override
1:     protected void setup(Context context) throws IOException,
0:         InterruptedException {
1:       super.setup(context);
1: 
0:       Path qJobPath = new Path(context.getConfiguration().get(PROP_QJOB_PATH));
1: 
0:       FileSystem fs = FileSystem.get(context.getConfiguration());
0:       // actually this is kind of dangerous
0:       // becuase this routine thinks we need to create file name for
0:       // our current job and this will use -m- so it's just serendipity we are
0:       // calling
0:       // it from the mapper too as the QJob did.
0:       Path qInputPath = new Path(qJobPath, FileOutputFormat.getUniqueFile(
0:           context, QJob.OUTPUT_QHAT, ""));
0:       qInput = new SequenceFile.Reader(fs, qInputPath,
0:           context.getConfiguration());
1: 
1:       blockNum = context.getTaskAttemptID().getTaskID().getId();
1: 
0:       // read all r files _in order of task ids_, i.e. partitions
0:       Path rPath = new Path(qJobPath, QJob.OUTPUT_R + "-*");
0:       FileStatus[] rFiles = fs.globStatus(rPath);
1: 
0:       if (rFiles == null)
0:         throw new IOException("Can't find R inputs ");
1: 
0:       Arrays.sort(rFiles, SSVDSolver.partitionComparator);
1: 
0:       QJobKeyWritable rKey = new QJobKeyWritable();
0:       VectorWritable rValue = new VectorWritable();
1: 
0:       int block = 0;
0:       for (FileStatus fstat : rFiles) {
0:         SequenceFile.Reader rReader = new SequenceFile.Reader(fs,
0:             fstat.getPath(), context.getConfiguration());
0:         try {
0:           rReader.next(rKey, rValue);
0:         } finally {
0:           rReader.close();
1:         }
0:         if (block < blockNum && block > 0)
0:           GivensThinSolver.mergeR(mRs.get(0),
0:               new UpperTriangular(rValue.get()));
0:         else
0:           mRs.add(new UpperTriangular(rValue.get()));
0:         block++;
1:       }
0:       outputs = new MultipleOutputs(new JobConf(context.getConfiguration()));
1:     }
1:   }
1: 
0:   public static class OuterProductReducer extends
0:       Reducer<IntWritable, VectorWritable, IntWritable, VectorWritable> {
1: 
0:     private VectorWritable oValue = new VectorWritable();
0:     private DenseVector accum;
1: 
1:     @Override
0:     protected void reduce(IntWritable key, Iterable<VectorWritable> values,
0:         Context ctx) throws IOException, InterruptedException {
0:       Iterator<VectorWritable> vwIter = values.iterator();
1: 
0:       Vector vec = vwIter.next().get();
0:       if (accum == null || accum.size() != vec.size()) {
0:         accum = new DenseVector(vec);
0:         oValue.set(accum);
0:       } else
0:         accum.assign(vec);
1: 
0:       while (vwIter.hasNext())
0:         accum.addAll(vwIter.next().get());
0:       ctx.write(key, oValue);
1:     }
1: 
1:   }
1: 
0:   public static void run(Configuration conf, Path inputPathA[],
0:       Path inputPathQJob, Path outputPath, int minSplitSize, int k, int p,
0:       int numReduceTasks, Class<? extends Writable> labelClass)
0:       throws ClassNotFoundException, InterruptedException, IOException {
1: 
1:     JobConf oldApiJob = new JobConf(conf);
0:     MultipleOutputs.addNamedOutput(oldApiJob, OUTPUT_Q,
0:         org.apache.hadoop.mapred.SequenceFileOutputFormat.class, labelClass,
0:         VectorWritable.class);
1: 
1:     Job job = new Job(oldApiJob);
1:     job.setJobName("Bt-job");
1:     job.setJarByClass(BtJob.class);
1: 
1:     job.setInputFormatClass(SequenceFileInputFormat.class);
1:     job.setOutputFormatClass(SequenceFileOutputFormat.class);
1:     FileInputFormat.setInputPaths(job, inputPathA);
0:     if (minSplitSize > 0)
0:       SequenceFileInputFormat.setMinInputSplitSize(job, minSplitSize);
1:     FileOutputFormat.setOutputPath(job, outputPath);
1: 
0:     // MultipleOutputs.addNamedOutput(job, OUTPUT_Bt,
0:     // SequenceFileOutputFormat.class,
0:     // QJobKeyWritable.class,QJobValueWritable.class);
1: 
0:     // Warn: tight hadoop integration here:
1:     job.getConfiguration().set("mapreduce.output.basename", OUTPUT_BT);
0:     SequenceFileOutputFormat.setCompressOutput(job, true);
0:     SequenceFileOutputFormat.setOutputCompressorClass(job, DefaultCodec.class);
0:     SequenceFileOutputFormat.setOutputCompressionType(job,
0:         CompressionType.BLOCK);
1: 
0:     job.setMapOutputKeyClass(IntWritable.class);
0:     job.setMapOutputValueClass(VectorWritable.class);
1: 
1:     job.setOutputKeyClass(IntWritable.class);
1:     job.setOutputValueClass(VectorWritable.class);
1: 
1:     job.setMapperClass(BtMapper.class);
0:     job.setCombinerClass(OuterProductReducer.class);
1:     job.setReducerClass(OuterProductReducer.class);
0:     // job.setPartitionerClass(QPartitioner.class);
1: 
0:     // job.getConfiguration().setInt(QJob.PROP_AROWBLOCK_SIZE,aBlockRows );
0:     // job.getConfiguration().setLong(PROP_OMEGA_SEED, seed);
1:     job.getConfiguration().setInt(QJob.PROP_K, k);
1:     job.getConfiguration().setInt(QJob.PROP_P, p);
1:     job.getConfiguration().set(PROP_QJOB_PATH, inputPathQJob.toString());
1: 
0:     // number of reduce tasks doesn't matter. we don't actually
0:     // send anything to reducers. in fact, the only reason
0:     // we need to configure reduce step is so that combiners can fire.
0:     // so reduce here is purely symbolic.
1:     job.setNumReduceTasks(numReduceTasks);
1: 
1:     job.submit();
1:     job.waitForCompletion(false);
1: 
0:     if (!job.isSuccessful())
1:       throw new IOException("Bt job unsuccessful.");
1: 
1:   }
1: 
1: }
author:Sebastian Schelter
-------------------------------------------------------------------------------
commit:6d9179e
/////////////////////////////////////////////////////////////////////////
1: import org.apache.mahout.common.HadoopUtil;
/////////////////////////////////////////////////////////////////////////
1:         Path[] rFiles = HadoopUtil.getCachedFiles(conf);
commit:6d16230
/////////////////////////////////////////////////////////////////////////
1:     private void outputQRow(Writable key, Vector qRow, Vector aRow) throws IOException {
commit:74f849b
/////////////////////////////////////////////////////////////////////////
0: import com.google.common.collect.Lists;
/////////////////////////////////////////////////////////////////////////
0:     private final List<UpperTriangular> mRs = Lists.newArrayList();
commit:d608a88
/////////////////////////////////////////////////////////////////////////
0: import com.google.common.io.Closeables;
/////////////////////////////////////////////////////////////////////////
0:       Closeables.closeQuietly(qInput);
/////////////////////////////////////////////////////////////////////////
0:           Closeables.closeQuietly(iterator);
author:Grant Ingersoll
-------------------------------------------------------------------------------
commit:25d59aa
/////////////////////////////////////////////////////////////////////////
1:         //TODO: this probably can be replaced w/ local fs makeQualified
author:Jacob Alexander Mannix
-------------------------------------------------------------------------------
commit:dc62944
/////////////////////////////////////////////////////////////////////////
/////////////////////////////////////////////////////////////////////////
1:         for (Vector.Element el : aRow.nonZeroes()) {
author:smarthi
-------------------------------------------------------------------------------
commit:67a531e
/////////////////////////////////////////////////////////////////////////
1: import org.apache.commons.lang3.Validate;
author:Sean R. Owen
-------------------------------------------------------------------------------
commit:8b6a26a
/////////////////////////////////////////////////////////////////////////
1:       computeSq = conf.get(PROP_XI_PATH) != null;
commit:229aeff
/////////////////////////////////////////////////////////////////////////
1:           if (sbAccum == null) {
1:           }
commit:1de8cec
/////////////////////////////////////////////////////////////////////////
1:       FileSystem fs = FileSystem.get(inputPathQJob.toUri(), conf);
commit:1499411
/////////////////////////////////////////////////////////////////////////
0:     private final Deque<Closeable> closeables = new ArrayDeque<Closeable>();
/////////////////////////////////////////////////////////////////////////
0:     protected void setup(Context context) throws IOException,
/////////////////////////////////////////////////////////////////////////
1:       for (SparseRowBlockWritable bw : values) {
1:       }
/////////////////////////////////////////////////////////////////////////
1:       for (SparseRowBlockWritable bw : values) {
1:       }
/////////////////////////////////////////////////////////////////////////
1:                 if (vj != 0.0) {
1:                 }
commit:3218e95
/////////////////////////////////////////////////////////////////////////
1: public final class BtJob {
/////////////////////////////////////////////////////////////////////////
0:     //private int qCount; // debug
/////////////////////////////////////////////////////////////////////////
0:       //qCount++;
/////////////////////////////////////////////////////////////////////////
0:       if ((aRow instanceof SequentialAccessSparseVector) || (aRow instanceof RandomAccessSparseVector)) {
/////////////////////////////////////////////////////////////////////////
0:       Arrays.sort(rFiles, SSVDSolver.PARTITION_COMPARATOR);
commit:a13b4b7
/////////////////////////////////////////////////////////////////////////
0: import org.apache.mahout.common.iterator.CopyConstructorIterator;
1: import org.apache.mahout.common.iterator.sequencefile.SequenceFileValueIterator;
/////////////////////////////////////////////////////////////////////////
1:   private BtJob() {
0:   }
0: 
0:   public static class BtMapper extends Mapper<Writable, VectorWritable, IntWritable, VectorWritable> {
0:     private final List<UpperTriangular> mRs = new ArrayList<UpperTriangular>();
0:     private final IntWritable btKey = new IntWritable();
0:     private final VectorWritable btValue = new VectorWritable();
1:     private final VectorWritable qRowValue = new VectorWritable();
0:     void loadNextQt() throws IOException {
0:       Writable key = new QJobKeyWritable();
0:           : 1, new CopyConstructorIterator<UpperTriangular>(mRs.iterator()));
0:       if (btValue.get() == null) {
0:       }
0:       if (qRowValue.get() == null) {
0:       }
/////////////////////////////////////////////////////////////////////////
0:     protected void cleanup(Context context) throws IOException, InterruptedException {
0:       if (qInput != null) {
0:       }
0:       if (outputs != null) {
0:       }
0:     protected void map(Writable key, VectorWritable value, Context context) throws IOException, InterruptedException {
0:       if (mQt != null && cnt++ == r) {
0:       }
0:         loadNextQt();
/////////////////////////////////////////////////////////////////////////
1:       for (int j = 0; j < kp; j++) {
0:       }
0:       outputs.getCollector(OUTPUT_Q, null).collect(key, qRowValue);
0:       // make sure Qs are inheriting A row labels.
0:       if ((aRow instanceof SequentialAccessSparseVector) ||
0:           (aRow instanceof RandomAccessSparseVector)) {
0:         for (Vector.Element el : aRow) {
1:           double mul = el.get();
1:           for (int j = 0; j < kp; j++) {
0:           }
/////////////////////////////////////////////////////////////////////////
0:           for (int j = 0; j < kp; j++) {
0:           }
/////////////////////////////////////////////////////////////////////////
0:     protected void setup(Context context) throws IOException, InterruptedException {
/////////////////////////////////////////////////////////////////////////
0:       Path qInputPath = new Path(qJobPath, FileOutputFormat.getUniqueFile(context, QJob.OUTPUT_QHAT, ""));
0:       qInput = new SequenceFile.Reader(fs, qInputPath, context.getConfiguration());
/////////////////////////////////////////////////////////////////////////
0:       if (rFiles == null) {
0:       }
0:         SequenceFileValueIterator<VectorWritable> iterator =
0:             new SequenceFileValueIterator<VectorWritable>(fstat.getPath(), true, context.getConfiguration());
0:         VectorWritable rValue;
0:           rValue = iterator.next();
0:           iterator.close();
0:         if (block < blockNum && block > 0) {
0:                                   new UpperTriangular(rValue.get()));
0:         } else {
0:         }
0:   public static class OuterProductReducer extends Reducer<IntWritable, VectorWritable, IntWritable, VectorWritable> {
0:     private final VectorWritable oValue = new VectorWritable();
/////////////////////////////////////////////////////////////////////////
0:       } else {
0:       }
0:       while (vwIter.hasNext()) {
0:       }
1:   public static void run(Configuration conf,
1:                          Path[] inputPathA,
1:                          Path inputPathQJob,
1:                          Path outputPath,
1:                          int minSplitSize,
1:                          int k,
1:                          int p,
1:                          int numReduceTasks,
0:                          Class<? extends Writable> labelClass)
0:     throws ClassNotFoundException, InterruptedException, IOException {
/////////////////////////////////////////////////////////////////////////
1:     if (minSplitSize > 0) {
1:       FileInputFormat.setMinInputSplitSize(job, minSplitSize);
0:     }
/////////////////////////////////////////////////////////////////////////
0:     FileOutputFormat.setCompressOutput(job, true);
1:     FileOutputFormat.setOutputCompressorClass(job, DefaultCodec.class);
/////////////////////////////////////////////////////////////////////////
1:     if (!job.isSuccessful()) {
0:     }
============================================================================