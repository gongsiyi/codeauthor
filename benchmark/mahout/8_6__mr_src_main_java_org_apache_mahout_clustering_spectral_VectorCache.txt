1:48d069f: /**
1:48d069f:  * Licensed to the Apache Software Foundation (ASF) under one or more
1:48d069f:  * contributor license agreements.  See the NOTICE file distributed with
1:48d069f:  * this work for additional information regarding copyright ownership.
1:48d069f:  * The ASF licenses this file to You under the Apache License, Version 2.0
1:48d069f:  * (the "License"); you may not use this file except in compliance with
1:48d069f:  * the License.  You may obtain a copy of the License at
4:48d069f:  *
1:48d069f:  *     http://www.apache.org/licenses/LICENSE-2.0
1:48d069f:  *
1:48d069f:  * Unless required by applicable law or agreed to in writing, software
1:48d069f:  * distributed under the License is distributed on an "AS IS" BASIS,
1:48d069f:  * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
1:48d069f:  * See the License for the specific language governing permissions and
1:48d069f:  * limitations under the License.
1:48d069f:  */
11:48d069f: 
1:b60c909: package org.apache.mahout.clustering.spectral;
1:48d069f: 
1:4a6453c: import java.io.IOException;
1:4a6453c: import java.net.URI;
1:4a6453c: import java.util.Arrays;
1:87d4b2e: 
1:48d069f: import org.apache.hadoop.conf.Configuration;
1:48d069f: import org.apache.hadoop.filecache.DistributedCache;
1:48d069f: import org.apache.hadoop.fs.FileSystem;
1:48d069f: import org.apache.hadoop.fs.Path;
1:48d069f: import org.apache.hadoop.io.IntWritable;
1:48d069f: import org.apache.hadoop.io.SequenceFile;
1:48d069f: import org.apache.hadoop.io.Writable;
1:48d069f: import org.apache.mahout.common.HadoopUtil;
1:a13b4b7: import org.apache.mahout.common.iterator.sequencefile.SequenceFileValueIterator;
1:48d069f: import org.apache.mahout.math.Vector;
1:48d069f: import org.apache.mahout.math.VectorWritable;
1:765834c: import org.slf4j.Logger;
1:765834c: import org.slf4j.LoggerFactory;
1:4a6453c: 
1:765834c: 
1:48d069f: /**
1:48d069f:  * This class handles reading and writing vectors to the Hadoop
1:48d069f:  * distributed cache. Created as a result of Eigencuts' liberal use
1:48d069f:  * of such functionality, but available to any algorithm requiring it.
1:48d069f:  */
1:48d069f: public final class VectorCache {
1:48d069f: 
1:765834c:   private static final Logger log = LoggerFactory.getLogger(VectorCache.class);
1:765834c: 
1:049e7dc:   private VectorCache() {
1:049e7dc:   }
1:4a6453c: 
1:48d069f:   /**
1:4a6453c:    * @param key    SequenceFile key
1:48d069f:    * @param vector Vector to save, to be wrapped as VectorWritable
1:48d069f:    */
1:a13b4b7:   public static void save(Writable key,
1:a13b4b7:                           Vector vector,
1:a13b4b7:                           Path output,
1:a13b4b7:                           Configuration conf,
1:a13b4b7:                           boolean overwritePath,
1:a13b4b7:                           boolean deleteOnExit) throws IOException {
1:4a6453c: 
1:1de8cec:     FileSystem fs = FileSystem.get(output.toUri(), conf);
1:48d069f:     output = fs.makeQualified(output);
1:48d069f:     if (overwritePath) {
1:a13b4b7:       HadoopUtil.delete(conf, output);
1:4a6453c:     }
1:049e7dc: 
1:48d069f:     // set the cache
1:4a6453c:     DistributedCache.setCacheFiles(new URI[]{output.toUri()}, conf);
1:4a6453c: 
1:48d069f:     // set up the writer
1:85f9ece:     try (SequenceFile.Writer writer = new SequenceFile.Writer(fs, conf, output,
1:85f9ece:         IntWritable.class, VectorWritable.class)){
1:d608a88:       writer.append(key, new VectorWritable(vector));
1:d608a88:     }
1:d608a88: 
1:48d069f:     if (deleteOnExit) {
1:48d069f:       fs.deleteOnExit(output);
1:4a6453c:     }
6:48d069f:   }
1:4a6453c: 
1:48d069f:   /**
1:48d069f:    * Calls the save() method, setting the cache to overwrite any previous
1:48d069f:    * Path and to delete the path after exiting
1:48d069f:    */
1:049e7dc:   public static void save(Writable key, Vector vector, Path output, Configuration conf) throws IOException {
1:049e7dc:     save(key, vector, output, conf, true, true);
1:4a6453c:   }
1:4a6453c: 
1:48d069f:   /**
1:a13b4b7:    * Loads the vector from {@link DistributedCache}. Returns null if no vector exists.
1:48d069f:    */
1:a13b4b7:   public static Vector load(Configuration conf) throws IOException {
1:6d9179e:     Path[] files = HadoopUtil.getCachedFiles(conf);
1:6d9179e: 
1:6d9179e:     if (files.length != 1) {
1:6d9179e:       throw new IOException("Cannot read Frequency list from Distributed Cache (" + files.length + ')');
1:4a6453c:     }
1:4a6453c: 
1:4a6453c:     if (log.isInfoEnabled()) {
1:4a6453c:       log.info("Files are: {}", Arrays.toString(files));
1:4a6453c:     }
1:4a6453c:     return load(conf, files[0]);
1:4a6453c:   }
1:4a6453c: 
1:48d069f:   /**
1:a13b4b7:    * Loads a Vector from the specified path. Returns null if no vector exists.
1:48d069f:    */
1:a13b4b7:   public static Vector load(Configuration conf, Path input) throws IOException {
1:765834c:     log.info("Loading vector from: {}", input);
1:85f9ece:     try (SequenceFileValueIterator<VectorWritable> iterator =
1:85f9ece:              new SequenceFileValueIterator<>(input, true, conf)){
1:d608a88:       return iterator.next().get();
1:d608a88:     }
1:48d069f:   }
1:48d069f: }
============================================================================
author:Suneel Marthi
-------------------------------------------------------------------------------
commit:85f9ece
/////////////////////////////////////////////////////////////////////////
/////////////////////////////////////////////////////////////////////////
1:     try (SequenceFile.Writer writer = new SequenceFile.Writer(fs, conf, output,
1:         IntWritable.class, VectorWritable.class)){
/////////////////////////////////////////////////////////////////////////
1:     try (SequenceFileValueIterator<VectorWritable> iterator =
1:              new SequenceFileValueIterator<>(input, true, conf)){
commit:87c15be
/////////////////////////////////////////////////////////////////////////
0:             new SequenceFileValueIterator<>(input, true, conf);
author:pferrel
-------------------------------------------------------------------------------
commit:b988c49
author:frankscholten
-------------------------------------------------------------------------------
commit:1a42d85
author:Sebastian Schelter
-------------------------------------------------------------------------------
commit:b60c909
/////////////////////////////////////////////////////////////////////////
1: package org.apache.mahout.clustering.spectral;
commit:6d9179e
/////////////////////////////////////////////////////////////////////////
/////////////////////////////////////////////////////////////////////////
1:     Path[] files = HadoopUtil.getCachedFiles(conf);
1: 
1:     if (files.length != 1) {
1:       throw new IOException("Cannot read Frequency list from Distributed Cache (" + files.length + ')');
commit:d608a88
/////////////////////////////////////////////////////////////////////////
0: import com.google.common.io.Closeables;
/////////////////////////////////////////////////////////////////////////
0:     try {
1:       writer.append(key, new VectorWritable(vector));
0:     } finally {
0:       Closeables.closeQuietly(writer);
1:     }
1: 
/////////////////////////////////////////////////////////////////////////
0:     try {
1:       return iterator.next().get();
0:     } finally {
0:       Closeables.closeQuietly(iterator);
1:     }
author:Grant Ingersoll
-------------------------------------------------------------------------------
commit:25d59aa
/////////////////////////////////////////////////////////////////////////
0:     LocalFileSystem localFs = FileSystem.getLocal(conf);
/////////////////////////////////////////////////////////////////////////
0:         throw new IOException("Cannot read Frequency list from Distributed Cache (" + filesURIs.length + ')');
0:           throw new IOException("Cannot read Frequency list from Distributed Cache (" + filesURIs.length + ')');
/////////////////////////////////////////////////////////////////////////
0:     files[0] = localFs.makeQualified(files[0]);
commit:31cb292
/////////////////////////////////////////////////////////////////////////
0:       Closeables.close(writer, true);
/////////////////////////////////////////////////////////////////////////
0:       Closeables.close(iterator, true);
commit:4a6453c
/////////////////////////////////////////////////////////////////////////
0: import org.apache.hadoop.fs.LocalFileSystem;
/////////////////////////////////////////////////////////////////////////
1: import java.io.IOException;
1: import java.net.URI;
1: import java.util.Arrays;
1: 
/////////////////////////////////////////////////////////////////////////
1:    * @param key    SequenceFile key
/////////////////////////////////////////////////////////////////////////
1: 
/////////////////////////////////////////////////////////////////////////
1:     DistributedCache.setCacheFiles(new URI[]{output.toUri()}, conf);
1: 
0:     SequenceFile.Writer writer = new SequenceFile.Writer(fs, conf, output,
0:             IntWritable.class, VectorWritable.class);
/////////////////////////////////////////////////////////////////////////
1: 
/////////////////////////////////////////////////////////////////////////
1: 
0:     Path[] files = DistributedCache.getLocalCacheFiles(conf);
0:       log.debug("getLocalCacheFiles failed, trying getCacheFiles");
0:       URI[] filesURIs = DistributedCache.getCacheFiles(conf);
0:       if (filesURIs == null) {
0:         throw new IOException("Cannot read Frequency list from Distributed Cache");
1:       }
0:       if (filesURIs.length != 1) {
0:         throw new IOException("Cannot read Frequency list from Distributed Cache (" + files.length + ')');
1:       }
0:       files = new Path[1];
0:       files[0] = new Path(filesURIs[0].getPath());
0:     } else {
0:       // Fallback if we are running locally.
0:       LocalFileSystem localFs = FileSystem.getLocal(conf);
0:       if (!localFs.exists(files[0])) {
0:         URI[] filesURIs = DistributedCache.getCacheFiles(conf);
0:         if (filesURIs == null) {
0:           throw new IOException("Cannot read Frequency list from Distributed Cache");
1:         }
0:         if (filesURIs.length != 1) {
0:           throw new IOException("Cannot read Frequency list from Distributed Cache (" + files.length + ')');
1:         }
0:         files[0] = new Path(filesURIs[0].getPath());
1:       }
1: 
1:     if (log.isInfoEnabled()) {
1:       log.info("Files are: {}", Arrays.toString(files));
1:     }
1: 
1:     return load(conf, files[0]);
1: 
0:             new SequenceFileValueIterator<VectorWritable>(input, true, conf);
author:dfilimon
-------------------------------------------------------------------------------
commit:87d4b2e
/////////////////////////////////////////////////////////////////////////
0: import java.io.IOException;
0: import java.net.URI;
0: import java.util.Arrays;
1: 
/////////////////////////////////////////////////////////////////////////
/////////////////////////////////////////////////////////////////////////
0:       Closeables.close(writer, false);
author:Sean R. Owen
-------------------------------------------------------------------------------
commit:1de8cec
/////////////////////////////////////////////////////////////////////////
1:     FileSystem fs = FileSystem.get(output.toUri(), conf);
commit:765834c
/////////////////////////////////////////////////////////////////////////
0: import java.util.Arrays;
/////////////////////////////////////////////////////////////////////////
1: import org.slf4j.Logger;
1: import org.slf4j.LoggerFactory;
1: 
/////////////////////////////////////////////////////////////////////////
1:   private static final Logger log = LoggerFactory.getLogger(VectorCache.class);
1: 
/////////////////////////////////////////////////////////////////////////
0:     log.info("Files are: {}", Arrays.toString(files));
/////////////////////////////////////////////////////////////////////////
1:     log.info("Loading vector from: {}", input);
commit:a13b4b7
/////////////////////////////////////////////////////////////////////////
1: import org.apache.mahout.common.iterator.sequencefile.SequenceFileValueIterator;
/////////////////////////////////////////////////////////////////////////
1:   public static void save(Writable key,
1:                           Vector vector,
1:                           Path output,
1:                           Configuration conf,
1:                           boolean overwritePath,
1:                           boolean deleteOnExit) throws IOException {
1:       HadoopUtil.delete(conf, output);
/////////////////////////////////////////////////////////////////////////
1:    * Loads the vector from {@link DistributedCache}. Returns null if no vector exists.
1:   public static Vector load(Configuration conf) throws IOException {
0:     URI[] files = DistributedCache.getCacheFiles(conf);
0:     return load(conf, new Path(files[0].getPath()));
1:    * Loads a Vector from the specified path. Returns null if no vector exists.
1:   public static Vector load(Configuration conf, Path input) throws IOException {
0:     SequenceFileValueIterator<VectorWritable> iterator =
0:         new SequenceFileValueIterator<VectorWritable>(input, true, conf);
0:     VectorWritable vectorWritable = iterator.next();
0:     iterator.close();
0:     return vectorWritable.get();
commit:049e7dc
/////////////////////////////////////////////////////////////////////////
1:   private VectorCache() {
1:   }
1: 
/////////////////////////////////////////////////////////////////////////
1:   public static void save(Writable key, Vector vector, Path output, Configuration conf) throws IOException {
1:     save(key, vector, output, conf, true, true);
0:     return load(key, conf, new Path(files[0].getPath()));
0:   public static Vector load(Writable key, Configuration conf, Path input) throws IOException {
author:Jeff Eastman
-------------------------------------------------------------------------------
commit:48d069f
/////////////////////////////////////////////////////////////////////////
1: /**
1:  * Licensed to the Apache Software Foundation (ASF) under one or more
1:  * contributor license agreements.  See the NOTICE file distributed with
1:  * this work for additional information regarding copyright ownership.
1:  * The ASF licenses this file to You under the Apache License, Version 2.0
1:  * (the "License"); you may not use this file except in compliance with
1:  * the License.  You may obtain a copy of the License at
1:  *
1:  *     http://www.apache.org/licenses/LICENSE-2.0
1:  *
1:  * Unless required by applicable law or agreed to in writing, software
1:  * distributed under the License is distributed on an "AS IS" BASIS,
1:  * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
1:  * See the License for the specific language governing permissions and
1:  * limitations under the License.
1:  */
1: 
0: package org.apache.mahout.clustering.spectral.common;
1: 
0: import java.io.IOException;
0: import java.net.URI;
1: 
1: import org.apache.hadoop.conf.Configuration;
1: import org.apache.hadoop.filecache.DistributedCache;
1: import org.apache.hadoop.fs.FileSystem;
1: import org.apache.hadoop.fs.Path;
1: import org.apache.hadoop.io.IntWritable;
1: import org.apache.hadoop.io.SequenceFile;
1: import org.apache.hadoop.io.Writable;
1: import org.apache.mahout.common.HadoopUtil;
1: import org.apache.mahout.math.Vector;
1: import org.apache.mahout.math.VectorWritable;
1: 
1: /**
1:  * This class handles reading and writing vectors to the Hadoop
1:  * distributed cache. Created as a result of Eigencuts' liberal use
1:  * of such functionality, but available to any algorithm requiring it.
1:  */
1: public final class VectorCache {
1: 
1:   /**
1:    * 
0:    * @param key SequenceFile key
1:    * @param vector Vector to save, to be wrapped as VectorWritable
0:    * @param output
0:    * @param conf
0:    * @param overwritePath
0:    * @param deleteOnExit
1:    */
0:   public static void save(Writable key, Vector vector, Path output, Configuration
0:       conf, boolean overwritePath, boolean deleteOnExit) throws IOException {
1:     
0:     FileSystem fs = FileSystem.get(conf);
1:     output = fs.makeQualified(output);
1:     if (overwritePath) {
0:       HadoopUtil.overwriteOutput(output);
1:     }
1: 
1:     // set the cache
0:     DistributedCache.setCacheFiles(new URI[] {output.toUri()}, conf);
1:     
1:     // set up the writer
0:     SequenceFile.Writer writer = new SequenceFile.Writer(fs, conf, output, 
0:         IntWritable.class, VectorWritable.class);
0:     writer.append(key, new VectorWritable(vector));
0:     writer.close();
1:     
1:     if (deleteOnExit) {
1:       fs.deleteOnExit(output);
1:     }
1:   }
1:   
1:   /**
1:    * Calls the save() method, setting the cache to overwrite any previous
1:    * Path and to delete the path after exiting
0:    * @param key
0:    * @param vector
0:    * @param output
0:    * @param conf
1:    */
0:   public static void save(Writable key, Vector vector, Path output, Configuration conf)
0:     throws IOException {
0:     VectorCache.save(key, vector, output, conf, true, true);
1:   }
1:   
1:   /**
0:    * Loads the vector with the specified key from the cache. Returns null
0:    * if nothing is found (up to the caller to handle this accordingly)
1:    * 
0:    * @param key
0:    * @param conf
0:    * @return
0:    * @throws IOException
1:    */
0:   public static Vector load(Writable key, Configuration conf) throws IOException {
0:     URI [] files = DistributedCache.getCacheFiles(conf);
0:     if (files == null || files.length < 1) {
0:       return null;
1:     }
0:     return VectorCache.load(key, conf, new Path(files[0].getPath()));
1:   }
1:   
1:   /**
0:    * Loads a Vector from the specified path
1:    * 
0:    * @param key
0:    * @param conf
0:    * @return
1:    */
0:   public static Vector load(Writable key, Configuration conf, Path input) 
0:     throws IOException {
1: 
0:     FileSystem fs = FileSystem.get(conf);
0:     SequenceFile.Reader reader = new SequenceFile.Reader(fs, input, conf);
0:     VectorWritable retval = new VectorWritable();
0:     reader.next(key, retval);
0:     reader.close();
0:     return retval.get();
1:   }
1: }
============================================================================