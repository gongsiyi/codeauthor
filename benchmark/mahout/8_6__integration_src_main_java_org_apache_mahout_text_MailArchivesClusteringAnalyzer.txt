1:c36923f: /**
1:c36923f:  * Licensed to the Apache Software Foundation (ASF) under one or more
1:c36923f:  * contributor license agreements.  See the NOTICE file distributed with
1:c36923f:  * this work for additional information regarding copyright ownership.
1:c36923f:  * The ASF licenses this file to You under the Apache License, Version 2.0
1:c36923f:  * (the "License"); you may not use this file except in compliance with
1:c36923f:  * the License.  You may obtain a copy of the License at
1:c36923f:  *
1:c36923f:  *     http://www.apache.org/licenses/LICENSE-2.0
1:c36923f:  *
1:c36923f:  * Unless required by applicable law or agreed to in writing, software
1:c36923f:  * distributed under the License is distributed on an "AS IS" BASIS,
1:c36923f:  * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
1:c36923f:  * See the License for the specific language governing permissions and
1:c36923f:  * limitations under the License.
1:c36923f:  */
1:c36923f: package org.apache.mahout.text;
1:864ba1a: 
1:c36923f: import org.apache.lucene.analysis.TokenFilter;
1:c36923f: import org.apache.lucene.analysis.TokenStream;
1:d3ccbe0: import org.apache.lucene.analysis.Tokenizer;
1:6a4942c: import org.apache.lucene.analysis.core.LowerCaseFilter;
1:6a4942c: import org.apache.lucene.analysis.core.StopFilter;
1:6a4942c: import org.apache.lucene.analysis.en.PorterStemFilter;
1:6a4942c: import org.apache.lucene.analysis.miscellaneous.ASCIIFoldingFilter;
1:c36923f: import org.apache.lucene.analysis.standard.StandardFilter;
1:c36923f: import org.apache.lucene.analysis.standard.StandardTokenizer;
1:a4778a4: import org.apache.lucene.analysis.tokenattributes.CharTermAttribute;
1:6a4942c: import org.apache.lucene.analysis.util.CharArraySet;
1:6a4942c: import org.apache.lucene.analysis.util.StopwordAnalyzerBase;
1:4d0cd66: 
1:4d0cd66: import java.io.IOException;
1:4d0cd66: import java.util.Arrays;
1:4d0cd66: import java.util.regex.Matcher;
1:4d0cd66: import java.util.regex.Pattern;
1:864ba1a: 
1:c36923f: /**
1:c36923f:  * Custom Lucene Analyzer designed for aggressive feature reduction
1:c36923f:  * for clustering the ASF Mail Archives using an extended set of
1:c36923f:  * stop words, excluding non-alpha-numeric tokens, and porter stemming.
1:c36923f:  */
1:d3ccbe0: public final class MailArchivesClusteringAnalyzer extends StopwordAnalyzerBase {
1:c36923f:   // extended set of stop words composed of common mail terms like "hi",
1:c36923f:   // HTML tags, and Java keywords asmany of the messages in the archives
1:c36923f:   // are subversion check-in notifications
1:a551b15:     
1:4d0cd66:   private static final CharArraySet STOP_SET = new CharArraySet(Arrays.asList(
1:c36923f:     "3d","7bit","a0","about","above","abstract","across","additional","after",
1:c36923f:     "afterwards","again","against","align","all","almost","alone","along",
1:c36923f:     "already","also","although","always","am","among","amongst","amoungst",
1:c36923f:     "amount","an","and","another","any","anybody","anyhow","anyone","anything",
1:c36923f:     "anyway","anywhere","are","arial","around","as","ascii","assert","at",
1:c36923f:     "back","background","base64","bcc","be","became","because","become","becomes",
1:c36923f:     "becoming","been","before","beforehand","behind","being","below","beside",
1:c36923f:     "besides","between","beyond","bgcolor","blank","blockquote","body","boolean",
1:c36923f:     "border","both","br","break","but","by","can","cannot","cant","case","catch",
1:c36923f:     "cc","cellpadding","cellspacing","center","char","charset","cheers","class",
1:c36923f:     "co","color","colspan","com","con","const","continue","could","couldnt",
1:c36923f:     "cry","css","de","dear","default","did","didnt","different","div","do",
1:c36923f:     "does","doesnt","done","dont","double","down","due","during","each","eg",
1:c36923f:     "eight","either","else","elsewhere","empty","encoding","enough","enum",
1:c36923f:     "etc","eu","even","ever","every","everyone","everything","everywhere",
1:c36923f:     "except","extends","face","family","few","ffffff","final","finally","float",
1:c36923f:     "font","for","former","formerly","fri","from","further","get","give","go",
1:c36923f:     "good","got","goto","gt","h1","ha","had","has","hasnt","have","he","head",
1:c36923f:     "height","hello","helvetica","hence","her","here","hereafter","hereby",
1:c36923f:     "herein","hereupon","hers","herself","hi","him","himself","his","how",
1:c36923f:     "however","hr","href","html","http","https","id","ie","if","ill","im",
1:c36923f:     "image","img","implements","import","in","inc","instanceof","int","interface",
1:c36923f:     "into","is","isnt","iso-8859-1","it","its","itself","ive","just","keep",
1:c36923f:     "last","latter","latterly","least","left","less","li","like","long","look",
1:c36923f:     "lt","ltd","mail","mailto","many","margin","may","me","meanwhile","message",
1:c36923f:     "meta","might","mill","mine","mon","more","moreover","most","mostly","mshtml",
1:c36923f:     "mso","much","must","my","myself","name","namely","native","nbsp","need",
1:c36923f:     "neither","never","nevertheless","new","next","nine","no","nobody","none",
1:c36923f:     "noone","nor","not","nothing","now","nowhere","null","of","off","often",
1:c36923f:     "ok","on","once","only","onto","or","org","other","others","otherwise",
1:c36923f:     "our","ours","ourselves","out","over","own","package","pad","per","perhaps",
1:c36923f:     "plain","please","pm","printable","private","protected","public","put",
1:c36923f:     "quot","quote","r1","r2","rather","re","really","regards","reply","return",
1:c36923f:     "right","said","same","sans","sat","say","saying","see","seem","seemed",
1:c36923f:     "seeming","seems","serif","serious","several","she","short","should","show",
1:c36923f:     "side","since","sincere","six","sixty","size","so","solid","some","somehow",
1:c36923f:     "someone","something","sometime","sometimes","somewhere","span","src",
1:c36923f:     "static","still","strictfp","string","strong","style","stylesheet","subject",
1:c36923f:     "such","sun","super","sure","switch","synchronized","table","take","target",
1:c36923f:     "td","text","th","than","thanks","that","the","their","them","themselves",
1:c36923f:     "then","thence","there","thereafter","thereby","therefore","therein","thereupon",
1:c36923f:     "these","they","thick","thin","think","third","this","those","though",
1:c36923f:     "three","through","throughout","throw","throws","thru","thu","thus","tm",
1:c36923f:     "to","together","too","top","toward","towards","tr","transfer","transient",
1:c36923f:     "try","tue","type","ul","un","under","unsubscribe","until","up","upon",
1:c36923f:     "us","use","used","uses","using","valign","verdana","very","via","void",
1:c36923f:     "volatile","want","was","we","wed","weight","well","were","what","whatever",
1:c36923f:     "when","whence","whenever","where","whereafter","whereas","whereby","wherein",
1:c36923f:     "whereupon","wherever","whether","which","while","whither","who","whoever",
1:c36923f:     "whole","whom","whose","why","width","will","with","within","without",
1:c36923f:     "wont","would","wrote","www","yes","yet","you","your","yours","yourself",
1:c36923f:     "yourselves"
1:d3ccbe0:   ), false);
1:a551b15: 
1:c36923f:   // Regex used to exclude non-alpha-numeric tokens
1:6d16230:   private static final Pattern ALPHA_NUMERIC = Pattern.compile("^[a-z][a-z0-9_]+$");
1:6d16230:   private static final Matcher MATCHER = ALPHA_NUMERIC.matcher("");
1:a551b15: 
1:c36923f:   public MailArchivesClusteringAnalyzer() {
1:4d0cd66:     super(STOP_SET);
1:a551b15:   }
1:864ba1a: 
1:864ba1a:   public MailArchivesClusteringAnalyzer(CharArraySet stopSet) {
1:4d0cd66:     super(stopSet);
1:864ba1a:   }
1:864ba1a:   
1:c36923f:   @Override
1:4d0cd66:   protected TokenStreamComponents createComponents(String fieldName) {
1:4d0cd66:     Tokenizer tokenizer = new StandardTokenizer();
1:4d0cd66:     TokenStream result = new StandardFilter(tokenizer);
1:4d0cd66:     result = new LowerCaseFilter(result);
1:c36923f:     result = new ASCIIFoldingFilter(result);
1:c36923f:     result = new AlphaNumericMaxLengthFilter(result);
1:4d0cd66:     result = new StopFilter(result, STOP_SET);
1:d3ccbe0:     result = new PorterStemFilter(result);
1:d3ccbe0:     return new TokenStreamComponents(tokenizer, result);
4:c36923f:   }
1:670a7d2: 
1:c36923f:   /**
1:c36923f:    * Matches alpha-numeric tokens between 2 and 40 chars long.
1:c36923f:    */
1:b16c260:   static class AlphaNumericMaxLengthFilter extends TokenFilter {
1:a4778a4:     private final CharTermAttribute termAtt;
1:c36923f:     private final char[] output = new char[28];
1:4cff542: 
1:3218e95:     AlphaNumericMaxLengthFilter(TokenStream in) {
1:c36923f:       super(in);
1:a4778a4:       termAtt = addAttribute(CharTermAttribute.class);
1:c36923f:     }
1:85f9ece: 
1:c36923f:     @Override
1:c36923f:     public final boolean incrementToken() throws IOException {
1:c36923f:       // return the first alpha-numeric token between 2 and 40 length
1:c36923f:       while (input.incrementToken()) {
1:a4778a4:         int length = termAtt.length();
1:c36923f:         if (length >= 2 && length <= 28) {
1:a4778a4:           char[] buf = termAtt.buffer();
1:c36923f:           int at = 0;
1:3c22856:           for (int c = 0; c < length; c++) {
1:3218e95:             char ch = buf[c];
1:c36923f:             if (ch != '\'') {
1:c36923f:               output[at++] = ch;
1:c36923f:             }
1:c36923f:           }
1:3218e95:           String term = new String(output, 0, at);
1:6d16230:           MATCHER.reset(term);
1:6d16230:           if (MATCHER.matches() && !term.startsWith("a0")) {
1:a4778a4:             termAtt.setEmpty();
1:a4778a4:             termAtt.append(term);
1:c36923f:             return true;
1:c36923f:           }
1:c36923f:         }
1:c36923f:       }
1:c36923f:       return false;
1:c36923f:     }
1:c36923f:   }
1:c36923f: }
============================================================================
author:smarthi
-------------------------------------------------------------------------------
commit:4d0cd66
/////////////////////////////////////////////////////////////////////////
/////////////////////////////////////////////////////////////////////////
1: 
1: import java.io.IOException;
1: import java.util.Arrays;
1: import java.util.regex.Matcher;
1: import java.util.regex.Pattern;
/////////////////////////////////////////////////////////////////////////
1:   private static final CharArraySet STOP_SET = new CharArraySet(Arrays.asList(
/////////////////////////////////////////////////////////////////////////
1:     super(STOP_SET);
1:     super(stopSet);
1:   protected TokenStreamComponents createComponents(String fieldName) {
1:     Tokenizer tokenizer = new StandardTokenizer();
1:     TokenStream result = new StandardFilter(tokenizer);
1:     result = new LowerCaseFilter(result);
1:     result = new StopFilter(result, STOP_SET);
commit:c88c240
/////////////////////////////////////////////////////////////////////////
0:   private static final Version LUCENE_VERSION = Version.LUCENE_45;
commit:c36dc71
/////////////////////////////////////////////////////////////////////////
0:   private static final Version LUCENE_VERSION = Version.LUCENE_42;
author:Andrew Musselman
-------------------------------------------------------------------------------
commit:864ba1a
/////////////////////////////////////////////////////////////////////////
1: 
/////////////////////////////////////////////////////////////////////////
0:   private static final Version LUCENE_VERSION = Version.LUCENE_46;
1:   
0:   private static final CharArraySet STOP_SET = new CharArraySet(LUCENE_VERSION, Arrays.asList(
/////////////////////////////////////////////////////////////////////////
0:     super(LUCENE_VERSION, STOP_SET);
1:   public MailArchivesClusteringAnalyzer(CharArraySet stopSet) {
0:     super(LUCENE_VERSION, stopSet);
1: 
1:   }
1:   
0:     Tokenizer tokenizer = new StandardTokenizer(LUCENE_VERSION, reader);
0:     TokenStream result = new StandardFilter(LUCENE_VERSION, tokenizer);
0:     result = new LowerCaseFilter(LUCENE_VERSION, result);
0:     result = new StopFilter(LUCENE_VERSION, result, STOP_SET);
commit:a551b15
/////////////////////////////////////////////////////////////////////////
0: import org.apache.lucene.util.Version;
/////////////////////////////////////////////////////////////////////////
0:   private static final Version LUCENE_VERSION = Version.LUCENE_46;
1:   
0:   private static final CharArraySet STOP_SET = new CharArraySet(LUCENE_VERSION, Arrays.asList(
/////////////////////////////////////////////////////////////////////////
0:     super(LUCENE_VERSION, STOP_SET);
0:   public MailArchivesClusteringAnalyzer(CharArraySet stopSet) {
0:     super(LUCENE_VERSION, stopSet);
1: 
1:   }
1:   
0:     Tokenizer tokenizer = new StandardTokenizer(LUCENE_VERSION, reader);
0:     TokenStream result = new StandardFilter(LUCENE_VERSION, tokenizer);
0:     result = new LowerCaseFilter(LUCENE_VERSION, result);
0:     result = new StopFilter(LUCENE_VERSION, result, STOP_SET);
author:Suneel Marthi
-------------------------------------------------------------------------------
commit:670a7d2
/////////////////////////////////////////////////////////////////////////
/////////////////////////////////////////////////////////////////////////
0:   private static final Version LUCENE_VERSION = Version.LUCENE_4_10_3;
1: 
0:   private static final CharArraySet STOP_SET = new CharArraySet(Arrays.asList(
/////////////////////////////////////////////////////////////////////////
0:     super(STOP_SET);
0:     Tokenizer tokenizer = new StandardTokenizer(reader);
0:     TokenStream result = new StandardFilter(tokenizer);
0:     result = new LowerCaseFilter(result);
0:     result = new StopFilter(result, STOP_SET);
commit:4cff542
/////////////////////////////////////////////////////////////////////////
/////////////////////////////////////////////////////////////////////////
1: 
0:   private static final CharArraySet STOP_SET = new CharArraySet(Arrays.asList(
/////////////////////////////////////////////////////////////////////////
0:     super(STOP_SET);
0:     Tokenizer tokenizer = new StandardTokenizer(reader);
0:     TokenStream result = new StandardFilter(tokenizer);
0:     result = new LowerCaseFilter(result);
0:     result = new StopFilter(result, STOP_SET);
commit:85f9ece
/////////////////////////////////////////////////////////////////////////
1: 
author:frankscholten
-------------------------------------------------------------------------------
commit:2e5449f
/////////////////////////////////////////////////////////////////////////
0:   private static final Version LUCENE_VERSION = Version.LUCENE_46;
author:Grant Ingersoll
-------------------------------------------------------------------------------
commit:e48eb4c
/////////////////////////////////////////////////////////////////////////
0:   private static final Version LUCENE_VERSION = Version.LUCENE_43;
commit:fc6c6f3
/////////////////////////////////////////////////////////////////////////
commit:6a4942c
/////////////////////////////////////////////////////////////////////////
0: import org.apache.lucene.analysis.Analyzer;
1: import org.apache.lucene.analysis.core.LowerCaseFilter;
1: import org.apache.lucene.analysis.core.StopFilter;
1: import org.apache.lucene.analysis.en.PorterStemFilter;
1: import org.apache.lucene.analysis.miscellaneous.ASCIIFoldingFilter;
1: import org.apache.lucene.analysis.util.CharArraySet;
1: import org.apache.lucene.analysis.util.StopwordAnalyzerBase;
/////////////////////////////////////////////////////////////////////////
0:   private static final Version LUCENE_VERSION = Version.LUCENE_41;
0:     
0: private static CharArraySet stopSet = new CharArraySet(LUCENE_VERSION, Arrays.asList(
/////////////////////////////////////////////////////////////////////////
0:     super(LUCENE_VERSION, stopSet);
0:   public MailArchivesClusteringAnalyzer(CharArraySet stopSet) {
0: 
/////////////////////////////////////////////////////////////////////////
0:     result = new StopFilter(LUCENE_VERSION, result, stopSet);
commit:a4778a4
/////////////////////////////////////////////////////////////////////////
0: 
1: import org.apache.lucene.analysis.tokenattributes.CharTermAttribute;
/////////////////////////////////////////////////////////////////////////
0: public final class MailArchivesClusteringAnalyzer extends Analyzer {
/////////////////////////////////////////////////////////////////////////
0:     stopSet = (CharArraySet)StopFilter.makeStopSet(Version.LUCENE_31, Arrays.asList(STOP_WORDS));
/////////////////////////////////////////////////////////////////////////
0: 
0:     TokenStream result = new StandardTokenizer(Version.LUCENE_31, reader);
0:     result = new StandardFilter(Version.LUCENE_31, result);
0:     result = new LowerCaseFilter(Version.LUCENE_31, result);
0:     result = new StopFilter(Version.LUCENE_31, result, stopSet);
/////////////////////////////////////////////////////////////////////////
1:     private final CharTermAttribute termAtt;
1:       termAtt = addAttribute(CharTermAttribute.class);
/////////////////////////////////////////////////////////////////////////
1:         int length = termAtt.length();
1:           char[] buf = termAtt.buffer();
/////////////////////////////////////////////////////////////////////////
1:             termAtt.setEmpty();
1:             termAtt.append(term);
commit:c36923f
/////////////////////////////////////////////////////////////////////////
1: /**
1:  * Licensed to the Apache Software Foundation (ASF) under one or more
1:  * contributor license agreements.  See the NOTICE file distributed with
1:  * this work for additional information regarding copyright ownership.
1:  * The ASF licenses this file to You under the Apache License, Version 2.0
1:  * (the "License"); you may not use this file except in compliance with
1:  * the License.  You may obtain a copy of the License at
1:  *
1:  *     http://www.apache.org/licenses/LICENSE-2.0
1:  *
1:  * Unless required by applicable law or agreed to in writing, software
1:  * distributed under the License is distributed on an "AS IS" BASIS,
1:  * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
1:  * See the License for the specific language governing permissions and
1:  * limitations under the License.
1:  */
1: package org.apache.mahout.text;
0: 
0: import java.io.IOException;
0: import java.io.StringReader;
0: import java.util.Arrays;
0: import java.util.regex.Matcher;
0: import java.util.regex.Pattern;
0: import org.apache.lucene.analysis.Analyzer;
0: import org.apache.lucene.analysis.ASCIIFoldingFilter;
0: import org.apache.lucene.analysis.CharArraySet;
0: import org.apache.lucene.analysis.LengthFilter;
0: import org.apache.lucene.analysis.LowerCaseFilter;
0: import org.apache.lucene.analysis.PorterStemFilter;
0: import org.apache.lucene.analysis.StopFilter;
1: import org.apache.lucene.analysis.TokenFilter;
1: import org.apache.lucene.analysis.TokenStream;
1: import org.apache.lucene.analysis.standard.StandardFilter;
1: import org.apache.lucene.analysis.standard.StandardTokenizer;
0: import org.apache.lucene.analysis.tokenattributes.TermAttribute;
0: import org.apache.lucene.util.Version;
0: 
1: /**
1:  * Custom Lucene Analyzer designed for aggressive feature reduction
1:  * for clustering the ASF Mail Archives using an extended set of
1:  * stop words, excluding non-alpha-numeric tokens, and porter stemming.
1:  */
0: public class MailArchivesClusteringAnalyzer extends Analyzer {
0:   
1:   // extended set of stop words composed of common mail terms like "hi",
1:   // HTML tags, and Java keywords asmany of the messages in the archives
1:   // are subversion check-in notifications
0: 	private static final String[] STOP_WORDS = new String[] {
1: 	  "3d","7bit","a0","about","above","abstract","across","additional","after",
1: 	  "afterwards","again","against","align","all","almost","alone","along",
1: 	  "already","also","although","always","am","among","amongst","amoungst",
1: 	  "amount","an","and","another","any","anybody","anyhow","anyone","anything",
1: 	  "anyway","anywhere","are","arial","around","as","ascii","assert","at",
1: 	  "back","background","base64","bcc","be","became","because","become","becomes",
1: 	  "becoming","been","before","beforehand","behind","being","below","beside",
1: 	  "besides","between","beyond","bgcolor","blank","blockquote","body","boolean",
1: 	  "border","both","br","break","but","by","can","cannot","cant","case","catch",
1: 	  "cc","cellpadding","cellspacing","center","char","charset","cheers","class",
1: 	  "co","color","colspan","com","con","const","continue","could","couldnt",
1: 	  "cry","css","de","dear","default","did","didnt","different","div","do",
1: 	  "does","doesnt","done","dont","double","down","due","during","each","eg",
1: 	  "eight","either","else","elsewhere","empty","encoding","enough","enum",
1: 	  "etc","eu","even","ever","every","everyone","everything","everywhere",
1: 	  "except","extends","face","family","few","ffffff","final","finally","float",
1: 	  "font","for","former","formerly","fri","from","further","get","give","go",
1: 	  "good","got","goto","gt","h1","ha","had","has","hasnt","have","he","head",
1: 	  "height","hello","helvetica","hence","her","here","hereafter","hereby",
1: 	  "herein","hereupon","hers","herself","hi","him","himself","his","how",
1: 	  "however","hr","href","html","http","https","id","ie","if","ill","im",
1: 	  "image","img","implements","import","in","inc","instanceof","int","interface",
1: 	  "into","is","isnt","iso-8859-1","it","its","itself","ive","just","keep",
1: 	  "last","latter","latterly","least","left","less","li","like","long","look",
1: 	  "lt","ltd","mail","mailto","many","margin","may","me","meanwhile","message",
1: 	  "meta","might","mill","mine","mon","more","moreover","most","mostly","mshtml",
1: 	  "mso","much","must","my","myself","name","namely","native","nbsp","need",
1: 	  "neither","never","nevertheless","new","next","nine","no","nobody","none",
1: 	  "noone","nor","not","nothing","now","nowhere","null","of","off","often",
1: 	  "ok","on","once","only","onto","or","org","other","others","otherwise",
1: 	  "our","ours","ourselves","out","over","own","package","pad","per","perhaps",
1: 	  "plain","please","pm","printable","private","protected","public","put",
1: 	  "quot","quote","r1","r2","rather","re","really","regards","reply","return",
1: 	  "right","said","same","sans","sat","say","saying","see","seem","seemed",
1: 	  "seeming","seems","serif","serious","several","she","short","should","show",
1: 	  "side","since","sincere","six","sixty","size","so","solid","some","somehow",
1: 	  "someone","something","sometime","sometimes","somewhere","span","src",
1: 	  "static","still","strictfp","string","strong","style","stylesheet","subject",
1: 	  "such","sun","super","sure","switch","synchronized","table","take","target",
1: 	  "td","text","th","than","thanks","that","the","their","them","themselves",
1: 	  "then","thence","there","thereafter","thereby","therefore","therein","thereupon",
1: 	  "these","they","thick","thin","think","third","this","those","though",
1: 	  "three","through","throughout","throw","throws","thru","thu","thus","tm",
1: 	  "to","together","too","top","toward","towards","tr","transfer","transient",
1: 	  "try","tue","type","ul","un","under","unsubscribe","until","up","upon",
1: 	  "us","use","used","uses","using","valign","verdana","very","via","void",
1: 	  "volatile","want","was","we","wed","weight","well","were","what","whatever",
1: 	  "when","whence","whenever","where","whereafter","whereas","whereby","wherein",
1: 	  "whereupon","wherever","whether","which","while","whither","who","whoever",
1: 	  "whole","whom","whose","why","width","will","with","within","without",
1: 	  "wont","would","wrote","www","yes","yet","you","your","yours","yourself",
1: 	  "yourselves"
0: 	};
0: 
1: 	// Regex used to exclude non-alpha-numeric tokens
0:   private static final Pattern alphaNumeric = Pattern.compile("^[a-z][a-z0-9_]+$");
0:   private final CharArraySet stopSet;
0: 
1: 	public MailArchivesClusteringAnalyzer() {
0: 		stopSet = (CharArraySet)StopFilter.makeStopSet(Arrays.asList(STOP_WORDS));
0: 		java.util.TreeSet<String> tmp = new java.util.TreeSet<String>();
0: 		java.util.Iterator iter = stopSet.iterator();
0: 		while (iter.hasNext()) {
0: 		  tmp.add((String)iter.next());
1: 		}
1: 	}
0: 
0: 	public MailArchivesClusteringAnalyzer(CharArraySet stopSet) {
0: 		this.stopSet = stopSet;
1: 	}
0: 
1: 	@Override
0: 	public TokenStream tokenStream(String fieldName, java.io.Reader reader) {
0: 		@SuppressWarnings("deprecation")
0: 		TokenStream result = new StandardTokenizer(Version.LUCENE_CURRENT, reader);
0: 		result = new StandardFilter(result);
0: 		result = new LowerCaseFilter(result);
1:     result = new ASCIIFoldingFilter(result);
1:     result = new AlphaNumericMaxLengthFilter(result);
0: 		result = new StopFilter(false, result, stopSet);
0: 		return new PorterStemFilter(result);
1: 	}
0: 
1:   /**
1:    * Matches alpha-numeric tokens between 2 and 40 chars long.
1:    */
0: 	class AlphaNumericMaxLengthFilter extends TokenFilter {
0:     private TermAttribute termAtt;
1:     private final char[] output = new char[28];
0:     private Matcher matcher;
0: 
0: 	  public AlphaNumericMaxLengthFilter(TokenStream in) {
1: 	    super(in);
0: 	    termAtt = addAttribute(TermAttribute.class);
0: 	    matcher = alphaNumeric.matcher("foo");
1: 	  }
0: 
1: 	  @Override
1: 	  public final boolean incrementToken() throws IOException {
1: 	    // return the first alpha-numeric token between 2 and 40 length
1: 	    while (input.incrementToken()) {
0: 	      final int length = termAtt.termLength();
1: 	      if (length >= 2 && length <= 28) {
0: 	        final char[] buf = termAtt.termBuffer();
1: 	        int at = 0;
0: 	        for (int c=0; c < length; c++) {
0: 	          final char ch = buf[c];
1: 	          if (ch != '\'') {
1: 	            output[at++] = ch;
1: 	          }
1: 	        }
0: 	        final String term = new String(output, 0, at);
0: 	        matcher.reset(term);
0: 	        if (matcher.matches() && !term.startsWith("a0")) {
0:             termAtt.setTermBuffer(term);
1:             return true;	            
1: 	        }
1: 	      }
1: 	    }
1: 	    return false;
1: 	  }
1:   }
1: }
author:Sebastian Schelter
-------------------------------------------------------------------------------
commit:6d16230
/////////////////////////////////////////////////////////////////////////
0:   private static final CharArraySet STOP_SET = new CharArraySet(LUCENE_VERSION, Arrays.asList(
/////////////////////////////////////////////////////////////////////////
1:   private static final Pattern ALPHA_NUMERIC = Pattern.compile("^[a-z][a-z0-9_]+$");
1:   private static final Matcher MATCHER = ALPHA_NUMERIC.matcher("");
0:     super(LUCENE_VERSION, STOP_SET);
/////////////////////////////////////////////////////////////////////////
0:     result = new StopFilter(LUCENE_VERSION, result, STOP_SET);
/////////////////////////////////////////////////////////////////////////
1:           MATCHER.reset(term);
1:           if (MATCHER.matches() && !term.startsWith("a0")) {
commit:3c22856
/////////////////////////////////////////////////////////////////////////
1:           for (int c = 0; c < length; c++) {
author:Sean R. Owen
-------------------------------------------------------------------------------
commit:10c535c
/////////////////////////////////////////////////////////////////////////
0:   private static final CharArraySet stopSet = new CharArraySet(LUCENE_VERSION, Arrays.asList(
commit:229aeff
/////////////////////////////////////////////////////////////////////////
0:   private static final Matcher matcher = alphaNumeric.matcher("");
commit:3d44c1e
/////////////////////////////////////////////////////////////////////////
0: import java.util.Set;
/////////////////////////////////////////////////////////////////////////
0:   private static final Set<?> STOP_WORDS = new CharArraySet(Version.LUCENE_31, Arrays.asList(
/////////////////////////////////////////////////////////////////////////
0:   public MailArchivesClusteringAnalyzer(Set<?> stopSet) {
commit:d3ccbe0
/////////////////////////////////////////////////////////////////////////
0: import java.io.Reader;
0: import org.apache.lucene.analysis.StopwordAnalyzerBase;
1: import org.apache.lucene.analysis.Tokenizer;
/////////////////////////////////////////////////////////////////////////
1: public final class MailArchivesClusteringAnalyzer extends StopwordAnalyzerBase {
0:   private static final CharArraySet STOP_WORDS = new CharArraySet(Version.LUCENE_31, Arrays.asList(
/////////////////////////////////////////////////////////////////////////
1:   ), false);
0:     super(Version.LUCENE_31, STOP_WORDS);
0:     super(Version.LUCENE_31, stopSet);
0:   
0:   protected TokenStreamComponents createComponents(String fieldName, Reader reader) {
0:     Tokenizer tokenizer = new StandardTokenizer(Version.LUCENE_31, reader);
0:     TokenStream result = new StandardFilter(Version.LUCENE_31, tokenizer);
0:     result = new StopFilter(Version.LUCENE_31, result, stopwords);
1:     result = new PorterStemFilter(result);
1:     return new TokenStreamComponents(tokenizer, result);
commit:50fd693
commit:3218e95
/////////////////////////////////////////////////////////////////////////
0:   private static final String[] STOP_WORDS = {
0:     "3d","7bit","a0","about","above","abstract","across","additional","after",
0:     "afterwards","again","against","align","all","almost","alone","along",
0:     "already","also","although","always","am","among","amongst","amoungst",
0:     "amount","an","and","another","any","anybody","anyhow","anyone","anything",
0:     "anyway","anywhere","are","arial","around","as","ascii","assert","at",
0:     "back","background","base64","bcc","be","became","because","become","becomes",
0:     "becoming","been","before","beforehand","behind","being","below","beside",
0:     "besides","between","beyond","bgcolor","blank","blockquote","body","boolean",
0:     "border","both","br","break","but","by","can","cannot","cant","case","catch",
0:     "cc","cellpadding","cellspacing","center","char","charset","cheers","class",
0:     "co","color","colspan","com","con","const","continue","could","couldnt",
0:     "cry","css","de","dear","default","did","didnt","different","div","do",
0:     "does","doesnt","done","dont","double","down","due","during","each","eg",
0:     "eight","either","else","elsewhere","empty","encoding","enough","enum",
0:     "etc","eu","even","ever","every","everyone","everything","everywhere",
0:     "except","extends","face","family","few","ffffff","final","finally","float",
0:     "font","for","former","formerly","fri","from","further","get","give","go",
0:     "good","got","goto","gt","h1","ha","had","has","hasnt","have","he","head",
0:     "height","hello","helvetica","hence","her","here","hereafter","hereby",
0:     "herein","hereupon","hers","herself","hi","him","himself","his","how",
0:     "however","hr","href","html","http","https","id","ie","if","ill","im",
0:     "image","img","implements","import","in","inc","instanceof","int","interface",
0:     "into","is","isnt","iso-8859-1","it","its","itself","ive","just","keep",
0:     "last","latter","latterly","least","left","less","li","like","long","look",
0:     "lt","ltd","mail","mailto","many","margin","may","me","meanwhile","message",
0:     "meta","might","mill","mine","mon","more","moreover","most","mostly","mshtml",
0:     "mso","much","must","my","myself","name","namely","native","nbsp","need",
0:     "neither","never","nevertheless","new","next","nine","no","nobody","none",
0:     "noone","nor","not","nothing","now","nowhere","null","of","off","often",
0:     "ok","on","once","only","onto","or","org","other","others","otherwise",
0:     "our","ours","ourselves","out","over","own","package","pad","per","perhaps",
0:     "plain","please","pm","printable","private","protected","public","put",
0:     "quot","quote","r1","r2","rather","re","really","regards","reply","return",
0:     "right","said","same","sans","sat","say","saying","see","seem","seemed",
0:     "seeming","seems","serif","serious","several","she","short","should","show",
0:     "side","since","sincere","six","sixty","size","so","solid","some","somehow",
0:     "someone","something","sometime","sometimes","somewhere","span","src",
0:     "static","still","strictfp","string","strong","style","stylesheet","subject",
0:     "such","sun","super","sure","switch","synchronized","table","take","target",
0:     "td","text","th","than","thanks","that","the","their","them","themselves",
0:     "then","thence","there","thereafter","thereby","therefore","therein","thereupon",
0:     "these","they","thick","thin","think","third","this","those","though",
0:     "three","through","throughout","throw","throws","thru","thu","thus","tm",
0:     "to","together","too","top","toward","towards","tr","transfer","transient",
0:     "try","tue","type","ul","un","under","unsubscribe","until","up","upon",
0:     "us","use","used","uses","using","valign","verdana","very","via","void",
0:     "volatile","want","was","we","wed","weight","well","were","what","whatever",
0:     "when","whence","whenever","where","whereafter","whereas","whereby","wherein",
0:     "whereupon","wherever","whether","which","while","whither","who","whoever",
0:     "whole","whom","whose","why","width","will","with","within","without",
0:     "wont","would","wrote","www","yes","yet","you","your","yours","yourself",
0:     "yourselves"
0:   };
0:   // Regex used to exclude non-alpha-numeric tokens
0:   public MailArchivesClusteringAnalyzer() {
0:     stopSet = (CharArraySet)StopFilter.makeStopSet(Arrays.asList(STOP_WORDS));
0:     Collection<String> tmp = new java.util.TreeSet<String>();
0:   }
0:   public MailArchivesClusteringAnalyzer(CharArraySet stopSet) {
0:     this.stopSet = stopSet;
0:   }
0:   @Override
0:   public TokenStream tokenStream(String fieldName, java.io.Reader reader) {
0:     @SuppressWarnings("deprecation")
0:     TokenStream result = new StandardTokenizer(Version.LUCENE_CURRENT, reader);
0:     result = new StandardFilter(result);
0:     result = new LowerCaseFilter(result);
0:     result = new StopFilter(false, result, stopSet);
0:     return new PorterStemFilter(result);
0:   }
/////////////////////////////////////////////////////////////////////////
1:     AlphaNumericMaxLengthFilter(TokenStream in) {
0:       super(in);
0:       termAtt = addAttribute(TermAttribute.class);
0:       matcher = alphaNumeric.matcher("foo");
0:     }
0:     @Override
0:     public final boolean incrementToken() throws IOException {
0:       // return the first alpha-numeric token between 2 and 40 length
0:       while (input.incrementToken()) {
0:         int length = termAtt.termLength();
0:         if (length >= 2 && length <= 28) {
0:           char[] buf = termAtt.termBuffer();
0:           int at = 0;
0:           for (int c=0; c < length; c++) {
1:             char ch = buf[c];
0:             if (ch != '\'') {
0:               output[at++] = ch;
0:             }
0:           }
1:           String term = new String(output, 0, at);
0:           matcher.reset(term);
0:           if (matcher.matches() && !term.startsWith("a0")) {
0:             return true;
0:           }
0:         }
0:       }
0:       return false;
0:     }
commit:b16c260
/////////////////////////////////////////////////////////////////////////
/////////////////////////////////////////////////////////////////////////
0: 	private static final String[] STOP_WORDS = {
/////////////////////////////////////////////////////////////////////////
0:     /*
0: 		Collection<String> tmp = new java.util.TreeSet<String>();
0:     for (Object entry : stopSet) {
0:       tmp.add(entry.toString());
0:     }
0:      */
/////////////////////////////////////////////////////////////////////////
1:   static class AlphaNumericMaxLengthFilter extends TokenFilter {
0:     private final TermAttribute termAtt;
0:     private final Matcher matcher;
0: 	  AlphaNumericMaxLengthFilter(TokenStream in) {
/////////////////////////////////////////////////////////////////////////
0: 	      int length = termAtt.termLength();
0: 	        char[] buf = termAtt.termBuffer();
0: 	          char ch = buf[c];
0: 	        String term = new String(output, 0, at);
author:Robin Anil
-------------------------------------------------------------------------------
commit:3beecc0
/////////////////////////////////////////////////////////////////////////
0:   private static final Version LUCENE_VERSION = Version.LUCENE_36;
0:   private static final Set<?> STOP_WORDS = new CharArraySet(LUCENE_VERSION, Arrays.asList(
/////////////////////////////////////////////////////////////////////////
0:   private final static Matcher matcher = alphaNumeric.matcher("");
0:     super(LUCENE_VERSION, STOP_WORDS);
0:     super(LUCENE_VERSION, stopSet);
0:     Tokenizer tokenizer = new StandardTokenizer(LUCENE_VERSION, reader);
0:     TokenStream result = new StandardFilter(LUCENE_VERSION, tokenizer);
0:     result = new LowerCaseFilter(LUCENE_VERSION, result);
0:     result = new StopFilter(LUCENE_VERSION, result, stopwords);
/////////////////////////////////////////////////////////////////////////
============================================================================