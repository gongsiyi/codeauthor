1:623be86: /**
1:623be86:  * Licensed to the Apache Software Foundation (ASF) under one
1:623be86:  * or more contributor license agreements. See the NOTICE file
1:623be86:  * distributed with this work for additional information
1:623be86:  * regarding copyright ownership. The ASF licenses this file
1:623be86:  * to you under the Apache License, Version 2.0 (the
1:623be86:  * "License"); you may not use this file except in compliance
1:623be86:  * with the License. You may obtain a copy of the License at
1:623be86:  *
1:623be86:  * http://www.apache.org/licenses/LICENSE-2.0
1:623be86:  *
1:623be86:  * Unless required by applicable law or agreed to in writing,
1:623be86:  * software distributed under the License is distributed on an
1:623be86:  * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
1:623be86:  * KIND, either express or implied. See the License for the
1:623be86:  * specific language governing permissions and limitations
1:623be86:  * under the License.
1:623be86:  */
2:623be86: 
1:623be86: package org.apache.mahout.utils.nlp.collocations.llr;
1:623be86: 
1:623be86: import java.io.IOException;
1:e0ec7c1: import java.io.Reader;
1:623be86: import java.io.StringReader;
1:623be86: import java.nio.ByteBuffer;
1:623be86: import java.nio.CharBuffer;
1:623be86: import java.nio.charset.CharsetEncoder;
1:b381afd: 
1:85f9ece: import org.apache.commons.io.Charsets;
1:623be86: import org.apache.hadoop.util.bloom.BloomFilter;
1:623be86: import org.apache.hadoop.util.bloom.Filter;
1:623be86: import org.apache.hadoop.util.bloom.Key;
1:623be86: import org.apache.hadoop.util.hash.Hash;
1:e0ec7c1: import org.apache.lucene.analysis.Analyzer;
1:623be86: import org.apache.lucene.analysis.TokenStream;
1:6a4942c: import org.apache.lucene.analysis.core.WhitespaceAnalyzer;
1:623be86: import org.apache.lucene.analysis.shingle.ShingleFilter;
1:b04eb0e: import org.apache.lucene.analysis.tokenattributes.CharTermAttribute;
1:b381afd: import org.apache.mahout.common.MahoutTestCase;
1:623be86: import org.junit.Test;
1:623be86: 
1:623be86: public final class BloomTokenFilterTest extends MahoutTestCase {
1:623be86:   
1:623be86:   private static final CharsetEncoder encoder = Charsets.UTF_8.newEncoder();
1:623be86: 
1:623be86:   private static final String input = "The best of times the worst of times";
1:623be86:   private static final String[] allTokens = {
1:623be86:       "The", "best", "of", "times", "the", "worst", "of", "times"
1:623be86:   };
1:623be86:   private static final String[] expectedNonKeepTokens = { "best", "times", "the", "worst", "times" };
1:623be86:   private static final String[] expectedKeepTokens = { "The", "of", "of" };
1:623be86:   private static final String[] filterTokens    = { "The", "of" };
1:623be86:   private static final String[] notFilterTokens = { "best", "worst", "the", "times"};
1:623be86:   private static final String[] shingleKeepTokens = {
1:623be86:       "The best", "best of times", "the worst", "worst of times", "of times"
1:623be86:   };
1:623be86:   private static final String[] expectedShingleTokens = {
1:623be86:       "The best", "best of times", "of times", "the worst", "worst of times", "of times"
1:623be86:   };
1:623be86:   
1:623be86:   /** test standalone filter without tokenfilter wrapping */
1:623be86:   @Test
1:623be86:   public void testFilter() throws IOException {
1:623be86:     Filter filter = getFilter(filterTokens);
1:623be86:     Key k = new Key();
1:623be86:     for (String s: filterTokens) {
1:623be86:       setKey(k,s);
1:623be86:       assertTrue("Key for string " + s + " should be filter member", filter.membershipTest(k));
1:623be86:     }
1:623be86:     
1:623be86:     for (String s: notFilterTokens)  {
1:623be86:       setKey(k,s);
1:623be86:       assertFalse("Key for string " + s + " should not be filter member", filter.membershipTest(k));
1:623be86:     }
1:623be86:   }
1:623be86:   
1:623be86:   /** normal case, unfiltered analyzer */
1:623be86:   @Test
1:623be86:   public void testAnalyzer() throws IOException {
1:e0ec7c1:     Reader reader = new StringReader(input);
1:4d0cd66:     Analyzer analyzer = new WhitespaceAnalyzer();
1:623be86:     TokenStream ts = analyzer.tokenStream(null, reader);
1:6a4942c:     ts.reset();
1:623be86:     validateTokens(allTokens, ts);
1:6a4942c:     ts.end();
1:6a4942c:     ts.close();
1:623be86:   }
1:623be86:   
1:623be86:   /** filtered analyzer */
1:623be86:   @Test
1:623be86:   public void testNonKeepdAnalyzer() throws IOException {
1:e0ec7c1:     Reader reader = new StringReader(input);
1:4d0cd66:     Analyzer analyzer = new WhitespaceAnalyzer();
1:623be86:     TokenStream ts = analyzer.tokenStream(null, reader);
1:6a4942c:     ts.reset();
1:e0ec7c1:     TokenStream f = new BloomTokenFilter(getFilter(filterTokens), false /* toss matching tokens */, ts);
1:623be86:     validateTokens(expectedNonKeepTokens, f);
1:6a4942c:     ts.end();
1:6a4942c:     ts.close();
1:623be86:   }
1:623be86: 
1:623be86:   /** keep analyzer */
1:623be86:   @Test
1:623be86:   public void testKeepAnalyzer() throws IOException {
1:e0ec7c1:     Reader reader = new StringReader(input);
1:4d0cd66:     Analyzer analyzer = new WhitespaceAnalyzer();
1:623be86:     TokenStream ts = analyzer.tokenStream(null, reader);
1:6a4942c:     ts.reset();
1:e0ec7c1:     TokenStream f = new BloomTokenFilter(getFilter(filterTokens), true /* keep matching tokens */, ts);
1:623be86:     validateTokens(expectedKeepTokens, f);
1:6a4942c:     ts.end();
1:6a4942c:     ts.close();
1:623be86:   }
1:623be86:   
1:623be86:   /** shingles, keep those matching whitelist */
1:623be86:   @Test
1:623be86:   public void testShingleFilteredAnalyzer() throws IOException {
1:e0ec7c1:     Reader reader = new StringReader(input);
1:4d0cd66:     Analyzer analyzer = new WhitespaceAnalyzer();
1:623be86:     TokenStream ts = analyzer.tokenStream(null, reader);
1:6a4942c:     ts.reset();
1:623be86:     ShingleFilter sf = new ShingleFilter(ts, 3);
1:e0ec7c1:     TokenStream f = new BloomTokenFilter(getFilter(shingleKeepTokens),  true, sf);
1:623be86:     validateTokens(expectedShingleTokens, f);
1:6a4942c:     ts.end();
1:6a4942c:     ts.close();
1:623be86:   }
1:623be86:   
1:623be86:   private static void setKey(Key k, String s) throws IOException {
1:623be86:     ByteBuffer buffer = encoder.encode(CharBuffer.wrap(s.toCharArray()));
1:623be86:     k.set(buffer.array(), 1.0);
1:623be86:   }
1:623be86:   
1:623be86:   private static void validateTokens(String[] expected, TokenStream ts) throws IOException {
1:623be86:     int pos = 0;
1:623be86:     while (ts.incrementToken()) {
1:623be86:       assertTrue("Analyzer produced too many tokens", pos <= expected.length);
1:b04eb0e:       CharTermAttribute termAttr = ts.getAttribute(CharTermAttribute.class);
1:b04eb0e:       assertEquals("Unexpected term", expected[pos++], termAttr.toString());
1:623be86:     }
1:623be86:     assertEquals("Analyzer produced too few terms", expected.length, pos);
1:623be86:   }
1:623be86: 
1:623be86:   private static Filter getFilter(String[] tokens) throws IOException {
1:623be86:     Filter filter = new BloomFilter(100,50, Hash.JENKINS_HASH);
1:623be86:     Key k = new Key();
1:623be86:     for (String s: tokens) {
1:623be86:       setKey(k,s);
1:623be86:       filter.add(k);
1:623be86:     }
1:623be86:     return filter;
1:623be86:   }
1:623be86:   
1:623be86: }
============================================================================
author:smarthi
-------------------------------------------------------------------------------
commit:4d0cd66
/////////////////////////////////////////////////////////////////////////
/////////////////////////////////////////////////////////////////////////
1:     Analyzer analyzer = new WhitespaceAnalyzer();
/////////////////////////////////////////////////////////////////////////
1:     Analyzer analyzer = new WhitespaceAnalyzer();
/////////////////////////////////////////////////////////////////////////
1:     Analyzer analyzer = new WhitespaceAnalyzer();
/////////////////////////////////////////////////////////////////////////
1:     Analyzer analyzer = new WhitespaceAnalyzer();
commit:c88c240
/////////////////////////////////////////////////////////////////////////
0:     Analyzer analyzer = new WhitespaceAnalyzer(Version.LUCENE_45);
/////////////////////////////////////////////////////////////////////////
0:     Analyzer analyzer = new WhitespaceAnalyzer(Version.LUCENE_45);
/////////////////////////////////////////////////////////////////////////
0:     Analyzer analyzer = new WhitespaceAnalyzer(Version.LUCENE_45);
/////////////////////////////////////////////////////////////////////////
0:     Analyzer analyzer = new WhitespaceAnalyzer(Version.LUCENE_45);
commit:c36dc71
/////////////////////////////////////////////////////////////////////////
0:     Analyzer analyzer = new WhitespaceAnalyzer(Version.LUCENE_42);
/////////////////////////////////////////////////////////////////////////
0:     Analyzer analyzer = new WhitespaceAnalyzer(Version.LUCENE_42);
/////////////////////////////////////////////////////////////////////////
0:     Analyzer analyzer = new WhitespaceAnalyzer(Version.LUCENE_42);
/////////////////////////////////////////////////////////////////////////
0:     Analyzer analyzer = new WhitespaceAnalyzer(Version.LUCENE_42);
author:Suneel Marthi
-------------------------------------------------------------------------------
commit:85f9ece
/////////////////////////////////////////////////////////////////////////
1: import org.apache.commons.io.Charsets;
author:frankscholten
-------------------------------------------------------------------------------
commit:2e5449f
/////////////////////////////////////////////////////////////////////////
0:     Analyzer analyzer = new WhitespaceAnalyzer(Version.LUCENE_46);
/////////////////////////////////////////////////////////////////////////
0:     Analyzer analyzer = new WhitespaceAnalyzer(Version.LUCENE_46);
/////////////////////////////////////////////////////////////////////////
0:     Analyzer analyzer = new WhitespaceAnalyzer(Version.LUCENE_46);
/////////////////////////////////////////////////////////////////////////
0:     Analyzer analyzer = new WhitespaceAnalyzer(Version.LUCENE_46);
author:sslavic
-------------------------------------------------------------------------------
commit:b381afd
/////////////////////////////////////////////////////////////////////////
1: 
/////////////////////////////////////////////////////////////////////////
1: import org.apache.mahout.common.MahoutTestCase;
author:Grant Ingersoll
-------------------------------------------------------------------------------
commit:e48eb4c
/////////////////////////////////////////////////////////////////////////
0:     Analyzer analyzer = new WhitespaceAnalyzer(Version.LUCENE_43);
/////////////////////////////////////////////////////////////////////////
0:     Analyzer analyzer = new WhitespaceAnalyzer(Version.LUCENE_43);
/////////////////////////////////////////////////////////////////////////
0:     Analyzer analyzer = new WhitespaceAnalyzer(Version.LUCENE_43);
/////////////////////////////////////////////////////////////////////////
0:     Analyzer analyzer = new WhitespaceAnalyzer(Version.LUCENE_43);
commit:6a4942c
/////////////////////////////////////////////////////////////////////////
1: import org.apache.lucene.analysis.core.WhitespaceAnalyzer;
/////////////////////////////////////////////////////////////////////////
0:     Analyzer analyzer = new WhitespaceAnalyzer(Version.LUCENE_41);
1:     ts.reset();
1:     ts.end();
1:     ts.close();
0:     Analyzer analyzer = new WhitespaceAnalyzer(Version.LUCENE_41);
1:     ts.reset();
1:     ts.end();
1:     ts.close();
0:     Analyzer analyzer = new WhitespaceAnalyzer(Version.LUCENE_41);
1:     ts.reset();
1:     ts.end();
1:     ts.close();
0:     Analyzer analyzer = new WhitespaceAnalyzer(Version.LUCENE_41);
1:     ts.reset();
1:     ts.end();
1:     ts.close();
author:Sean R. Owen
-------------------------------------------------------------------------------
commit:e0ec7c1
/////////////////////////////////////////////////////////////////////////
1: import java.io.Reader;
/////////////////////////////////////////////////////////////////////////
1: import org.apache.lucene.analysis.Analyzer;
/////////////////////////////////////////////////////////////////////////
1:     Reader reader = new StringReader(input);
0:     Analyzer analyzer = new WhitespaceAnalyzer(Version.LUCENE_31);
/////////////////////////////////////////////////////////////////////////
1:     Reader reader = new StringReader(input);
0:     Analyzer analyzer = new WhitespaceAnalyzer(Version.LUCENE_31);
1:     TokenStream f = new BloomTokenFilter(getFilter(filterTokens), false /* toss matching tokens */, ts);
1:     Reader reader = new StringReader(input);
0:     Analyzer analyzer = new WhitespaceAnalyzer(Version.LUCENE_31);
1:     TokenStream f = new BloomTokenFilter(getFilter(filterTokens), true /* keep matching tokens */, ts);
1:     Reader reader = new StringReader(input);
0:     Analyzer analyzer = new WhitespaceAnalyzer(Version.LUCENE_31);
1:     TokenStream f = new BloomTokenFilter(getFilter(shingleKeepTokens),  true, sf);
commit:b04eb0e
/////////////////////////////////////////////////////////////////////////
1: import org.apache.lucene.analysis.tokenattributes.CharTermAttribute;
/////////////////////////////////////////////////////////////////////////
1:       CharTermAttribute termAttr = ts.getAttribute(CharTermAttribute.class);
1:       assertEquals("Unexpected term", expected[pos++], termAttr.toString());
commit:4ef41c7
/////////////////////////////////////////////////////////////////////////
0: import org.apache.lucene.util.Version;
/////////////////////////////////////////////////////////////////////////
0:     WhitespaceAnalyzer analyzer = new WhitespaceAnalyzer(Version.LUCENE_31);
/////////////////////////////////////////////////////////////////////////
0:     WhitespaceAnalyzer analyzer = new WhitespaceAnalyzer(Version.LUCENE_31);
/////////////////////////////////////////////////////////////////////////
0:     WhitespaceAnalyzer analyzer = new WhitespaceAnalyzer(Version.LUCENE_31);
/////////////////////////////////////////////////////////////////////////
0:     WhitespaceAnalyzer analyzer = new WhitespaceAnalyzer(Version.LUCENE_31);
commit:623be86
/////////////////////////////////////////////////////////////////////////
1: /**
1:  * Licensed to the Apache Software Foundation (ASF) under one
1:  * or more contributor license agreements. See the NOTICE file
1:  * distributed with this work for additional information
1:  * regarding copyright ownership. The ASF licenses this file
1:  * to you under the Apache License, Version 2.0 (the
1:  * "License"); you may not use this file except in compliance
1:  * with the License. You may obtain a copy of the License at
1:  *
1:  * http://www.apache.org/licenses/LICENSE-2.0
1:  *
1:  * Unless required by applicable law or agreed to in writing,
1:  * software distributed under the License is distributed on an
1:  * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
1:  * KIND, either express or implied. See the License for the
1:  * specific language governing permissions and limitations
1:  * under the License.
1:  */
1: 
1: package org.apache.mahout.utils.nlp.collocations.llr;
1: 
1: import java.io.IOException;
1: import java.io.StringReader;
1: import java.nio.ByteBuffer;
1: import java.nio.CharBuffer;
1: import java.nio.charset.CharsetEncoder;
1: 
0: import com.google.common.base.Charsets;
1: import org.apache.hadoop.util.bloom.BloomFilter;
1: import org.apache.hadoop.util.bloom.Filter;
1: import org.apache.hadoop.util.bloom.Key;
1: import org.apache.hadoop.util.hash.Hash;
1: import org.apache.lucene.analysis.TokenStream;
0: import org.apache.lucene.analysis.WhitespaceAnalyzer;
1: import org.apache.lucene.analysis.shingle.ShingleFilter;
0: import org.apache.lucene.analysis.tokenattributes.TermAttribute;
0: import org.apache.mahout.utils.MahoutTestCase;
1: import org.junit.Test;
1: 
1: public final class BloomTokenFilterTest extends MahoutTestCase {
1:   
1:   private static final CharsetEncoder encoder = Charsets.UTF_8.newEncoder();
1: 
1:   private static final String input = "The best of times the worst of times";
1:   private static final String[] allTokens = {
1:       "The", "best", "of", "times", "the", "worst", "of", "times"
1:   };
1:   private static final String[] expectedNonKeepTokens = { "best", "times", "the", "worst", "times" };
1:   private static final String[] expectedKeepTokens = { "The", "of", "of" };
1:   private static final String[] filterTokens    = { "The", "of" };
1:   private static final String[] notFilterTokens = { "best", "worst", "the", "times"};
1:   private static final String[] shingleKeepTokens = {
1:       "The best", "best of times", "the worst", "worst of times", "of times"
1:   };
1:   private static final String[] expectedShingleTokens = {
1:       "The best", "best of times", "of times", "the worst", "worst of times", "of times"
1:   };
1:   
1:   /** test standalone filter without tokenfilter wrapping */
1:   @Test
1:   public void testFilter() throws IOException {
1:     Filter filter = getFilter(filterTokens);
1:     Key k = new Key();
1:     for (String s: filterTokens) {
1:       setKey(k,s);
1:       assertTrue("Key for string " + s + " should be filter member", filter.membershipTest(k));
1:     }
1:     
1:     for (String s: notFilterTokens)  {
1:       setKey(k,s);
1:       assertFalse("Key for string " + s + " should not be filter member", filter.membershipTest(k));
1:     }
1:   }
1:   
1:   /** normal case, unfiltered analyzer */
1:   @Test
1:   public void testAnalyzer() throws IOException {
0:     StringReader reader = new StringReader(input);
0:     WhitespaceAnalyzer analyzer = new WhitespaceAnalyzer();
1:     TokenStream ts = analyzer.tokenStream(null, reader);
1:     validateTokens(allTokens, ts);
1:   }
1:   
1:   /** filtered analyzer */
1:   @Test
1:   public void testNonKeepdAnalyzer() throws IOException {
0:     StringReader reader = new StringReader(input);
0:     WhitespaceAnalyzer analyzer = new WhitespaceAnalyzer();
1:     TokenStream ts = analyzer.tokenStream(null, reader);
0:     BloomTokenFilter f = new BloomTokenFilter(getFilter(filterTokens), false /* toss matching tokens */, ts);
1:     validateTokens(expectedNonKeepTokens, f);
1:   }
1: 
1:   /** keep analyzer */
1:   @Test
1:   public void testKeepAnalyzer() throws IOException {
0:     StringReader reader = new StringReader(input);
0:     WhitespaceAnalyzer analyzer = new WhitespaceAnalyzer();
1:     TokenStream ts = analyzer.tokenStream(null, reader);
0:     BloomTokenFilter f = new BloomTokenFilter(getFilter(filterTokens), true /* keep matching tokens */, ts);
1:     validateTokens(expectedKeepTokens, f);
1:   }
1:   
1:   /** shingles, keep those matching whitelist */
1:   @Test
1:   public void testShingleFilteredAnalyzer() throws IOException {
0:     StringReader reader = new StringReader(input);
0:     WhitespaceAnalyzer analyzer = new WhitespaceAnalyzer();
1:     TokenStream ts = analyzer.tokenStream(null, reader);
1:     ShingleFilter sf = new ShingleFilter(ts, 3);
0:     BloomTokenFilter f = new BloomTokenFilter(getFilter(shingleKeepTokens),  true, sf);
1:     validateTokens(expectedShingleTokens, f);
1:   }
1:   
1:   private static void setKey(Key k, String s) throws IOException {
1:     ByteBuffer buffer = encoder.encode(CharBuffer.wrap(s.toCharArray()));
1:     k.set(buffer.array(), 1.0);
1:   }
1:   
1:   private static void validateTokens(String[] expected, TokenStream ts) throws IOException {
1:     int pos = 0;
1:     while (ts.incrementToken()) {
1:       assertTrue("Analyzer produced too many tokens", pos <= expected.length);
0:       TermAttribute termAttr = ts.getAttribute(TermAttribute.class);
0:       assertEquals("Unexpected term", expected[pos++], termAttr.term());
1:     }
1:     assertEquals("Analyzer produced too few terms", expected.length, pos);
1:   }
1: 
1:   private static Filter getFilter(String[] tokens) throws IOException {
1:     Filter filter = new BloomFilter(100,50, Hash.JENKINS_HASH);
1:     Key k = new Key();
1:     for (String s: tokens) {
1:       setKey(k,s);
1:       filter.add(k);
1:     }
1:     return filter;
1:   }
1:   
1: }
============================================================================