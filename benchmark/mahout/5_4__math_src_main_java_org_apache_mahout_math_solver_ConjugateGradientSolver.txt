1:e0ec7c1: /**
1:e0ec7c1:  * Licensed to the Apache Software Foundation (ASF) under one or more
1:e0ec7c1:  * contributor license agreements.  See the NOTICE file distributed with
1:e0ec7c1:  * this work for additional information regarding copyright ownership.
1:e0ec7c1:  * The ASF licenses this file to You under the Apache License, Version 2.0
1:e0ec7c1:  * (the "License"); you may not use this file except in compliance with
1:e0ec7c1:  * the License.  You may obtain a copy of the License at
1:e0ec7c1:  *
1:e0ec7c1:  *     http://www.apache.org/licenses/LICENSE-2.0
1:e0ec7c1:  *
1:e0ec7c1:  * Unless required by applicable law or agreed to in writing, software
1:e0ec7c1:  * distributed under the License is distributed on an "AS IS" BASIS,
1:e0ec7c1:  * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
1:e0ec7c1:  * See the License for the specific language governing permissions and
1:e0ec7c1:  * limitations under the License.
1:e0ec7c1:  */
1:e0ec7c1: 
1:5fc5b65: package org.apache.mahout.math.solver;
3:5fc5b65: 
1:5fc5b65: import org.apache.mahout.math.CardinalityException;
1:5fc5b65: import org.apache.mahout.math.DenseVector;
1:5fc5b65: import org.apache.mahout.math.Vector;
1:5fc5b65: import org.apache.mahout.math.VectorIterable;
1:5fc5b65: import org.apache.mahout.math.function.Functions;
1:5fc5b65: import org.apache.mahout.math.function.PlusMult;
1:5fc5b65: import org.slf4j.Logger;
1:5fc5b65: import org.slf4j.LoggerFactory;
1:5fc5b65: 
1:5fc5b65: /**
1:5fc5b65:  * <p>Implementation of a conjugate gradient iterative solver for linear systems. Implements both
1:5fc5b65:  * standard conjugate gradient and pre-conditioned conjugate gradient. 
1:5fc5b65:  * 
1:5fc5b65:  * <p>Conjugate gradient requires the matrix A in the linear system Ax = b to be symmetric and positive
1:99fd9ba:  * definite. For convenience, this implementation could be extended relatively easily to handle the
1:99fd9ba:  * case where the input matrix to be be non-symmetric, in which case the system A'Ax = b would be solved.
1:99fd9ba:  * Because this requires only one pass through the matrix A, it is faster than explicitly computing A'A,
1:99fd9ba:  * then passing the results to the solver.
1:5fc5b65:  * 
1:5fc5b65:  * <p>For inputs that may be ill conditioned (often the case for highly sparse input), this solver
1:5fc5b65:  * also accepts a parameter, lambda, which adds a scaled identity to the matrix A, solving the system
1:5fc5b65:  * (A + lambda*I)x = b. This obviously changes the solution, but it will guarantee solvability. The
1:5fc5b65:  * ridge regression approach to linear regression is a common use of this feature.
1:5fc5b65:  * 
1:5fc5b65:  * <p>If only an approximate solution is required, the maximum number of iterations or the error threshold
1:5fc5b65:  * may be specified to end the algorithm early at the expense of accuracy. When the matrix A is ill conditioned,
1:5fc5b65:  * it may sometimes be necessary to increase the maximum number of iterations above the default of A.numCols()
1:5fc5b65:  * due to numerical issues.
1:5fc5b65:  * 
1:5fc5b65:  * <p>By default the solver will run a.numCols() iterations or until the residual falls below 1E-9.
1:5fc5b65:  * 
1:5fc5b65:  * <p>For more information on the conjugate gradient algorithm, see Golub & van Loan, "Matrix Computations", 
1:5fc5b65:  * sections 10.2 and 10.3 or the <a href="http://en.wikipedia.org/wiki/Conjugate_gradient">conjugate gradient
1:5fc5b65:  * wikipedia article</a>.
1:5fc5b65:  */
1:5fc5b65: 
1:6d16230: public class ConjugateGradientSolver {
1:6d16230: 
1:e0ec7c1:   public static final double DEFAULT_MAX_ERROR = 1.0e-9;
1:5fc5b65:   
1:5fc5b65:   private static final Logger log = LoggerFactory.getLogger(ConjugateGradientSolver.class);
1:6d16230:   private static final PlusMult PLUS_MULT = new PlusMult(1.0);
1:5fc5b65: 
1:5fc5b65:   private int iterations;
1:5fc5b65:   private double residualNormSquared;
1:5fc5b65:   
1:5fc5b65:   public ConjugateGradientSolver() {
1:5fc5b65:     this.iterations = 0;
1:5fc5b65:     this.residualNormSquared = Double.NaN;
1:5fc5b65:   }  
1:5fc5b65: 
1:5fc5b65:   /**
1:5fc5b65:    * Solves the system Ax = b with default termination criteria. A must be symmetric, square, and positive definite.
1:5fc5b65:    * Only the squareness of a is checked, since testing for symmetry and positive definiteness are too expensive. If
1:5fc5b65:    * an invalid matrix is specified, then the algorithm may not yield a valid result.
1:5fc5b65:    *  
1:5fc5b65:    * @param a  The linear operator A.
1:5fc5b65:    * @param b  The vector b.
1:5fc5b65:    * @return The result x of solving the system.
1:5fc5b65:    * @throws IllegalArgumentException if a is not square or if the size of b is not equal to the number of columns of a.
1:5fc5b65:    * 
1:5fc5b65:    */
1:5fc5b65:   public Vector solve(VectorIterable a, Vector b) {
1:99fd9ba:     return solve(a, b, null, b.size() + 2, DEFAULT_MAX_ERROR);
1:5fc5b65:   }
1:5fc5b65:   
1:5fc5b65:   /**
1:5fc5b65:    * Solves the system Ax = b with default termination criteria using the specified preconditioner. A must be 
1:5fc5b65:    * symmetric, square, and positive definite. Only the squareness of a is checked, since testing for symmetry 
1:5fc5b65:    * and positive definiteness are too expensive. If an invalid matrix is specified, then the algorithm may not 
1:5fc5b65:    * yield a valid result.
1:5fc5b65:    *  
1:5fc5b65:    * @param a  The linear operator A.
1:5fc5b65:    * @param b  The vector b.
1:5fc5b65:    * @param precond A preconditioner to use on A during the solution process.
1:5fc5b65:    * @return The result x of solving the system.
1:5fc5b65:    * @throws IllegalArgumentException if a is not square or if the size of b is not equal to the number of columns of a.
1:5fc5b65:    * 
1:5fc5b65:    */
1:5fc5b65:   public Vector solve(VectorIterable a, Vector b, Preconditioner precond) {
1:99fd9ba:     return solve(a, b, precond, b.size() + 2, DEFAULT_MAX_ERROR);
1:5fc5b65:   }
1:5fc5b65:   
1:5fc5b65: 
1:5fc5b65:   /**
1:5fc5b65:    * Solves the system Ax = b, where A is a linear operator and b is a vector. Uses the specified preconditioner
1:5fc5b65:    * to improve numeric stability and possibly speed convergence. This version of solve() allows control over the 
1:5fc5b65:    * termination and iteration parameters.
1:5fc5b65:    * 
1:5fc5b65:    * @param a  The matrix A.
1:5fc5b65:    * @param b  The vector b.
1:5fc5b65:    * @param preconditioner The preconditioner to apply.
1:5fc5b65:    * @param maxIterations The maximum number of iterations to run.
1:5fc5b65:    * @param maxError The maximum amount of residual error to tolerate. The algorithm will run until the residual falls 
1:5fc5b65:    * below this value or until maxIterations are completed.
1:5fc5b65:    * @return The result x of solving the system.
1:5fc5b65:    * @throws IllegalArgumentException if the matrix is not square, if the size of b is not equal to the number of 
1:5fc5b65:    * columns of A, if maxError is less than zero, or if maxIterations is not positive. 
1:5fc5b65:    */
1:5fc5b65:   
1:5fc5b65:   public Vector solve(VectorIterable a, 
1:5fc5b65:                       Vector b, 
1:5fc5b65:                       Preconditioner preconditioner, 
1:5fc5b65:                       int maxIterations, 
1:5fc5b65:                       double maxError) {
1:5fc5b65: 
1:5fc5b65:     if (a.numRows() != a.numCols()) {
1:5fc5b65:       throw new IllegalArgumentException("Matrix must be square, symmetric and positive definite.");
1:5fc5b65:     }
1:5fc5b65:     
1:5fc5b65:     if (a.numCols() != b.size()) {
1:5fc5b65:       throw new CardinalityException(a.numCols(), b.size());
1:5fc5b65:     }
1:5fc5b65: 
1:5fc5b65:     if (maxIterations <= 0) {
1:5fc5b65:       throw new IllegalArgumentException("Max iterations must be positive.");      
1:5fc5b65:     }
1:5fc5b65:     
1:5fc5b65:     if (maxError < 0.0) {
1:5fc5b65:       throw new IllegalArgumentException("Max error must be non-negative.");
1:5fc5b65:     }
1:5fc5b65:     
1:5fc5b65:     Vector x = new DenseVector(b.size());
1:5fc5b65: 
1:5fc5b65:     iterations = 0;
1:5fc5b65:     Vector residual = b.minus(a.times(x));
1:5fc5b65:     residualNormSquared = residual.dot(residual);
1:5fc5b65: 
1:10c535c:     log.info("Conjugate gradient initial residual norm = {}", Math.sqrt(residualNormSquared));
1:e0ec7c1:     double previousConditionedNormSqr = 0.0;
1:e0ec7c1:     Vector updateDirection = null;
1:5fc5b65:     while (Math.sqrt(residualNormSquared) > maxError && iterations < maxIterations) {
1:5fc5b65:       Vector conditionedResidual;
1:e0ec7c1:       double conditionedNormSqr;
1:5fc5b65:       if (preconditioner == null) {
1:5fc5b65:         conditionedResidual = residual;
1:5fc5b65:         conditionedNormSqr = residualNormSquared;
1:5fc5b65:       } else {
1:5fc5b65:         conditionedResidual = preconditioner.precondition(residual);
1:5fc5b65:         conditionedNormSqr = residual.dot(conditionedResidual);
1:5fc5b65:       }      
1:5fc5b65:       
1:5fc5b65:       ++iterations;
1:5fc5b65:       
1:5fc5b65:       if (iterations == 1) {
1:5fc5b65:         updateDirection = new DenseVector(conditionedResidual);
1:5fc5b65:       } else {
1:5fc5b65:         double beta = conditionedNormSqr / previousConditionedNormSqr;
1:5fc5b65:         
1:5fc5b65:         // updateDirection = residual + beta * updateDirection
1:5fc5b65:         updateDirection.assign(Functions.MULT, beta);
1:5fc5b65:         updateDirection.assign(conditionedResidual, Functions.PLUS);
1:5fc5b65:       }
1:5fc5b65:       
1:5fc5b65:       Vector aTimesUpdate = a.times(updateDirection);
1:5fc5b65:       
1:5fc5b65:       double alpha = conditionedNormSqr / updateDirection.dot(aTimesUpdate);
1:5fc5b65:       
1:5fc5b65:       // x = x + alpha * updateDirection
1:6d16230:       PLUS_MULT.setMultiplicator(alpha);
1:6d16230:       x.assign(updateDirection, PLUS_MULT);
1:5fc5b65: 
1:5fc5b65:       // residual = residual - alpha * A * updateDirection
1:6d16230:       PLUS_MULT.setMultiplicator(-alpha);
1:6d16230:       residual.assign(aTimesUpdate, PLUS_MULT);
1:5fc5b65:       
1:5fc5b65:       previousConditionedNormSqr = conditionedNormSqr;
1:5fc5b65:       residualNormSquared = residual.dot(residual);
1:5fc5b65:       
1:10c535c:       log.info("Conjugate gradient iteration {} residual norm = {}", iterations, Math.sqrt(residualNormSquared));
1:5fc5b65:     }
1:5fc5b65:     return x;
1:5fc5b65:   }
1:5fc5b65: 
1:5fc5b65:   /**
1:5fc5b65:    * Returns the number of iterations run once the solver is complete.
1:5fc5b65:    * 
1:5fc5b65:    * @return The number of iterations run.
1:5fc5b65:    */
1:5fc5b65:   public int getIterations() {
1:5fc5b65:     return iterations;
1:5fc5b65:   }
1:5fc5b65: 
1:5fc5b65:   /**
1:5fc5b65:    * Returns the norm of the residual at the completion of the solver. Usually this should be close to zero except in
1:5fc5b65:    * the case of a non positive definite matrix A, which results in an unsolvable system, or for ill conditioned A, in
1:5fc5b65:    * which case more iterations than the default may be needed.
1:5fc5b65:    * 
1:5fc5b65:    * @return The norm of the residual in the solution.
1:5fc5b65:    */
1:5fc5b65:   public double getResidualNorm() {
1:5fc5b65:     return Math.sqrt(residualNormSquared);
1:5fc5b65:   }  
1:5fc5b65: }
============================================================================
author:Ted Dunning
-------------------------------------------------------------------------------
commit:99fd9ba
/////////////////////////////////////////////////////////////////////////
1:  * definite. For convenience, this implementation could be extended relatively easily to handle the
1:  * case where the input matrix to be be non-symmetric, in which case the system A'Ax = b would be solved.
1:  * Because this requires only one pass through the matrix A, it is faster than explicitly computing A'A,
1:  * then passing the results to the solver.
/////////////////////////////////////////////////////////////////////////
1:     return solve(a, b, null, b.size() + 2, DEFAULT_MAX_ERROR);
/////////////////////////////////////////////////////////////////////////
1:     return solve(a, b, precond, b.size() + 2, DEFAULT_MAX_ERROR);
author:sslavic
-------------------------------------------------------------------------------
commit:d45c030
/////////////////////////////////////////////////////////////////////////
0:  * is faster than explicitly computing A'A, then passing the results to the solver.
author:Sebastian Schelter
-------------------------------------------------------------------------------
commit:6d16230
/////////////////////////////////////////////////////////////////////////
1: public class ConjugateGradientSolver {
1: 
1:   private static final PlusMult PLUS_MULT = new PlusMult(1.0);
/////////////////////////////////////////////////////////////////////////
1:       PLUS_MULT.setMultiplicator(alpha);
1:       x.assign(updateDirection, PLUS_MULT);
1:       PLUS_MULT.setMultiplicator(-alpha);
1:       residual.assign(aTimesUpdate, PLUS_MULT);
author:Sean R. Owen
-------------------------------------------------------------------------------
commit:10c535c
/////////////////////////////////////////////////////////////////////////
1:     log.info("Conjugate gradient initial residual norm = {}", Math.sqrt(residualNormSquared));
/////////////////////////////////////////////////////////////////////////
1:       log.info("Conjugate gradient iteration {} residual norm = {}", iterations, Math.sqrt(residualNormSquared));
commit:e0ec7c1
/////////////////////////////////////////////////////////////////////////
1: /**
1:  * Licensed to the Apache Software Foundation (ASF) under one or more
1:  * contributor license agreements.  See the NOTICE file distributed with
1:  * this work for additional information regarding copyright ownership.
1:  * The ASF licenses this file to You under the Apache License, Version 2.0
1:  * (the "License"); you may not use this file except in compliance with
1:  * the License.  You may obtain a copy of the License at
1:  *
1:  *     http://www.apache.org/licenses/LICENSE-2.0
1:  *
1:  * Unless required by applicable law or agreed to in writing, software
1:  * distributed under the License is distributed on an "AS IS" BASIS,
1:  * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
1:  * See the License for the specific language governing permissions and
1:  * limitations under the License.
1:  */
1: 
/////////////////////////////////////////////////////////////////////////
1:   public static final double DEFAULT_MAX_ERROR = 1.0e-9;
/////////////////////////////////////////////////////////////////////////
1:     double previousConditionedNormSqr = 0.0;
1:     Vector updateDirection = null;
1:       double conditionedNormSqr;
author:Jacob Alexander Mannix
-------------------------------------------------------------------------------
commit:5fc5b65
/////////////////////////////////////////////////////////////////////////
1: package org.apache.mahout.math.solver;
1: 
1: import org.apache.mahout.math.CardinalityException;
1: import org.apache.mahout.math.DenseVector;
0: import org.apache.mahout.math.Matrix;
1: import org.apache.mahout.math.Vector;
1: import org.apache.mahout.math.VectorIterable;
1: import org.apache.mahout.math.function.Functions;
1: import org.apache.mahout.math.function.PlusMult;
0: import org.apache.mahout.math.function.TimesFunction;
1: import org.slf4j.Logger;
1: import org.slf4j.LoggerFactory;
1: 
1: /**
1:  * <p>Implementation of a conjugate gradient iterative solver for linear systems. Implements both
1:  * standard conjugate gradient and pre-conditioned conjugate gradient. 
1:  * 
1:  * <p>Conjugate gradient requires the matrix A in the linear system Ax = b to be symmetric and positive
0:  * definite. For convenience, this implementation allows the input matrix to be be non-symmetric, in
0:  * which case the system A'Ax = b is solved. Because this requires only one pass through the matrix A, it
0:  * is faster than explictly computing A'A, then passing the results to the solver.
1:  * 
1:  * <p>For inputs that may be ill conditioned (often the case for highly sparse input), this solver
1:  * also accepts a parameter, lambda, which adds a scaled identity to the matrix A, solving the system
1:  * (A + lambda*I)x = b. This obviously changes the solution, but it will guarantee solvability. The
1:  * ridge regression approach to linear regression is a common use of this feature.
1:  * 
1:  * <p>If only an approximate solution is required, the maximum number of iterations or the error threshold
1:  * may be specified to end the algorithm early at the expense of accuracy. When the matrix A is ill conditioned,
1:  * it may sometimes be necessary to increase the maximum number of iterations above the default of A.numCols()
1:  * due to numerical issues.
1:  * 
1:  * <p>By default the solver will run a.numCols() iterations or until the residual falls below 1E-9.
1:  * 
1:  * <p>For more information on the conjugate gradient algorithm, see Golub & van Loan, "Matrix Computations", 
1:  * sections 10.2 and 10.3 or the <a href="http://en.wikipedia.org/wiki/Conjugate_gradient">conjugate gradient
1:  * wikipedia article</a>.
1:  */
1: 
0: public class ConjugateGradientSolver
0: {
0:   public static final double DEFAULT_MAX_ERROR = 1e-9;
1:   
1:   private static final Logger log = LoggerFactory.getLogger(ConjugateGradientSolver.class);
0:   private static final PlusMult plusMult = new PlusMult(1.0);
1: 
1:   private int iterations;
1:   private double residualNormSquared;
1:   
1:   public ConjugateGradientSolver() {
1:     this.iterations = 0;
1:     this.residualNormSquared = Double.NaN;
1:   }  
1: 
1:   /**
1:    * Solves the system Ax = b with default termination criteria. A must be symmetric, square, and positive definite.
1:    * Only the squareness of a is checked, since testing for symmetry and positive definiteness are too expensive. If
1:    * an invalid matrix is specified, then the algorithm may not yield a valid result.
1:    *  
1:    * @param a  The linear operator A.
1:    * @param b  The vector b.
1:    * @return The result x of solving the system.
1:    * @throws IllegalArgumentException if a is not square or if the size of b is not equal to the number of columns of a.
1:    * 
1:    */
1:   public Vector solve(VectorIterable a, Vector b) {
0:     return solve(a, b, null, b.size(), DEFAULT_MAX_ERROR);
1:   }
1:   
1:   /**
1:    * Solves the system Ax = b with default termination criteria using the specified preconditioner. A must be 
1:    * symmetric, square, and positive definite. Only the squareness of a is checked, since testing for symmetry 
1:    * and positive definiteness are too expensive. If an invalid matrix is specified, then the algorithm may not 
1:    * yield a valid result.
1:    *  
1:    * @param a  The linear operator A.
1:    * @param b  The vector b.
1:    * @param precond A preconditioner to use on A during the solution process.
1:    * @return The result x of solving the system.
1:    * @throws IllegalArgumentException if a is not square or if the size of b is not equal to the number of columns of a.
1:    * 
1:    */
1:   public Vector solve(VectorIterable a, Vector b, Preconditioner precond) {
0:     return solve(a, b, precond, b.size(), DEFAULT_MAX_ERROR);
1:   }
1:   
1: 
1:   /**
1:    * Solves the system Ax = b, where A is a linear operator and b is a vector. Uses the specified preconditioner
1:    * to improve numeric stability and possibly speed convergence. This version of solve() allows control over the 
1:    * termination and iteration parameters.
1:    * 
1:    * @param a  The matrix A.
1:    * @param b  The vector b.
1:    * @param preconditioner The preconditioner to apply.
1:    * @param maxIterations The maximum number of iterations to run.
1:    * @param maxError The maximum amount of residual error to tolerate. The algorithm will run until the residual falls 
1:    * below this value or until maxIterations are completed.
1:    * @return The result x of solving the system.
1:    * @throws IllegalArgumentException if the matrix is not square, if the size of b is not equal to the number of 
1:    * columns of A, if maxError is less than zero, or if maxIterations is not positive. 
1:    */
1:   
1:   public Vector solve(VectorIterable a, 
1:                       Vector b, 
1:                       Preconditioner preconditioner, 
1:                       int maxIterations, 
1:                       double maxError) {
1: 
1:     if (a.numRows() != a.numCols()) {
1:       throw new IllegalArgumentException("Matrix must be square, symmetric and positive definite.");
1:     }
1:     
1:     if (a.numCols() != b.size()) {
1:       throw new CardinalityException(a.numCols(), b.size());
1:     }
1: 
1:     if (maxIterations <= 0) {
1:       throw new IllegalArgumentException("Max iterations must be positive.");      
1:     }
1:     
1:     if (maxError < 0.0) {
1:       throw new IllegalArgumentException("Max error must be non-negative.");
1:     }
1:     
1:     Vector x = new DenseVector(b.size());
1: 
1:     iterations = 0;
1:     Vector residual = b.minus(a.times(x));
1:     residualNormSquared = residual.dot(residual);
1: 
0:     double conditionedNormSqr;
0:     double previousConditionedNormSqr = 0.0;
1: 
0:     Vector updateDirection = null;
1:     
0:     log.info("Conjugate gradient initial residual norm = " + Math.sqrt(residualNormSquared));
1:     while (Math.sqrt(residualNormSquared) > maxError && iterations < maxIterations) {
1:       Vector conditionedResidual;
1:       if (preconditioner == null) {
1:         conditionedResidual = residual;
1:         conditionedNormSqr = residualNormSquared;
1:       } else {
1:         conditionedResidual = preconditioner.precondition(residual);
1:         conditionedNormSqr = residual.dot(conditionedResidual);
1:       }      
1:       
1:       ++iterations;
1:       
1:       if (iterations == 1) {
1:         updateDirection = new DenseVector(conditionedResidual);
1:       } else {
1:         double beta = conditionedNormSqr / previousConditionedNormSqr;
1:         
1:         // updateDirection = residual + beta * updateDirection
1:         updateDirection.assign(Functions.MULT, beta);
1:         updateDirection.assign(conditionedResidual, Functions.PLUS);
1:       }
1:       
1:       Vector aTimesUpdate = a.times(updateDirection);
1:       
1:       double alpha = conditionedNormSqr / updateDirection.dot(aTimesUpdate);
1:       
1:       // x = x + alpha * updateDirection
0:       plusMult.setMultiplicator(alpha);
0:       x.assign(updateDirection, plusMult);
1: 
1:       // residual = residual - alpha * A * updateDirection
0:       plusMult.setMultiplicator(-alpha);
0:       residual.assign(aTimesUpdate, plusMult);
1:       
1:       previousConditionedNormSqr = conditionedNormSqr;
1:       residualNormSquared = residual.dot(residual);
1:       
0:       log.info("Conjugate gradient iteration " + iterations + " residual norm = " + Math.sqrt(residualNormSquared));
1:     }
1:     return x;
1:   }
1: 
1:   /**
1:    * Returns the number of iterations run once the solver is complete.
1:    * 
1:    * @return The number of iterations run.
1:    */
1:   public int getIterations() {
1:     return iterations;
1:   }
1: 
1:   /**
1:    * Returns the norm of the residual at the completion of the solver. Usually this should be close to zero except in
1:    * the case of a non positive definite matrix A, which results in an unsolvable system, or for ill conditioned A, in
1:    * which case more iterations than the default may be needed.
1:    * 
1:    * @return The norm of the residual in the solution.
1:    */
1:   public double getResidualNorm() {
1:     return Math.sqrt(residualNormSquared);
1:   }  
1: }
============================================================================