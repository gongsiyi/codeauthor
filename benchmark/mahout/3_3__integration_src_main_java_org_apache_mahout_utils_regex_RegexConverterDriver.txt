1:4fbfbc6: /*
1:2e4d306:  * Licensed to the Apache Software Foundation (ASF) under one or more
1:2e4d306:  * contributor license agreements.  See the NOTICE file distributed with
1:2e4d306:  * this work for additional information regarding copyright ownership.
1:2e4d306:  * The ASF licenses this file to You under the Apache License, Version 2.0
1:2e4d306:  * (the "License"); you may not use this file except in compliance with
1:2e4d306:  * the License.  You may obtain a copy of the License at
1:2e4d306:  *
1:2e4d306:  *     http://www.apache.org/licenses/LICENSE-2.0
1:2e4d306:  *
1:2e4d306:  * Unless required by applicable law or agreed to in writing, software
1:2e4d306:  * distributed under the License is distributed on an "AS IS" BASIS,
1:2e4d306:  * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
1:2e4d306:  * See the License for the specific language governing permissions and
1:2e4d306:  * limitations under the License.
1:2e4d306:  */
3:2e4d306: 
1:4fbfbc6: package org.apache.mahout.utils.regex;
1:4fbfbc6: 
1:4fbfbc6: import org.apache.hadoop.conf.Configuration;
1:4fbfbc6: import org.apache.hadoop.fs.Path;
1:4fbfbc6: import org.apache.hadoop.io.LongWritable;
1:4fbfbc6: import org.apache.hadoop.io.Text;
1:4fbfbc6: import org.apache.hadoop.mapreduce.Job;
1:4fbfbc6: import org.apache.hadoop.mapreduce.lib.input.TextInputFormat;
1:4fbfbc6: import org.apache.hadoop.mapreduce.lib.output.TextOutputFormat;
1:4fbfbc6: import org.apache.hadoop.util.ToolRunner;
1:4fbfbc6: import org.apache.lucene.analysis.Analyzer;
1:4fbfbc6: import org.apache.mahout.common.AbstractJob;
1:4fbfbc6: import org.apache.mahout.common.HadoopUtil;
1:4fbfbc6: import org.apache.mahout.common.commandline.DefaultOptionCreator;
1:2e4d306: 
2:2e4d306: /**
1:2e4d306:  * Experimental
1:2e4d306:  */
1:2e4d306: public class RegexConverterDriver extends AbstractJob {
1:2e4d306: 
1:2e4d306:   @Override
1:2e4d306:   public int run(String[] args) throws Exception {
1:2e4d306:     addInputOption();
1:2e4d306:     addOutputOption();
1:2e4d306:     addOption(DefaultOptionCreator.overwriteOption().create());
1:2e4d306:     addOption("regex", "regex",
1:2e4d306:             "The regular expression to use", true);
1:2e4d306:     addOption("groupsToKeep", "g",
1:2e4d306:             "The number of the capturing groups to keep", false);
1:2e4d306:     addOption("transformerClass", "t",
1:2e4d306:             "The optional class specifying the Regex Transformer", false);
1:2e4d306:     addOption("formatterClass", "t",
1:2e4d306:             "The optional class specifying the Regex Formatter", false);
1:2e4d306:     addOption(DefaultOptionCreator.analyzerOption().create());
1:2e4d306: 
1:2e4d306:     if (parseArguments(args) == null) {
1:2e4d306:       return -1;
1:2e4d306:     }
1:2e4d306: 
1:2e4d306:     Configuration conf = getConf();
1:2e4d306:     //TODO: How to deal with command line escaping?
1:44459bd:     conf.set(RegexMapper.REGEX, getOption("regex")); //
1:2e4d306:     String gtk = getOption("groupsToKeep");
1:2e4d306:     if (gtk != null) {
1:2e4d306:       conf.set(RegexMapper.GROUP_MATCHERS, gtk);
1:2e4d306:     }
1:2e4d306:     String trans = getOption("transformerClass");
1:2e4d306:     if (trans != null) {
1:4fbfbc6:       if ("url".equalsIgnoreCase(trans)) {
1:2e4d306:         trans = URLDecodeTransformer.class.getName();
1:2e4d306:       }
1:2e4d306:       conf.set(RegexMapper.TRANSFORMER_CLASS, trans);
1:2e4d306:     }
1:2e4d306:     String formatter = getOption("formatterClass");
1:2e4d306:     if (formatter != null) {
1:4fbfbc6:       if ("fpg".equalsIgnoreCase(formatter)) {
1:2e4d306:         formatter = FPGFormatter.class.getName();
1:2e4d306:       }
1:2e4d306:       conf.set(RegexMapper.FORMATTER_CLASS, formatter);
1:2e4d306:     }
1:2e4d306:     Path input = getInputPath();
1:2e4d306:     Path output = getOutputPath();
1:2e4d306:     if (hasOption(DefaultOptionCreator.OVERWRITE_OPTION)) {
1:2e4d306:       HadoopUtil.delete(getConf(), output);
1:2e4d306:     }
1:2e4d306:     Class<? extends Analyzer> analyzerClass = getAnalyzerClassFromOption();
1:2e4d306:     if (analyzerClass != null) {
1:2e4d306:       conf.set(RegexMapper.ANALYZER_NAME, analyzerClass.getName());
1:2e4d306:     }
1:2e4d306:     Job job = prepareJob(input, output,
1:2e4d306:             TextInputFormat.class,
1:2e4d306:             RegexMapper.class,
1:2e4d306:             LongWritable.class,
1:2e4d306:             Text.class,
1:2e4d306:             TextOutputFormat.class);
1:7c2b664:     boolean succeeded = job.waitForCompletion(true);
1:7c2b664:     return succeeded ? 0 : -1;
1:2e4d306:   }
1:2e4d306: 
1:2e4d306:   public static void main(String[] args) throws Exception {
1:2e4d306:     ToolRunner.run(new RegexConverterDriver(), args);
1:2e4d306:   }
1:2e4d306: 
1:2e4d306: }
============================================================================
author:tcp
-------------------------------------------------------------------------------
commit:44459bd
/////////////////////////////////////////////////////////////////////////
1:     conf.set(RegexMapper.REGEX, getOption("regex")); //
author:Sean R. Owen
-------------------------------------------------------------------------------
commit:7c2b664
/////////////////////////////////////////////////////////////////////////
1:     boolean succeeded = job.waitForCompletion(true);
1:     return succeeded ? 0 : -1;
commit:4fbfbc6
/////////////////////////////////////////////////////////////////////////
1: /*
/////////////////////////////////////////////////////////////////////////
1: package org.apache.mahout.utils.regex;
1: 
1: import org.apache.hadoop.conf.Configuration;
1: import org.apache.hadoop.fs.Path;
1: import org.apache.hadoop.io.LongWritable;
1: import org.apache.hadoop.io.Text;
1: import org.apache.hadoop.mapreduce.Job;
1: import org.apache.hadoop.mapreduce.lib.input.TextInputFormat;
1: import org.apache.hadoop.mapreduce.lib.output.TextOutputFormat;
1: import org.apache.hadoop.util.ToolRunner;
1: import org.apache.lucene.analysis.Analyzer;
1: import org.apache.mahout.common.AbstractJob;
1: import org.apache.mahout.common.HadoopUtil;
1: import org.apache.mahout.common.commandline.DefaultOptionCreator;
/////////////////////////////////////////////////////////////////////////
1:       if ("url".equalsIgnoreCase(trans)) {
1:       if ("fpg".equalsIgnoreCase(formatter)) {
author:Grant Ingersoll
-------------------------------------------------------------------------------
commit:2e4d306
/////////////////////////////////////////////////////////////////////////
0: package org.apache.mahout.utils.regex;
1: 
0: import org.apache.hadoop.conf.Configuration;
0: import org.apache.hadoop.fs.Path;
0: import org.apache.hadoop.io.LongWritable;
0: import org.apache.hadoop.io.Text;
0: import org.apache.hadoop.mapreduce.Job;
0: import org.apache.hadoop.mapreduce.lib.input.TextInputFormat;
0: import org.apache.hadoop.mapreduce.lib.output.TextOutputFormat;
0: import org.apache.hadoop.util.ToolRunner;
0: import org.apache.lucene.analysis.Analyzer;
0: import org.apache.mahout.common.AbstractJob;
0: import org.apache.mahout.common.HadoopUtil;
0: import org.apache.mahout.common.commandline.DefaultOptionCreator;
0: import org.apache.mahout.vectorizer.DefaultAnalyzer;
1: /**
1:  * Licensed to the Apache Software Foundation (ASF) under one or more
1:  * contributor license agreements.  See the NOTICE file distributed with
1:  * this work for additional information regarding copyright ownership.
1:  * The ASF licenses this file to You under the Apache License, Version 2.0
1:  * (the "License"); you may not use this file except in compliance with
1:  * the License.  You may obtain a copy of the License at
1:  *
1:  *     http://www.apache.org/licenses/LICENSE-2.0
1:  *
1:  * Unless required by applicable law or agreed to in writing, software
1:  * distributed under the License is distributed on an "AS IS" BASIS,
1:  * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
1:  * See the License for the specific language governing permissions and
1:  * limitations under the License.
1:  */
1: 
1: 
1: /**
1:  * Experimental
1:  */
1: public class RegexConverterDriver extends AbstractJob {
1: 
1:   @Override
1:   public int run(String[] args) throws Exception {
1:     addInputOption();
1:     addOutputOption();
1:     addOption(DefaultOptionCreator.overwriteOption().create());
1:     addOption("regex", "regex",
1:             "The regular expression to use", true);
1:     addOption("groupsToKeep", "g",
1:             "The number of the capturing groups to keep", false);
1:     addOption("transformerClass", "t",
1:             "The optional class specifying the Regex Transformer", false);
1:     addOption("formatterClass", "t",
1:             "The optional class specifying the Regex Formatter", false);
1:     addOption(DefaultOptionCreator.analyzerOption().create());
1: 
1:     if (parseArguments(args) == null) {
1:       return -1;
1:     }
1: 
1:     Configuration conf = getConf();
1:     //TODO: How to deal with command line escaping?
0:     conf.set(RegexMapper.REGEX, getOption("regex"));//
1:     String gtk = getOption("groupsToKeep");
1:     if (gtk != null) {
1:       conf.set(RegexMapper.GROUP_MATCHERS, gtk);
1:     }
1:     String trans = getOption("transformerClass");
1:     if (trans != null) {
0:       if (trans.equalsIgnoreCase("url")) {
1:         trans = URLDecodeTransformer.class.getName();
1:       }
1:       conf.set(RegexMapper.TRANSFORMER_CLASS, trans);
1:     }
1:     String formatter = getOption("formatterClass");
1:     if (formatter != null) {
0:       if (formatter.equalsIgnoreCase("fpg")) {
1:         formatter = FPGFormatter.class.getName();
1:       }
1:       conf.set(RegexMapper.FORMATTER_CLASS, formatter);
1:     }
1:     Path input = getInputPath();
1:     Path output = getOutputPath();
1:     if (hasOption(DefaultOptionCreator.OVERWRITE_OPTION)) {
1:       HadoopUtil.delete(getConf(), output);
1:     }
1:     Class<? extends Analyzer> analyzerClass = getAnalyzerClassFromOption();
1:     if (analyzerClass != null) {
1:       conf.set(RegexMapper.ANALYZER_NAME, analyzerClass.getName());
1:     }
1:     Job job = prepareJob(input, output,
1:             TextInputFormat.class,
1:             RegexMapper.class,
1:             LongWritable.class,
1:             Text.class,
1:             TextOutputFormat.class);
0:     job.waitForCompletion(true);
1: 
0:     return 0;
1:   }
1: 
1:   public static void main(String[] args) throws Exception {
1:     ToolRunner.run(new RegexConverterDriver(), args);
1:   }
1: 
1: }
============================================================================