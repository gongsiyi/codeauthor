1:1f672cb: /*
1:1f672cb:  * Licensed to the Apache Software Foundation (ASF) under one or more
1:1f672cb:  * contributor license agreements.  See the NOTICE file distributed with
1:1f672cb:  * this work for additional information regarding copyright ownership.
1:1f672cb:  * The ASF licenses this file to You under the Apache License, Version 2.0
1:1f672cb:  * (the "License"); you may not use this file except in compliance with
1:1f672cb:  * the License.  You may obtain a copy of the License at
1:1f672cb:  *
1:1f672cb:  *     http://www.apache.org/licenses/LICENSE-2.0
1:1f672cb:  *
1:1f672cb:  * Unless required by applicable law or agreed to in writing, software
1:1f672cb:  * distributed under the License is distributed on an "AS IS" BASIS,
1:1f672cb:  * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
1:1f672cb:  * See the License for the specific language governing permissions and
1:1f672cb:  * limitations under the License.
1:1f672cb:  */
1:1f672cb: 
1:1f672cb: package org.apache.mahout.classifier.sgd;
1:1f672cb: 
1:1f672cb: import org.apache.hadoop.io.Writable;
1:1f672cb: import org.apache.mahout.classifier.AbstractVectorClassifier;
1:1f672cb: import org.apache.mahout.classifier.OnlineLearner;
1:1f672cb: import org.apache.mahout.math.DenseMatrix;
1:1f672cb: import org.apache.mahout.math.DenseVector;
1:1f672cb: import org.apache.mahout.math.Matrix;
1:1f672cb: import org.apache.mahout.math.MatrixWritable;
1:1f672cb: import org.apache.mahout.math.Vector;
1:1f672cb: import org.apache.mahout.math.function.Functions;
1:1f672cb: import org.slf4j.Logger;
1:1f672cb: import org.slf4j.LoggerFactory;
1:1f672cb: 
1:1f672cb: import java.io.DataInput;
1:1f672cb: import java.io.DataOutput;
1:1f672cb: import java.io.IOException;
1:1f672cb: 
1:1f672cb: /**
1:1f672cb:  * Online passive aggressive learner that tries to minimize the label ranking hinge loss.
1:1f672cb:  * Implements a multi-class linear classifier minimizing rank loss.
1:1f672cb:  *  based on "Online passive aggressive algorithms" by Cramer et al, 2006.
1:1f672cb:  *  Note: Its better to use classifyNoLink because the loss function is based
1:1f672cb:  *  on ensuring that the score of the good label is larger than the next
1:1f672cb:  *  highest label by some margin. The conversion to probability is just done
1:1f672cb:  *  by exponentiating and dividing by the sum and is empirical at best.
1:1f672cb:  *  Your features should be pre-normalized in some sensible range, for example,
1:1f672cb:  *  by subtracting the mean and standard deviation, if they are very
1:1f672cb:  *  different in magnitude from each other.
1:1f672cb:  */
1:1f672cb: public class PassiveAggressive extends AbstractVectorClassifier implements OnlineLearner, Writable {
1:1f672cb: 
1:1f672cb:   private static final Logger log = LoggerFactory.getLogger(PassiveAggressive.class);
1:1f672cb: 
1:1f672cb:   public static final int WRITABLE_VERSION = 1;
1:1f672cb: 
1:1f672cb:   // the learning rate of the algorithm
1:1f672cb:   private double learningRate = 0.1;
1:1f672cb: 
1:1f672cb:   // loss statistics.
1:1f672cb:   private int lossCount = 0;
1:1f672cb:   private double lossSum = 0;
1:1f672cb: 
1:1f672cb:   // coefficients for the classification.  This is a dense matrix
1:1f672cb:   // that is (numCategories ) x numFeatures
1:1f672cb:   private Matrix weights;
1:1f672cb: 
1:1f672cb:   // number of categories we are classifying.
1:1f672cb:   private int numCategories;
1:1f672cb: 
1:1f672cb:   public PassiveAggressive(int numCategories, int numFeatures) {
1:1f672cb:     this.numCategories = numCategories;
1:1f672cb:     weights = new DenseMatrix(numCategories, numFeatures);
1:1f672cb:     weights.assign(0.0);
1:1f672cb:   }
1:1f672cb: 
1:1f672cb:   /**
1:1f672cb:    * Chainable configuration option.
1:1f672cb:    *
1:1f672cb:    * @param learningRate New value of initial learning rate.
1:1f672cb:    * @return This, so other configurations can be chained.
1:1f672cb:    */
1:1f672cb:   public PassiveAggressive learningRate(double learningRate) {
1:1f672cb:     this.learningRate = learningRate;
1:1f672cb:     return this;
1:1f672cb:   }
1:1f672cb: 
1:1f672cb:   public void copyFrom(PassiveAggressive other) {
1:1f672cb:     learningRate = other.learningRate;
1:1f672cb:     numCategories = other.numCategories;
1:1f672cb:     weights = other.weights;
1:1f672cb:   }
1:1f672cb: 
1:1f672cb:   @Override
1:1f672cb:   public int numCategories() {
1:1f672cb:     return numCategories;
1:1f672cb:   }
1:1f672cb: 
1:1f672cb:   @Override
1:1f672cb:   public Vector classify(Vector instance) {
1:1499411:     Vector result = classifyNoLink(instance);
1:1f672cb:     // Convert to probabilities by exponentiation.
1:1f672cb:     double max = result.maxValue();
1:1f672cb:     result.assign(Functions.minus(max)).assign(Functions.EXP);
1:1f672cb:     result = result.divide(result.norm(1));
1:1f672cb: 
1:1f672cb:     return result.viewPart(1, result.size() - 1);
1:1f672cb:   }
1:1f672cb: 
1:1f672cb:   @Override
1:1f672cb:   public Vector classifyNoLink(Vector instance) {
1:1f672cb:     Vector result = new DenseVector(weights.numRows());
1:1f672cb:     result.assign(0);
1:1f672cb:     for (int i = 0; i < weights.numRows(); i++) {
1:1f672cb:       result.setQuick(i, weights.viewRow(i).dot(instance));
1:1f672cb:     }
1:1f672cb:     return result;
1:1f672cb:   }
1:1f672cb: 
1:1f672cb:   @Override
1:1f672cb:   public double classifyScalar(Vector instance) {
1:1f672cb:     double v1 = weights.viewRow(0).dot(instance);
1:1f672cb:     double v2 = weights.viewRow(1).dot(instance);
1:1f672cb:     v1 = Math.exp(v1);
1:1f672cb:     v2 = Math.exp(v2);
1:1f672cb:     return v2 / (v1 + v2);
1:1f672cb:   }
1:1f672cb: 
1:1f672cb:   public int numFeatures() {
1:1f672cb:     return weights.numCols();
1:1f672cb:   }
1:1f672cb: 
1:1f672cb:   public PassiveAggressive copy() {
1:1f672cb:     close();
1:1f672cb:     PassiveAggressive r = new PassiveAggressive(numCategories(), numFeatures());
1:1f672cb:     r.copyFrom(this);
1:1f672cb:     return r;
1:1f672cb:   }
1:1f672cb: 
1:1f672cb:   @Override
1:1f672cb:   public void write(DataOutput out) throws IOException {
1:1f672cb:     out.writeInt(WRITABLE_VERSION);
1:1f672cb:     out.writeDouble(learningRate);
1:1f672cb:     out.writeInt(numCategories);
1:1f672cb:     MatrixWritable.writeMatrix(out, weights);
1:1f672cb:   }
1:1f672cb: 
1:1f672cb:   @Override
1:1f672cb:   public void readFields(DataInput in) throws IOException {
1:1f672cb:     int version = in.readInt();
1:1f672cb:     if (version == WRITABLE_VERSION) {
1:1f672cb:       learningRate = in.readDouble();
1:1f672cb:       numCategories = in.readInt();
1:1f672cb:       weights = MatrixWritable.readMatrix(in);
1:1f672cb:     } else {
1:1f672cb:       throw new IOException("Incorrect object version, wanted " + WRITABLE_VERSION + " got " + version);
1:1f672cb:     }
1:1f672cb:   }
1:1f672cb: 
1:1f672cb:   @Override
1:1f672cb:   public void close() {
1:1f672cb:       // This is an online classifier, nothing to do.
1:1f672cb:   }
1:1f672cb: 
1:1f672cb:   @Override
1:1f672cb:   public void train(long trackingKey, String groupKey, int actual, Vector instance) {
1:1f672cb:     if (lossCount > 1000) {
1:1f672cb:       log.info("Avg. Loss = {}", lossSum / lossCount);
1:1f672cb:       lossCount = 0;
1:1f672cb:       lossSum = 0;
1:1f672cb:     }
1:1f672cb:     Vector result = classifyNoLink(instance);
1:528ffcd:     double myScore = result.get(actual);
1:1f672cb:     // Find the highest score that is not actual.
1:528ffcd:     int otherIndex = result.maxValueIndex();
1:528ffcd:     double otherValue = result.get(otherIndex);
1:528ffcd:     if (otherIndex == actual) {
1:528ffcd:       result.setQuick(otherIndex, Double.NEGATIVE_INFINITY);
1:528ffcd:       otherIndex = result.maxValueIndex();
1:528ffcd:       otherValue = result.get(otherIndex);
1:1f672cb:     }
1:528ffcd:     double loss = 1.0 - myScore + otherValue;
1:1f672cb:     lossCount += 1;
1:1f672cb:     if (loss >= 0) {
1:1f672cb:       lossSum += loss;
1:1f672cb:       double tau = loss / (instance.dot(instance) + 0.5 / learningRate);
1:1f672cb:       Vector delta = instance.clone();
1:1f672cb:       delta.assign(Functions.mult(tau));
1:528ffcd:       weights.viewRow(actual).assign(delta, Functions.PLUS);
1:528ffcd: //      delta.addTo(weights.viewRow(actual));
1:1f672cb:       delta.assign(Functions.mult(-1));
1:528ffcd:       weights.viewRow(otherIndex).assign(delta, Functions.PLUS);
1:528ffcd: //      delta.addTo(weights.viewRow(otherIndex));
1:1f672cb:     }
1:1f672cb:   }
1:1f672cb: 
1:1f672cb:   @Override
1:1f672cb:   public void train(long trackingKey, int actual, Vector instance) {
1:1f672cb:     train(trackingKey, null, actual, instance);
1:1f672cb:   }
1:1f672cb: 
1:1f672cb:   @Override
1:1f672cb:   public void train(int actual, Vector instance) {
1:1f672cb:     train(0, null, actual, instance);
1:1f672cb:   }
1:1f672cb: 
1:1f672cb: }
============================================================================
author:pferrel
-------------------------------------------------------------------------------
commit:b988c49
author:frankscholten
-------------------------------------------------------------------------------
commit:1a42d85
author:Sean R. Owen
-------------------------------------------------------------------------------
commit:1499411
/////////////////////////////////////////////////////////////////////////
/////////////////////////////////////////////////////////////////////////
1:     Vector result = classifyNoLink(instance);
commit:1f672cb
/////////////////////////////////////////////////////////////////////////
1: /*
1:  * Licensed to the Apache Software Foundation (ASF) under one or more
1:  * contributor license agreements.  See the NOTICE file distributed with
1:  * this work for additional information regarding copyright ownership.
1:  * The ASF licenses this file to You under the Apache License, Version 2.0
1:  * (the "License"); you may not use this file except in compliance with
1:  * the License.  You may obtain a copy of the License at
1:  *
1:  *     http://www.apache.org/licenses/LICENSE-2.0
1:  *
1:  * Unless required by applicable law or agreed to in writing, software
1:  * distributed under the License is distributed on an "AS IS" BASIS,
1:  * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
1:  * See the License for the specific language governing permissions and
1:  * limitations under the License.
1:  */
1: 
1: package org.apache.mahout.classifier.sgd;
1: 
1: import org.apache.hadoop.io.Writable;
1: import org.apache.mahout.classifier.AbstractVectorClassifier;
1: import org.apache.mahout.classifier.OnlineLearner;
1: import org.apache.mahout.math.DenseMatrix;
1: import org.apache.mahout.math.DenseVector;
1: import org.apache.mahout.math.Matrix;
1: import org.apache.mahout.math.MatrixWritable;
1: import org.apache.mahout.math.Vector;
1: import org.apache.mahout.math.function.Functions;
1: import org.slf4j.Logger;
1: import org.slf4j.LoggerFactory;
1: 
1: import java.io.DataInput;
1: import java.io.DataOutput;
1: import java.io.IOException;
1: 
1: /**
1:  * Online passive aggressive learner that tries to minimize the label ranking hinge loss.
1:  * Implements a multi-class linear classifier minimizing rank loss.
1:  *  based on "Online passive aggressive algorithms" by Cramer et al, 2006.
1:  *  Note: Its better to use classifyNoLink because the loss function is based
1:  *  on ensuring that the score of the good label is larger than the next
1:  *  highest label by some margin. The conversion to probability is just done
1:  *  by exponentiating and dividing by the sum and is empirical at best.
1:  *  Your features should be pre-normalized in some sensible range, for example,
1:  *  by subtracting the mean and standard deviation, if they are very
1:  *  different in magnitude from each other.
1:  */
1: public class PassiveAggressive extends AbstractVectorClassifier implements OnlineLearner, Writable {
1: 
1:   private static final Logger log = LoggerFactory.getLogger(PassiveAggressive.class);
1: 
1:   public static final int WRITABLE_VERSION = 1;
1: 
1:   // the learning rate of the algorithm
1:   private double learningRate = 0.1;
1: 
1:   // loss statistics.
1:   private int lossCount = 0;
1:   private double lossSum = 0;
1: 
1:   // coefficients for the classification.  This is a dense matrix
1:   // that is (numCategories ) x numFeatures
1:   private Matrix weights;
1: 
1:   // number of categories we are classifying.
1:   private int numCategories;
1: 
1:   public PassiveAggressive(int numCategories, int numFeatures) {
1:     this.numCategories = numCategories;
1:     weights = new DenseMatrix(numCategories, numFeatures);
1:     weights.assign(0.0);
1:   }
1: 
1:   /**
1:    * Chainable configuration option.
1:    *
1:    * @param learningRate New value of initial learning rate.
1:    * @return This, so other configurations can be chained.
1:    */
1:   public PassiveAggressive learningRate(double learningRate) {
1:     this.learningRate = learningRate;
1:     return this;
1:   }
1: 
1:   public void copyFrom(PassiveAggressive other) {
1:     learningRate = other.learningRate;
1:     numCategories = other.numCategories;
1:     weights = other.weights;
1:   }
1: 
1:   @Override
1:   public int numCategories() {
1:     return numCategories;
1:   }
1: 
1:   @Override
1:   public Vector classify(Vector instance) {
0:     Vector result = (DenseVector) classifyNoLink(instance);
1:     // Convert to probabilities by exponentiation.
1:     double max = result.maxValue();
1:     result.assign(Functions.minus(max)).assign(Functions.EXP);
1:     result = result.divide(result.norm(1));
1: 
1:     return result.viewPart(1, result.size() - 1);
1:   }
1: 
1:   @Override
1:   public Vector classifyNoLink(Vector instance) {
1:     Vector result = new DenseVector(weights.numRows());
1:     result.assign(0);
1:     for (int i = 0; i < weights.numRows(); i++) {
1:       result.setQuick(i, weights.viewRow(i).dot(instance));
1:     }
1:     return result;
1:   }
1: 
1:   @Override
1:   public double classifyScalar(Vector instance) {
1:     double v1 = weights.viewRow(0).dot(instance);
1:     double v2 = weights.viewRow(1).dot(instance);
1:     v1 = Math.exp(v1);
1:     v2 = Math.exp(v2);
1:     return v2 / (v1 + v2);
1:   }
1: 
1:   public int numFeatures() {
1:     return weights.numCols();
1:   }
1: 
1:   public PassiveAggressive copy() {
1:     close();
1:     PassiveAggressive r = new PassiveAggressive(numCategories(), numFeatures());
1:     r.copyFrom(this);
1:     return r;
1:   }
1: 
1:   @Override
1:   public void write(DataOutput out) throws IOException {
1:     out.writeInt(WRITABLE_VERSION);
1:     out.writeDouble(learningRate);
1:     out.writeInt(numCategories);
1:     MatrixWritable.writeMatrix(out, weights);
1:   }
1: 
1:   @Override
1:   public void readFields(DataInput in) throws IOException {
1:     int version = in.readInt();
1:     if (version == WRITABLE_VERSION) {
1:       learningRate = in.readDouble();
1:       numCategories = in.readInt();
1:       weights = MatrixWritable.readMatrix(in);
1:     } else {
1:       throw new IOException("Incorrect object version, wanted " + WRITABLE_VERSION + " got " + version);
1:     }
1:   }
1: 
1:   @Override
1:   public void close() {
1:       // This is an online classifier, nothing to do.
1:   }
1: 
1:   @Override
1:   public void train(long trackingKey, String groupKey, int actual, Vector instance) {
1:     if (lossCount > 1000) {
1:       log.info("Avg. Loss = {}", lossSum / lossCount);
1:       lossCount = 0;
1:       lossSum = 0;
1:     }
1:     Vector result = classifyNoLink(instance);
0:     double my_score = result.get(actual);
1:     // Find the highest score that is not actual.
0:     int other_idx = result.maxValueIndex();
0:     double other_value = result.get(other_idx);
0:     if (other_idx == actual) {
0:       result.setQuick(other_idx, Double.NEGATIVE_INFINITY);
0:       other_idx = result.maxValueIndex();
0:       other_value = result.get(other_idx);
1:     }
0:     double loss = 1.0 - my_score + other_value;
1:     lossCount += 1;
1:     if (loss >= 0) {
1:       lossSum += loss;
1:       double tau = loss / (instance.dot(instance) + 0.5 / learningRate);
1:       Vector delta = instance.clone();
1:       delta.assign(Functions.mult(tau));
0:       delta.addTo(weights.getRow(actual));
1:       delta.assign(Functions.mult(-1));
0:       delta.addTo(weights.getRow(other_idx));
1:     }
1:   }
1: 
1:   @Override
1:   public void train(long trackingKey, int actual, Vector instance) {
1:     train(trackingKey, null, actual, instance);
1:   }
1: 
1:   @Override
1:   public void train(int actual, Vector instance) {
1:     train(0, null, actual, instance);
1:   }
1: 
1: }
author:Ted Dunning
-------------------------------------------------------------------------------
commit:528ffcd
/////////////////////////////////////////////////////////////////////////
0: import com.google.common.base.Function;
/////////////////////////////////////////////////////////////////////////
1:     double myScore = result.get(actual);
1:     int otherIndex = result.maxValueIndex();
1:     double otherValue = result.get(otherIndex);
1:     if (otherIndex == actual) {
1:       result.setQuick(otherIndex, Double.NEGATIVE_INFINITY);
1:       otherIndex = result.maxValueIndex();
1:       otherValue = result.get(otherIndex);
1:     double loss = 1.0 - myScore + otherValue;
1:       weights.viewRow(actual).assign(delta, Functions.PLUS);
1: //      delta.addTo(weights.viewRow(actual));
1:       weights.viewRow(otherIndex).assign(delta, Functions.PLUS);
1: //      delta.addTo(weights.viewRow(otherIndex));
============================================================================