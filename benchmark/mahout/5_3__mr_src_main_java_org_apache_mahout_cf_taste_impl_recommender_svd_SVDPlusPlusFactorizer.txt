1:d51e8b7: /**
1:d51e8b7:  * Licensed to the Apache Software Foundation (ASF) under one or more
1:d51e8b7:  * contributor license agreements.  See the NOTICE file distributed with
1:d51e8b7:  * this work for additional information regarding copyright ownership.
1:d51e8b7:  * The ASF licenses this file to You under the Apache License, Version 2.0
1:d51e8b7:  * (the "License"); you may not use this file except in compliance with
1:d51e8b7:  * the License.  You may obtain a copy of the License at
1:d51e8b7:  *
1:d51e8b7:  *     http://www.apache.org/licenses/LICENSE-2.0
1:d51e8b7:  *
1:d51e8b7:  * Unless required by applicable law or agreed to in writing, software
1:d51e8b7:  * distributed under the License is distributed on an "AS IS" BASIS,
1:d51e8b7:  * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
1:d51e8b7:  * See the License for the specific language governing permissions and
1:d51e8b7:  * limitations under the License.
1:d51e8b7:  */
1:d51e8b7: 
1:d51e8b7: package org.apache.mahout.cf.taste.impl.recommender.svd;
1:d51e8b7: 
1:85f9ece: import org.apache.mahout.cf.taste.common.TasteException;
1:d51e8b7: import org.apache.mahout.cf.taste.impl.common.FastIDSet;
1:d51e8b7: import org.apache.mahout.cf.taste.impl.common.LongPrimitiveIterator;
1:85f9ece: import org.apache.mahout.cf.taste.model.DataModel;
1:d51e8b7: import org.apache.mahout.common.RandomUtils;
1:d51e8b7: 
1:85f9ece: import java.util.ArrayList;
1:85f9ece: import java.util.HashMap;
1:d51e8b7: import java.util.List;
1:d51e8b7: import java.util.Map;
1:d51e8b7: import java.util.Random;
1:d51e8b7: 
1:d51e8b7: /**
1:d51e8b7:  * SVD++, an enhancement of classical matrix factorization for rating prediction.
1:d51e8b7:  * Additionally to using ratings (how did people rate?) for learning, this model also takes into account
1:d51e8b7:  * who rated what.
1:d51e8b7:  *
1:d51e8b7:  * Yehuda Koren: Factorization Meets the Neighborhood: a Multifaceted Collaborative Filtering Model, KDD 2008.
1:d51e8b7:  * http://research.yahoo.com/files/kdd08koren.pdf
1:d51e8b7:  */
1:d51e8b7: public final class SVDPlusPlusFactorizer extends RatingSGDFactorizer {
1:d51e8b7: 
1:d51e8b7:   private double[][] p;
1:d51e8b7:   private double[][] y;
1:d51e8b7:   private Map<Integer, List<Integer>> itemsByUser;
1:d51e8b7: 
1:d51e8b7:   public SVDPlusPlusFactorizer(DataModel dataModel, int numFeatures, int numIterations) throws TasteException {
1:d51e8b7:     this(dataModel, numFeatures, 0.01, 0.1, 0.01, numIterations, 1.0);
1:d51e8b7:     biasLearningRate = 0.7;
1:d51e8b7:     biasReg = 0.33;
3:d51e8b7:   }
1:d51e8b7: 
1:d51e8b7:   public SVDPlusPlusFactorizer(DataModel dataModel, int numFeatures, double learningRate, double preventOverfitting,
1:d51e8b7:       double randomNoise, int numIterations, double learningRateDecay) throws TasteException {
1:d51e8b7:     super(dataModel, numFeatures, learningRate, preventOverfitting, randomNoise, numIterations, learningRateDecay);
1:d51e8b7:   }
1:d51e8b7: 
1:d51e8b7:   @Override
1:d51e8b7:   protected void prepareTraining() throws TasteException {
1:d51e8b7:     super.prepareTraining();
1:d51e8b7:     Random random = RandomUtils.getRandom();
1:d51e8b7: 
1:d51e8b7:     p = new double[dataModel.getNumUsers()][numFeatures];
1:d51e8b7:     for (int i = 0; i < p.length; i++) {
1:8396a27:       for (int feature = 0; feature < FEATURE_OFFSET; feature++) {
1:d51e8b7:         p[i][feature] = 0;
1:d51e8b7:       }
1:8396a27:       for (int feature = FEATURE_OFFSET; feature < numFeatures; feature++) {
1:d51e8b7:         p[i][feature] = random.nextGaussian() * randomNoise;
1:d51e8b7:       }
1:d51e8b7:     }
1:d51e8b7: 
1:d51e8b7:     y = new double[dataModel.getNumItems()][numFeatures];
1:d51e8b7:     for (int i = 0; i < y.length; i++) {
1:8396a27:       for (int feature = 0; feature < FEATURE_OFFSET; feature++) {
1:d51e8b7:         y[i][feature] = 0;
1:d51e8b7:       }
1:8396a27:       for (int feature = FEATURE_OFFSET; feature < numFeatures; feature++) {
1:d51e8b7:         y[i][feature] = random.nextGaussian() * randomNoise;
1:d51e8b7:       }
1:d51e8b7:     }
1:d51e8b7: 
1:d51e8b7:     /* get internal item IDs which we will need several times */
1:85f9ece:     itemsByUser = new HashMap<>();
1:d51e8b7:     LongPrimitiveIterator userIDs = dataModel.getUserIDs();
1:8396a27:     while (userIDs.hasNext()) {
1:d51e8b7:       long userId = userIDs.nextLong();
1:d51e8b7:       int userIndex = userIndex(userId);
1:d51e8b7:       FastIDSet itemIDsFromUser = dataModel.getItemIDsFromUser(userId);
1:85f9ece:       List<Integer> itemIndexes = new ArrayList<>(itemIDsFromUser.size());
1:d51e8b7:       itemsByUser.put(userIndex, itemIndexes);
1:d51e8b7:       for (long itemID2 : itemIDsFromUser) {
1:d51e8b7:         int i2 = itemIndex(itemID2);
1:d51e8b7:         itemIndexes.add(i2);
1:d51e8b7:       }
1:d51e8b7:     }
1:d51e8b7:   }
1:d51e8b7: 
1:d51e8b7:   @Override
1:d51e8b7:   public Factorization factorize() throws TasteException {
1:d51e8b7:     prepareTraining();
1:d51e8b7: 
1:d51e8b7:     super.factorize();
1:d51e8b7: 
1:d51e8b7:     for (int userIndex = 0; userIndex < userVectors.length; userIndex++) {
1:d51e8b7:       for (int itemIndex : itemsByUser.get(userIndex)) {
1:8396a27:         for (int feature = FEATURE_OFFSET; feature < numFeatures; feature++) {
1:d51e8b7:           userVectors[userIndex][feature] += y[itemIndex][feature];
1:d51e8b7:         }
1:d51e8b7:       }
1:f2eccdc:       double denominator = Math.sqrt(itemsByUser.get(userIndex).size());
1:d51e8b7:       for (int feature = 0; feature < userVectors[userIndex].length; feature++) {
1:d51e8b7:         userVectors[userIndex][feature] =
1:d51e8b7:             (float) (userVectors[userIndex][feature] / denominator + p[userIndex][feature]);
1:d51e8b7:       }
1:d51e8b7:     }
1:d51e8b7: 
1:d51e8b7:     return createFactorization(userVectors, itemVectors);
1:d51e8b7:   }
1:d51e8b7: 
1:d51e8b7: 
1:d51e8b7:   @Override
1:8396a27:   protected void updateParameters(long userID, long itemID, float rating, double currentLearningRate) {
1:d51e8b7:     int userIndex = userIndex(userID);
1:d51e8b7:     int itemIndex = itemIndex(itemID);
1:d51e8b7: 
1:d51e8b7:     double[] userVector = p[userIndex];
1:d51e8b7:     double[] itemVector = itemVectors[itemIndex];
1:d51e8b7: 
1:d51e8b7:     double[] pPlusY = new double[numFeatures];
1:d51e8b7:     for (int i2 : itemsByUser.get(userIndex)) {
1:6d16230:       for (int f = FEATURE_OFFSET; f < numFeatures; f++) {
1:6d16230:         pPlusY[f] += y[i2][f];
1:6d16230:       }
1:d51e8b7:     }
1:f2eccdc:     double denominator = Math.sqrt(itemsByUser.get(userIndex).size());
1:8396a27:     for (int feature = 0; feature < pPlusY.length; feature++) {
1:d51e8b7:       pPlusY[feature] = (float) (pPlusY[feature] / denominator + p[userIndex][feature]);
1:d51e8b7:     }
1:d51e8b7: 
1:d51e8b7:     double prediction = predictRating(pPlusY, itemIndex);
1:d51e8b7:     double err = rating - prediction;
1:d51e8b7:     double normalized_error = err / denominator;
1:d51e8b7: 
1:d51e8b7:     // adjust user bias
1:d51e8b7:     userVector[USER_BIAS_INDEX] +=
1:d51e8b7:         biasLearningRate * currentLearningRate * (err - biasReg * preventOverfitting * userVector[USER_BIAS_INDEX]);
1:d51e8b7: 
1:d51e8b7:     // adjust item bias
1:d51e8b7:     itemVector[ITEM_BIAS_INDEX] +=
1:d51e8b7:         biasLearningRate * currentLearningRate * (err - biasReg * preventOverfitting * itemVector[ITEM_BIAS_INDEX]);
1:d51e8b7: 
1:d51e8b7:     // adjust features
1:8396a27:     for (int feature = FEATURE_OFFSET; feature < numFeatures; feature++) {
1:d51e8b7:       double pF = userVector[feature];
1:d51e8b7:       double iF = itemVector[feature];
1:d51e8b7: 
1:d51e8b7:       double deltaU = err * iF - preventOverfitting * pF;
1:d51e8b7:       userVector[feature] += currentLearningRate * deltaU;
1:d51e8b7: 
1:d51e8b7:       double deltaI = err * pPlusY[feature] - preventOverfitting * iF;
1:d51e8b7:       itemVector[feature] += currentLearningRate * deltaI;
1:d51e8b7: 
1:d51e8b7:       double commonUpdate = normalized_error * iF;
1:d51e8b7:       for (int itemIndex2 : itemsByUser.get(userIndex)) {
1:d51e8b7:         double deltaI2 = commonUpdate - preventOverfitting * y[itemIndex2][feature];
1:d51e8b7:         y[itemIndex2][feature] += learningRate * deltaI2;
1:d51e8b7:       }
1:d51e8b7:     }
1:d51e8b7:   }
1:d51e8b7: 
1:d51e8b7:   private double predictRating(double[] userVector, int itemID) {
1:d51e8b7:     double sum = 0;
1:d51e8b7:     for (int feature = 0; feature < numFeatures; feature++) {
1:d51e8b7:       sum += userVector[feature] * itemVectors[itemID][feature];
1:d51e8b7:     }
1:d51e8b7:     return sum;
1:4841efb:   }
1:d51e8b7: }
============================================================================
author:Suneel Marthi
-------------------------------------------------------------------------------
commit:85f9ece
/////////////////////////////////////////////////////////////////////////
1: import org.apache.mahout.cf.taste.common.TasteException;
1: import org.apache.mahout.cf.taste.model.DataModel;
1: import java.util.ArrayList;
1: import java.util.HashMap;
/////////////////////////////////////////////////////////////////////////
1:     itemsByUser = new HashMap<>();
1:       List<Integer> itemIndexes = new ArrayList<>(itemIDsFromUser.size());
author:pferrel
-------------------------------------------------------------------------------
commit:b988c49
author:frankscholten
-------------------------------------------------------------------------------
commit:1a42d85
author:Sebastian Schelter
-------------------------------------------------------------------------------
commit:6d16230
/////////////////////////////////////////////////////////////////////////
1:       for (int f = FEATURE_OFFSET; f < numFeatures; f++) {
1:         pPlusY[f] += y[i2][f];
1:       }
commit:4841efb
/////////////////////////////////////////////////////////////////////////
1: }
commit:d51e8b7
/////////////////////////////////////////////////////////////////////////
1: /**
1:  * Licensed to the Apache Software Foundation (ASF) under one or more
1:  * contributor license agreements.  See the NOTICE file distributed with
1:  * this work for additional information regarding copyright ownership.
1:  * The ASF licenses this file to You under the Apache License, Version 2.0
1:  * (the "License"); you may not use this file except in compliance with
1:  * the License.  You may obtain a copy of the License at
1:  *
1:  *     http://www.apache.org/licenses/LICENSE-2.0
1:  *
1:  * Unless required by applicable law or agreed to in writing, software
1:  * distributed under the License is distributed on an "AS IS" BASIS,
1:  * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
1:  * See the License for the specific language governing permissions and
1:  * limitations under the License.
1:  */
1: 
1: package org.apache.mahout.cf.taste.impl.recommender.svd;
1: 
0: import com.google.common.collect.Lists;
0: import com.google.common.collect.Maps;
1: import org.apache.mahout.cf.taste.impl.common.FastIDSet;
1: import org.apache.mahout.cf.taste.impl.common.LongPrimitiveIterator;
1: import org.apache.mahout.common.RandomUtils;
0: import org.slf4j.Logger;
0: import org.slf4j.LoggerFactory;
1: 
0: import org.apache.mahout.cf.taste.common.TasteException;
0: import org.apache.mahout.cf.taste.model.DataModel;
1: import java.util.List;
1: import java.util.Map;
0: import java.util.NoSuchElementException;
1: import java.util.Random;
1: 
1: /**
1:  * SVD++, an enhancement of classical matrix factorization for rating prediction.
1:  * Additionally to using ratings (how did people rate?) for learning, this model also takes into account
1:  * who rated what.
1:  *
1:  * Yehuda Koren: Factorization Meets the Neighborhood: a Multifaceted Collaborative Filtering Model, KDD 2008.
1:  * http://research.yahoo.com/files/kdd08koren.pdf
1:  */
1: public final class SVDPlusPlusFactorizer extends RatingSGDFactorizer {
1: 
0:   private static final Logger log = LoggerFactory.getLogger(SVDPlusPlusFactorizer.class);
1:   private double[][] p;
1:   private double[][] y;
1:   private Map<Integer, List<Integer>> itemsByUser;
1: 
1:   public SVDPlusPlusFactorizer(DataModel dataModel, int numFeatures, int numIterations) throws TasteException {
1:     this(dataModel, numFeatures, 0.01, 0.1, 0.01, numIterations, 1.0);
1:     biasLearningRate = 0.7;
1:     biasReg = 0.33;
1:   }
1: 
1:   public SVDPlusPlusFactorizer(DataModel dataModel, int numFeatures, double learningRate, double preventOverfitting,
1:       double randomNoise, int numIterations, double learningRateDecay) throws TasteException {
1:     super(dataModel, numFeatures, learningRate, preventOverfitting, randomNoise, numIterations, learningRateDecay);
1:   }
1: 
1:   @Override
1:   protected void prepareTraining() throws TasteException {
1:     super.prepareTraining();
1:     Random random = RandomUtils.getRandom();
1: 
1:     p = new double[dataModel.getNumUsers()][numFeatures];
1:     for (int i = 0; i < p.length; i++) {
0:       for (int feature = 0; feature < featureOffset; feature++) {
1:         p[i][feature] = 0;
1:       }
0:       for (int feature = featureOffset; feature < numFeatures; feature++) {
1:         p[i][feature] = random.nextGaussian() * randomNoise;
1:       }
1:     }
1: 
1:     y = new double[dataModel.getNumItems()][numFeatures];
1:     for (int i = 0; i < y.length; i++) {
0:       for (int feature = 0; feature < featureOffset; feature++) {
1:         y[i][feature] = 0;
1:       }
0:       for (int feature = featureOffset; feature < numFeatures; feature++) {
1:         y[i][feature] = random.nextGaussian() * randomNoise;
1:       }
1:     }
1: 
1:     /* get internal item IDs which we will need several times */
0:     itemsByUser = Maps.newHashMap();
1:     LongPrimitiveIterator userIDs = dataModel.getUserIDs();
0:     try {
0:       while (true) {
1:         long userId = userIDs.nextLong();
1:         int userIndex = userIndex(userId);
1:         FastIDSet itemIDsFromUser = dataModel.getItemIDsFromUser(userId);
0:         List<Integer> itemIndexes = Lists.newArrayListWithCapacity(itemIDsFromUser.size());
1:         itemsByUser.put(userIndex, itemIndexes);
1:         for (long itemID2 : itemIDsFromUser) {
1:           int i2 = itemIndex(itemID2);
1:           itemIndexes.add(i2);
1:         }
1:       }
0:     } catch (NoSuchElementException e) {
0:       // do nothing
1:     }
1:   }
1: 
1:   @Override
1:   public Factorization factorize() throws TasteException {
1:     prepareTraining();
1: 
1:     super.factorize();
1: 
1:     for (int userIndex = 0; userIndex < userVectors.length; userIndex++) {
1:       for (int itemIndex : itemsByUser.get(userIndex)) {
0:         for (int feature = featureOffset; feature < numFeatures; feature++) {
1:           userVectors[userIndex][feature] += y[itemIndex][feature];
1:         }
1:       }
0:       double denominator = Math.sqrt(itemsByUser.size());
1:       for (int feature = 0; feature < userVectors[userIndex].length; feature++) {
1:         userVectors[userIndex][feature] =
1:             (float) (userVectors[userIndex][feature] / denominator + p[userIndex][feature]);
1:       }
1:     }
1: 
1:     return createFactorization(userVectors, itemVectors);
1:   }
1: 
1: 
1:   @Override
0:   protected void updateParameters(long userID, long itemID, float rating, double currentLearningRate)
0:       throws TasteException {
1:     int userIndex = userIndex(userID);
1:     int itemIndex = itemIndex(itemID);
1: 
1:     double[] userVector = p[userIndex];
1:     double[] itemVector = itemVectors[itemIndex];
1: 
1:     double[] pPlusY = new double[numFeatures];
1:     for (int i2 : itemsByUser.get(userIndex)) {
0:         for (int f = featureOffset; f < numFeatures; f++) {
0:           pPlusY[f] += y[i2][f];
1:         }
1:     }
0:     double denominator = Math.sqrt(itemsByUser.size());
0:     for (int feature = 0; feature < pPlusY.length; feature++)
1:       pPlusY[feature] = (float) (pPlusY[feature] / denominator + p[userIndex][feature]);
1: 
1:     double prediction = predictRating(pPlusY, itemIndex);
1:     double err = rating - prediction;
1:     double normalized_error = err / denominator;
1: 
1:     // adjust user bias
1:     userVector[USER_BIAS_INDEX] +=
1:         biasLearningRate * currentLearningRate * (err - biasReg * preventOverfitting * userVector[USER_BIAS_INDEX]);
1: 
1:     // adjust item bias
1:     itemVector[ITEM_BIAS_INDEX] +=
1:         biasLearningRate * currentLearningRate * (err - biasReg * preventOverfitting * itemVector[ITEM_BIAS_INDEX]);
1: 
1:     // adjust features
0:     for (int feature = featureOffset; feature < numFeatures; feature++) {
1:       double pF = userVector[feature];
1:       double iF = itemVector[feature];
1: 
1:       double deltaU = err * iF - preventOverfitting * pF;
1:       userVector[feature] += currentLearningRate * deltaU;
1: 
1:       double deltaI = err * pPlusY[feature] - preventOverfitting * iF;
1:       itemVector[feature] += currentLearningRate * deltaI;
1: 
1:       double commonUpdate = normalized_error * iF;
1:       for (int itemIndex2 : itemsByUser.get(userIndex)) {
1:         double deltaI2 = commonUpdate - preventOverfitting * y[itemIndex2][feature];
1:         y[itemIndex2][feature] += learningRate * deltaI2;
1:       }
1:     }
1:   }
1: 
1:   private double predictRating(double[] userVector, int itemID) {
1:     double sum = 0;
1:     for (int feature = 0; feature < numFeatures; feature++) {
1:       sum += userVector[feature] * itemVectors[itemID][feature];
1:     }
1:     return sum;
1:   }
1: }
author:Sean R. Owen
-------------------------------------------------------------------------------
commit:f2eccdc
/////////////////////////////////////////////////////////////////////////
/////////////////////////////////////////////////////////////////////////
/////////////////////////////////////////////////////////////////////////
1:       double denominator = Math.sqrt(itemsByUser.get(userIndex).size());
/////////////////////////////////////////////////////////////////////////
1:     double denominator = Math.sqrt(itemsByUser.get(userIndex).size());
commit:8396a27
/////////////////////////////////////////////////////////////////////////
/////////////////////////////////////////////////////////////////////////
1:       for (int feature = 0; feature < FEATURE_OFFSET; feature++) {
1:       for (int feature = FEATURE_OFFSET; feature < numFeatures; feature++) {
1:       for (int feature = 0; feature < FEATURE_OFFSET; feature++) {
1:       for (int feature = FEATURE_OFFSET; feature < numFeatures; feature++) {
/////////////////////////////////////////////////////////////////////////
1:     while (userIDs.hasNext()) {
0:       long userId = userIDs.nextLong();
0:       int userIndex = userIndex(userId);
0:       FastIDSet itemIDsFromUser = dataModel.getItemIDsFromUser(userId);
0:       List<Integer> itemIndexes = Lists.newArrayListWithCapacity(itemIDsFromUser.size());
0:       itemsByUser.put(userIndex, itemIndexes);
0:       for (long itemID2 : itemIDsFromUser) {
0:         int i2 = itemIndex(itemID2);
0:         itemIndexes.add(i2);
/////////////////////////////////////////////////////////////////////////
1:         for (int feature = FEATURE_OFFSET; feature < numFeatures; feature++) {
/////////////////////////////////////////////////////////////////////////
1:   protected void updateParameters(long userID, long itemID, float rating, double currentLearningRate) {
/////////////////////////////////////////////////////////////////////////
0:         for (int f = FEATURE_OFFSET; f < numFeatures; f++) {
1:     for (int feature = 0; feature < pPlusY.length; feature++) {
0:     }
/////////////////////////////////////////////////////////////////////////
1:     for (int feature = FEATURE_OFFSET; feature < numFeatures; feature++) {
============================================================================