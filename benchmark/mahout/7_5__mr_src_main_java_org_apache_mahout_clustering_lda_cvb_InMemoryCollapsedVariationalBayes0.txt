1:131eb4a: /**
1:131eb4a:  * Licensed to the Apache Software Foundation (ASF) under one or more
1:131eb4a:  * contributor license agreements.  See the NOTICE file distributed with
1:131eb4a:  * this work for additional information regarding copyright ownership.
1:131eb4a:  * The ASF licenses this file to You under the Apache License, Version 2.0
1:131eb4a:  * (the "License"); you may not use this file except in compliance with
1:131eb4a:  * the License.  You may obtain a copy of the License at
2:131eb4a:  *
1:131eb4a:  *     http://www.apache.org/licenses/LICENSE-2.0
1:131eb4a:  *
1:131eb4a:  * Unless required by applicable law or agreed to in writing, software
1:131eb4a:  * distributed under the License is distributed on an "AS IS" BASIS,
1:131eb4a:  * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
1:131eb4a:  * See the License for the specific language governing permissions and
1:131eb4a:  * limitations under the License.
1:4fbfbc6:  */
1:131eb4a: package org.apache.mahout.clustering.lda.cvb;
13:131eb4a: 
1:85f9ece: import java.io.IOException;
1:85f9ece: import java.util.ArrayList;
1:85f9ece: import java.util.HashMap;
1:85f9ece: import java.util.List;
1:85f9ece: import java.util.Map;
1:85f9ece: 
1:131eb4a: import org.apache.commons.cli2.CommandLine;
1:131eb4a: import org.apache.commons.cli2.Group;
1:131eb4a: import org.apache.commons.cli2.Option;
1:131eb4a: import org.apache.commons.cli2.OptionException;
1:131eb4a: import org.apache.commons.cli2.builder.ArgumentBuilder;
1:131eb4a: import org.apache.commons.cli2.builder.DefaultOptionBuilder;
1:131eb4a: import org.apache.commons.cli2.builder.GroupBuilder;
1:131eb4a: import org.apache.commons.cli2.commandline.Parser;
1:131eb4a: import org.apache.hadoop.conf.Configuration;
1:131eb4a: import org.apache.hadoop.fs.FileStatus;
1:131eb4a: import org.apache.hadoop.fs.FileSystem;
1:131eb4a: import org.apache.hadoop.fs.Path;
1:131eb4a: import org.apache.hadoop.io.IntWritable;
1:131eb4a: import org.apache.hadoop.io.Writable;
1:131eb4a: import org.apache.hadoop.util.ToolRunner;
1:131eb4a: import org.apache.mahout.common.AbstractJob;
1:131eb4a: import org.apache.mahout.common.CommandLineUtil;
1:131eb4a: import org.apache.mahout.common.Pair;
1:4fbfbc6: import org.apache.mahout.common.RandomUtils;
1:131eb4a: import org.apache.mahout.common.commandline.DefaultOptionCreator;
1:07f8c0a: import org.apache.mahout.common.iterator.sequencefile.PathFilters;
1:131eb4a: import org.apache.mahout.common.iterator.sequencefile.SequenceFileIterable;
1:131eb4a: import org.apache.mahout.math.DenseMatrix;
1:131eb4a: import org.apache.mahout.math.DenseVector;
1:131eb4a: import org.apache.mahout.math.DistributedRowMatrixWriter;
1:131eb4a: import org.apache.mahout.math.Matrix;
1:85f9ece: import org.apache.mahout.math.NamedVector;
1:131eb4a: import org.apache.mahout.math.SparseRowMatrix;
1:131eb4a: import org.apache.mahout.math.Vector;
1:131eb4a: import org.apache.mahout.math.VectorWritable;
1:131eb4a: import org.slf4j.Logger;
1:131eb4a: import org.slf4j.LoggerFactory;
1:131eb4a: 
1:131eb4a: /**
1:131eb4a:  * Runs the same algorithm as {@link CVB0Driver}, but sequentially, in memory.  Memory requirements
1:131eb4a:  * are currently: the entire corpus is read into RAM, two copies of the model (each of size
1:131eb4a:  * numTerms * numTopics), and another matrix of size numDocs * numTopics is held in memory
1:131eb4a:  * (to store p(topic|doc) for all docs).
1:131eb4a:  *
1:131eb4a:  * But if all this fits in memory, this can be significantly faster than an iterative MR job.
2:131eb4a:  */
1:131eb4a: public class InMemoryCollapsedVariationalBayes0 extends AbstractJob {
1:4fbfbc6: 
1:131eb4a:   private static final Logger log = LoggerFactory.getLogger(InMemoryCollapsedVariationalBayes0.class);
1:131eb4a: 
1:131eb4a:   private int numTopics;
1:131eb4a:   private int numTerms;
1:131eb4a:   private int numDocuments;
1:131eb4a:   private double alpha;
1:131eb4a:   private double eta;
1:229aeff:   //private int minDfCt;
1:229aeff:   //private double maxDfPct;
1:131eb4a:   private boolean verbose = false;
1:131eb4a:   private String[] terms;  // of length numTerms;
1:131eb4a:   private Matrix corpusWeights; // length numDocs;
1:131eb4a:   private double totalCorpusWeight;
1:131eb4a:   private double initialModelCorpusFraction;
1:131eb4a:   private Matrix docTopicCounts;
1:131eb4a:   private int numTrainingThreads;
1:131eb4a:   private int numUpdatingThreads;
1:131eb4a:   private ModelTrainer modelTrainer;
1:131eb4a: 
1:131eb4a:   private InMemoryCollapsedVariationalBayes0() {
1:131eb4a:     // only for main usage
8:131eb4a:   }
1:131eb4a: 
1:131eb4a:   public void setVerbose(boolean verbose) {
1:131eb4a:     this.verbose = verbose;
1:131eb4a:   }
1:131eb4a:     
2:229aeff:   public InMemoryCollapsedVariationalBayes0(Matrix corpus,
2:229aeff:                                             String[] terms,
2:229aeff:                                             int numTopics,
2:229aeff:                                             double alpha,
1:229aeff:                                             double eta,
1:229aeff:                                             int numTrainingThreads,
1:229aeff:                                             int numUpdatingThreads,
1:229aeff:                                             double modelCorpusFraction) {
1:229aeff:     //this.seed = seed;
1:131eb4a:     this.numTopics = numTopics;
1:131eb4a:     this.alpha = alpha;
1:131eb4a:     this.eta = eta;
1:229aeff:     //this.minDfCt = 0;
1:229aeff:     //this.maxDfPct = 1.0f;
1:131eb4a:     corpusWeights = corpus;
1:131eb4a:     numDocuments = corpus.numRows();
1:131eb4a:     this.terms = terms;
1:131eb4a:     this.initialModelCorpusFraction = modelCorpusFraction;
1:131eb4a:     numTerms = terms != null ? terms.length : corpus.numCols();
1:85f9ece:     Map<String, Integer> termIdMap = new HashMap<>();
1:e64dd36:     if (terms != null) {
1:4841efb:       for (int t = 0; t < terms.length; t++) {
1:131eb4a:         termIdMap.put(terms[t], t);
1:131eb4a:       }
1:131eb4a:     }
1:131eb4a:     this.numTrainingThreads = numTrainingThreads;
1:131eb4a:     this.numUpdatingThreads = numUpdatingThreads;
1:131eb4a:     postInitCorpus();
1:131eb4a:     initializeModel();
1:131eb4a:   }
1:131eb4a: 
1:131eb4a:   private void postInitCorpus() {
1:131eb4a:     totalCorpusWeight = 0;
1:131eb4a:     int numNonZero = 0;
1:4841efb:     for (int i = 0; i < numDocuments; i++) {
1:131eb4a:       Vector v = corpusWeights.viewRow(i);
1:131eb4a:       double norm;
1:e64dd36:       if (v != null && (norm = v.norm(1)) != 0) {
1:131eb4a:         numNonZero += v.getNumNondefaultElements();
1:131eb4a:         totalCorpusWeight += norm;
1:131eb4a:       }
1:131eb4a:     }
1:131eb4a:     String s = "Initializing corpus with %d docs, %d terms, %d nonzero entries, total termWeight %f";
1:131eb4a:     log.info(String.format(s, numDocuments, numTerms, numNonZero, totalCorpusWeight));
1:131eb4a:   }
1:131eb4a: 
1:131eb4a:   private void initializeModel() {
1:229aeff:     TopicModel topicModel = new TopicModel(numTopics, numTerms, eta, alpha, RandomUtils.getRandom(), terms,
1:4841efb:         numUpdatingThreads, initialModelCorpusFraction == 0 ? 1 : initialModelCorpusFraction * totalCorpusWeight);
1:131eb4a:     topicModel.setConf(getConf());
1:131eb4a: 
1:229aeff:     TopicModel updatedModel = initialModelCorpusFraction == 0
1:131eb4a:         ? new TopicModel(numTopics, numTerms, eta, alpha, null, terms, numUpdatingThreads, 1)
1:131eb4a:         : topicModel;
1:131eb4a:     updatedModel.setConf(getConf());
1:131eb4a:     docTopicCounts = new DenseMatrix(numDocuments, numTopics);
1:4841efb:     docTopicCounts.assign(1.0 / numTopics);
1:131eb4a:     modelTrainer = new ModelTrainer(topicModel, updatedModel, numTrainingThreads, numTopics, numTerms);
1:131eb4a:   }
1:131eb4a: 
1:229aeff:   /*
1:131eb4a:   private void inferDocuments(double convergence, int maxIter, boolean recalculate) {
1:e64dd36:     for (int docId = 0; docId < corpusWeights.numRows() ; docId++) {
1:131eb4a:       Vector inferredDocument = topicModel.infer(corpusWeights.viewRow(docId),
1:131eb4a:           docTopicCounts.viewRow(docId));
1:131eb4a:       // do what now?
1:131eb4a:     }
1:131eb4a:   }
1:229aeff:    */
1:131eb4a: 
1:131eb4a:   public void trainDocuments() {
1:131eb4a:     trainDocuments(0);
1:131eb4a:   }
1:131eb4a: 
1:131eb4a:   public void trainDocuments(double testFraction) {
1:131eb4a:     long start = System.nanoTime();
1:131eb4a:     modelTrainer.start();
1:e64dd36:     for (int docId = 0; docId < corpusWeights.numRows(); docId++) {
1:4841efb:       if (testFraction == 0 || docId % (1 / testFraction) != 0) {
1:4841efb:         Vector docTopics = new DenseVector(numTopics).assign(1.0 / numTopics); // docTopicCounts.getRow(docId)
1:131eb4a:         modelTrainer.trainSync(corpusWeights.viewRow(docId), docTopics , true, 10);
1:131eb4a:       }
1:131eb4a:     }
1:131eb4a:     modelTrainer.stop();
1:131eb4a:     logTime("train documents", System.nanoTime() - start);
1:131eb4a:   }
1:131eb4a: 
1:4fbfbc6:   /*
1:131eb4a:   private double error(int docId) {
1:131eb4a:     Vector docTermCounts = corpusWeights.viewRow(docId);
1:e64dd36:     if (docTermCounts == null) {
1:131eb4a:       return 0;
1:131eb4a:     } else {
1:131eb4a:       Vector expectedDocTermCounts =
1:131eb4a:           topicModel.infer(corpusWeights.viewRow(docId), docTopicCounts.viewRow(docId));
1:131eb4a:       double expectedNorm = expectedDocTermCounts.norm(1);
1:131eb4a:       return expectedDocTermCounts.times(docTermCounts.norm(1)/expectedNorm)
1:131eb4a:           .minus(docTermCounts).norm(1);
1:131eb4a:     }
1:131eb4a:   }
1:131eb4a: 
1:131eb4a:   private double error() {
1:131eb4a:     long time = System.nanoTime();
1:131eb4a:     double error = 0;
1:e64dd36:     for (int docId = 0; docId < numDocuments; docId++) {
1:131eb4a:       error += error(docId);
1:131eb4a:     }
1:131eb4a:     logTime("error calculation", System.nanoTime() - time);
1:131eb4a:     return error / totalCorpusWeight;
1:131eb4a:   }
1:4fbfbc6:    */
1:131eb4a: 
1:131eb4a:   public double iterateUntilConvergence(double minFractionalErrorChange,
1:131eb4a:       int maxIterations, int minIter) {
1:131eb4a:     return iterateUntilConvergence(minFractionalErrorChange, maxIterations, minIter, 0);
1:131eb4a:   }
1:131eb4a: 
1:131eb4a:   public double iterateUntilConvergence(double minFractionalErrorChange,
1:131eb4a:       int maxIterations, int minIter, double testFraction) {
1:131eb4a:     int iter = 0;
1:131eb4a:     double oldPerplexity = 0;
1:e64dd36:     while (iter < minIter) {
1:131eb4a:       trainDocuments(testFraction);
1:e64dd36:       if (verbose) {
1:10c535c:         log.info("model after: {}: {}", iter, modelTrainer.getReadModel());
1:131eb4a:       }
1:10c535c:       log.info("iteration {} complete", iter);
1:131eb4a:       oldPerplexity = modelTrainer.calculatePerplexity(corpusWeights, docTopicCounts,
1:131eb4a:           testFraction);
1:10c535c:       log.info("{} = perplexity", oldPerplexity);
1:131eb4a:       iter++;
1:131eb4a:     }
1:131eb4a:     double newPerplexity = 0;
1:4fbfbc6:     double fractionalChange = Double.MAX_VALUE;
1:e64dd36:     while (iter < maxIterations && fractionalChange > minFractionalErrorChange) {
1:131eb4a:       trainDocuments();
1:e64dd36:       if (verbose) {
1:10c535c:         log.info("model after: {}: {}", iter, modelTrainer.getReadModel());
1:131eb4a:       }
1:131eb4a:       newPerplexity = modelTrainer.calculatePerplexity(corpusWeights, docTopicCounts,
1:131eb4a:           testFraction);
1:10c535c:       log.info("{} = perplexity", newPerplexity);
1:131eb4a:       iter++;
1:131eb4a:       fractionalChange = Math.abs(newPerplexity - oldPerplexity) / oldPerplexity;
1:10c535c:       log.info("{} = fractionalChange", fractionalChange);
1:131eb4a:       oldPerplexity = newPerplexity;
1:131eb4a:     }
1:e64dd36:     if (iter < maxIterations) {
1:131eb4a:       log.info(String.format("Converged! fractional error change: %f, error %f",
1:131eb4a:           fractionalChange, newPerplexity));
1:131eb4a:     } else {
1:131eb4a:       log.info(String.format("Reached max iteration count (%d), fractional error change: %f, error: %f",
1:131eb4a:           maxIterations, fractionalChange, newPerplexity));
1:131eb4a:     }
1:131eb4a:     return newPerplexity;
1:131eb4a:   }
1:131eb4a: 
1:131eb4a:   public void writeModel(Path outputPath) throws IOException {
1:131eb4a:     modelTrainer.persist(outputPath);
1:131eb4a:   }
1:131eb4a: 
1:4fbfbc6:   private static void logTime(String label, long nanos) {
1:4ca6b86:     log.info("{} time: {}ms", label, nanos / 1.0e6);
1:131eb4a:   }
1:131eb4a: 
1:131eb4a:   public static int main2(String[] args, Configuration conf) throws Exception {
1:131eb4a:     DefaultOptionBuilder obuilder = new DefaultOptionBuilder();
1:131eb4a:     ArgumentBuilder abuilder = new ArgumentBuilder();
1:131eb4a:     GroupBuilder gbuilder = new GroupBuilder();
1:131eb4a: 
1:131eb4a:     Option helpOpt = DefaultOptionCreator.helpOption();
1:131eb4a: 
1:131eb4a:     Option inputDirOpt = obuilder.withLongName("input").withRequired(true).withArgument(
1:131eb4a:       abuilder.withName("input").withMinimum(1).withMaximum(1).create()).withDescription(
1:131eb4a:       "The Directory on HDFS containing the collapsed, properly formatted files having "
1:131eb4a:           + "one doc per line").withShortName("i").create();
1:131eb4a: 
1:131eb4a:     Option dictOpt = obuilder.withLongName("dictionary").withRequired(false).withArgument(
1:131eb4a:       abuilder.withName("dictionary").withMinimum(1).withMaximum(1).create()).withDescription(
1:131eb4a:       "The path to the term-dictionary format is ... ").withShortName("d").create();
1:131eb4a: 
1:131eb4a:     Option dfsOpt = obuilder.withLongName("dfs").withRequired(false).withArgument(
1:131eb4a:       abuilder.withName("dfs").withMinimum(1).withMaximum(1).create()).withDescription(
1:131eb4a:       "HDFS namenode URI").withShortName("dfs").create();
1:131eb4a: 
1:131eb4a:     Option numTopicsOpt = obuilder.withLongName("numTopics").withRequired(true).withArgument(abuilder
1:131eb4a:         .withName("numTopics").withMinimum(1).withMaximum(1)
1:131eb4a:         .create()).withDescription("Number of topics to learn").withShortName("top").create();
1:131eb4a: 
1:131eb4a:     Option outputTopicFileOpt = obuilder.withLongName("topicOutputFile").withRequired(true).withArgument(
1:131eb4a:         abuilder.withName("topicOutputFile").withMinimum(1).withMaximum(1).create())
1:131eb4a:         .withDescription("File to write out p(term | topic)").withShortName("to").create();
1:131eb4a: 
1:131eb4a:     Option outputDocFileOpt = obuilder.withLongName("docOutputFile").withRequired(true).withArgument(
1:131eb4a:         abuilder.withName("docOutputFile").withMinimum(1).withMaximum(1).create())
1:131eb4a:         .withDescription("File to write out p(topic | docid)").withShortName("do").create();
1:131eb4a: 
1:131eb4a:     Option alphaOpt = obuilder.withLongName("alpha").withRequired(false).withArgument(abuilder
1:131eb4a:         .withName("alpha").withMinimum(1).withMaximum(1).withDefault("0.1").create())
1:131eb4a:         .withDescription("Smoothing parameter for p(topic | document) prior").withShortName("a").create();
1:131eb4a: 
1:131eb4a:     Option etaOpt = obuilder.withLongName("eta").withRequired(false).withArgument(abuilder
1:131eb4a:         .withName("eta").withMinimum(1).withMaximum(1).withDefault("0.1").create())
1:131eb4a:         .withDescription("Smoothing parameter for p(term | topic)").withShortName("e").create();
1:131eb4a: 
1:131eb4a:     Option maxIterOpt = obuilder.withLongName("maxIterations").withRequired(false).withArgument(abuilder
1:553252f:         .withName("maxIterations").withMinimum(1).withMaximum(1).withDefault("10").create())
1:131eb4a:         .withDescription("Maximum number of training passes").withShortName("m").create();
1:131eb4a: 
1:131eb4a:     Option modelCorpusFractionOption = obuilder.withLongName("modelCorpusFraction")
1:131eb4a:         .withRequired(false).withArgument(abuilder.withName("modelCorpusFraction").withMinimum(1)
1:553252f:         .withMaximum(1).withDefault("0.0").create()).withShortName("mcf")
1:131eb4a:         .withDescription("For online updates, initial value of |model|/|corpus|").create();
1:131eb4a: 
1:131eb4a:     Option burnInOpt = obuilder.withLongName("burnInIterations").withRequired(false).withArgument(abuilder
1:553252f:         .withName("burnInIterations").withMinimum(1).withMaximum(1).withDefault("5").create())
1:131eb4a:         .withDescription("Minimum number of iterations").withShortName("b").create();
1:131eb4a: 
1:131eb4a:     Option convergenceOpt = obuilder.withLongName("convergence").withRequired(false).withArgument(abuilder
1:131eb4a:         .withName("convergence").withMinimum(1).withMaximum(1).withDefault("0.0").create())
1:131eb4a:         .withDescription("Fractional rate of perplexity to consider convergence").withShortName("c").create();
1:131eb4a: 
1:131eb4a:     Option reInferDocTopicsOpt = obuilder.withLongName("reInferDocTopics").withRequired(false)
1:131eb4a:         .withArgument(abuilder.withName("reInferDocTopics").withMinimum(1).withMaximum(1)
1:131eb4a:         .withDefault("no").create())
1:131eb4a:         .withDescription("re-infer p(topic | doc) : [no | randstart | continue]")
1:131eb4a:         .withShortName("rdt").create();
1:131eb4a: 
1:131eb4a:     Option numTrainThreadsOpt = obuilder.withLongName("numTrainThreads").withRequired(false)
1:131eb4a:         .withArgument(abuilder.withName("numTrainThreads").withMinimum(1).withMaximum(1)
1:131eb4a:         .withDefault("1").create())
1:131eb4a:         .withDescription("number of threads to train with")
1:131eb4a:         .withShortName("ntt").create();
1:131eb4a: 
1:131eb4a:     Option numUpdateThreadsOpt = obuilder.withLongName("numUpdateThreads").withRequired(false)
1:131eb4a:         .withArgument(abuilder.withName("numUpdateThreads").withMinimum(1).withMaximum(1)
1:131eb4a:         .withDefault("1").create())
1:131eb4a:         .withDescription("number of threads to update the model with")
1:131eb4a:         .withShortName("nut").create();
1:131eb4a: 
1:131eb4a:     Option verboseOpt = obuilder.withLongName("verbose").withRequired(false)
1:131eb4a:         .withArgument(abuilder.withName("verbose").withMinimum(1).withMaximum(1)
1:131eb4a:         .withDefault("false").create())
1:131eb4a:         .withDescription("print verbose information, like top-terms in each topic, during iteration")
1:131eb4a:         .withShortName("v").create();
1:131eb4a: 
1:131eb4a:     Group group = gbuilder.withName("Options").withOption(inputDirOpt).withOption(numTopicsOpt)
1:131eb4a:         .withOption(alphaOpt).withOption(etaOpt)
1:131eb4a:         .withOption(maxIterOpt).withOption(burnInOpt).withOption(convergenceOpt)
1:131eb4a:         .withOption(dictOpt).withOption(reInferDocTopicsOpt)
1:131eb4a:         .withOption(outputDocFileOpt).withOption(outputTopicFileOpt).withOption(dfsOpt)
1:131eb4a:         .withOption(numTrainThreadsOpt).withOption(numUpdateThreadsOpt)
1:131eb4a:         .withOption(modelCorpusFractionOption).withOption(verboseOpt).create();
1:131eb4a: 
1:131eb4a:     try {
1:131eb4a:       Parser parser = new Parser();
1:131eb4a: 
1:131eb4a:       parser.setGroup(group);
1:131eb4a:       parser.setHelpOption(helpOpt);
1:131eb4a:       CommandLine cmdLine = parser.parse(args);
1:131eb4a:       if (cmdLine.hasOption(helpOpt)) {
1:131eb4a:         CommandLineUtil.printHelp(group);
1:131eb4a:         return -1;
1:131eb4a:       }
1:131eb4a: 
1:131eb4a:       String inputDirString = (String) cmdLine.getValue(inputDirOpt);
1:131eb4a:       String dictDirString = cmdLine.hasOption(dictOpt) ? (String)cmdLine.getValue(dictOpt) : null;
1:131eb4a:       int numTopics = Integer.parseInt((String) cmdLine.getValue(numTopicsOpt));
1:131eb4a:       double alpha = Double.parseDouble((String)cmdLine.getValue(alphaOpt));
1:131eb4a:       double eta = Double.parseDouble((String)cmdLine.getValue(etaOpt));
1:131eb4a:       int maxIterations = Integer.parseInt((String)cmdLine.getValue(maxIterOpt));
1:553252f:       int burnInIterations = Integer.parseInt((String)cmdLine.getValue(burnInOpt));
1:131eb4a:       double minFractionalErrorChange = Double.parseDouble((String) cmdLine.getValue(convergenceOpt));
1:131eb4a:       int numTrainThreads = Integer.parseInt((String)cmdLine.getValue(numTrainThreadsOpt));
1:131eb4a:       int numUpdateThreads = Integer.parseInt((String)cmdLine.getValue(numUpdateThreadsOpt));
1:131eb4a:       String topicOutFile = (String)cmdLine.getValue(outputTopicFileOpt);
1:131eb4a:       String docOutFile = (String)cmdLine.getValue(outputDocFileOpt);
1:229aeff:       //String reInferDocTopics = (String)cmdLine.getValue(reInferDocTopicsOpt);
1:131eb4a:       boolean verbose = Boolean.parseBoolean((String) cmdLine.getValue(verboseOpt));
1:553252f:       double modelCorpusFraction = Double.parseDouble((String)cmdLine.getValue(modelCorpusFractionOption));
1:131eb4a: 
1:131eb4a:       long start = System.nanoTime();
1:131eb4a: 
1:e64dd36:       if (conf.get("fs.default.name") == null) {
1:131eb4a:         String dfsNameNode = (String)cmdLine.getValue(dfsOpt);
1:131eb4a:         conf.set("fs.default.name", dfsNameNode);
1:131eb4a:       }
1:131eb4a:       String[] terms = loadDictionary(dictDirString, conf);
1:131eb4a:       logTime("dictionary loading", System.nanoTime() - start);
1:131eb4a:       start = System.nanoTime();
1:131eb4a:       Matrix corpus = loadVectors(inputDirString, conf);
1:131eb4a:       logTime("vector seqfile corpus loading", System.nanoTime() - start);
1:131eb4a:       start = System.nanoTime();
1:4fbfbc6:       InMemoryCollapsedVariationalBayes0 cvb0 =
1:4fbfbc6:           new InMemoryCollapsedVariationalBayes0(corpus, terms, numTopics, alpha, eta,
1:229aeff:                                                  numTrainThreads, numUpdateThreads, modelCorpusFraction);
1:131eb4a:       logTime("cvb0 init", System.nanoTime() - start);
1:131eb4a: 
1:131eb4a:       start = System.nanoTime();
1:131eb4a:       cvb0.setVerbose(verbose);
1:4fbfbc6:       cvb0.iterateUntilConvergence(minFractionalErrorChange, maxIterations, burnInIterations);
1:131eb4a:       logTime("total training time", System.nanoTime() - start);
1:131eb4a: 
1:229aeff:       /*
1:4fbfbc6:       if ("randstart".equalsIgnoreCase(reInferDocTopics)) {
1:131eb4a:         cvb0.inferDocuments(0.0, 100, true);
1:e64dd36:       } else if ("continue".equalsIgnoreCase(reInferDocTopics)) {
1:131eb4a:         cvb0.inferDocuments(0.0, 100, false);
1:131eb4a:       }
1:229aeff:        */
1:131eb4a: 
1:131eb4a:       start = System.nanoTime();
1:131eb4a:       cvb0.writeModel(new Path(topicOutFile));
1:131eb4a:       DistributedRowMatrixWriter.write(new Path(docOutFile), conf, cvb0.docTopicCounts);
1:131eb4a:       logTime("printTopics", System.nanoTime() - start);
1:131eb4a:     } catch (OptionException e) {
1:131eb4a:       log.error("Error while parsing options", e);
1:131eb4a:       CommandLineUtil.printHelp(group);
1:131eb4a:     }
1:131eb4a:     return 0;
1:131eb4a:   }
1:131eb4a: 
1:4fbfbc6:   private static String[] loadDictionary(String dictionaryPath, Configuration conf) {
1:e64dd36:     if (dictionaryPath == null) {
1:131eb4a:       return null;
1:131eb4a:     }
1:131eb4a:     Path dictionaryFile = new Path(dictionaryPath);
1:85f9ece:     List<Pair<Integer, String>> termList = new ArrayList<>();
1:131eb4a:     int maxTermId = 0;
1:131eb4a:      // key is word value is id
1:131eb4a:     for (Pair<Writable, IntWritable> record
1:131eb4a:             : new SequenceFileIterable<Writable, IntWritable>(dictionaryFile, true, conf)) {
1:87c15be:       termList.add(new Pair<>(record.getSecond().get(),
1:131eb4a:           record.getFirst().toString()));
1:131eb4a:       maxTermId = Math.max(maxTermId, record.getSecond().get());
1:131eb4a:     }
1:131eb4a:     String[] terms = new String[maxTermId + 1];
1:e64dd36:     for (Pair<Integer, String> pair : termList) {
1:131eb4a:       terms[pair.getFirst()] = pair.getSecond();
1:131eb4a:     }
1:131eb4a:     return terms;
1:131eb4a:   }
1:131eb4a: 
1:131eb4a:   @Override
1:131eb4a:   public Configuration getConf() {
1:131eb4a:     return super.getConf();
1:131eb4a:   }
1:131eb4a: 
1:131eb4a:   private static Matrix loadVectors(String vectorPathString, Configuration conf)
2:131eb4a:     throws IOException {
1:131eb4a:     Path vectorPath = new Path(vectorPathString);
1:131eb4a:     FileSystem fs = vectorPath.getFileSystem(conf);
1:85f9ece:     List<Path> subPaths = new ArrayList<>();
1:e64dd36:     if (fs.isFile(vectorPath)) {
1:131eb4a:       subPaths.add(vectorPath);
1:131eb4a:     } else {
1:e64dd36:       for (FileStatus fileStatus : fs.listStatus(vectorPath, PathFilters.logsCRCFilter())) {
1:131eb4a:         subPaths.add(fileStatus.getPath());
1:131eb4a:       }
1:131eb4a:     }
1:85f9ece:     List<Pair<Integer, Vector>> rowList = new ArrayList<>();
1:4300ff3:     int numRows = Integer.MIN_VALUE;
1:4300ff3:     int numCols = -1;
1:4300ff3:     boolean sequentialAccess = false;
1:e64dd36:     for (Path subPath : subPaths) {
1:e64dd36:       for (Pair<IntWritable, VectorWritable> record
1:131eb4a:           : new SequenceFileIterable<IntWritable, VectorWritable>(subPath, true, conf)) {
1:4300ff3:         int id = record.getFirst().get();
1:4300ff3:         Vector vector = record.getSecond().get();
1:4300ff3:         if (vector instanceof NamedVector) {
1:4300ff3:           vector = ((NamedVector)vector).getDelegate();
1:4300ff3:         }
1:4300ff3:         if (numCols < 0) {
1:4300ff3:           numCols = vector.size();
1:4300ff3:           sequentialAccess = vector.isSequentialAccess();
1:4300ff3:         }
1:4300ff3:         rowList.add(Pair.of(id, vector));
1:4300ff3:         numRows = Math.max(numRows, id);
1:131eb4a:       }
1:131eb4a:     }
1:4300ff3:     numRows++;
1:4300ff3:     Vector[] rowVectors = new Vector[numRows];
1:4300ff3:     for (Pair<Integer, Vector> pair : rowList) {
1:4300ff3:       rowVectors[pair.getFirst()] = pair.getSecond();
1:4300ff3:     }
1:4300ff3:     return new SparseRowMatrix(numRows, numCols, rowVectors, true, !sequentialAccess);
1:4300ff3: 
1:131eb4a:   }
1:131eb4a: 
1:131eb4a:   @Override
1:131eb4a:   public int run(String[] strings) throws Exception {
1:131eb4a:     return main2(strings, getConf());
1:131eb4a:   }
1:131eb4a: 
1:131eb4a:   public static void main(String[] args) throws Exception {
1:131eb4a:     ToolRunner.run(new InMemoryCollapsedVariationalBayes0(), args);
1:131eb4a:   }
1:131eb4a: }
============================================================================
author:Suneel Marthi
-------------------------------------------------------------------------------
commit:85f9ece
/////////////////////////////////////////////////////////////////////////
1: import java.io.IOException;
1: import java.util.ArrayList;
1: import java.util.HashMap;
1: import java.util.List;
1: import java.util.Map;
1: 
/////////////////////////////////////////////////////////////////////////
1: import org.apache.mahout.math.NamedVector;
/////////////////////////////////////////////////////////////////////////
1:     Map<String, Integer> termIdMap = new HashMap<>();
/////////////////////////////////////////////////////////////////////////
1:     List<Pair<Integer, String>> termList = new ArrayList<>();
/////////////////////////////////////////////////////////////////////////
1:     List<Path> subPaths = new ArrayList<>();
/////////////////////////////////////////////////////////////////////////
1:     List<Pair<Integer, Vector>> rowList = new ArrayList<>();
commit:87c15be
/////////////////////////////////////////////////////////////////////////
1:       termList.add(new Pair<>(record.getSecond().get(),
author:pferrel
-------------------------------------------------------------------------------
commit:b988c49
author:frankscholten
-------------------------------------------------------------------------------
commit:1a42d85
author:Sebastian Schelter
-------------------------------------------------------------------------------
commit:58cc1ae
/////////////////////////////////////////////////////////////////////////
/////////////////////////////////////////////////////////////////////////
commit:4841efb
/////////////////////////////////////////////////////////////////////////
1:       for (int t = 0; t < terms.length; t++) {
/////////////////////////////////////////////////////////////////////////
1:     for (int i = 0; i < numDocuments; i++) {
/////////////////////////////////////////////////////////////////////////
1:         numUpdatingThreads, initialModelCorpusFraction == 0 ? 1 : initialModelCorpusFraction * totalCorpusWeight);
/////////////////////////////////////////////////////////////////////////
1:     docTopicCounts.assign(1.0 / numTopics);
/////////////////////////////////////////////////////////////////////////
1:       if (testFraction == 0 || docId % (1 / testFraction) != 0) {
1:         Vector docTopics = new DenseVector(numTopics).assign(1.0 / numTopics); // docTopicCounts.getRow(docId)
commit:553252f
/////////////////////////////////////////////////////////////////////////
1:         .withName("maxIterations").withMinimum(1).withMaximum(1).withDefault("10").create())
1:         .withMaximum(1).withDefault("0.0").create()).withShortName("mcf")
1:         .withName("burnInIterations").withMinimum(1).withMaximum(1).withDefault("5").create())
/////////////////////////////////////////////////////////////////////////
1:       int burnInIterations = Integer.parseInt((String)cmdLine.getValue(burnInOpt));
/////////////////////////////////////////////////////////////////////////
1:       double modelCorpusFraction = Double.parseDouble((String)cmdLine.getValue(modelCorpusFractionOption));
commit:07f8c0a
/////////////////////////////////////////////////////////////////////////
1: import org.apache.mahout.common.iterator.sequencefile.PathFilters;
/////////////////////////////////////////////////////////////////////////
0:       for(FileStatus fileStatus : fs.listStatus(vectorPath, PathFilters.logsCRCFilter())) {
author:Sean R. Owen
-------------------------------------------------------------------------------
commit:4ca6b86
/////////////////////////////////////////////////////////////////////////
1:     log.info("{} time: {}ms", label, nanos / 1.0e6);
commit:10c535c
/////////////////////////////////////////////////////////////////////////
1:         log.info("model after: {}: {}", iter, modelTrainer.getReadModel());
1:       log.info("iteration {} complete", iter);
1:       log.info("{} = perplexity", oldPerplexity);
/////////////////////////////////////////////////////////////////////////
1:         log.info("model after: {}: {}", iter, modelTrainer.getReadModel());
1:       log.info("{} = perplexity", newPerplexity);
1:       log.info("{} = fractionalChange", fractionalChange);
commit:8b6a26a
/////////////////////////////////////////////////////////////////////////
commit:229aeff
/////////////////////////////////////////////////////////////////////////
1:   //private int minDfCt;
1:   //private double maxDfPct;
/////////////////////////////////////////////////////////////////////////
1:   public InMemoryCollapsedVariationalBayes0(Matrix corpus,
1:                                             String[] terms,
1:                                             int numTopics,
1:                                             double alpha,
0:                                             double eta) {
0:     this(corpus, terms, numTopics, alpha, eta, 1, 1, 0);
1:   public InMemoryCollapsedVariationalBayes0(Matrix corpus,
1:                                             String[] terms,
1:                                             int numTopics,
1:                                             double alpha,
1:                                             double eta,
1:                                             int numTrainingThreads,
1:                                             int numUpdatingThreads,
1:                                             double modelCorpusFraction) {
1:     //this.seed = seed;
1:     //this.minDfCt = 0;
1:     //this.maxDfPct = 1.0f;
0:     Map<String, Integer> termIdMap = Maps.newHashMap();
/////////////////////////////////////////////////////////////////////////
1:     TopicModel topicModel = new TopicModel(numTopics, numTerms, eta, alpha, RandomUtils.getRandom(), terms,
0:                                            numUpdatingThreads,
0:                                            initialModelCorpusFraction == 0 ? 1 : initialModelCorpusFraction * totalCorpusWeight);
1:     TopicModel updatedModel = initialModelCorpusFraction == 0
/////////////////////////////////////////////////////////////////////////
1:   /*
/////////////////////////////////////////////////////////////////////////
1:    */
/////////////////////////////////////////////////////////////////////////
1:       //String reInferDocTopics = (String)cmdLine.getValue(reInferDocTopicsOpt);
/////////////////////////////////////////////////////////////////////////
1:                                                  numTrainThreads, numUpdateThreads, modelCorpusFraction);
/////////////////////////////////////////////////////////////////////////
1:       /*
1:        */
commit:564c3e1
/////////////////////////////////////////////////////////////////////////
0:       if(testFraction == 0 || docId % (1/testFraction) != 0) {
commit:4fbfbc6
/////////////////////////////////////////////////////////////////////////
/////////////////////////////////////////////////////////////////////////
1: import org.apache.mahout.common.RandomUtils;
/////////////////////////////////////////////////////////////////////////
/////////////////////////////////////////////////////////////////////////
1: 
/////////////////////////////////////////////////////////////////////////
/////////////////////////////////////////////////////////////////////////
0:     topicModel = new TopicModel(numTopics, numTerms, eta, alpha, RandomUtils.getRandom(), terms,
/////////////////////////////////////////////////////////////////////////
1:   /*
/////////////////////////////////////////////////////////////////////////
1:    */
/////////////////////////////////////////////////////////////////////////
/////////////////////////////////////////////////////////////////////////
1:     double fractionalChange = Double.MAX_VALUE;
/////////////////////////////////////////////////////////////////////////
1:   private static void logTime(String label, long nanos) {
0:     log.info("{} time: {}ms", label, (double) nanos / 1.0e6);
/////////////////////////////////////////////////////////////////////////
0:         .withMaximum(1).withDefault(0.0).create()).withShortName("mcf")
/////////////////////////////////////////////////////////////////////////
/////////////////////////////////////////////////////////////////////////
1:       InMemoryCollapsedVariationalBayes0 cvb0 =
1:           new InMemoryCollapsedVariationalBayes0(corpus, terms, numTopics, alpha, eta,
0:                                                  numTrainThreads, numUpdateThreads, modelCorpusFraction, 1234);
1:       cvb0.iterateUntilConvergence(minFractionalErrorChange, maxIterations, burnInIterations);
1:       if ("randstart".equalsIgnoreCase(reInferDocTopics)) {
0:       } else if("continue".equalsIgnoreCase(reInferDocTopics)) {
/////////////////////////////////////////////////////////////////////////
0:   /*
/////////////////////////////////////////////////////////////////////////
1:    */
1:   private static String[] loadDictionary(String dictionaryPath, Configuration conf) {
author:Jacob Alexander Mannix
-------------------------------------------------------------------------------
commit:4300ff3
/////////////////////////////////////////////////////////////////////////
0: import org.apache.mahout.math.SparseMatrix;
0: import org.apache.mahout.math.RandomAccessSparseVector;
0: import org.apache.mahout.math.NamedVector;
/////////////////////////////////////////////////////////////////////////
0:     List<Pair<Integer, Vector>> rowList = Lists.newArrayList();
1:     int numRows = Integer.MIN_VALUE;
1:     int numCols = -1;
1:     boolean sequentialAccess = false;
1:         int id = record.getFirst().get();
1:         Vector vector = record.getSecond().get();
1:         if (vector instanceof NamedVector) {
1:           vector = ((NamedVector)vector).getDelegate();
1:         }
1:         if (numCols < 0) {
1:           numCols = vector.size();
1:           sequentialAccess = vector.isSequentialAccess();
1:         }
1:         rowList.add(Pair.of(id, vector));
1:         numRows = Math.max(numRows, id);
1:     numRows++;
1:     Vector[] rowVectors = new Vector[numRows];
1:     for (Pair<Integer, Vector> pair : rowList) {
1:       rowVectors[pair.getFirst()] = pair.getSecond();
1:     }
1:     return new SparseRowMatrix(numRows, numCols, rowVectors, true, !sequentialAccess);
1: 
commit:131eb4a
/////////////////////////////////////////////////////////////////////////
1: /**
1:  * Licensed to the Apache Software Foundation (ASF) under one or more
1:  * contributor license agreements.  See the NOTICE file distributed with
1:  * this work for additional information regarding copyright ownership.
1:  * The ASF licenses this file to You under the Apache License, Version 2.0
1:  * (the "License"); you may not use this file except in compliance with
1:  * the License.  You may obtain a copy of the License at
1:  *
1:  *     http://www.apache.org/licenses/LICENSE-2.0
1:  *
1:  * Unless required by applicable law or agreed to in writing, software
1:  * distributed under the License is distributed on an "AS IS" BASIS,
1:  * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
1:  * See the License for the specific language governing permissions and
1:  * limitations under the License.
1:  */
1: package org.apache.mahout.clustering.lda.cvb;
1: 
0: import com.google.common.base.Charsets;
0: import com.google.common.collect.Lists;
0: import com.google.common.collect.Maps;
0: import com.google.common.io.Resources;
1: import org.apache.commons.cli2.CommandLine;
1: import org.apache.commons.cli2.Group;
1: import org.apache.commons.cli2.Option;
1: import org.apache.commons.cli2.OptionException;
1: import org.apache.commons.cli2.builder.ArgumentBuilder;
1: import org.apache.commons.cli2.builder.DefaultOptionBuilder;
1: import org.apache.commons.cli2.builder.GroupBuilder;
1: import org.apache.commons.cli2.commandline.Parser;
1: import org.apache.hadoop.conf.Configuration;
1: import org.apache.hadoop.fs.FileStatus;
1: import org.apache.hadoop.fs.FileSystem;
1: import org.apache.hadoop.fs.Path;
1: import org.apache.hadoop.io.IntWritable;
1: import org.apache.hadoop.io.Writable;
1: import org.apache.hadoop.util.ToolRunner;
1: import org.apache.mahout.common.AbstractJob;
1: import org.apache.mahout.common.CommandLineUtil;
1: import org.apache.mahout.common.Pair;
1: import org.apache.mahout.common.commandline.DefaultOptionCreator;
1: import org.apache.mahout.common.iterator.sequencefile.SequenceFileIterable;
1: import org.apache.mahout.math.DenseMatrix;
1: import org.apache.mahout.math.DenseVector;
1: import org.apache.mahout.math.DistributedRowMatrixWriter;
1: import org.apache.mahout.math.Matrix;
1: import org.apache.mahout.math.SparseRowMatrix;
1: import org.apache.mahout.math.Vector;
1: import org.apache.mahout.math.VectorWritable;
1: import org.slf4j.Logger;
1: import org.slf4j.LoggerFactory;
1: 
0: import java.io.IOException;
0: import java.util.List;
0: import java.util.Map;
0: import java.util.Random;
1: 
1: /**
1:  * Runs the same algorithm as {@link CVB0Driver}, but sequentially, in memory.  Memory requirements
1:  * are currently: the entire corpus is read into RAM, two copies of the model (each of size
1:  * numTerms * numTopics), and another matrix of size numDocs * numTopics is held in memory
1:  * (to store p(topic|doc) for all docs).
1:  *
1:  * But if all this fits in memory, this can be significantly faster than an iterative MR job.
1:  *
0:  * TODO(jmannix) Clean up superfluous garbage in this class.
1:  */
1: public class InMemoryCollapsedVariationalBayes0 extends AbstractJob {
1:   private static final Logger log = LoggerFactory.getLogger(InMemoryCollapsedVariationalBayes0.class);
1: 
1:   private int numTopics;
1:   private int numTerms;
1:   private int numDocuments;
1:   private double alpha;
1:   private double eta;
0:   private int minDfCt;
0:   private double maxDfPct;
1:   private boolean verbose = false;
1: 
0:   private Map<String, Integer> termIdMap;
1:   private String[] terms;  // of length numTerms;
1: 
1:   private Matrix corpusWeights; // length numDocs;
1:   private double totalCorpusWeight;
1:   private double initialModelCorpusFraction;
1: 
1:   private Matrix docTopicCounts;
1: 
0:   private long seed;
1: 
0:   private TopicModel topicModel;
0:   private TopicModel updatedModel;
1: 
1:   private int numTrainingThreads;
1:   private int numUpdatingThreads;
1: 
1:   private ModelTrainer modelTrainer;
1: 
1:   private InMemoryCollapsedVariationalBayes0() {
1:     // only for main usage
1:   }
1: 
1:   public void setVerbose(boolean verbose) {
1:     this.verbose = verbose;
1:   }
1: 
0:   public InMemoryCollapsedVariationalBayes0(Matrix corpus, String[] terms, int numTopics,
0:       double alpha, double eta) {
0:     this(corpus, terms, numTopics, alpha, eta, 1, 1, 0, 1234);
1:   }
1:     
0:   public InMemoryCollapsedVariationalBayes0(Matrix corpus, String[] terms, int numTopics,
0:       double alpha, double eta, int numTrainingThreads, int numUpdatingThreads,
0:       double modelCorpusFraction, long seed) {
0:     this.seed = seed;
1:     this.numTopics = numTopics;
1:     this.alpha = alpha;
1:     this.eta = eta;
0:     this.minDfCt = 0;
0:     this.maxDfPct = 1.0f;
1:     corpusWeights = corpus;
1:     numDocuments = corpus.numRows();
1:     this.terms = terms;
1:     this.initialModelCorpusFraction = modelCorpusFraction;
1:     numTerms = terms != null ? terms.length : corpus.numCols();
0:     termIdMap = Maps.newHashMap();
0:     if(terms != null) {
0:       for(int t=0; t<terms.length; t++) {
1:         termIdMap.put(terms[t], t);
1:       }
1:     }
1:     this.numTrainingThreads = numTrainingThreads;
1:     this.numUpdatingThreads = numUpdatingThreads;
1:     postInitCorpus();
1:     initializeModel();
1:   }
1: 
1:   private void postInitCorpus() {
1:     totalCorpusWeight = 0;
1:     int numNonZero = 0;
0:     for(int i=0; i<numDocuments; i++) {
1:       Vector v = corpusWeights.viewRow(i);
1:       double norm;
0:       if(v != null && (norm = v.norm(1)) != 0) {
1:         numNonZero += v.getNumNondefaultElements();
1:         totalCorpusWeight += norm;
1:       }
1:     }
1:     String s = "Initializing corpus with %d docs, %d terms, %d nonzero entries, total termWeight %f";
1:     log.info(String.format(s, numDocuments, numTerms, numNonZero, totalCorpusWeight));
1:   }
1: 
1:   private void initializeModel() {
0:     topicModel = new TopicModel(numTopics, numTerms, eta, alpha, new Random(1234), terms,
0:         numUpdatingThreads,
0:         initialModelCorpusFraction == 0 ? 1 : initialModelCorpusFraction * totalCorpusWeight);
1:     topicModel.setConf(getConf());
1: 
0:     updatedModel = initialModelCorpusFraction == 0
1:         ? new TopicModel(numTopics, numTerms, eta, alpha, null, terms, numUpdatingThreads, 1)
1:         : topicModel;
1:     updatedModel.setConf(getConf());
1:     docTopicCounts = new DenseMatrix(numDocuments, numTopics);
0:     docTopicCounts.assign(1.0/numTopics);
1:     modelTrainer = new ModelTrainer(topicModel, updatedModel, numTrainingThreads, numTopics, numTerms);
1:   }
1: 
1:   private void inferDocuments(double convergence, int maxIter, boolean recalculate) {
0:     for(int docId = 0; docId < corpusWeights.numRows() ; docId++) {
1:       Vector inferredDocument = topicModel.infer(corpusWeights.viewRow(docId),
1:           docTopicCounts.viewRow(docId));
1:       // do what now?
1:     }
1:   }
1: 
1:   public void trainDocuments() {
1:     trainDocuments(0);
1:   }
1: 
1:   public void trainDocuments(double testFraction) {
1:     long start = System.nanoTime();
1:     modelTrainer.start();
0:     for(int docId = 0; docId < corpusWeights.numRows(); docId++) {
0:       if(testFraction == 0 || docId % ((int)1/testFraction) != 0) {
0:         Vector docTopics = new DenseVector(numTopics).assign(1.0/numTopics); // docTopicCounts.getRow(docId)
1:         modelTrainer.trainSync(corpusWeights.viewRow(docId), docTopics , true, 10);
1:       }
1:     }
1:     modelTrainer.stop();
1:     logTime("train documents", System.nanoTime() - start);
1:   }
1: 
1:   private double error(int docId) {
1:     Vector docTermCounts = corpusWeights.viewRow(docId);
0:     if(docTermCounts == null) {
1:       return 0;
1:     } else {
1:       Vector expectedDocTermCounts =
1:           topicModel.infer(corpusWeights.viewRow(docId), docTopicCounts.viewRow(docId));
1:       double expectedNorm = expectedDocTermCounts.norm(1);
1:       return expectedDocTermCounts.times(docTermCounts.norm(1)/expectedNorm)
1:           .minus(docTermCounts).norm(1);
1:     }
1:   }
1: 
1:   private double error() {
1:     long time = System.nanoTime();
1:     double error = 0;
0:     for(int docId = 0; docId < numDocuments; docId++) {
1:       error += error(docId);
1:     }
1:     logTime("error calculation", System.nanoTime() - time);
1:     return error / totalCorpusWeight;
1:   }
1: 
1: 
1: 
1:   public double iterateUntilConvergence(double minFractionalErrorChange,
1:       int maxIterations, int minIter) {
1:     return iterateUntilConvergence(minFractionalErrorChange, maxIterations, minIter, 0);
1:   }
1: 
1:   public double iterateUntilConvergence(double minFractionalErrorChange,
1:       int maxIterations, int minIter, double testFraction) {
0:     double fractionalChange = Double.MAX_VALUE;
1:     int iter = 0;
1:     double oldPerplexity = 0;
0:     while(iter < minIter) {
1:       trainDocuments(testFraction);
0:       if(verbose) {
0:         log.info("model after: " + iter + ": " + modelTrainer.getReadModel().toString());
1:       }
0:       log.info("iteration " + iter + " complete");
1:       oldPerplexity = modelTrainer.calculatePerplexity(corpusWeights, docTopicCounts,
1:           testFraction);
0:       log.info(oldPerplexity + " = perplexity");
1:       iter++;
1:     }
1:     double newPerplexity = 0;
0:     while(iter < maxIterations && fractionalChange > minFractionalErrorChange) {
1:       trainDocuments();
0:       if(verbose) {
0:         log.info("model after: " + iter + ": " + modelTrainer.getReadModel().toString());
1:       }
1:       newPerplexity = modelTrainer.calculatePerplexity(corpusWeights, docTopicCounts,
1:           testFraction);
0:       log.info(newPerplexity + " = perplexity");
1:       iter++;
1:       fractionalChange = Math.abs(newPerplexity - oldPerplexity) / oldPerplexity;
0:       log.info(fractionalChange + " = fractionalChange");
1:       oldPerplexity = newPerplexity;
1:     }
0:     if(iter < maxIterations) {
1:       log.info(String.format("Converged! fractional error change: %f, error %f",
1:           fractionalChange, newPerplexity));
1:     } else {
1:       log.info(String.format("Reached max iteration count (%d), fractional error change: %f, error: %f",
1:           maxIterations, fractionalChange, newPerplexity));
1:     }
1:     return newPerplexity;
1:   }
1: 
1:   public void writeModel(Path outputPath) throws IOException {
1:     modelTrainer.persist(outputPath);
1:   }
1: 
0:   private static final void logTime(String label, long nanos) {
0:     log.info(label + " time: " + (double)(nanos)/1e6 + "ms");
1:   }
1: 
1:   public static int main2(String[] args, Configuration conf) throws Exception {
1:     DefaultOptionBuilder obuilder = new DefaultOptionBuilder();
1:     ArgumentBuilder abuilder = new ArgumentBuilder();
1:     GroupBuilder gbuilder = new GroupBuilder();
1: 
1:     Option helpOpt = DefaultOptionCreator.helpOption();
1: 
1:     Option inputDirOpt = obuilder.withLongName("input").withRequired(true).withArgument(
1:       abuilder.withName("input").withMinimum(1).withMaximum(1).create()).withDescription(
1:       "The Directory on HDFS containing the collapsed, properly formatted files having "
1:           + "one doc per line").withShortName("i").create();
1: 
1:     Option dictOpt = obuilder.withLongName("dictionary").withRequired(false).withArgument(
1:       abuilder.withName("dictionary").withMinimum(1).withMaximum(1).create()).withDescription(
1:       "The path to the term-dictionary format is ... ").withShortName("d").create();
1: 
1:     Option dfsOpt = obuilder.withLongName("dfs").withRequired(false).withArgument(
1:       abuilder.withName("dfs").withMinimum(1).withMaximum(1).create()).withDescription(
1:       "HDFS namenode URI").withShortName("dfs").create();
1: 
1:     Option numTopicsOpt = obuilder.withLongName("numTopics").withRequired(true).withArgument(abuilder
1:         .withName("numTopics").withMinimum(1).withMaximum(1)
1:         .create()).withDescription("Number of topics to learn").withShortName("top").create();
1: 
1:     Option outputTopicFileOpt = obuilder.withLongName("topicOutputFile").withRequired(true).withArgument(
1:         abuilder.withName("topicOutputFile").withMinimum(1).withMaximum(1).create())
1:         .withDescription("File to write out p(term | topic)").withShortName("to").create();
1: 
1:     Option outputDocFileOpt = obuilder.withLongName("docOutputFile").withRequired(true).withArgument(
1:         abuilder.withName("docOutputFile").withMinimum(1).withMaximum(1).create())
1:         .withDescription("File to write out p(topic | docid)").withShortName("do").create();
1: 
1:     Option alphaOpt = obuilder.withLongName("alpha").withRequired(false).withArgument(abuilder
1:         .withName("alpha").withMinimum(1).withMaximum(1).withDefault("0.1").create())
1:         .withDescription("Smoothing parameter for p(topic | document) prior").withShortName("a").create();
1: 
1:     Option etaOpt = obuilder.withLongName("eta").withRequired(false).withArgument(abuilder
1:         .withName("eta").withMinimum(1).withMaximum(1).withDefault("0.1").create())
1:         .withDescription("Smoothing parameter for p(term | topic)").withShortName("e").create();
1: 
1:     Option maxIterOpt = obuilder.withLongName("maxIterations").withRequired(false).withArgument(abuilder
0:         .withName("maxIterations").withMinimum(1).withMaximum(1).withDefault(10).create())
1:         .withDescription("Maximum number of training passes").withShortName("m").create();
1: 
1:     Option modelCorpusFractionOption = obuilder.withLongName("modelCorpusFraction")
1:         .withRequired(false).withArgument(abuilder.withName("modelCorpusFraction").withMinimum(1)
0:         .withMaximum(1).withDefault(0d).create()).withShortName("mcf")
1:         .withDescription("For online updates, initial value of |model|/|corpus|").create();
1: 
1:     Option burnInOpt = obuilder.withLongName("burnInIterations").withRequired(false).withArgument(abuilder
0:         .withName("burnInIterations").withMinimum(1).withMaximum(1).withDefault(5).create())
1:         .withDescription("Minimum number of iterations").withShortName("b").create();
1: 
1:     Option convergenceOpt = obuilder.withLongName("convergence").withRequired(false).withArgument(abuilder
1:         .withName("convergence").withMinimum(1).withMaximum(1).withDefault("0.0").create())
1:         .withDescription("Fractional rate of perplexity to consider convergence").withShortName("c").create();
1: 
1:     Option reInferDocTopicsOpt = obuilder.withLongName("reInferDocTopics").withRequired(false)
1:         .withArgument(abuilder.withName("reInferDocTopics").withMinimum(1).withMaximum(1)
1:         .withDefault("no").create())
1:         .withDescription("re-infer p(topic | doc) : [no | randstart | continue]")
1:         .withShortName("rdt").create();
1: 
1:     Option numTrainThreadsOpt = obuilder.withLongName("numTrainThreads").withRequired(false)
1:         .withArgument(abuilder.withName("numTrainThreads").withMinimum(1).withMaximum(1)
1:         .withDefault("1").create())
1:         .withDescription("number of threads to train with")
1:         .withShortName("ntt").create();
1: 
1:     Option numUpdateThreadsOpt = obuilder.withLongName("numUpdateThreads").withRequired(false)
1:         .withArgument(abuilder.withName("numUpdateThreads").withMinimum(1).withMaximum(1)
1:         .withDefault("1").create())
1:         .withDescription("number of threads to update the model with")
1:         .withShortName("nut").create();
1: 
1:     Option verboseOpt = obuilder.withLongName("verbose").withRequired(false)
1:         .withArgument(abuilder.withName("verbose").withMinimum(1).withMaximum(1)
1:         .withDefault("false").create())
1:         .withDescription("print verbose information, like top-terms in each topic, during iteration")
1:         .withShortName("v").create();
1: 
1:     Group group = gbuilder.withName("Options").withOption(inputDirOpt).withOption(numTopicsOpt)
1:         .withOption(alphaOpt).withOption(etaOpt)
1:         .withOption(maxIterOpt).withOption(burnInOpt).withOption(convergenceOpt)
1:         .withOption(dictOpt).withOption(reInferDocTopicsOpt)
1:         .withOption(outputDocFileOpt).withOption(outputTopicFileOpt).withOption(dfsOpt)
1:         .withOption(numTrainThreadsOpt).withOption(numUpdateThreadsOpt)
1:         .withOption(modelCorpusFractionOption).withOption(verboseOpt).create();
1: 
1:     try {
1:       Parser parser = new Parser();
1: 
1:       parser.setGroup(group);
1:       parser.setHelpOption(helpOpt);
1:       CommandLine cmdLine = parser.parse(args);
1:       if (cmdLine.hasOption(helpOpt)) {
1:         CommandLineUtil.printHelp(group);
1:         return -1;
1:       }
1: 
1:       String inputDirString = (String) cmdLine.getValue(inputDirOpt);
1:       String dictDirString = cmdLine.hasOption(dictOpt) ? (String)cmdLine.getValue(dictOpt) : null;
1:       int numTopics = Integer.parseInt((String) cmdLine.getValue(numTopicsOpt));
1:       double alpha = Double.parseDouble((String)cmdLine.getValue(alphaOpt));
1:       double eta = Double.parseDouble((String)cmdLine.getValue(etaOpt));
1:       int maxIterations = Integer.parseInt((String)cmdLine.getValue(maxIterOpt));
0:       int burnInIterations = (Integer)cmdLine.getValue(burnInOpt);
1:       double minFractionalErrorChange = Double.parseDouble((String) cmdLine.getValue(convergenceOpt));
1:       int numTrainThreads = Integer.parseInt((String)cmdLine.getValue(numTrainThreadsOpt));
1:       int numUpdateThreads = Integer.parseInt((String)cmdLine.getValue(numUpdateThreadsOpt));
1:       String topicOutFile = (String)cmdLine.getValue(outputTopicFileOpt);
1:       String docOutFile = (String)cmdLine.getValue(outputDocFileOpt);
0:       String reInferDocTopics = (String)cmdLine.getValue(reInferDocTopicsOpt);
1:       boolean verbose = Boolean.parseBoolean((String) cmdLine.getValue(verboseOpt));
0:       double modelCorpusFraction = (Double) cmdLine.getValue(modelCorpusFractionOption);
1: 
1:       long start = System.nanoTime();
0:       InMemoryCollapsedVariationalBayes0 cvb0 = null;
1: 
0:       if(conf.get("fs.default.name") == null) {
1:         String dfsNameNode = (String)cmdLine.getValue(dfsOpt);
1:         conf.set("fs.default.name", dfsNameNode);
1:       }
1:       String[] terms = loadDictionary(dictDirString, conf);
1:       logTime("dictionary loading", System.nanoTime() - start);
1:       start = System.nanoTime();
1:       Matrix corpus = loadVectors(inputDirString, conf);
1:       logTime("vector seqfile corpus loading", System.nanoTime() - start);
1:       start = System.nanoTime();
0:       cvb0 = new InMemoryCollapsedVariationalBayes0(corpus, terms, numTopics, alpha, eta,
0:           numTrainThreads, numUpdateThreads, modelCorpusFraction, 1234);
1:       logTime("cvb0 init", System.nanoTime() - start);
1: 
1:       start = System.nanoTime();
1:       cvb0.setVerbose(verbose);
0:       double perplexity = cvb0.iterateUntilConvergence(minFractionalErrorChange, maxIterations,
0:           burnInIterations);
1:       logTime("total training time", System.nanoTime() - start);
1: 
0:       if(reInferDocTopics.equalsIgnoreCase("randstart")) {
1:         cvb0.inferDocuments(0.0, 100, true);
0:       } else if(reInferDocTopics.equalsIgnoreCase("continue")) {
1:         cvb0.inferDocuments(0.0, 100, false);
1:       }
1: 
1:       start = System.nanoTime();
1:       cvb0.writeModel(new Path(topicOutFile));
1:       DistributedRowMatrixWriter.write(new Path(docOutFile), conf, cvb0.docTopicCounts);
1:       logTime("printTopics", System.nanoTime() - start);
1:     } catch (OptionException e) {
1:       log.error("Error while parsing options", e);
1:       CommandLineUtil.printHelp(group);
1:     }
1:     return 0;
1:   }
1: 
0:   private static Map<Integer, Map<String, Integer>> loadCorpus(String path) throws IOException {
0:     List<String> lines = Resources.readLines(Resources.getResource(path), Charsets.UTF_8);
0:     Map<Integer, Map<String, Integer>> corpus = Maps.newHashMap();
0:     for(int i=0; i<lines.size(); i++) {
0:       String line = lines.get(i);
0:       Map<String, Integer> doc = Maps.newHashMap();
0:       for(String s : line.split(" ")) {
0:         s = s.replaceAll("\\W", "").toLowerCase().trim();
0:         if(s.length() == 0) {
0:           continue;
1:         }
0:         if(!doc.containsKey(s)) {
0:           doc.put(s, 0);
1:         }
0:         doc.put(s, doc.get(s) + 1);
1:       }
0:       corpus.put(i, doc);
1:     }
0:     return corpus;
1:   }
1: 
0:   private static String[] loadDictionary(String dictionaryPath, Configuration conf)
1:       throws IOException {
0:     if(dictionaryPath == null) {
1:       return null;
1:     }
1:     Path dictionaryFile = new Path(dictionaryPath);
0:     List<Pair<Integer, String>> termList = Lists.newArrayList();
1:     int maxTermId = 0;
1:      // key is word value is id
1:     for (Pair<Writable, IntWritable> record
1:             : new SequenceFileIterable<Writable, IntWritable>(dictionaryFile, true, conf)) {
0:       termList.add(new Pair<Integer, String>(record.getSecond().get(),
1:           record.getFirst().toString()));
1:       maxTermId = Math.max(maxTermId, record.getSecond().get());
1:     }
1:     String[] terms = new String[maxTermId + 1];
0:     for(Pair<Integer, String> pair : termList) {
1:       terms[pair.getFirst()] = pair.getSecond();
1:     }
1:     return terms;
1:   }
1: 
1:   @Override
1:   public Configuration getConf() {
0:     if(super.getConf() == null) {
0:       setConf(new Configuration());
1:     }
1:     return super.getConf();
1:   }
1: 
1:   private static Matrix loadVectors(String vectorPathString, Configuration conf)
1:     throws IOException {
1:     Path vectorPath = new Path(vectorPathString);
1:     FileSystem fs = vectorPath.getFileSystem(conf);
0:     List<Path> subPaths = Lists.newArrayList();
0:     if(fs.isFile(vectorPath)) {
1:       subPaths.add(vectorPath);
1:     } else {
0:       for(FileStatus fileStatus : fs.listStatus(vectorPath)) {
1:         subPaths.add(fileStatus.getPath());
1:       }
1:     }
0:     List<Vector> vectorList = Lists.newArrayList();
0:     for(Path subPath : subPaths) {
0:       for(Pair<IntWritable, VectorWritable> record
1:           : new SequenceFileIterable<IntWritable, VectorWritable>(subPath, true, conf)) {
0:         vectorList.add(record.getSecond().get());
1:       }
1:     }
0:     int numRows = vectorList.size();
0:     int numCols = vectorList.get(0).size();
0:     return new SparseRowMatrix(numRows, numCols,
0:         vectorList.toArray(new Vector[vectorList.size()]), true,
0:         vectorList.get(0).isSequentialAccess());
1:   }
1: 
1:   @Override
1:   public int run(String[] strings) throws Exception {
1:     return main2(strings, getConf());
1:   }
1: 
1:   public static void main(String[] args) throws Exception {
1:     ToolRunner.run(new InMemoryCollapsedVariationalBayes0(), args);
1:   }
1: }
author:tcp
-------------------------------------------------------------------------------
commit:e64dd36
/////////////////////////////////////////////////////////////////////////
1:     if (terms != null) {
0:       for (int t=0; t<terms.length; t++) {
/////////////////////////////////////////////////////////////////////////
0:     for (int i=0; i<numDocuments; i++) {
1:       if (v != null && (norm = v.norm(1)) != 0) {
/////////////////////////////////////////////////////////////////////////
1:     for (int docId = 0; docId < corpusWeights.numRows() ; docId++) {
/////////////////////////////////////////////////////////////////////////
1:     for (int docId = 0; docId < corpusWeights.numRows(); docId++) {
0:       if (testFraction == 0 || docId % (1/testFraction) != 0) {
/////////////////////////////////////////////////////////////////////////
1:     if (docTermCounts == null) {
/////////////////////////////////////////////////////////////////////////
1:     for (int docId = 0; docId < numDocuments; docId++) {
/////////////////////////////////////////////////////////////////////////
1:     while (iter < minIter) {
1:       if (verbose) {
/////////////////////////////////////////////////////////////////////////
1:     while (iter < maxIterations && fractionalChange > minFractionalErrorChange) {
1:       if (verbose) {
/////////////////////////////////////////////////////////////////////////
1:     if (iter < maxIterations) {
/////////////////////////////////////////////////////////////////////////
1:       if (conf.get("fs.default.name") == null) {
/////////////////////////////////////////////////////////////////////////
1:       } else if ("continue".equalsIgnoreCase(reInferDocTopics)) {
/////////////////////////////////////////////////////////////////////////
0:     for (int i=0; i<lines.size(); i++) {
0:       for (String s : line.split(" ")) {
0:         if (s.length() == 0) {
0:         if (!doc.containsKey(s)) {
/////////////////////////////////////////////////////////////////////////
1:     if (dictionaryPath == null) {
/////////////////////////////////////////////////////////////////////////
1:     for (Pair<Integer, String> pair : termList) {
/////////////////////////////////////////////////////////////////////////
0:     if (super.getConf() == null) {
/////////////////////////////////////////////////////////////////////////
1:     if (fs.isFile(vectorPath)) {
1:       for (FileStatus fileStatus : fs.listStatus(vectorPath, PathFilters.logsCRCFilter())) {
1:     for (Path subPath : subPaths) {
1:       for (Pair<IntWritable, VectorWritable> record
============================================================================