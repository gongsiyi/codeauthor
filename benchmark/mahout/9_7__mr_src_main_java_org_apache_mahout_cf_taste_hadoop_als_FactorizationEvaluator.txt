1:bd5d2f3: /**
1:bd5d2f3:  * Licensed to the Apache Software Foundation (ASF) under one or more
1:bd5d2f3:  * contributor license agreements.  See the NOTICE file distributed with
1:bd5d2f3:  * this work for additional information regarding copyright ownership.
1:bd5d2f3:  * The ASF licenses this file to You under the Apache License, Version 2.0
1:bd5d2f3:  * (the "License"); you may not use this file except in compliance with
1:bd5d2f3:  * the License.  You may obtain a copy of the License at
1:bd5d2f3:  *
1:bd5d2f3:  *     http://www.apache.org/licenses/LICENSE-2.0
1:bd5d2f3:  *
1:bd5d2f3:  * Unless required by applicable law or agreed to in writing, software
1:bd5d2f3:  * distributed under the License is distributed on an "AS IS" BASIS,
1:bd5d2f3:  * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
1:bd5d2f3:  * See the License for the specific language governing permissions and
1:bd5d2f3:  * limitations under the License.
1:bd5d2f3:  */
6:bd5d2f3: 
1:81d64c0: package org.apache.mahout.cf.taste.hadoop.als;
1:bd5d2f3: 
1:87d4b2e: import java.io.BufferedWriter;
1:87d4b2e: import java.io.IOException;
1:87d4b2e: import java.io.OutputStreamWriter;
1:87d4b2e: import java.util.List;
1:87d4b2e: import java.util.Map;
1:87d4b2e: 
1:85f9ece: import org.apache.commons.io.Charsets;
1:bbd2b7e: import org.apache.hadoop.conf.Configuration;
1:bd5d2f3: import org.apache.hadoop.fs.FSDataOutputStream;
1:bd5d2f3: import org.apache.hadoop.fs.FileSystem;
1:bd5d2f3: import org.apache.hadoop.fs.Path;
1:bd5d2f3: import org.apache.hadoop.io.DoubleWritable;
1:bd5d2f3: import org.apache.hadoop.io.LongWritable;
1:bd5d2f3: import org.apache.hadoop.io.NullWritable;
1:bd5d2f3: import org.apache.hadoop.io.Text;
1:bd5d2f3: import org.apache.hadoop.mapreduce.Job;
1:bd5d2f3: import org.apache.hadoop.mapreduce.Mapper;
1:bd5d2f3: import org.apache.hadoop.mapreduce.lib.input.TextInputFormat;
1:bd5d2f3: import org.apache.hadoop.mapreduce.lib.output.SequenceFileOutputFormat;
1:bd5d2f3: import org.apache.hadoop.util.ToolRunner;
1:bd5d2f3: import org.apache.mahout.cf.taste.hadoop.TasteHadoopUtils;
1:bd5d2f3: import org.apache.mahout.cf.taste.impl.common.FullRunningAverage;
1:bd5d2f3: import org.apache.mahout.cf.taste.impl.common.RunningAverage;
1:bd5d2f3: import org.apache.mahout.common.AbstractJob;
1:bd5d2f3: import org.apache.mahout.common.Pair;
1:9101588: import org.apache.mahout.common.iterator.sequencefile.PathFilters;
1:bd5d2f3: import org.apache.mahout.common.iterator.sequencefile.PathType;
1:bd5d2f3: import org.apache.mahout.common.iterator.sequencefile.SequenceFileDirIterable;
1:81d64c0: import org.apache.mahout.math.Vector;
1:81d64c0: import org.apache.mahout.math.map.OpenIntObjectHashMap;
1:bd5d2f3: 
1:bd5d2f3: /**
1:c91c0a5:  * <p>Measures the root-mean-squared error of a rating matrix factorization against a test set.</p>
1:bd5d2f3:  *
1:bd5d2f3:  * <p>Command line arguments specific to this class are:</p>
1:bd5d2f3:  *
1:bd5d2f3:  * <ol>
1:bd5d2f3:  * <li>--output (path): path where output should go</li>
1:bd5d2f3:  * <li>--pairs (path): path containing the test ratings, each line must be userID,itemID,rating</li>
1:bd5d2f3:  * <li>--userFeatures (path): path to the user feature matrix</li>
1:bd5d2f3:  * <li>--itemFeatures (path): path to the item feature matrix</li>
1:bd5d2f3:  * </ol>
1:bd5d2f3:  */
1:81d64c0: public class FactorizationEvaluator extends AbstractJob {
1:81d64c0: 
1:81d64c0:   private static final String USER_FEATURES_PATH = RecommenderJob.class.getName() + ".userFeatures";
1:81d64c0:   private static final String ITEM_FEATURES_PATH = RecommenderJob.class.getName() + ".itemFeatures";
1:bd5d2f3: 
1:bd5d2f3:   public static void main(String[] args) throws Exception {
1:81d64c0:     ToolRunner.run(new FactorizationEvaluator(), args);
5:bd5d2f3:   }
1:bd5d2f3: 
2:bd5d2f3:   @Override
1:bd5d2f3:   public int run(String[] args) throws Exception {
1:bd5d2f3: 
1:81d64c0:     addInputOption();
1:763c94c:     addOption("userFeatures", null, "path to the user feature matrix", true);
1:763c94c:     addOption("itemFeatures", null, "path to the item feature matrix", true);
1:bbd2b7e:     addOption("usesLongIDs", null, "input contains long IDs that need to be translated");
1:bd5d2f3:     addOutputOption();
1:bd5d2f3: 
1:6db7f62:     Map<String,List<String>> parsedArgs = parseArguments(args);
1:bd5d2f3:     if (parsedArgs == null) {
1:bd5d2f3:       return -1;
1:bd5d2f3:     }
1:bd5d2f3: 
1:81d64c0:     Path errors = getTempPath("errors");
1:bd5d2f3: 
1:81d64c0:     Job predictRatings = prepareJob(getInputPath(), errors, TextInputFormat.class, PredictRatingsMapper.class,
1:81d64c0:         DoubleWritable.class, NullWritable.class, SequenceFileOutputFormat.class);
1:6db7f62: 
1:bbd2b7e:     Configuration conf = predictRatings.getConfiguration();
1:bbd2b7e:     conf.set(USER_FEATURES_PATH, getOption("userFeatures"));
1:bbd2b7e:     conf.set(ITEM_FEATURES_PATH, getOption("itemFeatures"));
1:bbd2b7e: 
1:bbd2b7e:     boolean usesLongIDs = Boolean.parseBoolean(getOption("usesLongIDs"));
1:bbd2b7e:     if (usesLongIDs) {
1:bbd2b7e:       conf.set(ParallelALSFactorizationJob.USES_LONG_IDS, String.valueOf(true));
1:bbd2b7e:     }
1:bbd2b7e: 
1:bbd2b7e: 
1:7c2b664:     boolean succeeded = predictRatings.waitForCompletion(true);
1:229aeff:     if (!succeeded) {
1:7c2b664:       return -1;
1:bd5d2f3:     }
1:bd5d2f3: 
1:85f9ece:     FileSystem fs = FileSystem.get(getOutputPath().toUri(), getConf());
1:85f9ece:     FSDataOutputStream outputStream = fs.create(getOutputPath("rmse.txt"));
1:85f9ece:     try (BufferedWriter writer = new BufferedWriter(new OutputStreamWriter(outputStream, Charsets.UTF_8))){
1:bd5d2f3:       double rmse = computeRmse(errors);
1:bd5d2f3:       writer.write(String.valueOf(rmse));
1:bd5d2f3:     }
1:bd5d2f3:     return 0;
1:bd5d2f3:   }
1:bd5d2f3: 
1:e73fdb8:   private double computeRmse(Path errors) {
1:bd5d2f3:     RunningAverage average = new FullRunningAverage();
1:6d16230:     for (Pair<DoubleWritable,NullWritable> entry
1:6d16230:         : new SequenceFileDirIterable<DoubleWritable, NullWritable>(errors, PathType.LIST, PathFilters.logsCRCFilter(),
1:d317c1c:           getConf())) {
1:bd5d2f3:       DoubleWritable error = entry.getFirst();
1:bd5d2f3:       average.addDatum(error.get() * error.get());
1:bd5d2f3:     }
1:bd5d2f3: 
1:bd5d2f3:     return Math.sqrt(average.getAverage());
1:bd5d2f3:   }
1:bd5d2f3: 
1:81d64c0:   public static class PredictRatingsMapper extends Mapper<LongWritable,Text,DoubleWritable,NullWritable> {
1:81d64c0: 
1:81d64c0:     private OpenIntObjectHashMap<Vector> U;
1:81d64c0:     private OpenIntObjectHashMap<Vector> M;
1:81d64c0: 
1:bbd2b7e:     private boolean usesLongIDs;
1:bbd2b7e: 
1:bbd2b7e:     private final DoubleWritable error = new DoubleWritable();
1:bbd2b7e: 
1:81d64c0:     @Override
1:81d64c0:     protected void setup(Context ctx) throws IOException, InterruptedException {
1:bbd2b7e:       Configuration conf = ctx.getConfiguration();
1:81d64c0: 
1:bbd2b7e:       Path pathToU = new Path(conf.get(USER_FEATURES_PATH));
1:bbd2b7e:       Path pathToM = new Path(conf.get(ITEM_FEATURES_PATH));
1:bbd2b7e: 
1:bbd2b7e:       U = ALS.readMatrixByRows(pathToU, conf);
1:bbd2b7e:       M = ALS.readMatrixByRows(pathToM, conf);
1:bbd2b7e: 
1:bbd2b7e:       usesLongIDs = conf.getBoolean(ParallelALSFactorizationJob.USES_LONG_IDS, false);
1:81d64c0:     }
1:81d64c0: 
1:bd5d2f3:     @Override
1:bd5d2f3:     protected void map(LongWritable key, Text value, Context ctx) throws IOException, InterruptedException {
1:81d64c0: 
1:bd5d2f3:       String[] tokens = TasteHadoopUtils.splitPrefTokens(value.toString());
1:bbd2b7e: 
1:bbd2b7e:       int userID = TasteHadoopUtils.readID(tokens[TasteHadoopUtils.USER_ID_POS], usesLongIDs);
1:bbd2b7e:       int itemID = TasteHadoopUtils.readID(tokens[TasteHadoopUtils.ITEM_ID_POS], usesLongIDs);
1:bd5d2f3:       double rating = Double.parseDouble(tokens[2]);
1:bd5d2f3: 
1:81d64c0:       if (U.containsKey(userID) && M.containsKey(itemID)) {
1:81d64c0:         double estimate = U.get(userID).dot(M.get(itemID));
1:bbd2b7e:         error.set(rating - estimate);
1:bbd2b7e:         ctx.write(error, NullWritable.get());
1:81d64c0:       }
1:bd5d2f3:     }
1:bd5d2f3:   }
1:81d64c0: 
1:bd5d2f3: }
============================================================================
author:smarthi
-------------------------------------------------------------------------------
commit:e73fdb8
/////////////////////////////////////////////////////////////////////////
1:   private double computeRmse(Path errors) {
author:Suneel Marthi
-------------------------------------------------------------------------------
commit:85f9ece
/////////////////////////////////////////////////////////////////////////
1: import org.apache.commons.io.Charsets;
/////////////////////////////////////////////////////////////////////////
1:     FileSystem fs = FileSystem.get(getOutputPath().toUri(), getConf());
1:     FSDataOutputStream outputStream = fs.create(getOutputPath("rmse.txt"));
1:     try (BufferedWriter writer = new BufferedWriter(new OutputStreamWriter(outputStream, Charsets.UTF_8))){
author:pferrel
-------------------------------------------------------------------------------
commit:b988c49
author:frankscholten
-------------------------------------------------------------------------------
commit:1a42d85
author:sslavic
-------------------------------------------------------------------------------
commit:c91c0a5
/////////////////////////////////////////////////////////////////////////
1:  * <p>Measures the root-mean-squared error of a rating matrix factorization against a test set.</p>
author:dfilimon
-------------------------------------------------------------------------------
commit:87d4b2e
/////////////////////////////////////////////////////////////////////////
1: import java.io.BufferedWriter;
1: import java.io.IOException;
1: import java.io.OutputStreamWriter;
1: import java.util.List;
1: import java.util.Map;
1: 
/////////////////////////////////////////////////////////////////////////
/////////////////////////////////////////////////////////////////////////
0:       Closeables.close(writer, false);
author:Grant Ingersoll
-------------------------------------------------------------------------------
commit:31cb292
/////////////////////////////////////////////////////////////////////////
0:       Closeables.close(writer, true);
commit:6db7f62
/////////////////////////////////////////////////////////////////////////
0: import java.util.List;
/////////////////////////////////////////////////////////////////////////
1:     Map<String,List<String>> parsedArgs = parseArguments(args);
/////////////////////////////////////////////////////////////////////////
1: 
0:     predictRatings.getConfiguration().set(USER_FEATURES_PATH, getOption("userFeatures"));
0:     predictRatings.getConfiguration().set(ITEM_FEATURES_PATH, getOption("itemFeatures"));
0:     if (!succeeded)
author:Sebastian Schelter
-------------------------------------------------------------------------------
commit:bbd2b7e
/////////////////////////////////////////////////////////////////////////
1: import org.apache.hadoop.conf.Configuration;
/////////////////////////////////////////////////////////////////////////
1:     addOption("usesLongIDs", null, "input contains long IDs that need to be translated");
/////////////////////////////////////////////////////////////////////////
1:     Configuration conf = predictRatings.getConfiguration();
1:     conf.set(USER_FEATURES_PATH, getOption("userFeatures"));
1:     conf.set(ITEM_FEATURES_PATH, getOption("itemFeatures"));
1: 
1:     boolean usesLongIDs = Boolean.parseBoolean(getOption("usesLongIDs"));
1:     if (usesLongIDs) {
1:       conf.set(ParallelALSFactorizationJob.USES_LONG_IDS, String.valueOf(true));
1:     }
1: 
1: 
/////////////////////////////////////////////////////////////////////////
1:     private boolean usesLongIDs;
1: 
1:     private final DoubleWritable error = new DoubleWritable();
1: 
1:       Configuration conf = ctx.getConfiguration();
1:       Path pathToU = new Path(conf.get(USER_FEATURES_PATH));
1:       Path pathToM = new Path(conf.get(ITEM_FEATURES_PATH));
1: 
1:       U = ALS.readMatrixByRows(pathToU, conf);
1:       M = ALS.readMatrixByRows(pathToM, conf);
1: 
1:       usesLongIDs = conf.getBoolean(ParallelALSFactorizationJob.USES_LONG_IDS, false);
1: 
1:       int userID = TasteHadoopUtils.readID(tokens[TasteHadoopUtils.USER_ID_POS], usesLongIDs);
1:       int itemID = TasteHadoopUtils.readID(tokens[TasteHadoopUtils.ITEM_ID_POS], usesLongIDs);
1:         error.set(rating - estimate);
1:         ctx.write(error, NullWritable.get());
commit:6d16230
/////////////////////////////////////////////////////////////////////////
1:     for (Pair<DoubleWritable,NullWritable> entry
1:         : new SequenceFileDirIterable<DoubleWritable, NullWritable>(errors, PathType.LIST, PathFilters.logsCRCFilter(),
commit:5d66758
/////////////////////////////////////////////////////////////////////////
0:   double computeRmse(Path errors) {
commit:63c81f1
/////////////////////////////////////////////////////////////////////////
0:       U = ALS.readMatrixByRows(pathToU, ctx.getConfiguration());
0:       M = ALS.readMatrixByRows(pathToM, ctx.getConfiguration());
commit:d317c1c
/////////////////////////////////////////////////////////////////////////
0:       new SequenceFileDirIterable<DoubleWritable, NullWritable>(errors, PathType.LIST, PathFilters.logsCRCFilter(),
1:           getConf())) {
commit:81d64c0
/////////////////////////////////////////////////////////////////////////
1: package org.apache.mahout.cf.taste.hadoop.als;
/////////////////////////////////////////////////////////////////////////
1: import org.apache.mahout.math.Vector;
1: import org.apache.mahout.math.map.OpenIntObjectHashMap;
/////////////////////////////////////////////////////////////////////////
1: public class FactorizationEvaluator extends AbstractJob {
1: 
1:   private static final String USER_FEATURES_PATH = RecommenderJob.class.getName() + ".userFeatures";
1:   private static final String ITEM_FEATURES_PATH = RecommenderJob.class.getName() + ".itemFeatures";
1:     ToolRunner.run(new FactorizationEvaluator(), args);
1:     addInputOption();
/////////////////////////////////////////////////////////////////////////
1:     Path errors = getTempPath("errors");
1:     Job predictRatings = prepareJob(getInputPath(), errors, TextInputFormat.class, PredictRatingsMapper.class,
1:         DoubleWritable.class, NullWritable.class, SequenceFileOutputFormat.class);
0:     predictRatings.getConfiguration().set(USER_FEATURES_PATH, parsedArgs.get("--userFeatures"));
0:     predictRatings.getConfiguration().set(ITEM_FEATURES_PATH, parsedArgs.get("--itemFeatures"));
0:     predictRatings.waitForCompletion(true);
0:       FSDataOutputStream outputStream = fs.create(getOutputPath("rmse.txt"));
/////////////////////////////////////////////////////////////////////////
1:   public static class PredictRatingsMapper extends Mapper<LongWritable,Text,DoubleWritable,NullWritable> {
1: 
1:     private OpenIntObjectHashMap<Vector> U;
1:     private OpenIntObjectHashMap<Vector> M;
1: 
1:     @Override
1:     protected void setup(Context ctx) throws IOException, InterruptedException {
0:       Path pathToU = new Path(ctx.getConfiguration().get(USER_FEATURES_PATH));
0:       Path pathToM = new Path(ctx.getConfiguration().get(ITEM_FEATURES_PATH));
1: 
0:       U = ALSUtils.readMatrixByRows(pathToU, ctx.getConfiguration());
0:       M = ALSUtils.readMatrixByRows(pathToM, ctx.getConfiguration());
1:     }
1: 
1: 
0:       int userID = Integer.parseInt(tokens[0]);
0:       int itemID = Integer.parseInt(tokens[1]);
1:       if (U.containsKey(userID) && M.containsKey(itemID)) {
1:         double estimate = U.get(userID).dot(M.get(itemID));
0:         double err = rating - estimate;
0:         ctx.write(new DoubleWritable(err), NullWritable.get());
1: 
1: }
commit:346c98c
/////////////////////////////////////////////////////////////////////////
commit:74f849b
/////////////////////////////////////////////////////////////////////////
0: package org.apache.mahout.cf.taste.hadoop.als.eval;
0: import com.google.common.io.Closeables;
/////////////////////////////////////////////////////////////////////////
0:       Closeables.closeQuietly(writer);
commit:bd5d2f3
/////////////////////////////////////////////////////////////////////////
1: /**
1:  * Licensed to the Apache Software Foundation (ASF) under one or more
1:  * contributor license agreements.  See the NOTICE file distributed with
1:  * this work for additional information regarding copyright ownership.
1:  * The ASF licenses this file to You under the Apache License, Version 2.0
1:  * (the "License"); you may not use this file except in compliance with
1:  * the License.  You may obtain a copy of the License at
1:  *
1:  *     http://www.apache.org/licenses/LICENSE-2.0
1:  *
1:  * Unless required by applicable law or agreed to in writing, software
1:  * distributed under the License is distributed on an "AS IS" BASIS,
1:  * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
1:  * See the License for the specific language governing permissions and
1:  * limitations under the License.
1:  */
1: 
0: package org.apache.mahout.utils.eval;
1: 
1: import org.apache.hadoop.fs.FSDataOutputStream;
0: import org.apache.hadoop.fs.FileStatus;
1: import org.apache.hadoop.fs.FileSystem;
1: import org.apache.hadoop.fs.Path;
1: import org.apache.hadoop.io.DoubleWritable;
1: import org.apache.hadoop.io.LongWritable;
1: import org.apache.hadoop.io.NullWritable;
0: import org.apache.hadoop.io.SequenceFile;
1: import org.apache.hadoop.io.Text;
1: import org.apache.hadoop.mapreduce.Job;
1: import org.apache.hadoop.mapreduce.Mapper;
0: import org.apache.hadoop.mapreduce.Reducer;
1: import org.apache.hadoop.mapreduce.lib.input.TextInputFormat;
1: import org.apache.hadoop.mapreduce.lib.output.SequenceFileOutputFormat;
1: import org.apache.hadoop.util.ToolRunner;
1: import org.apache.mahout.cf.taste.hadoop.TasteHadoopUtils;
0: import org.apache.mahout.cf.taste.hadoop.als.PredictionJob;
1: import org.apache.mahout.cf.taste.impl.common.FullRunningAverage;
1: import org.apache.mahout.cf.taste.impl.common.RunningAverage;
1: import org.apache.mahout.common.AbstractJob;
0: import org.apache.mahout.common.IOUtils;
0: import org.apache.mahout.common.IntPairWritable;
1: import org.apache.mahout.common.Pair;
1: import org.apache.mahout.common.iterator.sequencefile.PathType;
1: import org.apache.mahout.common.iterator.sequencefile.SequenceFileDirIterable;
1: 
0: import java.io.BufferedWriter;
0: import java.io.IOException;
0: import java.io.OutputStreamWriter;
0: import java.util.Map;
1: 
1: /**
0:  * <p>Measures the root-mean-squared error of a ratring matrix factorization against a test set.</p>
1:  *
1:  * <p>Command line arguments specific to this class are:</p>
1:  *
1:  * <ol>
1:  * <li>--output (path): path where output should go</li>
1:  * <li>--pairs (path): path containing the test ratings, each line must be userID,itemID,rating</li>
1:  * <li>--userFeatures (path): path to the user feature matrix</li>
1:  * <li>--itemFeatures (path): path to the item feature matrix</li>
1:  * </ol>
1:  */
0: public class ParallelFactorizationEvaluator extends AbstractJob {
1: 
1:   public static void main(String[] args) throws Exception {
0:     ToolRunner.run(new ParallelFactorizationEvaluator(), args);
1:   }
1: 
1:   @Override
1:   public int run(String[] args) throws Exception {
1: 
0:     addOption("pairs", "p", "path containing the test ratings, each line must be userID,itemID,rating", true);
0:     addOption("userFeatures", "u", "path to the user feature matrix", true);
0:     addOption("itemFeatures", "i", "path to the item feature matrix", true);
1:     addOutputOption();
1: 
0:     Map<String,String> parsedArgs = parseArguments(args);
1:     if (parsedArgs == null) {
1:       return -1;
1:     }
1: 
0:     Path tempDir = new Path(parsedArgs.get("--tempDir"));
0:     Path predictions = new Path(tempDir, "predictions");
0:     Path errors = new Path(tempDir, "errors");
1: 
0:     ToolRunner.run(getConf(), new PredictionJob(), new String[] { "--output", predictions.toString(),
0:         "--pairs", parsedArgs.get("--pairs"), "--userFeatures", parsedArgs.get("--userFeatures"),
0:         "--itemFeatures", parsedArgs.get("--itemFeatures"),
0:         "--tempDir", tempDir.toString() });
1: 
0:     Job estimationErrors = prepareJob(new Path(parsedArgs.get("--pairs") + "," + predictions.toString()), errors,
0:         TextInputFormat.class, PairsWithRatingMapper.class, IntPairWritable.class, DoubleWritable.class,
0:         ErrorReducer.class, DoubleWritable.class, NullWritable.class, SequenceFileOutputFormat.class);
0:     estimationErrors.waitForCompletion(true);
1: 
0:     BufferedWriter writer  = null;
0:     try {
0:       FileSystem fs = FileSystem.get(getOutputPath().toUri(), getConf());
0:       FSDataOutputStream outputStream = fs.create(new Path(getOutputPath(), "rmse.txt"));
1:       double rmse = computeRmse(errors);
0:       writer = new BufferedWriter(new OutputStreamWriter(outputStream));
1:       writer.write(String.valueOf(rmse));
0:     } finally {
0:       IOUtils.quietClose(writer);
1:     }
1: 
1:     return 0;
1:   }
1: 
0:   protected double computeRmse(Path errors) {
1:     RunningAverage average = new FullRunningAverage();
0:     for (Pair<DoubleWritable,NullWritable> entry :
0:         new SequenceFileDirIterable<DoubleWritable, NullWritable>(errors, PathType.LIST, getConf())) {
1:       DoubleWritable error = entry.getFirst();
1:       average.addDatum(error.get() * error.get());
1:     }
1: 
1:     return Math.sqrt(average.getAverage());
1:   }
1: 
0:   public static class PairsWithRatingMapper extends Mapper<LongWritable,Text,IntPairWritable,DoubleWritable> {
1:     @Override
1:     protected void map(LongWritable key, Text value, Context ctx) throws IOException, InterruptedException {
1:       String[] tokens = TasteHadoopUtils.splitPrefTokens(value.toString());
0:       int userIDIndex = TasteHadoopUtils.idToIndex(Long.parseLong(tokens[0]));
0:       int itemIDIndex = TasteHadoopUtils.idToIndex(Long.parseLong(tokens[1]));
1:       double rating = Double.parseDouble(tokens[2]);
0:       ctx.write(new IntPairWritable(userIDIndex, itemIDIndex), new DoubleWritable(rating));
1:     }
1:   }
1: 
0:   public static class ErrorReducer extends Reducer<IntPairWritable,DoubleWritable,DoubleWritable,NullWritable> {
1:     @Override
0:     protected void reduce(IntPairWritable key, Iterable<DoubleWritable> ratingAndEstimate, Context ctx)
0:         throws IOException, InterruptedException {
1: 
0:       double error = Double.NaN;
0:       boolean bothFound = false;
0:       for (DoubleWritable ratingOrEstimate : ratingAndEstimate) {
0:         if (Double.isNaN(error)) {
0:           error = ratingOrEstimate.get();
0:         } else {
0:           error -= ratingOrEstimate.get();
0:           bothFound = true;
0:           break;
1:         }
1:       }
1: 
0:       if (bothFound) {
0:         ctx.write(new DoubleWritable(error), NullWritable.get());
1:       }
1:     }
1:   }
1: }
author:Sean R. Owen
-------------------------------------------------------------------------------
commit:229aeff
/////////////////////////////////////////////////////////////////////////
0: import com.google.common.base.Charsets;
/////////////////////////////////////////////////////////////////////////
1:     if (!succeeded) {
0:     }
0:       writer = new BufferedWriter(new OutputStreamWriter(outputStream, Charsets.UTF_8));
commit:7c2b664
/////////////////////////////////////////////////////////////////////////
1:     boolean succeeded = predictRatings.waitForCompletion(true);
0:     if (!succeeded) 
1:       return -1;
commit:763c94c
/////////////////////////////////////////////////////////////////////////
1:     addOption("userFeatures", null, "path to the user feature matrix", true);
1:     addOption("itemFeatures", null, "path to the item feature matrix", true);
commit:9101588
/////////////////////////////////////////////////////////////////////////
1: import org.apache.mahout.common.iterator.sequencefile.PathFilters;
/////////////////////////////////////////////////////////////////////////
0:         new SequenceFileDirIterable<DoubleWritable, NullWritable>(errors,
0:                                                                   PathType.LIST,
0:                                                                   PathFilters.logsCRCFilter(),
0:                                                                   getConf())) {
commit:50fd693
commit:3218e95
/////////////////////////////////////////////////////////////////////////
/////////////////////////////////////////////////////////////////////////
0:     Job estimationErrors = prepareJob(new Path(parsedArgs.get("--pairs") + ',' + predictions), errors,
============================================================================