1:d8e91f9: package org.apache.mahout.vectorizer;
1:d8e91f9: /**
1:d8e91f9:  * Licensed to the Apache Software Foundation (ASF) under one or more
1:d8e91f9:  * contributor license agreements.  See the NOTICE file distributed with
1:d8e91f9:  * this work for additional information regarding copyright ownership.
1:d8e91f9:  * The ASF licenses this file to You under the Apache License, Version 2.0
1:d8e91f9:  * (the "License"); you may not use this file except in compliance with
1:d8e91f9:  * the License.  You may obtain a copy of the License at
1:d8e91f9:  *
1:d8e91f9:  *     http://www.apache.org/licenses/LICENSE-2.0
1:d8e91f9:  *
1:d8e91f9:  * Unless required by applicable law or agreed to in writing, software
1:d8e91f9:  * distributed under the License is distributed on an "AS IS" BASIS,
1:d8e91f9:  * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
1:d8e91f9:  * See the License for the specific language governing permissions and
1:d8e91f9:  * limitations under the License.
1:d8e91f9:  */
2:d8e91f9: 
1:2e5449f: import com.carrotsearch.randomizedtesting.annotations.ThreadLeakScope;
1:58cc1ae: import com.google.common.collect.Lists;
1:d8e91f9: import org.apache.hadoop.conf.Configuration;
1:d8e91f9: import org.apache.hadoop.fs.FileSystem;
1:d8e91f9: import org.apache.hadoop.fs.Path;
1:d8e91f9: import org.apache.hadoop.io.IntWritable;
1:d8e91f9: import org.apache.hadoop.io.SequenceFile;
1:d8e91f9: import org.apache.hadoop.io.Text;
1:e3ec9d8: import org.apache.hadoop.util.ToolRunner;
1:d8e91f9: import org.apache.mahout.common.MahoutTestCase;
1:d8e91f9: import org.apache.mahout.common.Pair;
1:d8e91f9: import org.apache.mahout.common.iterator.sequencefile.PathFilters;
1:d8e91f9: import org.apache.mahout.common.iterator.sequencefile.PathType;
1:d8e91f9: import org.apache.mahout.common.iterator.sequencefile.SequenceFileDirIterable;
1:d8e91f9: import org.apache.mahout.common.iterator.sequencefile.SequenceFileDirValueIterable;
1:d8e91f9: import org.apache.mahout.math.NamedVector;
1:d8e91f9: import org.apache.mahout.math.Vector;
1:d8e91f9: import org.apache.mahout.math.VectorWritable;
1:d8e91f9: import org.junit.Before;
1:d8e91f9: import org.junit.Test;
1:d8e91f9: 
1:d8e91f9: import java.util.Arrays;
1:d8e91f9: import java.util.List;
1:d8e91f9: 
1:2e5449f: @ThreadLeakScope(ThreadLeakScope.Scope.NONE)
1:d8e91f9: public class HighDFWordsPrunerTest extends MahoutTestCase {
1:d8e91f9:   private static final int NUM_DOCS = 100;
1:d8e91f9: 
1:96ea501:   private static final String[] HIGH_DF_WORDS = {"has", "which", "what", "srtyui"};
1:d8e91f9: 
1:d8e91f9:   private Configuration conf;
1:d8e91f9:   private Path inputPath;
1:d8e91f9: 
1:d8e91f9:   @Override
1:d8e91f9:   @Before
1:d8e91f9:   public void setUp() throws Exception {
1:d8e91f9:     super.setUp();
1:e3ec9d8:     conf = getConfiguration();
1:d8e91f9: 
1:d8e91f9:     inputPath = getTestTempFilePath("documents/docs.file");
1:1de8cec:     FileSystem fs = FileSystem.get(inputPath.toUri(), conf);
1:1de8cec: 
1:d8e91f9:     SequenceFile.Writer writer = new SequenceFile.Writer(fs, conf, inputPath, Text.class, Text.class);
1:d8e91f9: 
1:d8e91f9:     RandomDocumentGenerator gen = new RandomDocumentGenerator();
1:d8e91f9: 
1:d8e91f9:     for (int i = 0; i < NUM_DOCS; i++) {
1:d8e91f9:       writer.append(new Text("Document::ID::" + i), new Text(enhanceWithHighDFWords(gen.getRandomDocument())));
1:d8e91f9:     }
1:d8e91f9:     writer.close();
1:d8e91f9:   }
1:d8e91f9: 
1:564c3e1:   private static String enhanceWithHighDFWords(String initialDoc) {
1:d8e91f9:     StringBuilder sb = new StringBuilder(initialDoc);
1:564c3e1:     for (String word : HIGH_DF_WORDS) {
1:564c3e1:       sb.append(' ').append(word);
1:d8e91f9:     }
1:d8e91f9:     return sb.toString();
1:d8e91f9:   }
1:d8e91f9: 
1:d8e91f9: 
1:d8e91f9:   @Test
1:d8e91f9:   public void testHighDFWordsPreserving() throws Exception {
1:d8e91f9:     runTest(false);
1:d8e91f9:   }
1:d8e91f9: 
1:d8e91f9:   @Test
1:d8e91f9:   public void testHighDFWordsPruning() throws Exception {
1:d8e91f9:     runTest(true);
1:d8e91f9:   }
1:d8e91f9: 
1:d8e91f9:   private void runTest(boolean prune) throws Exception {
1:d8e91f9:     Path outputPath = getTestTempFilePath("output");
1:d8e91f9: 
1:58cc1ae:     List<String> argList = Lists.newLinkedList();
1:d8e91f9:     argList.add("-i");
1:d8e91f9:     argList.add(inputPath.toString());
1:d8e91f9:     argList.add("-o");
1:d8e91f9:     argList.add(outputPath.toString());
1:d8e91f9:     if (prune) {
1:d8e91f9:       argList.add("-xs");
1:d8e91f9:       argList.add("3"); // we prune all words that are outside 3*sigma
1:96ea501:     } else {
1:96ea501:       argList.add("--maxDFPercent");
1:96ea501:       argList.add("100"); // the default if, -xs is not specified is to use maxDFPercent, which defaults to 99%
1:d8e91f9:     }
1:d8e91f9: 
1:d8e91f9:     argList.add("-seq");
1:d8e91f9:     argList.add("-nv");
1:d8e91f9: 
1:d8e91f9:     String[] args = argList.toArray(new String[argList.size()]);
1:d8e91f9: 
1:c88c240:     ToolRunner.run(conf, new SparseVectorsFromSequenceFiles(), args);
1:d8e91f9: 
1:d8e91f9:     Path dictionary = new Path(outputPath, "dictionary.file-0");
1:d8e91f9:     Path tfVectors = new Path(outputPath, "tf-vectors");
1:d8e91f9:     Path tfidfVectors = new Path(outputPath, "tfidf-vectors");
1:d8e91f9: 
1:d8e91f9:     int[] highDFWordsDictionaryIndices = getHighDFWordsDictionaryIndices(dictionary);
1:d8e91f9:     validateVectors(tfVectors, highDFWordsDictionaryIndices, prune);
1:d8e91f9:     validateVectors(tfidfVectors, highDFWordsDictionaryIndices, prune);
1:d8e91f9:   }
1:d8e91f9: 
1:d8e91f9:   private int[] getHighDFWordsDictionaryIndices(Path dictionaryPath) {
1:96ea501:     int[] highDFWordsDictionaryIndices = new int[HIGH_DF_WORDS.length];
1:d8e91f9: 
1:96ea501:     List<String> highDFWordsList = Arrays.asList(HIGH_DF_WORDS);
1:d8e91f9: 
1:d8e91f9:     for (Pair<Text, IntWritable> record : new SequenceFileDirIterable<Text, IntWritable>(dictionaryPath, PathType.GLOB,
1:d8e91f9:             null, null, true, conf)) {
1:d8e91f9:       int index = highDFWordsList.indexOf(record.getFirst().toString());
1:d8e91f9:       if (index > -1) {
1:d8e91f9:         highDFWordsDictionaryIndices[index] = record.getSecond().get();
1:d8e91f9:       }
1:d8e91f9:     }
1:d8e91f9: 
1:d8e91f9:     return highDFWordsDictionaryIndices;
1:d8e91f9:   }
1:d8e91f9: 
1:bf1d130:   private void validateVectors(Path vectorPath, int[] highDFWordsDictionaryIndices, boolean prune) throws Exception {
1:c88c240:     assertTrue("Path does not exist", vectorPath.getFileSystem(conf).exists(vectorPath));
1:d8e91f9:     for (VectorWritable value : new SequenceFileDirValueIterable<VectorWritable>(vectorPath, PathType.LIST, PathFilters
1:d8e91f9:             .partFilter(), null, true, conf)) {
1:d8e91f9:       Vector v = ((NamedVector) value.get()).getDelegate();
1:d8e91f9:       for (int i = 0; i < highDFWordsDictionaryIndices.length; i++) {
1:d8e91f9:         if (prune) {
1:564c3e1:           assertEquals("Found vector for which word '" + HIGH_DF_WORDS[i] + "' is not pruned", 0.0, v
1:564c3e1:               .get(highDFWordsDictionaryIndices[i]), 0.0);
1:d8e91f9:         } else {
1:96ea501:           assertTrue("Found vector for which word '" + HIGH_DF_WORDS[i] + "' is pruned, and shouldn't have been", v
1:d8e91f9:                   .get(highDFWordsDictionaryIndices[i]) != 0.0);
1:d8e91f9:         }
1:d8e91f9:       }
1:d8e91f9:     }
1:d8e91f9:   }
1:d8e91f9: }
============================================================================
author:pferrel
-------------------------------------------------------------------------------
commit:b988c49
author:frankscholten
-------------------------------------------------------------------------------
commit:1a42d85
commit:2e5449f
/////////////////////////////////////////////////////////////////////////
1: import com.carrotsearch.randomizedtesting.annotations.ThreadLeakScope;
/////////////////////////////////////////////////////////////////////////
1: @ThreadLeakScope(ThreadLeakScope.Scope.NONE)
author:smarthi
-------------------------------------------------------------------------------
commit:c88c240
/////////////////////////////////////////////////////////////////////////
1:     ToolRunner.run(conf, new SparseVectorsFromSequenceFiles(), args);
/////////////////////////////////////////////////////////////////////////
1:     assertTrue("Path does not exist", vectorPath.getFileSystem(conf).exists(vectorPath));
author:Sebastian Schelter
-------------------------------------------------------------------------------
commit:bf1d130
/////////////////////////////////////////////////////////////////////////
/////////////////////////////////////////////////////////////////////////
1:   private void validateVectors(Path vectorPath, int[] highDFWordsDictionaryIndices, boolean prune) throws Exception {
0:     assertTrue("Path does not exist", vectorPath.getFileSystem(getConfiguration()).exists(vectorPath));
commit:b60c909
/////////////////////////////////////////////////////////////////////////
commit:58cc1ae
/////////////////////////////////////////////////////////////////////////
1: import com.google.common.collect.Lists;
/////////////////////////////////////////////////////////////////////////
1:     List<String> argList = Lists.newLinkedList();
author:Isabel Drost
-------------------------------------------------------------------------------
commit:e3ec9d8
/////////////////////////////////////////////////////////////////////////
1: import org.apache.hadoop.util.ToolRunner;
/////////////////////////////////////////////////////////////////////////
1:     conf = getConfiguration();
/////////////////////////////////////////////////////////////////////////
0:     argList.add("--mapred");
0:     argList.add(getTestTempDir("mapred" + Math.random()).getAbsolutePath());
/////////////////////////////////////////////////////////////////////////
0:     ToolRunner.run(getConfiguration(), new SparseVectorsFromSequenceFiles(), args);
author:Sean R. Owen
-------------------------------------------------------------------------------
commit:1de8cec
/////////////////////////////////////////////////////////////////////////
1:     FileSystem fs = FileSystem.get(inputPath.toUri(), conf);
1: 
commit:564c3e1
/////////////////////////////////////////////////////////////////////////
1:   private static String enhanceWithHighDFWords(String initialDoc) {
1:     for (String word : HIGH_DF_WORDS) {
1:       sb.append(' ').append(word);
/////////////////////////////////////////////////////////////////////////
1:           assertEquals("Found vector for which word '" + HIGH_DF_WORDS[i] + "' is not pruned", 0.0, v
1:               .get(highDFWordsDictionaryIndices[i]), 0.0);
author:Grant Ingersoll
-------------------------------------------------------------------------------
commit:96ea501
/////////////////////////////////////////////////////////////////////////
1:   private static final String[] HIGH_DF_WORDS = {"has", "which", "what", "srtyui"};
/////////////////////////////////////////////////////////////////////////
0:     for (int i = 0; i < HIGH_DF_WORDS.length; i++) {
0:       sb.append(' ').append(HIGH_DF_WORDS[i]);
/////////////////////////////////////////////////////////////////////////
1:     } else {
1:       argList.add("--maxDFPercent");
1:       argList.add("100"); // the default if, -xs is not specified is to use maxDFPercent, which defaults to 99%
/////////////////////////////////////////////////////////////////////////
1:     int[] highDFWordsDictionaryIndices = new int[HIGH_DF_WORDS.length];
1:     List<String> highDFWordsList = Arrays.asList(HIGH_DF_WORDS);
/////////////////////////////////////////////////////////////////////////
0:           assertTrue("Found vector for which word '" + HIGH_DF_WORDS[i] + "' is not pruned", v
1:           assertTrue("Found vector for which word '" + HIGH_DF_WORDS[i] + "' is pruned, and shouldn't have been", v
commit:d8e91f9
/////////////////////////////////////////////////////////////////////////
1: package org.apache.mahout.vectorizer;
1: /**
1:  * Licensed to the Apache Software Foundation (ASF) under one or more
1:  * contributor license agreements.  See the NOTICE file distributed with
1:  * this work for additional information regarding copyright ownership.
1:  * The ASF licenses this file to You under the Apache License, Version 2.0
1:  * (the "License"); you may not use this file except in compliance with
1:  * the License.  You may obtain a copy of the License at
1:  *
1:  *     http://www.apache.org/licenses/LICENSE-2.0
1:  *
1:  * Unless required by applicable law or agreed to in writing, software
1:  * distributed under the License is distributed on an "AS IS" BASIS,
1:  * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
1:  * See the License for the specific language governing permissions and
1:  * limitations under the License.
1:  */
1: 
1: import org.apache.hadoop.conf.Configuration;
1: import org.apache.hadoop.fs.FileSystem;
1: import org.apache.hadoop.fs.Path;
1: import org.apache.hadoop.io.IntWritable;
1: import org.apache.hadoop.io.SequenceFile;
1: import org.apache.hadoop.io.Text;
1: import org.apache.mahout.common.MahoutTestCase;
1: import org.apache.mahout.common.Pair;
1: import org.apache.mahout.common.iterator.sequencefile.PathFilters;
1: import org.apache.mahout.common.iterator.sequencefile.PathType;
1: import org.apache.mahout.common.iterator.sequencefile.SequenceFileDirIterable;
1: import org.apache.mahout.common.iterator.sequencefile.SequenceFileDirValueIterable;
1: import org.apache.mahout.math.NamedVector;
1: import org.apache.mahout.math.Vector;
1: import org.apache.mahout.math.VectorWritable;
1: import org.junit.Before;
1: import org.junit.Test;
1: 
1: import java.util.Arrays;
0: import java.util.LinkedList;
1: import java.util.List;
1: 
1: public class HighDFWordsPrunerTest extends MahoutTestCase {
1:   private static final int NUM_DOCS = 100;
1: 
0:   private static final String[] HIGF_DF_WORDS = {"has", "which", "what", "srtyui"};
1: 
1:   private Configuration conf;
1:   private Path inputPath;
1: 
1:   @Override
1:   @Before
1:   public void setUp() throws Exception {
1:     super.setUp();
0:     conf = new Configuration();
0:     FileSystem fs = FileSystem.get(conf);
1: 
1:     inputPath = getTestTempFilePath("documents/docs.file");
1:     SequenceFile.Writer writer = new SequenceFile.Writer(fs, conf, inputPath, Text.class, Text.class);
1: 
1:     RandomDocumentGenerator gen = new RandomDocumentGenerator();
1: 
1:     for (int i = 0; i < NUM_DOCS; i++) {
1:       writer.append(new Text("Document::ID::" + i), new Text(enhanceWithHighDFWords(gen.getRandomDocument())));
1:     }
1:     writer.close();
1:   }
1: 
0:   private String enhanceWithHighDFWords(String initialDoc) {
1:     StringBuilder sb = new StringBuilder(initialDoc);
0:     for (int i = 0; i < HIGF_DF_WORDS.length; i++) {
0:       sb.append(' ').append(HIGF_DF_WORDS[i]);
1:     }
1: 
1:     return sb.toString();
1:   }
1: 
1: 
1:   @Test
1:   public void testHighDFWordsPreserving() throws Exception {
1:     runTest(false);
1:   }
1: 
1:   @Test
1:   public void testHighDFWordsPruning() throws Exception {
1:     runTest(true);
1:   }
1: 
1:   private void runTest(boolean prune) throws Exception {
1:     Path outputPath = getTestTempFilePath("output");
1: 
0:     List<String> argList = new LinkedList<String>();
1:     argList.add("-i");
1:     argList.add(inputPath.toString());
1:     argList.add("-o");
1:     argList.add(outputPath.toString());
1:     if (prune) {
1:       argList.add("-xs");
1:       argList.add("3"); // we prune all words that are outside 3*sigma
1:     }
1: 
1:     argList.add("-seq");
1:     argList.add("-nv");
1: 
1:     String[] args = argList.toArray(new String[argList.size()]);
1: 
0:     SparseVectorsFromSequenceFiles.main(args);
1: 
1:     Path dictionary = new Path(outputPath, "dictionary.file-0");
1:     Path tfVectors = new Path(outputPath, "tf-vectors");
1:     Path tfidfVectors = new Path(outputPath, "tfidf-vectors");
1: 
1:     int[] highDFWordsDictionaryIndices = getHighDFWordsDictionaryIndices(dictionary);
1:     validateVectors(tfVectors, highDFWordsDictionaryIndices, prune);
1:     validateVectors(tfidfVectors, highDFWordsDictionaryIndices, prune);
1:   }
1: 
1:   private int[] getHighDFWordsDictionaryIndices(Path dictionaryPath) {
0:     int[] highDFWordsDictionaryIndices = new int[HIGF_DF_WORDS.length];
1: 
0:     List<String> highDFWordsList = Arrays.asList(HIGF_DF_WORDS);
1: 
1:     for (Pair<Text, IntWritable> record : new SequenceFileDirIterable<Text, IntWritable>(dictionaryPath, PathType.GLOB,
1:             null, null, true, conf)) {
1:       int index = highDFWordsList.indexOf(record.getFirst().toString());
1:       if (index > -1) {
1:         highDFWordsDictionaryIndices[index] = record.getSecond().get();
1:       }
1:     }
1: 
1:     return highDFWordsDictionaryIndices;
1:   }
1: 
0:   private void validateVectors(Path vectorPath, int[] highDFWordsDictionaryIndices, boolean prune) {
1:     for (VectorWritable value : new SequenceFileDirValueIterable<VectorWritable>(vectorPath, PathType.LIST, PathFilters
1:             .partFilter(), null, true, conf)) {
1:       Vector v = ((NamedVector) value.get()).getDelegate();
1:       for (int i = 0; i < highDFWordsDictionaryIndices.length; i++) {
1:         if (prune) {
0:           assertTrue("Found vector for which word " + HIGF_DF_WORDS[i] + " is not pruned", v
0:                   .get(highDFWordsDictionaryIndices[i]) == 0.0);
1:         } else {
0:           assertTrue("Found vector for which word " + HIGF_DF_WORDS[i] + " is pruned, and shouldn't have been", v
1:                   .get(highDFWordsDictionaryIndices[i]) != 0.0);
1:         }
1:       }
1:     }
1:   }
1: }
============================================================================