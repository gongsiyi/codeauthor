1:af775ba: /**
1:af775ba:  * Licensed to the Apache Software Foundation (ASF) under one or more
1:af775ba:  * contributor license agreements.  See the NOTICE file distributed with
1:af775ba:  * this work for additional information regarding copyright ownership.
1:af775ba:  * The ASF licenses this file to You under the Apache License, Version 2.0
1:af775ba:  * (the "License"); you may not use this file except in compliance with
1:af775ba:  * the License.  You may obtain a copy of the License at
2:af775ba:  *
1:af775ba:  *     http://www.apache.org/licenses/LICENSE-2.0
1:af775ba:  *
1:af775ba:  * Unless required by applicable law or agreed to in writing, software
1:af775ba:  * distributed under the License is distributed on an "AS IS" BASIS,
1:af775ba:  * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
1:af775ba:  * See the License for the specific language governing permissions and
1:af775ba:  * limitations under the License.
2:af775ba:  */
1:81d64c0: 
1:af775ba: package org.apache.mahout.cf.taste.hadoop.als;
1:e200147: 
1:af775ba: import java.io.IOException;
1:af775ba: import java.util.List;
1:af775ba: import java.util.Map;
1:af775ba: import java.util.Random;
1:3631151: 
1:bbd2b7e: import com.google.common.base.Preconditions;
1:3631151: import org.apache.hadoop.conf.Configuration;
1:97f6db2: import org.apache.hadoop.filecache.DistributedCache;
1:97f6db2: import org.apache.hadoop.fs.FileStatus;
1:81d64c0: import org.apache.hadoop.fs.FileSystem;
1:af775ba: import org.apache.hadoop.fs.Path;
1:af775ba: import org.apache.hadoop.io.IntWritable;
1:af775ba: import org.apache.hadoop.io.LongWritable;
1:81d64c0: import org.apache.hadoop.io.SequenceFile;
1:af775ba: import org.apache.hadoop.io.Text;
1:97f6db2: import org.apache.hadoop.io.WritableComparable;
1:af775ba: import org.apache.hadoop.mapreduce.Job;
1:af775ba: import org.apache.hadoop.mapreduce.Mapper;
1:97f6db2: import org.apache.hadoop.mapreduce.Reducer;
1:af775ba: import org.apache.hadoop.mapreduce.lib.input.SequenceFileInputFormat;
1:af775ba: import org.apache.hadoop.mapreduce.lib.input.TextInputFormat;
1:63c81f1: import org.apache.hadoop.mapreduce.lib.map.MultithreadedMapper;
1:af775ba: import org.apache.hadoop.mapreduce.lib.output.SequenceFileOutputFormat;
1:af775ba: import org.apache.hadoop.util.ToolRunner;
1:af775ba: import org.apache.mahout.cf.taste.hadoop.TasteHadoopUtils;
1:af775ba: import org.apache.mahout.cf.taste.impl.common.FullRunningAverage;
1:af775ba: import org.apache.mahout.cf.taste.impl.common.RunningAverage;
1:af775ba: import org.apache.mahout.common.AbstractJob;
1:b16c260: import org.apache.mahout.common.RandomUtils;
1:97f6db2: import org.apache.mahout.common.iterator.sequencefile.PathFilters;
1:81d64c0: import org.apache.mahout.common.mapreduce.MergeVectorsCombiner;
1:81d64c0: import org.apache.mahout.common.mapreduce.MergeVectorsReducer;
1:81d64c0: import org.apache.mahout.common.mapreduce.TransposeMapper;
1:e90d901: import org.apache.mahout.common.mapreduce.VectorSumCombiner;
1:b16c260: import org.apache.mahout.math.DenseVector;
1:b16c260: import org.apache.mahout.math.RandomAccessSparseVector;
1:b16c260: import org.apache.mahout.math.SequentialAccessSparseVector;
1:bbd2b7e: import org.apache.mahout.math.VarIntWritable;
1:bbd2b7e: import org.apache.mahout.math.VarLongWritable;
1:b16c260: import org.apache.mahout.math.Vector;
1:b16c260: import org.apache.mahout.math.VectorWritable;
1:e90d901: import org.apache.mahout.math.hadoop.similarity.cooccurrence.Vectors;
1:d317c1c: import org.slf4j.Logger;
1:d317c1c: import org.slf4j.LoggerFactory;
1:3631151: 
1:af775ba: /**
1:3631151:  * <p>MapReduce implementation of the two factorization algorithms described in
1:3631151:  *
1:3631151:  * <p>"Large-scale Parallel Collaborative Filtering for the Netﬂix Prize" available at
1:3218e95:  * http://www.hpl.hp.com/personal/Robert_Schreiber/papers/2008%20AAIM%20Netflix/netflix_aaim08(submitted).pdf.</p>
1:81d64c0:  *
1:3631151:  * "<p>Collaborative Filtering for Implicit Feedback Datasets" available at
1:3631151:  * http://research.yahoo.com/pub/2433</p>
1:af775ba:  *
1:3631151:  * </p>
1:af775ba:  * <p>Command line arguments specific to this class are:</p>
1:af775ba:  *
1:af775ba:  * <ol>
1:af775ba:  * <li>--input (path): Directory containing one or more text files with the dataset</li>
1:af775ba:  * <li>--output (path): path where output should go</li>
1:af775ba:  * <li>--lambda (double): regularization parameter to avoid overfitting</li>
1:81d64c0:  * <li>--userFeatures (path): path to the user feature matrix</li>
1:81d64c0:  * <li>--itemFeatures (path): path to the item feature matrix</li>
1:df39ce4:  * <li>--numThreadsPerSolver (int): threads to use per solver mapper, (default: 1)</li>
1:af775ba:  * </ol>
1:81d64c0:  */
1:af775ba: public class ParallelALSFactorizationJob extends AbstractJob {
1:81d64c0: 
1:d317c1c:   private static final Logger log = LoggerFactory.getLogger(ParallelALSFactorizationJob.class);
1:d317c1c: 
1:af775ba:   static final String NUM_FEATURES = ParallelALSFactorizationJob.class.getName() + ".numFeatures";
1:af775ba:   static final String LAMBDA = ParallelALSFactorizationJob.class.getName() + ".lambda";
1:3631151:   static final String ALPHA = ParallelALSFactorizationJob.class.getName() + ".alpha";
1:97f6db2:   static final String NUM_ENTITIES = ParallelALSFactorizationJob.class.getName() + ".numEntities";
1:81d64c0: 
1:bbd2b7e:   static final String USES_LONG_IDS = ParallelALSFactorizationJob.class.getName() + ".usesLongIDs";
1:bbd2b7e:   static final String TOKEN_POS = ParallelALSFactorizationJob.class.getName() + ".tokenPos";
1:bbd2b7e: 
1:3631151:   private boolean implicitFeedback;
1:81d64c0:   private int numIterations;
1:81d64c0:   private int numFeatures;
1:81d64c0:   private double lambda;
1:3631151:   private double alpha;
1:63c81f1:   private int numThreadsPerSolver;
1:97f6db2: 
1:97f6db2:   enum Stats { NUM_USERS }
1:97f6db2: 
1:af775ba:   public static void main(String[] args) throws Exception {
1:af775ba:     ToolRunner.run(new ParallelALSFactorizationJob(), args);
26:af775ba:   }
1:97f6db2: 
1:3631151:   @Override
1:af775ba:   public int run(String[] args) throws Exception {
1:97f6db2: 
1:af775ba:     addInputOption();
1:af775ba:     addOutputOption();
1:3631151:     addOption("lambda", null, "regularization parameter", true);
1:3631151:     addOption("implicitFeedback", null, "data consists of implicit feedback?", String.valueOf(false));
1:3631151:     addOption("alpha", null, "confidence parameter (only used on implicit feedback)", String.valueOf(40));
1:3631151:     addOption("numFeatures", null, "dimension of the feature space", true);
1:763c94c:     addOption("numIterations", null, "number of iterations", true);
1:63c81f1:     addOption("numThreadsPerSolver", null, "threads per solver mapper", String.valueOf(1));
1:bbd2b7e:     addOption("usesLongIDs", null, "input contains long IDs that need to be translated");
1:63c81f1: 
1:6db7f62:     Map<String,List<String>> parsedArgs = parseArguments(args);
1:af775ba:     if (parsedArgs == null) {
1:af775ba:       return -1;
1:63c81f1:     }
1:63c81f1: 
1:6db7f62:     numFeatures = Integer.parseInt(getOption("numFeatures"));
1:6db7f62:     numIterations = Integer.parseInt(getOption("numIterations"));
1:6db7f62:     lambda = Double.parseDouble(getOption("lambda"));
1:6db7f62:     alpha = Double.parseDouble(getOption("alpha"));
1:6db7f62:     implicitFeedback = Boolean.parseBoolean(getOption("implicitFeedback"));
1:e200147: 
1:63c81f1:     numThreadsPerSolver = Integer.parseInt(getOption("numThreadsPerSolver"));
1:87c15be:     boolean usesLongIDs = Boolean.parseBoolean(getOption("usesLongIDs", String.valueOf(false)));
1:63c81f1: 
1:81d64c0:     /*
1:63c81f1:     * compute the factorization A = U M'
1:63c81f1:     *
1:63c81f1:     * where A (users x items) is the matrix of known ratings
1:63c81f1:     *           U (users x features) is the representation of users in the feature space
1:63c81f1:     *           M (items x features) is the representation of items in the feature space
1:63c81f1:     */
1:e200147: 
1:bbd2b7e:     if (usesLongIDs) {
1:bbd2b7e:       Job mapUsers = prepareJob(getInputPath(), getOutputPath("userIDIndex"), TextInputFormat.class,
1:bbd2b7e:           MapLongIDsMapper.class, VarIntWritable.class, VarLongWritable.class, IDMapReducer.class,
1:bbd2b7e:           VarIntWritable.class, VarLongWritable.class, SequenceFileOutputFormat.class);
1:bbd2b7e:       mapUsers.getConfiguration().set(TOKEN_POS, String.valueOf(TasteHadoopUtils.USER_ID_POS));
1:bbd2b7e:       mapUsers.waitForCompletion(true);
1:bbd2b7e: 
1:bbd2b7e:       Job mapItems = prepareJob(getInputPath(), getOutputPath("itemIDIndex"), TextInputFormat.class,
1:bbd2b7e:           MapLongIDsMapper.class, VarIntWritable.class, VarLongWritable.class, IDMapReducer.class,
1:bbd2b7e:           VarIntWritable.class, VarLongWritable.class, SequenceFileOutputFormat.class);
1:bbd2b7e:       mapItems.getConfiguration().set(TOKEN_POS, String.valueOf(TasteHadoopUtils.ITEM_ID_POS));
1:bbd2b7e:       mapItems.waitForCompletion(true);
1:bbd2b7e:     }
1:bbd2b7e: 
1:81d64c0:    /* create A' */
1:af775ba:     Job itemRatings = prepareJob(getInputPath(), pathToItemRatings(),
1:81d64c0:         TextInputFormat.class, ItemRatingVectorsMapper.class, IntWritable.class,
1:81d64c0:         VectorWritable.class, VectorSumReducer.class, IntWritable.class,
2:81d64c0:         VectorWritable.class, SequenceFileOutputFormat.class);
1:d40bf86:     itemRatings.setCombinerClass(VectorSumCombiner.class);
1:bbd2b7e:     itemRatings.getConfiguration().set(USES_LONG_IDS, String.valueOf(usesLongIDs));
1:7c2b664:     boolean succeeded = itemRatings.waitForCompletion(true);
1:229aeff:     if (!succeeded) {
1:7c2b664:       return -1;
1:63c81f1:     }
1:e200147: 
1:81d64c0:     /* create A */
1:81d64c0:     Job userRatings = prepareJob(pathToItemRatings(), pathToUserRatings(),
1:97f6db2:         TransposeMapper.class, IntWritable.class, VectorWritable.class, MergeUserVectorsReducer.class,
1:97f6db2:         IntWritable.class, VectorWritable.class);
1:81d64c0:     userRatings.setCombinerClass(MergeVectorsCombiner.class);
1:7c2b664:     succeeded = userRatings.waitForCompletion(true);
1:229aeff:     if (!succeeded) {
1:7c2b664:       return -1;
1:3631151:     }
1:3631151: 
1:81d64c0:     //TODO this could be fiddled into one of the upper jobs
1:81d64c0:     Job averageItemRatings = prepareJob(pathToItemRatings(), getTempPath("averageRatings"),
1:81d64c0:         AverageRatingMapper.class, IntWritable.class, VectorWritable.class, MergeVectorsReducer.class,
1:81d64c0:         IntWritable.class, VectorWritable.class);
1:81d64c0:     averageItemRatings.setCombinerClass(MergeVectorsCombiner.class);
1:7c2b664:     succeeded = averageItemRatings.waitForCompletion(true);
1:229aeff:     if (!succeeded) {
1:7c2b664:       return -1;
1:af775ba:     }
1:3631151: 
1:63c81f1:     Vector averageRatings = ALS.readFirstRow(getTempPath("averageRatings"), getConf());
1:97f6db2: 
1:87c15be:     int numItems = averageRatings.getNumNondefaultElements();
1:87c15be:     int numUsers = (int) userRatings.getCounters().findCounter(Stats.NUM_USERS).getValue();
1:3631151: 
1:97f6db2:     log.info("Found {} users and {} items", numUsers, numItems);
1:97f6db2: 
1:81d64c0:     /* create an initial M */
1:81d64c0:     initializeM(averageRatings);
1:3631151: 
1:81d64c0:     for (int currentIteration = 0; currentIteration < numIterations; currentIteration++) {
1:81d64c0:       /* broadcast M, read A row-wise, recompute U row-wise */
1:d317c1c:       log.info("Recomputing U (iteration {}/{})", currentIteration, numIterations);
1:97f6db2:       runSolver(pathToUserRatings(), pathToU(currentIteration), pathToM(currentIteration - 1), currentIteration, "U",
1:87c15be:           numItems);
1:3631151:       /* broadcast U, read A' row-wise, recompute M row-wise */
1:d317c1c:       log.info("Recomputing M (iteration {}/{})", currentIteration, numIterations);
1:97f6db2:       runSolver(pathToItemRatings(), pathToM(currentIteration), pathToU(currentIteration), currentIteration, "M",
1:87c15be:           numUsers);
1:af775ba:     }
1:3631151: 
1:af775ba:     return 0;
1:af775ba:   }
1:3631151: 
1:81d64c0:   private void initializeM(Vector averageRatings) throws IOException {
1:81d64c0:     Random random = RandomUtils.getRandom();
1:81d64c0: 
1:81d64c0:     FileSystem fs = FileSystem.get(pathToM(-1).toUri(), getConf());
1:85f9ece:     try (SequenceFile.Writer writer =
1:85f9ece:              new SequenceFile.Writer(fs, getConf(), new Path(pathToM(-1), "part-m-00000"),
1:85f9ece:                  IntWritable.class, VectorWritable.class)) {
1:df39ce4:       IntWritable index = new IntWritable();
1:df39ce4:       VectorWritable featureVector = new VectorWritable();
1:df39ce4: 
1:dc62944:       for (Vector.Element e : averageRatings.nonZeroes()) {
1:81d64c0:         Vector row = new DenseVector(numFeatures);
1:81d64c0:         row.setQuick(0, e.get());
1:81d64c0:         for (int m = 1; m < numFeatures; m++) {
1:81d64c0:           row.setQuick(m, random.nextDouble());
1:81d64c0:         }
1:df39ce4:         index.set(e.index());
1:df39ce4:         featureVector.set(row);
1:df39ce4:         writer.append(index, featureVector);
1:97f6db2:       }
1:97f6db2:     }
1:97f6db2:   }
1:97f6db2: 
1:97f6db2:   static class VectorSumReducer
2:97f6db2:       extends Reducer<WritableComparable<?>, VectorWritable, WritableComparable<?>, VectorWritable> {
1:97f6db2: 
2:97f6db2:     private final VectorWritable result = new VectorWritable();
1:97f6db2: 
1:97f6db2:     @Override
2:97f6db2:     protected void reduce(WritableComparable<?> key, Iterable<VectorWritable> values, Context ctx)
1:58cc1ae:       throws IOException, InterruptedException {
1:e90d901:       Vector sum = Vectors.sum(values.iterator());
1:97f6db2:       result.set(new SequentialAccessSparseVector(sum));
2:97f6db2:       ctx.write(key, result);
1:97f6db2:     }
1:97f6db2:   }
1:97f6db2: 
1:97f6db2:   static class MergeUserVectorsReducer extends
1:97f6db2:       Reducer<WritableComparable<?>,VectorWritable,WritableComparable<?>,VectorWritable> {
1:97f6db2: 
1:97f6db2:     private final VectorWritable result = new VectorWritable();
1:97f6db2: 
1:97f6db2:     @Override
1:97f6db2:     public void reduce(WritableComparable<?> key, Iterable<VectorWritable> vectors, Context ctx)
1:58cc1ae:       throws IOException, InterruptedException {
1:97f6db2:       Vector merged = VectorWritable.merge(vectors.iterator()).get();
1:97f6db2:       result.set(new SequentialAccessSparseVector(merged));
1:97f6db2:       ctx.write(key, result);
1:97f6db2:       ctx.getCounter(Stats.NUM_USERS).increment(1);
1:97f6db2:     }
1:3631151:   }
1:97f6db2: 
1:81d64c0:   static class ItemRatingVectorsMapper extends Mapper<LongWritable,Text,IntWritable,VectorWritable> {
1:e200147: 
1:10c535c:     private final IntWritable itemIDWritable = new IntWritable();
1:10c535c:     private final VectorWritable ratingsWritable = new VectorWritable(true);
1:97f6db2:     private final Vector ratings = new RandomAccessSparseVector(Integer.MAX_VALUE, 1);
1:e200147: 
1:bbd2b7e:     private boolean usesLongIDs;
1:bbd2b7e: 
1:bbd2b7e:     @Override
1:bbd2b7e:     protected void setup(Context ctx) throws IOException, InterruptedException {
1:bbd2b7e:       usesLongIDs = ctx.getConfiguration().getBoolean(USES_LONG_IDS, false);
1:bbd2b7e:     }
1:bbd2b7e: 
1:97f6db2:     @Override
1:af775ba:     protected void map(LongWritable offset, Text line, Context ctx) throws IOException, InterruptedException {
1:af775ba:       String[] tokens = TasteHadoopUtils.splitPrefTokens(line.toString());
1:bbd2b7e:       int userID = TasteHadoopUtils.readID(tokens[TasteHadoopUtils.USER_ID_POS], usesLongIDs);
1:bbd2b7e:       int itemID = TasteHadoopUtils.readID(tokens[TasteHadoopUtils.ITEM_ID_POS], usesLongIDs);
1:af775ba:       float rating = Float.parseFloat(tokens[2]);
1:e200147: 
1:e200147:       ratings.setQuick(userID, rating);
1:3631151: 
1:e200147:       itemIDWritable.set(itemID);
1:e200147:       ratingsWritable.set(ratings);
1:e200147: 
1:e200147:       ctx.write(itemIDWritable, ratingsWritable);
1:e200147: 
1:e200147:       // prepare instance for reuse
1:e200147:       ratings.setQuick(userID, 0.0d);
1:3631151:     }
1:81d64c0:   }
1:3631151: 
1:97f6db2:   private void runSolver(Path ratings, Path output, Path pathToUorM, int currentIteration, String matrixName,
1:97f6db2:                          int numEntities) throws ClassNotFoundException, IOException, InterruptedException {
1:97f6db2: 
1:97f6db2:     // necessary for local execution in the same JVM only
1:97f6db2:     SharingMapper.reset();
1:3631151: 
1:5d66758:     Class<? extends Mapper<IntWritable,VectorWritable,IntWritable,VectorWritable>> solverMapperClassInternal;
1:5d66758:     String name;
1:81d64c0: 
1:df39ce4:     if (implicitFeedback) {
1:5d66758:       solverMapperClassInternal = SolveImplicitFeedbackMapper.class;
1:f75ea67:       name = "Recompute " + matrixName + ", iteration (" + currentIteration + '/' + numIterations + "), "
1:58cc1ae:           + '(' + numThreadsPerSolver + " threads, " + numFeatures + " features, implicit feedback)";
3:63c81f1:     } else {
1:5d66758:       solverMapperClassInternal = SolveExplicitFeedbackMapper.class;
1:f75ea67:       name = "Recompute " + matrixName + ", iteration (" + currentIteration + '/' + numIterations + "), "
1:4ca6b86:           + '(' + numThreadsPerSolver + " threads, " + numFeatures + " features, explicit feedback)";
1:63c81f1:     }
1:63c81f1: 
1:df39ce4:     Job solverForUorI = prepareJob(ratings, output, SequenceFileInputFormat.class, MultithreadedSharingMapper.class,
1:df39ce4:         IntWritable.class, VectorWritable.class, SequenceFileOutputFormat.class, name);
1:3631151:     Configuration solverConf = solverForUorI.getConfiguration();
1:3631151:     solverConf.set(LAMBDA, String.valueOf(lambda));
1:3631151:     solverConf.set(ALPHA, String.valueOf(alpha));
1:3631151:     solverConf.setInt(NUM_FEATURES, numFeatures);
1:97f6db2:     solverConf.set(NUM_ENTITIES, String.valueOf(numEntities));
1:97f6db2: 
1:97f6db2:     FileSystem fs = FileSystem.get(pathToUorM.toUri(), solverConf);
1:97f6db2:     FileStatus[] parts = fs.listStatus(pathToUorM, PathFilters.partFilter());
1:97f6db2:     for (FileStatus part : parts) {
1:97f6db2:       if (log.isDebugEnabled()) {
1:97f6db2:         log.debug("Adding {} to distributed cache", part.getPath().toString());
1:97f6db2:       }
1:97f6db2:       DistributedCache.addCacheFile(part.getPath().toUri(), solverConf);
1:97f6db2:     }
1:63c81f1: 
1:df39ce4:     MultithreadedMapper.setMapperClass(solverForUorI, solverMapperClassInternal);
1:df39ce4:     MultithreadedMapper.setNumberOfThreads(solverForUorI, numThreadsPerSolver);
1:63c81f1: 
1:7c2b664:     boolean succeeded = solverForUorI.waitForCompletion(true);
1:229aeff:     if (!succeeded) {
1:7c2b664:       throw new IllegalStateException("Job failed!");
1:63c81f1:     }
1:af775ba:   }
1:81d64c0: 
1:81d64c0:   static class AverageRatingMapper extends Mapper<IntWritable,VectorWritable,IntWritable,VectorWritable> {
1:e200147: 
1:10c535c:     private final IntWritable firstIndex = new IntWritable(0);
1:10c535c:     private final Vector featureVector = new RandomAccessSparseVector(Integer.MAX_VALUE, 1);
1:10c535c:     private final VectorWritable featureVectorWritable = new VectorWritable();
1:e200147: 
1:3631151:     @Override
1:81d64c0:     protected void map(IntWritable r, VectorWritable v, Context ctx) throws IOException, InterruptedException {
1:81d64c0:       RunningAverage avg = new FullRunningAverage();
1:dc62944:       for (Vector.Element e : v.get().nonZeroes()) {
1:dc62944:         avg.addDatum(e.get());
1:81d64c0:       }
1:81d64c0: 
1:e200147:       featureVector.setQuick(r.get(), avg.getAverage());
1:e200147:       featureVectorWritable.set(featureVector);
1:e200147:       ctx.write(firstIndex, featureVectorWritable);
1:e200147: 
1:e200147:       // prepare instance for reuse
1:e200147:       featureVector.setQuick(r.get(), 0.0d);
1:81d64c0:     }
1:af775ba:   }
1:81d64c0: 
1:bbd2b7e:   static class MapLongIDsMapper extends Mapper<LongWritable,Text,VarIntWritable,VarLongWritable> {
1:bbd2b7e: 
1:bbd2b7e:     private int tokenPos;
1:335a993:     private final VarIntWritable index = new VarIntWritable();
1:335a993:     private final VarLongWritable idWritable = new VarLongWritable();
1:bbd2b7e: 
1:bbd2b7e:     @Override
1:bbd2b7e:     protected void setup(Context ctx) throws IOException, InterruptedException {
1:bbd2b7e:       tokenPos = ctx.getConfiguration().getInt(TOKEN_POS, -1);
1:bbd2b7e:       Preconditions.checkState(tokenPos >= 0);
1:bbd2b7e:     }
1:bbd2b7e: 
1:bbd2b7e:     @Override
1:bbd2b7e:     protected void map(LongWritable key, Text line, Context ctx) throws IOException, InterruptedException {
1:bbd2b7e:       String[] tokens = TasteHadoopUtils.splitPrefTokens(line.toString());
1:bbd2b7e: 
1:bbd2b7e:       long id = Long.parseLong(tokens[tokenPos]);
1:bbd2b7e: 
1:bbd2b7e:       index.set(TasteHadoopUtils.idToIndex(id));
1:bbd2b7e:       idWritable.set(id);
1:bbd2b7e:       ctx.write(index, idWritable);
1:bbd2b7e:     }
1:bbd2b7e:   }
1:bbd2b7e: 
1:bbd2b7e:   static class IDMapReducer extends Reducer<VarIntWritable,VarLongWritable,VarIntWritable,VarLongWritable> {
1:bbd2b7e:     @Override
1:bbd2b7e:     protected void reduce(VarIntWritable index, Iterable<VarLongWritable> ids, Context ctx)
1:58cc1ae:       throws IOException, InterruptedException {
1:bbd2b7e:       ctx.write(index, ids.iterator().next());
1:bbd2b7e:     }
1:bbd2b7e:   }
1:bbd2b7e: 
1:af775ba:   private Path pathToM(int iteration) {
1:81d64c0:     return iteration == numIterations - 1 ? getOutputPath("M") : getTempPath("M-" + iteration);
1:af775ba:   }
1:81d64c0: 
1:af775ba:   private Path pathToU(int iteration) {
1:81d64c0:     return iteration == numIterations - 1 ? getOutputPath("U") : getTempPath("U-" + iteration);
1:af775ba:   }
1:81d64c0: 
1:af775ba:   private Path pathToItemRatings() {
1:81d64c0:     return getTempPath("itemRatings");
1:af775ba:   }
1:81d64c0: 
1:af775ba:   private Path pathToUserRatings() {
1:81d64c0:     return getOutputPath("userRatings");
1:af775ba:   }
1:af775ba: }
============================================================================
author:Suneel Marthi
-------------------------------------------------------------------------------
commit:85f9ece
/////////////////////////////////////////////////////////////////////////
/////////////////////////////////////////////////////////////////////////
1:     try (SequenceFile.Writer writer =
1:              new SequenceFile.Writer(fs, getConf(), new Path(pathToM(-1), "part-m-00000"),
1:                  IntWritable.class, VectorWritable.class)) {
/////////////////////////////////////////////////////////////////////////
commit:87c15be
/////////////////////////////////////////////////////////////////////////
/////////////////////////////////////////////////////////////////////////
1:     boolean usesLongIDs = Boolean.parseBoolean(getOption("usesLongIDs", String.valueOf(false)));
/////////////////////////////////////////////////////////////////////////
1:     int numItems = averageRatings.getNumNondefaultElements();
1:     int numUsers = (int) userRatings.getCounters().findCounter(Stats.NUM_USERS).getValue();
/////////////////////////////////////////////////////////////////////////
1:           numItems);
1:           numUsers);
author:pferrel
-------------------------------------------------------------------------------
commit:b988c49
author:frankscholten
-------------------------------------------------------------------------------
commit:1a42d85
author:Sebastian Schelter
-------------------------------------------------------------------------------
commit:f75ea67
/////////////////////////////////////////////////////////////////////////
1:       name = "Recompute " + matrixName + ", iteration (" + currentIteration + '/' + numIterations + "), "
1:       name = "Recompute " + matrixName + ", iteration (" + currentIteration + '/' + numIterations + "), "
commit:e90d901
/////////////////////////////////////////////////////////////////////////
1: import org.apache.mahout.common.mapreduce.VectorSumCombiner;
/////////////////////////////////////////////////////////////////////////
1: import org.apache.mahout.math.hadoop.similarity.cooccurrence.Vectors;
/////////////////////////////////////////////////////////////////////////
/////////////////////////////////////////////////////////////////////////
1:       Vector sum = Vectors.sum(values.iterator());
commit:58cc1ae
/////////////////////////////////////////////////////////////////////////
1:       throws IOException, InterruptedException {
/////////////////////////////////////////////////////////////////////////
1:       throws IOException, InterruptedException {
/////////////////////////////////////////////////////////////////////////
1:       throws IOException, InterruptedException {
/////////////////////////////////////////////////////////////////////////
1:           + '(' + numThreadsPerSolver + " threads, " + numFeatures + " features, implicit feedback)";
/////////////////////////////////////////////////////////////////////////
0:       throws IOException, InterruptedException {
commit:bbd2b7e
/////////////////////////////////////////////////////////////////////////
1: import com.google.common.base.Preconditions;
/////////////////////////////////////////////////////////////////////////
1: import org.apache.mahout.math.VarIntWritable;
1: import org.apache.mahout.math.VarLongWritable;
/////////////////////////////////////////////////////////////////////////
1:   static final String USES_LONG_IDS = ParallelALSFactorizationJob.class.getName() + ".usesLongIDs";
1:   static final String TOKEN_POS = ParallelALSFactorizationJob.class.getName() + ".tokenPos";
1: 
1:   private boolean usesLongIDs;
/////////////////////////////////////////////////////////////////////////
1:     addOption("usesLongIDs", null, "input contains long IDs that need to be translated");
/////////////////////////////////////////////////////////////////////////
0:     usesLongIDs = Boolean.parseBoolean(getOption("usesLongIDs", String.valueOf(false)));
/////////////////////////////////////////////////////////////////////////
1:     if (usesLongIDs) {
1:       Job mapUsers = prepareJob(getInputPath(), getOutputPath("userIDIndex"), TextInputFormat.class,
1:           MapLongIDsMapper.class, VarIntWritable.class, VarLongWritable.class, IDMapReducer.class,
1:           VarIntWritable.class, VarLongWritable.class, SequenceFileOutputFormat.class);
1:       mapUsers.getConfiguration().set(TOKEN_POS, String.valueOf(TasteHadoopUtils.USER_ID_POS));
1:       mapUsers.waitForCompletion(true);
1: 
1:       Job mapItems = prepareJob(getInputPath(), getOutputPath("itemIDIndex"), TextInputFormat.class,
1:           MapLongIDsMapper.class, VarIntWritable.class, VarLongWritable.class, IDMapReducer.class,
1:           VarIntWritable.class, VarLongWritable.class, SequenceFileOutputFormat.class);
1:       mapItems.getConfiguration().set(TOKEN_POS, String.valueOf(TasteHadoopUtils.ITEM_ID_POS));
1:       mapItems.waitForCompletion(true);
1:     }
1: 
1:     itemRatings.getConfiguration().set(USES_LONG_IDS, String.valueOf(usesLongIDs));
/////////////////////////////////////////////////////////////////////////
0:     private boolean usesLongIDs;
1: 
1:     @Override
1:     protected void setup(Context ctx) throws IOException, InterruptedException {
1:       usesLongIDs = ctx.getConfiguration().getBoolean(USES_LONG_IDS, false);
1:     }
1: 
1:       int userID = TasteHadoopUtils.readID(tokens[TasteHadoopUtils.USER_ID_POS], usesLongIDs);
1:       int itemID = TasteHadoopUtils.readID(tokens[TasteHadoopUtils.ITEM_ID_POS], usesLongIDs);
/////////////////////////////////////////////////////////////////////////
1:   static class MapLongIDsMapper extends Mapper<LongWritable,Text,VarIntWritable,VarLongWritable> {
1: 
1:     private int tokenPos;
0:     private VarIntWritable index = new VarIntWritable();
0:     private VarLongWritable idWritable = new VarLongWritable();
1: 
1:     @Override
1:     protected void setup(Context ctx) throws IOException, InterruptedException {
1:       tokenPos = ctx.getConfiguration().getInt(TOKEN_POS, -1);
1:       Preconditions.checkState(tokenPos >= 0);
1:     }
1: 
1:     @Override
1:     protected void map(LongWritable key, Text line, Context ctx) throws IOException, InterruptedException {
1:       String[] tokens = TasteHadoopUtils.splitPrefTokens(line.toString());
1: 
1:       long id = Long.parseLong(tokens[tokenPos]);
1: 
1:       index.set(TasteHadoopUtils.idToIndex(id));
1:       idWritable.set(id);
1:       ctx.write(index, idWritable);
1:     }
1:   }
1: 
1:   static class IDMapReducer extends Reducer<VarIntWritable,VarLongWritable,VarIntWritable,VarLongWritable> {
1:     @Override
1:     protected void reduce(VarIntWritable index, Iterable<VarLongWritable> ids, Context ctx)
0:         throws IOException, InterruptedException {
1:       ctx.write(index, ids.iterator().next());
1:     }
1:   }
1: 
commit:d40bf86
/////////////////////////////////////////////////////////////////////////
1:     itemRatings.setCombinerClass(VectorSumCombiner.class);
/////////////////////////////////////////////////////////////////////////
0:       name = "Recompute " + matrixName + ", iteration (" + (iterationNumber + 1) + "/" + numIterations + "), "
0:       name = "Recompute " + matrixName + ", iteration (" + (iterationNumber + 1) + "/" + numIterations + "), "
commit:97f6db2
/////////////////////////////////////////////////////////////////////////
1: import org.apache.hadoop.filecache.DistributedCache;
1: import org.apache.hadoop.fs.FileStatus;
1: import org.apache.hadoop.io.WritableComparable;
1: import org.apache.hadoop.mapreduce.Reducer;
/////////////////////////////////////////////////////////////////////////
1: import org.apache.mahout.common.iterator.sequencefile.PathFilters;
0: import org.apache.mahout.math.function.Functions;
/////////////////////////////////////////////////////////////////////////
1:   static final String NUM_ENTITIES = ParallelALSFactorizationJob.class.getName() + ".numEntities";
/////////////////////////////////////////////////////////////////////////
0:   private int numItems;
0:   private int numUsers;
1: 
1:   enum Stats { NUM_USERS }
1: 
/////////////////////////////////////////////////////////////////////////
1:         TransposeMapper.class, IntWritable.class, VectorWritable.class, MergeUserVectorsReducer.class,
1:         IntWritable.class, VectorWritable.class);
/////////////////////////////////////////////////////////////////////////
0:     numItems = averageRatings.getNumNondefaultElements();
0:     numUsers = (int) userRatings.getCounters().findCounter(Stats.NUM_USERS).getValue();
1: 
1:     log.info("Found {} users and {} items", numUsers, numItems);
1: 
1:       runSolver(pathToUserRatings(), pathToU(currentIteration), pathToM(currentIteration - 1), currentIteration, "U",
0:                 numItems);
1:       runSolver(pathToItemRatings(), pathToM(currentIteration), pathToU(currentIteration), currentIteration, "M",
0:                 numUsers);
/////////////////////////////////////////////////////////////////////////
0:       Closeables.close(writer, true);
1:     }
1:   }
1: 
0:   static class VectorSumCombiner
1:       extends Reducer<WritableComparable<?>, VectorWritable, WritableComparable<?>, VectorWritable> {
1: 
1:     private final VectorWritable result = new VectorWritable();
1: 
1:     @Override
1:     protected void reduce(WritableComparable<?> key, Iterable<VectorWritable> values, Context ctx)
0:         throws IOException, InterruptedException {
0:       result.set(ALS.sum(values.iterator()));
1:       ctx.write(key, result);
1:     }
1:   }
1: 
1:   static class VectorSumReducer
1:       extends Reducer<WritableComparable<?>, VectorWritable, WritableComparable<?>, VectorWritable> {
1: 
1:     private final VectorWritable result = new VectorWritable();
1: 
1:     @Override
1:     protected void reduce(WritableComparable<?> key, Iterable<VectorWritable> values, Context ctx)
0:         throws IOException, InterruptedException {
0:       Vector sum = ALS.sum(values.iterator());
1:       result.set(new SequentialAccessSparseVector(sum));
1:       ctx.write(key, result);
1:     }
1:   }
1: 
1:   static class MergeUserVectorsReducer extends
1:       Reducer<WritableComparable<?>,VectorWritable,WritableComparable<?>,VectorWritable> {
1: 
1:     private final VectorWritable result = new VectorWritable();
1: 
1:     @Override
1:     public void reduce(WritableComparable<?> key, Iterable<VectorWritable> vectors, Context ctx)
0:         throws IOException, InterruptedException {
1:       Vector merged = VectorWritable.merge(vectors.iterator()).get();
1:       result.set(new SequentialAccessSparseVector(merged));
1:       ctx.write(key, result);
1:       ctx.getCounter(Stats.NUM_USERS).increment(1);
/////////////////////////////////////////////////////////////////////////
1:     private final Vector ratings = new RandomAccessSparseVector(Integer.MAX_VALUE, 1);
/////////////////////////////////////////////////////////////////////////
1:   private void runSolver(Path ratings, Path output, Path pathToUorM, int currentIteration, String matrixName,
1:                          int numEntities) throws ClassNotFoundException, IOException, InterruptedException {
1: 
1:     // necessary for local execution in the same JVM only
1:     SharingMapper.reset();
/////////////////////////////////////////////////////////////////////////
0:           + "(" + numThreadsPerSolver + " threads, " + numFeatures +" features, implicit feedback)";
0:           + "(" + numThreadsPerSolver + " threads, " + numFeatures + " features, explicit feedback)";
/////////////////////////////////////////////////////////////////////////
1:     solverConf.set(NUM_ENTITIES, String.valueOf(numEntities));
1: 
1:     FileSystem fs = FileSystem.get(pathToUorM.toUri(), solverConf);
1:     FileStatus[] parts = fs.listStatus(pathToUorM, PathFilters.partFilter());
1:     for (FileStatus part : parts) {
1:       if (log.isDebugEnabled()) {
1:         log.debug("Adding {} to distributed cache", part.getPath().toString());
1:       }
1:       DistributedCache.addCacheFile(part.getPath().toUri(), solverConf);
1:     }
commit:6d16230
/////////////////////////////////////////////////////////////////////////
0:     throws ClassNotFoundException, IOException, InterruptedException {
/////////////////////////////////////////////////////////////////////////
0:       name = "Recompute " + matrixName + ", iteration (" + iterationNumber + "/" + numIterations + "), "
0:           + "(" + numThreadsPerSolver + " threads, implicit feedback)";
0:       name = "Recompute " + matrixName + ", iteration (" + iterationNumber + "/" + numIterations + "), "
0:           + "(" + numThreadsPerSolver + " threads, explicit feedback)";
commit:4841efb
/////////////////////////////////////////////////////////////////////////
0:       name = "Recompute " + matrixName + ", iteration (" + iterationNumber + "/" + numIterations + "), " +
0:       name = "Recompute " + matrixName + ", iteration (" + iterationNumber + "/" + numIterations + "), " +
commit:5d66758
/////////////////////////////////////////////////////////////////////////
1:     Class<? extends Mapper<IntWritable,VectorWritable,IntWritable,VectorWritable>> solverMapperClassInternal;
1:     String name;
1:       solverMapperClassInternal = SolveImplicitFeedbackMapper.class;
1:       solverMapperClassInternal = SolveExplicitFeedbackMapper.class;
commit:df39ce4
/////////////////////////////////////////////////////////////////////////
1:  * <li>--numThreadsPerSolver (int): threads to use per solver mapper, (default: 1)</li>
/////////////////////////////////////////////////////////////////////////
1:       IntWritable index = new IntWritable();
1:       VectorWritable featureVector = new VectorWritable();
1: 
/////////////////////////////////////////////////////////////////////////
1:         index.set(e.index());
1:         featureVector.set(row);
1:         writer.append(index, featureVector);
/////////////////////////////////////////////////////////////////////////
1:     if (implicitFeedback) {
0:       solverMapperClassInternal = SharingSolveImplicitFeedbackMapper.class;
0:       name = "Recompute " + matrixName +", iteration (" + iterationNumber + "/" + numIterations + "), " +
0:           "(" + numThreadsPerSolver + " threads, implicit feedback)";
0:       solverMapperClassInternal = SharingSolveExplicitFeedbackMapper.class;
0:       name = "Recompute " + matrixName +", iteration (" + iterationNumber + "/" + numIterations + "), " +
0:           "(" + numThreadsPerSolver + " threads, explicit feedback)";
1:     Job solverForUorI = prepareJob(ratings, output, SequenceFileInputFormat.class, MultithreadedSharingMapper.class,
1:         IntWritable.class, VectorWritable.class, SequenceFileOutputFormat.class, name);
1:     MultithreadedMapper.setMapperClass(solverForUorI, solverMapperClassInternal);
1:     MultithreadedMapper.setNumberOfThreads(solverForUorI, numThreadsPerSolver);
commit:63c81f1
/////////////////////////////////////////////////////////////////////////
/////////////////////////////////////////////////////////////////////////
1: import org.apache.hadoop.mapreduce.lib.map.MultithreadedMapper;
/////////////////////////////////////////////////////////////////////////
/////////////////////////////////////////////////////////////////////////
0:  * <li>--numThreadsPerSolver (int): threads to use per solver mapper, default: 1 (no multithreading)</li>
/////////////////////////////////////////////////////////////////////////
1:   private int numThreadsPerSolver;
/////////////////////////////////////////////////////////////////////////
1:     addOption("numThreadsPerSolver", null, "threads per solver mapper", String.valueOf(1));
/////////////////////////////////////////////////////////////////////////
1:     numThreadsPerSolver = Integer.parseInt(getOption("numThreadsPerSolver"));
1: 
1:     * compute the factorization A = U M'
1:     *
1:     * where A (users x items) is the matrix of known ratings
1:     *           U (users x features) is the representation of users in the feature space
1:     *           M (items x features) is the representation of items in the feature space
1:     */
/////////////////////////////////////////////////////////////////////////
1:     Vector averageRatings = ALS.readFirstRow(getTempPath("averageRatings"), getConf());
/////////////////////////////////////////////////////////////////////////
0:       runSolver(pathToUserRatings(), pathToU(currentIteration), pathToM(currentIteration - 1), currentIteration, "U");
0:       runSolver(pathToItemRatings(), pathToM(currentIteration), pathToU(currentIteration), currentIteration, "M");
/////////////////////////////////////////////////////////////////////////
0:   private void runSolver(Path ratings, Path output, Path pathToUorI, int currentIteration, String matrixName)
0:     int iterationNumber = currentIteration + 1;
0:     Class<? extends Mapper> solverMapperClass;
0:     Class<? extends Mapper<IntWritable,VectorWritable,IntWritable,VectorWritable>> solverMapperClassInternal = null;
0:     String name = null;
0:     // no multithreading
0:     if (numThreadsPerSolver == 1) {
0:       if (implicitFeedback) {
0:         solverMapperClass = SolveImplicitFeedbackMapper.class;
0:         name = "Recompute " + matrixName +", iteration (" + iterationNumber + "/" + numIterations + "), " +
0:             "(single-threaded, implicit feedback)";
1:       } else {
0:         solverMapperClass = SolveExplicitFeedbackMapper.class;
0:         name = "Recompute " + matrixName +", iteration (" + iterationNumber + "/" + numIterations + "), " +
0:             "(single-threaded, explicit feedback)";
1:       }
1: 
1:     } else {
0:       solverMapperClass = MultithreadedSharingMapper.class;
1: 
0:       if (implicitFeedback) {
0:         solverMapperClassInternal = SharingSolveImplicitFeedbackMapper.class;
0:         name = "Recompute " + matrixName +", iteration (" + iterationNumber + "/" + numIterations + "), " +
0:             "(" + numThreadsPerSolver + " threads, implicit feedback)";
1:       } else {
0:         solverMapperClassInternal = SharingSolveExplicitFeedbackMapper.class;
0:         name = "Recompute " + matrixName +", iteration (" + iterationNumber + "/" + numIterations + "), " +
0:             "(" + numThreadsPerSolver + " threads, explicit feedback)";
1:       }
1:     }
1: 
0:     Job solverForUorI = prepareJob(ratings, output, SequenceFileInputFormat.class, solverMapperClass, IntWritable.class,
0:         VectorWritable.class, SequenceFileOutputFormat.class, name);
1: 
0:     if (numThreadsPerSolver > 1) {
0:       MultithreadedMapper.setMapperClass(solverForUorI, solverMapperClassInternal);
0:       MultithreadedMapper.setNumberOfThreads(solverForUorI, numThreadsPerSolver);
1:     }
1: 
commit:c3154b8
/////////////////////////////////////////////////////////////////////////
0:       List<Vector> featureVectors = Lists.newArrayListWithCapacity(ratings.getNumNondefaultElements());
commit:e200147
/////////////////////////////////////////////////////////////////////////
1: 
0:     private IntWritable itemIDWritable = new IntWritable();
0:     private VectorWritable ratingsWritable = new VectorWritable(true);
1: 
0:     private Vector ratings = new SequentialAccessSparseVector(Integer.MAX_VALUE, 1);
1: 
/////////////////////////////////////////////////////////////////////////
1:       ratings.setQuick(userID, rating);
1:       itemIDWritable.set(itemID);
1:       ratingsWritable.set(ratings);
1: 
1:       ctx.write(itemIDWritable, ratingsWritable);
1: 
1:       // prepare instance for reuse
1:       ratings.setQuick(userID, 0.0d);
/////////////////////////////////////////////////////////////////////////
0:     private VectorWritable uiOrmjWritable = new VectorWritable();
1: 
/////////////////////////////////////////////////////////////////////////
0:       Vector ratings = ratingsWritable.get();
1: 
/////////////////////////////////////////////////////////////////////////
0:       uiOrmjWritable.set(uiOrmj);
0:       ctx.write(userOrItemID, uiOrmjWritable);
/////////////////////////////////////////////////////////////////////////
0:     private VectorWritable uiOrmjWritable = new VectorWritable();
1: 
/////////////////////////////////////////////////////////////////////////
0:       Vector uiOrmj = solver.solve(ratingsWritable.get());
0:       uiOrmjWritable.set(uiOrmj);
0:       ctx.write(userOrItemID, uiOrmjWritable);
1: 
0:     private IntWritable firstIndex = new IntWritable(0);
0:     private Vector featureVector = new RandomAccessSparseVector(Integer.MAX_VALUE, 1);
0:     private VectorWritable featureVectorWritable = new VectorWritable();
1: 
/////////////////////////////////////////////////////////////////////////
1: 
1:       featureVector.setQuick(r.get(), avg.getAverage());
1:       featureVectorWritable.set(featureVector);
1:       ctx.write(firstIndex, featureVectorWritable);
1: 
1:       // prepare instance for reuse
1:       featureVector.setQuick(r.get(), 0.0d);
commit:d317c1c
/////////////////////////////////////////////////////////////////////////
1: import org.slf4j.Logger;
1: import org.slf4j.LoggerFactory;
/////////////////////////////////////////////////////////////////////////
1:   private static final Logger log = LoggerFactory.getLogger(ParallelALSFactorizationJob.class);
1: 
/////////////////////////////////////////////////////////////////////////
1:       log.info("Recomputing U (iteration {}/{})", currentIteration, numIterations);
1:       log.info("Recomputing M (iteration {}/{})", currentIteration, numIterations);
commit:3631151
/////////////////////////////////////////////////////////////////////////
1: import org.apache.hadoop.conf.Configuration;
/////////////////////////////////////////////////////////////////////////
0: import org.apache.mahout.math.als.AlternatingLeastSquaresSolver;
0: import org.apache.mahout.math.als.ImplicitFeedbackAlternatingLeastSquaresSolver;
/////////////////////////////////////////////////////////////////////////
1:  * <p>MapReduce implementation of the two factorization algorithms described in
1:  *
1:  * <p>"Large-scale Parallel Collaborative Filtering for the Netﬂix Prize" available at
1:  * "<p>Collaborative Filtering for Implicit Feedback Datasets" available at
1:  * http://research.yahoo.com/pub/2433</p>
1:  * </p>
/////////////////////////////////////////////////////////////////////////
1:   static final String ALPHA = ParallelALSFactorizationJob.class.getName() + ".alpha";
1:   private boolean implicitFeedback;
1:   private double alpha;
/////////////////////////////////////////////////////////////////////////
1:     addOption("lambda", null, "regularization parameter", true);
1:     addOption("implicitFeedback", null, "data consists of implicit feedback?", String.valueOf(false));
1:     addOption("alpha", null, "confidence parameter (only used on implicit feedback)", String.valueOf(40));
1:     addOption("numFeatures", null, "dimension of the feature space", true);
/////////////////////////////////////////////////////////////////////////
0:     alpha = Double.parseDouble(parsedArgs.get("--alpha"));
0:     implicitFeedback = Boolean.parseBoolean(parsedArgs.get("--implicitFeedback"));
/////////////////////////////////////////////////////////////////////////
1:       /* broadcast U, read A' row-wise, recompute M row-wise */
/////////////////////////////////////////////////////////////////////////
1: 
0:     Class<? extends Mapper> solverMapper = implicitFeedback ?
0:         SolveImplicitFeedbackMapper.class : SolveExplicitFeedbackMapper.class;
1: 
0:     Job solverForUorI = prepareJob(ratings, output, SequenceFileInputFormat.class, solverMapper, IntWritable.class,
1:     Configuration solverConf = solverForUorI.getConfiguration();
1:     solverConf.set(LAMBDA, String.valueOf(lambda));
1:     solverConf.set(ALPHA, String.valueOf(alpha));
1:     solverConf.setInt(NUM_FEATURES, numFeatures);
0:     solverConf.set(FEATURE_MATRIX, pathToUorI.toString());
0:   static class SolveExplicitFeedbackMapper extends Mapper<IntWritable,VectorWritable,IntWritable,VectorWritable> {
0:     private AlternatingLeastSquaresSolver solver;
0:       solver = new AlternatingLeastSquaresSolver();
/////////////////////////////////////////////////////////////////////////
0:   static class SolveImplicitFeedbackMapper extends Mapper<IntWritable,VectorWritable,IntWritable,VectorWritable> {
1: 
0:     private ImplicitFeedbackAlternatingLeastSquaresSolver solver;
1: 
1:     @Override
0:     protected void setup(Mapper.Context ctx) throws IOException, InterruptedException {
0:       double lambda = Double.parseDouble(ctx.getConfiguration().get(LAMBDA));
0:       double alpha = Double.parseDouble(ctx.getConfiguration().get(ALPHA));
0:       int numFeatures = ctx.getConfiguration().getInt(NUM_FEATURES, -1);
1: 
0:       Path YPath = new Path(ctx.getConfiguration().get(FEATURE_MATRIX));
0:       OpenIntObjectHashMap<Vector> Y = ALSUtils.readMatrixByRows(YPath, ctx.getConfiguration());
1: 
0:       solver = new ImplicitFeedbackAlternatingLeastSquaresSolver(numFeatures, lambda, alpha, Y);
1: 
0:       Preconditions.checkArgument(numFeatures > 0, "numFeatures was not set correctly!");
1:     }
1: 
1:     @Override
0:     protected void map(IntWritable userOrItemID, VectorWritable ratingsWritable, Context ctx)
0:         throws IOException, InterruptedException {
0:       Vector ratings = new SequentialAccessSparseVector(ratingsWritable.get());
1: 
0:       Vector uiOrmj = solver.solve(ratings);
1: 
0:       ctx.write(userOrItemID, new VectorWritable(uiOrmj));
1:     }
1:   }
1: 
commit:81d64c0
/////////////////////////////////////////////////////////////////////////
0: import com.google.common.io.Closeables;
1: import org.apache.hadoop.fs.FileSystem;
1: import org.apache.hadoop.io.SequenceFile;
/////////////////////////////////////////////////////////////////////////
1: import org.apache.mahout.common.mapreduce.MergeVectorsCombiner;
1: import org.apache.mahout.common.mapreduce.MergeVectorsReducer;
1: import org.apache.mahout.common.mapreduce.TransposeMapper;
0: import org.apache.mahout.common.mapreduce.VectorSumReducer;
0: import org.apache.mahout.math.map.OpenIntObjectHashMap;
0: import java.util.Iterator;
0:  * <p>MapReduce implementation of the factorization algorithm described in "Large-scale Parallel Collaborative Filtering for the Netﬂix Prize"
/////////////////////////////////////////////////////////////////////////
1:  * <li>--userFeatures (path): path to the user feature matrix</li>
1:  * <li>--itemFeatures (path): path to the item feature matrix</li>
0:   static final String FEATURE_MATRIX = ParallelALSFactorizationJob.class.getName() + ".featureMatrix";
1:   private int numIterations;
1:   private int numFeatures;
1:   private double lambda;
/////////////////////////////////////////////////////////////////////////
0:     numFeatures = Integer.parseInt(parsedArgs.get("--numFeatures"));
0:     numIterations = Integer.parseInt(parsedArgs.get("--numIterations"));
0:     lambda = Double.parseDouble(parsedArgs.get("--lambda"));
1:     /*
0:         * compute the factorization A = U M'
1:         *
0:         * where A (users x items) is the matrix of known ratings
0:         *           U (users x features) is the representation of users in the feature space
0:         *           M (items x features) is the representation of items in the feature space
1:         */
1: 
1:    /* create A' */
1:         TextInputFormat.class, ItemRatingVectorsMapper.class, IntWritable.class,
1:         VectorWritable.class, VectorSumReducer.class, IntWritable.class,
1:         VectorWritable.class, SequenceFileOutputFormat.class);
0:     itemRatings.setCombinerClass(VectorSumReducer.class);
1: 
1:     /* create A */
1:     Job userRatings = prepareJob(pathToItemRatings(), pathToUserRatings(),
0:         TransposeMapper.class, IntWritable.class, VectorWritable.class, MergeVectorsReducer.class, IntWritable.class,
0:         VectorWritable.class);
1:     userRatings.setCombinerClass(MergeVectorsCombiner.class);
1:     //TODO this could be fiddled into one of the upper jobs
1:     Job averageItemRatings = prepareJob(pathToItemRatings(), getTempPath("averageRatings"),
1:         AverageRatingMapper.class, IntWritable.class, VectorWritable.class, MergeVectorsReducer.class,
1:         IntWritable.class, VectorWritable.class);
1:     averageItemRatings.setCombinerClass(MergeVectorsCombiner.class);
0:     averageItemRatings.waitForCompletion(true);
0:     Vector averageRatings = ALSUtils.readFirstRow(getTempPath("averageRatings"), getConf());
1: 
1:     /* create an initial M */
1:     initializeM(averageRatings);
1: 
1:     for (int currentIteration = 0; currentIteration < numIterations; currentIteration++) {
1:       /* broadcast M, read A row-wise, recompute U row-wise */
0:       runSolver(pathToUserRatings(), pathToU(currentIteration), pathToM(currentIteration - 1));
0:       /* broadcast U, read A' row-wise, recompute I row-wise */
0:       runSolver(pathToItemRatings(), pathToM(currentIteration), pathToU(currentIteration));
1:   private void initializeM(Vector averageRatings) throws IOException {
1:     Random random = RandomUtils.getRandom();
1: 
1:     FileSystem fs = FileSystem.get(pathToM(-1).toUri(), getConf());
0:     SequenceFile.Writer writer = null;
0:     try {
0:       writer = new SequenceFile.Writer(fs, getConf(), new Path(pathToM(-1), "part-m-00000"), IntWritable.class,
0:           VectorWritable.class);
1: 
0:       Iterator<Vector.Element> averages = averageRatings.iterateNonZero();
0:       while (averages.hasNext()) {
0:         Vector.Element e = averages.next();
1:         Vector row = new DenseVector(numFeatures);
1:         row.setQuick(0, e.get());
1:         for (int m = 1; m < numFeatures; m++) {
1:           row.setQuick(m, random.nextDouble());
1:         }
0:         writer.append(new IntWritable(e.index()), new VectorWritable(row));
1:       }
0:     } finally {
0:       Closeables.closeQuietly(writer);
1:   static class ItemRatingVectorsMapper extends Mapper<LongWritable,Text,IntWritable,VectorWritable> {
0:       int userID = Integer.parseInt(tokens[0]);
0:       int itemID = Integer.parseInt(tokens[1]);
1: 
0:       Vector ratings = new RandomAccessSparseVector(Integer.MAX_VALUE, 1);
0:       ratings.set(userID, rating);
1: 
0:       ctx.write(new IntWritable(itemID), new VectorWritable(ratings, true));
0:   private void runSolver(Path ratings, Path output, Path pathToUorI)
0:       throws ClassNotFoundException, IOException, InterruptedException {
0:     Job solverForUorI = prepareJob(ratings, output, SequenceFileInputFormat.class, SolveMapper.class, IntWritable.class,
1:         VectorWritable.class, SequenceFileOutputFormat.class);
0:     solverForUorI.getConfiguration().set(LAMBDA, String.valueOf(lambda));
0:     solverForUorI.getConfiguration().setInt(NUM_FEATURES, numFeatures);
0:     solverForUorI.getConfiguration().set(FEATURE_MATRIX, pathToUorI.toString());
0:     solverForUorI.waitForCompletion(true);
0:   static class SolveMapper extends Mapper<IntWritable,VectorWritable,IntWritable,VectorWritable> {
0:     private int numFeatures;
1: 
0:     private OpenIntObjectHashMap<Vector> UorM;
1: 
0:     protected void setup(Mapper.Context ctx) throws IOException, InterruptedException {
0:       Path UOrIPath = new Path(ctx.getConfiguration().get(FEATURE_MATRIX));
1: 
0:       UorM = ALSUtils.readMatrixByRows(UOrIPath, ctx.getConfiguration());
0:     protected void map(IntWritable userOrItemID, VectorWritable ratingsWritable, Context ctx)
0:       Vector ratings = new SequentialAccessSparseVector(ratingsWritable.get());
0:       List<Vector> featureVectors = Lists.newArrayList();
0:       Iterator<Vector.Element> interactions = ratings.iterateNonZero();
0:       while (interactions.hasNext()) {
0:         int index = interactions.next().index();
0:         featureVectors.add(UorM.get(index));
0:       Vector uiOrmj = solver.solve(featureVectors, ratings, lambda, numFeatures);
0:       ctx.write(userOrItemID, new VectorWritable(uiOrmj));
1:   static class AverageRatingMapper extends Mapper<IntWritable,VectorWritable,IntWritable,VectorWritable> {
0:     @Override
1:     protected void map(IntWritable r, VectorWritable v, Context ctx) throws IOException, InterruptedException {
1:       RunningAverage avg = new FullRunningAverage();
0:       Iterator<Vector.Element> elements = v.get().iterateNonZero();
0:       while (elements.hasNext()) {
0:         avg.addDatum(elements.next().get());
1:       }
0:       Vector vector = new RandomAccessSparseVector(Integer.MAX_VALUE, 1);
0:       vector.setQuick(r.get(), avg.getAverage());
0:       ctx.write(new IntWritable(0), new VectorWritable(vector));
1:     }
1:     return iteration == numIterations - 1 ? getOutputPath("M") : getTempPath("M-" + iteration);
1:     return iteration == numIterations - 1 ? getOutputPath("U") : getTempPath("U-" + iteration);
1:     return getTempPath("itemRatings");
1:     return getOutputPath("userRatings");
commit:74f849b
/////////////////////////////////////////////////////////////////////////
0: import com.google.common.collect.Lists;
0: import com.google.common.collect.Maps;
/////////////////////////////////////////////////////////////////////////
/////////////////////////////////////////////////////////////////////////
0:       Map<Integer,Float> ratings = Maps.newHashMap();
/////////////////////////////////////////////////////////////////////////
0:       List<Vector> UorMColumns = Lists.newArrayList();
commit:96024a7
/////////////////////////////////////////////////////////////////////////
0: import com.google.common.base.Preconditions;
/////////////////////////////////////////////////////////////////////////
0:     addOption("lambda", "l", "regularization parameter", true);
0:     addOption("numFeatures", "f", "dimension of the feature space", true);
0:     addOption("numIterations", "i", "number of iterations", true);
/////////////////////////////////////////////////////////////////////////
0:       throws IOException, ClassNotFoundException, InterruptedException {
/////////////////////////////////////////////////////////////////////////
/////////////////////////////////////////////////////////////////////////
0: 
0:       Preconditions.checkArgument(numFeatures > 0, "numFeatures was not set correctly!");
/////////////////////////////////////////////////////////////////////////
0: 
0:       Preconditions.checkArgument(numFeatures > 0, "numFeatures was not set correctly!");
0:         throws IOException, InterruptedException {
commit:af775ba
/////////////////////////////////////////////////////////////////////////
1: /**
1:  * Licensed to the Apache Software Foundation (ASF) under one or more
1:  * contributor license agreements.  See the NOTICE file distributed with
1:  * this work for additional information regarding copyright ownership.
1:  * The ASF licenses this file to You under the Apache License, Version 2.0
1:  * (the "License"); you may not use this file except in compliance with
1:  * the License.  You may obtain a copy of the License at
1:  *
1:  *     http://www.apache.org/licenses/LICENSE-2.0
1:  *
1:  * Unless required by applicable law or agreed to in writing, software
1:  * distributed under the License is distributed on an "AS IS" BASIS,
1:  * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
1:  * See the License for the specific language governing permissions and
1:  * limitations under the License.
1:  */
0: 
1: package org.apache.mahout.cf.taste.hadoop.als;
0: 
0: import org.apache.hadoop.conf.Configuration;
1: import org.apache.hadoop.fs.Path;
0: import org.apache.hadoop.io.FloatWritable;
1: import org.apache.hadoop.io.IntWritable;
1: import org.apache.hadoop.io.LongWritable;
1: import org.apache.hadoop.io.Text;
1: import org.apache.hadoop.mapreduce.Job;
1: import org.apache.hadoop.mapreduce.Mapper;
0: import org.apache.hadoop.mapreduce.Reducer;
1: import org.apache.hadoop.mapreduce.lib.input.SequenceFileInputFormat;
1: import org.apache.hadoop.mapreduce.lib.input.TextInputFormat;
1: import org.apache.hadoop.mapreduce.lib.output.SequenceFileOutputFormat;
0: import org.apache.hadoop.mapreduce.lib.partition.HashPartitioner;
1: import org.apache.hadoop.util.ToolRunner;
1: import org.apache.mahout.cf.taste.hadoop.TasteHadoopUtils;
1: import org.apache.mahout.cf.taste.impl.common.FullRunningAverage;
1: import org.apache.mahout.cf.taste.impl.common.RunningAverage;
1: import org.apache.mahout.common.AbstractJob;
0: import org.apache.mahout.math.*;
0: import org.apache.mahout.math.als.AlternateLeastSquaresSolver;
0: 
1: import java.io.IOException;
0: import java.util.ArrayList;
0: import java.util.HashMap;
1: import java.util.List;
1: import java.util.Map;
1: import java.util.Random;
0: import java.util.SortedMap;
0: import java.util.TreeMap;
0: 
1: /**
0:  * <p>MapReduce implementation of the factorization algorithm described in "Large-scale Parallel Collaborative Filtering for the Netﬂix Prize"
0:  * available at http://www.hpl.hp.com/personal/Robert_Schreiber/papers/2008%20AAIM%20Netflix/netflix_aaim08(submitted).pdf.</p>
1:  *
0:  * <p>Implements a parallel algorithm that uses "Alternating-Least-Squares with Weighted-λ-Regularization" to factorize the
0:  * preference-matrix </p>
1:  *
1:  * <p>Command line arguments specific to this class are:</p>
1:  *
1:  * <ol>
1:  * <li>--input (path): Directory containing one or more text files with the dataset</li>
1:  * <li>--output (path): path where output should go</li>
1:  * <li>--lambda (double): regularization parameter to avoid overfitting</li>
0:  * <li>--numFeatures (int): number of features to use for decomposition </li>
0: * <li>--numIterations (int): number of iterations to run</li>
1:  * </ol>
1:  */
1: public class ParallelALSFactorizationJob extends AbstractJob {
0: 
1:   static final String NUM_FEATURES = ParallelALSFactorizationJob.class.getName() + ".numFeatures";
1:   static final String LAMBDA = ParallelALSFactorizationJob.class.getName() + ".lambda";
0:   static final String MAP_TRANSPOSED = ParallelALSFactorizationJob.class.getName() + ".mapTransposed";
0: 
0:   static final String STEP_ONE = "fixMcomputeU";
0:   static final String STEP_TWO = "fixUcomputeM";
0: 
0:   private String tempDir;
0: 
1:   public static void main(String[] args) throws Exception {
1:     ToolRunner.run(new ParallelALSFactorizationJob(), args);
1:   }
0: 
0:   @Override
1:   public int run(String[] args) throws Exception {
0: 
1:     addInputOption();
1:     addOutputOption();
0:     addOption("lambda", "l", "", true);
0:     addOption("numFeatures", "f", "", true);
0:     addOption("numIterations", "i", "", true);
0: 
0:     Map<String,String> parsedArgs = parseArguments(args);
1:     if (parsedArgs == null) {
1:       return -1;
1:     }
0: 
0:     int numFeatures = Integer.parseInt(parsedArgs.get("--numFeatures"));
0:     int numIterations = Integer.parseInt(parsedArgs.get("--numIterations"));
0:     double lambda = Double.parseDouble(parsedArgs.get("--lambda"));
0:     tempDir = parsedArgs.get("--tempDir");
0: 
1:     Job itemRatings = prepareJob(getInputPath(), pathToItemRatings(),
0:         TextInputFormat.class, PrefsToRatingsMapper.class, VarIntWritable.class,
0:         FeatureVectorWithRatingWritable.class, Reducer.class, VarIntWritable.class,
0:         FeatureVectorWithRatingWritable.class, SequenceFileOutputFormat.class);
0:     itemRatings.waitForCompletion(true);
0:     
0:     Job userRatings = prepareJob(getInputPath(), pathToUserRatings(),
0:         TextInputFormat.class, PrefsToRatingsMapper.class, VarIntWritable.class,
0:         FeatureVectorWithRatingWritable.class, Reducer.class, VarIntWritable.class,
0:         FeatureVectorWithRatingWritable.class, SequenceFileOutputFormat.class);
0:     userRatings.getConfiguration().setBoolean(MAP_TRANSPOSED, Boolean.TRUE);
0:     userRatings.waitForCompletion(true);
0: 
0:     Job initializeM = prepareJob(getInputPath(), pathToM(-1), TextInputFormat.class, ItemIDRatingMapper.class,
0:         VarLongWritable.class, FloatWritable.class, InitializeMReducer.class, VarIntWritable.class,
0:         FeatureVectorWithRatingWritable.class, SequenceFileOutputFormat.class);
0:     initializeM.getConfiguration().setInt(NUM_FEATURES, numFeatures);
0:     initializeM.waitForCompletion(true);
0: 
0:     for (int n = 0; n < numIterations; n++) {
0:       iterate(n, numFeatures, lambda);
1:     }
0: 
0:     Job uAsMatrix = prepareJob(pathToU(numIterations - 1), new Path(getOutputPath(), "U"),
0:         SequenceFileInputFormat.class, ToMatrixMapper.class, IntWritable.class, VectorWritable.class, Reducer.class,
0:         IntWritable.class, VectorWritable.class, SequenceFileOutputFormat.class);
0:     uAsMatrix.waitForCompletion(true);
0: 
0:     Job mAsMatrix = prepareJob(pathToM(numIterations - 1), new Path(getOutputPath(), "M"),
0:         SequenceFileInputFormat.class, ToMatrixMapper.class, IntWritable.class, VectorWritable.class, Reducer.class,
0:         IntWritable.class, VectorWritable.class, SequenceFileOutputFormat.class);
0:     mAsMatrix.waitForCompletion(true);
0: 
1:     return 0;
1:   }
0: 
0:   static class ToMatrixMapper
0:       extends Mapper<VarIntWritable,FeatureVectorWithRatingWritable,IntWritable,VectorWritable> {
0:     @Override
0:     protected void map(VarIntWritable key, FeatureVectorWithRatingWritable value, Context ctx) 
0:         throws IOException, InterruptedException {
0:       ctx.write(new IntWritable(key.get()), new VectorWritable(value.getFeatureVector()));
1:     }
1:   }
0: 
0: 
0:   private void iterate(int currentIteration, int numFeatures, double lambda)
0:       throws IOException, ClassNotFoundException, InterruptedException {
0:     /* fix M, compute U */
0:     joinAndSolve(pathToM(currentIteration - 1), pathToItemRatings(), pathToU(currentIteration), numFeatures,
0:         lambda, currentIteration, STEP_ONE);
0:     /* fix U, compute M */
0:     joinAndSolve(pathToU(currentIteration), pathToUserRatings(), pathToM(currentIteration), numFeatures,
0:         lambda, currentIteration, STEP_TWO);
1:   }
0: 
0:   private void joinAndSolve(Path featureMatrix, Path ratingMatrix, Path outputPath, int numFeatures, double lambda,
0:       int currentIteration, String step) throws IOException, ClassNotFoundException, InterruptedException  {
0: 
0:     Path joinPath = new Path(ratingMatrix.toString() + "," + featureMatrix.toString());
0:     Path featureVectorWithRatingPath = joinAndSolvePath(currentIteration, step);
0: 
0:     Job joinToFeatureVectorWithRating = prepareJob(joinPath, featureVectorWithRatingPath, SequenceFileInputFormat.class,
0:         Mapper.class, VarIntWritable.class, FeatureVectorWithRatingWritable.class,
0:         JoinFeatureVectorAndRatingsReducer.class, IndexedVarIntWritable.class, FeatureVectorWithRatingWritable.class,
0:         SequenceFileOutputFormat.class);
0:     joinToFeatureVectorWithRating.waitForCompletion(true);
0: 
0:     Job solve = prepareJob(featureVectorWithRatingPath, outputPath, SequenceFileInputFormat.class, Mapper.class,
0:         IndexedVarIntWritable.class, FeatureVectorWithRatingWritable.class, SolvingReducer.class, VarIntWritable.class,
0:         FeatureVectorWithRatingWritable.class, SequenceFileOutputFormat.class);
0:     Configuration solveConf = solve.getConfiguration();
0:     solve.setPartitionerClass(HashPartitioner.class);
0:     solve.setGroupingComparatorClass(IndexedVarIntWritable.GroupingComparator.class);
0:     solveConf.setInt(NUM_FEATURES, numFeatures);
0:     solveConf.set(LAMBDA, String.valueOf(lambda));
0:     solve.waitForCompletion(true);    
1:   }
0: 
0:   static class PrefsToRatingsMapper
0:       extends Mapper<LongWritable,Text,VarIntWritable,FeatureVectorWithRatingWritable> {
0: 
0:     private boolean transpose;
0: 
0:     @Override
0:     protected void setup(Context ctx) throws IOException, InterruptedException {
0:       transpose = ctx.getConfiguration().getBoolean(MAP_TRANSPOSED, false);
1:     }
0: 
0:     @Override
1:     protected void map(LongWritable offset, Text line, Context ctx) throws IOException, InterruptedException {
1:       String[] tokens = TasteHadoopUtils.splitPrefTokens(line.toString());
0:       int keyIDIndex = TasteHadoopUtils.idToIndex(Long.parseLong(tokens[transpose ? 0 : 1]));
0:       int valueIDIndex = TasteHadoopUtils.idToIndex(Long.parseLong(tokens[transpose ? 1 : 0]));
1:       float rating = Float.parseFloat(tokens[2]);
0:       ctx.write(new VarIntWritable(keyIDIndex), new FeatureVectorWithRatingWritable(valueIDIndex, rating));
1:     }
1:   }
0: 
0:   static class JoinFeatureVectorAndRatingsReducer
0:       extends Reducer<VarIntWritable,FeatureVectorWithRatingWritable,IndexedVarIntWritable,FeatureVectorWithRatingWritable> {
0: 
0:     @Override
0:     protected void reduce(VarIntWritable id, Iterable<FeatureVectorWithRatingWritable> values, Context ctx)
0:         throws IOException, InterruptedException {
0:       Vector featureVector = null;
0:       Map<Integer,Float> ratings = new HashMap<Integer,Float>();
0:       for (FeatureVectorWithRatingWritable value : values) {
0:         if (value.getFeatureVector() == null) {
0:           ratings.put(value.getIDIndex(), new Float(value.getRating()));
0:         } else {
0:           featureVector = value.getFeatureVector().clone();          
1:         }
1:       }
0: 
0:       if (featureVector == null || ratings.isEmpty()) {
0:         throw new IllegalStateException("Unable to join data for " + id);
1:       }      
0:       for (Map.Entry<Integer,Float> rating : ratings.entrySet()) {
0:         ctx.write(new IndexedVarIntWritable(rating.getKey(), id.get()),
0:             new FeatureVectorWithRatingWritable(id.get(), rating.getValue(), featureVector));
1:       }
1:     }
1:   }
0: 
0:   static class SolvingReducer
0:       extends Reducer<IndexedVarIntWritable,FeatureVectorWithRatingWritable,VarIntWritable,FeatureVectorWithRatingWritable> {
0: 
0:     private int numFeatures;
0:     private double lambda;
0:     private AlternateLeastSquaresSolver solver;
0: 
0:     @Override
0:     protected void setup(Context ctx) throws IOException, InterruptedException {
0:       super.setup(ctx);
0:       lambda = Double.parseDouble(ctx.getConfiguration().get(LAMBDA));
0:       numFeatures = ctx.getConfiguration().getInt(NUM_FEATURES, -1);
0:       if (numFeatures < 1) {
0:         throw new IllegalStateException("numFeatures was not set correctly!");
1:       }
0:       solver = new AlternateLeastSquaresSolver();
1:     }
0: 
0:     @Override
0:     protected void reduce(IndexedVarIntWritable key, Iterable<FeatureVectorWithRatingWritable> values, Context ctx)
0:         throws IOException, InterruptedException {
0:       List<Vector> UorMColumns = new ArrayList<Vector>();
0:       Vector ratingVector = new RandomAccessSparseVector(Integer.MAX_VALUE);
0:       int n = 0;
0:       for (FeatureVectorWithRatingWritable value : values) {
0:         ratingVector.setQuick(n++, value.getRating());
0:         UorMColumns.add(value.getFeatureVector());
1:       }
0:       Vector uiOrmj = solver.solve(UorMColumns, new SequentialAccessSparseVector(ratingVector), lambda, numFeatures);
0:       ctx.write(new VarIntWritable(key.getValue()), new FeatureVectorWithRatingWritable(key.getValue(), uiOrmj));
1:     }
1:   }
0: 
0:   static class ItemIDRatingMapper extends Mapper<LongWritable,Text,VarLongWritable,FloatWritable> {
0:     @Override
0:     protected void map(LongWritable key, Text value, Context ctx) throws IOException, InterruptedException {
0:       String[] tokens = TasteHadoopUtils.splitPrefTokens(value.toString());
0:       ctx.write(new VarLongWritable(Long.parseLong(tokens[1])), new FloatWritable(Float.parseFloat(tokens[2])));
1:     }
1:   }
0: 
0:   static class InitializeMReducer
0:       extends Reducer<VarLongWritable,FloatWritable,VarIntWritable,FeatureVectorWithRatingWritable> {
0: 
0:     private int numFeatures;
0:     private static final Random random = new Random();
0: 
0:     @Override
0:     protected void setup(Context ctx) throws IOException, InterruptedException {
0:       super.setup(ctx);
0:       numFeatures = ctx.getConfiguration().getInt(NUM_FEATURES, -1);
0:       if (numFeatures < 1) {
0:         throw new IllegalStateException("numFeatures was not set correctly!");
1:       }
1:     }
0: 
0:     @Override
0:     protected void reduce(VarLongWritable itemID, Iterable<FloatWritable> ratings, Context ctx) 
0:         throws IOException, InterruptedException {
0: 
0:       RunningAverage averageRating = new FullRunningAverage();
0:       for (FloatWritable rating : ratings) {
0:         averageRating.addDatum(rating.get());
1:       }
0: 
0:       int itemIDIndex = TasteHadoopUtils.idToIndex(itemID.get());
0:       Vector columnOfM = new DenseVector(numFeatures);
0: 
0:       columnOfM.setQuick(0, averageRating.getAverage());
0:       for (int n = 1; n < numFeatures; n++) {
0:         columnOfM.setQuick(n, random.nextDouble());
1:       }
0: 
0:       ctx.write(new VarIntWritable(itemIDIndex), new FeatureVectorWithRatingWritable(itemIDIndex, columnOfM));
1:     }
1:   }
0: 
0:   private Path joinAndSolvePath(int currentIteration, String step) {
0:     return new Path(tempDir, "joinAndSolve-" + currentIteration + "-" + step);
1:   }
0: 
1:   private Path pathToM(int iteration) {
0:     return new Path(tempDir, "M-" + iteration);
1:   }
0: 
1:   private Path pathToU(int iteration) {
0:     return new Path(tempDir, "U-" + iteration);
1:   }
0: 
1:   private Path pathToItemRatings() {
0:     return new Path(tempDir, "itemsAsFeatureWithRatingWritable");
1:   }
0: 
1:   private Path pathToUserRatings() {
0:     return new Path(tempDir, "usersAsFeatureWithRatingWritable");
1:   }
1: }
author:Sean R. Owen
-------------------------------------------------------------------------------
commit:335a993
/////////////////////////////////////////////////////////////////////////
1:     private final VarIntWritable index = new VarIntWritable();
1:     private final VarLongWritable idWritable = new VarLongWritable();
commit:4ca6b86
/////////////////////////////////////////////////////////////////////////
0:       name = "Recompute " + matrixName + ", iteration (" + (iterationNumber + 1) + '/' + numIterations + "), "
0:           + '(' + numThreadsPerSolver + " threads, " + numFeatures +" features, implicit feedback)";
0:       name = "Recompute " + matrixName + ", iteration (" + (iterationNumber + 1) + '/' + numIterations + "), "
1:           + '(' + numThreadsPerSolver + " threads, " + numFeatures + " features, explicit feedback)";
commit:10c535c
/////////////////////////////////////////////////////////////////////////
1:     private final IntWritable itemIDWritable = new IntWritable();
1:     private final VectorWritable ratingsWritable = new VectorWritable(true);
0:     private final Vector ratings = new SequentialAccessSparseVector(Integer.MAX_VALUE, 1);
/////////////////////////////////////////////////////////////////////////
0:     private final VectorWritable uiOrmjWritable = new VectorWritable();
/////////////////////////////////////////////////////////////////////////
0:     private final VectorWritable uiOrmjWritable = new VectorWritable();
/////////////////////////////////////////////////////////////////////////
1:     private final IntWritable firstIndex = new IntWritable(0);
1:     private final Vector featureVector = new RandomAccessSparseVector(Integer.MAX_VALUE, 1);
1:     private final VectorWritable featureVectorWritable = new VectorWritable();
commit:229aeff
/////////////////////////////////////////////////////////////////////////
1:     if (!succeeded) {
0:     }
/////////////////////////////////////////////////////////////////////////
1:     if (!succeeded) {
0:     }
/////////////////////////////////////////////////////////////////////////
1:     if (!succeeded) {
0:     }
/////////////////////////////////////////////////////////////////////////
1:     if (!succeeded) {
0:     }
/////////////////////////////////////////////////////////////////////////
0:       Vector uiOrmj = AlternatingLeastSquaresSolver.solve(featureVectors, ratings, lambda, numFeatures);
commit:7c2b664
/////////////////////////////////////////////////////////////////////////
1:     boolean succeeded = itemRatings.waitForCompletion(true);
0:     if (!succeeded) 
1:       return -1;
1:     succeeded = userRatings.waitForCompletion(true);
0:     if (!succeeded) 
1:       return -1;
1:     succeeded = averageItemRatings.waitForCompletion(true);
0:     if (!succeeded) 
1:       return -1;
/////////////////////////////////////////////////////////////////////////
1:     boolean succeeded = solverForUorI.waitForCompletion(true);
0:     if (!succeeded) 
1:       throw new IllegalStateException("Job failed!");
commit:763c94c
/////////////////////////////////////////////////////////////////////////
1:     addOption("numIterations", null, "number of iterations", true);
commit:96117d3
/////////////////////////////////////////////////////////////////////////
commit:a26d3b1
/////////////////////////////////////////////////////////////////////////
0:     private final Random random = RandomUtils.getRandom();
commit:3218e95
/////////////////////////////////////////////////////////////////////////
0:  * <p>MapReduce implementation of the factorization algorithm described in
0:  * "Large-scale Parallel Collaborative Filtering for the Netﬂix Prize"
0:  * available at
1:  * http://www.hpl.hp.com/personal/Robert_Schreiber/papers/2008%20AAIM%20Netflix/netflix_aaim08(submitted).pdf.</p>
0:  * <p>Implements a parallel algorithm that uses "Alternating-Least-Squares with Weighted-λ-Regularization"
0:  * to factorize the preference-matrix </p>
/////////////////////////////////////////////////////////////////////////
0:       throws IOException, InterruptedException {
0:     throws IOException, ClassNotFoundException, InterruptedException {
/////////////////////////////////////////////////////////////////////////
0:     Path joinPath = new Path(ratingMatrix.toString() + ',' + featureMatrix);
/////////////////////////////////////////////////////////////////////////
0:       throws IOException, InterruptedException {
/////////////////////////////////////////////////////////////////////////
0:       throws IOException, InterruptedException {
/////////////////////////////////////////////////////////////////////////
0:       throws IOException, InterruptedException {
/////////////////////////////////////////////////////////////////////////
0:     return new Path(tempDir, "joinAndSolve-" + currentIteration + '-' + step);
commit:b16c260
/////////////////////////////////////////////////////////////////////////
1: import org.apache.mahout.common.RandomUtils;
1: import org.apache.mahout.math.DenseVector;
1: import org.apache.mahout.math.RandomAccessSparseVector;
1: import org.apache.mahout.math.SequentialAccessSparseVector;
0: import org.apache.mahout.math.VarIntWritable;
0: import org.apache.mahout.math.VarLongWritable;
1: import org.apache.mahout.math.Vector;
1: import org.apache.mahout.math.VectorWritable;
/////////////////////////////////////////////////////////////////////////
/////////////////////////////////////////////////////////////////////////
0:           ratings.put(value.getIDIndex(), value.getRating());
/////////////////////////////////////////////////////////////////////////
0:     private static final Random random = RandomUtils.getRandom();
author:dfilimon
-------------------------------------------------------------------------------
commit:87d4b2e
/////////////////////////////////////////////////////////////////////////
0: import java.io.IOException;
0: import java.util.List;
0: import java.util.Map;
0: import java.util.Random;
0: 
/////////////////////////////////////////////////////////////////////////
/////////////////////////////////////////////////////////////////////////
0:       Closeables.close(writer, false);
author:Jacob Alexander Mannix
-------------------------------------------------------------------------------
commit:dc62944
/////////////////////////////////////////////////////////////////////////
/////////////////////////////////////////////////////////////////////////
1:       for (Vector.Element e : averageRatings.nonZeroes()) {
/////////////////////////////////////////////////////////////////////////
1:       for (Vector.Element e : v.get().nonZeroes()) {
1:         avg.addDatum(e.get());
author:Grant Ingersoll
-------------------------------------------------------------------------------
commit:6db7f62
/////////////////////////////////////////////////////////////////////////
1:     Map<String,List<String>> parsedArgs = parseArguments(args);
1:     numFeatures = Integer.parseInt(getOption("numFeatures"));
1:     numIterations = Integer.parseInt(getOption("numIterations"));
1:     lambda = Double.parseDouble(getOption("lambda"));
1:     alpha = Double.parseDouble(getOption("alpha"));
1:     implicitFeedback = Boolean.parseBoolean(getOption("implicitFeedback"));
============================================================================