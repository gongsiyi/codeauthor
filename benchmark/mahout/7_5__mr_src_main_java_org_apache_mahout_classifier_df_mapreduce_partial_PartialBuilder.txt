4:287b72b: /**
1:287b72b:  * Licensed to the Apache Software Foundation (ASF) under one or more
1:287b72b:  * contributor license agreements.  See the NOTICE file distributed with
1:287b72b:  * this work for additional information regarding copyright ownership.
1:287b72b:  * The ASF licenses this file to You under the Apache License, Version 2.0
1:287b72b:  * (the "License"); you may not use this file except in compliance with
1:287b72b:  * the License.  You may obtain a copy of the License at
5:287b72b:  *
1:287b72b:  *     http://www.apache.org/licenses/LICENSE-2.0
1:287b72b:  *
1:287b72b:  * Unless required by applicable law or agreed to in writing, software
1:287b72b:  * distributed under the License is distributed on an "AS IS" BASIS,
1:287b72b:  * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
1:287b72b:  * See the License for the specific language governing permissions and
1:287b72b:  * limitations under the License.
4:287b72b:  */
22:287b72b: 
1:52ce412: package org.apache.mahout.classifier.df.mapreduce.partial;
1:8ca83b1: 
1:d6aba1a: import com.google.common.base.Preconditions;
1:287b72b: import org.apache.hadoop.conf.Configuration;
1:287b72b: import org.apache.hadoop.fs.FileSystem;
1:287b72b: import org.apache.hadoop.fs.Path;
1:287b72b: import org.apache.hadoop.mapreduce.Job;
1:2bc819e: import org.apache.hadoop.mapreduce.JobContext;
1:287b72b: import org.apache.hadoop.mapreduce.lib.input.FileInputFormat;
1:287b72b: import org.apache.hadoop.mapreduce.lib.input.TextInputFormat;
1:287b72b: import org.apache.hadoop.mapreduce.lib.output.FileOutputFormat;
1:287b72b: import org.apache.hadoop.mapreduce.lib.output.SequenceFileOutputFormat;
1:52ce412: import org.apache.mahout.classifier.df.DFUtils;
1:52ce412: import org.apache.mahout.classifier.df.DecisionForest;
1:52ce412: import org.apache.mahout.classifier.df.builder.TreeBuilder;
1:52ce412: import org.apache.mahout.classifier.df.mapreduce.Builder;
1:52ce412: import org.apache.mahout.classifier.df.mapreduce.MapredOutput;
1:52ce412: import org.apache.mahout.classifier.df.node.Node;
1:d6aba1a: import org.apache.mahout.common.Pair;
1:d6aba1a: import org.apache.mahout.common.iterator.sequencefile.SequenceFileIterable;
1:4e1b7a6: import org.slf4j.Logger;
1:4e1b7a6: import org.slf4j.LoggerFactory;
1:287b72b: 
1:d6aba1a: import java.io.IOException;
1:d6aba1a: import java.util.Arrays;
1:4e1b7a6: import java.util.List;
1:287b72b: 
1:287b72b: /**
1:ad11134:  * Builds a random forest using partial data. Each mapper uses only the data given by its InputSplit
1:287b72b:  */
1:1ffa3a4: @Deprecated
1:287b72b: public class PartialBuilder extends Builder {
1:e9cc323: 
1:4e1b7a6:   private static final Logger log = LoggerFactory.getLogger(PartialBuilder.class);
1:4e1b7a6: 
1:d6aba1a:   public PartialBuilder(TreeBuilder treeBuilder, Path dataPath, Path datasetPath, Long seed) {
1:287b72b:     this(treeBuilder, dataPath, datasetPath, seed, new Configuration());
1:2563b6e:   }
1:287b72b:   
1:ad11134:   public PartialBuilder(TreeBuilder treeBuilder,
1:ad11134:                         Path dataPath,
1:ad11134:                         Path datasetPath,
1:ad11134:                         Long seed,
1:ad11134:                         Configuration conf) {
1:287b72b:     super(treeBuilder, dataPath, datasetPath, seed, conf);
10:287b72b:   }
1:287b72b: 
1:287b72b:   @Override
1:e9cc323:   protected void configureJob(Job job) throws IOException {
1:287b72b:     Configuration conf = job.getConfiguration();
1:287b72b:     
1:287b72b:     job.setJarByClass(PartialBuilder.class);
1:287b72b:     
1:fc74924:     FileInputFormat.setInputPaths(job, getDataPath());
1:287b72b:     FileOutputFormat.setOutputPath(job, getOutputPath(conf));
1:287b72b:     
1:287b72b:     job.setOutputKeyClass(TreeID.class);
1:287b72b:     job.setOutputValueClass(MapredOutput.class);
1:287b72b:     
1:287b72b:     job.setMapperClass(Step1Mapper.class);
1:287b72b:     job.setNumReduceTasks(0); // no reducers
1:287b72b:     
1:287b72b:     job.setInputFormatClass(TextInputFormat.class);
1:287b72b:     job.setOutputFormatClass(SequenceFileOutputFormat.class);
1:4e1b7a6: 
1:4e1b7a6:     // For this implementation to work, mapred.map.tasks needs to be set to the actual
1:4e1b7a6:     // number of mappers Hadoop will use:
1:4e1b7a6:     TextInputFormat inputFormat = new TextInputFormat();
1:4e1b7a6:     List<?> splits = inputFormat.getSplits(job);
1:4e1b7a6:     if (splits == null || splits.isEmpty()) {
1:4e1b7a6:       log.warn("Unable to compute number of splits?");
1:4e1b7a6:     } else {
1:4e1b7a6:       int numSplits = splits.size();
1:4e1b7a6:       log.info("Setting mapred.map.tasks = {}", numSplits);
1:4e1b7a6:       conf.setInt("mapred.map.tasks", numSplits);
1:4e1b7a6:     }
1:287b72b:   }
1:287b72b:   
1:287b72b:   @Override
1:4194a28:   protected DecisionForest parseOutput(Job job) throws IOException {
1:287b72b:     Configuration conf = job.getConfiguration();
1:287b72b:     
1:ad11134:     int numTrees = Builder.getNbTrees(conf);
1:287b72b:     
1:287b72b:     Path outputPath = getOutputPath(conf);
1:287b72b:     
1:287b72b:     TreeID[] keys = new TreeID[numTrees];
1:287b72b:     Node[] trees = new Node[numTrees];
1:ac83cf3:         
1:4fbfbc6:     processOutput(job, outputPath, keys, trees);
1:8ca83b1:     
1:287b72b:     return new DecisionForest(Arrays.asList(trees));
1:8ca83b1:   }
1:8ca83b1:   
1:287b72b:   /**
1:287b72b:    * Processes the output from the output path.<br>
1:287b72b:    * 
1:ad11134:    * @param outputPath
1:ad11134:    *          directory that contains the output of the job
2:287b72b:    * @param keys
2:ad11134:    *          can be null
1:ad11134:    * @param trees
1:ad11134:    *          can be null
1:e9cc323:    * @throws java.io.IOException
1:287b72b:    */
1:ad11134:   protected static void processOutput(JobContext job,
1:ad11134:                                       Path outputPath,
1:ad11134:                                       TreeID[] keys,
1:ac83cf3:                                       Node[] trees) throws IOException {
1:39fe224:     Preconditions.checkArgument(keys == null && trees == null || keys != null && trees != null,
1:d61a0ee:         "if keys is null, trees should also be null");
1:d61a0ee:     Preconditions.checkArgument(keys == null || keys.length == trees.length, "keys.length != trees.length");
1:8ca83b1: 
1:287b72b:     Configuration conf = job.getConfiguration();
1:287b72b: 
1:287b72b:     FileSystem fs = outputPath.getFileSystem(conf);
1:287b72b: 
1:287b72b:     Path[] outfiles = DFUtils.listOutputFiles(fs, outputPath);
1:287b72b: 
1:287b72b:     // read all the outputs
1:287b72b:     int index = 0;
1:287b72b:     for (Path path : outfiles) {
1:a13b4b7:       for (Pair<TreeID,MapredOutput> record : new SequenceFileIterable<TreeID, MapredOutput>(path, conf)) {
1:a13b4b7:         TreeID key = record.getFirst();
1:a13b4b7:         MapredOutput value = record.getSecond();
1:287b72b:         if (keys != null) {
1:a13b4b7:           keys[index] = key;
1:287b72b:         }
1:287b72b:         if (trees != null) {
1:287b72b:           trees[index] = value.getTree();
1:8ca83b1:         }
1:287b72b:         index++;
1:287b72b:       }
1:287b72b:     }
1:287b72b: 
1:287b72b:     // make sure we got all the keys/values
1:2563b6e:     if (keys != null && index != keys.length) {
1:2563b6e:       throw new IllegalStateException("Some key/values are missing from the output");
1:287b72b:     }
1:287b72b:   }
1:287b72b: }
============================================================================
author:smarthi
-------------------------------------------------------------------------------
commit:1ffa3a4
/////////////////////////////////////////////////////////////////////////
1: @Deprecated
commit:4e1b7a6
/////////////////////////////////////////////////////////////////////////
1: import org.slf4j.Logger;
1: import org.slf4j.LoggerFactory;
1: import java.util.List;
1:   private static final Logger log = LoggerFactory.getLogger(PartialBuilder.class);
1: 
/////////////////////////////////////////////////////////////////////////
1: 
1:     // For this implementation to work, mapred.map.tasks needs to be set to the actual
1:     // number of mappers Hadoop will use:
1:     TextInputFormat inputFormat = new TextInputFormat();
1:     List<?> splits = inputFormat.getSplits(job);
1:     if (splits == null || splits.isEmpty()) {
1:       log.warn("Unable to compute number of splits?");
1:     } else {
1:       int numSplits = splits.size();
1:       log.info("Setting mapred.map.tasks = {}", numSplits);
1:       conf.setInt("mapred.map.tasks", numSplits);
1:     }
author:pferrel
-------------------------------------------------------------------------------
commit:b988c49
author:frankscholten
-------------------------------------------------------------------------------
commit:1a42d85
author:Abdel Hakim Deneche
-------------------------------------------------------------------------------
commit:d6aba1a
/////////////////////////////////////////////////////////////////////////
1: import com.google.common.base.Preconditions;
/////////////////////////////////////////////////////////////////////////
1: import org.apache.mahout.common.Pair;
1: import org.apache.mahout.common.iterator.sequencefile.SequenceFileIterable;
1: import java.io.IOException;
1: import java.util.Arrays;
1:   public PartialBuilder(TreeBuilder treeBuilder, Path dataPath, Path datasetPath, Long seed) {
commit:e9cc323
/////////////////////////////////////////////////////////////////////////
/////////////////////////////////////////////////////////////////////////
1: 
0:     public PartialBuilder(TreeBuilder treeBuilder, Path dataPath, Path datasetPath, Long seed) {
/////////////////////////////////////////////////////////////////////////
1:   protected void configureJob(Job job) throws IOException {
/////////////////////////////////////////////////////////////////////////
1:    * @throws java.io.IOException
commit:ac83cf3
/////////////////////////////////////////////////////////////////////////
/////////////////////////////////////////////////////////////////////////
/////////////////////////////////////////////////////////////////////////
0:   protected void configureJob(Job job, int nbTrees) throws IOException {
/////////////////////////////////////////////////////////////////////////
0:   protected DecisionForest parseOutput(Job job)
/////////////////////////////////////////////////////////////////////////
1:         
0:     processOutput(job, outputPath, firstIds, keys, trees);
/////////////////////////////////////////////////////////////////////////
1:                                       Node[] trees) throws IOException {
/////////////////////////////////////////////////////////////////////////
/////////////////////////////////////////////////////////////////////////
commit:8ca83b1
/////////////////////////////////////////////////////////////////////////
0:     int[] firstIds = null;
0:     Step0Output[] partitions = null;
0:     int numMaps = 0;
0:     if (callback != null) {
0: 	    log.info("Computing partitions' first ids...");
0: 	    Step0Job step0 = new Step0Job(getOutputPath(conf), getDataPath(), getDatasetPath());
0: 	    partitions = step0.run(new Configuration(conf));
1: 	    
0: 	    log.info("Processing the output...");
0: 	    firstIds = Step0Output.extractFirstIds(partitions);
1: 	    
0: 	    numMaps = partitions.length;
1:     }
1:     
0:     processOutput(job, outputPath, firstIds, keys, trees, callback);
/////////////////////////////////////////////////////////////////////////
0:         if (callback != null) {
0:         	processOutput(firstIds, key, value, callback);
1:         }
/////////////////////////////////////////////////////////////////////////
0:     int[] predictions = value.getPredictions();
1:     
0:     for (int instanceId = 0; instanceId < predictions.length; instanceId++) {
0:       callback.prediction(key.treeId(), firstIds[key.partition()] + instanceId, predictions[instanceId]);
commit:2563b6e
/////////////////////////////////////////////////////////////////////////
0:    * @param keys can be null
0:    * @param trees can be null
0:     if ((keys != null && trees == null)||(keys == null && trees != null)) {
0:       throw new IllegalArgumentException("if keys is null, trees should also be null");
1:     }
0:     if (keys != null && keys.length != trees.length) {
/////////////////////////////////////////////////////////////////////////
1:     if (keys != null && index != keys.length) {
1:       throw new IllegalStateException("Some key/values are missing from the output");
commit:287b72b
/////////////////////////////////////////////////////////////////////////
1: /**
1:  * Licensed to the Apache Software Foundation (ASF) under one or more
1:  * contributor license agreements.  See the NOTICE file distributed with
1:  * this work for additional information regarding copyright ownership.
1:  * The ASF licenses this file to You under the Apache License, Version 2.0
1:  * (the "License"); you may not use this file except in compliance with
1:  * the License.  You may obtain a copy of the License at
1:  *
1:  *     http://www.apache.org/licenses/LICENSE-2.0
1:  *
1:  * Unless required by applicable law or agreed to in writing, software
1:  * distributed under the License is distributed on an "AS IS" BASIS,
1:  * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
1:  * See the License for the specific language governing permissions and
1:  * limitations under the License.
1:  */
1: 
0: package org.apache.mahout.df.mapreduce.partial;
1: 
0: import java.io.IOException;
0: import java.util.Arrays;
1: 
1: import org.apache.hadoop.conf.Configuration;
1: import org.apache.hadoop.fs.FileSystem;
1: import org.apache.hadoop.fs.Path;
0: import org.apache.hadoop.io.SequenceFile.Reader;
1: import org.apache.hadoop.mapreduce.Job;
1: import org.apache.hadoop.mapreduce.lib.input.FileInputFormat;
1: import org.apache.hadoop.mapreduce.lib.input.TextInputFormat;
1: import org.apache.hadoop.mapreduce.lib.output.FileOutputFormat;
1: import org.apache.hadoop.mapreduce.lib.output.SequenceFileOutputFormat;
0: import org.apache.mahout.df.DFUtils;
0: import org.apache.mahout.df.DecisionForest;
0: import org.apache.mahout.df.builder.TreeBuilder;
0: import org.apache.mahout.df.callback.PredictionCallback;
0: import org.apache.mahout.df.mapreduce.Builder;
0: import org.apache.mahout.df.mapreduce.partial.Step0Job.Step0Output;
0: import org.apache.mahout.df.mapreduce.MapredOutput;
0: import org.apache.mahout.df.node.Node;
0: import org.slf4j.Logger;
0: import org.slf4j.LoggerFactory;
1: 
1: /**
0:  * Builds a random forest using partial data. Each mapper uses only the data
0:  * given by its InputSplit
1:  */
1: public class PartialBuilder extends Builder {
1: 
0:   private final static Logger log = LoggerFactory.getLogger(PartialBuilder.class);
1: 
1:   /**
0:    * Indicates if we should run the second step of the builder.<br>
0:    * This parameter is only meant for debuging, so we keep it protected.
1:    * 
0:    * @param conf
0:    * @return
1:    */
0:   protected static boolean isStep2(Configuration conf) {
0:     return conf.getBoolean("debug.mahout.rf.partial.step2", true);
1:   }
1: 
1:   /**
0:    * Should run the second step of the builder ?
1:    * 
0:    * @param conf
0:    * @param value true to indicate that the second step will be launched
1:    * 
1:    */
0:   protected static void setStep2(Configuration conf, boolean value) {
0:     conf.setBoolean("debug.mahout.rf.partial.step2", value);
1:   }
1: 
0:   public PartialBuilder(TreeBuilder treeBuilder, Path dataPath,
0:       Path datasetPath, Long seed) {
1:     this(treeBuilder, dataPath, datasetPath, seed, new Configuration());
1:   }
1: 
0:   public PartialBuilder(TreeBuilder treeBuilder, Path dataPath,
0:       Path datasetPath, Long seed, Configuration conf) {
1:     super(treeBuilder, dataPath, datasetPath, seed, conf);
1:   }
1: 
1:   
1:   @Override
0:   protected void configureJob(Job job, int nbTrees, boolean oobEstimate)
0:       throws IOException {
1:     Configuration conf = job.getConfiguration();
1:     
1:     job.setJarByClass(PartialBuilder.class);
1:     
0:     FileInputFormat.setInputPaths(job, dataPath);
1:     FileOutputFormat.setOutputPath(job, getOutputPath(conf));
1: 
1:     job.setOutputKeyClass(TreeID.class);
1:     job.setOutputValueClass(MapredOutput.class);
1: 
1:     job.setMapperClass(Step1Mapper.class);
1:     job.setNumReduceTasks(0); // no reducers
1: 
1:     job.setInputFormatClass(TextInputFormat.class);
1:     job.setOutputFormatClass(SequenceFileOutputFormat.class);
1:   }
1: 
1:   @Override
0:   protected DecisionForest parseOutput(Job job, PredictionCallback callback)
0:       throws Exception {
1:     Configuration conf = job.getConfiguration();
1:     
0:     int numTrees = getNbTrees(conf);
1: 
1:     Path outputPath = getOutputPath(conf);
1: 
0:     log.info("Computing partitions' first ids...");
0:     Step0Job step0 = new Step0Job(getOutputPath(conf), dataPath, datasetPath);
0:     Step0Output[] partitions = step0.run(new Configuration(conf));
1: 
0:     log.info("Processing the output...");
1:     TreeID[] keys = new TreeID[numTrees];
1:     Node[] trees = new Node[numTrees];
0:     int[] firstIds = Step0Output.extractFirstIds(partitions);
0:     processOutput(job, outputPath, firstIds, keys, trees, callback);
1: 
0:     // JobClient should have updated numMaps to the correct number of maps
0:     int numMaps = partitions.length;
1: 
0:     // call the second step in order to complete the oob predictions
0:     if (callback != null && numMaps > 1 && isStep2(conf)) {
0:       log.info("*****************************");
0:       log.info("Second Step");
0:       log.info("*****************************");
0:       Step2Job step2 = new Step2Job(getOutputPath(conf), dataPath, datasetPath, partitions);
1: 
0:       step2.run(new Configuration(conf), keys, trees, callback);
1:     }
1: 
1:     return new DecisionForest(Arrays.asList(trees));
1:   }
1: 
1:   /**
1:    * Processes the output from the output path.<br>
1:    * 
0:    * @param job
0:    * @param outputPath directory that contains the output of the job
0:    * @param firstIds partitions' first ids in hadoop's order
1:    * @param keys
0:    * @param values
0:    * @param callback can be null
0:    * @throws IOException
1:    */
0:   protected static void processOutput(Job job, Path outputPath,
0:       int[] firstIds, TreeID[] keys, Node[] trees, PredictionCallback callback)
0:       throws IOException {
0:     assert keys.length == trees.length : "keys.length != trees.length";
1:     
1:     Configuration conf = job.getConfiguration();
1:     
1:     FileSystem fs = outputPath.getFileSystem(conf);
1: 
1:     Path[] outfiles = DFUtils.listOutputFiles(fs, outputPath);
1: 
1:     // read all the outputs
0:     TreeID key = new TreeID();
0:     MapredOutput value = new MapredOutput();
1:     
1:     int index = 0;
1:     for (Path path : outfiles) {
0:       Reader reader = new Reader(fs, path, conf);
1: 
0:       try {
0:         while (reader.next(key, value)) {
1:           if (keys != null) {
0:             keys[index] = key.clone();
1:           }
1:           
1:           if (trees != null) {
1:             trees[index] = value.getTree();
1:           }
1:           
0:           processOutput(firstIds, key, value, callback);
1:           
1:           index++;
1:         }
0:       } finally {
0:         reader.close();
1:       }
1:     }
1: 
1:     // make sure we got all the keys/values
0:     assert index == keys.length;
1:   }
1: 
1:   /**
0:    * Process the output, extracting the trees and passing the predictions to the
0:    * callback
1:    * 
0:    * @param firstIds partitions' first ids in hadoop's order
1:    * @param keys
0:    * @param values
0:    * @param callback
0:    * @return
1:    */
0:   private static void processOutput(int[] firstIds, TreeID key,
0:       MapredOutput value, PredictionCallback callback) {
1: 
0:     if (callback != null) {
0:       int[] predictions = value.getPredictions();
1: 
0:       for (int instanceId = 0; instanceId < predictions.length; instanceId++) {
0:         callback.prediction(key.treeId(), firstIds[key.partition()] + instanceId, 
0:             predictions[instanceId]);
1:       }
1:     }
1:   }
1: }
author:Sean R. Owen
-------------------------------------------------------------------------------
commit:4fbfbc6
/////////////////////////////////////////////////////////////////////////
1:     processOutput(job, outputPath, keys, trees);
/////////////////////////////////////////////////////////////////////////
/////////////////////////////////////////////////////////////////////////
commit:4194a28
/////////////////////////////////////////////////////////////////////////
1:   protected DecisionForest parseOutput(Job job) throws IOException {
commit:39fe224
/////////////////////////////////////////////////////////////////////////
0:     if (callback != null && numMaps > 1 && isStep2(conf)) {
/////////////////////////////////////////////////////////////////////////
1:     Preconditions.checkArgument(keys == null && trees == null || keys != null && trees != null,
commit:a13b4b7
/////////////////////////////////////////////////////////////////////////
0: import org.apache.mahout.common.Pair;
0: import org.apache.mahout.common.iterator.sequencefile.SequenceFileIterable;
/////////////////////////////////////////////////////////////////////////
0:   protected DecisionForest parseOutput(Job job, PredictionCallback callback)
0:     throws IOException, ClassNotFoundException, InterruptedException {
/////////////////////////////////////////////////////////////////////////
/////////////////////////////////////////////////////////////////////////
1:       for (Pair<TreeID,MapredOutput> record : new SequenceFileIterable<TreeID, MapredOutput>(path, conf)) {
1:         TreeID key = record.getFirst();
1:         MapredOutput value = record.getSecond();
0:         if (keys != null) {
1:           keys[index] = key;
0:         if (trees != null) {
0:           trees[index] = value.getTree();
0:         }
0:         processOutput(firstIds, key, value, callback);
0:         index++;
0:     if (keys != null && index != keys.length) {
commit:d61a0ee
/////////////////////////////////////////////////////////////////////////
1:         "if keys is null, trees should also be null");
1:     Preconditions.checkArgument(keys == null || keys.length == trees.length, "keys.length != trees.length");
commit:69ba194
/////////////////////////////////////////////////////////////////////////
0: import com.google.common.base.Preconditions;
0: 
/////////////////////////////////////////////////////////////////////////
0:     Preconditions.checkArgument((keys == null && trees == null) || (keys != null && trees != null),
0:         "if keys is null, trees should also be null" );
0:     Preconditions.checkArgument(keys == null || keys.length == trees.length, "keys.length != trees.length" );
0: 
0: 
0: 
0: 
0: 
0: 
0: 
0: 
0: 
0: 
commit:515bac4
/////////////////////////////////////////////////////////////////////////
/////////////////////////////////////////////////////////////////////////
/////////////////////////////////////////////////////////////////////////
commit:f824f90
commit:210fac3
commit:2bc819e
/////////////////////////////////////////////////////////////////////////
1: import org.apache.hadoop.mapreduce.JobContext;
/////////////////////////////////////////////////////////////////////////
0:   protected static void processOutput(JobContext job, Path outputPath,
commit:fc74924
/////////////////////////////////////////////////////////////////////////
1:     FileInputFormat.setInputPaths(job, getDataPath());
/////////////////////////////////////////////////////////////////////////
0:     Step0Job step0 = new Step0Job(getOutputPath(conf), getDataPath(), getDatasetPath());
/////////////////////////////////////////////////////////////////////////
0:       Step2Job step2 = new Step2Job(getOutputPath(conf), getDataPath(), getDatasetPath(), partitions);
commit:7f0d774
/////////////////////////////////////////////////////////////////////////
0:     if (keys.length != trees.length) {
0:       throw new IllegalArgumentException("keys.length != trees.length");
0:     }
/////////////////////////////////////////////////////////////////////////
0:     if (index != keys.length) {
0:       throw new IllegalStateException();
0:     }
commit:8547de7
/////////////////////////////////////////////////////////////////////////
0:   private static final Logger log = LoggerFactory.getLogger(PartialBuilder.class);
/////////////////////////////////////////////////////////////////////////
0:       throws IOException, ClassNotFoundException, InterruptedException {
/////////////////////////////////////////////////////////////////////////
/////////////////////////////////////////////////////////////////////////
author:Grant Ingersoll
-------------------------------------------------------------------------------
commit:52ce412
/////////////////////////////////////////////////////////////////////////
1: package org.apache.mahout.classifier.df.mapreduce.partial;
/////////////////////////////////////////////////////////////////////////
1: import org.apache.mahout.classifier.df.DFUtils;
1: import org.apache.mahout.classifier.df.DecisionForest;
1: import org.apache.mahout.classifier.df.builder.TreeBuilder;
1: import org.apache.mahout.classifier.df.mapreduce.Builder;
1: import org.apache.mahout.classifier.df.mapreduce.MapredOutput;
1: import org.apache.mahout.classifier.df.node.Node;
author:Robin Anil
-------------------------------------------------------------------------------
commit:297bef5
/////////////////////////////////////////////////////////////////////////
0:     log.info("Computing partitions' first ids...");
0:     log.info("Processing the output...");
/////////////////////////////////////////////////////////////////////////
0:       log.info("*****************************");
0:       log.info("Second Step");
0:       log.info("*****************************");
commit:ad11134
/////////////////////////////////////////////////////////////////////////
0: import org.apache.mahout.df.mapreduce.partial.Step0Job.Step0Output;
1:  * Builds a random forest using partial data. Each mapper uses only the data given by its InputSplit
0:   
0:   
/////////////////////////////////////////////////////////////////////////
0:   
0:    * @param value
0:    *          true to indicate that the second step will be launched
0:   
0:   public PartialBuilder(TreeBuilder treeBuilder, Path dataPath, Path datasetPath, Long seed) {
0:   
1:   public PartialBuilder(TreeBuilder treeBuilder,
1:                         Path dataPath,
1:                         Path datasetPath,
1:                         Long seed,
1:                         Configuration conf) {
0:   protected void configureJob(Job job, int nbTrees, boolean oobEstimate) throws IOException {
0:     
0:     
0:     
0:   
0:   protected DecisionForest parseOutput(Job job, PredictionCallback callback) throws IOException,
0:                                                                             ClassNotFoundException,
0:                                                                             InterruptedException {
1:     int numTrees = Builder.getNbTrees(conf);
0:     
0:     
0:     PartialBuilder.log.info("Computing partitions' first ids...");
0:     
0:     PartialBuilder.log.info("Processing the output...");
0:     PartialBuilder.processOutput(job, outputPath, firstIds, keys, trees, callback);
0:     
0:     
0:     if ((callback != null) && (numMaps > 1) && PartialBuilder.isStep2(conf)) {
0:       PartialBuilder.log.info("*****************************");
0:       PartialBuilder.log.info("Second Step");
0:       PartialBuilder.log.info("*****************************");
0:       
0:     
0:   
1:    * @param outputPath
1:    *          directory that contains the output of the job
0:    * @param firstIds
0:    *          partitions' first ids in hadoop's order
0:    * @param keys
1:    *          can be null
1:    * @param trees
1:    *          can be null
0:    * @param callback
1:    *          can be null
1:   protected static void processOutput(JobContext job,
1:                                       Path outputPath,
0:                                       int[] firstIds,
1:                                       TreeID[] keys,
0:                                       Node[] trees,
0:                                       PredictionCallback callback) throws IOException {
0:     if (((keys != null) && (trees == null)) || ((keys == null) && (trees != null))) {
0:     if ((keys != null) && (keys.length != trees.length)) {
0:     
0:     
/////////////////////////////////////////////////////////////////////////
0:       
/////////////////////////////////////////////////////////////////////////
0:           PartialBuilder.processOutput(firstIds, key, value, callback);
/////////////////////////////////////////////////////////////////////////
0:     
0:     if ((keys != null) && (index != keys.length)) {
0:   
0:    * Process the output, extracting the trees and passing the predictions to the callback
0:    * @param firstIds
0:    *          partitions' first ids in hadoop's order
0:   private static void processOutput(int[] firstIds,
0:                                     TreeID key,
0:                                     MapredOutput value,
0:                                     PredictionCallback callback) {
0:     
0:       
0:         callback.prediction(key.treeId(), firstIds[key.partition()] + instanceId, predictions[instanceId]);
============================================================================