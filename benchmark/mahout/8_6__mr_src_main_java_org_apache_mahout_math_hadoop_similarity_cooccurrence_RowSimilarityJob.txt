1:845cbcd: /**
1:845cbcd:  * Licensed to the Apache Software Foundation (ASF) under one or more
1:845cbcd:  * contributor license agreements.  See the NOTICE file distributed with
1:845cbcd:  * this work for additional information regarding copyright ownership.
1:845cbcd:  * The ASF licenses this file to You under the Apache License, Version 2.0
1:845cbcd:  * (the "License"); you may not use this file except in compliance with
1:845cbcd:  * the License.  You may obtain a copy of the License at
1:845cbcd:  *
1:845cbcd:  *     http://www.apache.org/licenses/LICENSE-2.0
1:845cbcd:  *
1:845cbcd:  * Unless required by applicable law or agreed to in writing, software
1:845cbcd:  * distributed under the License is distributed on an "AS IS" BASIS,
1:845cbcd:  * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
1:845cbcd:  * See the License for the specific language governing permissions and
1:845cbcd:  * limitations under the License.
1:845cbcd:  */
1:d95bdda: 
1:845cbcd: package org.apache.mahout.math.hadoop.similarity.cooccurrence;
3:845cbcd: 
1:845cbcd: import com.google.common.base.Preconditions;
1:845cbcd: import com.google.common.primitives.Ints;
1:845cbcd: import org.apache.hadoop.conf.Configuration;
1:845cbcd: import org.apache.hadoop.fs.Path;
1:845cbcd: import org.apache.hadoop.io.IntWritable;
1:e90d901: import org.apache.hadoop.io.NullWritable;
1:845cbcd: import org.apache.hadoop.mapreduce.Job;
1:845cbcd: import org.apache.hadoop.mapreduce.Mapper;
1:845cbcd: import org.apache.hadoop.mapreduce.Reducer;
1:845cbcd: import org.apache.hadoop.util.ToolRunner;
1:845cbcd: import org.apache.mahout.common.AbstractJob;
1:845cbcd: import org.apache.mahout.common.ClassUtils;
1:298eef9: import org.apache.mahout.common.HadoopUtil;
1:e90d901: import org.apache.mahout.common.RandomUtils;
1:298eef9: import org.apache.mahout.common.commandline.DefaultOptionCreator;
1:e90d901: import org.apache.mahout.common.mapreduce.VectorSumCombiner;
1:845cbcd: import org.apache.mahout.common.mapreduce.VectorSumReducer;
1:845cbcd: import org.apache.mahout.math.RandomAccessSparseVector;
1:845cbcd: import org.apache.mahout.math.Vector;
1:dc62944: import org.apache.mahout.math.Vector.Element;
1:845cbcd: import org.apache.mahout.math.VectorWritable;
1:845cbcd: import org.apache.mahout.math.hadoop.similarity.cooccurrence.measures.VectorSimilarityMeasures;
1:845cbcd: import org.apache.mahout.math.hadoop.similarity.cooccurrence.measures.VectorSimilarityMeasure;
1:845cbcd: import org.apache.mahout.math.map.OpenIntIntHashMap;
1:845cbcd: 
1:845cbcd: import java.io.IOException;
1:845cbcd: import java.util.Arrays;
1:845cbcd: import java.util.Comparator;
1:845cbcd: import java.util.Iterator;
1:6db7f62: import java.util.List;
1:845cbcd: import java.util.Map;
1:e90d901: import java.util.Random;
1:845cbcd: import java.util.concurrent.atomic.AtomicInteger;
1:845cbcd: 
1:845cbcd: public class RowSimilarityJob extends AbstractJob {
1:845cbcd: 
1:b70621d:   public static final double NO_THRESHOLD = Double.MIN_VALUE;
1:e90d901:   public static final long NO_FIXED_RANDOM_SEED = Long.MIN_VALUE;
1:b70621d: 
1:5d66758:   private static final String SIMILARITY_CLASSNAME = RowSimilarityJob.class + ".distributedSimilarityClassname";
1:5d66758:   private static final String NUMBER_OF_COLUMNS = RowSimilarityJob.class + ".numberOfColumns";
1:5d66758:   private static final String MAX_SIMILARITIES_PER_ROW = RowSimilarityJob.class + ".maxSimilaritiesPerRow";
1:5d66758:   private static final String EXCLUDE_SELF_SIMILARITY = RowSimilarityJob.class + ".excludeSelfSimilarity";
1:845cbcd: 
1:5d66758:   private static final String THRESHOLD = RowSimilarityJob.class + ".threshold";
1:5d66758:   private static final String NORMS_PATH = RowSimilarityJob.class + ".normsPath";
1:5d66758:   private static final String MAXVALUES_PATH = RowSimilarityJob.class + ".maxWeightsPath";
1:845cbcd: 
1:5d66758:   private static final String NUM_NON_ZERO_ENTRIES_PATH = RowSimilarityJob.class + ".nonZeroEntriesPath";
1:845cbcd:   private static final int DEFAULT_MAX_SIMILARITIES_PER_ROW = 100;
1:845cbcd: 
1:e90d901:   private static final String OBSERVATIONS_PER_COLUMN_PATH = RowSimilarityJob.class + ".observationsPerColumnPath";
1:e90d901: 
1:e90d901:   private static final String MAX_OBSERVATIONS_PER_ROW = RowSimilarityJob.class + ".maxObservationsPerRow";
1:e90d901:   private static final String MAX_OBSERVATIONS_PER_COLUMN = RowSimilarityJob.class + ".maxObservationsPerColumn";
1:e90d901:   private static final String RANDOM_SEED = RowSimilarityJob.class + ".randomSeed";
1:e90d901: 
1:e90d901:   private static final int DEFAULT_MAX_OBSERVATIONS_PER_ROW = 500;
1:e90d901:   private static final int DEFAULT_MAX_OBSERVATIONS_PER_COLUMN = 500;
1:e90d901: 
1:845cbcd:   private static final int NORM_VECTOR_MARKER = Integer.MIN_VALUE;
1:845cbcd:   private static final int MAXVALUE_VECTOR_MARKER = Integer.MIN_VALUE + 1;
1:845cbcd:   private static final int NUM_NON_ZERO_ENTRIES_VECTOR_MARKER = Integer.MIN_VALUE + 2;
1:845cbcd: 
1:e90d901:   enum Counters { ROWS, USED_OBSERVATIONS, NEGLECTED_OBSERVATIONS, COOCCURRENCES, PRUNED_COOCCURRENCES }
1:845cbcd: 
1:845cbcd:   public static void main(String[] args) throws Exception {
1:845cbcd:     ToolRunner.run(new RowSimilarityJob(), args);
1:845cbcd:   }
1:b70621d: 
1:845cbcd:   @Override
1:845cbcd:   public int run(String[] args) throws Exception {
1:845cbcd: 
1:845cbcd:     addInputOption();
1:845cbcd:     addOutputOption();
1:3bf7a1c:     addOption("numberOfColumns", "r", "Number of columns in the input matrix", false);
1:845cbcd:     addOption("similarityClassname", "s", "Name of distributed similarity class to instantiate, alternatively use "
1:845cbcd:         + "one of the predefined similarities (" + VectorSimilarityMeasures.list() + ')');
1:845cbcd:     addOption("maxSimilaritiesPerRow", "m", "Number of maximum similarities per row (default: "
1:845cbcd:         + DEFAULT_MAX_SIMILARITIES_PER_ROW + ')', String.valueOf(DEFAULT_MAX_SIMILARITIES_PER_ROW));
1:845cbcd:     addOption("excludeSelfSimilarity", "ess", "compute similarity of rows to themselves?", String.valueOf(false));
1:b70621d:     addOption("threshold", "tr", "discard row pairs with a similarity value below this", false);
1:e90d901:     addOption("maxObservationsPerRow", null, "sample rows down to this number of entries",
1:e90d901:         String.valueOf(DEFAULT_MAX_OBSERVATIONS_PER_ROW));
1:e90d901:     addOption("maxObservationsPerColumn", null, "sample columns down to this number of entries",
1:e90d901:         String.valueOf(DEFAULT_MAX_OBSERVATIONS_PER_COLUMN));
1:e90d901:     addOption("randomSeed", null, "use this seed for sampling", false);
1:298eef9:     addOption(DefaultOptionCreator.overwriteOption().create());
1:845cbcd: 
1:6db7f62:     Map<String,List<String>> parsedArgs = parseArguments(args);
1:845cbcd:     if (parsedArgs == null) {
1:845cbcd:       return -1;
1:845cbcd:     }
1:845cbcd: 
1:3bf7a1c:     int numberOfColumns;
1:3bf7a1c: 
1:3bf7a1c:     if (hasOption("numberOfColumns")) {
1:3bf7a1c:       // Number of columns explicitly specified via CLI
1:3bf7a1c:       numberOfColumns = Integer.parseInt(getOption("numberOfColumns"));
1:3bf7a1c:     } else {
1:3bf7a1c:       // else get the number of columns by determining the cardinality of a vector in the input matrix
1:3bf7a1c:       numberOfColumns = getDimensions(getInputPath());
1:3bf7a1c:     }
1:3bf7a1c: 
1:6db7f62:     String similarityClassnameArg = getOption("similarityClassname");
1:845cbcd:     String similarityClassname;
1:845cbcd:     try {
1:845cbcd:       similarityClassname = VectorSimilarityMeasures.valueOf(similarityClassnameArg).getClassname();
1:845cbcd:     } catch (IllegalArgumentException iae) {
1:845cbcd:       similarityClassname = similarityClassnameArg;
1:845cbcd:     }
1:845cbcd: 
1:298eef9:     // Clear the output and temp paths if the overwrite option has been set
1:298eef9:     if (hasOption(DefaultOptionCreator.OVERWRITE_OPTION)) {
1:298eef9:       // Clear the temp path
1:298eef9:       HadoopUtil.delete(getConf(), getTempPath());
1:298eef9:       // Clear the output path
1:298eef9:       HadoopUtil.delete(getConf(), getOutputPath());
1:298eef9:     }
1:298eef9: 
1:6db7f62:     int maxSimilaritiesPerRow = Integer.parseInt(getOption("maxSimilaritiesPerRow"));
1:6db7f62:     boolean excludeSelfSimilarity = Boolean.parseBoolean(getOption("excludeSelfSimilarity"));
1:6d16230:     double threshold = hasOption("threshold")
1:6d16230:         ? Double.parseDouble(getOption("threshold")) : NO_THRESHOLD;
1:e90d901:     long randomSeed = hasOption("randomSeed")
1:e90d901:         ? Long.parseLong(getOption("randomSeed")) : NO_FIXED_RANDOM_SEED;
1:e90d901: 
1:e90d901:     int maxObservationsPerRow = Integer.parseInt(getOption("maxObservationsPerRow"));
1:e90d901:     int maxObservationsPerColumn = Integer.parseInt(getOption("maxObservationsPerColumn"));
1:845cbcd: 
1:845cbcd:     Path weightsPath = getTempPath("weights");
1:845cbcd:     Path normsPath = getTempPath("norms.bin");
1:845cbcd:     Path numNonZeroEntriesPath = getTempPath("numNonZeroEntries.bin");
1:845cbcd:     Path maxValuesPath = getTempPath("maxValues.bin");
1:845cbcd:     Path pairwiseSimilarityPath = getTempPath("pairwiseSimilarity");
1:845cbcd: 
1:e90d901:     Path observationsPerColumnPath = getTempPath("observationsPerColumn.bin");
1:e90d901: 
1:845cbcd:     AtomicInteger currentPhase = new AtomicInteger();
1:845cbcd: 
1:e90d901:     Job countObservations = prepareJob(getInputPath(), getTempPath("notUsed"), CountObservationsMapper.class,
1:e90d901:         NullWritable.class, VectorWritable.class, SumObservationsReducer.class, NullWritable.class,
1:e90d901:         VectorWritable.class);
1:e90d901:     countObservations.setCombinerClass(VectorSumCombiner.class);
1:e90d901:     countObservations.getConfiguration().set(OBSERVATIONS_PER_COLUMN_PATH, observationsPerColumnPath.toString());
1:e90d901:     countObservations.setNumReduceTasks(1);
1:e90d901:     countObservations.waitForCompletion(true);
1:e90d901: 
1:845cbcd:     if (shouldRunNextPhase(parsedArgs, currentPhase)) {
1:845cbcd:       Job normsAndTranspose = prepareJob(getInputPath(), weightsPath, VectorNormMapper.class, IntWritable.class,
1:845cbcd:           VectorWritable.class, MergeVectorsReducer.class, IntWritable.class, VectorWritable.class);
1:845cbcd:       normsAndTranspose.setCombinerClass(MergeVectorsCombiner.class);
1:845cbcd:       Configuration normsAndTransposeConf = normsAndTranspose.getConfiguration();
1:845cbcd:       normsAndTransposeConf.set(THRESHOLD, String.valueOf(threshold));
1:845cbcd:       normsAndTransposeConf.set(NORMS_PATH, normsPath.toString());
1:845cbcd:       normsAndTransposeConf.set(NUM_NON_ZERO_ENTRIES_PATH, numNonZeroEntriesPath.toString());
1:845cbcd:       normsAndTransposeConf.set(MAXVALUES_PATH, maxValuesPath.toString());
1:845cbcd:       normsAndTransposeConf.set(SIMILARITY_CLASSNAME, similarityClassname);
1:e90d901:       normsAndTransposeConf.set(OBSERVATIONS_PER_COLUMN_PATH, observationsPerColumnPath.toString());
1:e90d901:       normsAndTransposeConf.set(MAX_OBSERVATIONS_PER_ROW, String.valueOf(maxObservationsPerRow));
1:e90d901:       normsAndTransposeConf.set(MAX_OBSERVATIONS_PER_COLUMN, String.valueOf(maxObservationsPerColumn));
1:e90d901:       normsAndTransposeConf.set(RANDOM_SEED, String.valueOf(randomSeed));
1:e90d901: 
1:7c2b664:       boolean succeeded = normsAndTranspose.waitForCompletion(true);
1:7c2b664:       if (!succeeded) {
1:7c2b664:         return -1;
1:845cbcd:       }
1:7c2b664:     }
1:845cbcd: 
1:845cbcd:     if (shouldRunNextPhase(parsedArgs, currentPhase)) {
1:845cbcd:       Job pairwiseSimilarity = prepareJob(weightsPath, pairwiseSimilarityPath, CooccurrencesMapper.class,
1:845cbcd:           IntWritable.class, VectorWritable.class, SimilarityReducer.class, IntWritable.class, VectorWritable.class);
1:845cbcd:       pairwiseSimilarity.setCombinerClass(VectorSumReducer.class);
1:845cbcd:       Configuration pairwiseConf = pairwiseSimilarity.getConfiguration();
1:845cbcd:       pairwiseConf.set(THRESHOLD, String.valueOf(threshold));
1:845cbcd:       pairwiseConf.set(NORMS_PATH, normsPath.toString());
1:845cbcd:       pairwiseConf.set(NUM_NON_ZERO_ENTRIES_PATH, numNonZeroEntriesPath.toString());
1:845cbcd:       pairwiseConf.set(MAXVALUES_PATH, maxValuesPath.toString());
1:845cbcd:       pairwiseConf.set(SIMILARITY_CLASSNAME, similarityClassname);
1:845cbcd:       pairwiseConf.setInt(NUMBER_OF_COLUMNS, numberOfColumns);
1:845cbcd:       pairwiseConf.setBoolean(EXCLUDE_SELF_SIMILARITY, excludeSelfSimilarity);
1:7c2b664:       boolean succeeded = pairwiseSimilarity.waitForCompletion(true);
1:7c2b664:       if (!succeeded) {
1:7c2b664:         return -1;
1:845cbcd:       }
1:7c2b664:     }
1:845cbcd: 
1:845cbcd:     if (shouldRunNextPhase(parsedArgs, currentPhase)) {
1:845cbcd:       Job asMatrix = prepareJob(pairwiseSimilarityPath, getOutputPath(), UnsymmetrifyMapper.class,
1:845cbcd:           IntWritable.class, VectorWritable.class, MergeToTopKSimilaritiesReducer.class, IntWritable.class,
1:845cbcd:           VectorWritable.class);
1:845cbcd:       asMatrix.setCombinerClass(MergeToTopKSimilaritiesReducer.class);
1:845cbcd:       asMatrix.getConfiguration().setInt(MAX_SIMILARITIES_PER_ROW, maxSimilaritiesPerRow);
1:7c2b664:       boolean succeeded = asMatrix.waitForCompletion(true);
1:7c2b664:       if (!succeeded) {
1:7c2b664:         return -1;
1:845cbcd:       }
1:7c2b664:     }
1:845cbcd: 
1:845cbcd:     return 0;
1:845cbcd:   }
1:845cbcd: 
1:e90d901:   public static class CountObservationsMapper extends Mapper<IntWritable,VectorWritable,NullWritable,VectorWritable> {
1:e90d901: 
1:e90d901:     private Vector columnCounts = new RandomAccessSparseVector(Integer.MAX_VALUE);
1:e90d901: 
1:e90d901:     @Override
1:e90d901:     protected void map(IntWritable rowIndex, VectorWritable rowVectorWritable, Context ctx)
1:e90d901:       throws IOException, InterruptedException {
1:e90d901: 
1:e90d901:       Vector row = rowVectorWritable.get();
1:e90d901:       for (Vector.Element elem : row.nonZeroes()) {
1:e90d901:         columnCounts.setQuick(elem.index(), columnCounts.getQuick(elem.index()) + 1);
1:e90d901:       }
1:e90d901:     }
1:e90d901: 
1:e90d901:     @Override
1:e90d901:     protected void cleanup(Context ctx) throws IOException, InterruptedException {
1:e90d901:       ctx.write(NullWritable.get(), new VectorWritable(columnCounts));
1:e90d901:     }
1:e90d901:   }
1:e90d901: 
1:e90d901:   public static class SumObservationsReducer extends Reducer<NullWritable,VectorWritable,NullWritable,VectorWritable> {
1:e90d901:     @Override
1:e90d901:     protected void reduce(NullWritable nullWritable, Iterable<VectorWritable> partialVectors, Context ctx)
1:e90d901:     throws IOException, InterruptedException {
1:e90d901:       Vector counts = Vectors.sum(partialVectors.iterator());
1:e90d901:       Vectors.write(counts, new Path(ctx.getConfiguration().get(OBSERVATIONS_PER_COLUMN_PATH)), ctx.getConfiguration());
1:e90d901:     }
1:e90d901:   }
1:e90d901: 
1:845cbcd:   public static class VectorNormMapper extends Mapper<IntWritable,VectorWritable,IntWritable,VectorWritable> {
1:845cbcd: 
1:845cbcd:     private VectorSimilarityMeasure similarity;
1:845cbcd:     private Vector norms;
1:845cbcd:     private Vector nonZeroEntries;
1:845cbcd:     private Vector maxValues;
1:845cbcd:     private double threshold;
1:845cbcd: 
1:e90d901:     private OpenIntIntHashMap observationsPerColumn;
1:e90d901:     private int maxObservationsPerRow;
1:e90d901:     private int maxObservationsPerColumn;
1:e90d901: 
1:e90d901:     private Random random;
1:e90d901: 
1:845cbcd:     @Override
1:845cbcd:     protected void setup(Context ctx) throws IOException, InterruptedException {
1:e90d901: 
1:e90d901:       Configuration conf = ctx.getConfiguration();
1:e90d901: 
1:e90d901:       similarity = ClassUtils.instantiateAs(conf.get(SIMILARITY_CLASSNAME), VectorSimilarityMeasure.class);
1:845cbcd:       norms = new RandomAccessSparseVector(Integer.MAX_VALUE);
1:845cbcd:       nonZeroEntries = new RandomAccessSparseVector(Integer.MAX_VALUE);
1:845cbcd:       maxValues = new RandomAccessSparseVector(Integer.MAX_VALUE);
1:e90d901:       threshold = Double.parseDouble(conf.get(THRESHOLD));
1:e90d901: 
1:e90d901:       observationsPerColumn = Vectors.readAsIntMap(new Path(conf.get(OBSERVATIONS_PER_COLUMN_PATH)), conf);
1:e90d901:       maxObservationsPerRow = conf.getInt(MAX_OBSERVATIONS_PER_ROW, DEFAULT_MAX_OBSERVATIONS_PER_ROW);
1:e90d901:       maxObservationsPerColumn = conf.getInt(MAX_OBSERVATIONS_PER_COLUMN, DEFAULT_MAX_OBSERVATIONS_PER_COLUMN);
1:e90d901: 
1:e90d901:       long seed = Long.parseLong(conf.get(RANDOM_SEED));
1:e90d901:       if (seed == NO_FIXED_RANDOM_SEED) {
1:e90d901:         random = RandomUtils.getRandom();
1:e90d901:       } else {
1:e90d901:         random = RandomUtils.getRandom(seed);
1:e90d901:       }
1:e90d901:     }
1:e90d901: 
1:e90d901:     private Vector sampleDown(Vector rowVector, Context ctx) {
1:e90d901: 
1:e90d901:       int observationsPerRow = rowVector.getNumNondefaultElements();
1:841b783:       double rowSampleRate = (double) Math.min(maxObservationsPerRow, observationsPerRow) / (double) observationsPerRow;
1:e90d901: 
1:e90d901:       Vector downsampledRow = rowVector.like();
1:e90d901:       long usedObservations = 0;
1:e90d901:       long neglectedObservations = 0;
1:e90d901: 
1:e90d901:       for (Vector.Element elem : rowVector.nonZeroes()) {
1:e90d901: 
1:e90d901:         int columnCount = observationsPerColumn.get(elem.index());
1:841b783:         double columnSampleRate = (double) Math.min(maxObservationsPerColumn, columnCount) / (double) columnCount;
1:e90d901: 
1:e90d901:         if (random.nextDouble() <= Math.min(rowSampleRate, columnSampleRate)) {
1:e90d901:           downsampledRow.setQuick(elem.index(), elem.get());
1:e90d901:           usedObservations++;
1:e90d901:         } else {
1:e90d901:           neglectedObservations++;
1:e90d901:         }
1:e90d901: 
1:e90d901:       }
1:e90d901: 
1:e90d901:       ctx.getCounter(Counters.USED_OBSERVATIONS).increment(usedObservations);
1:e90d901:       ctx.getCounter(Counters.NEGLECTED_OBSERVATIONS).increment(neglectedObservations);
1:e90d901: 
1:e90d901:       return downsampledRow;
1:845cbcd:     }
1:845cbcd: 
1:845cbcd:     @Override
1:845cbcd:     protected void map(IntWritable row, VectorWritable vectorWritable, Context ctx)
2:845cbcd:       throws IOException, InterruptedException {
1:845cbcd: 
1:e90d901:       Vector sampledRowVector = sampleDown(vectorWritable.get(), ctx);
1:e90d901: 
1:e90d901:       Vector rowVector = similarity.normalize(sampledRowVector);
1:845cbcd: 
1:845cbcd:       int numNonZeroEntries = 0;
1:845cbcd:       double maxValue = Double.MIN_VALUE;
1:845cbcd: 
1:dc62944:       for (Vector.Element element : rowVector.nonZeroes()) {
1:845cbcd:         RandomAccessSparseVector partialColumnVector = new RandomAccessSparseVector(Integer.MAX_VALUE);
1:845cbcd:         partialColumnVector.setQuick(row.get(), element.get());
1:845cbcd:         ctx.write(new IntWritable(element.index()), new VectorWritable(partialColumnVector));
1:845cbcd: 
1:845cbcd:         numNonZeroEntries++;
1:845cbcd:         if (maxValue < element.get()) {
1:845cbcd:           maxValue = element.get();
1:845cbcd:         }
1:845cbcd:       }
1:845cbcd: 
1:845cbcd:       if (threshold != NO_THRESHOLD) {
1:845cbcd:         nonZeroEntries.setQuick(row.get(), numNonZeroEntries);
1:845cbcd:         maxValues.setQuick(row.get(), maxValue);
1:845cbcd:       }
1:845cbcd:       norms.setQuick(row.get(), similarity.norm(rowVector));
1:845cbcd: 
1:845cbcd:       ctx.getCounter(Counters.ROWS).increment(1);
1:845cbcd:     }
1:845cbcd: 
1:845cbcd:     @Override
1:845cbcd:     protected void cleanup(Context ctx) throws IOException, InterruptedException {
1:845cbcd:       ctx.write(new IntWritable(NORM_VECTOR_MARKER), new VectorWritable(norms));
1:845cbcd:       ctx.write(new IntWritable(NUM_NON_ZERO_ENTRIES_VECTOR_MARKER), new VectorWritable(nonZeroEntries));
1:845cbcd:       ctx.write(new IntWritable(MAXVALUE_VECTOR_MARKER), new VectorWritable(maxValues));
1:845cbcd:     }
1:845cbcd:   }
1:845cbcd: 
1:5d66758:   private static class MergeVectorsCombiner extends Reducer<IntWritable,VectorWritable,IntWritable,VectorWritable> {
1:845cbcd:     @Override
1:845cbcd:     protected void reduce(IntWritable row, Iterable<VectorWritable> partialVectors, Context ctx)
1:845cbcd:       throws IOException, InterruptedException {
1:845cbcd:       ctx.write(row, new VectorWritable(Vectors.merge(partialVectors)));
1:845cbcd:     }
1:845cbcd:   }
1:845cbcd: 
1:845cbcd:   public static class MergeVectorsReducer extends Reducer<IntWritable,VectorWritable,IntWritable,VectorWritable> {
1:845cbcd: 
1:845cbcd:     private Path normsPath;
1:845cbcd:     private Path numNonZeroEntriesPath;
1:845cbcd:     private Path maxValuesPath;
1:845cbcd: 
1:845cbcd:     @Override
1:845cbcd:     protected void setup(Context ctx) throws IOException, InterruptedException {
1:845cbcd:       normsPath = new Path(ctx.getConfiguration().get(NORMS_PATH));
1:845cbcd:       numNonZeroEntriesPath = new Path(ctx.getConfiguration().get(NUM_NON_ZERO_ENTRIES_PATH));
1:845cbcd:       maxValuesPath = new Path(ctx.getConfiguration().get(MAXVALUES_PATH));
1:845cbcd:     }
1:845cbcd: 
1:845cbcd:     @Override
1:845cbcd:     protected void reduce(IntWritable row, Iterable<VectorWritable> partialVectors, Context ctx)
1:845cbcd:       throws IOException, InterruptedException {
1:845cbcd:       Vector partialVector = Vectors.merge(partialVectors);
1:845cbcd: 
1:845cbcd:       if (row.get() == NORM_VECTOR_MARKER) {
1:845cbcd:         Vectors.write(partialVector, normsPath, ctx.getConfiguration());
1:845cbcd:       } else if (row.get() == MAXVALUE_VECTOR_MARKER) {
1:845cbcd:         Vectors.write(partialVector, maxValuesPath, ctx.getConfiguration());
1:845cbcd:       } else if (row.get() == NUM_NON_ZERO_ENTRIES_VECTOR_MARKER) {
1:845cbcd:         Vectors.write(partialVector, numNonZeroEntriesPath, ctx.getConfiguration(), true);
1:845cbcd:       } else {
1:845cbcd:         ctx.write(row, new VectorWritable(partialVector));
1:845cbcd:       }
1:845cbcd:     }
1:845cbcd:   }
1:845cbcd: 
1:845cbcd: 
1:845cbcd:   public static class CooccurrencesMapper extends Mapper<IntWritable,VectorWritable,IntWritable,VectorWritable> {
1:845cbcd: 
1:845cbcd:     private VectorSimilarityMeasure similarity;
1:845cbcd: 
1:845cbcd:     private OpenIntIntHashMap numNonZeroEntries;
1:845cbcd:     private Vector maxValues;
1:845cbcd:     private double threshold;
1:845cbcd: 
1:845cbcd:     private static final Comparator<Vector.Element> BY_INDEX = new Comparator<Vector.Element>() {
1:845cbcd:       @Override
1:845cbcd:       public int compare(Vector.Element one, Vector.Element two) {
1:845cbcd:         return Ints.compare(one.index(), two.index());
1:845cbcd:       }
1:845cbcd:     };
1:845cbcd: 
1:845cbcd:     @Override
1:845cbcd:     protected void setup(Context ctx) throws IOException, InterruptedException {
2:845cbcd:       similarity = ClassUtils.instantiateAs(ctx.getConfiguration().get(SIMILARITY_CLASSNAME),
2:845cbcd:           VectorSimilarityMeasure.class);
1:845cbcd:       numNonZeroEntries = Vectors.readAsIntMap(new Path(ctx.getConfiguration().get(NUM_NON_ZERO_ENTRIES_PATH)),
1:845cbcd:           ctx.getConfiguration());
1:845cbcd:       maxValues = Vectors.read(new Path(ctx.getConfiguration().get(MAXVALUES_PATH)), ctx.getConfiguration());
2:845cbcd:       threshold = Double.parseDouble(ctx.getConfiguration().get(THRESHOLD));
1:845cbcd:     }
1:845cbcd: 
1:845cbcd:     private boolean consider(Vector.Element occurrenceA, Vector.Element occurrenceB) {
1:845cbcd:       int numNonZeroEntriesA = numNonZeroEntries.get(occurrenceA.index());
1:845cbcd:       int numNonZeroEntriesB = numNonZeroEntries.get(occurrenceB.index());
1:845cbcd: 
1:845cbcd:       double maxValueA = maxValues.get(occurrenceA.index());
1:4cad7a0:       double maxValueB = maxValues.get(occurrenceB.index());
1:845cbcd: 
1:4cad7a0:       return similarity.consider(numNonZeroEntriesA, numNonZeroEntriesB, maxValueA, maxValueB, threshold);
1:845cbcd:     }
1:845cbcd: 
1:845cbcd:     @Override
1:845cbcd:     protected void map(IntWritable column, VectorWritable occurrenceVector, Context ctx)
1:845cbcd:       throws IOException, InterruptedException {
1:845cbcd:       Vector.Element[] occurrences = Vectors.toArray(occurrenceVector);
1:845cbcd:       Arrays.sort(occurrences, BY_INDEX);
1:845cbcd: 
1:845cbcd:       int cooccurrences = 0;
1:845cbcd:       int prunedCooccurrences = 0;
1:845cbcd:       for (int n = 0; n < occurrences.length; n++) {
1:845cbcd:         Vector.Element occurrenceA = occurrences[n];
1:845cbcd:         Vector dots = new RandomAccessSparseVector(Integer.MAX_VALUE);
1:845cbcd:         for (int m = n; m < occurrences.length; m++) {
1:845cbcd:           Vector.Element occurrenceB = occurrences[m];
1:845cbcd:           if (threshold == NO_THRESHOLD || consider(occurrenceA, occurrenceB)) {
1:845cbcd:             dots.setQuick(occurrenceB.index(), similarity.aggregate(occurrenceA.get(), occurrenceB.get()));
1:845cbcd:             cooccurrences++;
1:845cbcd:           } else {
1:845cbcd:             prunedCooccurrences++;
1:845cbcd:           }
1:845cbcd:         }
1:845cbcd:         ctx.write(new IntWritable(occurrenceA.index()), new VectorWritable(dots));
1:845cbcd:       }
1:845cbcd:       ctx.getCounter(Counters.COOCCURRENCES).increment(cooccurrences);
1:845cbcd:       ctx.getCounter(Counters.PRUNED_COOCCURRENCES).increment(prunedCooccurrences);
1:845cbcd:     }
1:845cbcd:   }
1:845cbcd: 
1:845cbcd: 
1:229aeff:   public static class SimilarityReducer extends Reducer<IntWritable,VectorWritable,IntWritable,VectorWritable> {
1:845cbcd: 
1:845cbcd:     private VectorSimilarityMeasure similarity;
1:845cbcd:     private int numberOfColumns;
1:845cbcd:     private boolean excludeSelfSimilarity;
1:845cbcd:     private Vector norms;
1:845cbcd:     private double treshold;
1:845cbcd: 
1:845cbcd:     @Override
1:845cbcd:     protected void setup(Context ctx) throws IOException, InterruptedException {
1:845cbcd:       similarity = ClassUtils.instantiateAs(ctx.getConfiguration().get(SIMILARITY_CLASSNAME),
1:845cbcd:           VectorSimilarityMeasure.class);
1:845cbcd:       numberOfColumns = ctx.getConfiguration().getInt(NUMBER_OF_COLUMNS, -1);
1:cd167f9:       Preconditions.checkArgument(numberOfColumns > 0, "Number of columns must be greater then 0! But numberOfColumns = " + numberOfColumns);
1:845cbcd:       excludeSelfSimilarity = ctx.getConfiguration().getBoolean(EXCLUDE_SELF_SIMILARITY, false);
1:845cbcd:       norms = Vectors.read(new Path(ctx.getConfiguration().get(NORMS_PATH)), ctx.getConfiguration());
1:845cbcd:       treshold = Double.parseDouble(ctx.getConfiguration().get(THRESHOLD));
1:845cbcd:     }
1:845cbcd: 
1:845cbcd:     @Override
1:845cbcd:     protected void reduce(IntWritable row, Iterable<VectorWritable> partialDots, Context ctx)
1:845cbcd:       throws IOException, InterruptedException {
1:845cbcd:       Iterator<VectorWritable> partialDotsIterator = partialDots.iterator();
1:845cbcd:       Vector dots = partialDotsIterator.next().get();
1:845cbcd:       while (partialDotsIterator.hasNext()) {
1:845cbcd:         Vector toAdd = partialDotsIterator.next().get();
1:dc62944:         for (Element nonZeroElement : toAdd.nonZeroes()) {
1:845cbcd:           dots.setQuick(nonZeroElement.index(), dots.getQuick(nonZeroElement.index()) + nonZeroElement.get());
1:845cbcd:         }
1:845cbcd:       }
1:845cbcd: 
1:845cbcd:       Vector similarities = dots.like();
1:845cbcd:       double normA = norms.getQuick(row.get());
1:dc62944:       for (Element b : dots.nonZeroes()) {
1:845cbcd:         double similarityValue = similarity.similarity(b.get(), normA, norms.getQuick(b.index()), numberOfColumns);
1:845cbcd:         if (similarityValue >= treshold) {
1:845cbcd:           similarities.set(b.index(), similarityValue);
1:845cbcd:         }
1:845cbcd:       }
1:845cbcd:       if (excludeSelfSimilarity) {
1:845cbcd:         similarities.setQuick(row.get(), 0);
1:845cbcd:       }
1:845cbcd:       ctx.write(row, new VectorWritable(similarities));
1:845cbcd:     }
1:845cbcd:   }
1:845cbcd: 
1:845cbcd:   public static class UnsymmetrifyMapper extends Mapper<IntWritable,VectorWritable,IntWritable,VectorWritable>  {
1:845cbcd: 
1:845cbcd:     private int maxSimilaritiesPerRow;
1:845cbcd: 
1:845cbcd:     @Override
1:845cbcd:     protected void setup(Mapper.Context ctx) throws IOException, InterruptedException {
1:845cbcd:       maxSimilaritiesPerRow = ctx.getConfiguration().getInt(MAX_SIMILARITIES_PER_ROW, 0);
1:cd167f9:       Preconditions.checkArgument(maxSimilaritiesPerRow > 0, "Maximum number of similarities per row must be greater then 0!");
1:845cbcd:     }
1:845cbcd: 
1:845cbcd:     @Override
1:845cbcd:     protected void map(IntWritable row, VectorWritable similaritiesWritable, Context ctx)
1:845cbcd:       throws IOException, InterruptedException {
1:845cbcd:       Vector similarities = similaritiesWritable.get();
1:4841efb:       // For performance, the creation of transposedPartial is moved out of the while loop and it is reused inside
1:7d90a58:       Vector transposedPartial = new RandomAccessSparseVector(similarities.size(), 1);
1:d95bdda:       TopElementsQueue topKQueue = new TopElementsQueue(maxSimilaritiesPerRow);
1:dc62944:       for (Element nonZeroElement : similarities.nonZeroes()) {
1:d95bdda:         MutableElement top = topKQueue.top();
1:d95bdda:         double candidateValue = nonZeroElement.get();
1:d95bdda:         if (candidateValue > top.get()) {
1:d95bdda:           top.setIndex(nonZeroElement.index());
1:d95bdda:           top.set(candidateValue);
1:d95bdda:           topKQueue.updateTop();
1:d95bdda:         }
1:d95bdda: 
1:d95bdda:         transposedPartial.setQuick(row.get(), candidateValue);
1:845cbcd:         ctx.write(new IntWritable(nonZeroElement.index()), new VectorWritable(transposedPartial));
1:9d08db8:         transposedPartial.setQuick(row.get(), 0.0);
1:845cbcd:       }
1:7d90a58:       Vector topKSimilarities = new RandomAccessSparseVector(similarities.size(), maxSimilaritiesPerRow);
1:d95bdda:       for (Vector.Element topKSimilarity : topKQueue.getTopElements()) {
1:845cbcd:         topKSimilarities.setQuick(topKSimilarity.index(), topKSimilarity.get());
1:845cbcd:       }
1:845cbcd:       ctx.write(row, new VectorWritable(topKSimilarities));
1:845cbcd:     }
1:845cbcd:   }
1:845cbcd: 
1:845cbcd:   public static class MergeToTopKSimilaritiesReducer
2:845cbcd:       extends Reducer<IntWritable,VectorWritable,IntWritable,VectorWritable> {
1:845cbcd: 
1:845cbcd:     private int maxSimilaritiesPerRow;
1:845cbcd: 
1:845cbcd:     @Override
1:845cbcd:     protected void setup(Context ctx) throws IOException, InterruptedException {
1:845cbcd:       maxSimilaritiesPerRow = ctx.getConfiguration().getInt(MAX_SIMILARITIES_PER_ROW, 0);
1:cd167f9:       Preconditions.checkArgument(maxSimilaritiesPerRow > 0, "Maximum number of similarities per row must be greater then 0!");
1:845cbcd:     }
1:845cbcd: 
1:845cbcd:     @Override
1:845cbcd:     protected void reduce(IntWritable row, Iterable<VectorWritable> partials, Context ctx)
1:6d16230:       throws IOException, InterruptedException {
1:845cbcd:       Vector allSimilarities = Vectors.merge(partials);
1:845cbcd:       Vector topKSimilarities = Vectors.topKElements(maxSimilaritiesPerRow, allSimilarities);
1:845cbcd:       ctx.write(row, new VectorWritable(topKSimilarities));
1:845cbcd:     }
1:845cbcd:   }
1:845cbcd: 
1:845cbcd: }
============================================================================
author:pferrel
-------------------------------------------------------------------------------
commit:b988c49
author:frankscholten
-------------------------------------------------------------------------------
commit:1a42d85
author:smarthi
-------------------------------------------------------------------------------
commit:cd167f9
/////////////////////////////////////////////////////////////////////////
1:       Preconditions.checkArgument(numberOfColumns > 0, "Number of columns must be greater then 0! But numberOfColumns = " + numberOfColumns);
/////////////////////////////////////////////////////////////////////////
1:       Preconditions.checkArgument(maxSimilaritiesPerRow > 0, "Maximum number of similarities per row must be greater then 0!");
/////////////////////////////////////////////////////////////////////////
1:       Preconditions.checkArgument(maxSimilaritiesPerRow > 0, "Maximum number of similarities per row must be greater then 0!");
author:sslavic
-------------------------------------------------------------------------------
commit:841b783
/////////////////////////////////////////////////////////////////////////
1:       double rowSampleRate = (double) Math.min(maxObservationsPerRow, observationsPerRow) / (double) observationsPerRow;
/////////////////////////////////////////////////////////////////////////
1:         double columnSampleRate = (double) Math.min(maxObservationsPerColumn, columnCount) / (double) columnCount;
author:Sebastian Schelter
-------------------------------------------------------------------------------
commit:e90d901
/////////////////////////////////////////////////////////////////////////
1: import org.apache.hadoop.io.NullWritable;
/////////////////////////////////////////////////////////////////////////
1: import org.apache.mahout.common.RandomUtils;
1: import org.apache.mahout.common.mapreduce.VectorSumCombiner;
/////////////////////////////////////////////////////////////////////////
1: import java.util.Random;
1:   public static final long NO_FIXED_RANDOM_SEED = Long.MIN_VALUE;
/////////////////////////////////////////////////////////////////////////
1:   private static final String OBSERVATIONS_PER_COLUMN_PATH = RowSimilarityJob.class + ".observationsPerColumnPath";
1: 
1:   private static final String MAX_OBSERVATIONS_PER_ROW = RowSimilarityJob.class + ".maxObservationsPerRow";
1:   private static final String MAX_OBSERVATIONS_PER_COLUMN = RowSimilarityJob.class + ".maxObservationsPerColumn";
1:   private static final String RANDOM_SEED = RowSimilarityJob.class + ".randomSeed";
1: 
1:   private static final int DEFAULT_MAX_OBSERVATIONS_PER_ROW = 500;
1:   private static final int DEFAULT_MAX_OBSERVATIONS_PER_COLUMN = 500;
1: 
1:   enum Counters { ROWS, USED_OBSERVATIONS, NEGLECTED_OBSERVATIONS, COOCCURRENCES, PRUNED_COOCCURRENCES }
/////////////////////////////////////////////////////////////////////////
1:     addOption("maxObservationsPerRow", null, "sample rows down to this number of entries",
1:         String.valueOf(DEFAULT_MAX_OBSERVATIONS_PER_ROW));
1:     addOption("maxObservationsPerColumn", null, "sample columns down to this number of entries",
1:         String.valueOf(DEFAULT_MAX_OBSERVATIONS_PER_COLUMN));
1:     addOption("randomSeed", null, "use this seed for sampling", false);
/////////////////////////////////////////////////////////////////////////
1:     long randomSeed = hasOption("randomSeed")
1:         ? Long.parseLong(getOption("randomSeed")) : NO_FIXED_RANDOM_SEED;
1: 
1:     int maxObservationsPerRow = Integer.parseInt(getOption("maxObservationsPerRow"));
1:     int maxObservationsPerColumn = Integer.parseInt(getOption("maxObservationsPerColumn"));
/////////////////////////////////////////////////////////////////////////
1:     Path observationsPerColumnPath = getTempPath("observationsPerColumn.bin");
1: 
1:     Job countObservations = prepareJob(getInputPath(), getTempPath("notUsed"), CountObservationsMapper.class,
1:         NullWritable.class, VectorWritable.class, SumObservationsReducer.class, NullWritable.class,
1:         VectorWritable.class);
1:     countObservations.setCombinerClass(VectorSumCombiner.class);
1:     countObservations.getConfiguration().set(OBSERVATIONS_PER_COLUMN_PATH, observationsPerColumnPath.toString());
1:     countObservations.setNumReduceTasks(1);
1:     countObservations.waitForCompletion(true);
1: 
/////////////////////////////////////////////////////////////////////////
1:       normsAndTransposeConf.set(OBSERVATIONS_PER_COLUMN_PATH, observationsPerColumnPath.toString());
1:       normsAndTransposeConf.set(MAX_OBSERVATIONS_PER_ROW, String.valueOf(maxObservationsPerRow));
1:       normsAndTransposeConf.set(MAX_OBSERVATIONS_PER_COLUMN, String.valueOf(maxObservationsPerColumn));
1:       normsAndTransposeConf.set(RANDOM_SEED, String.valueOf(randomSeed));
1: 
/////////////////////////////////////////////////////////////////////////
1:   public static class CountObservationsMapper extends Mapper<IntWritable,VectorWritable,NullWritable,VectorWritable> {
1: 
1:     private Vector columnCounts = new RandomAccessSparseVector(Integer.MAX_VALUE);
1: 
1:     @Override
1:     protected void map(IntWritable rowIndex, VectorWritable rowVectorWritable, Context ctx)
1:       throws IOException, InterruptedException {
1: 
1:       Vector row = rowVectorWritable.get();
1:       for (Vector.Element elem : row.nonZeroes()) {
1:         columnCounts.setQuick(elem.index(), columnCounts.getQuick(elem.index()) + 1);
1:       }
1:     }
1: 
1:     @Override
1:     protected void cleanup(Context ctx) throws IOException, InterruptedException {
1:       ctx.write(NullWritable.get(), new VectorWritable(columnCounts));
1:     }
1:   }
1: 
1:   public static class SumObservationsReducer extends Reducer<NullWritable,VectorWritable,NullWritable,VectorWritable> {
1:     @Override
1:     protected void reduce(NullWritable nullWritable, Iterable<VectorWritable> partialVectors, Context ctx)
1:     throws IOException, InterruptedException {
1:       Vector counts = Vectors.sum(partialVectors.iterator());
1:       Vectors.write(counts, new Path(ctx.getConfiguration().get(OBSERVATIONS_PER_COLUMN_PATH)), ctx.getConfiguration());
1:     }
1:   }
1: 
/////////////////////////////////////////////////////////////////////////
1:     private OpenIntIntHashMap observationsPerColumn;
1:     private int maxObservationsPerRow;
1:     private int maxObservationsPerColumn;
1: 
1:     private Random random;
1: 
1: 
1:       Configuration conf = ctx.getConfiguration();
1: 
1:       similarity = ClassUtils.instantiateAs(conf.get(SIMILARITY_CLASSNAME), VectorSimilarityMeasure.class);
1:       threshold = Double.parseDouble(conf.get(THRESHOLD));
1: 
1:       observationsPerColumn = Vectors.readAsIntMap(new Path(conf.get(OBSERVATIONS_PER_COLUMN_PATH)), conf);
1:       maxObservationsPerRow = conf.getInt(MAX_OBSERVATIONS_PER_ROW, DEFAULT_MAX_OBSERVATIONS_PER_ROW);
1:       maxObservationsPerColumn = conf.getInt(MAX_OBSERVATIONS_PER_COLUMN, DEFAULT_MAX_OBSERVATIONS_PER_COLUMN);
1: 
1:       long seed = Long.parseLong(conf.get(RANDOM_SEED));
1:       if (seed == NO_FIXED_RANDOM_SEED) {
1:         random = RandomUtils.getRandom();
1:       } else {
1:         random = RandomUtils.getRandom(seed);
1:       }
1:     }
1: 
1:     private Vector sampleDown(Vector rowVector, Context ctx) {
1: 
1:       int observationsPerRow = rowVector.getNumNondefaultElements();
0:       double rowSampleRate = Math.min(maxObservationsPerRow, observationsPerRow) / observationsPerRow;
1: 
1:       Vector downsampledRow = rowVector.like();
1:       long usedObservations = 0;
1:       long neglectedObservations = 0;
1: 
1:       for (Vector.Element elem : rowVector.nonZeroes()) {
1: 
1:         int columnCount = observationsPerColumn.get(elem.index());
0:         double columnSampleRate = Math.min(maxObservationsPerColumn, columnCount) / columnCount;
1: 
1:         if (random.nextDouble() <= Math.min(rowSampleRate, columnSampleRate)) {
1:           downsampledRow.setQuick(elem.index(), elem.get());
1:           usedObservations++;
1:         } else {
1:           neglectedObservations++;
1:         }
1: 
1:       }
1: 
1:       ctx.getCounter(Counters.USED_OBSERVATIONS).increment(usedObservations);
1:       ctx.getCounter(Counters.NEGLECTED_OBSERVATIONS).increment(neglectedObservations);
1: 
1:       return downsampledRow;
1:       Vector sampledRowVector = sampleDown(vectorWritable.get(), ctx);
1: 
1:       Vector rowVector = similarity.normalize(sampledRowVector);
/////////////////////////////////////////////////////////////////////////
commit:6d16230
/////////////////////////////////////////////////////////////////////////
1:     double threshold = hasOption("threshold")
1:         ? Double.parseDouble(getOption("threshold")) : NO_THRESHOLD;
/////////////////////////////////////////////////////////////////////////
1:       throws IOException, InterruptedException {
commit:d95bdda
/////////////////////////////////////////////////////////////////////////
/////////////////////////////////////////////////////////////////////////
1:       TopElementsQueue topKQueue = new TopElementsQueue(maxSimilaritiesPerRow);
1: 
1:         MutableElement top = topKQueue.top();
1:         double candidateValue = nonZeroElement.get();
1:         if (candidateValue > top.get()) {
1:           top.setIndex(nonZeroElement.index());
1:           top.set(candidateValue);
1:           topKQueue.updateTop();
1:         }
1: 
1:         transposedPartial.setQuick(row.get(), candidateValue);
1:       for (Vector.Element topKSimilarity : topKQueue.getTopElements()) {
commit:4841efb
/////////////////////////////////////////////////////////////////////////
1:       // For performance, the creation of transposedPartial is moved out of the while loop and it is reused inside
commit:5d66758
/////////////////////////////////////////////////////////////////////////
1:   private static final String SIMILARITY_CLASSNAME = RowSimilarityJob.class + ".distributedSimilarityClassname";
1:   private static final String NUMBER_OF_COLUMNS = RowSimilarityJob.class + ".numberOfColumns";
1:   private static final String MAX_SIMILARITIES_PER_ROW = RowSimilarityJob.class + ".maxSimilaritiesPerRow";
1:   private static final String EXCLUDE_SELF_SIMILARITY = RowSimilarityJob.class + ".excludeSelfSimilarity";
1:   private static final String THRESHOLD = RowSimilarityJob.class + ".threshold";
1:   private static final String NORMS_PATH = RowSimilarityJob.class + ".normsPath";
1:   private static final String MAXVALUES_PATH = RowSimilarityJob.class + ".maxWeightsPath";
1:   private static final String NUM_NON_ZERO_ENTRIES_PATH = RowSimilarityJob.class + ".nonZeroEntriesPath";
/////////////////////////////////////////////////////////////////////////
1:   private static class MergeVectorsCombiner extends Reducer<IntWritable,VectorWritable,IntWritable,VectorWritable> {
commit:7d90a58
/////////////////////////////////////////////////////////////////////////
0:       // For performance reasons, the creation of transposedPartial is moved out of the while loop and it is reused inside
1:       Vector transposedPartial = new RandomAccessSparseVector(similarities.size(), 1);
/////////////////////////////////////////////////////////////////////////
1:       Vector topKSimilarities = new RandomAccessSparseVector(similarities.size(), maxSimilaritiesPerRow);
commit:f60ceca
/////////////////////////////////////////////////////////////////////////
commit:3bf7a1c
/////////////////////////////////////////////////////////////////////////
0: import org.apache.hadoop.fs.FileSystem;
0: import org.apache.hadoop.io.SequenceFile;
/////////////////////////////////////////////////////////////////////////
1:     addOption("numberOfColumns", "r", "Number of columns in the input matrix", false);
/////////////////////////////////////////////////////////////////////////
1:     int numberOfColumns;
1: 
1:     if (hasOption("numberOfColumns")) {
1:       // Number of columns explicitly specified via CLI
1:       numberOfColumns = Integer.parseInt(getOption("numberOfColumns"));
1:     } else {
1:       // else get the number of columns by determining the cardinality of a vector in the input matrix
1:       numberOfColumns = getDimensions(getInputPath());
1:     }
1: 
commit:298eef9
/////////////////////////////////////////////////////////////////////////
1: import org.apache.mahout.common.HadoopUtil;
1: import org.apache.mahout.common.commandline.DefaultOptionCreator;
/////////////////////////////////////////////////////////////////////////
1:     addOption(DefaultOptionCreator.overwriteOption().create());
/////////////////////////////////////////////////////////////////////////
1:     // Clear the output and temp paths if the overwrite option has been set
1:     if (hasOption(DefaultOptionCreator.OVERWRITE_OPTION)) {
1:       // Clear the temp path
1:       HadoopUtil.delete(getConf(), getTempPath());
1:       // Clear the output path
1:       HadoopUtil.delete(getConf(), getOutputPath());
1:     }
1: 
commit:4cad7a0
/////////////////////////////////////////////////////////////////////////
1:       double maxValueB = maxValues.get(occurrenceB.index());
1:       return similarity.consider(numNonZeroEntriesA, numNonZeroEntriesB, maxValueA, maxValueB, threshold);
commit:b70621d
/////////////////////////////////////////////////////////////////////////
/////////////////////////////////////////////////////////////////////////
1:   public static final double NO_THRESHOLD = Double.MIN_VALUE;
1: 
0:   static final String THRESHOLD = RowSimilarityJob.class + ".threshold";
0:   static final String NUM_NON_ZERO_ENTRIES_PATH = RowSimilarityJob.class + ".nonZeroEntriesPath";
/////////////////////////////////////////////////////////////////////////
1: 
/////////////////////////////////////////////////////////////////////////
1:     addOption("threshold", "tr", "discard row pairs with a similarity value below this", false);
commit:845cbcd
/////////////////////////////////////////////////////////////////////////
1: /**
1:  * Licensed to the Apache Software Foundation (ASF) under one or more
1:  * contributor license agreements.  See the NOTICE file distributed with
1:  * this work for additional information regarding copyright ownership.
1:  * The ASF licenses this file to You under the Apache License, Version 2.0
1:  * (the "License"); you may not use this file except in compliance with
1:  * the License.  You may obtain a copy of the License at
1:  *
1:  *     http://www.apache.org/licenses/LICENSE-2.0
1:  *
1:  * Unless required by applicable law or agreed to in writing, software
1:  * distributed under the License is distributed on an "AS IS" BASIS,
1:  * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
1:  * See the License for the specific language governing permissions and
1:  * limitations under the License.
1:  */
1: 
1: package org.apache.mahout.math.hadoop.similarity.cooccurrence;
1: 
1: import com.google.common.base.Preconditions;
1: import com.google.common.primitives.Ints;
1: import org.apache.hadoop.conf.Configuration;
1: import org.apache.hadoop.fs.Path;
1: import org.apache.hadoop.io.IntWritable;
1: import org.apache.hadoop.mapreduce.Job;
1: import org.apache.hadoop.mapreduce.Mapper;
1: import org.apache.hadoop.mapreduce.Reducer;
1: import org.apache.hadoop.util.ToolRunner;
0: import org.apache.mahout.cf.taste.common.TopK;
1: import org.apache.mahout.common.AbstractJob;
1: import org.apache.mahout.common.ClassUtils;
1: import org.apache.mahout.common.mapreduce.VectorSumReducer;
1: import org.apache.mahout.math.RandomAccessSparseVector;
1: import org.apache.mahout.math.Vector;
1: import org.apache.mahout.math.VectorWritable;
1: import org.apache.mahout.math.hadoop.similarity.cooccurrence.measures.VectorSimilarityMeasures;
1: import org.apache.mahout.math.hadoop.similarity.cooccurrence.measures.VectorSimilarityMeasure;
0: import org.apache.mahout.math.map.OpenIntDoubleHashMap;
1: import org.apache.mahout.math.map.OpenIntIntHashMap;
1: 
1: import java.io.IOException;
1: import java.util.Arrays;
1: import java.util.Comparator;
1: import java.util.Iterator;
1: import java.util.Map;
1: import java.util.concurrent.atomic.AtomicInteger;
1: 
1: public class RowSimilarityJob extends AbstractJob {
1: 
0:   static final String SIMILARITY_CLASSNAME = RowSimilarityJob.class + ".distributedSimilarityClassname";
0:   static final String NUMBER_OF_COLUMNS = RowSimilarityJob.class + ".numberOfColumns";
0:   static final String MAX_SIMILARITIES_PER_ROW = RowSimilarityJob.class + ".maxSimilaritiesPerRow";
0:   static final String EXCLUDE_SELF_SIMILARITY = RowSimilarityJob.class + ".excludeSelfSimilarity";
0:   static final String THRESHOLD = RowSimilarityJob.class + ".threshold";
1: 
0:   static final String NORMS_PATH = RowSimilarityJob.class + ".normsPath";
0:   static final String MAXVALUES_PATH = RowSimilarityJob.class + ".maxWeightsPath";
0:   static final String NUM_NON_ZERO_ENTRIES_PATH = RowSimilarityJob.class + ".nonZeroEntriesPath";
1: 
1:   private static final int DEFAULT_MAX_SIMILARITIES_PER_ROW = 100;
0:   private static final double NO_THRESHOLD = Double.MIN_VALUE;
1: 
1:   private static final int NORM_VECTOR_MARKER = Integer.MIN_VALUE;
1:   private static final int MAXVALUE_VECTOR_MARKER = Integer.MIN_VALUE + 1;
1:   private static final int NUM_NON_ZERO_ENTRIES_VECTOR_MARKER = Integer.MIN_VALUE + 2;
1: 
0:   static enum Counters { ROWS, COOCCURRENCES, PRUNED_COOCCURRENCES }
1: 
1:   public static void main(String[] args) throws Exception {
1:     ToolRunner.run(new RowSimilarityJob(), args);
1:   }
1: 
1:   @Override
1:   public int run(String[] args) throws Exception {
1: 
1:     addInputOption();
1:     addOutputOption();
0:     addOption("numberOfColumns", "r", "Number of columns in the input matrix");
1:     addOption("similarityClassname", "s", "Name of distributed similarity class to instantiate, alternatively use "
1:         + "one of the predefined similarities (" + VectorSimilarityMeasures.list() + ')');
1:     addOption("maxSimilaritiesPerRow", "m", "Number of maximum similarities per row (default: "
1:         + DEFAULT_MAX_SIMILARITIES_PER_ROW + ')', String.valueOf(DEFAULT_MAX_SIMILARITIES_PER_ROW));
1:     addOption("excludeSelfSimilarity", "ess", "compute similarity of rows to themselves?", String.valueOf(false));
0:     addOption("threshold", "tr", "drop row pairs with a similarity value below this");
1: 
0:     Map<String,String> parsedArgs = parseArguments(args);
1:     if (parsedArgs == null) {
1:       return -1;
1:     }
1: 
0:     int numberOfColumns = Integer.parseInt(parsedArgs.get("--numberOfColumns"));
0:     String similarityClassnameArg = parsedArgs.get("--similarityClassname");
1:     String similarityClassname;
1:     try {
1:       similarityClassname = VectorSimilarityMeasures.valueOf(similarityClassnameArg).getClassname();
1:     } catch (IllegalArgumentException iae) {
1:       similarityClassname = similarityClassnameArg;
1:     }
1: 
0:     int maxSimilaritiesPerRow = Integer.parseInt(parsedArgs.get("--maxSimilaritiesPerRow"));
0:     boolean excludeSelfSimilarity = Boolean.parseBoolean(parsedArgs.get("--excludeSelfSimilarity"));
0:     double threshold = parsedArgs.containsKey("--threshold") ?
0:         Double.parseDouble(parsedArgs.get("--threshold")) : NO_THRESHOLD;
1: 
1:     Path weightsPath = getTempPath("weights");
1:     Path normsPath = getTempPath("norms.bin");
1:     Path numNonZeroEntriesPath = getTempPath("numNonZeroEntries.bin");
1:     Path maxValuesPath = getTempPath("maxValues.bin");
1:     Path pairwiseSimilarityPath = getTempPath("pairwiseSimilarity");
1: 
1:     AtomicInteger currentPhase = new AtomicInteger();
1: 
1:     if (shouldRunNextPhase(parsedArgs, currentPhase)) {
1:       Job normsAndTranspose = prepareJob(getInputPath(), weightsPath, VectorNormMapper.class, IntWritable.class,
1:           VectorWritable.class, MergeVectorsReducer.class, IntWritable.class, VectorWritable.class);
1:       normsAndTranspose.setCombinerClass(MergeVectorsCombiner.class);
1:       Configuration normsAndTransposeConf = normsAndTranspose.getConfiguration();
1:       normsAndTransposeConf.set(THRESHOLD, String.valueOf(threshold));
1:       normsAndTransposeConf.set(NORMS_PATH, normsPath.toString());
1:       normsAndTransposeConf.set(NUM_NON_ZERO_ENTRIES_PATH, numNonZeroEntriesPath.toString());
1:       normsAndTransposeConf.set(MAXVALUES_PATH, maxValuesPath.toString());
1:       normsAndTransposeConf.set(SIMILARITY_CLASSNAME, similarityClassname);
0:       normsAndTranspose.waitForCompletion(true);
1:     }
1: 
1:     if (shouldRunNextPhase(parsedArgs, currentPhase)) {
1:       Job pairwiseSimilarity = prepareJob(weightsPath, pairwiseSimilarityPath, CooccurrencesMapper.class,
1:           IntWritable.class, VectorWritable.class, SimilarityReducer.class, IntWritable.class, VectorWritable.class);
1:       pairwiseSimilarity.setCombinerClass(VectorSumReducer.class);
1:       Configuration pairwiseConf = pairwiseSimilarity.getConfiguration();
1:       pairwiseConf.set(THRESHOLD, String.valueOf(threshold));
1:       pairwiseConf.set(NORMS_PATH, normsPath.toString());
1:       pairwiseConf.set(NUM_NON_ZERO_ENTRIES_PATH, numNonZeroEntriesPath.toString());
1:       pairwiseConf.set(MAXVALUES_PATH, maxValuesPath.toString());
1:       pairwiseConf.set(SIMILARITY_CLASSNAME, similarityClassname);
1:       pairwiseConf.setInt(NUMBER_OF_COLUMNS, numberOfColumns);
1:       pairwiseConf.setBoolean(EXCLUDE_SELF_SIMILARITY, excludeSelfSimilarity);
0:       pairwiseSimilarity.waitForCompletion(true);
1:     }
1: 
1:     if (shouldRunNextPhase(parsedArgs, currentPhase)) {
1:       Job asMatrix = prepareJob(pairwiseSimilarityPath, getOutputPath(), UnsymmetrifyMapper.class,
1:           IntWritable.class, VectorWritable.class, MergeToTopKSimilaritiesReducer.class, IntWritable.class,
1:           VectorWritable.class);
1:       asMatrix.setCombinerClass(MergeToTopKSimilaritiesReducer.class);
1:       asMatrix.getConfiguration().setInt(MAX_SIMILARITIES_PER_ROW, maxSimilaritiesPerRow);
0:       asMatrix.waitForCompletion(true);
1:     }
1: 
1:     return 0;
1:   }
1: 
1:   public static class VectorNormMapper extends Mapper<IntWritable,VectorWritable,IntWritable,VectorWritable> {
1: 
1:     private VectorSimilarityMeasure similarity;
1:     private Vector norms;
1:     private Vector nonZeroEntries;
1:     private Vector maxValues;
1:     private double threshold;
1: 
1:     @Override
1:     protected void setup(Context ctx) throws IOException, InterruptedException {
1:       similarity = ClassUtils.instantiateAs(ctx.getConfiguration().get(SIMILARITY_CLASSNAME),
1:           VectorSimilarityMeasure.class);
1:       norms = new RandomAccessSparseVector(Integer.MAX_VALUE);
1:       nonZeroEntries = new RandomAccessSparseVector(Integer.MAX_VALUE);
1:       maxValues = new RandomAccessSparseVector(Integer.MAX_VALUE);
1:       threshold = Double.parseDouble(ctx.getConfiguration().get(THRESHOLD));
1:     }
1: 
1:     @Override
1:     protected void map(IntWritable row, VectorWritable vectorWritable, Context ctx)
1:         throws IOException, InterruptedException {
1: 
0:       Vector rowVector = similarity.normalize(vectorWritable.get());
1: 
1:       int numNonZeroEntries = 0;
1:       double maxValue = Double.MIN_VALUE;
1: 
0:       Iterator<Vector.Element> nonZeroElements = rowVector.iterateNonZero();
0:       while (nonZeroElements.hasNext()) {
0:         Vector.Element element = nonZeroElements.next();
1:         RandomAccessSparseVector partialColumnVector = new RandomAccessSparseVector(Integer.MAX_VALUE);
1:         partialColumnVector.setQuick(row.get(), element.get());
1:         ctx.write(new IntWritable(element.index()), new VectorWritable(partialColumnVector));
1: 
1:         numNonZeroEntries++;
1:         if (maxValue < element.get()) {
1:           maxValue = element.get();
1:         }
1:       }
1: 
1:       if (threshold != NO_THRESHOLD) {
1:         nonZeroEntries.setQuick(row.get(), numNonZeroEntries);
1:         maxValues.setQuick(row.get(), maxValue);
1:       }
1:       norms.setQuick(row.get(), similarity.norm(rowVector));
1: 
1:       ctx.getCounter(Counters.ROWS).increment(1);
1:     }
1: 
1:     @Override
1:     protected void cleanup(Context ctx) throws IOException, InterruptedException {
0:       super.cleanup(ctx);
0:       // dirty trick
1:       ctx.write(new IntWritable(NORM_VECTOR_MARKER), new VectorWritable(norms));
1:       ctx.write(new IntWritable(NUM_NON_ZERO_ENTRIES_VECTOR_MARKER), new VectorWritable(nonZeroEntries));
1:       ctx.write(new IntWritable(MAXVALUE_VECTOR_MARKER), new VectorWritable(maxValues));
1:     }
1:   }
1: 
0:   public static class MergeVectorsCombiner extends Reducer<IntWritable,VectorWritable,IntWritable,VectorWritable> {
1:     @Override
1:     protected void reduce(IntWritable row, Iterable<VectorWritable> partialVectors, Context ctx)
1:         throws IOException, InterruptedException {
1:       ctx.write(row, new VectorWritable(Vectors.merge(partialVectors)));
1:     }
1:   }
1: 
1:   public static class MergeVectorsReducer extends Reducer<IntWritable,VectorWritable,IntWritable,VectorWritable> {
1: 
1:     private Path normsPath;
1:     private Path numNonZeroEntriesPath;
1:     private Path maxValuesPath;
1: 
1:     @Override
1:     protected void setup(Context ctx) throws IOException, InterruptedException {
1:       normsPath = new Path(ctx.getConfiguration().get(NORMS_PATH));
1:       numNonZeroEntriesPath = new Path(ctx.getConfiguration().get(NUM_NON_ZERO_ENTRIES_PATH));
1:       maxValuesPath = new Path(ctx.getConfiguration().get(MAXVALUES_PATH));
1:     }
1: 
1:     @Override
1:     protected void reduce(IntWritable row, Iterable<VectorWritable> partialVectors, Context ctx)
1:         throws IOException, InterruptedException {
1:       Vector partialVector = Vectors.merge(partialVectors);
1: 
1:       if (row.get() == NORM_VECTOR_MARKER) {
1:         Vectors.write(partialVector, normsPath, ctx.getConfiguration());
1:       } else if (row.get() == MAXVALUE_VECTOR_MARKER) {
1:         Vectors.write(partialVector, maxValuesPath, ctx.getConfiguration());
1:       } else if (row.get() == NUM_NON_ZERO_ENTRIES_VECTOR_MARKER) {
1:         Vectors.write(partialVector, numNonZeroEntriesPath, ctx.getConfiguration(), true);
1:       } else {
1:         ctx.write(row, new VectorWritable(partialVector));
1:       }
1:     }
1:   }
1: 
1: 
1:   public static class CooccurrencesMapper extends Mapper<IntWritable,VectorWritable,IntWritable,VectorWritable> {
1: 
1:     private VectorSimilarityMeasure similarity;
1: 
1:     private OpenIntIntHashMap numNonZeroEntries;
1:     private Vector maxValues;
1:     private double threshold;
1: 
1:     private static final Comparator<Vector.Element> BY_INDEX = new Comparator<Vector.Element>() {
1:       @Override
1:       public int compare(Vector.Element one, Vector.Element two) {
1:         return Ints.compare(one.index(), two.index());
1:       }
1:     };
1: 
1:     @Override
1:     protected void setup(Context ctx) throws IOException, InterruptedException {
1:       similarity = ClassUtils.instantiateAs(ctx.getConfiguration().get(SIMILARITY_CLASSNAME),
1:           VectorSimilarityMeasure.class);
1:       numNonZeroEntries = Vectors.readAsIntMap(new Path(ctx.getConfiguration().get(NUM_NON_ZERO_ENTRIES_PATH)),
1:           ctx.getConfiguration());
1:       maxValues = Vectors.read(new Path(ctx.getConfiguration().get(MAXVALUES_PATH)), ctx.getConfiguration());
1:       threshold = Double.parseDouble(ctx.getConfiguration().get(THRESHOLD));
1:     }
1: 
1:     private boolean consider(Vector.Element occurrenceA, Vector.Element occurrenceB) {
1:       int numNonZeroEntriesA = numNonZeroEntries.get(occurrenceA.index());
1:       int numNonZeroEntriesB = numNonZeroEntries.get(occurrenceB.index());
1: 
1:       double maxValueA = maxValues.get(occurrenceA.index());
1: 
0:       return similarity.consider(numNonZeroEntriesA, numNonZeroEntriesB, maxValueA, threshold);
1:     }
1: 
1:     @Override
1:     protected void map(IntWritable column, VectorWritable occurrenceVector, Context ctx)
1:         throws IOException, InterruptedException {
1:       Vector.Element[] occurrences = Vectors.toArray(occurrenceVector);
1:       Arrays.sort(occurrences, BY_INDEX);
1: 
1:       int cooccurrences = 0;
1:       int prunedCooccurrences = 0;
1:       for (int n = 0; n < occurrences.length; n++) {
1:         Vector.Element occurrenceA = occurrences[n];
1:         Vector dots = new RandomAccessSparseVector(Integer.MAX_VALUE);
1:         for (int m = n; m < occurrences.length; m++) {
1:           Vector.Element occurrenceB = occurrences[m];
1:           if (threshold == NO_THRESHOLD || consider(occurrenceA, occurrenceB)) {
1:             dots.setQuick(occurrenceB.index(), similarity.aggregate(occurrenceA.get(), occurrenceB.get()));
1:             cooccurrences++;
1:           } else {
1:             prunedCooccurrences++;
1:           }
1:         }
1:         ctx.write(new IntWritable(occurrenceA.index()), new VectorWritable(dots));
1:       }
1:       ctx.getCounter(Counters.COOCCURRENCES).increment(cooccurrences);
1:       ctx.getCounter(Counters.PRUNED_COOCCURRENCES).increment(prunedCooccurrences);
1:     }
1:   }
1: 
1: 
0:   public static class SimilarityReducer
1:       extends Reducer<IntWritable,VectorWritable,IntWritable,VectorWritable> {
1: 
1:     private VectorSimilarityMeasure similarity;
1:     private int numberOfColumns;
1:     private boolean excludeSelfSimilarity;
1:     private Vector norms;
1:     private double treshold;
1: 
1:     @Override
1:     protected void setup(Context ctx) throws IOException, InterruptedException {
1:       similarity = ClassUtils.instantiateAs(ctx.getConfiguration().get(SIMILARITY_CLASSNAME),
1:           VectorSimilarityMeasure.class);
1:       numberOfColumns = ctx.getConfiguration().getInt(NUMBER_OF_COLUMNS, -1);
0:       Preconditions.checkArgument(numberOfColumns > 0, "Incorrect number of columns!");
1:       excludeSelfSimilarity = ctx.getConfiguration().getBoolean(EXCLUDE_SELF_SIMILARITY, false);
1:       norms = Vectors.read(new Path(ctx.getConfiguration().get(NORMS_PATH)), ctx.getConfiguration());
1:       treshold = Double.parseDouble(ctx.getConfiguration().get(THRESHOLD));
1:     }
1: 
1:     @Override
1:     protected void reduce(IntWritable row, Iterable<VectorWritable> partialDots, Context ctx)
1:         throws IOException, InterruptedException {
1:       Iterator<VectorWritable> partialDotsIterator = partialDots.iterator();
1:       Vector dots = partialDotsIterator.next().get();
1:       while (partialDotsIterator.hasNext()) {
1:         Vector toAdd = partialDotsIterator.next().get();
0:         Iterator<Vector.Element> nonZeroElements = toAdd.iterateNonZero();
0:         while (nonZeroElements.hasNext()) {
0:           Vector.Element nonZeroElement = nonZeroElements.next();
1:           dots.setQuick(nonZeroElement.index(), dots.getQuick(nonZeroElement.index()) + nonZeroElement.get());
1:         }
1:       }
1: 
1:       Vector similarities = dots.like();
1:       double normA = norms.getQuick(row.get());
0:       Iterator<Vector.Element> dotsWith = dots.iterateNonZero();
0:       while (dotsWith.hasNext()) {
0:         Vector.Element b = dotsWith.next();
1:         double similarityValue = similarity.similarity(b.get(), normA, norms.getQuick(b.index()), numberOfColumns);
1:         if (similarityValue >= treshold) {
1:           similarities.set(b.index(), similarityValue);
1:         }
1:       }
1:       if (excludeSelfSimilarity) {
1:         similarities.setQuick(row.get(), 0);
1:       }
1:       ctx.write(row, new VectorWritable(similarities));
1:     }
1:   }
1: 
1:   public static class UnsymmetrifyMapper extends Mapper<IntWritable,VectorWritable,IntWritable,VectorWritable>  {
1: 
1:     private int maxSimilaritiesPerRow;
1: 
1:     @Override
1:     protected void setup(Mapper.Context ctx) throws IOException, InterruptedException {
1:       maxSimilaritiesPerRow = ctx.getConfiguration().getInt(MAX_SIMILARITIES_PER_ROW, 0);
0:       Preconditions.checkArgument(maxSimilaritiesPerRow > 0, "Incorrect maximum number of similarities per row!");
1:     }
1: 
1:     @Override
1:     protected void map(IntWritable row, VectorWritable similaritiesWritable, Context ctx)
1:         throws IOException, InterruptedException {
1:       Vector similarities = similaritiesWritable.get();
0:       TopK<Vector.Element> topKQueue = new TopK<Vector.Element>(maxSimilaritiesPerRow, Vectors.BY_VALUE);
0:       Iterator<Vector.Element> nonZeroElements = similarities.iterateNonZero();
0:       while (nonZeroElements.hasNext()) {
0:         Vector.Element nonZeroElement = nonZeroElements.next();
0:         topKQueue.offer(new Vectors.TemporaryElement(nonZeroElement));
0:         Vector transposedPartial = similarities.like();
0:         transposedPartial.setQuick(row.get(), nonZeroElement.get());
1:         ctx.write(new IntWritable(nonZeroElement.index()), new VectorWritable(transposedPartial));
1:       }
0:       Vector topKSimilarities = similarities.like();
0:       for (Vector.Element topKSimilarity : topKQueue.retrieve()) {
1:         topKSimilarities.setQuick(topKSimilarity.index(), topKSimilarity.get());
1:       }
1:       ctx.write(row, new VectorWritable(topKSimilarities));
1:     }
1:   }
1: 
1:   public static class MergeToTopKSimilaritiesReducer
1:       extends Reducer<IntWritable,VectorWritable,IntWritable,VectorWritable> {
1: 
1:     private int maxSimilaritiesPerRow;
1: 
1:     @Override
1:     protected void setup(Context ctx) throws IOException, InterruptedException {
1:       maxSimilaritiesPerRow = ctx.getConfiguration().getInt(MAX_SIMILARITIES_PER_ROW, 0);
0:       Preconditions.checkArgument(maxSimilaritiesPerRow > 0, "Incorrect maximum number of similarities per row!");
1:     }
1: 
1:     @Override
1:     protected void reduce(IntWritable row, Iterable<VectorWritable> partials, Context ctx)
1:         throws IOException, InterruptedException {
1:       Vector allSimilarities = Vectors.merge(partials);
1:       Vector topKSimilarities = Vectors.topKElements(maxSimilaritiesPerRow, allSimilarities);
1:       ctx.write(row, new VectorWritable(topKSimilarities));
1:     }
1:   }
1: 
1: }
author:Jacob Alexander Mannix
-------------------------------------------------------------------------------
commit:dc62944
/////////////////////////////////////////////////////////////////////////
1: import org.apache.mahout.math.Vector.Element;
/////////////////////////////////////////////////////////////////////////
1:       for (Vector.Element element : rowVector.nonZeroes()) {
/////////////////////////////////////////////////////////////////////////
1:         for (Element nonZeroElement : toAdd.nonZeroes()) {
1:       for (Element b : dots.nonZeroes()) {
/////////////////////////////////////////////////////////////////////////
1:       for (Element nonZeroElement : similarities.nonZeroes()) {
author:Sean R. Owen
-------------------------------------------------------------------------------
commit:229aeff
/////////////////////////////////////////////////////////////////////////
0:       throws IOException, InterruptedException {
/////////////////////////////////////////////////////////////////////////
0:       throws IOException, InterruptedException {
/////////////////////////////////////////////////////////////////////////
0:       throws IOException, InterruptedException {
/////////////////////////////////////////////////////////////////////////
0:       throws IOException, InterruptedException {
/////////////////////////////////////////////////////////////////////////
1:   public static class SimilarityReducer extends Reducer<IntWritable,VectorWritable,IntWritable,VectorWritable> {
/////////////////////////////////////////////////////////////////////////
0:       throws IOException, InterruptedException {
/////////////////////////////////////////////////////////////////////////
0:       throws IOException, InterruptedException {
commit:9d08db8
/////////////////////////////////////////////////////////////////////////
0:       // For performance reasons moved transposedPartial creation out of the while loop and reusing the same vector
0:       Vector transposedPartial = similarities.like();
1:         transposedPartial.setQuick(row.get(), 0.0);
commit:7c2b664
/////////////////////////////////////////////////////////////////////////
1:       boolean succeeded = normsAndTranspose.waitForCompletion(true);
1:       if (!succeeded) {
1:         return -1;
1:       }
/////////////////////////////////////////////////////////////////////////
1:       boolean succeeded = pairwiseSimilarity.waitForCompletion(true);
1:       if (!succeeded) {
1:         return -1;
1:       }
/////////////////////////////////////////////////////////////////////////
1:       boolean succeeded = asMatrix.waitForCompletion(true);
1:       if (!succeeded) {
1:         return -1;
1:       }
commit:1499411
/////////////////////////////////////////////////////////////////////////
0:   enum Counters { ROWS, COOCCURRENCES, PRUNED_COOCCURRENCES }
author:Grant Ingersoll
-------------------------------------------------------------------------------
commit:6db7f62
/////////////////////////////////////////////////////////////////////////
1: import java.util.List;
/////////////////////////////////////////////////////////////////////////
1:     Map<String,List<String>> parsedArgs = parseArguments(args);
0:     int numberOfColumns = Integer.parseInt(getOption("numberOfColumns"));
1:     String similarityClassnameArg = getOption("similarityClassname");
/////////////////////////////////////////////////////////////////////////
1:     int maxSimilaritiesPerRow = Integer.parseInt(getOption("maxSimilaritiesPerRow"));
1:     boolean excludeSelfSimilarity = Boolean.parseBoolean(getOption("excludeSelfSimilarity"));
0:     double threshold = hasOption("threshold") ?
0:         Double.parseDouble(getOption("threshold")) : NO_THRESHOLD;
============================================================================