1:f3a9cc1: /**
1:f3a9cc1:  * Licensed to the Apache Software Foundation (ASF) under one or more
1:f3a9cc1:  * contributor license agreements.  See the NOTICE file distributed with
1:f3a9cc1:  * this work for additional information regarding copyright ownership.
1:f3a9cc1:  * The ASF licenses this file to You under the Apache License, Version 2.0
1:f3a9cc1:  * (the "License"); you may not use this file except in compliance with
1:f3a9cc1:  * the License.  You may obtain a copy of the License at
1:f3a9cc1:  *
1:f3a9cc1:  *     http://www.apache.org/licenses/LICENSE-2.0
1:f3a9cc1:  *
1:f3a9cc1:  * Unless required by applicable law or agreed to in writing, software
1:f3a9cc1:  * distributed under the License is distributed on an "AS IS" BASIS,
1:f3a9cc1:  * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
1:f3a9cc1:  * See the License for the specific language governing permissions and
1:f3a9cc1:  * limitations under the License.
1:f3a9cc1:  */
1:f3a9cc1: 
1:58d9bf0: package org.apache.mahout.vectorizer;
4:58d9bf0: 
1:87d4b2e: import java.util.Arrays;
1:87d4b2e: 
1:2e5449f: import com.carrotsearch.randomizedtesting.annotations.ThreadLeakScope;
1:d608a88: import com.google.common.io.Closeables;
1:58d9bf0: import org.apache.hadoop.conf.Configuration;
1:58d9bf0: import org.apache.hadoop.fs.FileStatus;
1:58d9bf0: import org.apache.hadoop.fs.FileSystem;
1:58d9bf0: import org.apache.hadoop.fs.Path;
1:58d9bf0: import org.apache.hadoop.io.SequenceFile;
1:58d9bf0: import org.apache.hadoop.io.Text;
1:87d4b2e: import org.apache.lucene.analysis.standard.StandardAnalyzer;
1:e0ec7c1: import org.apache.mahout.common.ClassUtils;
1:58d9bf0: import org.apache.mahout.common.MahoutTestCase;
1:58d9bf0: import org.apache.mahout.common.StringTuple;
1:9101588: import org.apache.mahout.common.iterator.sequencefile.PathFilters;
1:58d9bf0: import org.junit.Test;
1:58d9bf0: 
1:58d9bf0: /**
1:9101588:  * Tests tokenizing of {@link SequenceFile}s containing document ID and text (both as {@link Text})
1:9101588:  * by the {@link DocumentProcessor} into {@link SequenceFile}s of document ID and tokens (as
1:9101588:  * {@link StringTuple}).
1:58d9bf0:  */
1:2e5449f: @ThreadLeakScope(ThreadLeakScope.Scope.NONE)
1:58d9bf0: public class DocumentProcessorTest extends MahoutTestCase {
1:58d9bf0: 
1:58d9bf0:   @Test
1:58d9bf0:   public void testTokenizeDocuments() throws Exception {
1:e3ec9d8:     Configuration configuration = getConfiguration();
1:58d9bf0:     Path input = new Path(getTestTempDirPath(), "inputDir");
1:58d9bf0:     Path output = new Path(getTestTempDirPath(), "outputDir");
1:1de8cec:     FileSystem fs = FileSystem.get(input.toUri(), configuration);
1:58d9bf0: 
1:58d9bf0:     String documentId1 = "123";
1:bbb43bd:     String documentId2 = "456";
1:d608a88: 
1:d608a88:     SequenceFile.Writer writer = new SequenceFile.Writer(fs, configuration, input, Text.class, Text.class);
1:d608a88:     try {
1:96117d3:       String text1 = "A test for the document processor";
1:d608a88:       writer.append(new Text(documentId1), new Text(text1));
1:96117d3:       String text2 = "and another one";
1:d608a88:       writer.append(new Text(documentId2), new Text(text2));
1:d608a88:     } finally {
1:87d4b2e:       Closeables.close(writer, false);
1:d608a88:     }
1:58d9bf0: 
1:6a4942c:     DocumentProcessor.tokenizeDocuments(input, StandardAnalyzer.class, output, configuration);
1:58d9bf0: 
1:9101588:     FileStatus[] statuses = fs.listStatus(output, PathFilters.logsCRCFilter());
1:58d9bf0:     assertEquals(1, statuses.length);
1:58d9bf0:     Path filePath = statuses[0].getPath();
1:58d9bf0:     SequenceFile.Reader reader = new SequenceFile.Reader(fs, filePath, configuration);
1:e0ec7c1:     Text key = ClassUtils.instantiateAs((Class<? extends Text>) reader.getKeyClass(), Text.class);
1:e0ec7c1:     StringTuple value =
1:e0ec7c1:         ClassUtils.instantiateAs((Class<? extends StringTuple>) reader.getValueClass(), StringTuple.class);
1:58d9bf0:     reader.next(key, value);
1:58d9bf0:     assertEquals(documentId1, key.toString());
1:58d9bf0:     assertEquals(Arrays.asList("test", "document", "processor"), value.getEntries());
1:58d9bf0:     reader.next(key, value);
1:58d9bf0:     assertEquals(documentId2, key.toString());
1:58d9bf0:     assertEquals(Arrays.asList("another", "one"), value.getEntries());
1:58d9bf0:   }
1:58d9bf0: }
============================================================================
author:pferrel
-------------------------------------------------------------------------------
commit:b988c49
author:frankscholten
-------------------------------------------------------------------------------
commit:1a42d85
commit:2e5449f
/////////////////////////////////////////////////////////////////////////
1: import com.carrotsearch.randomizedtesting.annotations.ThreadLeakScope;
/////////////////////////////////////////////////////////////////////////
1: @ThreadLeakScope(ThreadLeakScope.Scope.NONE)
author:dfilimon
-------------------------------------------------------------------------------
commit:87d4b2e
/////////////////////////////////////////////////////////////////////////
1: import java.util.Arrays;
1: 
/////////////////////////////////////////////////////////////////////////
1: import org.apache.lucene.analysis.standard.StandardAnalyzer;
/////////////////////////////////////////////////////////////////////////
1:       Closeables.close(writer, false);
author:Grant Ingersoll
-------------------------------------------------------------------------------
commit:31cb292
/////////////////////////////////////////////////////////////////////////
0:       Closeables.close(writer, true);
commit:6a4942c
/////////////////////////////////////////////////////////////////////////
0: import org.apache.lucene.analysis.standard.StandardAnalyzer;
/////////////////////////////////////////////////////////////////////////
1:     DocumentProcessor.tokenizeDocuments(input, StandardAnalyzer.class, output, configuration);
author:Isabel Drost
-------------------------------------------------------------------------------
commit:e3ec9d8
/////////////////////////////////////////////////////////////////////////
1:     Configuration configuration = getConfiguration();
author:Sean R. Owen
-------------------------------------------------------------------------------
commit:1de8cec
/////////////////////////////////////////////////////////////////////////
1:     FileSystem fs = FileSystem.get(input.toUri(), configuration);
commit:e0ec7c1
/////////////////////////////////////////////////////////////////////////
1: import org.apache.mahout.common.ClassUtils;
/////////////////////////////////////////////////////////////////////////
1:     Text key = ClassUtils.instantiateAs((Class<? extends Text>) reader.getKeyClass(), Text.class);
1:     StringTuple value =
1:         ClassUtils.instantiateAs((Class<? extends StringTuple>) reader.getValueClass(), StringTuple.class);
commit:9101588
/////////////////////////////////////////////////////////////////////////
1: import org.apache.mahout.common.iterator.sequencefile.PathFilters;
1:  * Tests tokenizing of {@link SequenceFile}s containing document ID and text (both as {@link Text})
1:  * by the {@link DocumentProcessor} into {@link SequenceFile}s of document ID and tokens (as
1:  * {@link StringTuple}).
/////////////////////////////////////////////////////////////////////////
1:     FileStatus[] statuses = fs.listStatus(output, PathFilters.logsCRCFilter());
commit:96117d3
/////////////////////////////////////////////////////////////////////////
1:       String text1 = "A test for the document processor";
1:       String text2 = "and another one";
commit:f3a9cc1
/////////////////////////////////////////////////////////////////////////
1: /**
1:  * Licensed to the Apache Software Foundation (ASF) under one or more
1:  * contributor license agreements.  See the NOTICE file distributed with
1:  * this work for additional information regarding copyright ownership.
1:  * The ASF licenses this file to You under the Apache License, Version 2.0
1:  * (the "License"); you may not use this file except in compliance with
1:  * the License.  You may obtain a copy of the License at
1:  *
1:  *     http://www.apache.org/licenses/LICENSE-2.0
1:  *
1:  * Unless required by applicable law or agreed to in writing, software
1:  * distributed under the License is distributed on an "AS IS" BASIS,
1:  * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
1:  * See the License for the specific language governing permissions and
1:  * limitations under the License.
1:  */
1: 
commit:d2915a0
/////////////////////////////////////////////////////////////////////////
0:     DocumentProcessor.tokenizeDocuments(input, DefaultAnalyzer.class, output, configuration);
commit:bbb43bd
/////////////////////////////////////////////////////////////////////////
1:     String documentId2 = "456";
0:     String text2 = "and another one";
author:Sebastian Schelter
-------------------------------------------------------------------------------
commit:d608a88
/////////////////////////////////////////////////////////////////////////
1: import com.google.common.io.Closeables;
/////////////////////////////////////////////////////////////////////////
1: 
1:     SequenceFile.Writer writer = new SequenceFile.Writer(fs, configuration, input, Text.class, Text.class);
1:     try {
1:       writer.append(new Text(documentId1), new Text(text1));
1:       writer.append(new Text(documentId2), new Text(text2));
1:     } finally {
0:       Closeables.closeQuietly(writer);
1:     }
commit:58d9bf0
/////////////////////////////////////////////////////////////////////////
1: package org.apache.mahout.vectorizer;
1: 
1: import org.apache.hadoop.conf.Configuration;
1: import org.apache.hadoop.fs.FileStatus;
1: import org.apache.hadoop.fs.FileSystem;
1: import org.apache.hadoop.fs.Path;
1: import org.apache.hadoop.io.SequenceFile;
1: import org.apache.hadoop.io.Text;
1: import org.apache.mahout.common.MahoutTestCase;
1: import org.apache.mahout.common.StringTuple;
1: import org.junit.Test;
1: 
0: import java.util.Arrays;
1: 
1: /**
0:  * Tests tokenizing of <Text documentId, Text text> {@link SequenceFile}s by the {@link DocumentProcessor} into
0:  * <Text documentId, StringTuple tokens> sequence files
1:  */
1: public class DocumentProcessorTest extends MahoutTestCase {
1: 
1:   @Test
1:   public void testTokenizeDocuments() throws Exception {
0:     Configuration configuration = new Configuration();
0:     FileSystem fs = FileSystem.get(configuration);
1:     Path input = new Path(getTestTempDirPath(), "inputDir");
1:     Path output = new Path(getTestTempDirPath(), "outputDir");
1: 
1:     String documentId1 = "123";
0:     String text1 = "A test for the document processor";
0:     String documentId2 = "456";
0:     String text2 = "and another one";
1: 
0:     SequenceFile.Writer writer = new SequenceFile.Writer(fs, configuration, input, Text.class, Text.class);
0:     writer.append(new Text(documentId1), new Text(text1));
0:     writer.append(new Text(documentId2), new Text(text2));
0:     writer.close();
1: 
0:     DocumentProcessor.tokenizeDocuments(input, DefaultAnalyzer.class, output);
1: 
0:     FileStatus[] statuses = fs.listStatus(output);
1:     assertEquals(1, statuses.length);
1:     Path filePath = statuses[0].getPath();
1:     SequenceFile.Reader reader = new SequenceFile.Reader(fs, filePath, configuration);
0:     Text key = reader.getKeyClass().asSubclass(Text.class).newInstance();
0:     StringTuple value = reader.getValueClass().asSubclass(StringTuple.class).newInstance();
1: 
1:     reader.next(key, value);
1:     assertEquals(documentId1, key.toString());
1:     assertEquals(Arrays.asList("test", "document", "processor"), value.getEntries());
1:     reader.next(key, value);
1:     assertEquals(documentId2, key.toString());
1:     assertEquals(Arrays.asList("another", "one"), value.getEntries());
1:   }
1: }
============================================================================