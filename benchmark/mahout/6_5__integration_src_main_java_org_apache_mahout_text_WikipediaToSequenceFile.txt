1:9a15cb8: /*
1:9a15cb8:  * Licensed to the Apache Software Foundation (ASF) under one or more
1:9a15cb8:  * contributor license agreements.  See the NOTICE file distributed with
1:9a15cb8:  * this work for additional information regarding copyright ownership.
1:9a15cb8:  * The ASF licenses this file to You under the Apache License, Version 2.0
1:9a15cb8:  * (the "License"); you may not use this file except in compliance with
1:9a15cb8:  * the License.  You may obtain a copy of the License at
1:9a15cb8:  *
1:9a15cb8:  *     http://www.apache.org/licenses/LICENSE-2.0
1:9a15cb8:  *
1:9a15cb8:  * Unless required by applicable law or agreed to in writing, software
1:9a15cb8:  * distributed under the License is distributed on an "AS IS" BASIS,
1:9a15cb8:  * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
1:9a15cb8:  * See the License for the specific language governing permissions and
1:9a15cb8:  * limitations under the License.
1:9a15cb8:  */
7:9a15cb8: 
1:9a15cb8: package org.apache.mahout.text;
1:9a15cb8: 
1:9a15cb8: import java.io.File;
1:9a15cb8: import java.io.IOException;
1:85f9ece: import java.util.HashSet;
1:9a15cb8: import java.util.Locale;
1:9a15cb8: import java.util.Set;
1:9a15cb8: 
1:9a15cb8: import org.apache.commons.cli2.CommandLine;
1:9a15cb8: import org.apache.commons.cli2.Group;
1:9a15cb8: import org.apache.commons.cli2.Option;
1:9a15cb8: import org.apache.commons.cli2.OptionException;
1:9a15cb8: import org.apache.commons.cli2.builder.ArgumentBuilder;
1:9a15cb8: import org.apache.commons.cli2.builder.DefaultOptionBuilder;
1:9a15cb8: import org.apache.commons.cli2.builder.GroupBuilder;
1:9a15cb8: import org.apache.commons.cli2.commandline.Parser;
1:9a15cb8: import org.apache.hadoop.conf.Configuration;
1:9a15cb8: import org.apache.hadoop.fs.Path;
1:9a15cb8: import org.apache.hadoop.io.DefaultStringifier;
1:4ca6b86: import org.apache.hadoop.io.Stringifier;
1:9a15cb8: import org.apache.hadoop.io.Text;
1:9a15cb8: import org.apache.hadoop.mapreduce.Job;
1:9a15cb8: import org.apache.hadoop.mapreduce.Reducer;
1:9a15cb8: import org.apache.hadoop.mapreduce.lib.input.FileInputFormat;
1:9a15cb8: import org.apache.hadoop.mapreduce.lib.output.FileOutputFormat;
1:9a15cb8: import org.apache.hadoop.mapreduce.lib.output.SequenceFileOutputFormat;
1:9a15cb8: import org.apache.hadoop.util.GenericsUtil;
1:9a15cb8: import org.apache.mahout.common.CommandLineUtil;
1:9a15cb8: import org.apache.mahout.common.HadoopUtil;
1:9a15cb8: import org.apache.mahout.common.commandline.DefaultOptionCreator;
1:9a15cb8: import org.apache.mahout.common.iterator.FileLineIterable;
1:9a15cb8: import org.apache.mahout.text.wikipedia.WikipediaMapper;
1:9a15cb8: import org.apache.mahout.text.wikipedia.XmlInputFormat;
1:9a15cb8: import org.slf4j.Logger;
1:9a15cb8: import org.slf4j.LoggerFactory;
1:9a15cb8: 
1:9a15cb8: /**
1:9a15cb8:  * Create and run the Wikipedia Dataset Creator.
1:9a15cb8:  */
1:9a15cb8: public final class WikipediaToSequenceFile {
1:9a15cb8: 
1:9a15cb8:   private static final Logger log = LoggerFactory.getLogger(WikipediaToSequenceFile.class);
1:9a15cb8:   
1:9a15cb8:   private WikipediaToSequenceFile() { }
1:9a15cb8:   
1:9a15cb8:   /**
1:9a15cb8:    * Takes in two arguments:
1:9a15cb8:    * <ol>
1:9a15cb8:    * <li>The input {@link org.apache.hadoop.fs.Path} where the input documents live</li>
1:9a15cb8:    * <li>The output {@link org.apache.hadoop.fs.Path} where to write the classifier as a
1:9a15cb8:    * {@link org.apache.hadoop.io.SequenceFile}</li>
1:9a15cb8:    * </ol>
1:9a15cb8:    */
1:d3c09ba:   public static void main(String[] args)  throws IOException {
1:9a15cb8:     DefaultOptionBuilder obuilder = new DefaultOptionBuilder();
1:9a15cb8:     ArgumentBuilder abuilder = new ArgumentBuilder();
1:9a15cb8:     GroupBuilder gbuilder = new GroupBuilder();
1:9a15cb8:     
1:9a15cb8:     Option dirInputPathOpt = DefaultOptionCreator.inputOption().create();
1:9a15cb8:     
1:9a15cb8:     Option dirOutputPathOpt = DefaultOptionCreator.outputOption().create();
1:9a15cb8:     
1:9a15cb8:     Option categoriesOpt = obuilder.withLongName("categories").withArgument(
1:9a15cb8:       abuilder.withName("categories").withMinimum(1).withMaximum(1).create()).withDescription(
1:9a15cb8:       "Location of the categories file.  One entry per line. "
1:9a15cb8:           + "Will be used to make a string match in Wikipedia Category field").withShortName("c").create();
1:9a15cb8:     
1:9a15cb8:     Option exactMatchOpt = obuilder.withLongName("exactMatch").withDescription(
1:9a15cb8:       "If set, then the category name must exactly match the "
1:9a15cb8:           + "entry in the categories file. Default is false").withShortName("e").create();
1:9a15cb8:     
1:9a15cb8:     Option allOpt = obuilder.withLongName("all")
1:9a15cb8:         .withDescription("If set, Select all files. Default is false").withShortName("all").create();
1:6dd0c92: 
1:6dd0c92:     Option removeLabelOpt = obuilder.withLongName("removeLabels")
1:6dd0c92:         .withDescription("If set, remove [[Category:labels]] from document text after extracting label."
1:6dd0c92:           + "Default is false").withShortName("rl").create();
1:6dd0c92: 
1:9a15cb8:     Option helpOpt = DefaultOptionCreator.helpOption();
1:9a15cb8:     
1:9a15cb8:     Group group = gbuilder.withName("Options").withOption(categoriesOpt).withOption(dirInputPathOpt)
1:9a15cb8:         .withOption(dirOutputPathOpt).withOption(exactMatchOpt).withOption(allOpt).withOption(helpOpt)
1:6dd0c92:         .withOption(removeLabelOpt).create();
1:9a15cb8:     
1:9a15cb8:     Parser parser = new Parser();
1:9a15cb8:     parser.setGroup(group);
1:9a15cb8:     parser.setHelpOption(helpOpt);
1:9a15cb8:     try {
1:9a15cb8:       CommandLine cmdLine = parser.parse(args);
1:9a15cb8:       if (cmdLine.hasOption(helpOpt)) {
3:9a15cb8:         CommandLineUtil.printHelp(group);
1:9a15cb8:         return;
4:9a15cb8:       }
1:9a15cb8:       
1:9a15cb8:       String inputPath = (String) cmdLine.getValue(dirInputPathOpt);
1:9a15cb8:       String outputPath = (String) cmdLine.getValue(dirOutputPathOpt);
1:9a15cb8:       
1:9a15cb8:       String catFile = "";
1:9a15cb8:       if (cmdLine.hasOption(categoriesOpt)) {
1:9a15cb8:         catFile = (String) cmdLine.getValue(categoriesOpt);
1:9a15cb8:       }
1:9a15cb8:       
1:9a15cb8:       boolean all = false;
1:9a15cb8:       if (cmdLine.hasOption(allOpt)) {
1:9a15cb8:         all = true;
1:9a15cb8:       }
1:6dd0c92: 
1:6dd0c92:       boolean removeLabels = false;
1:6dd0c92:       if (cmdLine.hasOption(removeLabelOpt)) {
1:6dd0c92:           removeLabels = true;
1:6dd0c92:       }
1:6dd0c92: 
1:6dd0c92:       runJob(inputPath, outputPath, catFile, cmdLine.hasOption(exactMatchOpt), all, removeLabels);
1:87c15be:     } catch (OptionException | InterruptedException | ClassNotFoundException e) {
3:9a15cb8:       log.error("Exception", e);
1:9a15cb8:       CommandLineUtil.printHelp(group);
1:9a15cb8:     }
1:9a15cb8:   }
1:9a15cb8:   
1:9a15cb8:   /**
1:9a15cb8:    * Run the job
1:9a15cb8:    * 
1:9a15cb8:    * @param input
1:9a15cb8:    *          the input pathname String
1:9a15cb8:    * @param output
1:9a15cb8:    *          the output pathname String
1:9a15cb8:    * @param catFile
1:9a15cb8:    *          the file containing the Wikipedia categories
1:9a15cb8:    * @param exactMatchOnly
1:9a15cb8:    *          if true, then the Wikipedia category must match exactly instead of simply containing the
1:9a15cb8:    *          category string
1:9a15cb8:    * @param all
1:9a15cb8:    *          if true select all categories
1:6dd0c92:    * @param removeLabels
1:6dd0c92:    *          if true remove Category labels from document text after extracting.
1:6dd0c92:    *
1:9a15cb8:    */
1:9a15cb8:   public static void runJob(String input,
1:9a15cb8:                             String output,
1:9a15cb8:                             String catFile,
1:9a15cb8:                             boolean exactMatchOnly,
1:6dd0c92:                             boolean all,
1:6dd0c92:                             boolean removeLabels) throws IOException, InterruptedException, ClassNotFoundException {
1:9a15cb8:     Configuration conf = new Configuration();
1:9a15cb8:     conf.set("xmlinput.start", "<page>");
1:9a15cb8:     conf.set("xmlinput.end", "</page>");
1:9a15cb8:     conf.setBoolean("exact.match.only", exactMatchOnly);
1:9a15cb8:     conf.setBoolean("all.files", all);
1:6dd0c92:     conf.setBoolean("remove.labels", removeLabels);
1:9a15cb8:     conf.set("io.serializations",
1:9a15cb8:              "org.apache.hadoop.io.serializer.JavaSerialization,"
1:9a15cb8:              + "org.apache.hadoop.io.serializer.WritableSerialization");
1:9a15cb8:     
1:85f9ece:     Set<String> categories = new HashSet<>();
1:d3c09ba:     if (!catFile.isEmpty()) {
1:d3c09ba:       for (String line : new FileLineIterable(new File(catFile))) {
1:d3c09ba:         categories.add(line.trim().toLowerCase(Locale.ENGLISH));
1:d3c09ba:       }
1:d3c09ba:     }
1:d3c09ba:     
1:d3c09ba:     Stringifier<Set<String>> setStringifier =
1:87c15be:         new DefaultStringifier<>(conf, GenericsUtil.getClass(categories));
1:d3c09ba:     
1:d3c09ba:     String categoriesStr = setStringifier.toString(categories);    
1:d3c09ba:     conf.set("wikipedia.categories", categoriesStr);
1:d3c09ba:     
1:9a15cb8:     Job job = new Job(conf);
1:8396a27:     log.info("Input: {} Out: {} Categories: {} All Files: {}", input, output, catFile, all);
1:9a15cb8:     job.setOutputKeyClass(Text.class);
1:9a15cb8:     job.setOutputValueClass(Text.class);
1:9a15cb8:     FileInputFormat.setInputPaths(job, new Path(input));
1:9a15cb8:     Path outPath = new Path(output);
1:9a15cb8:     FileOutputFormat.setOutputPath(job, outPath);
1:9a15cb8:     job.setMapperClass(WikipediaMapper.class);
1:9a15cb8:     job.setInputFormatClass(XmlInputFormat.class);
1:9a15cb8:     job.setReducerClass(Reducer.class);
1:9a15cb8:     job.setOutputFormatClass(SequenceFileOutputFormat.class);
1:9a15cb8:     job.setJarByClass(WikipediaToSequenceFile.class);
1:9a15cb8:     
1:9a15cb8:     /*
1:9a15cb8:      * conf.set("mapred.compress.map.output", "true"); conf.set("mapred.map.output.compression.type",
1:9a15cb8:      * "BLOCK"); conf.set("mapred.output.compress", "true"); conf.set("mapred.output.compression.type",
1:9a15cb8:      * "BLOCK"); conf.set("mapred.output.compression.codec", "org.apache.hadoop.io.compress.GzipCodec");
1:9a15cb8:      */
1:9a15cb8:     HadoopUtil.delete(conf, outPath);
1:d3c09ba: 
1:9a15cb8:     boolean succeeded = job.waitForCompletion(true);
1:9a15cb8:     if (!succeeded) {
1:9a15cb8:       throw new IllegalStateException("Job failed!");
1:9a15cb8:     }
1:d3c09ba:   
1:9a15cb8:   }
1:9a15cb8: }
============================================================================
author:Suneel Marthi
-------------------------------------------------------------------------------
commit:85f9ece
/////////////////////////////////////////////////////////////////////////
1: import java.util.HashSet;
/////////////////////////////////////////////////////////////////////////
1:     Set<String> categories = new HashSet<>();
commit:87c15be
/////////////////////////////////////////////////////////////////////////
1:     } catch (OptionException | InterruptedException | ClassNotFoundException e) {
/////////////////////////////////////////////////////////////////////////
1:         new DefaultStringifier<>(conf, GenericsUtil.getClass(categories));
author:Andrew Palumbo
-------------------------------------------------------------------------------
commit:6dd0c92
/////////////////////////////////////////////////////////////////////////
1: 
1:     Option removeLabelOpt = obuilder.withLongName("removeLabels")
1:         .withDescription("If set, remove [[Category:labels]] from document text after extracting label."
1:           + "Default is false").withShortName("rl").create();
1: 
1:         .withOption(removeLabelOpt).create();
/////////////////////////////////////////////////////////////////////////
1: 
1:       boolean removeLabels = false;
1:       if (cmdLine.hasOption(removeLabelOpt)) {
1:           removeLabels = true;
1:       }
1: 
1:       runJob(inputPath, outputPath, catFile, cmdLine.hasOption(exactMatchOpt), all, removeLabels);
/////////////////////////////////////////////////////////////////////////
1:    * @param removeLabels
1:    *          if true remove Category labels from document text after extracting.
1:    *
1:                             boolean all,
1:                             boolean removeLabels) throws IOException, InterruptedException, ClassNotFoundException {
1:     conf.setBoolean("remove.labels", removeLabels);
author:Sebastian Schelter
-------------------------------------------------------------------------------
commit:d3c09ba
/////////////////////////////////////////////////////////////////////////
1:   public static void main(String[] args)  throws IOException {
/////////////////////////////////////////////////////////////////////////
0:     Set<String> categories = Sets.newHashSet();
1:     if (!catFile.isEmpty()) {
1:       for (String line : new FileLineIterable(new File(catFile))) {
1:         categories.add(line.trim().toLowerCase(Locale.ENGLISH));
1:       }
1:     }
1:     
1:     Stringifier<Set<String>> setStringifier =
0:         new DefaultStringifier<Set<String>>(conf, GenericsUtil.getClass(categories));
1:     
1:     String categoriesStr = setStringifier.toString(categories);    
1:     conf.set("wikipedia.categories", categoriesStr);
1:     
/////////////////////////////////////////////////////////////////////////
1: 
1:   
commit:210b265
/////////////////////////////////////////////////////////////////////////
0: import com.google.common.collect.Sets;
/////////////////////////////////////////////////////////////////////////
0:     Set<String> categories = Sets.newHashSet();;
author:Sean R. Owen
-------------------------------------------------------------------------------
commit:4ca6b86
/////////////////////////////////////////////////////////////////////////
1: import org.apache.hadoop.io.Stringifier;
/////////////////////////////////////////////////////////////////////////
0:     Stringifier<Set<String>> setStringifier =
commit:8396a27
/////////////////////////////////////////////////////////////////////////
1:     log.info("Input: {} Out: {} Categories: {} All Files: {}", input, output, catFile, all);
author:Ted Dunning
-------------------------------------------------------------------------------
commit:402e296
/////////////////////////////////////////////////////////////////////////
/////////////////////////////////////////////////////////////////////////
0:     Set<String> categories = Sets.newHashSet();
author:Robin Anil
-------------------------------------------------------------------------------
commit:9a15cb8
/////////////////////////////////////////////////////////////////////////
1: /*
1:  * Licensed to the Apache Software Foundation (ASF) under one or more
1:  * contributor license agreements.  See the NOTICE file distributed with
1:  * this work for additional information regarding copyright ownership.
1:  * The ASF licenses this file to You under the Apache License, Version 2.0
1:  * (the "License"); you may not use this file except in compliance with
1:  * the License.  You may obtain a copy of the License at
1:  *
1:  *     http://www.apache.org/licenses/LICENSE-2.0
1:  *
1:  * Unless required by applicable law or agreed to in writing, software
1:  * distributed under the License is distributed on an "AS IS" BASIS,
1:  * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
1:  * See the License for the specific language governing permissions and
1:  * limitations under the License.
1:  */
1: 
1: package org.apache.mahout.text;
1: 
1: import java.io.File;
1: import java.io.IOException;
0: import java.util.HashSet;
1: import java.util.Locale;
1: import java.util.Set;
1: 
1: import org.apache.commons.cli2.CommandLine;
1: import org.apache.commons.cli2.Group;
1: import org.apache.commons.cli2.Option;
1: import org.apache.commons.cli2.OptionException;
1: import org.apache.commons.cli2.builder.ArgumentBuilder;
1: import org.apache.commons.cli2.builder.DefaultOptionBuilder;
1: import org.apache.commons.cli2.builder.GroupBuilder;
1: import org.apache.commons.cli2.commandline.Parser;
1: import org.apache.hadoop.conf.Configuration;
1: import org.apache.hadoop.fs.Path;
1: import org.apache.hadoop.io.DefaultStringifier;
1: import org.apache.hadoop.io.Text;
1: import org.apache.hadoop.mapreduce.Job;
1: import org.apache.hadoop.mapreduce.Reducer;
1: import org.apache.hadoop.mapreduce.lib.input.FileInputFormat;
1: import org.apache.hadoop.mapreduce.lib.output.FileOutputFormat;
1: import org.apache.hadoop.mapreduce.lib.output.SequenceFileOutputFormat;
1: import org.apache.hadoop.util.GenericsUtil;
1: import org.apache.mahout.common.CommandLineUtil;
1: import org.apache.mahout.common.HadoopUtil;
1: import org.apache.mahout.common.commandline.DefaultOptionCreator;
1: import org.apache.mahout.common.iterator.FileLineIterable;
1: import org.apache.mahout.text.wikipedia.WikipediaMapper;
1: import org.apache.mahout.text.wikipedia.XmlInputFormat;
1: import org.slf4j.Logger;
1: import org.slf4j.LoggerFactory;
1: 
1: /**
1:  * Create and run the Wikipedia Dataset Creator.
1:  */
1: public final class WikipediaToSequenceFile {
1: 
1:   private static final Logger log = LoggerFactory.getLogger(WikipediaToSequenceFile.class);
1:   
1:   private WikipediaToSequenceFile() { }
1:   
1:   /**
1:    * Takes in two arguments:
1:    * <ol>
1:    * <li>The input {@link org.apache.hadoop.fs.Path} where the input documents live</li>
1:    * <li>The output {@link org.apache.hadoop.fs.Path} where to write the classifier as a
1:    * {@link org.apache.hadoop.io.SequenceFile}</li>
1:    * </ol>
1:    */
0:   public static void main(String[] args) throws IOException {
1:     DefaultOptionBuilder obuilder = new DefaultOptionBuilder();
1:     ArgumentBuilder abuilder = new ArgumentBuilder();
1:     GroupBuilder gbuilder = new GroupBuilder();
1:     
1:     Option dirInputPathOpt = DefaultOptionCreator.inputOption().create();
1:     
1:     Option dirOutputPathOpt = DefaultOptionCreator.outputOption().create();
1:     
1:     Option categoriesOpt = obuilder.withLongName("categories").withArgument(
1:       abuilder.withName("categories").withMinimum(1).withMaximum(1).create()).withDescription(
1:       "Location of the categories file.  One entry per line. "
1:           + "Will be used to make a string match in Wikipedia Category field").withShortName("c").create();
1:     
1:     Option exactMatchOpt = obuilder.withLongName("exactMatch").withDescription(
1:       "If set, then the category name must exactly match the "
1:           + "entry in the categories file. Default is false").withShortName("e").create();
1:     
1:     Option allOpt = obuilder.withLongName("all")
1:         .withDescription("If set, Select all files. Default is false").withShortName("all").create();
1:     
1:     Option helpOpt = DefaultOptionCreator.helpOption();
1:     
1:     Group group = gbuilder.withName("Options").withOption(categoriesOpt).withOption(dirInputPathOpt)
1:         .withOption(dirOutputPathOpt).withOption(exactMatchOpt).withOption(allOpt).withOption(helpOpt)
0:         .create();
1:     
1:     Parser parser = new Parser();
1:     parser.setGroup(group);
1:     parser.setHelpOption(helpOpt);
1:     try {
1:       CommandLine cmdLine = parser.parse(args);
1:       if (cmdLine.hasOption(helpOpt)) {
1:         CommandLineUtil.printHelp(group);
1:         return;
1:       }
1:       
1:       String inputPath = (String) cmdLine.getValue(dirInputPathOpt);
1:       String outputPath = (String) cmdLine.getValue(dirOutputPathOpt);
1:       
1:       String catFile = "";
1:       if (cmdLine.hasOption(categoriesOpt)) {
1:         catFile = (String) cmdLine.getValue(categoriesOpt);
1:       }
1:       
1:       boolean all = false;
1:       if (cmdLine.hasOption(allOpt)) {
1:         all = true;
1:       }
0:       runJob(inputPath, outputPath, catFile, cmdLine.hasOption(exactMatchOpt), all);
0:     } catch (OptionException e) {
1:       log.error("Exception", e);
1:       CommandLineUtil.printHelp(group);
0:     } catch (InterruptedException e) {
1:       log.error("Exception", e);
1:       CommandLineUtil.printHelp(group);
0:     } catch (ClassNotFoundException e) {
1:       log.error("Exception", e);
1:       CommandLineUtil.printHelp(group);
1:     }
1:   }
1:   
1:   /**
1:    * Run the job
1:    * 
1:    * @param input
1:    *          the input pathname String
1:    * @param output
1:    *          the output pathname String
1:    * @param catFile
1:    *          the file containing the Wikipedia categories
1:    * @param exactMatchOnly
1:    *          if true, then the Wikipedia category must match exactly instead of simply containing the
1:    *          category string
1:    * @param all
1:    *          if true select all categories
1:    */
1:   public static void runJob(String input,
1:                             String output,
1:                             String catFile,
1:                             boolean exactMatchOnly,
0:                             boolean all) throws IOException, InterruptedException, ClassNotFoundException {
1:     Configuration conf = new Configuration();
1:     conf.set("xmlinput.start", "<page>");
1:     conf.set("xmlinput.end", "</page>");
1:     conf.setBoolean("exact.match.only", exactMatchOnly);
1:     conf.setBoolean("all.files", all);
1:     conf.set("io.serializations",
1:              "org.apache.hadoop.io.serializer.JavaSerialization,"
1:              + "org.apache.hadoop.io.serializer.WritableSerialization");
1:     
1:     Job job = new Job(conf);
0:     if (log.isInfoEnabled()) {
0:       log.info("Input: {} Out: {} Categories: {} All Files: {}", new Object[] {input, output, catFile, all});
1:     }
1:     job.setOutputKeyClass(Text.class);
1:     job.setOutputValueClass(Text.class);
1:     FileInputFormat.setInputPaths(job, new Path(input));
1:     Path outPath = new Path(output);
1:     FileOutputFormat.setOutputPath(job, outPath);
1:     job.setMapperClass(WikipediaMapper.class);
1:     job.setInputFormatClass(XmlInputFormat.class);
1:     job.setReducerClass(Reducer.class);
1:     job.setOutputFormatClass(SequenceFileOutputFormat.class);
1:     job.setJarByClass(WikipediaToSequenceFile.class);
1:     
1:     /*
1:      * conf.set("mapred.compress.map.output", "true"); conf.set("mapred.map.output.compression.type",
1:      * "BLOCK"); conf.set("mapred.output.compress", "true"); conf.set("mapred.output.compression.type",
1:      * "BLOCK"); conf.set("mapred.output.compression.codec", "org.apache.hadoop.io.compress.GzipCodec");
1:      */
1:     HadoopUtil.delete(conf, outPath);
1:     
0:     Set<String> categories = new HashSet<String>();
0:     if (!catFile.isEmpty()) {
0:       for (String line : new FileLineIterable(new File(catFile))) {
0:         categories.add(line.trim().toLowerCase(Locale.ENGLISH));
1:       }
1:     }
1:     
0:     DefaultStringifier<Set<String>> setStringifier =
0:         new DefaultStringifier<Set<String>>(conf, GenericsUtil.getClass(categories));
1:     
0:     String categoriesStr = setStringifier.toString(categories);
1:     
0:     conf.set("wikipedia.categories", categoriesStr);
1:     
1:     boolean succeeded = job.waitForCompletion(true);
1:     if (!succeeded) {
1:       throw new IllegalStateException("Job failed!");
1:     }
1:   }
1: }
============================================================================