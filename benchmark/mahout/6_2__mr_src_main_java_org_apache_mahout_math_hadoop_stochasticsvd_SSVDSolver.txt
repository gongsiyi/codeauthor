4:151de0d: /**
1:151de0d:  * Licensed to the Apache Software Foundation (ASF) under one or more
1:151de0d:  * contributor license agreements.  See the NOTICE file distributed with
1:151de0d:  * this work for additional information regarding copyright ownership.
1:151de0d:  * The ASF licenses this file to You under the Apache License, Version 2.0
1:151de0d:  * (the "License"); you may not use this file except in compliance with
1:151de0d:  * the License.  You may obtain a copy of the License at
1:478cad9:  *
1:151de0d:  *     http://www.apache.org/licenses/LICENSE-2.0
1:478cad9:  *
1:151de0d:  * Unless required by applicable law or agreed to in writing, software
1:151de0d:  * distributed under the License is distributed on an "AS IS" BASIS,
1:151de0d:  * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
1:151de0d:  * See the License for the specific language governing permissions and
1:151de0d:  * limitations under the License.
7:151de0d:  */
49:151de0d: 
1:151de0d: package org.apache.mahout.math.hadoop.stochasticsvd;
1:ebeade9: 
1:bc8eafd: import com.google.common.collect.Lists;
1:151de0d: import org.apache.hadoop.conf.Configuration;
1:151de0d: import org.apache.hadoop.fs.FileSystem;
1:151de0d: import org.apache.hadoop.fs.Path;
1:151de0d: import org.apache.hadoop.io.Writable;
1:151de0d: import org.apache.mahout.common.IOUtils;
1:a13b4b7: import org.apache.mahout.common.RandomUtils;
1:f43adfe: import org.apache.mahout.math.*;
1:175701c: import org.apache.mahout.math.function.Functions;
1:bc8eafd: import org.apache.mahout.math.solver.EigenDecomposition;
1:5a2250c: 
1:bc8eafd: import java.io.Closeable;
1:bc8eafd: import java.io.IOException;
1:bc8eafd: import java.util.Deque;
1:bc8eafd: import java.util.Random;
1:ffc7fab: 
1:ffc7fab: /**
1:151de0d:  * Stochastic SVD solver (API class).
1:bc8eafd:  * <p/>
1:bc8eafd:  * <p/>
1:151de0d:  * Implementation details are in my working notes in MAHOUT-376
1:151de0d:  * (https://issues.apache.org/jira/browse/MAHOUT-376).
1:bc8eafd:  * <p/>
1:bc8eafd:  * <p/>
1:ffc7fab:  * As of the time of this writing, I don't have benchmarks for this method in
1:ffc7fab:  * comparison to other methods. However, non-hadoop differentiating
1:ffc7fab:  * characteristics of this method are thought to be :
1:ffc7fab:  * <LI>"faster" and precision is traded off in favor of speed. However, there's
1:ffc7fab:  * lever in terms of "oversampling parameter" p. Higher values of p produce
1:ffc7fab:  * better precision but are trading off speed (and minimum RAM requirement).
1:ffc7fab:  * This also means that this method is almost guaranteed to be less precise than
1:ffc7fab:  * Lanczos unless full rank SVD decomposition is sought.
1:ffc7fab:  * <LI>"more scale" -- can presumably take on larger problems than Lanczos one
1:151de0d:  * (not confirmed by benchmark at this time)
1:bc8eafd:  * <p/>
1:bc8eafd:  * <p/>
1:bc8eafd:  * <p/>
1:ffc7fab:  * Specifically in regards to this implementation, <i>I think</i> couple of
1:ffc7fab:  * other differentiating points are:
1:ffc7fab:  * <LI>no need to specify input matrix height or width in command line, it is
1:ffc7fab:  * what it gets to be.
1:ffc7fab:  * <LI>supports any Writable as DRM row keys and copies them to correspondent
1:ffc7fab:  * rows of U matrix;
1:ffc7fab:  * <LI>can request U or V or U<sub>&sigma;</sub>=U* &Sigma;<sup>0.5</sup> or
1:ffc7fab:  * V<sub>&sigma;</sub>=V* &Sigma;<sup>0.5</sup> none of which would require pass
1:ffc7fab:  * over input A and these jobs are parallel map-only jobs.
1:bc8eafd:  * <p/>
1:bc8eafd:  * <p/>
1:bc8eafd:  * <p/>
1:151de0d:  * This class is central public API for SSVD solver. The use pattern is as
1:151de0d:  * follows:
1:bc8eafd:  * <p/>
1:151de0d:  * <UL>
1:151de0d:  * <LI>create the solver using constructor and supplying computation parameters.
1:151de0d:  * <LI>set optional parameters thru setter methods.
1:151de0d:  * <LI>call {@link #run()}.
1:151de0d:  * <LI> {@link #getUPath()} (if computed) returns the path to the directory
1:151de0d:  * containing m x k U matrix file(s).
1:151de0d:  * <LI> {@link #getVPath()} (if computed) returns the path to the directory
1:151de0d:  * containing n x k V matrix file(s).
1:bc8eafd:  * <p/>
1:151de0d:  * </UL>
1:5a2250c:  */
1:8b6a26a: public final class SSVDSolver {
1:151de0d: 
1:175701c:   private Vector svalues;
1:151de0d:   private boolean computeU = true;
1:151de0d:   private boolean computeV = true;
1:151de0d:   private String uPath;
1:151de0d:   private String vPath;
1:478cad9:   private String uSigmaPath;
1:478cad9:   private String uHalfSigmaPath;
1:478cad9:   private String vSigmaPath;
1:478cad9:   private String vHalfSigmaPath;
1:5a2250c:   private int outerBlockHeight = 30000;
1:5a2250c:   private int abtBlockHeight = 200000;
1:151de0d: 
1:151de0d:   // configured stuff
1:a13b4b7:   private final Configuration conf;
1:a13b4b7:   private final Path[] inputPath;
1:a13b4b7:   private final Path outputPath;
1:a13b4b7:   private final int ablockRows;
1:a13b4b7:   private final int k;
1:a13b4b7:   private final int p;
1:ffc7fab:   private int q;
1:a13b4b7:   private final int reduceTasks;
1:151de0d:   private int minSplitSize = -1;
1:151de0d:   private boolean cUHalfSigma;
1:478cad9:   private boolean cUSigma;
1:151de0d:   private boolean cVHalfSigma;
1:478cad9:   private boolean cVSigma;
1:c4550a1:   private boolean overwrite;
1:8bac914:   private boolean broadcast = true;
1:175701c:   private Path pcaMeanPath;
1:151de0d: 
1:b717cfc:   // for debugging
1:b717cfc:   private long omegaSeed;
1:b717cfc: 
1:151de0d:   /**
1:151de0d:    * create new SSVD solver. Required parameters are passed to constructor to
1:151de0d:    * ensure they are set. Optional parameters can be set using setters .
1:bc8eafd:    * <p/>
1:bc8eafd:    *
1:bc8eafd:    * @param conf        hadoop configuration
1:bc8eafd:    * @param inputPath   Input path (should be compatible with DistributedRowMatrix as of
1:bc8eafd:    *                    the time of this writing).
1:bc8eafd:    * @param outputPath  Output path containing U, V and singular values vector files.
1:bc8eafd:    * @param ablockRows  The vertical hight of a q-block (bigger value require more memory
1:bc8eafd:    *                    in mappers+ perhaps larger {@code minSplitSize} values
1:bc8eafd:    * @param k           desired rank
1:bc8eafd:    * @param p           SSVD oversampling parameter
1:bc8eafd:    * @param reduceTasks Number of reduce tasks (where applicable)
1:151de0d:    */
1:a13b4b7:   public SSVDSolver(Configuration conf,
1:a13b4b7:                     Path[] inputPath,
1:a13b4b7:                     Path outputPath,
1:a13b4b7:                     int ablockRows,
1:a13b4b7:                     int k,
1:a13b4b7:                     int p,
1:a13b4b7:                     int reduceTasks) {
1:151de0d:     this.conf = conf;
1:151de0d:     this.inputPath = inputPath;
1:151de0d:     this.outputPath = outputPath;
1:151de0d:     this.ablockRows = ablockRows;
1:151de0d:     this.k = k;
1:151de0d:     this.p = p;
1:151de0d:     this.reduceTasks = reduceTasks;
1:ffc7fab:   }
1:5a2250c: 
1:ffc7fab:   public int getQ() {
1:ffc7fab:     return q;
1:5a2250c:   }
1:5a2250c: 
1:5214b1d:   /**
1:ffc7fab:    * sets q, amount of additional power iterations to increase precision
1:ffc7fab:    * (0..2!). Defaults to 0.
1:bc8eafd:    *
1:ffc7fab:    * @param q
1:5a2250c:    */
1:ffc7fab:   public void setQ(int q) {
1:ffc7fab:     this.q = q;
1:ffc7fab:   }
1:ffc7fab: 
1:ffc7fab:   /**
1:151de0d:    * The setting controlling whether to compute U matrix of low rank SSVD.
1:478cad9:    * Default true.
1:5a2250c:    */
1:151de0d:   public void setComputeU(boolean val) {
1:151de0d:     computeU = val;
1:ffc7fab:   }
1:ffc7fab: 
1:ffc7fab:   /**
1:151de0d:    * Setting controlling whether to compute V matrix of low-rank SSVD.
1:bc8eafd:    *
1:bc8eafd:    * @param val true if we want to output V matrix. Default is true.
1:ffc7fab:    */
1:151de0d:   public void setComputeV(boolean val) {
1:151de0d:     computeV = val;
1:ffc7fab:   }
1:ffc7fab: 
1:151de0d:   /**
1:478cad9:    * @param cUHat whether produce U*Sigma^0.5 as well (default false)
1:478cad9:    */
1:478cad9:   public void setcUHalfSigma(boolean cUHat) {
1:478cad9:     this.cUHalfSigma = cUHat;
1:478cad9:   }
1:478cad9: 
1:478cad9:   /**
1:478cad9:    * @param cVHat whether produce V*Sigma^0.5 as well (default false)
1:478cad9:    */
1:478cad9:   public void setcVHalfSigma(boolean cVHat) {
1:478cad9:     this.cVHalfSigma = cVHat;
1:478cad9:   }
1:478cad9: 
1:478cad9:   /**
1:478cad9:    * @param cUSigma whether produce U*Sigma output as well (default false)
1:478cad9:    */
1:478cad9:   public void setcUSigma(boolean cUSigma) {
1:478cad9:     this.cUSigma = cUSigma;
1:478cad9:   }
1:478cad9: 
1:478cad9:   /**
1:478cad9:    * @param cVSigma whether produce V*Sigma output as well (default false)
1:478cad9:    */
1:478cad9:   public void setcVSigma(boolean cVSigma) {
1:478cad9:     this.cVSigma = cVSigma;
1:478cad9:   }
1:478cad9: 
1:478cad9:   /**
1:151de0d:    * Sometimes, if requested A blocks become larger than a split, we may need to
1:151de0d:    * use that to ensure at least k+p rows of A get into a split. This is
1:151de0d:    * requirement necessary to obtain orthonormalized Q blocks of SSVD.
1:bc8eafd:    *
1:bc8eafd:    * @param size the minimum split size to use
1:ffc7fab:    */
1:151de0d:   public void setMinSplitSize(int size) {
1:151de0d:     minSplitSize = size;
1:ffc7fab:   }
1:ffc7fab: 
1:151de0d:   /**
1:151de0d:    * This contains k+p singular values resulted from the solver run.
1:bc8eafd:    *
1:151de0d:    * @return singlular values (largest to smallest)
1:ffc7fab:    */
1:175701c:   public Vector getSingularValues() {
1:151de0d:     return svalues;
1:ffc7fab:   }
1:ffc7fab: 
1:151de0d:   /**
1:151de0d:    * returns U path (if computation were requested and successful).
1:bc8eafd:    *
1:151de0d:    * @return U output hdfs path, or null if computation was not completed for
1:151de0d:    *         whatever reason.
1:151de0d:    */
1:151de0d:   public String getUPath() {
1:151de0d:     return uPath;
1:ffc7fab:   }
1:ffc7fab: 
1:151de0d:   /**
1:151de0d:    * return V path ( if computation was requested and successful ) .
1:bc8eafd:    *
1:151de0d:    * @return V output hdfs path, or null if computation was not completed for
1:151de0d:    *         whatever reason.
1:151de0d:    */
1:151de0d:   public String getVPath() {
1:151de0d:     return vPath;
1:ffc7fab:   }
1:ffc7fab: 
1:478cad9:   public String getuSigmaPath() {
1:478cad9:     return uSigmaPath;
1:478cad9:   }
1:478cad9: 
1:478cad9:   public String getuHalfSigmaPath() {
1:478cad9:     return uHalfSigmaPath;
1:478cad9:   }
1:478cad9: 
1:478cad9:   public String getvSigmaPath() {
1:478cad9:     return vSigmaPath;
1:478cad9:   }
1:478cad9: 
1:478cad9:   public String getvHalfSigmaPath() {
1:478cad9:     return vHalfSigmaPath;
1:478cad9:   }
1:478cad9: 
1:c4550a1:   public boolean isOverwrite() {
1:c4550a1:     return overwrite;
1:ffc7fab:   }
1:ffc7fab: 
1:151de0d:   /**
1:c4550a1:    * if true, driver to clean output folder first if exists.
1:bc8eafd:    *
1:c4550a1:    * @param overwrite
1:151de0d:    */
1:c4550a1:   public void setOverwrite(boolean overwrite) {
1:c4550a1:     this.overwrite = overwrite;
1:ffc7fab:   }
1:151de0d: 
1:5a2250c:   public int getOuterBlockHeight() {
1:5a2250c:     return outerBlockHeight;
1:5a2250c:   }
1:5a2250c: 
1:5a2250c:   /**
1:5a2250c:    * The height of outer blocks during Q'A multiplication. Higher values allow
1:5a2250c:    * to produce less keys for combining and shuffle and sort therefore somewhat
1:5a2250c:    * improving running time; but require larger blocks to be formed in RAM (so
1:5a2250c:    * setting this too high can lead to OOM).
1:bc8eafd:    *
1:5a2250c:    * @param outerBlockHeight
1:5a2250c:    */
1:5a2250c:   public void setOuterBlockHeight(int outerBlockHeight) {
1:5a2250c:     this.outerBlockHeight = outerBlockHeight;
1:5a2250c:   }
1:5a2250c: 
1:5a2250c:   public int getAbtBlockHeight() {
1:5a2250c:     return abtBlockHeight;
1:5a2250c:   }
1:5a2250c: 
1:5a2250c:   /**
1:5a2250c:    * the block height of Y_i during power iterations. It is probably important
1:5a2250c:    * to set it higher than default 200,000 for extremely sparse inputs and when
1:5a2250c:    * more ram is available. y_i block height and ABt job would occupy approx.
1:5a2250c:    * abtBlockHeight x (k+p) x sizeof (double) (as dense).
1:bc8eafd:    *
1:5a2250c:    * @param abtBlockHeight
1:5a2250c:    */
1:5a2250c:   public void setAbtBlockHeight(int abtBlockHeight) {
1:5a2250c:     this.abtBlockHeight = abtBlockHeight;
1:5214b1d:   }
1:5214b1d: 
1:8bac914:   public boolean isBroadcast() {
1:8bac914:     return broadcast;
1:8bac914:   }
1:8bac914: 
1:8bac914:   /**
1:8bac914:    * If this property is true, use DestributedCache mechanism to broadcast some
1:8bac914:    * stuff around. May improve efficiency. Default is false.
1:bc8eafd:    *
1:8bac914:    * @param broadcast
1:8bac914:    */
1:8bac914:   public void setBroadcast(boolean broadcast) {
1:8bac914:     this.broadcast = broadcast;
1:8bac914:   }
1:8bac914: 
1:151de0d:   /**
1:175701c:    * Optional. Single-vector file path for a vector (aka xi in MAHOUT-817
1:175701c:    * working notes) to be subtracted from each row of input.
1:bc8eafd:    * <p/>
1:bc8eafd:    * <p/>
1:175701c:    * Brute force approach would force would turn input into a dense input, which
1:175701c:    * is often not very desirable. By supplying this offset to SSVD solver, we
1:175701c:    * can avoid most of that overhead due to increased input density.
1:bc8eafd:    * <p/>
1:bc8eafd:    * <p/>
1:175701c:    * The vector size for this offest is n (width of A input). In PCA and R this
1:175701c:    * is known as "column means", but in this case it can be any offset of row
1:175701c:    * vectors of course to propagate into SSVD solution.
1:bc8eafd:    * <p/>
1:175701c:    */
1:175701c:   public Path getPcaMeanPath() {
1:175701c:     return pcaMeanPath;
1:175701c:   }
1:175701c: 
1:175701c:   public void setPcaMeanPath(Path pcaMeanPath) {
1:175701c:     this.pcaMeanPath = pcaMeanPath;
1:175701c:   }
1:175701c: 
1:b717cfc:   long getOmegaSeed() {
1:b717cfc:     return omegaSeed;
1:b717cfc:   }
1:b717cfc: 
1:175701c:   /**
1:151de0d:    * run all SSVD jobs.
1:bc8eafd:    *
1:bc8eafd:    * @throws IOException if I/O condition occurs.
1:5214b1d:    */
1:151de0d:   public void run() throws IOException {
1:5214b1d: 
1:564c3e1:     Deque<Closeable> closeables = Lists.newLinkedList();
4:151de0d:     try {
1:ffc7fab:       Class<? extends Writable> labelType =
1:175701c:         SSVDHelper.sniffInputLabelType(inputPath, conf);
1:175701c:       FileSystem fs = FileSystem.get(conf);
1:5a2250c: 
1:151de0d:       Path qPath = new Path(outputPath, "Q-job");
1:151de0d:       Path btPath = new Path(outputPath, "Bt-job");
1:151de0d:       Path uHatPath = new Path(outputPath, "UHat");
1:151de0d:       Path svPath = new Path(outputPath, "Sigma");
1:151de0d:       Path uPath = new Path(outputPath, "U");
1:478cad9:       Path uSigmaPath = new Path(outputPath, "USigma");
1:478cad9:       Path uHalfSigmaPath = new Path(outputPath, "UHalfSigma");
1:151de0d:       Path vPath = new Path(outputPath, "V");
1:478cad9:       Path vHalfSigmaPath = new Path(outputPath, "VHalfSigma");
1:478cad9:       Path vSigmaPath = new Path(outputPath, "VSigma");
1:175701c: 
1:175701c:       Path pcaBasePath = new Path(outputPath, "pca");
1:175701c: 
1:b717cfc:       if (overwrite) {
1:b717cfc:         fs.delete(outputPath, true);
1:b717cfc:       }
1:b717cfc: 
1:229aeff:       if (pcaMeanPath != null) {
1:175701c:         fs.mkdirs(pcaBasePath);
1:5214b1d:       }
1:175701c:       Random rnd = RandomUtils.getRandom();
1:b717cfc:       omegaSeed = rnd.nextLong();
1:175701c: 
1:175701c:       Path sbPath = null;
1:229aeff:       double xisquaredlen = 0.0;
1:175701c:       if (pcaMeanPath != null) {
1:175701c:         /*
1:175701c:          * combute s_b0 if pca offset present.
1:478cad9:          * 
1:175701c:          * Just in case, we treat xi path as a possible reduce or otherwise
1:175701c:          * multiple task output that we assume we need to sum up partial
1:175701c:          * components. If it is just one file, it will work too.
1:175701c:          */
1:175701c: 
1:175701c:         Vector xi = SSVDHelper.loadAndSumUpVectors(pcaMeanPath, conf);
1:03e6875:         if (xi == null) {
1:03e6875:           throw new IOException(String.format("unable to load mean path xi from %s.",
1:03e6875:                                               pcaMeanPath.toString()));
1:03e6875:         }
1:03e6875: 
1:175701c:         xisquaredlen = xi.dot(xi);
1:b717cfc:         Omega omega = new Omega(omegaSeed, k + p);
1:175701c:         Vector s_b0 = omega.mutlithreadedTRightMultiply(xi);
1:175701c: 
1:b717cfc:         SSVDHelper.saveVector(s_b0, sbPath = new Path(pcaBasePath, "somega.seq"), conf);
1:175701c:       }
1:175701c: 
1:175701c:       /*
1:175701c:        * if we work with pca offset, we need to precompute s_bq0 aka s_omega for
1:175701c:        * jobs to use.
1:175701c:        */
1:151de0d: 
1:ffc7fab:       QJob.run(conf,
2:ffc7fab:                inputPath,
1:175701c:                sbPath,
2:ffc7fab:                qPath,
2:ffc7fab:                ablockRows,
2:ffc7fab:                minSplitSize,
1:5a2250c:                k,
2:ffc7fab:                p,
1:b717cfc:                omegaSeed,
1:ebeade9:                reduceTasks);
1:151de0d: 
1:8bac914:       /*
1:8bac914:        * restrict number of reducers to a reasonable number so we don't have to
1:8bac914:        * run too many additions in the frontend when reconstructing BBt for the
1:8bac914:        * last B' and BB' computations. The user may not realize that and gives a
1:8bac914:        * bit too many (I would be happy i that were ever the case though).
1:8bac914:        */
1:175701c: 
1:ffc7fab:       BtJob.run(conf,
1:ffc7fab:                 inputPath,
1:ffc7fab:                 qPath,
1:175701c:                 pcaMeanPath,
1:ffc7fab:                 btPath,
1:ffc7fab:                 minSplitSize,
4:ffc7fab:                 k,
1:ffc7fab:                 p,
2:ffc7fab:                 outerBlockHeight,
1:8bac914:                 q <= 0 ? Math.min(1000, reduceTasks) : reduceTasks,
1:8bac914:                 broadcast,
1:175701c:                 labelType,
1:1499411:                 q <= 0);
1:ffc7fab: 
1:175701c:       sbPath = new Path(btPath, BtJob.OUTPUT_SB + "-*");
1:229aeff:       Path sqPath = new Path(btPath, BtJob.OUTPUT_SQ + "-*");
1:175701c: 
1:ffc7fab:       // power iterations
1:7be37c0:       for (int i = 0; i < q; i++) {
1:ffc7fab: 
1:ffc7fab:         qPath = new Path(outputPath, String.format("ABt-job-%d", i + 1));
1:5a2250c:         Path btPathGlob = new Path(btPath, BtJob.OUTPUT_BT + "-*");
1:5a2250c:         ABtDenseOutJob.run(conf,
1:5a2250c:                            inputPath,
1:5a2250c:                            btPathGlob,
1:175701c:                            pcaMeanPath,
1:175701c:                            sqPath,
1:175701c:                            sbPath,
1:5a2250c:                            qPath,
1:5a2250c:                            ablockRows,
1:5a2250c:                            minSplitSize,
1:175701c:                            k,
1:5a2250c:                            p,
1:5a2250c:                            abtBlockHeight,
2:175701c:                            reduceTasks,
1:8bac914:                            broadcast);
1:ffc7fab: 
1:ffc7fab:         btPath = new Path(outputPath, String.format("Bt-job-%d", i + 1));
1:ffc7fab: 
1:ffc7fab:         BtJob.run(conf,
1:ffc7fab:                   inputPath,
1:ffc7fab:                   qPath,
1:175701c:                   pcaMeanPath,
1:ffc7fab:                   btPath,
1:ffc7fab:                   minSplitSize,
1:175701c:                   k,
1:ffc7fab:                   p,
1:ffc7fab:                   outerBlockHeight,
1:8bac914:                   i == q - 1 ? Math.min(1000, reduceTasks) : reduceTasks,
1:8bac914:                   broadcast,
1:478cad9:                   labelType,
1:1499411:                   i == q - 1);
1:175701c:         sbPath = new Path(btPath, BtJob.OUTPUT_SB + "-*");
1:175701c:         sqPath = new Path(btPath, BtJob.OUTPUT_SQ + "-*");
1:5a2250c:       }
1:ffc7fab: 
1:bc8eafd:       DenseSymmetricMatrix bbt =
1:bc8eafd:         SSVDHelper.loadAndSumUpperTriangularMatricesAsSymmetric(new Path(btPath,
1:bc8eafd:                                                                          BtJob.OUTPUT_BBT
1:bc8eafd:                                                                            + "-*"), conf);
1:151de0d: 
1:151de0d:       // convert bbt to something our eigensolver could understand
1:bc8eafd:       assert bbt.columnSize() == k + p;
1:151de0d: 
1:175701c:       /*
1:175701c:        * we currently use a 3rd party in-core eigensolver. So we need just a
1:175701c:        * dense array representation for it.
1:175701c:        */
1:bc8eafd:       Matrix bbtSquare = new DenseMatrix(k + p, k + p);
1:bc8eafd:       bbtSquare.assign(bbt);
1:175701c: 
1:175701c:       // MAHOUT-817
1:175701c:       if (pcaMeanPath != null) {
1:175701c:         Vector sq = SSVDHelper.loadAndSumUpVectors(sqPath, conf);
1:175701c:         Vector sb = SSVDHelper.loadAndSumUpVectors(sbPath, conf);
1:175701c:         Matrix mC = sq.cross(sb);
1:151de0d: 
1:175701c:         bbtSquare.assign(mC, Functions.MINUS);
1:175701c:         bbtSquare.assign(mC.transpose(), Functions.MINUS);
1:151de0d: 
1:175701c:         Matrix outerSq = sq.cross(sq);
1:175701c:         outerSq.assign(Functions.mult(xisquaredlen));
1:175701c:         bbtSquare.assign(outerSq, Functions.PLUS);
1:175701c: 
21:151de0d:       }
1:151de0d: 
1:bc8eafd:       EigenDecomposition eigen = new EigenDecomposition(bbtSquare);
1:bc8eafd: 
1:bc8eafd:       Matrix uHat = eigen.getV();
1:bc8eafd:       svalues = eigen.getRealEigenvalues().clone();
1:151de0d: 
1:175701c:       svalues.assign(Functions.SQRT);
1:175701c: 
1:151de0d:       // save/redistribute UHat
1:151de0d:       fs.mkdirs(uHatPath);
1:175701c:       DistributedRowMatrixWriter.write(uHatPath =
1:bc8eafd:                                          new Path(uHatPath, "uhat.seq"), conf, uHat);
1:151de0d: 
1:175701c:       // save sigma.
1:175701c:       SSVDHelper.saveVector(svalues,
1:175701c:                             svPath = new Path(svPath, "svalues.seq"),
1:175701c:                             conf);
1:151de0d: 
1:151de0d:       UJob ujob = null;
1:a13b4b7:       if (computeU) {
1:a13b4b7:         ujob = new UJob();
1:175701c:         ujob.run(conf,
2:478cad9:                  new Path(btPath, BtJob.OUTPUT_Q + "-*"),
1:478cad9:                  uHatPath,
1:478cad9:                  svPath,
1:175701c:                  uPath,
1:478cad9:                  k,
1:478cad9:                  reduceTasks,
1:478cad9:                  labelType,
1:478cad9:                  OutputScalingEnum.NOSCALING);
1:ffc7fab:         // actually this is map-only job anyway
1:151de0d:       }
1:151de0d: 
1:478cad9:       UJob uhsjob = null;
1:478cad9:       if (cUHalfSigma) {
1:478cad9:         uhsjob = new UJob();
1:478cad9:         uhsjob.run(conf,
1:bc8eafd:                    new Path(btPath, BtJob.OUTPUT_Q + "-*"),
1:bc8eafd:                    uHatPath,
1:bc8eafd:                    svPath,
1:bc8eafd:                    uHalfSigmaPath,
1:bc8eafd:                    k,
1:bc8eafd:                    reduceTasks,
1:bc8eafd:                    labelType,
1:bc8eafd:                    OutputScalingEnum.HALFSIGMA);
1:478cad9:       }
1:478cad9: 
1:478cad9:       UJob usjob = null;
1:478cad9:       if (cUSigma) {
1:478cad9:         usjob = new UJob();
1:478cad9:         usjob.run(conf,
1:bc8eafd:                   new Path(btPath, BtJob.OUTPUT_Q + "-*"),
1:bc8eafd:                   uHatPath,
1:bc8eafd:                   svPath,
1:bc8eafd:                   uSigmaPath,
1:bc8eafd:                   k,
1:bc8eafd:                   reduceTasks,
1:bc8eafd:                   labelType,
1:bc8eafd:                   OutputScalingEnum.SIGMA);
1:478cad9:       }
1:478cad9: 
1:151de0d:       VJob vjob = null;
1:a13b4b7:       if (computeV) {
1:a13b4b7:         vjob = new VJob();
1:175701c:         vjob.run(conf,
1:175701c:                  new Path(btPath, BtJob.OUTPUT_BT + "-*"),
1:175701c:                  pcaMeanPath,
1:175701c:                  sqPath,
1:478cad9:                  uHatPath,
1:478cad9:                  svPath,
1:175701c:                  vPath,
1:478cad9:                  k,
1:478cad9:                  reduceTasks,
1:478cad9:                  OutputScalingEnum.NOSCALING);
1:478cad9:       }
1:478cad9: 
1:478cad9:       VJob vhsjob = null;
1:478cad9:       if (cVHalfSigma) {
1:478cad9:         vhsjob = new VJob();
1:478cad9:         vhsjob.run(conf,
1:478cad9:                    new Path(btPath, BtJob.OUTPUT_BT + "-*"),
1:478cad9:                    pcaMeanPath,
1:478cad9:                    sqPath,
1:478cad9:                    uHatPath,
1:478cad9:                    svPath,
1:478cad9:                    vHalfSigmaPath,
1:478cad9:                    k,
1:478cad9:                    reduceTasks,
1:478cad9:                    OutputScalingEnum.HALFSIGMA);
1:478cad9:       }
1:478cad9: 
1:478cad9:       VJob vsjob = null;
1:478cad9:       if (cVSigma) {
1:478cad9:         vsjob = new VJob();
1:478cad9:         vsjob.run(conf,
1:478cad9:                   new Path(btPath, BtJob.OUTPUT_BT + "-*"),
1:478cad9:                   pcaMeanPath,
1:478cad9:                   sqPath,
1:478cad9:                   uHatPath,
1:478cad9:                   svPath,
1:478cad9:                   vSigmaPath,
1:478cad9:                   k,
1:478cad9:                   reduceTasks,
1:478cad9:                   OutputScalingEnum.SIGMA);
1:ffc7fab:       }
1:151de0d: 
1:151de0d:       if (ujob != null) {
1:151de0d:         ujob.waitForCompletion();
1:151de0d:         this.uPath = uPath.toString();
1:151de0d:       }
1:478cad9:       if (uhsjob != null) {
1:478cad9:         uhsjob.waitForCompletion();
1:478cad9:         this.uHalfSigmaPath = uHalfSigmaPath.toString();
1:478cad9:       }
1:478cad9:       if (usjob != null) {
1:478cad9:         usjob.waitForCompletion();
1:478cad9:         this.uSigmaPath = uSigmaPath.toString();
1:478cad9:       }
1:151de0d:       if (vjob != null) {
1:151de0d:         vjob.waitForCompletion();
1:151de0d:         this.vPath = vPath.toString();
1:151de0d:       }
1:478cad9:       if (vhsjob != null) {
1:478cad9:         vhsjob.waitForCompletion();
1:478cad9:         this.vHalfSigmaPath = vHalfSigmaPath.toString();
1:478cad9:       }
1:478cad9:       if (vsjob != null) {
1:478cad9:         vsjob.waitForCompletion();
1:478cad9:         this.vSigmaPath = vSigmaPath.toString();
1:478cad9:       }
1:151de0d: 
1:151de0d:     } catch (InterruptedException exc) {
1:151de0d:       throw new IOException("Interrupted", exc);
1:151de0d:     } catch (ClassNotFoundException exc) {
1:151de0d:       throw new IOException(exc);
1:151de0d: 
4:151de0d:     } finally {
1:151de0d:       IOUtils.close(closeables);
1:478cad9:     }
1:151de0d:   }
1:151de0d: 
1:8b6a26a:   enum OutputScalingEnum {
1:478cad9:     NOSCALING, SIGMA, HALFSIGMA
1:151de0d:   }
1:151de0d: }
============================================================================
author:pferrel
-------------------------------------------------------------------------------
commit:b988c49
author:Suneel Marthi
-------------------------------------------------------------------------------
commit:87c15be
/////////////////////////////////////////////////////////////////////////
author:frankscholten
-------------------------------------------------------------------------------
commit:1a42d85
author:Dmitriy Lyubimov
-------------------------------------------------------------------------------
commit:b717cfc
/////////////////////////////////////////////////////////////////////////
1:   // for debugging
1:   private long omegaSeed;
1: 
/////////////////////////////////////////////////////////////////////////
1:   long getOmegaSeed() {
1:     return omegaSeed;
1:   }
1: 
/////////////////////////////////////////////////////////////////////////
1:       if (overwrite) {
1:         fs.delete(outputPath, true);
1:       }
1: 
1:       omegaSeed = rnd.nextLong();
/////////////////////////////////////////////////////////////////////////
1:         Omega omega = new Omega(omegaSeed, k + p);
1:         SSVDHelper.saveVector(s_b0, sbPath = new Path(pcaBasePath, "somega.seq"), conf);
/////////////////////////////////////////////////////////////////////////
1:                omegaSeed,
commit:bc8eafd
/////////////////////////////////////////////////////////////////////////
1: import com.google.common.collect.Lists;
/////////////////////////////////////////////////////////////////////////
1: import org.apache.mahout.math.solver.EigenDecomposition;
1: import java.io.Closeable;
1: import java.io.IOException;
1: import java.util.Deque;
1: import java.util.Random;
1:  * <p/>
1:  * <p/>
1:  * <p/>
1:  * <p/>
/////////////////////////////////////////////////////////////////////////
1:  * <p/>
1:  * <p/>
1:  * <p/>
/////////////////////////////////////////////////////////////////////////
1:  * <p/>
1:  * <p/>
1:  * <p/>
1:  * <p/>
/////////////////////////////////////////////////////////////////////////
1:  * <p/>
/////////////////////////////////////////////////////////////////////////
1:    * <p/>
1:    *
1:    * @param conf        hadoop configuration
1:    * @param inputPath   Input path (should be compatible with DistributedRowMatrix as of
1:    *                    the time of this writing).
1:    * @param outputPath  Output path containing U, V and singular values vector files.
1:    * @param ablockRows  The vertical hight of a q-block (bigger value require more memory
1:    *                    in mappers+ perhaps larger {@code minSplitSize} values
1:    * @param k           desired rank
1:    * @param p           SSVD oversampling parameter
1:    * @param reduceTasks Number of reduce tasks (where applicable)
0:    * @throws IOException when IO condition occurs.
/////////////////////////////////////////////////////////////////////////
1:    *
/////////////////////////////////////////////////////////////////////////
/////////////////////////////////////////////////////////////////////////
1:    *
1:    * @param val true if we want to output V matrix. Default is true.
/////////////////////////////////////////////////////////////////////////
/////////////////////////////////////////////////////////////////////////
/////////////////////////////////////////////////////////////////////////
1:    *
1:    * @param size the minimum split size to use
/////////////////////////////////////////////////////////////////////////
1:    *
/////////////////////////////////////////////////////////////////////////
1:    *
/////////////////////////////////////////////////////////////////////////
1:    *
/////////////////////////////////////////////////////////////////////////
1:    *
/////////////////////////////////////////////////////////////////////////
1:    *
/////////////////////////////////////////////////////////////////////////
1:    *
/////////////////////////////////////////////////////////////////////////
1:    *
/////////////////////////////////////////////////////////////////////////
1:    * <p/>
1:    * <p/>
1:    * <p/>
1:    * <p/>
1:    * <p/>
/////////////////////////////////////////////////////////////////////////
1:    *
1:    * @throws IOException if I/O condition occurs.
/////////////////////////////////////////////////////////////////////////
1:       DenseSymmetricMatrix bbt =
1:         SSVDHelper.loadAndSumUpperTriangularMatricesAsSymmetric(new Path(btPath,
1:                                                                          BtJob.OUTPUT_BBT
1:                                                                            + "-*"), conf);
1:       assert bbt.columnSize() == k + p;
1:       Matrix bbtSquare = new DenseMatrix(k + p, k + p);
1:       bbtSquare.assign(bbt);
/////////////////////////////////////////////////////////////////////////
1:       EigenDecomposition eigen = new EigenDecomposition(bbtSquare);
1: 
1:       Matrix uHat = eigen.getV();
1:       svalues = eigen.getRealEigenvalues().clone();
1:                                          new Path(uHatPath, "uhat.seq"), conf, uHat);
/////////////////////////////////////////////////////////////////////////
1:                    new Path(btPath, BtJob.OUTPUT_Q + "-*"),
1:                    uHatPath,
1:                    svPath,
1:                    uHalfSigmaPath,
1:                    k,
1:                    reduceTasks,
1:                    labelType,
1:                    OutputScalingEnum.HALFSIGMA);
1:                   new Path(btPath, BtJob.OUTPUT_Q + "-*"),
1:                   uHatPath,
1:                   svPath,
1:                   uSigmaPath,
1:                   k,
1:                   reduceTasks,
1:                   labelType,
1:                   OutputScalingEnum.SIGMA);
commit:f43adfe
/////////////////////////////////////////////////////////////////////////
1: import org.apache.mahout.math.*;
/////////////////////////////////////////////////////////////////////////
0:       org.apache.mahout.math.UpperTriangular bbtTriangular =
commit:03e6875
/////////////////////////////////////////////////////////////////////////
1:         if (xi == null) {
1:           throw new IOException(String.format("unable to load mean path xi from %s.",
1:                                               pcaMeanPath.toString()));
1:         }
1: 
commit:478cad9
/////////////////////////////////////////////////////////////////////////
1:   private String uSigmaPath;
1:   private String uHalfSigmaPath;
1:   private String vSigmaPath;
1:   private String vHalfSigmaPath;
/////////////////////////////////////////////////////////////////////////
1:   private boolean cUSigma;
1:   private boolean cVSigma;
/////////////////////////////////////////////////////////////////////////
/////////////////////////////////////////////////////////////////////////
1:    * Default true.
/////////////////////////////////////////////////////////////////////////
1:    * 
1:    * @param cUHat whether produce U*Sigma^0.5 as well (default false)
1:    */
1:   public void setcUHalfSigma(boolean cUHat) {
1:     this.cUHalfSigma = cUHat;
1:   }
1: 
1:   /**
1:    * 
1:    * @param cVHat whether produce V*Sigma^0.5 as well (default false)
1:    */
1:   public void setcVHalfSigma(boolean cVHat) {
1:     this.cVHalfSigma = cVHat;
1:   }
1: 
1:   /**
1:    * 
1:    * @param cUSigma whether produce U*Sigma output as well (default false)
1:    */
1:   public void setcUSigma(boolean cUSigma) {
1:     this.cUSigma = cUSigma;
1:   }
1: 
1:   /**
1:    * @param cVSigma whether produce V*Sigma output as well (default false)
1:    */
1:   public void setcVSigma(boolean cVSigma) {
1:     this.cVSigma = cVSigma;
1:   }
1: 
1:   /**
/////////////////////////////////////////////////////////////////////////
1:   public String getuSigmaPath() {
1:     return uSigmaPath;
1:   }
1: 
1:   public String getuHalfSigmaPath() {
1:     return uHalfSigmaPath;
1:   }
1: 
1:   public String getvSigmaPath() {
1:     return vSigmaPath;
1:   }
1: 
1:   public String getvHalfSigmaPath() {
1:     return vHalfSigmaPath;
1:   }
1: 
/////////////////////////////////////////////////////////////////////////
1:       Path uSigmaPath = new Path(outputPath, "USigma");
1:       Path uHalfSigmaPath = new Path(outputPath, "UHalfSigma");
1:       Path vHalfSigmaPath = new Path(outputPath, "VHalfSigma");
1:       Path vSigmaPath = new Path(outputPath, "VSigma");
/////////////////////////////////////////////////////////////////////////
/////////////////////////////////////////////////////////////////////////
1:                  OutputScalingEnum.NOSCALING);
1:       UJob uhsjob = null;
1:       if (cUHalfSigma) {
1:         uhsjob = new UJob();
1:         uhsjob.run(conf,
1:                  new Path(btPath, BtJob.OUTPUT_Q + "-*"),
1:                  uHatPath,
1:                  svPath,
0:                  uHalfSigmaPath,
1:                  k,
1:                  reduceTasks,
1:                  labelType,
1:                  OutputScalingEnum.HALFSIGMA);
1:       }
1: 
1:       UJob usjob = null;
1:       if (cUSigma) {
1:         usjob = new UJob();
1:         usjob.run(conf,
1:                  new Path(btPath, BtJob.OUTPUT_Q + "-*"),
1:                  uHatPath,
1:                  svPath,
0:                  uSigmaPath,
1:                  k,
1:                  reduceTasks,
1:                  labelType,
1:                  OutputScalingEnum.SIGMA);
1:       }
1: 
/////////////////////////////////////////////////////////////////////////
1:                  OutputScalingEnum.NOSCALING);
1:       }
1: 
1:       VJob vhsjob = null;
1:       if (cVHalfSigma) {
1:         vhsjob = new VJob();
1:         vhsjob.run(conf,
1:                    new Path(btPath, BtJob.OUTPUT_BT + "-*"),
1:                    pcaMeanPath,
1:                    sqPath,
1:                    uHatPath,
1:                    svPath,
1:                    vHalfSigmaPath,
1:                    k,
1:                    reduceTasks,
0:                    OutputScalingEnum.HALFSIGMA);
1:       }
1: 
1:       VJob vsjob = null;
1:       if (cVSigma) {
1:         vsjob = new VJob();
1:         vsjob.run(conf,
1:                   new Path(btPath, BtJob.OUTPUT_BT + "-*"),
1:                   pcaMeanPath,
1:                   sqPath,
1:                   uHatPath,
1:                   svPath,
1:                   vSigmaPath,
1:                   k,
1:                   reduceTasks,
0:                   OutputScalingEnum.SIGMA);
1:       if (uhsjob != null) {
1:         uhsjob.waitForCompletion();
1:         this.uHalfSigmaPath = uHalfSigmaPath.toString();
1:       }
1:       if (usjob != null) {
1:         usjob.waitForCompletion();
1:         this.uSigmaPath = uSigmaPath.toString();
1:       }
1:       if (vhsjob != null) {
1:         vhsjob.waitForCompletion();
1:         this.vHalfSigmaPath = vHalfSigmaPath.toString();
1:       }
1:       if (vsjob != null) {
1:         vsjob.waitForCompletion();
1:         this.vSigmaPath = vSigmaPath.toString();
1:       }
/////////////////////////////////////////////////////////////////////////
1:   }
0:   static enum OutputScalingEnum {
1:     NOSCALING, SIGMA, HALFSIGMA
commit:175701c
/////////////////////////////////////////////////////////////////////////
0: import org.apache.mahout.math.DenseMatrix;
0: import org.apache.mahout.math.DistributedRowMatrixWriter;
0: import org.apache.mahout.math.Matrix;
1: import org.apache.mahout.math.function.Functions;
/////////////////////////////////////////////////////////////////////////
1:   private Vector svalues;
/////////////////////////////////////////////////////////////////////////
1:   private Path pcaMeanPath;
/////////////////////////////////////////////////////////////////////////
1:   public Vector getSingularValues() {
/////////////////////////////////////////////////////////////////////////
1:    * Optional. Single-vector file path for a vector (aka xi in MAHOUT-817
1:    * working notes) to be subtracted from each row of input.
0:    * <P>
0:    * 
1:    * Brute force approach would force would turn input into a dense input, which
1:    * is often not very desirable. By supplying this offset to SSVD solver, we
1:    * can avoid most of that overhead due to increased input density.
0:    * <P>
0:    * 
1:    * The vector size for this offest is n (width of A input). In PCA and R this
1:    * is known as "column means", but in this case it can be any offset of row
1:    * vectors of course to propagate into SSVD solution.
0:    * <P>
0:    * 
1:    */
1:   public Path getPcaMeanPath() {
1:     return pcaMeanPath;
1:   }
1: 
1:   public void setPcaMeanPath(Path pcaMeanPath) {
1:     this.pcaMeanPath = pcaMeanPath;
1:   }
1: 
1:   /**
/////////////////////////////////////////////////////////////////////////
1:         SSVDHelper.sniffInputLabelType(inputPath, conf);
1:       FileSystem fs = FileSystem.get(conf);
/////////////////////////////////////////////////////////////////////////
1:       Path pcaBasePath = new Path(outputPath, "pca");
1:       Path sbPath = null;
0:       Path sqPath = null;
1: 
0:       double xisquaredlen = 0;
1: 
0:       if (pcaMeanPath != null)
1:         fs.mkdirs(pcaBasePath);
1:       Random rnd = RandomUtils.getRandom();
0:       long seed = rnd.nextLong();
1: 
1:       if (pcaMeanPath != null) {
1:         /*
1:          * combute s_b0 if pca offset present.
0:          * 
1:          * Just in case, we treat xi path as a possible reduce or otherwise
1:          * multiple task output that we assume we need to sum up partial
1:          * components. If it is just one file, it will work too.
1:          */
1: 
1:         Vector xi = SSVDHelper.loadAndSumUpVectors(pcaMeanPath, conf);
1:         xisquaredlen = xi.dot(xi);
0:         Omega omega = new Omega(seed, k + p);
1:         Vector s_b0 = omega.mutlithreadedTRightMultiply(xi);
1: 
0:         SSVDHelper.saveVector(s_b0, sbPath =
0:           new Path(pcaBasePath, "somega.seq"), conf);
1:       }
1: 
1:       /*
1:        * if we work with pca offset, we need to precompute s_bq0 aka s_omega for
1:        * jobs to use.
1:        */
1:                sbPath,
/////////////////////////////////////////////////////////////////////////
0:       sbPath = new Path(pcaBasePath, "sb0");
0:       sqPath = new Path(pcaBasePath, "sq0");
1: 
1:                 pcaMeanPath,
/////////////////////////////////////////////////////////////////////////
1:       sbPath = new Path(btPath, BtJob.OUTPUT_SB + "-*");
1:       sqPath = new Path(btPath, BtJob.OUTPUT_SQ + "-*");
1: 
/////////////////////////////////////////////////////////////////////////
1:                            pcaMeanPath,
1:                            sqPath,
1:                            sbPath,
/////////////////////////////////////////////////////////////////////////
1:                   pcaMeanPath,
/////////////////////////////////////////////////////////////////////////
1:         sbPath = new Path(btPath, BtJob.OUTPUT_SB + "-*");
0:         sqPath = new Path(btPath, BtJob.OUTPUT_SQ + "-*");
0:       UpperTriangular bbtTriangular =
0:         SSVDHelper.loadAndSumUpperTriangularMatrices(new Path(btPath,
0:                                                               BtJob.OUTPUT_BBT
0:                                                                   + "-*"), conf);
0:       assert bbtTriangular.columnSize() == k + p;
1:       /*
1:        * we currently use a 3rd party in-core eigensolver. So we need just a
1:        * dense array representation for it.
1:        */
0:       DenseMatrix bbtSquare = new DenseMatrix(k+p,k+p);
0:           double val = bbtTriangular.getQuick(i, j);
0:           bbtSquare.setQuick(i, j, val);
0:           bbtSquare.setQuick(j, i, val);
1:       
1:       // MAHOUT-817
1:       if (pcaMeanPath != null) {
1:         Vector sq = SSVDHelper.loadAndSumUpVectors(sqPath, conf);
1:         Vector sb = SSVDHelper.loadAndSumUpVectors(sbPath, conf);
1:         Matrix mC = sq.cross(sb);
1:         bbtSquare.assign(mC, Functions.MINUS);
1:         bbtSquare.assign(mC.transpose(), Functions.MINUS);
0:         mC = null;
1:         Matrix outerSq = sq.cross(sq);
1:         outerSq.assign(Functions.mult(xisquaredlen));
1:         bbtSquare.assign(outerSq, Functions.PLUS);
0:       EigenSolverWrapper eigenWrapper = new EigenSolverWrapper(SSVDHelper.extractRawData(bbtSquare));
0:       Matrix uHat = new DenseMatrix(eigenWrapper.getUHat());
0:       svalues = new DenseVector(eigenWrapper.getEigenValues());
1: 
1:       svalues.assign(Functions.SQRT);
1: 
1:       DistributedRowMatrixWriter.write(uHatPath =
0:         new Path(uHatPath, "uhat.seq"), conf, uHat);
1:       // save sigma.
1:       SSVDHelper.saveVector(svalues,
1:                             svPath = new Path(svPath, "svalues.seq"),
1:                             conf);
1:         ujob.run(conf,
0:                  new Path(btPath, BtJob.OUTPUT_Q + "-*"),
0:                  uHatPath,
0:                  svPath,
1:                  uPath,
1:                  k,
1:                  reduceTasks,
1:                  labelType,
0:                  cUHalfSigma);
1:         vjob.run(conf,
1:                  new Path(btPath, BtJob.OUTPUT_BT + "-*"),
1:                  pcaMeanPath,
1:                  sqPath,
0:                  uHatPath,
0:                  svPath,
1:                  vPath,
1:                  k,
1:                  reduceTasks,
0:                  cVHalfSigma);
/////////////////////////////////////////////////////////////////////////
commit:8bac914
/////////////////////////////////////////////////////////////////////////
1:   private boolean broadcast = true;
/////////////////////////////////////////////////////////////////////////
1:   public boolean isBroadcast() {
1:     return broadcast;
1:   }
1: 
1:   /**
1:    * If this property is true, use DestributedCache mechanism to broadcast some
1:    * stuff around. May improve efficiency. Default is false.
0:    * 
1:    * @param broadcast
1:    */
1:   public void setBroadcast(boolean broadcast) {
1:     this.broadcast = broadcast;
1:   }
1: 
/////////////////////////////////////////////////////////////////////////
0:     Deque<Closeable> closeables = Lists.<Closeable> newLinkedList();
/////////////////////////////////////////////////////////////////////////
1:       /*
1:        * restrict number of reducers to a reasonable number so we don't have to
1:        * run too many additions in the frontend when reconstructing BBt for the
1:        * last B' and BB' computations. The user may not realize that and gives a
1:        * bit too many (I would be happy i that were ever the case though).
1:        */
/////////////////////////////////////////////////////////////////////////
1:                 q <= 0 ? Math.min(1000, reduceTasks) : reduceTasks,
1:                 broadcast,
/////////////////////////////////////////////////////////////////////////
0:                            reduceTasks,
1:                            broadcast);
/////////////////////////////////////////////////////////////////////////
1:                   i == q - 1 ? Math.min(1000, reduceTasks) : reduceTasks,
1:                   broadcast,
/////////////////////////////////////////////////////////////////////////
0:         firstSeqFile =
0:           fs.listStatus(fstats[0].getPath(), PathFilters.logsCRCFilter())[0];
/////////////////////////////////////////////////////////////////////////
commit:ebeade9
/////////////////////////////////////////////////////////////////////////
/////////////////////////////////////////////////////////////////////////
/////////////////////////////////////////////////////////////////////////
0:     Deque<Closeable> closeables = Lists.<Closeable>newLinkedList();
/////////////////////////////////////////////////////////////////////////
1:                            reduceTasks);
/////////////////////////////////////////////////////////////////////////
0:         firstSeqFile = fs.listStatus(fstats[0].getPath(), PathFilters.logsCRCFilter())[0];
/////////////////////////////////////////////////////////////////////////
1: 
commit:5214b1d
/////////////////////////////////////////////////////////////////////////
0:   private boolean broadcast = true;
/////////////////////////////////////////////////////////////////////////
0:   public boolean isBroadcast() {
0:     return broadcast;
1:   }
1: 
1:   /**
0:    * If this property is true, use DestributedCache mechanism to broadcast some
0:    * stuff around. May improve efficiency. Default is false.
0:    * 
0:    * @param broadcast
1:    */
0:   public void setBroadcast(boolean broadcast) {
0:     this.broadcast = broadcast;
1:   }
1: 
/////////////////////////////////////////////////////////////////////////
0:     Deque<Closeable> closeables = Lists.<Closeable> newLinkedList();
/////////////////////////////////////////////////////////////////////////
0:                            reduceTasks,
0:                            broadcast);
/////////////////////////////////////////////////////////////////////////
0:         firstSeqFile =
0:           fs.listStatus(fstats[0].getPath(), PathFilters.logsCRCFilter())[0];
/////////////////////////////////////////////////////////////////////////
commit:5a2250c
/////////////////////////////////////////////////////////////////////////
/////////////////////////////////////////////////////////////////////////
0: import org.apache.mahout.common.iterator.sequencefile.PathFilters;
0: import com.google.common.collect.Lists;
0: import com.google.common.io.Closeables;
1: 
/////////////////////////////////////////////////////////////////////////
1:   private int outerBlockHeight = 30000;
1:   private int abtBlockHeight = 200000;
/////////////////////////////////////////////////////////////////////////
/////////////////////////////////////////////////////////////////////////
1:   public int getOuterBlockHeight() {
1:     return outerBlockHeight;
1:   }
1: 
1:   /**
1:    * The height of outer blocks during Q'A multiplication. Higher values allow
1:    * to produce less keys for combining and shuffle and sort therefore somewhat
1:    * improving running time; but require larger blocks to be formed in RAM (so
1:    * setting this too high can lead to OOM).
0:    * 
1:    * @param outerBlockHeight
1:    */
1:   public void setOuterBlockHeight(int outerBlockHeight) {
1:     this.outerBlockHeight = outerBlockHeight;
1:   }
1: 
1:   public int getAbtBlockHeight() {
1:     return abtBlockHeight;
1:   }
1: 
1:   /**
1:    * the block height of Y_i during power iterations. It is probably important
1:    * to set it higher than default 200,000 for extremely sparse inputs and when
1:    * more ram is available. y_i block height and ABt job would occupy approx.
1:    * abtBlockHeight x (k+p) x sizeof (double) (as dense).
0:    * 
1:    * @param abtBlockHeight
1:    */
1:   public void setAbtBlockHeight(int abtBlockHeight) {
1:     this.abtBlockHeight = abtBlockHeight;
1:   }
1: 
/////////////////////////////////////////////////////////////////////////
0:     Deque<Closeable> closeables = Lists.<Closeable>newLinkedList();
/////////////////////////////////////////////////////////////////////////
0:                 Math.min(1000, reduceTasks),
/////////////////////////////////////////////////////////////////////////
1:         Path btPathGlob = new Path(btPath, BtJob.OUTPUT_BT + "-*");
1:         ABtDenseOutJob.run(conf,
1:                            inputPath,
1:                            btPathGlob,
1:                            qPath,
1:                            ablockRows,
1:                            minSplitSize,
1:                            k,
1:                            p,
1:                            abtBlockHeight,
0:                            reduceTasks);
/////////////////////////////////////////////////////////////////////////
0:                   Math.min(1000, reduceTasks),
/////////////////////////////////////////////////////////////////////////
/////////////////////////////////////////////////////////////////////////
1: 
0:       FileStatus firstSeqFile;
0:       if (!fstats[0].isDir()) {
0:         firstSeqFile = fstats[0];
0:       } else {
0:         firstSeqFile = fs.listStatus(fstats[0].getPath(), PathFilters.logsCRCFilter())[0];
1:       }
1: 
0:       SequenceFile.Reader r = null;
0:         r = new SequenceFile.Reader(fs, firstSeqFile.getPath(), conf);
/////////////////////////////////////////////////////////////////////////
0:   private static final Pattern OUTPUT_FILE_PATTERN =
0:     Pattern.compile("(\\w+)-(m|r)-(\\d+)(\\.\\w+)?");
/////////////////////////////////////////////////////////////////////////
0:     /*
0:      * assume it is partitioned output, so we need to read them up in order of
0:      * partitions.
1:      */
0:       for (VectorWritable value : new SequenceFileValueIterable<VectorWritable>(fstat.getPath(),
/////////////////////////////////////////////////////////////////////////
0:     /*
0:      * assume it is partitioned output, so we need to read them up in order of
0:      * partitions.
1:      */
0:       for (VectorWritable value : new SequenceFileValueIterable<VectorWritable>(fstat.getPath(),
/////////////////////////////////////////////////////////////////////////
0:     /*
0:      * assume it is partitioned output, so we need to read them up in order of
0:      * partitions.
1:      */
0:       for (VectorWritable value : new SequenceFileValueIterable<VectorWritable>(fstat.getPath(),
commit:7be37c0
/////////////////////////////////////////////////////////////////////////
1:       for (int i = 0; i < q; i++) {
commit:ffc7fab
/////////////////////////////////////////////////////////////////////////
1:  * As of the time of this writing, I don't have benchmarks for this method in
1:  * comparison to other methods. However, non-hadoop differentiating
1:  * characteristics of this method are thought to be :
1:  * <LI>"faster" and precision is traded off in favor of speed. However, there's
1:  * lever in terms of "oversampling parameter" p. Higher values of p produce
1:  * better precision but are trading off speed (and minimum RAM requirement).
1:  * This also means that this method is almost guaranteed to be less precise than
1:  * Lanczos unless full rank SVD decomposition is sought.
1:  * <LI>"more scale" -- can presumably take on larger problems than Lanczos one
0:  * <P>
0:  * <P>
1:  * Specifically in regards to this implementation, <i>I think</i> couple of
1:  * other differentiating points are:
1:  * <LI>no need to specify input matrix height or width in command line, it is
1:  * what it gets to be.
1:  * <LI>supports any Writable as DRM row keys and copies them to correspondent
1:  * rows of U matrix;
1:  * <LI>can request U or V or U<sub>&sigma;</sub>=U* &Sigma;<sup>0.5</sup> or
1:  * V<sub>&sigma;</sub>=V* &Sigma;<sup>0.5</sup> none of which would require pass
1:  * over input A and these jobs are parallel map-only jobs.
0:  * <P>
0:  * <P>
/////////////////////////////////////////////////////////////////////////
0:   private final int outerBlockHeight;
1:   private int q;
/////////////////////////////////////////////////////////////////////////
0:    * 
/////////////////////////////////////////////////////////////////////////
0:                     int outerBlockHeight,
0:     this.outerBlockHeight = outerBlockHeight;
/////////////////////////////////////////////////////////////////////////
1:   public int getQ() {
1:     return q;
1:   }
1: 
1:   /**
1:    * sets q, amount of additional power iterations to increase precision
1:    * (0..2!). Defaults to 0.
0:    * 
1:    * @param q
1:    */
1:   public void setQ(int q) {
1:     this.q = q;
1:   }
1: 
/////////////////////////////////////////////////////////////////////////
1: 
/////////////////////////////////////////////////////////////////////////
1:       Class<? extends Writable> labelType =
0:         sniffInputLabelType(inputPath, conf);
/////////////////////////////////////////////////////////////////////////
1:       QJob.run(conf,
1:                inputPath,
1:                qPath,
1:                ablockRows,
1:                minSplitSize,
1:                k,
1:                p,
0:                seed,
0:                reduceTasks);
0:       // restrict number of reducers to a reasonable number
0:       // so we don't have to run too many additions in the frontend.
1:       BtJob.run(conf,
1:                 inputPath,
1:                 qPath,
1:                 btPath,
1:                 minSplitSize,
1:                 k,
1:                 p,
1:                 outerBlockHeight,
0:                 reduceTasks > 1000 ? 1000 : reduceTasks,
0:                 labelType,
0:                 q > 0 ? false : true);
1:       // power iterations
0:       for (int i = 0; i < q; q--) {
1: 
1:         qPath = new Path(outputPath, String.format("ABt-job-%d", i + 1));
0:         ABtJob.run(conf,
1:                    inputPath,
0:                    new Path(btPath, BtJob.OUTPUT_BT + "-*"),
1:                    qPath,
1:                    ablockRows,
1:                    minSplitSize,
1:                    k,
1:                    p,
1:                    outerBlockHeight,
0:                    reduceTasks);
1: 
1:         btPath = new Path(outputPath, String.format("Bt-job-%d", i + 1));
1: 
1:         BtJob.run(conf,
1:                   inputPath,
1:                   qPath,
1:                   btPath,
1:                   minSplitSize,
1:                   k,
1:                   p,
1:                   outerBlockHeight,
0:                   reduceTasks > 1000 ? 1000 : reduceTasks,
0:                   labelType,
0:                   i == q - 1 ? true : false);
1:       }
1: 
0:       // we don't need BBt now.
0:       // BBtJob.run(conf, new Path(btPath, BtJob.OUTPUT_BT + "-*"), bbtPath, 1);
1: 
0:       UpperTriangular bbt =
0:         loadAndSumUpperTriangularMatrices(fs, new Path(btPath, BtJob.OUTPUT_BBT
0:             + "-*"), conf);
/////////////////////////////////////////////////////////////////////////
0:       SequenceFile.Writer uHatWriter =
0:         SequenceFile.createWriter(fs,
0:                                   conf,
0:                                   uHatPath = new Path(uHatPath, "uhat.seq"),
0:                                   IntWritable.class,
0:                                   VectorWritable.class,
0:                                   CompressionType.BLOCK);
/////////////////////////////////////////////////////////////////////////
0:       SequenceFile.Writer svWriter =
0:         SequenceFile.createWriter(fs,
0:                                   conf,
0:                                   svPath = new Path(svPath, "svalues.seq"),
0:                                   IntWritable.class,
0:                                   VectorWritable.class,
0:                                   CompressionType.BLOCK);
/////////////////////////////////////////////////////////////////////////
0:                    uHatPath,
0:                    svPath,
0:                    uPath,
0:                    k,
0:                    reduceTasks,
0:                    labelType,
0:                    cUHalfSigma);
1:         // actually this is map-only job anyway
/////////////////////////////////////////////////////////////////////////
0:                    uHatPath,
0:                    svPath,
0:                    vPath,
0:                    k,
0:                    reduceTasks,
0:                    cVHalfSigma);
/////////////////////////////////////////////////////////////////////////
0:   private static Class<? extends Writable>
0:       sniffInputLabelType(Path[] inputPath, Configuration conf)
0:         throws IOException {
0:       SequenceFile.Reader r =
0:         new SequenceFile.Reader(fs, fstats[0].getPath(), conf);
/////////////////////////////////////////////////////////////////////////
0:   private static final Pattern OUTPUT_FILE_PATTERN = Pattern
0:     .compile("(\\w+)-(m|r)-(\\d+)(\\.\\w+)?");
0:   static final Comparator<FileStatus> PARTITION_COMPARATOR =
0:     new Comparator<FileStatus>() {
0:       private final Matcher matcher = OUTPUT_FILE_PATTERN.matcher("");
0:       @Override
0:       public int compare(FileStatus o1, FileStatus o2) {
0:         matcher.reset(o1.getPath().getName());
0:         if (!matcher.matches()) {
0:           throw new IllegalArgumentException("Unexpected file name, unable to deduce partition #:"
0:               + o1.getPath());
1:         }
0:         int p1 = Integer.parseInt(matcher.group(3));
0:         matcher.reset(o2.getPath().getName());
0:         if (!matcher.matches()) {
0:           throw new IllegalArgumentException("Unexpected file name, unable to deduce partition #:"
0:               + o2.getPath());
1:         }
1: 
0:         int p2 = Integer.parseInt(matcher.group(3));
0:         return p1 - p2;
0:     };
/////////////////////////////////////////////////////////////////////////
0:   public static double[][] loadDistributedRowMatrix(FileSystem fs,
0:                                                     Path glob,
0:                                                     Configuration conf)
0:     throws IOException {
/////////////////////////////////////////////////////////////////////////
0:       for (VectorWritable value : new SequenceFileValueIterable<VectorWritable>(fstat
0:                                                                                   .getPath(),
0:                                                                                 true,
0:                                                                                 conf)) {
/////////////////////////////////////////////////////////////////////////
1:   /**
0:    * Load multiplel upper triangular matrices and sum them up.
0:    * 
0:    * @param fs
0:    * @param glob
0:    * @param conf
0:    * @return the sum of upper triangular inputs.
0:    * @throws IOException
1:    */
0:   public static UpperTriangular
0:       loadAndSumUpperTriangularMatrices(FileSystem fs,
0:                                         Path glob,
0:                                         Configuration conf) throws IOException {
1: 
0:     FileStatus[] files = fs.globStatus(glob);
0:     if (files == null) {
0:       return null;
1:     }
1: 
0:     // assume it is partitioned output, so we need to read them up
0:     // in order of partitions.
0:     Arrays.sort(files, PARTITION_COMPARATOR);
1: 
0:     DenseVector result = null;
0:     for (FileStatus fstat : files) {
0:       for (VectorWritable value : new SequenceFileValueIterable<VectorWritable>(fstat
0:                                                                                   .getPath(),
0:                                                                                 true,
0:                                                                                 conf)) {
0:         Vector v = value.get();
0:         if (result == null) {
0:           result = new DenseVector(v);
0:         } else {
0:           result.addAll(v);
1:         }
1:       }
1:     }
1: 
0:     if (result == null) {
0:       throw new IOException("Unexpected underrun in upper triangular matrix files");
1:     }
0:     return new UpperTriangular(result);
1:   }
1: 
1:   /**
0:    * Load only one upper triangular matrix and issue error if mroe than one is
0:    * found.
0:    * 
0:    * @param fs
0:    * @param glob
0:    * @param conf
0:    * @return
0:    * @throws IOException
1:    */
0:                                                           Configuration conf)
0:     throws IOException {
/////////////////////////////////////////////////////////////////////////
0:       for (VectorWritable value : new SequenceFileValueIterable<VectorWritable>(fstat
0:                                                                                   .getPath(),
0:                                                                                 true,
0:                                                                                 conf)) {
commit:151de0d
/////////////////////////////////////////////////////////////////////////
1: /**
1:  * Licensed to the Apache Software Foundation (ASF) under one or more
1:  * contributor license agreements.  See the NOTICE file distributed with
1:  * this work for additional information regarding copyright ownership.
1:  * The ASF licenses this file to You under the Apache License, Version 2.0
1:  * (the "License"); you may not use this file except in compliance with
1:  * the License.  You may obtain a copy of the License at
0:  *
1:  *     http://www.apache.org/licenses/LICENSE-2.0
0:  *
1:  * Unless required by applicable law or agreed to in writing, software
1:  * distributed under the License is distributed on an "AS IS" BASIS,
1:  * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
1:  * See the License for the specific language governing permissions and
1:  * limitations under the License.
1:  */
1: 
1: package org.apache.mahout.math.hadoop.stochasticsvd;
1: 
0: import java.io.Closeable;
0: import java.io.IOException;
0: import java.util.ArrayList;
0: import java.util.Arrays;
0: import java.util.Comparator;
0: import java.util.LinkedList;
0: import java.util.List;
0: import java.util.Random;
0: import java.util.regex.Matcher;
0: import java.util.regex.Pattern;
1: 
1: import org.apache.hadoop.conf.Configuration;
0: import org.apache.hadoop.fs.FileStatus;
1: import org.apache.hadoop.fs.FileSystem;
1: import org.apache.hadoop.fs.Path;
0: import org.apache.hadoop.io.IntWritable;
0: import org.apache.hadoop.io.SequenceFile;
0: import org.apache.hadoop.io.SequenceFile.CompressionType;
1: import org.apache.hadoop.io.Writable;
1: import org.apache.mahout.common.IOUtils;
0: import org.apache.mahout.math.DenseVector;
0: import org.apache.mahout.math.Vector;
0: import org.apache.mahout.math.VectorWritable;
0: import org.apache.mahout.math.ssvd.EigenSolverWrapper;
1: 
1: /**
1:  * Stochastic SVD solver (API class).
0:  * <P>
0:  * 
1:  * Implementation details are in my working notes in MAHOUT-376
1:  * (https://issues.apache.org/jira/browse/MAHOUT-376).
0:  * <P>
0:  * 
0:  * As of the time of this writing, I don't have benchmarks for this method 
0:  * in comparison to other methods. However, non-hadoop differentiating 
0:  * characteristics of this method are thought to be : 
0:  * <LI> "faster" and precision is traded off in favor of speed. However, 
0:  * there's lever in terms of "oversampling parameter" p. Higher values of p 
0:  * produce better precision but are trading off speed (and minimum RAM requirement).
0:  * This also means that this method is almost guaranteed to be less precise 
0:  * than Lanczos unless full rank SVD decomposition is sought. 
0:  * <LI> "more scale" -- can presumably take on larger problems than Lanczos one
1:  * (not confirmed by benchmark at this time)
0:  * <P><P>
0:  * 
0:  * Specifically in regards to this implementation, <i>I think</i> 
0:  * couple of other differentiating points are: 
0:  * <LI> no need to specify input matrix height or width in command line, it is what it 
0:  * gets to be. 
0:  * <LI> supports any Writable as DRM row keys and copies them to correspondent rows 
0:  * of U matrix;
0:  * <LI> can request U or V or U<sub>&sigma;</sub>=U* &Sigma;<sup>0.5</sup> or 
0:  * V<sub>&sigma;</sub>=V* &Sigma;<sup>0.5</sup> none of which 
0:  * would require pass over input A and these jobs are parallel map-only jobs.
0:  * <P><P>
0:  * 
1:  * This class is central public API for SSVD solver. The use pattern is as
1:  * follows:
0:  * 
1:  * <UL>
1:  * <LI>create the solver using constructor and supplying computation parameters.
1:  * <LI>set optional parameters thru setter methods.
1:  * <LI>call {@link #run()}.
1:  * <LI> {@link #getUPath()} (if computed) returns the path to the directory
1:  * containing m x k U matrix file(s).
1:  * <LI> {@link #getVPath()} (if computed) returns the path to the directory
1:  * containing n x k V matrix file(s).
0:  * 
1:  * </UL>
0:  * 
0:  * 
0:  * 
1:  */
0: public class SSVDSolver {
1: 
0:   private double[] svalues;
1:   private boolean computeU = true;
1:   private boolean computeV = true;
1:   private String uPath;
1:   private String vPath;
1: 
1:   // configured stuff
0:   private Configuration conf;
0:   private Path[] inputPath;
0:   private Path outputPath;
0:   private int ablockRows;
0:   private int k;
0:   private int p;
0:   private int reduceTasks;
1:   private int minSplitSize = -1;
1:   private boolean cUHalfSigma;
1:   private boolean cVHalfSigma;
1: 
1:   /**
1:    * create new SSVD solver. Required parameters are passed to constructor to
1:    * ensure they are set. Optional parameters can be set using setters .
0:    * <P>
0:    * 
0:    * @param conf
0:    *          hadoop configuration
0:    * @param inputPath
0:    *          Input path (should be compatible with DistributedRowMatrix as of
0:    *          the time of this writing).
0:    * @param outputPath
0:    *          Output path containing U, V and singular values vector files.
0:    * @param ablockRows
0:    *          The vertical hight of a q-block (bigger value require more memory
0:    *          in mappers+ perhaps larger <code>minSplitSize</code> values
0:    * @param k
0:    *          desired rank
0:    * @param p
0:    *          SSVD oversampling parameter
0:    * @param reduceTasks
0:    *          Number of reduce tasks (where applicable)
0:    * @throws IOException
0:    *           when IO condition occurs.
1:    */
1: 
0:   public SSVDSolver(Configuration conf, Path[] inputPath, Path outputPath,
0:       int ablockRows, int k, int p, int reduceTasks) throws IOException {
1:     this.conf = conf;
1:     this.inputPath = inputPath;
1:     this.outputPath = outputPath;
1:     this.ablockRows = ablockRows;
1:     this.k = k;
1:     this.p = p;
1:     this.reduceTasks = reduceTasks;
1:   }
1: 
0:   public void setcUHalfSigma(boolean cUHat) {
0:     this.cUHalfSigma = cUHat;
1:   }
1: 
0:   public void setcVHalfSigma(boolean cVHat) {
0:     this.cVHalfSigma = cVHat;
1:   }
1: 
1:   /**
1:    * The setting controlling whether to compute U matrix of low rank SSVD.
0:    * 
1:    */
1:   public void setComputeU(boolean val) {
1:     computeU = val;
1:   }
1: 
1:   /**
1:    * Setting controlling whether to compute V matrix of low-rank SSVD.
0:    * 
0:    * @param val
0:    *          true if we want to output V matrix. Default is true.
1:    */
1:   public void setComputeV(boolean val) {
1:     computeV = val;
1:   }
1: 
1:   /**
1:    * Sometimes, if requested A blocks become larger than a split, we may need to
1:    * use that to ensure at least k+p rows of A get into a split. This is
1:    * requirement necessary to obtain orthonormalized Q blocks of SSVD.
0:    * 
0:    * @param size
0:    *          the minimum split size to use
1:    */
1:   public void setMinSplitSize(int size) {
1:     minSplitSize = size;
1:   }
1: 
1:   /**
1:    * This contains k+p singular values resulted from the solver run.
0:    * 
1:    * @return singlular values (largest to smallest)
1:    */
0:   public double[] getSingularValues() {
1:     return svalues;
1:   }
1: 
1:   /**
1:    * returns U path (if computation were requested and successful).
0:    * 
1:    * @return U output hdfs path, or null if computation was not completed for
1:    *         whatever reason.
1:    */
1:   public String getUPath() {
1:     return uPath;
1:   }
1: 
1:   /**
1:    * return V path ( if computation was requested and successful ) .
0:    * 
1:    * @return V output hdfs path, or null if computation was not completed for
1:    *         whatever reason.
1:    */
1:   public String getVPath() {
1:     return vPath;
1:   }
1: 
1:   /**
1:    * run all SSVD jobs.
0:    * 
0:    * @throws IOException
0:    *           if I/O condition occurs.
1:    */
1:   public void run() throws IOException {
1: 
0:     LinkedList<Closeable> closeables = new LinkedList<Closeable>();
1:     try {
0:       Class<? extends Writable> labelType = sniffInputLabelType(inputPath,
0:           conf, closeables);
0:       FileSystem fs = FileSystem.get(conf);
1: 
1:       Path qPath = new Path(outputPath, "Q-job");
1:       Path btPath = new Path(outputPath, "Bt-job");
0:       Path bbtPath = new Path(outputPath, "BBt-job");
1:       Path uHatPath = new Path(outputPath, "UHat");
1:       Path svPath = new Path(outputPath, "Sigma");
1:       Path uPath = new Path(outputPath, "U");
1:       Path vPath = new Path(outputPath, "V");
1: 
0:       fs.delete(qPath, true); // or we can't re-run it repeatedly, just in case.
0:       fs.delete(btPath, true);
0:       fs.delete(bbtPath, true);
0:       fs.delete(uHatPath, true);
0:       fs.delete(svPath, true);
0:       fs.delete(uPath, true);
0:       fs.delete(vPath, true);
1: 
0:       Random rnd = new Random();
0:       long seed = rnd.nextLong();
1: 
0:       QJob.run(conf, inputPath, qPath, ablockRows, minSplitSize, k, p, seed,
0:           reduceTasks);
1: 
0:       BtJob.run(conf, inputPath, qPath, btPath, minSplitSize, k, p,
0:           reduceTasks, labelType);
1: 
0:       BBtJob.run(conf, new Path(btPath, BtJob.OUTPUT_BT + "-*"), bbtPath, 1);
1: 
0:       UpperTriangular bbt = loadUpperTriangularMatrix(fs, new Path(bbtPath,
0:           BBtJob.OUTPUT_BBT + "-*"), conf);
1: 
1:       // convert bbt to something our eigensolver could understand
0:       assert bbt.columnSize() == k + p;
1: 
0:       double[][] bbtSquare = new double[k + p][];
0:       for (int i = 0; i < k + p; i++)
0:         bbtSquare[i] = new double[k + p];
1: 
0:       for (int i = 0; i < k + p; i++)
0:         for (int j = i; j < k + p; j++)
0:           bbtSquare[i][j] = bbtSquare[j][i] = bbt.getQuick(i, j);
1: 
0:       svalues = new double[k + p];
1: 
0:       // try something else.
0:       EigenSolverWrapper eigenWrapper = new EigenSolverWrapper(bbtSquare);
1: 
0:       double[] eigenva2 = eigenWrapper.getEigenValues();
0:       for (int i = 0; i < k + p; i++)
0:         svalues[i] = Math.sqrt(eigenva2[i]); // sqrt?
1: 
1:       // save/redistribute UHat
0:       //
0:       double[][] uHat = eigenWrapper.getUHat();
1: 
1:       fs.mkdirs(uHatPath);
0:       SequenceFile.Writer uHatWriter = SequenceFile.createWriter(fs, conf,
0:           uHatPath = new Path(uHatPath, "uhat.seq"), IntWritable.class,
0:           VectorWritable.class, CompressionType.BLOCK);
0:       closeables.addFirst(uHatWriter);
1: 
0:       int m = uHat.length;
0:       IntWritable iw = new IntWritable();
0:       VectorWritable vw = new VectorWritable();
0:       for (int i = 0; i < m; i++) {
0:         vw.set(new DenseVector(uHat[i], true));
0:         iw.set(i);
0:         uHatWriter.append(iw, vw);
1:       }
1: 
0:       closeables.remove(uHatWriter);
0:       uHatWriter.close();
1: 
0:       SequenceFile.Writer svWriter = SequenceFile.createWriter(fs, conf,
0:           svPath = new Path(svPath, "svalues.seq"), IntWritable.class,
0:           VectorWritable.class, CompressionType.BLOCK);
1: 
0:       closeables.addFirst(svWriter);
1: 
0:       vw.set(new DenseVector(svalues, true));
0:       svWriter.append(iw, vw);
1: 
0:       closeables.remove(svWriter);
0:       svWriter.close();
1: 
1:       UJob ujob = null;
1:       VJob vjob = null;
0:       if (computeU)
0:         (ujob = new UJob()).start(conf,
0:             new Path(btPath, BtJob.OUTPUT_Q + "-*"), uHatPath, svPath, uPath,
0:             k, reduceTasks, labelType, cUHalfSigma); // actually this is
0:                                                      // map-only job anyway
1: 
0:       if (computeV)
0:         (vjob = new VJob()).start(conf,
0:             new Path(btPath, BtJob.OUTPUT_BT + "-*"), uHatPath, svPath, vPath,
0:             k, reduceTasks, cVHalfSigma);
1: 
1:       if (ujob != null) {
1:         ujob.waitForCompletion();
1:         this.uPath = uPath.toString();
1:       }
1:       if (vjob != null) {
1:         vjob.waitForCompletion();
1:         this.vPath = vPath.toString();
1:       }
1: 
1:     } catch (InterruptedException exc) {
1:       throw new IOException("Interrupted", exc);
1:     } catch (ClassNotFoundException exc) {
1:       throw new IOException(exc);
1: 
1:     } finally {
1:       IOUtils.close(closeables);
1:     }
1: 
1:   }
1: 
0:   private static Class<? extends Writable> sniffInputLabelType(
0:       Path[] inputPath, Configuration conf, LinkedList<Closeable> closeables)
0:     throws IOException {
0:     FileSystem fs = FileSystem.get(conf);
0:     for (Path p : inputPath) {
0:       FileStatus[] fstats = fs.globStatus(p);
0:       if (fstats == null || fstats.length == 0)
0:         continue;
0:       SequenceFile.Reader r = new SequenceFile.Reader(fs, fstats[0].getPath(),
0:           conf);
0:       closeables.addFirst(r);
1: 
1:       try {
0:         return r.getKeyClass().asSubclass(Writable.class);
1:       } finally {
0:         closeables.remove(r);
0:         r.close();
1:       }
1:     }
1: 
0:     throw new IOException(
0:         "Unable to open input files to determine input label type.");
1:   }
1: 
0:   private static final Pattern OUTPUT_FILE_PATTERN = Pattern
0:       .compile("(\\w+)-(m|r)-(\\d+)(\\.\\w+)?");
1: 
0:   static Comparator<FileStatus> partitionComparator = new Comparator<FileStatus>() {
0:     private Matcher matcher = OUTPUT_FILE_PATTERN.matcher("");
1: 
0:     @Override
0:     public int compare(FileStatus o1, FileStatus o2) {
0:       matcher.reset(o1.getPath().getName());
0:       if (!matcher.matches())
0:         throw new RuntimeException(String.format(
0:             "Unexpected file name, unable to deduce partition #:%s", o1
0:                 .getPath().toString()));
0:       int p1 = Integer.parseInt(matcher.group(3));
0:       matcher.reset(o2.getPath().getName());
0:       if (!matcher.matches())
0:         throw new RuntimeException(String.format(
0:             "Unexpected file name, unable to deduce partition #:%s", o2
0:                 .getPath().toString()));
1: 
0:       int p2 = Integer.parseInt(matcher.group(3));
0:       return p1 - p2;
1:     }
1: 
0:   };
1: 
1:   /**
0:    * helper capabiltiy to load distributed row matrices into dense matrix (to
0:    * support tests mainly).
0:    * 
0:    * @param fs
0:    *          filesystem
0:    * @param glob
0:    *          FS glob
0:    * @param conf
0:    *          configuration
0:    * @return Dense matrix array
0:    * @throws IOException
0:    *           when I/O occurs.
1:    */
0:   public static double[][] loadDistributedRowMatrix(FileSystem fs, Path glob,
0:       Configuration conf) throws IOException {
1: 
0:     FileStatus[] files = fs.globStatus(glob);
0:     if (files == null)
0:       return null;
1: 
0:     List<double[]> denseData = new ArrayList<double[]>();
0:     IntWritable iw = new IntWritable();
0:     VectorWritable vw = new VectorWritable();
1: 
0:     // int m=0;
1: 
0:     // assume it is partitioned output, so we need to read them up
0:     // in order of partitions.
0:     Arrays.sort(files, partitionComparator);
1: 
0:     for (FileStatus fstat : files) {
0:       Path file = fstat.getPath();
0:       SequenceFile.Reader reader = new SequenceFile.Reader(fs, file, conf);
1:       try {
0:         while (reader.next(iw, vw)) {
0:           Vector v = vw.get();
0:           int size;
0:           double[] row = new double[size = v.size()];
0:           for (int i = 0; i < size; i++)
0:             row[i] = v.get(i);
0:           // ignore row label.
0:           // int rowIndex=iw.get();
0:           denseData.add(row);
1: 
1:         }
1:       } finally {
0:         reader.close();
1:       }
1:     }
1: 
0:     return denseData.toArray(new double[denseData.size()][]);
1:   }
1: 
0:   public static UpperTriangular loadUpperTriangularMatrix(FileSystem fs,
0:       Path glob, Configuration conf) throws IOException {
1: 
0:     FileStatus[] files = fs.globStatus(glob);
0:     if (files == null)
0:       return null;
1: 
0:     IntWritable iw = new IntWritable();
0:     VectorWritable vw = new VectorWritable();
0:     UpperTriangular result = null;
1: 
0:     // assume it is partitioned output, so we need to read them up
0:     // in order of partitions.
0:     Arrays.sort(files, partitionComparator);
1: 
0:     for (FileStatus fstat : files) {
0:       Path file = fstat.getPath();
0:       SequenceFile.Reader reader = new SequenceFile.Reader(fs, file, conf);
1:       try {
0:         while (reader.next(iw, vw)) {
0:           Vector v = vw.get();
0:           if (result == null)
0:             result = new UpperTriangular(v);
0:           else
0:             throw new IOException(
0:                 "Unexpected overrun in upper triangular matrix files");
1:         }
1:       } finally {
0:         reader.close();
1:       }
1:     }
1: 
0:     if (result == null)
0:       throw new IOException(
0:           "Unexpected underrun in upper triangular matrix files");
0:     return result;
1:   }
1: 
1: }
author:Sebastian Schelter
-------------------------------------------------------------------------------
commit:4841efb
/////////////////////////////////////////////////////////////////////////
0:       DenseMatrix bbtSquare = new DenseMatrix(k + p, k + p);
commit:74f849b
/////////////////////////////////////////////////////////////////////////
/////////////////////////////////////////////////////////////////////////
0: import com.google.common.collect.Lists;
/////////////////////////////////////////////////////////////////////////
0:     List<double[]> denseData = Lists.newArrayList();
commit:d608a88
/////////////////////////////////////////////////////////////////////////
0: import com.google.common.io.Closeables;
/////////////////////////////////////////////////////////////////////////
0:       Closeables.closeQuietly(uHatWriter);
/////////////////////////////////////////////////////////////////////////
0:       Closeables.closeQuietly(svWriter);
/////////////////////////////////////////////////////////////////////////
0:         Closeables.closeQuietly(r);
author:Sean R. Owen
-------------------------------------------------------------------------------
commit:8b6a26a
/////////////////////////////////////////////////////////////////////////
1: public final class SSVDSolver {
/////////////////////////////////////////////////////////////////////////
1:   enum OutputScalingEnum {
commit:229aeff
/////////////////////////////////////////////////////////////////////////
1:       if (pcaMeanPath != null) {
0:       }
0:       Path sbPath = null;
1:       double xisquaredlen = 0.0;
/////////////////////////////////////////////////////////////////////////
0:       //sbPath = new Path(pcaBasePath, "sb0");
0:       //sqPath = new Path(pcaBasePath, "sq0");
/////////////////////////////////////////////////////////////////////////
1:       Path sqPath = new Path(btPath, BtJob.OUTPUT_SQ + "-*");
/////////////////////////////////////////////////////////////////////////
commit:1de8cec
/////////////////////////////////////////////////////////////////////////
0:       FileSystem fs = FileSystem.get(outputPath.toUri(), conf);
/////////////////////////////////////////////////////////////////////////
0:     FileSystem fs = FileSystem.get(inputPath[0].toUri(), conf);
commit:15925a5
/////////////////////////////////////////////////////////////////////////
commit:564c3e1
/////////////////////////////////////////////////////////////////////////
1:     Deque<Closeable> closeables = Lists.newLinkedList();
/////////////////////////////////////////////////////////////////////////
0:       if (fstats[0].isDir()) {
0:         firstSeqFile = fs.listStatus(fstats[0].getPath(), PathFilters.logsCRCFilter())[0];
0:         firstSeqFile = fstats[0];
commit:1499411
/////////////////////////////////////////////////////////////////////////
1:                 q <= 0);
/////////////////////////////////////////////////////////////////////////
1:                   i == q - 1);
commit:39fe224
/////////////////////////////////////////////////////////////////////////
0:    *
/////////////////////////////////////////////////////////////////////////
0:    *          in mappers+ perhaps larger {@code minSplitSize} values
commit:3218e95
/////////////////////////////////////////////////////////////////////////
0:   static final Comparator<FileStatus> PARTITION_COMPARATOR = new Comparator<FileStatus>() {
/////////////////////////////////////////////////////////////////////////
0:     Arrays.sort(files, PARTITION_COMPARATOR);
/////////////////////////////////////////////////////////////////////////
0:                                                           Path glob,
0:                                                           Configuration conf) throws IOException {
/////////////////////////////////////////////////////////////////////////
0:     Arrays.sort(files, PARTITION_COMPARATOR);
commit:a13b4b7
/////////////////////////////////////////////////////////////////////////
0: import java.util.Deque;
/////////////////////////////////////////////////////////////////////////
1: import org.apache.mahout.common.RandomUtils;
0: import org.apache.mahout.common.iterator.sequencefile.SequenceFileValueIterable;
/////////////////////////////////////////////////////////////////////////
/////////////////////////////////////////////////////////////////////////
1:   private final Configuration conf;
1:   private final Path[] inputPath;
1:   private final Path outputPath;
1:   private final int ablockRows;
1:   private final int k;
1:   private final int p;
1:   private final int reduceTasks;
/////////////////////////////////////////////////////////////////////////
1:   public SSVDSolver(Configuration conf,
1:                     Path[] inputPath,
1:                     Path outputPath,
1:                     int ablockRows,
1:                     int k,
1:                     int p,
1:                     int reduceTasks) {
/////////////////////////////////////////////////////////////////////////
0:     Deque<Closeable> closeables = new LinkedList<Closeable>();
0:       Class<? extends Writable> labelType = sniffInputLabelType(inputPath, conf);
/////////////////////////////////////////////////////////////////////////
0:       Random rnd = RandomUtils.getRandom();
0:       QJob.run(conf, inputPath, qPath, ablockRows, minSplitSize, k, p, seed, reduceTasks);
0:       BtJob.run(conf, inputPath, qPath, btPath, minSplitSize, k, p, reduceTasks, labelType);
0:       UpperTriangular bbt = loadUpperTriangularMatrix(fs, new Path(bbtPath, BBtJob.OUTPUT_BBT + "-*"), conf);
0:       for (int i = 0; i < k + p; i++) {
0:       }
0:       for (int i = 0; i < k + p; i++) {
0:         for (int j = i; j < k + p; j++) {
0:         }
0:       }
/////////////////////////////////////////////////////////////////////////
0:       for (int i = 0; i < k + p; i++) {
0:       }
/////////////////////////////////////////////////////////////////////////
1:       if (computeU) {
1:         ujob = new UJob();
0:         ujob.start(conf,
0:                    new Path(btPath, BtJob.OUTPUT_Q + "-*"),
0:                    uHatPath, svPath, uPath, k, reduceTasks, labelType, cUHalfSigma);
0:                                   // actually this is  map-only job anyway
0:       }
0:       VJob vjob = null;
1:       if (computeV) {
1:         vjob = new VJob();
0:         vjob.start(conf,
0:                    new Path(btPath, BtJob.OUTPUT_BT + "-*"),
0:                    uHatPath, svPath, vPath, k, reduceTasks, cVHalfSigma);
0:       }
/////////////////////////////////////////////////////////////////////////
0:   private static Class<? extends Writable> sniffInputLabelType(Path[] inputPath, Configuration conf)
0:       if (fstats == null || fstats.length == 0) {
0:       }
0:       SequenceFile.Reader r = new SequenceFile.Reader(fs, fstats[0].getPath(), conf);
0:     throw new IOException("Unable to open input files to determine input label type.");
0:   private static final Pattern OUTPUT_FILE_PATTERN = Pattern.compile("(\\w+)-(m|r)-(\\d+)(\\.\\w+)?");
0:   static final Comparator<FileStatus> partitionComparator = new Comparator<FileStatus>() {
0:     private final Matcher matcher = OUTPUT_FILE_PATTERN.matcher("");
0:       if (!matcher.matches()) {
0:         throw new IllegalArgumentException("Unexpected file name, unable to deduce partition #:" + o1.getPath());
0:       }
0:       if (!matcher.matches()) {
0:         throw new IllegalArgumentException("Unexpected file name, unable to deduce partition #:" + o2.getPath());
0:       }
/////////////////////////////////////////////////////////////////////////
0:   public static double[][] loadDistributedRowMatrix(FileSystem fs, Path glob, Configuration conf) throws IOException {
0:     if (files == null) {
0:     }
/////////////////////////////////////////////////////////////////////////
0:       for (VectorWritable value : new SequenceFileValueIterable<VectorWritable>(fstat.getPath(), true, conf)) {
0:         Vector v = value.get();
0:         int size = v.size();
0:         double[] row = new double[size];
0:         for (int i = 0; i < size; i++) {
0:           row[i] = v.get(i);
0:         // ignore row label.
0:         denseData.add(row);
/////////////////////////////////////////////////////////////////////////
0:     if (files == null) {
0:     }
0:     UpperTriangular result = null;
0:       for (VectorWritable value : new SequenceFileValueIterable<VectorWritable>(fstat.getPath(), true, conf)) {
0:         Vector v = value.get();
0:         if (result == null) {
0:           result = new UpperTriangular(v);
0:         } else {
0:           throw new IOException("Unexpected overrun in upper triangular matrix files");
0:     if (result == null) {
0:       throw new IOException("Unexpected underrun in upper triangular matrix files");
0:     }
commit:c4550a1
/////////////////////////////////////////////////////////////////////////
1:   private boolean overwrite;
/////////////////////////////////////////////////////////////////////////
0:   
1:   public boolean isOverwrite() {
1:     return overwrite;
0:   }
0: 
0:   /**
1:    * if true, driver to clean output folder first if exists.
0:    * 
1:    * @param overwrite
0:    */
1:   public void setOverwrite(boolean overwrite) {
1:     this.overwrite = overwrite;
0:   }
/////////////////////////////////////////////////////////////////////////
0:       if (overwrite) {
0:         fs.delete(outputPath, true);
0:       }
/////////////////////////////////////////////////////////////////////////
0: }
============================================================================