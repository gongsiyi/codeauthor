1:9a15cb8: /**
1:9a15cb8:  * Licensed to the Apache Software Foundation (ASF) under one or more
1:9a15cb8:  * contributor license agreements.  See the NOTICE file distributed with
1:9a15cb8:  * this work for additional information regarding copyright ownership.
1:9a15cb8:  * The ASF licenses this file to You under the Apache License, Version 2.0
1:9a15cb8:  * (the "License"); you may not use this file except in compliance with
1:9a15cb8:  * the License.  You may obtain a copy of the License at
1:9a15cb8:  *
1:9a15cb8:  *     http://www.apache.org/licenses/LICENSE-2.0
1:9a15cb8:  *
1:9a15cb8:  * Unless required by applicable law or agreed to in writing, software
1:9a15cb8:  * distributed under the License is distributed on an "AS IS" BASIS,
1:9a15cb8:  * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
1:9a15cb8:  * See the License for the specific language governing permissions and
1:9a15cb8:  * limitations under the License.
1:9a15cb8:  */
16:9a15cb8: 
1:9a15cb8: package org.apache.mahout.text.wikipedia;
1:9a15cb8: 
1:9a15cb8: import java.io.BufferedWriter;
1:9a15cb8: import java.io.File;
1:9a15cb8: import java.io.FileInputStream;
1:9a15cb8: import java.io.IOException;
1:9a15cb8: import java.io.OutputStreamWriter;
1:9a15cb8: import java.net.URI;
1:9a15cb8: import java.text.DecimalFormat;
1:9a15cb8: import java.text.NumberFormat;
1:9a15cb8: 
1:9a15cb8: import org.apache.commons.cli2.CommandLine;
1:9a15cb8: import org.apache.commons.cli2.Group;
1:9a15cb8: import org.apache.commons.cli2.Option;
1:9a15cb8: import org.apache.commons.cli2.OptionException;
1:9a15cb8: import org.apache.commons.cli2.builder.ArgumentBuilder;
1:9a15cb8: import org.apache.commons.cli2.builder.DefaultOptionBuilder;
1:9a15cb8: import org.apache.commons.cli2.builder.GroupBuilder;
1:9a15cb8: import org.apache.commons.cli2.commandline.Parser;
1:9a15cb8: import org.apache.hadoop.conf.Configuration;
1:9a15cb8: import org.apache.hadoop.fs.FileSystem;
1:9a15cb8: import org.apache.hadoop.fs.Path;
1:9a15cb8: import org.apache.hadoop.io.compress.BZip2Codec;
1:9a15cb8: import org.apache.hadoop.io.compress.CompressionCodec;
1:9a15cb8: import org.apache.mahout.common.CommandLineUtil;
1:9a15cb8: import org.apache.mahout.common.iterator.FileLineIterator;
1:9a15cb8: import org.slf4j.Logger;
1:9a15cb8: import org.slf4j.LoggerFactory;
1:9a15cb8: 
1:9a15cb8: /**
1:9a15cb8:  * <p>The Bayes example package provides some helper classes for training the Naive Bayes classifier
1:229aeff:  * on the Twenty Newsgroups data. See {@code PrepareTwentyNewsgroups}
1:9a15cb8:  * for details on running the trainer and
1:9a15cb8:  * formatting the Twenty Newsgroups data properly for the training.</p>
1:9a15cb8:  *
1:9a15cb8:  * <p>The easiest way to prepare the data is to use the ant task in core/build.xml:</p>
1:9a15cb8:  *
1:9a15cb8:  * <p>{@code ant extract-20news-18828}</p>
1:9a15cb8:  *
1:9a15cb8:  * <p>This runs the arg line:</p>
1:9a15cb8:  *
1:9a15cb8:  * <p>{@code -p $\{working.dir\}/20news-18828/ -o $\{working.dir\}/20news-18828-collapse -a $\{analyzer\} -c UTF-8}</p>
1:9a15cb8:  *
1:9a15cb8:  * <p>To Run the Wikipedia examples (assumes you've built the Mahout Job jar):</p>
1:9a15cb8:  *
1:9a15cb8:  * <ol>
1:9a15cb8:  *  <li>Download the Wikipedia Dataset. Use the Ant target: {@code ant enwiki-files}</li>
1:9a15cb8:  *  <li>Chunk the data using the WikipediaXmlSplitter (from the Hadoop home):
1:9a15cb8:  *   {@code bin/hadoop jar $MAHOUT_HOME/target/mahout-examples-0.x
1:9a15cb8:  *   org.apache.mahout.classifier.bayes.WikipediaXmlSplitter
1:9a15cb8:  *   -d $MAHOUT_HOME/examples/temp/enwiki-latest-pages-articles.xml
1:9a15cb8:  *   -o $MAHOUT_HOME/examples/work/wikipedia/chunks/ -c 64}</li>
1:9a15cb8:  * </ol>
1:9a15cb8:  */
1:9a15cb8: public final class WikipediaXmlSplitter {
1:09b4ef1: 
1:9a15cb8:   private static final Logger log = LoggerFactory.getLogger(WikipediaXmlSplitter.class);
1:09b4ef1: 
1:9a15cb8:   private WikipediaXmlSplitter() { }
1:09b4ef1: 
1:9a15cb8:   public static void main(String[] args) throws IOException {
1:9a15cb8:     DefaultOptionBuilder obuilder = new DefaultOptionBuilder();
1:9a15cb8:     ArgumentBuilder abuilder = new ArgumentBuilder();
1:9a15cb8:     GroupBuilder gbuilder = new GroupBuilder();
1:09b4ef1: 
1:9a15cb8:     Option dumpFileOpt = obuilder.withLongName("dumpFile").withRequired(true).withArgument(
1:9a15cb8:       abuilder.withName("dumpFile").withMinimum(1).withMaximum(1).create()).withDescription(
1:9a15cb8:       "The path to the wikipedia dump file (.bz2 or uncompressed)").withShortName("d").create();
1:09b4ef1: 
1:9a15cb8:     Option outputDirOpt = obuilder.withLongName("outputDir").withRequired(true).withArgument(
1:9a15cb8:       abuilder.withName("outputDir").withMinimum(1).withMaximum(1).create()).withDescription(
1:9a15cb8:       "The output directory to place the splits in:\n"
1:9a15cb8:           + "local files:\n\t/var/data/wikipedia-xml-chunks or\n\tfile:///var/data/wikipedia-xml-chunks\n"
1:9a15cb8:           + "Hadoop DFS:\n\thdfs://wikipedia-xml-chunks\n"
1:9a15cb8:           + "AWS S3 (blocks):\n\ts3://bucket-name/wikipedia-xml-chunks\n"
1:9a15cb8:           + "AWS S3 (native files):\n\ts3n://bucket-name/wikipedia-xml-chunks\n")
1:9a15cb8: 
1:9a15cb8:     .withShortName("o").create();
1:09b4ef1: 
1:9a15cb8:     Option s3IdOpt = obuilder.withLongName("s3ID").withRequired(false).withArgument(
1:9a15cb8:       abuilder.withName("s3Id").withMinimum(1).withMaximum(1).create()).withDescription("Amazon S3 ID key")
1:9a15cb8:         .withShortName("i").create();
1:9a15cb8:     Option s3SecretOpt = obuilder.withLongName("s3Secret").withRequired(false).withArgument(
1:9a15cb8:       abuilder.withName("s3Secret").withMinimum(1).withMaximum(1).create()).withDescription(
1:9a15cb8:       "Amazon S3 secret key").withShortName("s").create();
1:09b4ef1: 
1:9a15cb8:     Option chunkSizeOpt = obuilder.withLongName("chunkSize").withRequired(true).withArgument(
1:9a15cb8:       abuilder.withName("chunkSize").withMinimum(1).withMaximum(1).create()).withDescription(
1:9a15cb8:       "The Size of the chunk, in megabytes").withShortName("c").create();
1:9a15cb8:     Option numChunksOpt = obuilder
1:9a15cb8:         .withLongName("numChunks")
1:9a15cb8:         .withRequired(false)
1:9a15cb8:         .withArgument(abuilder.withName("numChunks").withMinimum(1).withMaximum(1).create())
1:9a15cb8:         .withDescription(
1:9a15cb8:           "The maximum number of chunks to create.  If specified, program will only create a subset of the chunks")
1:9a15cb8:         .withShortName("n").create();
1:9a15cb8:     Group group = gbuilder.withName("Options").withOption(dumpFileOpt).withOption(outputDirOpt).withOption(
1:9a15cb8:       chunkSizeOpt).withOption(numChunksOpt).withOption(s3IdOpt).withOption(s3SecretOpt).create();
1:09b4ef1: 
1:9a15cb8:     Parser parser = new Parser();
1:9a15cb8:     parser.setGroup(group);
1:9a15cb8:     CommandLine cmdLine;
2:9a15cb8:     try {
1:9a15cb8:       cmdLine = parser.parse(args);
1:9a15cb8:     } catch (OptionException e) {
1:9a15cb8:       log.error("Error while parsing options", e);
1:9a15cb8:       CommandLineUtil.printHelp(group);
1:9a15cb8:       return;
1:9a15cb8:     }
1:09b4ef1: 
1:9a15cb8:     Configuration conf = new Configuration();
1:9a15cb8:     String dumpFilePath = (String) cmdLine.getValue(dumpFileOpt);
1:9a15cb8:     String outputDirPath = (String) cmdLine.getValue(outputDirOpt);
1:09b4ef1: 
1:9a15cb8:     if (cmdLine.hasOption(s3IdOpt)) {
1:9a15cb8:       String id = (String) cmdLine.getValue(s3IdOpt);
1:9a15cb8:       conf.set("fs.s3n.awsAccessKeyId", id);
1:9a15cb8:       conf.set("fs.s3.awsAccessKeyId", id);
1:9a15cb8:     }
1:9a15cb8:     if (cmdLine.hasOption(s3SecretOpt)) {
1:9a15cb8:       String secret = (String) cmdLine.getValue(s3SecretOpt);
1:9a15cb8:       conf.set("fs.s3n.awsSecretAccessKey", secret);
1:9a15cb8:       conf.set("fs.s3.awsSecretAccessKey", secret);
1:9a15cb8:     }
1:9a15cb8:     // do not compute crc file when using local FS
1:9a15cb8:     conf.set("fs.file.impl", "org.apache.hadoop.fs.RawLocalFileSystem");
1:9a15cb8:     FileSystem fs = FileSystem.get(URI.create(outputDirPath), conf);
1:09b4ef1: 
1:9a15cb8:     int chunkSize = 1024 * 1024 * Integer.parseInt((String) cmdLine.getValue(chunkSizeOpt));
1:09b4ef1: 
1:9a15cb8:     int numChunks = Integer.MAX_VALUE;
1:9a15cb8:     if (cmdLine.hasOption(numChunksOpt)) {
1:9a15cb8:       numChunks = Integer.parseInt((String) cmdLine.getValue(numChunksOpt));
1:9a15cb8:     }
1:09b4ef1: 
1:9a15cb8:     String header = "<mediawiki xmlns=\"http://www.mediawiki.org/xml/export-0.3/\" "
1:9a15cb8:                     + "xmlns:xsi=\"http://www.w3.org/2001/XMLSchema-instance\" "
1:9a15cb8:                     + "xsi:schemaLocation=\"http://www.mediawiki.org/xml/export-0.3/ "
1:9a15cb8:                     + "http://www.mediawiki.org/xml/export-0.3.xsd\" " + "version=\"0.3\" "
1:9a15cb8:                     + "xml:lang=\"en\">\n" + "  <siteinfo>\n" + "<sitename>Wikipedia</sitename>\n"
1:9a15cb8:                     + "    <base>http://en.wikipedia.org/wiki/Main_Page</base>\n"
1:9a15cb8:                     + "    <generator>MediaWiki 1.13alpha</generator>\n" + "    <case>first-letter</case>\n"
1:9a15cb8:                     + "    <namespaces>\n" + "      <namespace key=\"-2\">Media</namespace>\n"
1:9a15cb8:                     + "      <namespace key=\"-1\">Special</namespace>\n" + "      <namespace key=\"0\" />\n"
1:9a15cb8:                     + "      <namespace key=\"1\">Talk</namespace>\n"
1:9a15cb8:                     + "      <namespace key=\"2\">User</namespace>\n"
1:9a15cb8:                     + "      <namespace key=\"3\">User talk</namespace>\n"
1:9a15cb8:                     + "      <namespace key=\"4\">Wikipedia</namespace>\n"
1:9a15cb8:                     + "      <namespace key=\"5\">Wikipedia talk</namespace>\n"
1:9a15cb8:                     + "      <namespace key=\"6\">Image</namespace>\n"
1:9a15cb8:                     + "      <namespace key=\"7\">Image talk</namespace>\n"
1:9a15cb8:                     + "      <namespace key=\"8\">MediaWiki</namespace>\n"
1:9a15cb8:                     + "      <namespace key=\"9\">MediaWiki talk</namespace>\n"
1:9a15cb8:                     + "      <namespace key=\"10\">Template</namespace>\n"
1:9a15cb8:                     + "      <namespace key=\"11\">Template talk</namespace>\n"
1:9a15cb8:                     + "      <namespace key=\"12\">Help</namespace>\n"
1:9a15cb8:                     + "      <namespace key=\"13\">Help talk</namespace>\n"
1:9a15cb8:                     + "      <namespace key=\"14\">Category</namespace>\n"
1:9a15cb8:                     + "      <namespace key=\"15\">Category talk</namespace>\n"
1:9a15cb8:                     + "      <namespace key=\"100\">Portal</namespace>\n"
1:9a15cb8:                     + "      <namespace key=\"101\">Portal talk</namespace>\n" + "    </namespaces>\n"
1:9a15cb8:                     + "  </siteinfo>\n";
1:09b4ef1: 
1:9a15cb8:     StringBuilder content = new StringBuilder();
1:9a15cb8:     content.append(header);
1:9a15cb8:     NumberFormat decimalFormatter = new DecimalFormat("0000");
1:9a15cb8:     File dumpFile = new File(dumpFilePath);
1:09b4ef1: 
1:09b4ef1:     // If the specified path for the input file is incorrect, return immediately
1:09b4ef1:     if (!dumpFile.exists()) {
1:09b4ef1:       log.error("Input file path {} doesn't exist", dumpFilePath);
1:09b4ef1:       return;
1:09b4ef1:     }
1:09b4ef1: 
1:9a15cb8:     FileLineIterator it;
1:9a15cb8:     if (dumpFilePath.endsWith(".bz2")) {
1:9a15cb8:       // default compression format from http://download.wikimedia.org
1:9a15cb8:       CompressionCodec codec = new BZip2Codec();
1:9a15cb8:       it = new FileLineIterator(codec.createInputStream(new FileInputStream(dumpFile)));
1:9a15cb8:     } else {
1:9a15cb8:       // assume the user has previously de-compressed the dump file
1:9a15cb8:       it = new FileLineIterator(dumpFile);
1:9a15cb8:     }
1:09b4ef1:     int fileNumber = 0;
1:9a15cb8:     while (it.hasNext()) {
1:9a15cb8:       String thisLine = it.next();
1:9a15cb8:       if (thisLine.trim().startsWith("<page>")) {
1:9a15cb8:         boolean end = false;
1:9a15cb8:         while (!thisLine.trim().startsWith("</page>")) {
1:9a15cb8:           content.append(thisLine).append('\n');
1:9a15cb8:           if (it.hasNext()) {
1:9a15cb8:             thisLine = it.next();
1:9a15cb8:           } else {
1:9a15cb8:             end = true;
1:9a15cb8:             break;
1:9a15cb8:           }
1:9a15cb8:         }
1:9a15cb8:         content.append(thisLine).append('\n');
1:09b4ef1: 
1:9a15cb8:         if (content.length() > chunkSize || end) {
1:9a15cb8:           content.append("</mediawiki>");
1:09b4ef1:           fileNumber++;
1:09b4ef1:           String filename = outputDirPath + "/chunk-" + decimalFormatter.format(fileNumber) + ".xml";
1:85f9ece:           try (BufferedWriter chunkWriter =
1:85f9ece:                    new BufferedWriter(new OutputStreamWriter(fs.create(new Path(filename)), "UTF-8"))) {
1:9a15cb8:             chunkWriter.write(content.toString(), 0, content.length());
1:9a15cb8:           }
1:09b4ef1:           if (fileNumber >= numChunks) {
1:9a15cb8:             break;
1:9a15cb8:           }
1:9a15cb8:           content = new StringBuilder();
1:9a15cb8:           content.append(header);
1:9a15cb8:         }
1:9a15cb8:       }
1:9a15cb8:     }
1:9a15cb8:   }
1:9a15cb8: }
============================================================================
author:Suneel Marthi
-------------------------------------------------------------------------------
commit:85f9ece
/////////////////////////////////////////////////////////////////////////
/////////////////////////////////////////////////////////////////////////
1:           try (BufferedWriter chunkWriter =
1:                    new BufferedWriter(new OutputStreamWriter(fs.create(new Path(filename)), "UTF-8"))) {
author:smarthi
-------------------------------------------------------------------------------
commit:09b4ef1
/////////////////////////////////////////////////////////////////////////
1: 
1: 
1: 
1: 
1: 
/////////////////////////////////////////////////////////////////////////
1: 
1: 
/////////////////////////////////////////////////////////////////////////
1: 
/////////////////////////////////////////////////////////////////////////
1: 
1: 
/////////////////////////////////////////////////////////////////////////
1: 
1: 
1: 
/////////////////////////////////////////////////////////////////////////
1: 
1: 
1:     // If the specified path for the input file is incorrect, return immediately
1:     if (!dumpFile.exists()) {
1:       log.error("Input file path {} doesn't exist", dumpFilePath);
1:       return;
1:     }
1: 
/////////////////////////////////////////////////////////////////////////
1:     int fileNumber = 0;
/////////////////////////////////////////////////////////////////////////
1: 
1:           fileNumber++;
1:           String filename = outputDirPath + "/chunk-" + decimalFormatter.format(fileNumber) + ".xml";
/////////////////////////////////////////////////////////////////////////
1:           if (fileNumber >= numChunks) {
author:dfilimon
-------------------------------------------------------------------------------
commit:87d4b2e
/////////////////////////////////////////////////////////////////////////
0:             Closeables.close(chunkWriter, false);
author:Grant Ingersoll
-------------------------------------------------------------------------------
commit:31cb292
/////////////////////////////////////////////////////////////////////////
0:             Closeables.close(chunkWriter, true);
author:Sean R. Owen
-------------------------------------------------------------------------------
commit:229aeff
/////////////////////////////////////////////////////////////////////////
1:  * on the Twenty Newsgroups data. See {@code PrepareTwentyNewsgroups}
author:Robin Anil
-------------------------------------------------------------------------------
commit:9a15cb8
/////////////////////////////////////////////////////////////////////////
1: /**
1:  * Licensed to the Apache Software Foundation (ASF) under one or more
1:  * contributor license agreements.  See the NOTICE file distributed with
1:  * this work for additional information regarding copyright ownership.
1:  * The ASF licenses this file to You under the Apache License, Version 2.0
1:  * (the "License"); you may not use this file except in compliance with
1:  * the License.  You may obtain a copy of the License at
1:  *
1:  *     http://www.apache.org/licenses/LICENSE-2.0
1:  *
1:  * Unless required by applicable law or agreed to in writing, software
1:  * distributed under the License is distributed on an "AS IS" BASIS,
1:  * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
1:  * See the License for the specific language governing permissions and
1:  * limitations under the License.
1:  */
1: 
1: package org.apache.mahout.text.wikipedia;
1: 
1: import java.io.BufferedWriter;
1: import java.io.File;
1: import java.io.FileInputStream;
1: import java.io.IOException;
1: import java.io.OutputStreamWriter;
1: import java.net.URI;
1: import java.text.DecimalFormat;
1: import java.text.NumberFormat;
1: 
0: import com.google.common.io.Closeables;
1: import org.apache.commons.cli2.CommandLine;
1: import org.apache.commons.cli2.Group;
1: import org.apache.commons.cli2.Option;
1: import org.apache.commons.cli2.OptionException;
1: import org.apache.commons.cli2.builder.ArgumentBuilder;
1: import org.apache.commons.cli2.builder.DefaultOptionBuilder;
1: import org.apache.commons.cli2.builder.GroupBuilder;
1: import org.apache.commons.cli2.commandline.Parser;
1: import org.apache.hadoop.conf.Configuration;
1: import org.apache.hadoop.fs.FileSystem;
1: import org.apache.hadoop.fs.Path;
1: import org.apache.hadoop.io.compress.BZip2Codec;
1: import org.apache.hadoop.io.compress.CompressionCodec;
1: import org.apache.mahout.common.CommandLineUtil;
1: import org.apache.mahout.common.iterator.FileLineIterator;
1: import org.slf4j.Logger;
1: import org.slf4j.LoggerFactory;
1: 
1: /**
1:  * <p>The Bayes example package provides some helper classes for training the Naive Bayes classifier
0:  * on the Twenty Newsgroups data. See {@link org.apache.mahout.examples.wikipedia.PrepareTwentyNewsgroups}
1:  * for details on running the trainer and
1:  * formatting the Twenty Newsgroups data properly for the training.</p>
1:  *
1:  * <p>The easiest way to prepare the data is to use the ant task in core/build.xml:</p>
1:  *
1:  * <p>{@code ant extract-20news-18828}</p>
1:  *
1:  * <p>This runs the arg line:</p>
1:  *
1:  * <p>{@code -p $\{working.dir\}/20news-18828/ -o $\{working.dir\}/20news-18828-collapse -a $\{analyzer\} -c UTF-8}</p>
1:  *
1:  * <p>To Run the Wikipedia examples (assumes you've built the Mahout Job jar):</p>
1:  *
1:  * <ol>
1:  *  <li>Download the Wikipedia Dataset. Use the Ant target: {@code ant enwiki-files}</li>
1:  *  <li>Chunk the data using the WikipediaXmlSplitter (from the Hadoop home):
1:  *   {@code bin/hadoop jar $MAHOUT_HOME/target/mahout-examples-0.x
1:  *   org.apache.mahout.classifier.bayes.WikipediaXmlSplitter
1:  *   -d $MAHOUT_HOME/examples/temp/enwiki-latest-pages-articles.xml
1:  *   -o $MAHOUT_HOME/examples/work/wikipedia/chunks/ -c 64}</li>
1:  * </ol>
1:  */
1: public final class WikipediaXmlSplitter {
1:   
1:   private static final Logger log = LoggerFactory.getLogger(WikipediaXmlSplitter.class);
1:   
1:   private WikipediaXmlSplitter() { }
1:   
1:   public static void main(String[] args) throws IOException {
1:     DefaultOptionBuilder obuilder = new DefaultOptionBuilder();
1:     ArgumentBuilder abuilder = new ArgumentBuilder();
1:     GroupBuilder gbuilder = new GroupBuilder();
1:     
1:     Option dumpFileOpt = obuilder.withLongName("dumpFile").withRequired(true).withArgument(
1:       abuilder.withName("dumpFile").withMinimum(1).withMaximum(1).create()).withDescription(
1:       "The path to the wikipedia dump file (.bz2 or uncompressed)").withShortName("d").create();
1:     
1:     Option outputDirOpt = obuilder.withLongName("outputDir").withRequired(true).withArgument(
1:       abuilder.withName("outputDir").withMinimum(1).withMaximum(1).create()).withDescription(
1:       "The output directory to place the splits in:\n"
1:           + "local files:\n\t/var/data/wikipedia-xml-chunks or\n\tfile:///var/data/wikipedia-xml-chunks\n"
1:           + "Hadoop DFS:\n\thdfs://wikipedia-xml-chunks\n"
1:           + "AWS S3 (blocks):\n\ts3://bucket-name/wikipedia-xml-chunks\n"
1:           + "AWS S3 (native files):\n\ts3n://bucket-name/wikipedia-xml-chunks\n")
1: 
1:     .withShortName("o").create();
1:     
1:     Option s3IdOpt = obuilder.withLongName("s3ID").withRequired(false).withArgument(
1:       abuilder.withName("s3Id").withMinimum(1).withMaximum(1).create()).withDescription("Amazon S3 ID key")
1:         .withShortName("i").create();
1:     Option s3SecretOpt = obuilder.withLongName("s3Secret").withRequired(false).withArgument(
1:       abuilder.withName("s3Secret").withMinimum(1).withMaximum(1).create()).withDescription(
1:       "Amazon S3 secret key").withShortName("s").create();
1:     
1:     Option chunkSizeOpt = obuilder.withLongName("chunkSize").withRequired(true).withArgument(
1:       abuilder.withName("chunkSize").withMinimum(1).withMaximum(1).create()).withDescription(
1:       "The Size of the chunk, in megabytes").withShortName("c").create();
1:     Option numChunksOpt = obuilder
1:         .withLongName("numChunks")
1:         .withRequired(false)
1:         .withArgument(abuilder.withName("numChunks").withMinimum(1).withMaximum(1).create())
1:         .withDescription(
1:           "The maximum number of chunks to create.  If specified, program will only create a subset of the chunks")
1:         .withShortName("n").create();
1:     Group group = gbuilder.withName("Options").withOption(dumpFileOpt).withOption(outputDirOpt).withOption(
1:       chunkSizeOpt).withOption(numChunksOpt).withOption(s3IdOpt).withOption(s3SecretOpt).create();
1:     
1:     Parser parser = new Parser();
1:     parser.setGroup(group);
1:     CommandLine cmdLine;
1:     try {
1:       cmdLine = parser.parse(args);
1:     } catch (OptionException e) {
1:       log.error("Error while parsing options", e);
1:       CommandLineUtil.printHelp(group);
1:       return;
1:     }
1:     
1:     Configuration conf = new Configuration();
1:     String dumpFilePath = (String) cmdLine.getValue(dumpFileOpt);
1:     String outputDirPath = (String) cmdLine.getValue(outputDirOpt);
1:     
1:     if (cmdLine.hasOption(s3IdOpt)) {
1:       String id = (String) cmdLine.getValue(s3IdOpt);
1:       conf.set("fs.s3n.awsAccessKeyId", id);
1:       conf.set("fs.s3.awsAccessKeyId", id);
1:     }
1:     if (cmdLine.hasOption(s3SecretOpt)) {
1:       String secret = (String) cmdLine.getValue(s3SecretOpt);
1:       conf.set("fs.s3n.awsSecretAccessKey", secret);
1:       conf.set("fs.s3.awsSecretAccessKey", secret);
1:     }
1:     // do not compute crc file when using local FS
1:     conf.set("fs.file.impl", "org.apache.hadoop.fs.RawLocalFileSystem");
1:     FileSystem fs = FileSystem.get(URI.create(outputDirPath), conf);
1:     
1:     int chunkSize = 1024 * 1024 * Integer.parseInt((String) cmdLine.getValue(chunkSizeOpt));
1:     
1:     int numChunks = Integer.MAX_VALUE;
1:     if (cmdLine.hasOption(numChunksOpt)) {
1:       numChunks = Integer.parseInt((String) cmdLine.getValue(numChunksOpt));
1:     }
1:     
1:     String header = "<mediawiki xmlns=\"http://www.mediawiki.org/xml/export-0.3/\" "
1:                     + "xmlns:xsi=\"http://www.w3.org/2001/XMLSchema-instance\" "
1:                     + "xsi:schemaLocation=\"http://www.mediawiki.org/xml/export-0.3/ "
1:                     + "http://www.mediawiki.org/xml/export-0.3.xsd\" " + "version=\"0.3\" "
1:                     + "xml:lang=\"en\">\n" + "  <siteinfo>\n" + "<sitename>Wikipedia</sitename>\n"
1:                     + "    <base>http://en.wikipedia.org/wiki/Main_Page</base>\n"
1:                     + "    <generator>MediaWiki 1.13alpha</generator>\n" + "    <case>first-letter</case>\n"
1:                     + "    <namespaces>\n" + "      <namespace key=\"-2\">Media</namespace>\n"
1:                     + "      <namespace key=\"-1\">Special</namespace>\n" + "      <namespace key=\"0\" />\n"
1:                     + "      <namespace key=\"1\">Talk</namespace>\n"
1:                     + "      <namespace key=\"2\">User</namespace>\n"
1:                     + "      <namespace key=\"3\">User talk</namespace>\n"
1:                     + "      <namespace key=\"4\">Wikipedia</namespace>\n"
1:                     + "      <namespace key=\"5\">Wikipedia talk</namespace>\n"
1:                     + "      <namespace key=\"6\">Image</namespace>\n"
1:                     + "      <namespace key=\"7\">Image talk</namespace>\n"
1:                     + "      <namespace key=\"8\">MediaWiki</namespace>\n"
1:                     + "      <namespace key=\"9\">MediaWiki talk</namespace>\n"
1:                     + "      <namespace key=\"10\">Template</namespace>\n"
1:                     + "      <namespace key=\"11\">Template talk</namespace>\n"
1:                     + "      <namespace key=\"12\">Help</namespace>\n"
1:                     + "      <namespace key=\"13\">Help talk</namespace>\n"
1:                     + "      <namespace key=\"14\">Category</namespace>\n"
1:                     + "      <namespace key=\"15\">Category talk</namespace>\n"
1:                     + "      <namespace key=\"100\">Portal</namespace>\n"
1:                     + "      <namespace key=\"101\">Portal talk</namespace>\n" + "    </namespaces>\n"
1:                     + "  </siteinfo>\n";
1:     
1:     StringBuilder content = new StringBuilder();
1:     content.append(header);
1:     NumberFormat decimalFormatter = new DecimalFormat("0000");
1:     File dumpFile = new File(dumpFilePath);
1:     FileLineIterator it;
1:     if (dumpFilePath.endsWith(".bz2")) {
1:       // default compression format from http://download.wikimedia.org
1:       CompressionCodec codec = new BZip2Codec();
1:       it = new FileLineIterator(codec.createInputStream(new FileInputStream(dumpFile)));
1:     } else {
1:       // assume the user has previously de-compressed the dump file
1:       it = new FileLineIterator(dumpFile);
1:     }
0:     int filenumber = 0;
1:     while (it.hasNext()) {
1:       String thisLine = it.next();
1:       if (thisLine.trim().startsWith("<page>")) {
1:         boolean end = false;
1:         while (!thisLine.trim().startsWith("</page>")) {
1:           content.append(thisLine).append('\n');
1:           if (it.hasNext()) {
1:             thisLine = it.next();
1:           } else {
1:             end = true;
1:             break;
1:           }
1:         }
1:         content.append(thisLine).append('\n');
1:         
1:         if (content.length() > chunkSize || end) {
1:           content.append("</mediawiki>");
0:           filenumber++;
0:           String filename = outputDirPath + "/chunk-" + decimalFormatter.format(filenumber) + ".xml";
0:           BufferedWriter chunkWriter =
0:               new BufferedWriter(new OutputStreamWriter(fs.create(new Path(filename)), "UTF-8"));
1:           try {
1:             chunkWriter.write(content.toString(), 0, content.length());
0:           } finally {
0:             Closeables.closeQuietly(chunkWriter);
1:           }
0:           if (filenumber >= numChunks) {
1:             break;
1:           }
1:           content = new StringBuilder();
1:           content.append(header);
1:         }
1:       }
1:     }
1:   }
1: }
============================================================================