1:48d069f: /**
1:48d069f:  * Licensed to the Apache Software Foundation (ASF) under one or more
1:48d069f:  * contributor license agreements.  See the NOTICE file distributed with
1:48d069f:  * this work for additional information regarding copyright ownership.
1:48d069f:  * The ASF licenses this file to You under the Apache License, Version 2.0
1:48d069f:  * (the "License"); you may not use this file except in compliance with
1:48d069f:  * the License.  You may obtain a copy of the License at
1:4b1c133:  * <p/>
1:4b1c133:  * http://www.apache.org/licenses/LICENSE-2.0
1:4b1c133:  * <p/>
1:48d069f:  * Unless required by applicable law or agreed to in writing, software
1:48d069f:  * distributed under the License is distributed on an "AS IS" BASIS,
1:48d069f:  * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
1:48d069f:  * See the License for the specific language governing permissions and
1:48d069f:  * limitations under the License.
1:48d069f:  */
5:25b5146: 
1:48d069f: package org.apache.mahout.clustering.spectral.kmeans;
1:b35e54f: 
1:25b5146: import java.io.IOException;
1:4b1c133: import java.util.ArrayList;
1:25b5146: import java.util.List;
1:25b5146: import java.util.Map;
1:733f4b6: 
1:48d069f: import org.apache.hadoop.conf.Configuration;
1:b35e54f: import org.apache.hadoop.fs.FileSystem;
1:48d069f: import org.apache.hadoop.fs.Path;
1:b35e54f: import org.apache.hadoop.io.IntWritable;
1:b35e54f: import org.apache.hadoop.io.SequenceFile;
1:b35e54f: import org.apache.hadoop.io.Text;
1:48d069f: import org.apache.hadoop.util.ToolRunner;
1:48d069f: import org.apache.mahout.clustering.Cluster;
1:b35e54f: import org.apache.mahout.clustering.classify.WeightedVectorWritable;
1:48d069f: import org.apache.mahout.clustering.kmeans.KMeansDriver;
1:b60c909: import org.apache.mahout.clustering.spectral.AffinityMatrixInputJob;
1:b60c909: import org.apache.mahout.clustering.spectral.MatrixDiagonalizeJob;
1:b60c909: import org.apache.mahout.clustering.spectral.UnitVectorizerJob;
1:b60c909: import org.apache.mahout.clustering.spectral.VectorMatrixMultiplicationJob;
1:48d069f: import org.apache.mahout.common.AbstractJob;
1:e0ec7c1: import org.apache.mahout.common.ClassUtils;
1:cf76c3f: import org.apache.mahout.common.HadoopUtil;
1:b35e54f: import org.apache.mahout.common.Pair;
1:cf76c3f: import org.apache.mahout.common.commandline.DefaultOptionCreator;
1:48d069f: import org.apache.mahout.common.distance.DistanceMeasure;
1:b35e54f: import org.apache.mahout.common.iterator.sequencefile.SequenceFileIterable;
1:48d069f: import org.apache.mahout.math.Vector;
1:48d069f: import org.apache.mahout.math.hadoop.DistributedRowMatrix;
1:25b5146: import org.apache.mahout.math.hadoop.stochasticsvd.SSVDSolver;
1:b35e54f: import org.slf4j.Logger;
1:b35e54f: import org.slf4j.LoggerFactory;
1:25b5146: 
1:48d069f: /**
1:b35e54f:  * Performs spectral k-means clustering on the top k eigenvectors of the input affinity matrix.
1:48d069f:  */
1:48d069f: public class SpectralKMeansDriver extends AbstractJob {
1:b35e54f:   private static final Logger log = LoggerFactory.getLogger(SpectralKMeansDriver.class);
1:25b5146: 
1:4841efb:   public static final int REDUCERS = 10;
1:4841efb:   public static final int BLOCKHEIGHT = 30000;
1:4841efb:   public static final int OVERSAMPLING = 15;
1:4841efb:   public static final int POWERITERS = 0;
1:25b5146: 
1:4841efb:   public static void main(String[] args) throws Exception {
1:4841efb:     ToolRunner.run(new SpectralKMeansDriver(), args);
1:4841efb:   }
1:4841efb: 
1:4841efb:   @Override
1:4ca6b86:   public int run(String[] arg0) throws Exception {
1:4841efb: 
1:4841efb:     Configuration conf = getConf();
1:4841efb:     addInputOption();
1:4841efb:     addOutputOption();
1:4841efb:     addOption("dimensions", "d", "Square dimensions of affinity matrix", true);
1:4841efb:     addOption("clusters", "k", "Number of clusters and top eigenvectors", true);
1:4841efb:     addOption(DefaultOptionCreator.distanceMeasureOption().create());
1:4841efb:     addOption(DefaultOptionCreator.convergenceOption().create());
1:4841efb:     addOption(DefaultOptionCreator.maxIterationsOption().create());
1:4841efb:     addOption(DefaultOptionCreator.overwriteOption().create());
1:4841efb:     addFlag("usessvd", "ssvd", "Uses SSVD as the eigensolver. Default is the Lanczos solver.");
1:4841efb:     addOption("reduceTasks", "t", "Number of reducers for SSVD", String.valueOf(REDUCERS));
1:4841efb:     addOption("outerProdBlockHeight", "oh", "Block height of outer products for SSVD", String.valueOf(BLOCKHEIGHT));
1:4841efb:     addOption("oversampling", "p", "Oversampling parameter for SSVD", String.valueOf(OVERSAMPLING));
1:4841efb:     addOption("powerIter", "q", "Additional power iterations for SSVD", String.valueOf(POWERITERS));
1:4841efb: 
1:4b1c133:     Map<String, List<String>> parsedArgs = parseArguments(arg0);
1:4841efb:     if (parsedArgs == null) {
1:4841efb:       return 0;
1:4841efb:     }
1:4841efb: 
1:4841efb:     Path input = getInputPath();
1:4841efb:     Path output = getOutputPath();
1:4841efb:     if (hasOption(DefaultOptionCreator.OVERWRITE_OPTION)) {
1:ebec1e3:       HadoopUtil.delete(conf, getTempPath());
1:ebec1e3:       HadoopUtil.delete(conf, getOutputPath());
1:4841efb:     }
1:4841efb:     int numDims = Integer.parseInt(getOption("dimensions"));
1:4841efb:     int clusters = Integer.parseInt(getOption("clusters"));
1:4841efb:     String measureClass = getOption(DefaultOptionCreator.DISTANCE_MEASURE_OPTION);
1:4841efb:     DistanceMeasure measure = ClassUtils.instantiateAs(measureClass, DistanceMeasure.class);
1:4841efb:     double convergenceDelta = Double.parseDouble(getOption(DefaultOptionCreator.CONVERGENCE_DELTA_OPTION));
1:4841efb:     int maxIterations = Integer.parseInt(getOption(DefaultOptionCreator.MAX_ITERATIONS_OPTION));
1:4841efb: 
1:4841efb:     Path tempdir = new Path(getOption("tempDir"));
1:4b1c133:     int reducers = Integer.parseInt(getOption("reduceTasks"));
1:4b1c133:     int blockheight = Integer.parseInt(getOption("outerProdBlockHeight"));
1:4b1c133:     int oversampling = Integer.parseInt(getOption("oversampling"));
1:4b1c133:     int poweriters = Integer.parseInt(getOption("powerIter"));
1:4b1c133:     run(conf, input, output, numDims, clusters, measure, convergenceDelta, maxIterations, tempdir, reducers,
1:4b1c133:         blockheight, oversampling, poweriters);
1:4841efb: 
1:4841efb:     return 0;
1:4841efb:   }
1:4841efb: 
1:b35e54f:   public static void run(Configuration conf, Path input, Path output, int numDims, int clusters,
1:4b1c133:                          DistanceMeasure measure, double convergenceDelta, int maxIterations, Path tempDir)
1:6d16230:       throws IOException, InterruptedException, ClassNotFoundException {
1:4b1c133:     run(conf, input, output, numDims, clusters, measure, convergenceDelta, maxIterations, tempDir, REDUCERS,
1:6d16230:         BLOCKHEIGHT, OVERSAMPLING, POWERITERS);
1:4841efb:   }
1:b35e54f: 
1:cf76c3f:   /**
1:cf76c3f:    * Run the Spectral KMeans clustering on the supplied arguments
1:b35e54f:    *
1:b35e54f:    * @param conf
1:b35e54f:    *          the Configuration to be used
1:b35e54f:    * @param input
1:b35e54f:    *          the Path to the input tuples directory
1:b35e54f:    * @param output
1:b35e54f:    *          the Path to the output directory
1:b35e54f:    * @param numDims
1:b35e54f:    *          the int number of dimensions of the affinity matrix
1:b35e54f:    * @param clusters
1:b35e54f:    *          the int number of eigenvectors and thus clusters to produce
1:b35e54f:    * @param measure
1:b35e54f:    *          the DistanceMeasure for the k-Means calculations
1:b35e54f:    * @param convergenceDelta
1:b35e54f:    *          the double convergence delta for the k-Means calculations
1:b35e54f:    * @param maxIterations
1:b35e54f:    *          the int maximum number of iterations for the k-Means calculations
1:b35e54f:    * @param tempDir
1:b35e54f:    *          Temporary directory for intermediate calculations
1:95cf05f:    * @param numReducers
1:8405928:    *          Number of reducers
1:95cf05f:    * @param blockHeight
1:95cf05f:    * @param oversampling
1:95cf05f:    * @param poweriters
1:cf76c3f:    */
1:b35e54f:   public static void run(Configuration conf, Path input, Path output, int numDims, int clusters,
1:4b1c133:                          DistanceMeasure measure, double convergenceDelta, int maxIterations, Path tempDir,
1:4b1c133:                          int numReducers, int blockHeight, int oversampling, int poweriters)
1:4b1c133:       throws IOException, InterruptedException, ClassNotFoundException {
1:4841efb: 
1:ebec1e3:     HadoopUtil.delete(conf, tempDir);
1:4841efb:     Path outputCalc = new Path(tempDir, "calculations");
1:4841efb:     Path outputTmp = new Path(tempDir, "temporary");
1:4841efb: 
1:4841efb:     // Take in the raw CSV text file and split it ourselves,
1:4841efb:     // creating our own SequenceFiles for the matrices to read later
1:4841efb:     // (similar to the style of syntheticcontrol.canopy.InputMapper)
1:4841efb:     Path affSeqFiles = new Path(outputCalc, "seqfile");
1:4841efb:     AffinityMatrixInputJob.runJob(input, affSeqFiles, numDims, numDims);
1:4841efb: 
1:4841efb:     // Construct the affinity matrix using the newly-created sequence files
1:b35e54f:     DistributedRowMatrix A = new DistributedRowMatrix(affSeqFiles, new Path(outputTmp, "afftmp"), numDims, numDims);
1:4841efb: 
1:4841efb:     Configuration depConf = new Configuration(conf);
1:4841efb:     A.setConf(depConf);
1:4841efb: 
1:4841efb:     // Construct the diagonal matrix D (represented as a vector)
1:4841efb:     Vector D = MatrixDiagonalizeJob.runJob(affSeqFiles, numDims);
1:4841efb: 
1:b35e54f:     // Calculate the normalized Laplacian of the form: L = D^(-0.5)AD^(-0.5)
1:b35e54f:     DistributedRowMatrix L = VectorMatrixMultiplicationJob.runJob(affSeqFiles, D, new Path(outputCalc, "laplacian"),
1:b35e54f:         new Path(outputCalc, outputCalc));
1:4841efb:     L.setConf(depConf);
1:4841efb: 
1:4841efb:     Path data;
1:4841efb: 
1:4b1c133:     // SSVD requires an array of Paths to function. So we pass in an array of length one
1:4b1c133:     Path[] LPath = new Path[1];
1:4b1c133:     LPath[0] = L.getRowPath();
1:4841efb: 
1:4b1c133:     Path SSVDout = new Path(outputCalc, "SSVD");
1:4841efb: 
1:4b1c133:     SSVDSolver solveIt = new SSVDSolver(depConf, LPath, SSVDout, blockHeight, clusters, oversampling, numReducers);
1:4841efb: 
1:4b1c133:     solveIt.setComputeV(false);
1:4b1c133:     solveIt.setComputeU(true);
1:4b1c133:     solveIt.setOverwrite(true);
1:4b1c133:     solveIt.setQ(poweriters);
1:4b1c133:     // solveIt.setBroadcast(false);
1:4b1c133:     solveIt.run();
1:4b1c133:     data = new Path(solveIt.getUPath());
1:4841efb: 
1:4841efb:     // Normalize the rows of Wt to unit length
1:b35e54f:     // normalize is important because it reduces the occurrence of two unique clusters combining into one
1:4841efb:     Path unitVectors = new Path(outputCalc, "unitvectors");
1:4841efb: 
1:4841efb:     UnitVectorizerJob.runJob(data, unitVectors);
1:4841efb: 
1:b35e54f:     DistributedRowMatrix Wt = new DistributedRowMatrix(unitVectors, new Path(unitVectors, "tmp"), clusters, numDims);
1:4841efb:     Wt.setConf(depConf);
1:4841efb:     data = Wt.getRowPath();
1:4841efb: 
1:b35e54f:     // Generate initial clusters using EigenSeedGenerator which picks rows as centroids if that row contains max
1:b35e54f:     // eigen value in that column
1:b35e54f:     Path initialclusters = EigenSeedGenerator.buildFromEigens(conf, data,
1:4841efb:         new Path(output, Cluster.INITIAL_CLUSTERS_DIR), clusters, measure);
1:4841efb: 
1:4841efb:     // Run the KMeansDriver
1:4841efb:     Path answer = new Path(output, "kmeans_out");
1:8405928:     KMeansDriver.run(conf, data, initialclusters, answer, convergenceDelta, maxIterations, true, 0.0, false);
1:4841efb: 
1:b35e54f:     // Restore name to id mapping and read through the cluster assignments
1:b35e54f:     Path mappingPath = new Path(new Path(conf.get("hadoop.tmp.dir")), "generic_input_mapping");
1:4b1c133:     List<String> mapping = new ArrayList<>();
1:b35e54f:     FileSystem fs = FileSystem.get(mappingPath.toUri(), conf);
1:b35e54f:     if (fs.exists(mappingPath)) {
1:b35e54f:       SequenceFile.Reader reader = new SequenceFile.Reader(fs, mappingPath, conf);
1:b35e54f:       Text mappingValue = new Text();
1:b35e54f:       IntWritable mappingIndex = new IntWritable();
1:b35e54f:       while (reader.next(mappingIndex, mappingValue)) {
1:99a5ce8:         String s = mappingValue.toString();
1:b35e54f:         mapping.add(s);
1:6d16230:       }
1:b35e54f:       HadoopUtil.delete(conf, mappingPath);
1:4841efb:     } else {
1:b35e54f:       log.warn("generic input mapping file not found!");
1:4841efb:     }
1:4841efb: 
1:b35e54f:     Path clusteredPointsPath = new Path(answer, "clusteredPoints");
1:b35e54f:     Path inputPath = new Path(clusteredPointsPath, "part-m-00000");
1:b35e54f:     int id = 0;
1:b35e54f:     for (Pair<IntWritable, WeightedVectorWritable> record :
1:4b1c133:         new SequenceFileIterable<IntWritable, WeightedVectorWritable>(inputPath, conf)) {
1:b35e54f:       if (!mapping.isEmpty()) {
1:b35e54f:         log.info("{}: {}", mapping.get(id++), record.getFirst().get());
1:4841efb:       } else {
1:b35e54f:         log.info("{}: {}", id++, record.getFirst().get());
1:4841efb:       }
2:b35e54f:     }
1:b35e54f:   }
1:b35e54f: }
============================================================================
author:pferrel
-------------------------------------------------------------------------------
commit:b988c49
author:Suneel Marthi
-------------------------------------------------------------------------------
commit:4b1c133
/////////////////////////////////////////////////////////////////////////
1:  * <p/>
1:  * http://www.apache.org/licenses/LICENSE-2.0
1:  * <p/>
/////////////////////////////////////////////////////////////////////////
1: import java.util.ArrayList;
/////////////////////////////////////////////////////////////////////////
/////////////////////////////////////////////////////////////////////////
1:     Map<String, List<String>> parsedArgs = parseArguments(arg0);
/////////////////////////////////////////////////////////////////////////
1:     int reducers = Integer.parseInt(getOption("reduceTasks"));
1:     int blockheight = Integer.parseInt(getOption("outerProdBlockHeight"));
1:     int oversampling = Integer.parseInt(getOption("oversampling"));
1:     int poweriters = Integer.parseInt(getOption("powerIter"));
1:     run(conf, input, output, numDims, clusters, measure, convergenceDelta, maxIterations, tempdir, reducers,
1:         blockheight, oversampling, poweriters);
1:                          DistanceMeasure measure, double convergenceDelta, int maxIterations, Path tempDir)
1:     run(conf, input, output, numDims, clusters, measure, convergenceDelta, maxIterations, tempDir, REDUCERS,
/////////////////////////////////////////////////////////////////////////
/////////////////////////////////////////////////////////////////////////
1:                          DistanceMeasure measure, double convergenceDelta, int maxIterations, Path tempDir,
1:                          int numReducers, int blockHeight, int oversampling, int poweriters)
1:       throws IOException, InterruptedException, ClassNotFoundException {
/////////////////////////////////////////////////////////////////////////
1:     // SSVD requires an array of Paths to function. So we pass in an array of length one
1:     Path[] LPath = new Path[1];
1:     LPath[0] = L.getRowPath();
1:     Path SSVDout = new Path(outputCalc, "SSVD");
1:     SSVDSolver solveIt = new SSVDSolver(depConf, LPath, SSVDout, blockHeight, clusters, oversampling, numReducers);
1:     solveIt.setComputeV(false);
1:     solveIt.setComputeU(true);
1:     solveIt.setOverwrite(true);
1:     solveIt.setQ(poweriters);
1:     // solveIt.setBroadcast(false);
1:     solveIt.run();
1:     data = new Path(solveIt.getUPath());
/////////////////////////////////////////////////////////////////////////
1:     List<String> mapping = new ArrayList<>();
/////////////////////////////////////////////////////////////////////////
1:         new SequenceFileIterable<IntWritable, WeightedVectorWritable>(inputPath, conf)) {
author:frankscholten
-------------------------------------------------------------------------------
commit:1a42d85
commit:7120506
/////////////////////////////////////////////////////////////////////////
author:akm
-------------------------------------------------------------------------------
commit:ebec1e3
/////////////////////////////////////////////////////////////////////////
1:       HadoopUtil.delete(conf, getTempPath());
1:       HadoopUtil.delete(conf, getOutputPath());
/////////////////////////////////////////////////////////////////////////
1:     HadoopUtil.delete(conf, tempDir);
author:smarthi
-------------------------------------------------------------------------------
commit:8405928
/////////////////////////////////////////////////////////////////////////
1:    *          Number of reducers
/////////////////////////////////////////////////////////////////////////
1:     KMeansDriver.run(conf, data, initialclusters, answer, convergenceDelta, maxIterations, true, 0.0, false);
commit:99a5ce8
/////////////////////////////////////////////////////////////////////////
1:         String s = mappingValue.toString();
author:Sebastian Schelter
-------------------------------------------------------------------------------
commit:b60c909
/////////////////////////////////////////////////////////////////////////
1: import org.apache.mahout.clustering.spectral.AffinityMatrixInputJob;
1: import org.apache.mahout.clustering.spectral.MatrixDiagonalizeJob;
1: import org.apache.mahout.clustering.spectral.UnitVectorizerJob;
1: import org.apache.mahout.clustering.spectral.VectorMatrixMultiplicationJob;
commit:6d16230
/////////////////////////////////////////////////////////////////////////
0:     throws IOException, ClassNotFoundException, InstantiationException, IllegalAccessException, InterruptedException {
/////////////////////////////////////////////////////////////////////////
0:       int reducers = Integer.parseInt(getOption("reduceTasks"));
0:       int blockheight = Integer.parseInt(getOption("outerProdBlockHeight"));
0:       int oversampling = Integer.parseInt(getOption("oversampling"));
0:       int poweriters = Integer.parseInt(getOption("powerIter"));
0:       run(conf, input, output, numDims, clusters, measure, convergenceDelta, maxIterations, tempdir, true, reducers,
0:           blockheight, oversampling, poweriters);
0:       run(conf, input, output, numDims, clusters, measure, convergenceDelta, maxIterations, tempdir, false);
/////////////////////////////////////////////////////////////////////////
0:     run(conf, input, output, numDims, clusters, measure, convergenceDelta, maxIterations, tempDir, ssvd, REDUCERS,
1:         BLOCKHEIGHT, OVERSAMPLING, POWERITERS);
/////////////////////////////////////////////////////////////////////////
1:     throws IOException, InterruptedException, ClassNotFoundException {
/////////////////////////////////////////////////////////////////////////
0:     KMeansDriver.run(conf, data, initialclusters, answer, measure,convergenceDelta, maxIterations, true, 0.0, false);
1:   }
commit:4841efb
/////////////////////////////////////////////////////////////////////////
0:   public static final double OVERSHOOTMULTIPLIER = 2.0;
1:   public static final int REDUCERS = 10;
1:   public static final int BLOCKHEIGHT = 30000;
1:   public static final int OVERSAMPLING = 15;
1:   public static final int POWERITERS = 0;
1:   public static void main(String[] args) throws Exception {
1:     ToolRunner.run(new SpectralKMeansDriver(), args);
1:   }
1: 
1:   @Override
0:   public int run(String[] arg0)
0:       throws IOException, ClassNotFoundException, InstantiationException, IllegalAccessException, InterruptedException {
1: 
1:     Configuration conf = getConf();
1:     addInputOption();
1:     addOutputOption();
1:     addOption("dimensions", "d", "Square dimensions of affinity matrix", true);
1:     addOption("clusters", "k", "Number of clusters and top eigenvectors", true);
1:     addOption(DefaultOptionCreator.distanceMeasureOption().create());
1:     addOption(DefaultOptionCreator.convergenceOption().create());
1:     addOption(DefaultOptionCreator.maxIterationsOption().create());
1:     addOption(DefaultOptionCreator.overwriteOption().create());
1:     addFlag("usessvd", "ssvd", "Uses SSVD as the eigensolver. Default is the Lanczos solver.");
1:     addOption("reduceTasks", "t", "Number of reducers for SSVD", String.valueOf(REDUCERS));
1:     addOption("outerProdBlockHeight", "oh", "Block height of outer products for SSVD", String.valueOf(BLOCKHEIGHT));
1:     addOption("oversampling", "p", "Oversampling parameter for SSVD", String.valueOf(OVERSAMPLING));
1:     addOption("powerIter", "q", "Additional power iterations for SSVD", String.valueOf(POWERITERS));
1: 
0:     Map<String, List<String>> parsedArgs = parseArguments(arg0);
1:     if (parsedArgs == null) {
1:       return 0;
1:     }
1: 
1:     Path input = getInputPath();
1:     Path output = getOutputPath();
1:     if (hasOption(DefaultOptionCreator.OVERWRITE_OPTION)) {
0:       HadoopUtil.delete(conf, output);
1:     }
1:     int numDims = Integer.parseInt(getOption("dimensions"));
1:     int clusters = Integer.parseInt(getOption("clusters"));
1:     String measureClass = getOption(DefaultOptionCreator.DISTANCE_MEASURE_OPTION);
1:     DistanceMeasure measure = ClassUtils.instantiateAs(measureClass, DistanceMeasure.class);
1:     double convergenceDelta = Double.parseDouble(getOption(DefaultOptionCreator.CONVERGENCE_DELTA_OPTION));
1:     int maxIterations = Integer.parseInt(getOption(DefaultOptionCreator.MAX_ITERATIONS_OPTION));
1: 
1:     Path tempdir = new Path(getOption("tempDir"));
0:     boolean ssvd = parsedArgs.containsKey("--usessvd");
0:     if (ssvd) {
0:         int reducers = Integer.parseInt(getOption("reduceTasks"));
0:         int blockheight = Integer.parseInt(getOption("outerProdBlockHeight"));
0:         int oversampling = Integer.parseInt(getOption("oversampling"));
0:         int poweriters = Integer.parseInt(getOption("powerIter"));
0:         run(conf, input, output, numDims, clusters, measure, convergenceDelta,
0:                 maxIterations, tempdir, true, reducers, blockheight, oversampling, poweriters);
1:     } else {
0:         run(conf, input, output, numDims, clusters, measure, convergenceDelta,
0:                 maxIterations, tempdir, false);
1:     }
1: 
1:     return 0;
1:   }
1: 
0:   public static void run(
0:           Configuration conf,
0:           Path input,
0:           Path output,
0:           int numDims,
0:           int clusters,
0:           DistanceMeasure measure,
0:           double convergenceDelta,
0:           int maxIterations,
0:           Path tempDir,
0:           boolean ssvd) throws IOException, InterruptedException, ClassNotFoundException {
0:       run(conf, input, output, numDims, clusters, measure, convergenceDelta,
0:               maxIterations, tempDir, ssvd, REDUCERS, BLOCKHEIGHT, OVERSAMPLING, POWERITERS);
1:   }
/////////////////////////////////////////////////////////////////////////
0:   public static void run(
0:       Configuration conf,
0:       Path input,
0:       Path output,
0:       int numDims,
0:       int clusters,
0:       DistanceMeasure measure,
0:       double convergenceDelta,
0:       int maxIterations,
0:       Path tempDir,
0:       boolean ssvd,
0:       int numReducers,
0:       int blockHeight,
0:       int oversampling,
0:       int poweriters)
0:           throws IOException, InterruptedException, ClassNotFoundException {
1:     Path outputCalc = new Path(tempDir, "calculations");
1:     Path outputTmp = new Path(tempDir, "temporary");
1: 
1:     // Take in the raw CSV text file and split it ourselves,
1:     // creating our own SequenceFiles for the matrices to read later
1:     // (similar to the style of syntheticcontrol.canopy.InputMapper)
1:     Path affSeqFiles = new Path(outputCalc, "seqfile");
1:     AffinityMatrixInputJob.runJob(input, affSeqFiles, numDims, numDims);
1: 
1:     // Construct the affinity matrix using the newly-created sequence files
0:     DistributedRowMatrix A =
0:         new DistributedRowMatrix(affSeqFiles, new Path(outputTmp, "afftmp"), numDims, numDims);
1: 
1:     Configuration depConf = new Configuration(conf);
1:     A.setConf(depConf);
1: 
1:     // Construct the diagonal matrix D (represented as a vector)
1:     Vector D = MatrixDiagonalizeJob.runJob(affSeqFiles, numDims);
1: 
0:     //Calculate the normalized Laplacian of the form: L = D^(-0.5)AD^(-0.5)
0:     DistributedRowMatrix L = VectorMatrixMultiplicationJob.runJob(affSeqFiles, D,
0:         new Path(outputCalc, "laplacian"), new Path(outputCalc, outputCalc));
1:     L.setConf(depConf);
1: 
1:     Path data;
1: 
0:     if (ssvd) {
0:       // SSVD requires an array of Paths to function. So we pass in an array of length one
0:       Path [] LPath = new Path[1];
0:       LPath[0] = L.getRowPath();
1: 
0:       Path SSVDout = new Path(outputCalc, "SSVD");
1: 
0:       SSVDSolver solveIt = new SSVDSolver(
0:           depConf,
0:           LPath,
0:           SSVDout,
0:           blockHeight,
0:           clusters,
0:           oversampling,
0:           numReducers);
1: 
0:       solveIt.setComputeV(false);
0:       solveIt.setComputeU(true);
0:       solveIt.setOverwrite(true);
0:       solveIt.setQ(poweriters);
0:       //solveIt.setBroadcast(false);
0:       solveIt.run();
0:       data = new Path(solveIt.getUPath());
1:     } else {
0:       // Perform eigen-decomposition using LanczosSolver
0:       // since some of the eigen-output is spurious and will be eliminated
0:       // upon verification, we have to aim to overshoot and then discard
0:       // unnecessary vectors later
0:       int overshoot = Math.min((int) ((double) clusters * OVERSHOOTMULTIPLIER), numDims);
0:       DistributedLanczosSolver solver = new DistributedLanczosSolver();
0:       LanczosState state = new LanczosState(L, overshoot, solver.getInitialVector(L));
0:       Path lanczosSeqFiles = new Path(outputCalc, "eigenvectors");
1: 
0:       solver.runJob(conf,
0:                     state,
0:                     overshoot,
0:                     true,
0:                     lanczosSeqFiles.toString());
1: 
0:       // perform a verification
0:       EigenVerificationJob verifier = new EigenVerificationJob();
0:       Path verifiedEigensPath = new Path(outputCalc, "eigenverifier");
0:       verifier.runJob(conf,
0:               lanczosSeqFiles,
0:               L.getRowPath(),
0:               verifiedEigensPath,
0:               true,
0:               1.0,
0:               clusters);
1: 
0:       Path cleanedEigens = verifier.getCleanedEigensPath();
0:       DistributedRowMatrix W = new DistributedRowMatrix(
0:           cleanedEigens, new Path(cleanedEigens, "tmp"), clusters, numDims);
0:       W.setConf(depConf);
0:       DistributedRowMatrix Wtrans = W.transpose();
0:       data = Wtrans.getRowPath();
1:     }
1: 
1:     // Normalize the rows of Wt to unit length
0:     // normalize is important because it reduces the occurrence of two unique clusters  combining into one
1:     Path unitVectors = new Path(outputCalc, "unitvectors");
1: 
1:     UnitVectorizerJob.runJob(data, unitVectors);
1: 
0:     DistributedRowMatrix Wt = new DistributedRowMatrix(
0:         unitVectors, new Path(unitVectors, "tmp"), clusters, numDims);
1:     Wt.setConf(depConf);
1:     data = Wt.getRowPath();
1: 
0:     // Generate random initial clusters
0:     Path initialclusters = RandomSeedGenerator.buildRandom(conf, data,
1:         new Path(output, Cluster.INITIAL_CLUSTERS_DIR), clusters, measure);
1: 
1:         // Run the KMeansDriver
1:     Path answer = new Path(output, "kmeans_out");
0:     KMeansDriver.run(conf, data, initialclusters, answer,
0:         measure,convergenceDelta, maxIterations, true, 0.0, false);
commit:b90e846
/////////////////////////////////////////////////////////////////////////
0:     LanczosState state = new LanczosState(L, overshoot, DistributedLanczosSolver.getInitialVector(L));
commit:cfd85f8
/////////////////////////////////////////////////////////////////////////
0:     LanczosState state = new LanczosState(L, clusters, solver.getInitialVector(L));
author:Robin Anil
-------------------------------------------------------------------------------
commit:b35e54f
/////////////////////////////////////////////////////////////////////////
1: import org.apache.hadoop.fs.FileSystem;
1: import org.apache.hadoop.io.IntWritable;
1: import org.apache.hadoop.io.SequenceFile;
1: import org.apache.hadoop.io.Text;
1: import org.apache.mahout.clustering.classify.WeightedVectorWritable;
0: import org.apache.mahout.clustering.kmeans.EigenSeedGenerator;
/////////////////////////////////////////////////////////////////////////
1: import org.apache.mahout.common.Pair;
1: import org.apache.mahout.common.iterator.sequencefile.SequenceFileIterable;
1: import org.slf4j.Logger;
1: import org.slf4j.LoggerFactory;
0: import com.google.common.collect.Lists;
1:  * Performs spectral k-means clustering on the top k eigenvectors of the input affinity matrix.
1:   private static final Logger log = LoggerFactory.getLogger(SpectralKMeansDriver.class);
/////////////////////////////////////////////////////////////////////////
0:     Map<String,List<String>> parsedArgs = parseArguments(arg0);
/////////////////////////////////////////////////////////////////////////
1:   public static void run(Configuration conf, Path input, Path output, int numDims, int clusters,
0:       DistanceMeasure measure, double convergenceDelta, int maxIterations, Path tempDir, boolean ssvd)
0:       throws IOException, InterruptedException, ClassNotFoundException {
1:    *
1:    * @param conf
1:    *          the Configuration to be used
1:    * @param input
1:    *          the Path to the input tuples directory
1:    * @param output
1:    *          the Path to the output directory
1:    * @param numDims
1:    *          the int number of dimensions of the affinity matrix
1:    * @param clusters
1:    *          the int number of eigenvectors and thus clusters to produce
1:    * @param measure
1:    *          the DistanceMeasure for the k-Means calculations
1:    * @param convergenceDelta
1:    *          the double convergence delta for the k-Means calculations
1:    * @param maxIterations
1:    *          the int maximum number of iterations for the k-Means calculations
1:    * @param tempDir
1:    *          Temporary directory for intermediate calculations
0:    * @param ssvd
0:    *          Flag to indicate the eigensolver to use
1:   public static void run(Configuration conf, Path input, Path output, int numDims, int clusters,
0:       DistanceMeasure measure, double convergenceDelta, int maxIterations, Path tempDir, boolean ssvd, int numReducers,
0:       int blockHeight, int oversampling, int poweriters) throws IOException, InterruptedException,
0:       ClassNotFoundException {
/////////////////////////////////////////////////////////////////////////
1:     DistributedRowMatrix A = new DistributedRowMatrix(affSeqFiles, new Path(outputTmp, "afftmp"), numDims, numDims);
/////////////////////////////////////////////////////////////////////////
1:     // Calculate the normalized Laplacian of the form: L = D^(-0.5)AD^(-0.5)
1:     DistributedRowMatrix L = VectorMatrixMultiplicationJob.runJob(affSeqFiles, D, new Path(outputCalc, "laplacian"),
1:         new Path(outputCalc, outputCalc));
0:       Path[] LPath = new Path[1];
0:       SSVDSolver solveIt = new SSVDSolver(depConf, LPath, SSVDout, blockHeight, clusters, oversampling, numReducers);
0:       // solveIt.setBroadcast(false);
/////////////////////////////////////////////////////////////////////////
0:       solver.runJob(conf, state, overshoot, true, lanczosSeqFiles.toString());
0:       verifier.runJob(conf, lanczosSeqFiles, L.getRowPath(), verifiedEigensPath, true, 1.0, clusters);
0:       DistributedRowMatrix W = new DistributedRowMatrix(cleanedEigens, new Path(cleanedEigens, "tmp"), clusters,
0:           numDims);
1:     // normalize is important because it reduces the occurrence of two unique clusters combining into one
1:     DistributedRowMatrix Wt = new DistributedRowMatrix(unitVectors, new Path(unitVectors, "tmp"), clusters, numDims);
1:     // Generate initial clusters using EigenSeedGenerator which picks rows as centroids if that row contains max
1:     // eigen value in that column
1:     Path initialclusters = EigenSeedGenerator.buildFromEigens(conf, data,
0:     // Run the KMeansDriver
0:     KMeansDriver.run(conf, data, initialclusters, answer, measure, convergenceDelta, maxIterations, true, 0.0, false);
1: 
1:     // Restore name to id mapping and read through the cluster assignments
1:     Path mappingPath = new Path(new Path(conf.get("hadoop.tmp.dir")), "generic_input_mapping");
0:     List<String> mapping = Lists.newArrayList();
1:     FileSystem fs = FileSystem.get(mappingPath.toUri(), conf);
1:     if (fs.exists(mappingPath)) {
1:       SequenceFile.Reader reader = new SequenceFile.Reader(fs, mappingPath, conf);
1:       Text mappingValue = new Text();
1:       IntWritable mappingIndex = new IntWritable();
1:       while (reader.next(mappingIndex, mappingValue)) {
0:         String s = new String(mappingValue.toString());
1:         mapping.add(s);
1:       }
1:       HadoopUtil.delete(conf, mappingPath);
0:     } else {
1:       log.warn("generic input mapping file not found!");
1:     }
1: 
1:     Path clusteredPointsPath = new Path(answer, "clusteredPoints");
1:     Path inputPath = new Path(clusteredPointsPath, "part-m-00000");
1:     int id = 0;
1:     for (Pair<IntWritable, WeightedVectorWritable> record :
0:          new SequenceFileIterable<IntWritable, WeightedVectorWritable>(inputPath, conf)) {
1:       if (!mapping.isEmpty()) {
1:         log.info("{}: {}", mapping.get(id++), record.getFirst().get());
0:       } else {
1:         log.info("{}: {}", id++, record.getFirst().get());
1:       }
1:     }
author:Sean R. Owen
-------------------------------------------------------------------------------
commit:4ca6b86
/////////////////////////////////////////////////////////////////////////
1:   public int run(String[] arg0) throws Exception {
/////////////////////////////////////////////////////////////////////////
0:       int overshoot = Math.min((int) (clusters * OVERSHOOTMULTIPLIER), numDims);
0:       LanczosState state = new LanczosState(L, overshoot, DistributedLanczosSolver.getInitialVector(L));
commit:229aeff
/////////////////////////////////////////////////////////////////////////
0:   public int run(String[] arg0) throws IOException, ClassNotFoundException, InterruptedException {
/////////////////////////////////////////////////////////////////////////
0:     LanczosState state = new LanczosState(L, clusters, DistributedLanczosSolver.getInitialVector(L));
commit:822a5e1
/////////////////////////////////////////////////////////////////////////
/////////////////////////////////////////////////////////////////////////
/////////////////////////////////////////////////////////////////////////
commit:4fbfbc6
/////////////////////////////////////////////////////////////////////////
0:     LanczosState state = new LanczosState(L, numDims, solver.getInitialVector(L));
/////////////////////////////////////////////////////////////////////////
0:     verifier.runJob(conf, lanczosSeqFiles, L.getRowPath(), verifiedEigensPath, true, 1.0, clusters);
commit:e0ec7c1
/////////////////////////////////////////////////////////////////////////
1: import org.apache.mahout.common.ClassUtils;
/////////////////////////////////////////////////////////////////////////
0:     DistanceMeasure measure = ClassUtils.instantiateAs(measureClass, DistanceMeasure.class);
commit:3218e95
/////////////////////////////////////////////////////////////////////////
0:     KMeansDriver.run(conf,
0:                      Wt.getRowPath(),
0:                      initialclusters,
0:                      output,
0:                      measure,
0:                      convergenceDelta,
0:                      maxIterations,
0:                      true,
0:                      false);
0:     for (Pair<IntWritable,WeightedVectorWritable> record 
0:          : new SequenceFileIterable<IntWritable, WeightedVectorWritable>(inputPath, conf)) {
commit:0681eb1
/////////////////////////////////////////////////////////////////////////
0:     Path initialclusters = RandomSeedGenerator.buildRandom(conf,
0:                                                            Wt.getRowPath(),
commit:a13b4b7
/////////////////////////////////////////////////////////////////////////
/////////////////////////////////////////////////////////////////////////
0: import org.apache.mahout.common.Pair;
0: import org.apache.mahout.common.iterator.sequencefile.SequenceFileIterable;
/////////////////////////////////////////////////////////////////////////
0:       HadoopUtil.delete(conf, output);
/////////////////////////////////////////////////////////////////////////
0:     throws IOException, InterruptedException, ClassNotFoundException {
/////////////////////////////////////////////////////////////////////////
0:     Path inputPath = new Path(clusteredPointsPath, "part-m-00000");
0:     for (Pair<IntWritable,WeightedVectorWritable> record :
0:          new SequenceFileIterable<IntWritable, WeightedVectorWritable>(inputPath, conf)) {
0:       log.info("{}: {}", id++, record.getFirst().get());
commit:04a0324
/////////////////////////////////////////////////////////////////////////
/////////////////////////////////////////////////////////////////////////
0:     Configuration depConf = new Configuration(conf);
0:     A.setConf(depConf);
/////////////////////////////////////////////////////////////////////////
0:     L.setConf(depConf);
/////////////////////////////////////////////////////////////////////////
0:     W.setConf(depConf);
/////////////////////////////////////////////////////////////////////////
0:     Wt.setConf(depConf);
commit:049e7dc
/////////////////////////////////////////////////////////////////////////
0: import org.slf4j.Logger;
0: import org.slf4j.LoggerFactory;
0:   private static final Logger log = LoggerFactory.getLogger(SpectralKMeansDriver.class);
0:   public static void main(String[] args) throws Exception {
0:   public int run(String[] arg0)
0:     throws IOException, ClassNotFoundException, InstantiationException, IllegalAccessException, InterruptedException {
/////////////////////////////////////////////////////////////////////////
/////////////////////////////////////////////////////////////////////////
0:                          int maxIterations)
0:     throws IOException, InterruptedException, ClassNotFoundException, IllegalAccessException, InstantiationException {
/////////////////////////////////////////////////////////////////////////
0:     DistributedRowMatrix L =
0:         VectorMatrixMultiplicationJob.runJob(affSeqFiles, D,
0:             new Path(outputCalc, "laplacian-" + (System.nanoTime() & 0xFF)));
0:     int overshoot = (int) ((double) clusters * OVERSHOOT_MULTIPLIER);
/////////////////////////////////////////////////////////////////////////
0:     //    DistributedRowMatrix Wt = W.transpose();
/////////////////////////////////////////////////////////////////////////
/////////////////////////////////////////////////////////////////////////
0:       log.info("{}: {}", id++, clusterId.get());
author:Shannon Quinn
-------------------------------------------------------------------------------
commit:95cf05f
/////////////////////////////////////////////////////////////////////////
0: 	public static final int POWERITERS = 0;
/////////////////////////////////////////////////////////////////////////
0: 		addOption("reduceTasks", "t", "Number of reducers for SSVD", String.valueOf(REDUCERS));
0: 		addOption("outerProdBlockHeight", "oh", "Block height of outer products for SSVD", String.valueOf(BLOCKHEIGHT));
0: 		addOption("oversampling", "p", "Oversampling parameter for SSVD", String.valueOf(OVERSAMPLING));
0: 		addOption("powerIter", "q", "Additional power iterations for SSVD", String.valueOf(POWERITERS));
/////////////////////////////////////////////////////////////////////////
0: 		    int reducers = Integer.parseInt(getOption("reduceTasks"));
0: 		    int blockheight = Integer.parseInt(getOption("outerProdBlockHeight"));
0: 		    int poweriters = Integer.parseInt(getOption("powerIter"));
0: 		            maxIterations, tempdir, true, reducers, blockheight, oversampling, poweriters);
/////////////////////////////////////////////////////////////////////////
0: 	            maxIterations, tempDir, ssvd, REDUCERS, BLOCKHEIGHT, OVERSAMPLING, POWERITERS);
/////////////////////////////////////////////////////////////////////////
1:    * @param numReducers
1:    * @param blockHeight
1:    * @param oversampling
1:    * @param poweriters
/////////////////////////////////////////////////////////////////////////
0: 		  int oversampling,
0: 		  int poweriters)
/////////////////////////////////////////////////////////////////////////
0: 			solveIt.setQ(poweriters);
commit:733f4b6
/////////////////////////////////////////////////////////////////////////
0: 	public static final int REDUCERS = 10;
0: 	public static final int BLOCKHEIGHT = 30000;
0: 	public static final int OVERSAMPLING = 15;
/////////////////////////////////////////////////////////////////////////
0: 		addOption("ssvdreducers", "r", "Number of reducers for SSVD", String.valueOf(REDUCERS));
0: 		addOption("ssvdblockheight", "h", "Block height for SSVD", String.valueOf(BLOCKHEIGHT));
0: 		addOption("ssvdoversampling", "p", "Oversampling parameter for SSVD", String.valueOf(OVERSAMPLING));
/////////////////////////////////////////////////////////////////////////
0: 		if (ssvd) {
0: 		    int reducers = Integer.parseInt(getOption("ssvdreducers"));
0: 		    int blockheight = Integer.parseInt(getOption("ssvdblockheight"));
0: 		    int oversampling = Integer.parseInt(getOption("oversampling"));
0: 		    run(conf, input, output, numDims, clusters, measure, convergenceDelta,
0: 		            maxIterations, tempdir, true, reducers, blockheight, oversampling);
0: 		} else {
0: 		    run(conf, input, output, numDims, clusters, measure, convergenceDelta,
0: 		            maxIterations, tempdir, false);
0: 		}
1: 	
0: 	public static void run(
0: 	        Configuration conf,
0: 	        Path input,
0: 	        Path output,
0: 	        int numDims,
0: 	        int clusters,
0: 	        DistanceMeasure measure,
0: 	        double convergenceDelta,
0: 	        int maxIterations,
0: 	        Path tempDir,
0: 	        boolean ssvd) throws IOException, InterruptedException, ClassNotFoundException {
0: 	    run(conf, input, output, numDims, clusters, measure, convergenceDelta,
0: 	            maxIterations, tempDir, ssvd, REDUCERS, BLOCKHEIGHT, OVERSAMPLING);
0: 	}
/////////////////////////////////////////////////////////////////////////
0: 		  boolean ssvd,
0: 		  int numReducers,
0: 		  int blockHeight,
0: 		  int oversampling)
/////////////////////////////////////////////////////////////////////////
0: 					blockHeight,
0: 					oversampling, 
0: 					numReducers);
commit:f2dc2fd
/////////////////////////////////////////////////////////////////////////
0:  * Performs spectral k-means clustering on the top k eigenvectors of the input
0:  * affinity matrix. 
commit:25b5146
/////////////////////////////////////////////////////////////////////////
1: import java.io.IOException;
1: import java.util.List;
1: import java.util.Map;
1: 
/////////////////////////////////////////////////////////////////////////
1: import org.apache.mahout.math.hadoop.stochasticsvd.SSVDSolver;
0:  * This implementation is for testing and debugging. 
0:  * 
0:  * Using the variables below the user can:
0:  * 		select to use either SSVDSolver or DistributedLanczosSolver for the Eigen decomposition. 
0:  * 		change the number of iterations in SSVD
0:  * 		choose whether to keep the temp files that are created during a job
0:  * 		have the output printed to a text file 
0:  * 
0:  * All of the steps involved in testing have timers built around them and the result is printed at
0:  * the top of the output text file. 
0:  * 
0:  * See the README file for a description of the algorithm, testing results, and other details.
0: 	public static final double OVERSHOOTMULTIPLIER = 2.0;
0: 	public static void main(String[] args) throws Exception {
0: 		ToolRunner.run(new SpectralKMeansDriver(), args);
0: 	}
1:   
0: 	@Override
0: 	public int run(String[] arg0)
0: 			throws IOException, ClassNotFoundException, InstantiationException, IllegalAccessException, InterruptedException {
1:     
0: 		Configuration conf = getConf();
0: 		addInputOption();
0: 		addOutputOption();
0: 		addOption("dimensions", "d", "Square dimensions of affinity matrix", true);
0: 		addOption("clusters", "k", "Number of clusters and top eigenvectors", true);
0: 		addOption(DefaultOptionCreator.distanceMeasureOption().create());
0: 		addOption(DefaultOptionCreator.convergenceOption().create());
0: 		addOption(DefaultOptionCreator.maxIterationsOption().create());
0: 		addOption(DefaultOptionCreator.overwriteOption().create());
0: 		addFlag("usessvd", "ssvd", "Uses SSVD as the eigensolver. Default is the Lanczos solver.");
1: 		
0: 		Map<String, List<String>> parsedArgs = parseArguments(arg0);
0: 		if (parsedArgs == null) {
0: 		  return 0;
0: 		}
1: 		
0: 		Path input = getInputPath();
0: 		Path output = getOutputPath();
0: 		if (hasOption(DefaultOptionCreator.OVERWRITE_OPTION)) {
0: 			HadoopUtil.delete(conf, output);
0: 		}
0: 		int numDims = Integer.parseInt(getOption("dimensions"));
0: 		int clusters = Integer.parseInt(getOption("clusters"));
0: 		String measureClass = getOption(DefaultOptionCreator.DISTANCE_MEASURE_OPTION);
0: 		DistanceMeasure measure = ClassUtils.instantiateAs(measureClass, DistanceMeasure.class);
0: 		double convergenceDelta = Double.parseDouble(getOption(DefaultOptionCreator.CONVERGENCE_DELTA_OPTION));
0: 		int maxIterations = Integer.parseInt(getOption(DefaultOptionCreator.MAX_ITERATIONS_OPTION));
1: 		
0: 		Path tempdir = new Path(getOption("tempDir"));
0: 		boolean ssvd = parsedArgs.containsKey("--usessvd");
1: 		
0: 		run(conf, input, output, numDims, clusters, measure, convergenceDelta, maxIterations, tempdir, ssvd);
1: 		
0: 		return 0;
0: 	}
/////////////////////////////////////////////////////////////////////////
0:    * @param tempDir Temporary directory for intermediate calculations
0:    * @param ssvd Flag to indicate the eigensolver to use
0: 	public static void run(
0: 		  Configuration conf,
0: 		  Path input,
0: 		  Path output,
0: 		  int numDims,
0: 		  int clusters,
0: 		  DistanceMeasure measure,
0: 		  double convergenceDelta,
0: 		  int maxIterations,
0: 		  Path tempDir,
0: 		  boolean ssvd)
0: 				  throws IOException, InterruptedException, ClassNotFoundException {
0: 		Path outputCalc = new Path(tempDir, "calculations");
0: 		Path outputTmp = new Path(tempDir, "temporary");
0: 
0: 		// Take in the raw CSV text file and split it ourselves,
0: 		// creating our own SequenceFiles for the matrices to read later 
0: 		// (similar to the style of syntheticcontrol.canopy.InputMapper)
0: 		Path affSeqFiles = new Path(outputCalc, "seqfile");
0: 		AffinityMatrixInputJob.runJob(input, affSeqFiles, numDims, numDims);
0: 		
0: 		// Construct the affinity matrix using the newly-created sequence files
0: 		DistributedRowMatrix A = 
0: 				new DistributedRowMatrix(affSeqFiles, new Path(outputTmp, "afftmp"), numDims, numDims); 
0: 		
0: 		Configuration depConf = new Configuration(conf);
0: 		A.setConf(depConf);
0: 		
0: 		// Construct the diagonal matrix D (represented as a vector)
0: 		Vector D = MatrixDiagonalizeJob.runJob(affSeqFiles, numDims);
0: 		
0: 		//Calculate the normalized Laplacian of the form: L = D^(-0.5)AD^(-0.5)
0: 		DistributedRowMatrix L = VectorMatrixMultiplicationJob.runJob(affSeqFiles, D,
0: 				new Path(outputCalc, "laplacian"), new Path(outputCalc, outputCalc));
0: 		L.setConf(depConf);
0: 		
0: 		Path data;
0: 		
0: 		if (ssvd) {
0: 			// SSVD requires an array of Paths to function. So we pass in an array of length one
0: 			Path [] LPath = new Path[1];
0: 			LPath[0] = L.getRowPath();
0: 			
0: 			Path SSVDout = new Path(outputCalc, "SSVD");
0: 			
0: 			SSVDSolver solveIt = new SSVDSolver(
0: 					depConf, 
0: 					LPath, 
0: 					SSVDout, 
0: 					1000, // Vertical height of a q-block
0: 					clusters, 
0: 					15, // Oversampling 
0: 					10);
0: 			
0: 			solveIt.setComputeV(false); 
0: 			solveIt.setComputeU(true);
0: 			solveIt.setOverwrite(true);
0: 			solveIt.setQ(0);
0: 			
0: 			// May want to update SSVD documentation on this one: method doc
0: 			// says "false" is the default, yet it's set to true in the 
0: 			// variable definition.
0: 			//solveIt.setBroadcast(false);
0: 			solveIt.run();
0: 			data = new Path(solveIt.getUPath());
0: 		} else {
0: 			// Perform eigen-decomposition using LanczosSolver
0: 			// since some of the eigen-output is spurious and will be eliminated
0: 			// upon verification, we have to aim to overshoot and then discard
0: 			// unnecessary vectors later
0: 			int overshoot = Math.min((int) ((double) clusters * OVERSHOOTMULTIPLIER), numDims);
0: 			DistributedLanczosSolver solver = new DistributedLanczosSolver();
0: 			LanczosState state = new LanczosState(L, overshoot, solver.getInitialVector(L));
0: 			Path lanczosSeqFiles = new Path(outputCalc, "eigenvectors");
0: 			
0: 			solver.runJob(conf,
0: 			              state,
0: 			              overshoot,
0: 			              true,
0: 			              lanczosSeqFiles.toString());
0: 			
0: 			// perform a verification
0: 			EigenVerificationJob verifier = new EigenVerificationJob();
0: 			Path verifiedEigensPath = new Path(outputCalc, "eigenverifier");
0: 			verifier.runJob(conf, 
0: 							lanczosSeqFiles, 
0: 							L.getRowPath(), 
0: 							verifiedEigensPath, 
0: 							true, 
0: 							1.0, 
0: 							clusters);
0: 			
0: 			Path cleanedEigens = verifier.getCleanedEigensPath();
0: 			DistributedRowMatrix W = new DistributedRowMatrix(
0: 					cleanedEigens, new Path(cleanedEigens, "tmp"), clusters, numDims);
0: 			W.setConf(depConf);
0: 			DistributedRowMatrix Wtrans = W.transpose();
0: 			data = Wtrans.getRowPath();
0: 		}
0: 		
0: 		// Normalize the rows of Wt to unit length
0: 		// normalize is important because it reduces the occurrence of two unique clusters  combining into one 
0: 		Path unitVectors = new Path(outputCalc, "unitvectors");
0: 		
0: 		UnitVectorizerJob.runJob(data, unitVectors);
0: 		
0: 		DistributedRowMatrix Wt = new DistributedRowMatrix(
0: 				unitVectors, new Path(unitVectors, "tmp"), clusters, numDims);
0: 		Wt.setConf(depConf);
0: 		data = Wt.getRowPath();
0: 		
0: 		// Generate random initial clusters
0: 		Path initialclusters = RandomSeedGenerator.buildRandom(conf, data,
0: 				new Path(output, Cluster.INITIAL_CLUSTERS_DIR), clusters, measure);
0: 		   
0: 		    // Run the KMeansDriver
0: 		Path answer = new Path(output, "kmeans_out");
0: 		KMeansDriver.run(conf, data, initialclusters, answer,
0: 				measure,convergenceDelta, maxIterations, true, 0.0, false);
0:     }
author:pranjan
-------------------------------------------------------------------------------
commit:2cfaf19
/////////////////////////////////////////////////////////////////////////
0:                      0.0, 
author:Grant Ingersoll
-------------------------------------------------------------------------------
commit:6db7f62
/////////////////////////////////////////////////////////////////////////
0: import java.util.List;
/////////////////////////////////////////////////////////////////////////
0:     Map<String, List<String>> parsedArgs = parseArguments(arg0);
/////////////////////////////////////////////////////////////////////////
0:     int numDims = Integer.parseInt(getOption("dimensions"));
0:     int clusters = Integer.parseInt(getOption("clusters"));
author:Jeff Eastman
-------------------------------------------------------------------------------
commit:78545ff
/////////////////////////////////////////////////////////////////////////
0:             new Path(outputCalc, "laplacian-" + (System.nanoTime() & 0xFF)), new Path(outputCalc, "laplacian-tmp-" + (System.nanoTime() & 0xFF)));
/////////////////////////////////////////////////////////////////////////
0:     
0:     // The output format is the same as the K-means output format.
0:     // TODO: Perhaps a conversion of the output format from points and clusters
0:     // in eigenspace to the original dataset. Currently, the user has to perform
0:     // the association step after this job finishes on their own.
/////////////////////////////////////////////////////////////////////////
commit:903575e
/////////////////////////////////////////////////////////////////////////
0:     L.configure(depConf);
/////////////////////////////////////////////////////////////////////////
0:     verifier.runJob(conf, lanczosSeqFiles, L.getRowPath(), verifiedEigensPath, true, 1.0, 0.0, clusters);
0:     W.configure(depConf);
/////////////////////////////////////////////////////////////////////////
0:     Wt.configure(depConf);
commit:cf76c3f
/////////////////////////////////////////////////////////////////////////
0: import java.io.IOException;
/////////////////////////////////////////////////////////////////////////
1: import org.apache.mahout.common.HadoopUtil;
1: import org.apache.mahout.common.commandline.DefaultOptionCreator;
0: import org.apache.mahout.common.distance.SquaredEuclideanDistanceMeasure;
/////////////////////////////////////////////////////////////////////////
0:     Configuration conf = getConf();
0:     addInputOption();
0:     addOutputOption();
0:     addOption(DefaultOptionCreator.distanceMeasureOption().create());
0:     addOption(DefaultOptionCreator.convergenceOption().create());
0:     addOption(DefaultOptionCreator.maxIterationsOption().create());
0:     addOption(DefaultOptionCreator.overwriteOption().create());
0:     Path input = getInputPath();
0:     Path output = getOutputPath();
0:     if (hasOption(DefaultOptionCreator.OVERWRITE_OPTION)) {
0:       HadoopUtil.overwriteOutput(output);
0:     }
0:     String measureClass = getOption(DefaultOptionCreator.DISTANCE_MEASURE_OPTION);
0:     ClassLoader ccl = Thread.currentThread().getContextClassLoader();
0:     DistanceMeasure measure = ccl.loadClass(measureClass).asSubclass(DistanceMeasure.class).newInstance();
0:     double convergenceDelta = Double.parseDouble(getOption(DefaultOptionCreator.CONVERGENCE_DELTA_OPTION));
0:     int maxIterations = Integer.parseInt(getOption(DefaultOptionCreator.MAX_ITERATIONS_OPTION));
0:     run(conf, input, output, numDims, clusters, measure, convergenceDelta, maxIterations);
0: 
0:     return 0;
0:   }
0: 
1:   /**
1:    * Run the Spectral KMeans clustering on the supplied arguments
0:    * 
0:    * @param conf the Configuration to be used
0:    * @param input the Path to the input tuples directory
0:    * @param output the Path to the output directory
0:    * @param numDims the int number of dimensions of the affinity matrix
0:    * @param clusters the int number of eigenvectors and thus clusters to produce
0:    * @param measure the DistanceMeasure for the k-Means calculations
0:    * @param convergenceDelta the double convergence delta for the k-Means calculations
0:    * @param maxIterations the int maximum number of iterations for the k-Means calculations
0:    * 
0:    * @throws IOException
0:    * @throws InterruptedException
0:    * @throws ClassNotFoundException
0:    * @throws IllegalAccessException
0:    * @throws InstantiationException
1:    */
0:   public static void run(Configuration conf,
0:                          Path input,
0:                          Path output,
0:                          int numDims,
0:                          int clusters,
0:                          DistanceMeasure measure,
0:                          double convergenceDelta,
0:                          int maxIterations) throws IOException, InterruptedException, ClassNotFoundException,
0:       IllegalAccessException, InstantiationException {
/////////////////////////////////////////////////////////////////////////
0:     KMeansDriver.run(conf, Wt.getRowPath(), initialclusters, output, measure, convergenceDelta, maxIterations, true, false);
/////////////////////////////////////////////////////////////////////////
commit:48d069f
/////////////////////////////////////////////////////////////////////////
1: /**
1:  * Licensed to the Apache Software Foundation (ASF) under one or more
1:  * contributor license agreements.  See the NOTICE file distributed with
1:  * this work for additional information regarding copyright ownership.
1:  * The ASF licenses this file to You under the Apache License, Version 2.0
1:  * (the "License"); you may not use this file except in compliance with
1:  * the License.  You may obtain a copy of the License at
0:  *
0:  *     http://www.apache.org/licenses/LICENSE-2.0
0:  *
1:  * Unless required by applicable law or agreed to in writing, software
1:  * distributed under the License is distributed on an "AS IS" BASIS,
1:  * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
1:  * See the License for the specific language governing permissions and
1:  * limitations under the License.
1:  */
0: 
1: package org.apache.mahout.clustering.spectral.kmeans;
0: 
0: import java.util.ArrayList;
0: import java.util.List;
0: import java.util.Map;
0: 
1: import org.apache.hadoop.conf.Configuration;
0: import org.apache.hadoop.fs.FileSystem;
1: import org.apache.hadoop.fs.Path;
0: import org.apache.hadoop.io.IntWritable;
0: import org.apache.hadoop.io.SequenceFile;
0: import org.apache.hadoop.mapred.JobConf;
1: import org.apache.hadoop.util.ToolRunner;
1: import org.apache.mahout.clustering.Cluster;
0: import org.apache.mahout.clustering.WeightedVectorWritable;
1: import org.apache.mahout.clustering.kmeans.KMeansDriver;
0: import org.apache.mahout.clustering.kmeans.RandomSeedGenerator;
0: import org.apache.mahout.clustering.spectral.common.AffinityMatrixInputJob;
0: import org.apache.mahout.clustering.spectral.common.MatrixDiagonalizeJob;
0: import org.apache.mahout.clustering.spectral.common.UnitVectorizerJob;
0: import org.apache.mahout.clustering.spectral.common.VectorMatrixMultiplicationJob;
1: import org.apache.mahout.common.AbstractJob;
1: import org.apache.mahout.common.distance.DistanceMeasure;
0: import org.apache.mahout.common.distance.EuclideanDistanceMeasure;
0: import org.apache.mahout.math.DenseMatrix;
0: import org.apache.mahout.math.Matrix;
1: import org.apache.mahout.math.Vector;
1: import org.apache.mahout.math.hadoop.DistributedRowMatrix;
0: import org.apache.mahout.math.hadoop.decomposer.DistributedLanczosSolver;
0: import org.apache.mahout.math.hadoop.decomposer.EigenVerificationJob;
0: 
1: /**
0:  * Implementation of the EigenCuts spectral clustering algorithm.
1:  */
1: public class SpectralKMeansDriver extends AbstractJob {
0: 
0:   public static final boolean DEBUG = false;
0: 
0:   public static final double OVERSHOOT_MULTIPLIER = 2.0;
0: 
0:   public static void main(String args[]) throws Exception {
0:     ToolRunner.run(new SpectralKMeansDriver(), args);
0:   }
0: 
0:   @Override
0:   public int run(String[] arg0) throws Exception {
0:     // set up command line options
0:     Configuration conf = new Configuration();
0:     addOption("input", "i", "Path to input affinity matrix data", true);
0:     addOption("output", "o", "Output of clusterings", true);
0:     addOption("dimensions", "d", "Square dimensions of affinity matrix", true);
0:     addOption("clusters", "k", "Number of clusters and top eigenvectors", true);
0:     Map<String, String> parsedArgs = parseArguments(arg0);
0:     if (parsedArgs == null) {
0:       return 0;
0:     }
0: 
0:     // TODO: Need to be able to read all k-means parameters, though
0:     // they will be optional parameters to the algorithm
0:     // read the values of the command line
0:     Path input = new Path(parsedArgs.get("--input"));
0:     Path output = new Path(parsedArgs.get("--output"));
0:     int numDims = Integer.parseInt(parsedArgs.get("--dimensions"));
0:     int clusters = Integer.parseInt(parsedArgs.get("--clusters"));
0: 
0:     // create a few new Paths for temp files and transformations
0:     Path outputCalc = new Path(output, "calculations");
0:     Path outputTmp = new Path(output, "temporary");
0: 
0:     // Take in the raw CSV text file and split it ourselves,
0:     // creating our own SequenceFiles for the matrices to read later 
0:     // (similar to the style of syntheticcontrol.canopy.InputMapper)
0:     Path affSeqFiles = new Path(outputCalc, "seqfile-" + (System.nanoTime() & 0xFF));
0:     AffinityMatrixInputJob.runJob(input, affSeqFiles, numDims, numDims);
0: 
0:     // Next step: construct the affinity matrix using the newly-created
0:     // sequence files
0:     DistributedRowMatrix A = new DistributedRowMatrix(affSeqFiles,
0:                                                       new Path(outputTmp, "afftmp-" + (System.nanoTime() & 0xFF)),
0:                                                       numDims,
0:                                                       numDims);
0:     JobConf depConf = new JobConf(conf);
0:     A.configure(depConf);
0: 
0:     // Next step: construct the diagonal matrix D (represented as a vector)
0:     // and calculate the normalized Laplacian of the form:
0:     // L = D^(-0.5)AD^(-0.5)
0:     Vector D = MatrixDiagonalizeJob.runJob(affSeqFiles, numDims);
0:     DistributedRowMatrix L = VectorMatrixMultiplicationJob.runJob(affSeqFiles, D, new Path(outputCalc, "laplacian-"
0:         + (System.nanoTime() & 0xFF)));
0:     L.configure(new JobConf(conf));
0: 
0:     // Next step: perform eigen-decomposition using LanczosSolver
0:     // since some of the eigen-output is spurious and will be eliminated
0:     // upon verification, we have to aim to overshoot and then discard
0:     // unnecessary vectors later
0:     int overshoot = (int) ((double) clusters * SpectralKMeansDriver.OVERSHOOT_MULTIPLIER);
0:     List<Double> eigenValues = new ArrayList<Double>(overshoot);
0:     Matrix eigenVectors = new DenseMatrix(overshoot, numDims);
0:     DistributedLanczosSolver solver = new DistributedLanczosSolver();
0:     Path lanczosSeqFiles = new Path(outputCalc, "eigenvectors-" + (System.nanoTime() & 0xFF));
0:     solver.runJob(conf,
0:                   L.getRowPath(),
0:                   new Path(outputTmp, "lanczos-" + (System.nanoTime() & 0xFF)),
0:                   L.numRows(),
0:                   L.numCols(),
0:                   true,
0:                   overshoot,
0:                   eigenVectors,
0:                   eigenValues,
0:                   lanczosSeqFiles.toString());
0: 
0:     // perform a verification
0:     EigenVerificationJob verifier = new EigenVerificationJob();
0:     Path verifiedEigensPath = new Path(outputCalc, "eigenverifier");
0:     verifier.runJob(lanczosSeqFiles, L.getRowPath(), verifiedEigensPath, true, 1.0, 0.0, clusters);
0:     Path cleanedEigens = verifier.getCleanedEigensPath();
0:     DistributedRowMatrix W = new DistributedRowMatrix(cleanedEigens, new Path(cleanedEigens, "tmp"), clusters, numDims);
0:     W.configure(new JobConf());
0:     DistributedRowMatrix Wtrans = W.transpose();
0:     //		DistributedRowMatrix Wt = W.transpose();
0: 
0:     // next step: normalize the rows of Wt to unit length
0:     Path unitVectors = new Path(outputCalc, "unitvectors-" + (System.nanoTime() & 0xFF));
0:     UnitVectorizerJob.runJob(Wtrans.getRowPath(), unitVectors);
0:     DistributedRowMatrix Wt = new DistributedRowMatrix(unitVectors, new Path(unitVectors, "tmp"), clusters, numDims);
0:     Wt.configure(new JobConf());
0: 
0:     //		Iterator<MatrixSlice> i = W.iterator();
0:     //		int x = 0;
0:     //		while (i.hasNext()) {
0:     //			Vector v = i.next().vector();
0:     //			System.out.println("EIGENVECTOR " + (++x));
0:     //			for (int c = 0; c < v.size(); c++) {
0:     //				System.out.print(v.get(c) + " ");
0:     //			}
0:     //			System.out.println();
0:     //		}
0:     //		System.exit(0);
0: 
0:     // Finally, perform k-means clustering on the rows of L (or W)
0:     // generate random initial clusters
0:     DistanceMeasure measure = new EuclideanDistanceMeasure();
0:     Path initialclusters = RandomSeedGenerator.buildRandom(Wt.getRowPath(),
0:                                                            new Path(output, Cluster.INITIAL_CLUSTERS_DIR),
0:                                                            clusters,
0:                                                            measure);
0:     KMeansDriver.run(new Configuration(), Wt.getRowPath(), initialclusters, output, measure, 0.001, 10, true, false);
0: 
0:     // Read through the cluster assignments
0:     Path clusteredPointsPath = new Path(output, "clusteredPoints");
0:     FileSystem fs = FileSystem.get(conf);
0:     SequenceFile.Reader reader = new SequenceFile.Reader(fs, new Path(clusteredPointsPath, "part-m-00000"), conf);
0:     // The key is the clusterId
0:     IntWritable clusterId = new IntWritable(0);
0:     // The value is the weighted vector
0:     WeightedVectorWritable value = new WeightedVectorWritable();
0: 
0:     //	    Map<Integer, Integer> map = new HashMap<Integer, Integer>();
0:     int id = 0;
0:     while (reader.next(clusterId, value)) {
0:       //	    	Integer key = new Integer(clusterId.get());
0:       //	    	if (map.containsKey(key)) {
0:       //	    		Integer count = map.remove(key);
0:       //	    		map.put(key, new Integer(count.intValue() + 1));
0:       //	    	} else {
0:       //	    		map.put(key, new Integer(1));
0:       //	    	}
0:       System.out.println((id++) + ": " + clusterId.get());
0:       clusterId = new IntWritable(0);
0:       value = new WeightedVectorWritable();
0:     }
0:     reader.close();
0: 
0:     // TODO: output format???
0: 
0:     return 0;
0:   }
0: }
author:Jacob Alexander Mannix
-------------------------------------------------------------------------------
commit:522ee0b
/////////////////////////////////////////////////////////////////////////
/////////////////////////////////////////////////////////////////////////
0: import org.apache.mahout.math.decomposer.lanczos.LanczosState;
0: import java.io.IOException;
0: import java.util.Map;
0: 
/////////////////////////////////////////////////////////////////////////
0:     LanczosState state = new LanczosState(L, overshoot, numDims, solver.getInitialVector(L));
0:                   state,
0:                   true,
============================================================================