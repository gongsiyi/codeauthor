1:b4c5400: /*
1:b4c5400:  * Licensed to the Apache Software Foundation (ASF) under one or more
1:b4c5400:  * contributor license agreements.  See the NOTICE file distributed with
1:b4c5400:  * this work for additional information regarding copyright ownership.
1:b4c5400:  * The ASF licenses this file to You under the Apache License, Version 2.0
1:b4c5400:  * (the "License"); you may not use this file except in compliance with
1:b4c5400:  * the License.  You may obtain a copy of the License at
1:b4c5400:  *
1:b4c5400:  *     http://www.apache.org/licenses/LICENSE-2.0
1:b4c5400:  *
1:b4c5400:  * Unless required by applicable law or agreed to in writing, software
1:b4c5400:  * distributed under the License is distributed on an "AS IS" BASIS,
1:b4c5400:  * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
1:b4c5400:  * See the License for the specific language governing permissions and
1:b4c5400:  * limitations under the License.
1:b4c5400:  */
1:b4c5400: 
1:b4c5400: package org.apache.mahout.classifier.sgd;
1:b4c5400: 
1:b4c5400: import org.apache.hadoop.io.Writable;
1:b4c5400: import org.apache.mahout.classifier.AbstractVectorClassifier;
1:b4c5400: import org.apache.mahout.classifier.OnlineLearner;
1:b4c5400: import org.apache.mahout.common.RandomUtils;
1:b4c5400: import org.apache.mahout.math.DenseVector;
1:b4c5400: import org.apache.mahout.math.Vector;
1:b4c5400: import org.apache.mahout.math.VectorWritable;
1:b4c5400: import org.apache.mahout.math.function.Functions;
1:b4c5400: 
1:b4c5400: import java.io.DataInput;
1:b4c5400: import java.io.DataOutput;
1:b4c5400: import java.io.IOException;
1:b4c5400: import java.util.Collection;
1:85f9ece: import java.util.HashSet;
1:b4c5400: import java.util.Random;
1:b4c5400: 
1:b4c5400: /**
1:b4c5400:  * Online gradient machine learner that tries to minimize the label ranking hinge loss.
1:b4c5400:  * Implements a gradient machine with one sigmpod hidden layer.
1:b4c5400:  * It tries to minimize the ranking loss of some given set of labels,
1:b4c5400:  * so this can be used for multi-class, multi-label
1:b4c5400:  * or auto-encoding of sparse data (e.g. text).
1:b4c5400:  */
1:b4c5400: public class GradientMachine extends AbstractVectorClassifier implements OnlineLearner, Writable {
1:b4c5400: 
1:b4c5400:   public static final int WRITABLE_VERSION = 1;
1:b4c5400: 
1:b4c5400:   // the learning rate of the algorithm
1:b4c5400:   private double learningRate = 0.1;
1:b4c5400: 
1:b4c5400:   // the regularization term, a positive number that controls the size of the weight vector
1:b4c5400:   private double regularization = 0.1;
1:b4c5400: 
1:b4c5400:   // the sparsity term, a positive number that controls the sparsity of the hidden layer. (0 - 1)
1:b4c5400:   private double sparsity = 0.1;
1:b4c5400: 
1:b4c5400:   // the sparsity learning rate.
1:b4c5400:   private double sparsityLearningRate = 0.1;
1:b4c5400: 
1:b4c5400:   // the number of features
1:b4c5400:   private int numFeatures = 10;
1:b4c5400:   // the number of hidden nodes
1:b4c5400:   private int numHidden = 100;
1:b4c5400:   // the number of output nodes
1:b4c5400:   private int numOutput = 2;
1:b4c5400: 
1:b4c5400:   // coefficients for the input to hidden layer.
1:b4c5400:   // There are numHidden Vectors of dimension numFeatures.
1:b4c5400:   private Vector[] hiddenWeights;
1:b4c5400: 
1:b4c5400:   // coefficients for the hidden to output layer.
1:b4c5400:   // There are numOuput Vectors of dimension numHidden.
1:b4c5400:   private Vector[] outputWeights;
1:b4c5400: 
1:b4c5400:   // hidden unit bias
1:b4c5400:   private Vector hiddenBias;
1:b4c5400: 
1:b4c5400:   // output unit bias
1:b4c5400:   private Vector outputBias;
1:b4c5400: 
1:b4c5400:   private final Random rnd;
1:b4c5400: 
1:b4c5400:   public GradientMachine(int numFeatures, int numHidden, int numOutput) {
1:b4c5400:     this.numFeatures = numFeatures;
1:b4c5400:     this.numHidden = numHidden;
1:b4c5400:     this.numOutput = numOutput;
1:b4c5400:     hiddenWeights = new DenseVector[numHidden];
1:b4c5400:     for (int i = 0; i < numHidden; i++) {
1:b4c5400:       hiddenWeights[i] = new DenseVector(numFeatures);
1:b4c5400:       hiddenWeights[i].assign(0);
1:b4c5400:     }
1:b4c5400:     hiddenBias = new DenseVector(numHidden);
1:b4c5400:     hiddenBias.assign(0);
1:b4c5400:     outputWeights = new DenseVector[numOutput];
1:b4c5400:     for (int i = 0; i < numOutput; i++) {
1:b4c5400:       outputWeights[i] = new DenseVector(numHidden);
1:b4c5400:       outputWeights[i].assign(0);
1:b4c5400:     }
1:b4c5400:     outputBias = new DenseVector(numOutput);
1:b4c5400:     outputBias.assign(0);
1:b4c5400:     rnd = RandomUtils.getRandom();
1:b4c5400:   }
1:b4c5400: 
1:b4c5400:   /**
1:b4c5400:    * Initialize weights.
1:b4c5400:    *
1:b4c5400:    * @param gen random number generator.
1:b4c5400:    */
1:b4c5400:   public void initWeights(Random gen) {
1:2deba36:     double hiddenFanIn = 1.0 / Math.sqrt(numFeatures);
1:b4c5400:     for (int i = 0; i < numHidden; i++) {
1:b4c5400:       for (int j = 0; j < numFeatures; j++) {
1:b4c5400:         double val = (2.0 * gen.nextDouble() - 1.0) * hiddenFanIn;
1:b4c5400:         hiddenWeights[i].setQuick(j, val);
1:b4c5400:       }
1:b4c5400:     }
1:2deba36:     double outputFanIn = 1.0 / Math.sqrt(numHidden);
1:b4c5400:     for (int i = 0; i < numOutput; i++) {
1:b4c5400:       for (int j = 0; j < numHidden; j++) {
1:b4c5400:         double val = (2.0 * gen.nextDouble() - 1.0) * outputFanIn;
1:b4c5400:         outputWeights[i].setQuick(j, val);
1:b4c5400:       }
1:b4c5400:     }
1:b4c5400:   }
1:b4c5400: 
1:b4c5400:   /**
1:b4c5400:    * Chainable configuration option.
1:b4c5400:    *
1:b4c5400:    * @param learningRate New value of initial learning rate.
1:b4c5400:    * @return This, so other configurations can be chained.
1:b4c5400:    */
1:b4c5400:   public GradientMachine learningRate(double learningRate) {
1:b4c5400:     this.learningRate = learningRate;
1:b4c5400:     return this;
1:b4c5400:   }
1:b4c5400: 
1:b4c5400:   /**
1:b4c5400:    * Chainable configuration option.
1:b4c5400:    *
1:b4c5400:    * @param regularization A positive value that controls the weight vector size.
1:b4c5400:    * @return This, so other configurations can be chained.
1:b4c5400:    */
1:b4c5400:   public GradientMachine regularization(double regularization) {
1:b4c5400:     this.regularization = regularization;
1:b4c5400:     return this;
1:b4c5400:   }
1:b4c5400: 
1:b4c5400:   /**
1:b4c5400:    * Chainable configuration option.
1:b4c5400:    *
1:b4c5400:    * @param sparsity A value between zero and one that controls the fraction of hidden units
1:b4c5400:    *                 that are activated on average.
1:b4c5400:    * @return This, so other configurations can be chained.
1:b4c5400:    */
1:b4c5400:   public GradientMachine sparsity(double sparsity) {
1:b4c5400:     this.sparsity = sparsity;
1:b4c5400:     return this;
1:b4c5400:   }
1:b4c5400: 
1:b4c5400:   /**
1:b4c5400:    * Chainable configuration option.
1:b4c5400:    *
1:b4c5400:    * @param sparsityLearningRate New value of initial learning rate for sparsity.
1:b4c5400:    * @return This, so other configurations can be chained.
1:b4c5400:    */
1:b4c5400:   public GradientMachine sparsityLearningRate(double sparsityLearningRate) {
1:b4c5400:     this.sparsityLearningRate = sparsityLearningRate;
1:b4c5400:     return this;
1:b4c5400:   }
1:b4c5400: 
1:b4c5400:   public void copyFrom(GradientMachine other) {
1:b4c5400:     numFeatures = other.numFeatures;
1:b4c5400:     numHidden = other.numHidden;
1:b4c5400:     numOutput = other.numOutput;
1:b4c5400:     learningRate = other.learningRate;
1:b4c5400:     regularization = other.regularization;
1:b4c5400:     sparsity = other.sparsity;
1:b4c5400:     sparsityLearningRate = other.sparsityLearningRate;
1:b4c5400:     hiddenWeights = new DenseVector[numHidden];
1:b4c5400:     for (int i = 0; i < numHidden; i++) {
1:b4c5400:       hiddenWeights[i] = other.hiddenWeights[i].clone();
1:b4c5400:     }
1:b4c5400:     hiddenBias = other.hiddenBias.clone();
1:b4c5400:     outputWeights = new DenseVector[numOutput];
1:b4c5400:     for (int i = 0; i < numOutput; i++) {
1:b4c5400:       outputWeights[i] = other.outputWeights[i].clone();
1:b4c5400:     }
1:b4c5400:     outputBias = other.outputBias.clone();
1:b4c5400:   }
1:b4c5400: 
1:b4c5400:   @Override
1:b4c5400:   public int numCategories() {
1:b4c5400:     return numOutput;
1:b4c5400:   }
1:b4c5400: 
1:b4c5400:   public int numFeatures() {
1:b4c5400:     return numFeatures;
1:b4c5400:   }
1:b4c5400: 
1:b4c5400:   public int numHidden() {
1:b4c5400:     return numHidden;
1:b4c5400:   }
1:b4c5400: 
1:b4c5400:   /**
1:b4c5400:    * Feeds forward from input to hidden unit..
1:b4c5400:    *
1:b4c5400:    * @return Hidden unit activations.
1:b4c5400:    */
1:b4c5400:   public DenseVector inputToHidden(Vector input) {
1:b4c5400:     DenseVector activations = new DenseVector(numHidden);
1:b4c5400:     for (int i = 0; i < numHidden; i++) {
1:b4c5400:       activations.setQuick(i, hiddenWeights[i].dot(input));
1:b4c5400:     }
1:528ffcd:     activations.assign(hiddenBias, Functions.PLUS);
1:b4c5400:     activations.assign(Functions.min(40.0)).assign(Functions.max(-40));
1:b4c5400:     activations.assign(Functions.SIGMOID);
1:b4c5400:     return activations;
1:b4c5400:   }
1:b4c5400: 
1:b4c5400:   /**
1:b4c5400:    * Feeds forward from hidden to output
1:b4c5400:    *
1:b4c5400:    * @return Output unit activations.
1:b4c5400:    */
1:b4c5400:   public DenseVector hiddenToOutput(Vector hiddenActivation) {
1:b4c5400:     DenseVector activations = new DenseVector(numOutput);
1:b4c5400:     for (int i = 0; i < numOutput; i++) {
1:b4c5400:       activations.setQuick(i, outputWeights[i].dot(hiddenActivation));
1:b4c5400:     }
1:528ffcd:     activations.assign(outputBias, Functions.PLUS);
1:b4c5400:     return activations;
1:b4c5400:   }
1:b4c5400: 
1:b4c5400:   /**
1:b4c5400:    * Updates using ranking loss.
1:b4c5400:    *
1:b4c5400:    * @param hiddenActivation the hidden unit's activation
1:b4c5400:    * @param goodLabels       the labels you want ranked above others.
1:b4c5400:    * @param numTrials        how many times you want to search for the highest scoring bad label.
1:b4c5400:    * @param gen              Random number generator.
1:b4c5400:    */
1:b4c5400:   public void updateRanking(Vector hiddenActivation,
1:b4c5400:                             Collection<Integer> goodLabels,
1:b4c5400:                             int numTrials,
1:b4c5400:                             Random gen) {
1:b4c5400:     // All the labels are good, do nothing.
1:b4c5400:     if (goodLabels.size() >= numOutput) {
1:b4c5400:       return;
1:b4c5400:     }
1:b4c5400:     for (Integer good : goodLabels) {
1:b4c5400:       double goodScore = outputWeights[good].dot(hiddenActivation);
1:b4c5400:       int highestBad = -1;
1:b4c5400:       double highestBadScore = Double.NEGATIVE_INFINITY;
1:b4c5400:       for (int i = 0; i < numTrials; i++) {
1:b4c5400:         int bad = gen.nextInt(numOutput);
1:b4c5400:         while (goodLabels.contains(bad)) {
1:b4c5400:           bad = gen.nextInt(numOutput);
1:b4c5400:         }
1:b4c5400:         double badScore = outputWeights[bad].dot(hiddenActivation);
1:b4c5400:         if (badScore > highestBadScore) {
1:b4c5400:           highestBadScore = badScore;
1:b4c5400:           highestBad = bad;
1:b4c5400:         }
1:b4c5400:       }
1:b4c5400:       int bad = highestBad;
1:b4c5400:       double loss = 1.0 - goodScore + highestBadScore;
1:b4c5400:       if (loss < 0.0) {
1:b4c5400:         continue;
1:b4c5400:       }
1:b4c5400:       // Note from the loss above the gradient dloss/dy , y being the label is -1 for good
1:b4c5400:       // and +1 for bad.
1:b4c5400:       // dy / dw is just w since  y = x' * w + b.
1:b4c5400:       // Hence by the chain rule, dloss / dw = dloss / dy * dy / dw = -w.
1:b4c5400:       // For the regularization part, 0.5 * lambda * w' w, the gradient is lambda * w.
1:b4c5400:       // dy / db = 1.
1:b4c5400:       Vector gradGood = outputWeights[good].clone();
1:b4c5400:       gradGood.assign(Functions.NEGATE);
1:b4c5400:       Vector propHidden = gradGood.clone();
1:b4c5400:       Vector gradBad = outputWeights[bad].clone();
1:528ffcd:       propHidden.assign(gradBad, Functions.PLUS);
1:b4c5400:       gradGood.assign(Functions.mult(-learningRate * (1.0 - regularization)));
1:528ffcd:       outputWeights[good].assign(gradGood, Functions.PLUS);
1:b4c5400:       gradBad.assign(Functions.mult(-learningRate * (1.0 + regularization)));
1:528ffcd:       outputWeights[bad].assign(gradBad, Functions.PLUS);
1:b4c5400:       outputBias.setQuick(good, outputBias.get(good) + learningRate);
1:b4c5400:       outputBias.setQuick(bad, outputBias.get(bad) - learningRate);
1:b4c5400:       // Gradient of sigmoid is s * (1 -s).
1:b4c5400:       Vector gradSig = hiddenActivation.clone();
1:b4c5400:       gradSig.assign(Functions.SIGMOIDGRADIENT);
1:b4c5400:       // Multiply by the change caused by the ranking loss.
1:b4c5400:       for (int i = 0; i < numHidden; i++) {
1:b4c5400:         gradSig.setQuick(i, gradSig.get(i) * propHidden.get(i));
1:b4c5400:       }
1:b4c5400:       for (int i = 0; i < numHidden; i++) {
1:b4c5400:         for (int j = 0; j < numFeatures; j++) {
1:b4c5400:           double v = hiddenWeights[i].get(j);
1:b4c5400:           v -= learningRate * (gradSig.get(i) + regularization * v);
1:b4c5400:           hiddenWeights[i].setQuick(j, v);
1:b4c5400:         }
1:b4c5400:       }
1:b4c5400:     }
1:b4c5400:   }
1:b4c5400: 
1:b4c5400:   @Override
1:b4c5400:   public Vector classify(Vector instance) {
1:b4c5400:     Vector result = classifyNoLink(instance);
1:b4c5400:     // Find the max value's index.
1:b4c5400:     int max = result.maxValueIndex();
1:b4c5400:     result.assign(0);
1:b4c5400:     result.setQuick(max, 1.0);
1:b4c5400:     return result.viewPart(1, result.size() - 1);
1:b4c5400:   }
1:b4c5400: 
1:b4c5400:   @Override
1:b4c5400:   public Vector classifyNoLink(Vector instance) {
1:b4c5400:     DenseVector hidden = inputToHidden(instance);
1:b4c5400:     return hiddenToOutput(hidden);
1:b4c5400:   }
1:b4c5400: 
1:b4c5400:   @Override
1:b4c5400:   public double classifyScalar(Vector instance) {
1:b4c5400:     Vector output = classifyNoLink(instance);
1:b4c5400:     if (output.get(0) > output.get(1)) {
1:b4c5400:       return 0;
1:b4c5400:     }
1:b4c5400:     return 1;
1:b4c5400:   }
1:b4c5400: 
1:b4c5400:   public GradientMachine copy() {
1:b4c5400:     close();
1:b4c5400:     GradientMachine r = new GradientMachine(numFeatures(), numHidden(), numCategories());
1:b4c5400:     r.copyFrom(this);
1:b4c5400:     return r;
1:b4c5400:   }
1:b4c5400: 
1:b4c5400:   @Override
1:b4c5400:   public void write(DataOutput out) throws IOException {
1:b4c5400:     out.writeInt(WRITABLE_VERSION);
1:b4c5400:     out.writeDouble(learningRate);
1:b4c5400:     out.writeDouble(regularization);
1:b4c5400:     out.writeDouble(sparsity);
1:b4c5400:     out.writeDouble(sparsityLearningRate);
1:b4c5400:     out.writeInt(numFeatures);
1:b4c5400:     out.writeInt(numHidden);
1:b4c5400:     out.writeInt(numOutput);
1:b4c5400:     VectorWritable.writeVector(out, hiddenBias);
1:b4c5400:     for (int i = 0; i < numHidden; i++) {
1:b4c5400:       VectorWritable.writeVector(out, hiddenWeights[i]);
1:b4c5400:     }
1:b4c5400:     VectorWritable.writeVector(out, outputBias);
1:b4c5400:     for (int i = 0; i < numOutput; i++) {
1:b4c5400:       VectorWritable.writeVector(out, outputWeights[i]);
1:b4c5400:     }
1:b4c5400:   }
1:b4c5400: 
1:b4c5400:   @Override
1:b4c5400:   public void readFields(DataInput in) throws IOException {
1:b4c5400:     int version = in.readInt();
1:b4c5400:     if (version == WRITABLE_VERSION) {
1:b4c5400:       learningRate = in.readDouble();
1:b4c5400:       regularization = in.readDouble();
1:b4c5400:       sparsity = in.readDouble();
1:b4c5400:       sparsityLearningRate = in.readDouble();
1:b4c5400:       numFeatures = in.readInt();
1:b4c5400:       numHidden = in.readInt();
1:b4c5400:       numOutput = in.readInt();
1:b4c5400:       hiddenWeights = new DenseVector[numHidden];
1:b4c5400:       hiddenBias = VectorWritable.readVector(in);
1:b4c5400:       for (int i = 0; i < numHidden; i++) {
1:b4c5400:         hiddenWeights[i] = VectorWritable.readVector(in);
1:b4c5400:       }
1:b4c5400:       outputWeights = new DenseVector[numOutput];
1:b4c5400:       outputBias = VectorWritable.readVector(in);
1:b4c5400:       for (int i = 0; i < numOutput; i++) {
1:b4c5400:         outputWeights[i] = VectorWritable.readVector(in);
1:b4c5400:       }
1:b4c5400:     } else {
1:b4c5400:       throw new IOException("Incorrect object version, wanted " + WRITABLE_VERSION + " got " + version);
1:b4c5400:     }
1:b4c5400:   }
1:b4c5400: 
1:b4c5400:   @Override
1:b4c5400:   public void close() {
1:b4c5400:     // This is an online classifier, nothing to do.
1:b4c5400:   }
1:b4c5400: 
1:b4c5400:   @Override
1:b4c5400:   public void train(long trackingKey, String groupKey, int actual, Vector instance) {
1:b4c5400:     Vector hiddenActivation = inputToHidden(instance);
1:1499411:     hiddenToOutput(hiddenActivation);
1:85f9ece:     Collection<Integer> goodLabels = new HashSet<>();
1:b4c5400:     goodLabels.add(actual);
1:b4c5400:     updateRanking(hiddenActivation, goodLabels, 2, rnd);
1:b4c5400:   }
1:b4c5400: 
1:b4c5400:   @Override
1:b4c5400:   public void train(long trackingKey, int actual, Vector instance) {
1:b4c5400:     train(trackingKey, null, actual, instance);
1:b4c5400:   }
1:b4c5400: 
1:b4c5400:   @Override
1:b4c5400:   public void train(int actual, Vector instance) {
1:b4c5400:     train(0, null, actual, instance);
1:b4c5400:   }
1:b4c5400: 
1:b4c5400: }
============================================================================
author:Suneel Marthi
-------------------------------------------------------------------------------
commit:85f9ece
/////////////////////////////////////////////////////////////////////////
/////////////////////////////////////////////////////////////////////////
1: import java.util.HashSet;
/////////////////////////////////////////////////////////////////////////
1:     Collection<Integer> goodLabels = new HashSet<>();
author:pferrel
-------------------------------------------------------------------------------
commit:b988c49
author:frankscholten
-------------------------------------------------------------------------------
commit:1a42d85
author:Ted Dunning
-------------------------------------------------------------------------------
commit:402e296
/////////////////////////////////////////////////////////////////////////
commit:528ffcd
/////////////////////////////////////////////////////////////////////////
1:     activations.assign(hiddenBias, Functions.PLUS);
/////////////////////////////////////////////////////////////////////////
1:     activations.assign(outputBias, Functions.PLUS);
/////////////////////////////////////////////////////////////////////////
1:       propHidden.assign(gradBad, Functions.PLUS);
1:       outputWeights[good].assign(gradGood, Functions.PLUS);
1:       outputWeights[bad].assign(gradBad, Functions.PLUS);
author:Sebastian Schelter
-------------------------------------------------------------------------------
commit:210b265
/////////////////////////////////////////////////////////////////////////
0: import com.google.common.collect.Sets;
/////////////////////////////////////////////////////////////////////////
0:     Collection<Integer> goodLabels = Sets.newHashSet();
author:Sean R. Owen
-------------------------------------------------------------------------------
commit:2deba36
/////////////////////////////////////////////////////////////////////////
1:     double hiddenFanIn = 1.0 / Math.sqrt(numFeatures);
1:     double outputFanIn = 1.0 / Math.sqrt(numHidden);
commit:1499411
/////////////////////////////////////////////////////////////////////////
1:     hiddenToOutput(hiddenActivation);
commit:b4c5400
/////////////////////////////////////////////////////////////////////////
1: /*
1:  * Licensed to the Apache Software Foundation (ASF) under one or more
1:  * contributor license agreements.  See the NOTICE file distributed with
1:  * this work for additional information regarding copyright ownership.
1:  * The ASF licenses this file to You under the Apache License, Version 2.0
1:  * (the "License"); you may not use this file except in compliance with
1:  * the License.  You may obtain a copy of the License at
1:  *
1:  *     http://www.apache.org/licenses/LICENSE-2.0
1:  *
1:  * Unless required by applicable law or agreed to in writing, software
1:  * distributed under the License is distributed on an "AS IS" BASIS,
1:  * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
1:  * See the License for the specific language governing permissions and
1:  * limitations under the License.
1:  */
1: 
1: package org.apache.mahout.classifier.sgd;
1: 
1: import org.apache.hadoop.io.Writable;
1: import org.apache.mahout.classifier.AbstractVectorClassifier;
1: import org.apache.mahout.classifier.OnlineLearner;
1: import org.apache.mahout.common.RandomUtils;
1: import org.apache.mahout.math.DenseVector;
1: import org.apache.mahout.math.Vector;
1: import org.apache.mahout.math.VectorWritable;
1: import org.apache.mahout.math.function.Functions;
1: 
1: import java.io.DataInput;
1: import java.io.DataOutput;
1: import java.io.IOException;
1: import java.util.Collection;
0: import java.util.HashSet;
1: import java.util.Random;
1: 
1: /**
1:  * Online gradient machine learner that tries to minimize the label ranking hinge loss.
1:  * Implements a gradient machine with one sigmpod hidden layer.
1:  * It tries to minimize the ranking loss of some given set of labels,
1:  * so this can be used for multi-class, multi-label
1:  * or auto-encoding of sparse data (e.g. text).
1:  */
1: public class GradientMachine extends AbstractVectorClassifier implements OnlineLearner, Writable {
1: 
1:   public static final int WRITABLE_VERSION = 1;
1: 
1:   // the learning rate of the algorithm
1:   private double learningRate = 0.1;
1: 
1:   // the regularization term, a positive number that controls the size of the weight vector
1:   private double regularization = 0.1;
1: 
1:   // the sparsity term, a positive number that controls the sparsity of the hidden layer. (0 - 1)
1:   private double sparsity = 0.1;
1: 
1:   // the sparsity learning rate.
1:   private double sparsityLearningRate = 0.1;
1: 
1:   // the number of features
1:   private int numFeatures = 10;
1:   // the number of hidden nodes
1:   private int numHidden = 100;
1:   // the number of output nodes
1:   private int numOutput = 2;
1: 
1:   // coefficients for the input to hidden layer.
1:   // There are numHidden Vectors of dimension numFeatures.
1:   private Vector[] hiddenWeights;
1: 
1:   // coefficients for the hidden to output layer.
1:   // There are numOuput Vectors of dimension numHidden.
1:   private Vector[] outputWeights;
1: 
1:   // hidden unit bias
1:   private Vector hiddenBias;
1: 
1:   // output unit bias
1:   private Vector outputBias;
1: 
1:   private final Random rnd;
1: 
1:   public GradientMachine(int numFeatures, int numHidden, int numOutput) {
1:     this.numFeatures = numFeatures;
1:     this.numHidden = numHidden;
1:     this.numOutput = numOutput;
1:     hiddenWeights = new DenseVector[numHidden];
1:     for (int i = 0; i < numHidden; i++) {
1:       hiddenWeights[i] = new DenseVector(numFeatures);
1:       hiddenWeights[i].assign(0);
1:     }
1:     hiddenBias = new DenseVector(numHidden);
1:     hiddenBias.assign(0);
1:     outputWeights = new DenseVector[numOutput];
1:     for (int i = 0; i < numOutput; i++) {
1:       outputWeights[i] = new DenseVector(numHidden);
1:       outputWeights[i].assign(0);
1:     }
1:     outputBias = new DenseVector(numOutput);
1:     outputBias.assign(0);
1:     rnd = RandomUtils.getRandom();
1:   }
1: 
1:   /**
1:    * Initialize weights.
1:    *
1:    * @param gen random number generator.
1:    */
1:   public void initWeights(Random gen) {
0:     double hiddenFanIn = 1.0f / Math.sqrt(numFeatures);
1:     for (int i = 0; i < numHidden; i++) {
1:       for (int j = 0; j < numFeatures; j++) {
1:         double val = (2.0 * gen.nextDouble() - 1.0) * hiddenFanIn;
1:         hiddenWeights[i].setQuick(j, val);
1:       }
1:     }
0:     double outputFanIn = 1.0f / Math.sqrt(numHidden);
1:     for (int i = 0; i < numOutput; i++) {
1:       for (int j = 0; j < numHidden; j++) {
1:         double val = (2.0 * gen.nextDouble() - 1.0) * outputFanIn;
1:         outputWeights[i].setQuick(j, val);
1:       }
1:     }
1:   }
1: 
1:   /**
1:    * Chainable configuration option.
1:    *
1:    * @param learningRate New value of initial learning rate.
1:    * @return This, so other configurations can be chained.
1:    */
1:   public GradientMachine learningRate(double learningRate) {
1:     this.learningRate = learningRate;
1:     return this;
1:   }
1: 
1:   /**
1:    * Chainable configuration option.
1:    *
1:    * @param regularization A positive value that controls the weight vector size.
1:    * @return This, so other configurations can be chained.
1:    */
1:   public GradientMachine regularization(double regularization) {
1:     this.regularization = regularization;
1:     return this;
1:   }
1: 
1:   /**
1:    * Chainable configuration option.
1:    *
1:    * @param sparsity A value between zero and one that controls the fraction of hidden units
1:    *                 that are activated on average.
1:    * @return This, so other configurations can be chained.
1:    */
1:   public GradientMachine sparsity(double sparsity) {
1:     this.sparsity = sparsity;
1:     return this;
1:   }
1: 
1:   /**
1:    * Chainable configuration option.
1:    *
1:    * @param sparsityLearningRate New value of initial learning rate for sparsity.
1:    * @return This, so other configurations can be chained.
1:    */
1:   public GradientMachine sparsityLearningRate(double sparsityLearningRate) {
1:     this.sparsityLearningRate = sparsityLearningRate;
1:     return this;
1:   }
1: 
1:   public void copyFrom(GradientMachine other) {
1:     numFeatures = other.numFeatures;
1:     numHidden = other.numHidden;
1:     numOutput = other.numOutput;
1:     learningRate = other.learningRate;
1:     regularization = other.regularization;
1:     sparsity = other.sparsity;
1:     sparsityLearningRate = other.sparsityLearningRate;
1:     hiddenWeights = new DenseVector[numHidden];
1:     for (int i = 0; i < numHidden; i++) {
1:       hiddenWeights[i] = other.hiddenWeights[i].clone();
1:     }
1:     hiddenBias = other.hiddenBias.clone();
1:     outputWeights = new DenseVector[numOutput];
1:     for (int i = 0; i < numOutput; i++) {
1:       outputWeights[i] = other.outputWeights[i].clone();
1:     }
1:     outputBias = other.outputBias.clone();
1:   }
1: 
1:   @Override
1:   public int numCategories() {
1:     return numOutput;
1:   }
1: 
1:   public int numFeatures() {
1:     return numFeatures;
1:   }
1: 
1:   public int numHidden() {
1:     return numHidden;
1:   }
1: 
1:   /**
1:    * Feeds forward from input to hidden unit..
1:    *
1:    * @return Hidden unit activations.
1:    */
1:   public DenseVector inputToHidden(Vector input) {
1:     DenseVector activations = new DenseVector(numHidden);
1:     for (int i = 0; i < numHidden; i++) {
1:       activations.setQuick(i, hiddenWeights[i].dot(input));
1:     }
0:     hiddenBias.addTo(activations);
1:     activations.assign(Functions.min(40.0)).assign(Functions.max(-40));
1:     activations.assign(Functions.SIGMOID);
1:     return activations;
1:   }
1: 
1:   /**
1:    * Feeds forward from hidden to output
1:    *
1:    * @return Output unit activations.
1:    */
1:   public DenseVector hiddenToOutput(Vector hiddenActivation) {
1:     DenseVector activations = new DenseVector(numOutput);
1:     for (int i = 0; i < numOutput; i++) {
1:       activations.setQuick(i, outputWeights[i].dot(hiddenActivation));
1:     }
0:     outputBias.addTo(activations);
1:     return activations;
1:   }
1: 
1:   /**
1:    * Updates using ranking loss.
1:    *
1:    * @param hiddenActivation the hidden unit's activation
1:    * @param goodLabels       the labels you want ranked above others.
1:    * @param numTrials        how many times you want to search for the highest scoring bad label.
1:    * @param gen              Random number generator.
1:    */
1:   public void updateRanking(Vector hiddenActivation,
1:                             Collection<Integer> goodLabels,
1:                             int numTrials,
1:                             Random gen) {
1:     // All the labels are good, do nothing.
1:     if (goodLabels.size() >= numOutput) {
1:       return;
1:     }
1:     for (Integer good : goodLabels) {
1:       double goodScore = outputWeights[good].dot(hiddenActivation);
1:       int highestBad = -1;
1:       double highestBadScore = Double.NEGATIVE_INFINITY;
1:       for (int i = 0; i < numTrials; i++) {
1:         int bad = gen.nextInt(numOutput);
1:         while (goodLabels.contains(bad)) {
1:           bad = gen.nextInt(numOutput);
1:         }
1:         double badScore = outputWeights[bad].dot(hiddenActivation);
1:         if (badScore > highestBadScore) {
1:           highestBadScore = badScore;
1:           highestBad = bad;
1:         }
1:       }
1:       int bad = highestBad;
1:       double loss = 1.0 - goodScore + highestBadScore;
1:       if (loss < 0.0) {
1:         continue;
1:       }
1:       // Note from the loss above the gradient dloss/dy , y being the label is -1 for good
1:       // and +1 for bad.
1:       // dy / dw is just w since  y = x' * w + b.
1:       // Hence by the chain rule, dloss / dw = dloss / dy * dy / dw = -w.
1:       // For the regularization part, 0.5 * lambda * w' w, the gradient is lambda * w.
1:       // dy / db = 1.
1:       Vector gradGood = outputWeights[good].clone();
1:       gradGood.assign(Functions.NEGATE);
1:       Vector propHidden = gradGood.clone();
1:       Vector gradBad = outputWeights[bad].clone();
0:       gradBad.addTo(propHidden);
1:       gradGood.assign(Functions.mult(-learningRate * (1.0 - regularization)));
0:       gradGood.addTo(outputWeights[good]);
1:       gradBad.assign(Functions.mult(-learningRate * (1.0 + regularization)));
0:       gradBad.addTo(outputWeights[bad]);
1:       outputBias.setQuick(good, outputBias.get(good) + learningRate);
1:       outputBias.setQuick(bad, outputBias.get(bad) - learningRate);
1:       // Gradient of sigmoid is s * (1 -s).
1:       Vector gradSig = hiddenActivation.clone();
1:       gradSig.assign(Functions.SIGMOIDGRADIENT);
1:       // Multiply by the change caused by the ranking loss.
1:       for (int i = 0; i < numHidden; i++) {
1:         gradSig.setQuick(i, gradSig.get(i) * propHidden.get(i));
1:       }
1:       for (int i = 0; i < numHidden; i++) {
1:         for (int j = 0; j < numFeatures; j++) {
1:           double v = hiddenWeights[i].get(j);
1:           v -= learningRate * (gradSig.get(i) + regularization * v);
1:           hiddenWeights[i].setQuick(j, v);
1:         }
1:       }
1:     }
1:   }
1: 
1:   @Override
1:   public Vector classify(Vector instance) {
1:     Vector result = classifyNoLink(instance);
1:     // Find the max value's index.
1:     int max = result.maxValueIndex();
1:     result.assign(0);
1:     result.setQuick(max, 1.0);
1:     return result.viewPart(1, result.size() - 1);
1:   }
1: 
1:   @Override
1:   public Vector classifyNoLink(Vector instance) {
1:     DenseVector hidden = inputToHidden(instance);
1:     return hiddenToOutput(hidden);
1:   }
1: 
1:   @Override
1:   public double classifyScalar(Vector instance) {
1:     Vector output = classifyNoLink(instance);
1:     if (output.get(0) > output.get(1)) {
1:       return 0;
1:     }
1:     return 1;
1:   }
1: 
1:   public GradientMachine copy() {
1:     close();
1:     GradientMachine r = new GradientMachine(numFeatures(), numHidden(), numCategories());
1:     r.copyFrom(this);
1:     return r;
1:   }
1: 
1:   @Override
1:   public void write(DataOutput out) throws IOException {
1:     out.writeInt(WRITABLE_VERSION);
1:     out.writeDouble(learningRate);
1:     out.writeDouble(regularization);
1:     out.writeDouble(sparsity);
1:     out.writeDouble(sparsityLearningRate);
1:     out.writeInt(numFeatures);
1:     out.writeInt(numHidden);
1:     out.writeInt(numOutput);
1:     VectorWritable.writeVector(out, hiddenBias);
1:     for (int i = 0; i < numHidden; i++) {
1:       VectorWritable.writeVector(out, hiddenWeights[i]);
1:     }
1:     VectorWritable.writeVector(out, outputBias);
1:     for (int i = 0; i < numOutput; i++) {
1:       VectorWritable.writeVector(out, outputWeights[i]);
1:     }
1:   }
1: 
1:   @Override
1:   public void readFields(DataInput in) throws IOException {
1:     int version = in.readInt();
1:     if (version == WRITABLE_VERSION) {
1:       learningRate = in.readDouble();
1:       regularization = in.readDouble();
1:       sparsity = in.readDouble();
1:       sparsityLearningRate = in.readDouble();
1:       numFeatures = in.readInt();
1:       numHidden = in.readInt();
1:       numOutput = in.readInt();
1:       hiddenWeights = new DenseVector[numHidden];
1:       hiddenBias = VectorWritable.readVector(in);
1:       for (int i = 0; i < numHidden; i++) {
1:         hiddenWeights[i] = VectorWritable.readVector(in);
1:       }
1:       outputWeights = new DenseVector[numOutput];
1:       outputBias = VectorWritable.readVector(in);
1:       for (int i = 0; i < numOutput; i++) {
1:         outputWeights[i] = VectorWritable.readVector(in);
1:       }
1:     } else {
1:       throw new IOException("Incorrect object version, wanted " + WRITABLE_VERSION + " got " + version);
1:     }
1:   }
1: 
1:   @Override
1:   public void close() {
1:     // This is an online classifier, nothing to do.
1:   }
1: 
1:   @Override
1:   public void train(long trackingKey, String groupKey, int actual, Vector instance) {
1:     Vector hiddenActivation = inputToHidden(instance);
0:     Vector outputActivation = hiddenToOutput(hiddenActivation);
0:     Collection<Integer> goodLabels = new HashSet<Integer>();
1:     goodLabels.add(actual);
1:     updateRanking(hiddenActivation, goodLabels, 2, rnd);
1:   }
1: 
1:   @Override
1:   public void train(long trackingKey, int actual, Vector instance) {
1:     train(trackingKey, null, actual, instance);
1:   }
1: 
1:   @Override
1:   public void train(int actual, Vector instance) {
1:     train(0, null, actual, instance);
1:   }
1: 
1: }
============================================================================