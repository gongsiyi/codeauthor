1:151de0d: /**
1:151de0d:  * Licensed to the Apache Software Foundation (ASF) under one or more
1:151de0d:  * contributor license agreements.  See the NOTICE file distributed with
1:151de0d:  * this work for additional information regarding copyright ownership.
1:151de0d:  * The ASF licenses this file to You under the Apache License, Version 2.0
1:151de0d:  * (the "License"); you may not use this file except in compliance with
1:151de0d:  * the License.  You may obtain a copy of the License at
1:151de0d:  *
1:151de0d:  *     http://www.apache.org/licenses/LICENSE-2.0
1:151de0d:  *
1:151de0d:  * Unless required by applicable law or agreed to in writing, software
1:151de0d:  * distributed under the License is distributed on an "AS IS" BASIS,
1:151de0d:  * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
1:151de0d:  * See the License for the specific language governing permissions and
1:151de0d:  * limitations under the License.
1:151de0d:  */
36:151de0d: 
1:151de0d: package org.apache.mahout.math.hadoop.stochasticsvd;
1:ffc7fab: 
1:151de0d: import java.io.Closeable;
1:151de0d: import java.io.IOException;
1:a13b4b7: import java.util.Deque;
1:151de0d: 
1:58cc1ae: import com.google.common.collect.Lists;
1:151de0d: import org.apache.hadoop.conf.Configuration;
1:151de0d: import org.apache.hadoop.fs.Path;
1:151de0d: import org.apache.hadoop.io.SequenceFile.CompressionType;
1:151de0d: import org.apache.hadoop.io.Writable;
1:151de0d: import org.apache.hadoop.io.compress.DefaultCodec;
1:151de0d: import org.apache.hadoop.mapred.JobConf;
1:ffc7fab: import org.apache.hadoop.mapred.OutputCollector;
1:151de0d: import org.apache.hadoop.mapred.lib.MultipleOutputs;
1:151de0d: import org.apache.hadoop.mapreduce.Job;
1:151de0d: import org.apache.hadoop.mapreduce.Mapper;
1:151de0d: import org.apache.hadoop.mapreduce.lib.input.FileInputFormat;
1:151de0d: import org.apache.hadoop.mapreduce.lib.input.SequenceFileInputFormat;
1:151de0d: import org.apache.hadoop.mapreduce.lib.output.FileOutputFormat;
1:151de0d: import org.apache.hadoop.mapreduce.lib.output.SequenceFileOutputFormat;
1:151de0d: import org.apache.mahout.common.IOUtils;
1:151de0d: import org.apache.mahout.math.DenseVector;
1:ffc7fab: import org.apache.mahout.math.Vector;
1:151de0d: import org.apache.mahout.math.VectorWritable;
1:175701c: import org.apache.mahout.math.function.Functions;
1:ffc7fab: import org.apache.mahout.math.hadoop.stochasticsvd.qr.QRFirstStep;
1:151de0d: 
1:151de0d: /**
1:151de0d:  * Compute first level of QHat-transpose blocks.
1:b479bd2:  * <P>
1:151de0d:  * 
1:b479bd2:  * See Mahout-376 working notes for details.
1:b479bd2:  * <P>
1:151de0d:  * 
1:b479bd2:  * Uses some of Hadoop deprecated api wherever newer api is not available.
1:b479bd2:  * Hence, @SuppressWarnings("deprecation") for imports (MAHOUT-593).
1:b479bd2:  * <P>
1:151de0d:  * 
1:151de0d:  */
1:b479bd2: @SuppressWarnings("deprecation")
1:3218e95: public final class QJob {
1:151de0d: 
1:151de0d:   public static final String PROP_OMEGA_SEED = "ssvd.omegaseed";
1:ffc7fab:   public static final String PROP_K = QRFirstStep.PROP_K;
1:ffc7fab:   public static final String PROP_P = QRFirstStep.PROP_P;
1:175701c:   public static final String PROP_SB_PATH = "ssvdpca.sb.path";
1:175701c:   public static final String PROP_AROWBLOCK_SIZE =
1:175701c:     QRFirstStep.PROP_AROWBLOCK_SIZE;
1:151de0d: 
1:ffc7fab:   public static final String OUTPUT_RHAT = "R";
1:151de0d:   public static final String OUTPUT_QHAT = "QHat";
1:151de0d: 
1:a13b4b7:   private QJob() {
16:151de0d:   }
1:b479bd2: 
1:ffc7fab:   public static class QMapper
1:ffc7fab:       extends
1:ffc7fab:       Mapper<Writable, VectorWritable, SplitPartitionedWritable, VectorWritable> {
1:b479bd2: 
1:151de0d:     private MultipleOutputs outputs;
1:58cc1ae:     private final Deque<Closeable> closeables = Lists.newLinkedList();
1:ffc7fab:     private SplitPartitionedWritable qHatKey;
1:ffc7fab:     private SplitPartitionedWritable rHatKey;
1:ffc7fab:     private Vector yRow;
1:175701c:     private Vector sb;
1:ffc7fab:     private Omega omega;
1:ffc7fab:     private int kp;
1:151de0d: 
1:ffc7fab:     private QRFirstStep qr;
1:151de0d: 
5:151de0d:     @Override
1:ffc7fab:     protected void setup(Context context) throws IOException,
1:ffc7fab:       InterruptedException {
1:ffc7fab: 
1:175701c:       Configuration conf = context.getConfiguration();
1:175701c:       int k = Integer.parseInt(conf.get(PROP_K));
1:175701c:       int p = Integer.parseInt(conf.get(PROP_P));
1:175701c:       kp = k + p;
1:175701c:       long omegaSeed = Long.parseLong(conf.get(PROP_OMEGA_SEED));
1:175701c:       omega = new Omega(omegaSeed, k + p);
1:175701c: 
1:175701c:       String sbPathStr = conf.get(PROP_SB_PATH);
1:175701c:       if (sbPathStr != null) {
1:175701c:         sb = SSVDHelper.loadAndSumUpVectors(new Path(sbPathStr), conf);
1:b717cfc:         if (sb == null)
1:b717cfc:           throw new IOException(String.format("Unable to load s_omega from path %s.", sbPathStr));
1:175701c:       }
1:175701c: 
1:175701c:       outputs = new MultipleOutputs(new JobConf(conf));
1:151de0d:       closeables.addFirst(new Closeable() {
1:151de0d:         @Override
1:151de0d:         public void close() throws IOException {
1:151de0d:           outputs.close();
1:b479bd2:         }
1:151de0d:       });
1:ffc7fab: 
1:ffc7fab:       qHatKey = new SplitPartitionedWritable(context);
1:ffc7fab:       rHatKey = new SplitPartitionedWritable(context);
1:175701c: 
1:ffc7fab:       OutputCollector<Writable, DenseBlockWritable> qhatCollector =
1:ffc7fab:         new OutputCollector<Writable, DenseBlockWritable>() {
1:ffc7fab: 
1:ffc7fab:           @Override
1:ffc7fab:           @SuppressWarnings("unchecked")
1:ffc7fab:           public void collect(Writable nil, DenseBlockWritable dbw)
1:ffc7fab:             throws IOException {
1:ffc7fab:             outputs.getCollector(OUTPUT_QHAT, null).collect(qHatKey, dbw);
1:ffc7fab:             qHatKey.incrementItemOrdinal();
1:ffc7fab:           }
1:ffc7fab:         };
1:175701c: 
1:ffc7fab:       OutputCollector<Writable, VectorWritable> rhatCollector =
1:ffc7fab:         new OutputCollector<Writable, VectorWritable>() {
1:ffc7fab: 
1:ffc7fab:           @Override
1:ffc7fab:           @SuppressWarnings("unchecked")
1:ffc7fab:           public void collect(Writable nil, VectorWritable rhat)
1:ffc7fab:             throws IOException {
1:ffc7fab:             outputs.getCollector(OUTPUT_RHAT, null).collect(rHatKey, rhat);
1:ffc7fab:             rHatKey.incrementItemOrdinal();
1:ffc7fab:           }
1:ffc7fab:         };
1:ffc7fab: 
1:175701c:       qr = new QRFirstStep(conf, qhatCollector, rhatCollector);
1:44459bd:       closeables.addFirst(qr); // important: qr closes first!!
1:175701c:       yRow = new DenseVector(kp);
1:ffc7fab:     }
1:175701c: 
1:ffc7fab:     @Override
1:ffc7fab:     protected void map(Writable key, VectorWritable value, Context context)
1:ffc7fab:       throws IOException, InterruptedException {
1:ffc7fab:       omega.computeYRow(value.get(), yRow);
1:175701c:       if (sb != null) {
1:175701c:         yRow.assign(sb, Functions.MINUS);
1:175701c:       }
1:ffc7fab:       qr.collect(key, yRow);
1:b479bd2:     }
1:ffc7fab: 
1:151de0d:     @Override
1:175701c:     protected void cleanup(Context context) throws IOException,
1:175701c:       InterruptedException {
1:151de0d:       IOUtils.close(closeables);
1:151de0d:     }
1:151de0d:   }
1:ffc7fab: 
1:a13b4b7:   public static void run(Configuration conf,
1:a13b4b7:                          Path[] inputPaths,
1:175701c:                          Path sbPath,
1:a13b4b7:                          Path outputPath,
1:a13b4b7:                          int aBlockRows,
1:a13b4b7:                          int minSplitSize,
1:a13b4b7:                          int k,
1:a13b4b7:                          int p,
1:a13b4b7:                          long seed,
1:ffc7fab:                          int numReduceTasks) throws ClassNotFoundException,
1:ffc7fab:     InterruptedException, IOException {
1:ffc7fab: 
1:151de0d:     JobConf oldApiJob = new JobConf(conf);
1:175701c:     MultipleOutputs.addNamedOutput(oldApiJob,
1:175701c:                                    OUTPUT_QHAT,
1:175701c:                                    org.apache.hadoop.mapred.SequenceFileOutputFormat.class,
1:175701c:                                    SplitPartitionedWritable.class,
1:175701c:                                    DenseBlockWritable.class);
1:175701c:     MultipleOutputs.addNamedOutput(oldApiJob,
1:175701c:                                    OUTPUT_RHAT,
1:175701c:                                    org.apache.hadoop.mapred.SequenceFileOutputFormat.class,
1:175701c:                                    SplitPartitionedWritable.class,
1:175701c:                                    VectorWritable.class);
1:b479bd2: 
1:151de0d:     Job job = new Job(oldApiJob);
1:151de0d:     job.setJobName("Q-job");
1:151de0d:     job.setJarByClass(QJob.class);
1:151de0d: 
1:151de0d:     job.setInputFormatClass(SequenceFileInputFormat.class);
1:151de0d:     FileInputFormat.setInputPaths(job, inputPaths);
1:a13b4b7:     if (minSplitSize > 0) {
1:b16c260:       FileInputFormat.setMinInputSplitSize(job, minSplitSize);
1:151de0d:     }
1:151de0d: 
1:151de0d:     FileOutputFormat.setOutputPath(job, outputPath);
1:151de0d: 
1:b16c260:     FileOutputFormat.setCompressOutput(job, true);
1:b16c260:     FileOutputFormat.setOutputCompressorClass(job, DefaultCodec.class);
1:ffc7fab:     SequenceFileOutputFormat.setOutputCompressionType(job,
1:ffc7fab:                                                       CompressionType.BLOCK);
1:151de0d: 
1:ffc7fab:     job.setMapOutputKeyClass(SplitPartitionedWritable.class);
1:151de0d:     job.setMapOutputValueClass(VectorWritable.class);
1:151de0d: 
1:ffc7fab:     job.setOutputKeyClass(SplitPartitionedWritable.class);
1:151de0d:     job.setOutputValueClass(VectorWritable.class);
1:151de0d: 
1:151de0d:     job.setMapperClass(QMapper.class);
1:151de0d: 
1:151de0d:     job.getConfiguration().setInt(PROP_AROWBLOCK_SIZE, aBlockRows);
1:151de0d:     job.getConfiguration().setLong(PROP_OMEGA_SEED, seed);
1:151de0d:     job.getConfiguration().setInt(PROP_K, k);
1:151de0d:     job.getConfiguration().setInt(PROP_P, p);
1:175701c:     if (sbPath != null) {
1:175701c:       job.getConfiguration().set(PROP_SB_PATH, sbPath.toString());
1:175701c:     }
1:b479bd2: 
1:5a2250c:     /*
1:5a2250c:      * number of reduce tasks doesn't matter. we don't actually send anything to
1:5a2250c:      * reducers.
1:5a2250c:      */
1:151de0d: 
1:151de0d:     job.setNumReduceTasks(0 /* numReduceTasks */);
1:151de0d: 
1:151de0d:     job.submit();
1:151de0d:     job.waitForCompletion(false);
1:151de0d: 
1:a13b4b7:     if (!job.isSuccessful()) {
1:151de0d:       throw new IOException("Q job unsuccessful.");
1:151de0d:     }
1:151de0d: 
1:151de0d:   }
1:151de0d: 
1:151de0d: }
============================================================================
author:pferrel
-------------------------------------------------------------------------------
commit:b988c49
author:frankscholten
-------------------------------------------------------------------------------
commit:1a42d85
author:Dmitriy Lyubimov
-------------------------------------------------------------------------------
commit:b717cfc
/////////////////////////////////////////////////////////////////////////
1:         if (sb == null)
1:           throw new IOException(String.format("Unable to load s_omega from path %s.", sbPathStr));
commit:175701c
/////////////////////////////////////////////////////////////////////////
1: import org.apache.mahout.math.function.Functions;
/////////////////////////////////////////////////////////////////////////
1:   public static final String PROP_SB_PATH = "ssvdpca.sb.path";
1:   public static final String PROP_AROWBLOCK_SIZE =
1:     QRFirstStep.PROP_AROWBLOCK_SIZE;
/////////////////////////////////////////////////////////////////////////
1:     private Vector sb;
1:       Configuration conf = context.getConfiguration();
1:       int k = Integer.parseInt(conf.get(PROP_K));
1:       int p = Integer.parseInt(conf.get(PROP_P));
1:       kp = k + p;
1:       long omegaSeed = Long.parseLong(conf.get(PROP_OMEGA_SEED));
1:       omega = new Omega(omegaSeed, k + p);
1: 
1:       String sbPathStr = conf.get(PROP_SB_PATH);
1:       if (sbPathStr != null) {
1:         sb = SSVDHelper.loadAndSumUpVectors(new Path(sbPathStr), conf);
1:       }
1: 
1:       outputs = new MultipleOutputs(new JobConf(conf));
/////////////////////////////////////////////////////////////////////////
1: 
/////////////////////////////////////////////////////////////////////////
1: 
/////////////////////////////////////////////////////////////////////////
1:       qr = new QRFirstStep(conf, qhatCollector, rhatCollector);
1:       yRow = new DenseVector(kp);
1: 
1:       if (sb != null) {
1:         yRow.assign(sb, Functions.MINUS);
1:       }
1:     protected void cleanup(Context context) throws IOException,
1:       InterruptedException {
1:                          Path sbPath,
/////////////////////////////////////////////////////////////////////////
1:     MultipleOutputs.addNamedOutput(oldApiJob,
1:                                    OUTPUT_QHAT,
1:                                    org.apache.hadoop.mapred.SequenceFileOutputFormat.class,
1:                                    SplitPartitionedWritable.class,
1:                                    DenseBlockWritable.class);
1:     MultipleOutputs.addNamedOutput(oldApiJob,
1:                                    OUTPUT_RHAT,
1:                                    org.apache.hadoop.mapred.SequenceFileOutputFormat.class,
1:                                    SplitPartitionedWritable.class,
1:                                    VectorWritable.class);
/////////////////////////////////////////////////////////////////////////
1:     if (sbPath != null) {
1:       job.getConfiguration().set(PROP_SB_PATH, sbPath.toString());
1:     }
commit:5a2250c
/////////////////////////////////////////////////////////////////////////
/////////////////////////////////////////////////////////////////////////
1:     /*
1:      * number of reduce tasks doesn't matter. we don't actually send anything to
1:      * reducers.
1:      */
commit:ffc7fab
/////////////////////////////////////////////////////////////////////////
1: import org.apache.hadoop.mapred.OutputCollector;
/////////////////////////////////////////////////////////////////////////
1: import org.apache.mahout.math.Vector;
1: import org.apache.mahout.math.hadoop.stochasticsvd.qr.QRFirstStep;
/////////////////////////////////////////////////////////////////////////
1:   public static final String PROP_K = QRFirstStep.PROP_K;
1:   public static final String PROP_P = QRFirstStep.PROP_P;
0:   public static final String PROP_AROWBLOCK_SIZE = QRFirstStep.PROP_AROWBLOCK_SIZE;
1:   public static final String OUTPUT_RHAT = "R";
1:   public static class QMapper
1:       extends
1:       Mapper<Writable, VectorWritable, SplitPartitionedWritable, VectorWritable> {
1:     private SplitPartitionedWritable qHatKey;
1:     private SplitPartitionedWritable rHatKey;
1:     private Vector yRow;
1:     private Omega omega;
1:     private int kp;
1:     private QRFirstStep qr;
1:     protected void setup(Context context) throws IOException,
1:       InterruptedException {
1:       
1: 
/////////////////////////////////////////////////////////////////////////
1:       qHatKey = new SplitPartitionedWritable(context);
1:       rHatKey = new SplitPartitionedWritable(context);
1:       OutputCollector<Writable, DenseBlockWritable> qhatCollector =
1:         new OutputCollector<Writable, DenseBlockWritable>() {
1: 
1:           @Override
1:           @SuppressWarnings("unchecked")
1:           public void collect(Writable nil, DenseBlockWritable dbw)
1:             throws IOException {
1:             outputs.getCollector(OUTPUT_QHAT, null).collect(qHatKey, dbw);
1:             qHatKey.incrementItemOrdinal();
1:           }
1:         };
1:       OutputCollector<Writable, VectorWritable> rhatCollector =
1:         new OutputCollector<Writable, VectorWritable>() {
1: 
1:           @Override
1:           @SuppressWarnings("unchecked")
1:           public void collect(Writable nil, VectorWritable rhat)
1:             throws IOException {
1:             outputs.getCollector(OUTPUT_RHAT, null).collect(rHatKey, rhat);
1:             rHatKey.incrementItemOrdinal();
1:           }
1:         };
1: 
0:       qr =
0:         new QRFirstStep(context.getConfiguration(),
0:                         qhatCollector,
0:                         rhatCollector);
0:       closeables.addFirst(qr);// important: qr closes first!!
0:       yRow=new DenseVector(kp);
1:     }
1:     
1:     @Override
1:     protected void map(Writable key, VectorWritable value, Context context)
1:       throws IOException, InterruptedException {
0:       // omega.computeYRow(value.get(), yRow);
1:       omega.computeYRow(value.get(), yRow);
1:       qr.collect(key, yRow);
1: 
1: 
0:     protected void cleanup(Context context) throws IOException,
0:       InterruptedException {
/////////////////////////////////////////////////////////////////////////
1:                          int numReduceTasks) throws ClassNotFoundException,
1:     InterruptedException, IOException {
0:     MultipleOutputs
0:       .addNamedOutput(oldApiJob,
0:                       OUTPUT_QHAT,
0:                       org.apache.hadoop.mapred.SequenceFileOutputFormat.class,
0:                       SplitPartitionedWritable.class,
0:                       DenseBlockWritable.class);
0:     MultipleOutputs
0:       .addNamedOutput(oldApiJob,
0:                       OUTPUT_RHAT,
0:                       org.apache.hadoop.mapred.SequenceFileOutputFormat.class,
0:                       SplitPartitionedWritable.class,
0:                       VectorWritable.class);
/////////////////////////////////////////////////////////////////////////
1:     SequenceFileOutputFormat.setOutputCompressionType(job,
1:                                                       CompressionType.BLOCK);
1:     job.setMapOutputKeyClass(SplitPartitionedWritable.class);
1:     job.setOutputKeyClass(SplitPartitionedWritable.class);
/////////////////////////////////////////////////////////////////////////
1: 
commit:b479bd2
/////////////////////////////////////////////////////////////////////////
1:  * <P>
1:  * See Mahout-376 working notes for details.
1:  * <P>
1:  * Uses some of Hadoop deprecated api wherever newer api is not available.
1:  * Hence, @SuppressWarnings("deprecation") for imports (MAHOUT-593).
1:  * <P>
1: @SuppressWarnings("deprecation")
/////////////////////////////////////////////////////////////////////////
1: 
0:   // public static final String OUTPUT_BT = "Bt";
/////////////////////////////////////////////////////////////////////////
0:                                                  // a sparse row matrix,
/////////////////////////////////////////////////////////////////////////
0:         outputQHat(key, value);
0:         outputR(key, new VectorWritable(new DenseVector(qSolver.getRTilde().getData(), true)));
0:     @SuppressWarnings("unchecked")
0:     private void outputQHat(Writable key, Writable value) throws IOException {
0:       outputs.getCollector(OUTPUT_QHAT, null).collect(key, value);
1:     }
1: 
0:     @SuppressWarnings("unchecked")
0:     private void outputR(Writable key, Writable value) throws IOException {
0:       outputs.getCollector(OUTPUT_R, null).collect(key, value);
1:     }
1: 
/////////////////////////////////////////////////////////////////////////
0:         value.setBlock(GivensThinSolver.computeQtHat(value.getBlock(), qCnt,
0:             new CopyConstructorIterator<UpperTriangular>(rSubseq.iterator())));
/////////////////////////////////////////////////////////////////////////
0:         outputQHat(key, value);
0:       outputR(key, new VectorWritable(new DenseVector(rSubseq.get(0).getData(), true)));
0:     protected void map(Writable key, VectorWritable value, Context context) throws IOException, InterruptedException {
/////////////////////////////////////////////////////////////////////////
0:         tempQw = SequenceFile.createWriter(localFs, context.getConfiguration(), tempQPath, IntWritable.class,
/////////////////////////////////////////////////////////////////////////
0:     MultipleOutputs.addNamedOutput(oldApiJob, OUTPUT_QHAT, org.apache.hadoop.mapred.SequenceFileOutputFormat.class,
0:     MultipleOutputs.addNamedOutput(oldApiJob, OUTPUT_R, org.apache.hadoop.mapred.SequenceFileOutputFormat.class,
/////////////////////////////////////////////////////////////////////////
0:     SequenceFileOutputFormat.setOutputCompressionType(job, CompressionType.BLOCK);
/////////////////////////////////////////////////////////////////////////
0:     // send anything to reducers.
1:     
commit:151de0d
/////////////////////////////////////////////////////////////////////////
1: /**
1:  * Licensed to the Apache Software Foundation (ASF) under one or more
1:  * contributor license agreements.  See the NOTICE file distributed with
1:  * this work for additional information regarding copyright ownership.
1:  * The ASF licenses this file to You under the Apache License, Version 2.0
1:  * (the "License"); you may not use this file except in compliance with
1:  * the License.  You may obtain a copy of the License at
1:  *
1:  *     http://www.apache.org/licenses/LICENSE-2.0
1:  *
1:  * Unless required by applicable law or agreed to in writing, software
1:  * distributed under the License is distributed on an "AS IS" BASIS,
1:  * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
1:  * See the License for the specific language governing permissions and
1:  * limitations under the License.
1:  */
1: 
1: package org.apache.mahout.math.hadoop.stochasticsvd;
1: 
1: import java.io.Closeable;
0: import java.io.DataInput;
0: import java.io.DataOutput;
0: import java.io.File;
1: import java.io.IOException;
0: import java.util.ArrayList;
0: import java.util.LinkedList;
0: import java.util.List;
1: 
1: import org.apache.hadoop.conf.Configuration;
0: import org.apache.hadoop.fs.FileSystem;
1: import org.apache.hadoop.fs.Path;
0: import org.apache.hadoop.io.IntWritable;
0: import org.apache.hadoop.io.SequenceFile;
1: import org.apache.hadoop.io.SequenceFile.CompressionType;
1: import org.apache.hadoop.io.Writable;
0: import org.apache.hadoop.io.WritableComparable;
1: import org.apache.hadoop.io.compress.DefaultCodec;
1: import org.apache.hadoop.mapred.JobConf;
1: import org.apache.hadoop.mapred.lib.MultipleOutputs;
1: import org.apache.hadoop.mapreduce.Job;
1: import org.apache.hadoop.mapreduce.Mapper;
1: import org.apache.hadoop.mapreduce.lib.input.FileInputFormat;
1: import org.apache.hadoop.mapreduce.lib.input.SequenceFileInputFormat;
1: import org.apache.hadoop.mapreduce.lib.output.FileOutputFormat;
1: import org.apache.hadoop.mapreduce.lib.output.SequenceFileOutputFormat;
1: import org.apache.mahout.common.IOUtils;
1: import org.apache.mahout.math.DenseVector;
1: import org.apache.mahout.math.VectorWritable;
1: 
1: /**
1:  * Compute first level of QHat-transpose blocks.
1:  * 
0:  * See Mahout-376 woking notes for details.
1:  * 
1:  * 
1:  */
0: @SuppressWarnings("deprecation")
0: public class QJob {
1: 
1:   public static final String PROP_OMEGA_SEED = "ssvd.omegaseed";
0:   public static final String PROP_K = "ssvd.k";
0:   public static final String PROP_P = "ssvd.p";
0:   public static final String PROP_AROWBLOCK_SIZE = "ssvd.arowblock.size";
1: 
0:   public static final String OUTPUT_R = "R";
1:   public static final String OUTPUT_QHAT = "QHat";
0:   // public static final String OUTPUT_Q="Q";
0:   public static final String OUTPUT_BT = "Bt";
1: 
0:   public static class QJobKeyWritable implements
0:       WritableComparable<QJobKeyWritable> {
1: 
0:     private int taskId;
0:     private int taskRowOrdinal;
1: 
1:     @Override
0:     public void readFields(DataInput in) throws IOException {
0:       taskId = in.readInt();
0:       taskRowOrdinal = in.readInt();
1:     }
1: 
1:     @Override
0:     public void write(DataOutput out) throws IOException {
0:       out.writeInt(taskId);
0:       out.writeInt(taskRowOrdinal);
1:     }
1: 
1:     @Override
0:     public int compareTo(QJobKeyWritable o) {
0:       if (taskId < o.taskId)
0:         return -1;
0:       else if (taskId > o.taskId)
0:         return 1;
0:       if (taskRowOrdinal < o.taskRowOrdinal)
0:         return -1;
0:       else if (taskRowOrdinal > o.taskRowOrdinal)
0:         return 1;
0:       return 0;
1:     }
1: 
1:   }
1: 
0:   public static class QMapper extends
0:       Mapper<Writable, VectorWritable, QJobKeyWritable, VectorWritable> {
1: 
0:     private int kp;
0:     private Omega omega;
0:     private List<double[]> yLookahead;
0:     private GivensThinSolver qSolver;
0:     private int blockCnt;
0:     // private int m_reducerCount;
0:     private int r;
0:     private DenseBlockWritable value = new DenseBlockWritable();
0:     private QJobKeyWritable key = new QJobKeyWritable();
0:     private IntWritable tempKey = new IntWritable();
1:     private MultipleOutputs outputs;
0:     private LinkedList<Closeable> closeables = new LinkedList<Closeable>();
0:     private SequenceFile.Writer tempQw;
0:     private Path tempQPath;
0:     private List<UpperTriangular> rSubseq = new ArrayList<UpperTriangular>();
1: 
0:     private void flushSolver(Context context) throws IOException,
0:         InterruptedException {
0:       UpperTriangular r = qSolver.getRTilde();
0:       double[][] qt = qSolver.getThinQtTilde();
1: 
0:       rSubseq.add(r);
1: 
0:       value.setBlock(qt);
0:       getTempQw(context).append(tempKey, value); // this probably should be
0:                                                      // a sparse row matrix,
0:       // but compressor should get it for disk and in memory we want it
0:       // dense anyway, sparse random implementations would be
0:       // a mostly a memory management disaster consisting of rehashes and GC
0:       // thrashing. (IMHO)
0:       value.setBlock(null);
0:       qSolver.reset();
1:     }
1: 
0:     // second pass to run a modified version of computeQHatSequence.
0:     @SuppressWarnings("unchecked")
0:     private void flushQBlocks(Context ctx) throws IOException,
0:         InterruptedException {
0:       if (blockCnt == 1) {
0:         // only one block, no temp file, no second pass. should be the default
0:         // mode
0:         // for efficiency in most cases. Sure mapper should be able to load
0:         // the entire split in memory -- and we don't require even that.
0:         value.setBlock(qSolver.getThinQtTilde());
0:         outputs.getCollector(OUTPUT_QHAT, null).collect(key, value);
0:         outputs.getCollector(OUTPUT_R, null).collect(
0:             key,
0:             new VectorWritable(new DenseVector(qSolver.getRTilde().getData(),
0:                 true)));
1: 
0:       } else
0:         secondPass(ctx);
1:     }
1: 
0:     @SuppressWarnings("unchecked")
0:     private void secondPass(Context ctx) throws IOException,
0:         InterruptedException {
0:       qSolver = null; // release mem
0:       FileSystem localFs = FileSystem.getLocal(ctx.getConfiguration());
0:       SequenceFile.Reader tempQr = new SequenceFile.Reader(localFs,
0:           tempQPath, ctx.getConfiguration());
0:       closeables.addFirst(tempQr);
0:       int qCnt = 0;
0:       while (tempQr.next(tempKey, value)) {
0:         value
0:             .setBlock(GivensThinSolver.computeQtHat(value.getBlock(), qCnt,
0:                 new GivensThinSolver.DeepCopyUTIterator(rSubseq.iterator())));
0:         if (qCnt == 1) // just merge r[0] <- r[1] so it doesn't have to repeat
0:                        // in subsequent computeQHat iterators
0:           GivensThinSolver.mergeR(rSubseq.get(0), rSubseq.remove(1));
1: 
0:         else
0:           qCnt++;
0:         outputs.getCollector(OUTPUT_QHAT, null).collect(key, value);
1:       }
1: 
0:       assert rSubseq.size() == 1;
1: 
0:       // m_value.setR(m_rSubseq.get(0));
0:       outputs.getCollector(OUTPUT_R, null)
0:           .collect(
0:               key,
0:               new VectorWritable(new DenseVector(rSubseq.get(0).getData(),
0:                   true)));
1: 
1:     }
1: 
1:     @Override
0:     protected void map(Writable key, VectorWritable value, Context context)
0:       throws IOException, InterruptedException {
0:       double[] yRow = null;
0:       if (yLookahead.size() == kp) {
0:         if (qSolver.isFull()) {
1: 
0:           flushSolver(context);
0:           blockCnt++;
1: 
1:         }
0:         yRow = yLookahead.remove(0);
1: 
0:         qSolver.appendRow(yRow);
0:       } else
0:         yRow = new double[kp];
0:       omega.computeYRow(value.get(), yRow);
0:       yLookahead.add(yRow);
1:     }
1: 
1:     @Override
0:     protected void setup(final Context context) throws IOException,
0:         InterruptedException {
1: 
0:       int k = Integer.parseInt(context.getConfiguration().get(PROP_K));
0:       int p = Integer.parseInt(context.getConfiguration().get(PROP_P));
0:       kp = k + p;
0:       long omegaSeed = Long.parseLong(context.getConfiguration().get(
0:           PROP_OMEGA_SEED));
0:       r = Integer.parseInt(context.getConfiguration()
0:           .get(PROP_AROWBLOCK_SIZE));
0:       omega = new Omega(omegaSeed, k, p);
0:       yLookahead = new ArrayList<double[]>(kp);
0:       qSolver = new GivensThinSolver(r, kp);
0:       outputs = new MultipleOutputs(new JobConf(context.getConfiguration()));
1:       closeables.addFirst(new Closeable() {
1:         @Override
1:         public void close() throws IOException {
1:           outputs.close();
1:         }
1:       });
1: 
1:     }
1: 
1:     @Override
0:     protected void cleanup(Context context) throws IOException,
0:         InterruptedException {
0:       try {
0:         if (qSolver == null && yLookahead.size() == 0)
0:           return;
0:         if (qSolver == null)
0:           qSolver = new GivensThinSolver(yLookahead.size(), kp);
0:         // grow q solver up if necessary
1: 
0:         qSolver.adjust(qSolver.getCnt() + yLookahead.size());
0:         while (yLookahead.size() > 0) {
1: 
0:           qSolver.appendRow(yLookahead.remove(0));
1: 
1:         }
0:         assert qSolver.isFull();
0:         if (++blockCnt > 1) {
0:           flushSolver(context);
0:           assert tempQw != null;
0:           closeables.remove(tempQw);
0:           tempQw.close();
1:         }
0:         flushQBlocks(context);
1: 
0:       } finally {
1:         IOUtils.close(closeables);
1:       }
1: 
1:     }
1: 
0:     private SequenceFile.Writer getTempQw(Context context) throws IOException {
0:       if (tempQw == null) {
0:         // temporary Q output
0:         // hopefully will not exceed size of IO cache in which case it is only
0:         // good since it
0:         // is going to be maanged by kernel, not java GC. And if IO cache is not
0:         // good enough,
0:         // then at least it is always sequential.
0:         String taskTmpDir = System.getProperty("java.io.tmpdir");
0:         FileSystem localFs = FileSystem.getLocal(context.getConfiguration());
0:         tempQPath = new Path(new Path(taskTmpDir), "q-temp.seq");
0:         tempQw = SequenceFile.createWriter(localFs,
0:             context.getConfiguration(), tempQPath, IntWritable.class,
0:             DenseBlockWritable.class, CompressionType.BLOCK);
0:         closeables.addFirst(tempQw);
0:         closeables.addFirst(new IOUtils.DeleteFileOnClose(new File(tempQw
0:             .toString())));
1: 
1:       }
0:       return tempQw;
1:     }
1:   }
1: 
0:   public static void run(Configuration conf, Path[] inputPaths,
0:       Path outputPath, int aBlockRows, int minSplitSize, int k, int p,
0:       long seed, int numReduceTasks) throws ClassNotFoundException,
0:       InterruptedException, IOException {
1: 
1:     JobConf oldApiJob = new JobConf(conf);
0:     MultipleOutputs.addNamedOutput(oldApiJob, OUTPUT_QHAT,
0:         org.apache.hadoop.mapred.SequenceFileOutputFormat.class,
0:         QJobKeyWritable.class, DenseBlockWritable.class);
0:     MultipleOutputs.addNamedOutput(oldApiJob, OUTPUT_R,
0:         org.apache.hadoop.mapred.SequenceFileOutputFormat.class,
0:         QJobKeyWritable.class, VectorWritable.class);
1: 
1:     Job job = new Job(oldApiJob);
1:     job.setJobName("Q-job");
1:     job.setJarByClass(QJob.class);
1: 
1:     job.setInputFormatClass(SequenceFileInputFormat.class);
1:     FileInputFormat.setInputPaths(job, inputPaths);
0:     if (minSplitSize > 0)
0:       SequenceFileInputFormat.setMinInputSplitSize(job, minSplitSize);
1: 
1:     FileOutputFormat.setOutputPath(job, outputPath);
1: 
0:     SequenceFileOutputFormat.setCompressOutput(job, true);
0:     SequenceFileOutputFormat.setOutputCompressorClass(job, DefaultCodec.class);
0:     SequenceFileOutputFormat.setOutputCompressionType(job,
0:         CompressionType.BLOCK);
1: 
0:     job.setMapOutputKeyClass(QJobKeyWritable.class);
1:     job.setMapOutputValueClass(VectorWritable.class);
1: 
0:     job.setOutputKeyClass(QJobKeyWritable.class);
1:     job.setOutputValueClass(VectorWritable.class);
1: 
1:     job.setMapperClass(QMapper.class);
1: 
1:     job.getConfiguration().setInt(PROP_AROWBLOCK_SIZE, aBlockRows);
1:     job.getConfiguration().setLong(PROP_OMEGA_SEED, seed);
1:     job.getConfiguration().setInt(PROP_K, k);
1:     job.getConfiguration().setInt(PROP_P, p);
1: 
0:     // number of reduce tasks doesn't matter. we don't actually
0:     // send anything to reducers. in fact, the only reason
0:     // we need to configure reduce step is so that combiners can fire.
0:     // so reduce here is purely symbolic.
1:     job.setNumReduceTasks(0 /* numReduceTasks */);
1: 
1:     job.submit();
1:     job.waitForCompletion(false);
1: 
0:     if (!job.isSuccessful())
1:       throw new IOException("Q job unsuccessful.");
1: 
1:   }
1: 
0:   public static enum QJobCntEnum {
0:     NUM_Q_BLOCKS;
1:   }
1: 
1: }
author:Sean R. Owen
-------------------------------------------------------------------------------
commit:335a993
/////////////////////////////////////////////////////////////////////////
commit:e0ec7c1
/////////////////////////////////////////////////////////////////////////
0:     protected void cleanup(Context context) throws IOException, InterruptedException {
0:       IOUtils.close(closeables);
commit:3218e95
/////////////////////////////////////////////////////////////////////////
1: public final class QJob {
commit:b16c260
/////////////////////////////////////////////////////////////////////////
1:       FileInputFormat.setMinInputSplitSize(job, minSplitSize);
1:     FileOutputFormat.setCompressOutput(job, true);
1:     FileOutputFormat.setOutputCompressorClass(job, DefaultCodec.class);
commit:a13b4b7
/////////////////////////////////////////////////////////////////////////
1: import java.util.Deque;
/////////////////////////////////////////////////////////////////////////
0: import org.apache.mahout.common.iterator.CopyConstructorIterator;
/////////////////////////////////////////////////////////////////////////
/////////////////////////////////////////////////////////////////////////
1:   private QJob() {
0:   }
0:   // public static final String OUTPUT_Q="Q";
0:   //public static final String OUTPUT_BT = "Bt";
0: 
0:   public static class QJobKeyWritable implements WritableComparable<QJobKeyWritable> {
/////////////////////////////////////////////////////////////////////////
0:       if (taskId < o.taskId) {
0:       } else if (taskId > o.taskId) {
0:       }
0:       if (taskRowOrdinal < o.taskRowOrdinal) {
0:       } else if (taskRowOrdinal > o.taskRowOrdinal) {
0:       }
0:   public static class QMapper extends Mapper<Writable, VectorWritable, QJobKeyWritable, VectorWritable> {
/////////////////////////////////////////////////////////////////////////
0:     private final DenseBlockWritable value = new DenseBlockWritable();
0:     private final QJobKeyWritable key = new QJobKeyWritable();
0:     private final Writable tempKey = new IntWritable();
0:     private final Deque<Closeable> closeables = new LinkedList<Closeable>();
0:     private final List<UpperTriangular> rSubseq = new ArrayList<UpperTriangular>();
0:     private void flushSolver(Context context) throws IOException {
/////////////////////////////////////////////////////////////////////////
0:     private void flushQBlocks(Context ctx) throws IOException {
/////////////////////////////////////////////////////////////////////////
0:       } else {
0:       }
0:     private void secondPass(Context ctx) throws IOException {
0:       SequenceFile.Reader tempQr = new SequenceFile.Reader(localFs, tempQPath, ctx.getConfiguration());
0:         value.setBlock(GivensThinSolver.computeQtHat(value.getBlock(),
0:                                                      qCnt,
0:                                                      new CopyConstructorIterator<UpperTriangular>(rSubseq.iterator())));
0:         if (qCnt == 1) {
0:           // just merge r[0] <- r[1] so it doesn't have to repeat
0:           // in subsequent computeQHat iterators
0:         } else {
0:         }
0:       outputs.getCollector(OUTPUT_R, null).collect(
0:           key,
0:           new VectorWritable(new DenseVector(rSubseq.get(0).getData(),
0:                                              true)));
0:       double[] yRow;
/////////////////////////////////////////////////////////////////////////
0:       } else {
0:       }
0:     protected void setup(Context context) throws IOException, InterruptedException {
0:       long omegaSeed = Long.parseLong(context.getConfiguration().get(PROP_OMEGA_SEED));
0:       r = Integer.parseInt(context.getConfiguration().get(PROP_AROWBLOCK_SIZE));
/////////////////////////////////////////////////////////////////////////
0:     protected void cleanup(Context context) throws IOException, InterruptedException {
0:         if (qSolver == null && yLookahead.isEmpty()) {
0:         }
0:         if (qSolver == null) {
0:         }
0:         while (!yLookahead.isEmpty()) {
/////////////////////////////////////////////////////////////////////////
0:         closeables.addFirst(new IOUtils.DeleteFileOnClose(new File(tempQPath.toString())));
1:   public static void run(Configuration conf,
1:                          Path[] inputPaths,
1:                          Path outputPath,
1:                          int aBlockRows,
1:                          int minSplitSize,
1:                          int k,
1:                          int p,
1:                          long seed,
0:                          int numReduceTasks) throws ClassNotFoundException, InterruptedException, IOException {
/////////////////////////////////////////////////////////////////////////
1:     if (minSplitSize > 0) {
0:     }
/////////////////////////////////////////////////////////////////////////
1:     if (!job.isSuccessful()) {
0:     }
author:Sebastian Schelter
-------------------------------------------------------------------------------
commit:58cc1ae
/////////////////////////////////////////////////////////////////////////
1: import com.google.common.collect.Lists;
/////////////////////////////////////////////////////////////////////////
1:     private final Deque<Closeable> closeables = Lists.newLinkedList();
commit:74f849b
/////////////////////////////////////////////////////////////////////////
0: import com.google.common.collect.Lists;
/////////////////////////////////////////////////////////////////////////
0:     private final List<UpperTriangular> rSubseq = Lists.newArrayList();
/////////////////////////////////////////////////////////////////////////
0:       yLookahead = Lists.newArrayListWithCapacity(kp);
commit:d608a88
/////////////////////////////////////////////////////////////////////////
0: import com.google.common.io.Closeables;
/////////////////////////////////////////////////////////////////////////
0:           Closeables.closeQuietly(tempQw);
author:tcp
-------------------------------------------------------------------------------
commit:44459bd
/////////////////////////////////////////////////////////////////////////
1:       closeables.addFirst(qr); // important: qr closes first!!
============================================================================