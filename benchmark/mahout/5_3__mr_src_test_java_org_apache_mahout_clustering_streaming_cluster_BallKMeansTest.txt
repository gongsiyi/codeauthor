1:fd24254: /*
1:fd24254:  * Licensed to the Apache Software Foundation (ASF) under one or more
1:fd24254:  * contributor license agreements.  See the NOTICE file distributed with
1:fd24254:  * this work for additional information regarding copyright ownership.
1:fd24254:  * The ASF licenses this file to You under the Apache License, Version 2.0
1:fd24254:  * (the "License"); you may not use this file except in compliance with
1:fd24254:  * the License.  You may obtain a copy of the License at
1:fd24254:  *
1:fd24254:  *     http://www.apache.org/licenses/LICENSE-2.0
1:fd24254:  *
1:fd24254:  * Unless required by applicable law or agreed to in writing, software
1:fd24254:  * distributed under the License is distributed on an "AS IS" BASIS,
1:fd24254:  * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
1:fd24254:  * See the License for the specific language governing permissions and
1:fd24254:  * limitations under the License.
1:fd24254:  */
1:fd24254: 
1:fd24254: package org.apache.mahout.clustering.streaming.cluster;
1:fd24254: 
1:fd24254: import java.util.List;
1:fd24254: 
1:fd24254: import com.google.common.collect.Lists;
1:fd24254: import org.apache.mahout.clustering.ClusteringUtils;
1:fd24254: import org.apache.mahout.common.Pair;
1:9abf755: import org.apache.mahout.common.RandomUtils;
1:fd24254: import org.apache.mahout.common.distance.EuclideanDistanceMeasure;
1:fd24254: import org.apache.mahout.common.distance.SquaredEuclideanDistanceMeasure;
1:fd24254: import org.apache.mahout.math.Centroid;
1:fd24254: import org.apache.mahout.math.ConstantVector;
1:fd24254: import org.apache.mahout.math.DenseMatrix;
1:fd24254: import org.apache.mahout.math.DenseVector;
1:fd24254: import org.apache.mahout.math.Matrix;
1:fd24254: import org.apache.mahout.math.SingularValueDecomposition;
1:fd24254: import org.apache.mahout.math.Vector;
1:fd24254: import org.apache.mahout.math.WeightedVector;
1:fd24254: import org.apache.mahout.math.function.Functions;
1:fd24254: import org.apache.mahout.math.function.VectorFunction;
1:fd24254: import org.apache.mahout.math.neighborhood.BruteSearch;
1:fd24254: import org.apache.mahout.math.neighborhood.Searcher;
1:fd24254: import org.apache.mahout.math.neighborhood.UpdatableSearcher;
1:fd24254: import org.apache.mahout.math.random.MultiNormal;
1:fd24254: import org.apache.mahout.math.random.WeightedThing;
1:fd24254: import org.apache.mahout.math.stats.OnlineSummarizer;
1:9abf755: import org.junit.Before;
1:9abf755: import org.junit.BeforeClass;
1:fd24254: import org.junit.Test;
1:fd24254: 
1:fd24254: import static org.apache.mahout.clustering.ClusteringUtils.totalWeight;
1:fd24254: import static org.junit.Assert.assertEquals;
1:fd24254: import static org.junit.Assert.assertTrue;
1:fd24254: 
1:fd24254: public class BallKMeansTest {
1:fd24254:   private static final int NUM_DATA_POINTS = 10000;
1:fd24254:   private static final int NUM_DIMENSIONS = 4;
1:fd24254:   private static final int NUM_ITERATIONS = 20;
1:fd24254:   private static final double DISTRIBUTION_RADIUS = 0.01;
1:fd24254: 
1:9abf755:   @BeforeClass
1:9abf755:   public static void setUp() {
1:9abf755:     RandomUtils.useTestSeed();
1:9abf755:     syntheticData = DataUtils.sampleMultiNormalHypercube(NUM_DIMENSIONS, NUM_DATA_POINTS, DISTRIBUTION_RADIUS);
1:9abf755: 
1:9abf755:   }
1:9abf755: 
1:9abf755:   private static Pair<List<Centroid>, List<Centroid>> syntheticData;
1:fd24254:   private static final int K1 = 100;
1:9abf755: 
1:fd24254: 
1:fd24254:   @Test
1:fd24254:   public void testClusteringMultipleRuns() {
1:fd24254:     for (int i = 1; i <= 10; ++i) {
1:fd24254:       BallKMeans clusterer = new BallKMeans(new BruteSearch(new SquaredEuclideanDistanceMeasure()),
1:fd24254:           1 << NUM_DIMENSIONS, NUM_ITERATIONS, true, i);
2:fd24254:       clusterer.cluster(syntheticData.getFirst());
1:fd24254:       double costKMeansPlusPlus = ClusteringUtils.totalClusterCost(syntheticData.getFirst(), clusterer);
1:fd24254: 
1:fd24254:       clusterer = new BallKMeans(new BruteSearch(new SquaredEuclideanDistanceMeasure()),
1:fd24254:           1 << NUM_DIMENSIONS, NUM_ITERATIONS, false, i);
1:fd24254:       clusterer.cluster(syntheticData.getFirst());
1:fd24254:       double costKMeansRandom = ClusteringUtils.totalClusterCost(syntheticData.getFirst(), clusterer);
1:fd24254: 
1:fd24254:       System.out.printf("%d runs; kmeans++: %f; random: %f\n", i, costKMeansPlusPlus, costKMeansRandom);
1:fd24254:       assertTrue("kmeans++ cost should be less than random cost", costKMeansPlusPlus < costKMeansRandom);
1:fd24254:     }
1:fd24254:   }
1:fd24254: 
1:fd24254:   @Test
1:fd24254:   public void testClustering() {
1:fd24254:     UpdatableSearcher searcher = new BruteSearch(new SquaredEuclideanDistanceMeasure());
1:fd24254:     BallKMeans clusterer = new BallKMeans(searcher, 1 << NUM_DIMENSIONS, NUM_ITERATIONS);
1:fd24254: 
1:fd24254:     long startTime = System.currentTimeMillis();
1:9abf755:     Pair<List<Centroid>, List<Centroid>> data = syntheticData;
1:9abf755:     clusterer.cluster(data.getFirst());
1:fd24254:     long endTime = System.currentTimeMillis();
1:fd24254: 
1:9abf755:     long hash = 0;
1:9abf755:     for (Centroid centroid : data.getFirst()) {
1:9abf755:       for (Vector.Element element : centroid.all()) {
1:9abf755:         hash = 31 * hash + 17 * element.index() + Double.toHexString(element.get()).hashCode();
1:9abf755:       }
1:9abf755:     }
1:9abf755:     System.out.printf("Hash = %08x\n", hash);
1:9abf755: 
1:4ca6b86:     assertEquals("Total weight not preserved", totalWeight(syntheticData.getFirst()), totalWeight(clusterer), 1.0e-9);
1:fd24254: 
1:fd24254:     // Verify that each corner of the cube has a centroid very nearby.
1:fd24254:     // This is probably FALSE for large-dimensional spaces!
1:fd24254:     OnlineSummarizer summarizer = new OnlineSummarizer();
1:fd24254:     for (Vector mean : syntheticData.getSecond()) {
1:fd24254:       WeightedThing<Vector> v = searcher.search(mean, 1).get(0);
1:fd24254:       summarizer.add(v.getWeight());
1:fd24254:     }
1:fd24254:     assertTrue(String.format("Median weight [%f] too large [>%f]", summarizer.getMedian(),
1:fd24254:         DISTRIBUTION_RADIUS), summarizer.getMedian() < DISTRIBUTION_RADIUS);
1:fd24254: 
1:fd24254:     double clusterTime = (endTime - startTime) / 1000.0;
1:fd24254:     System.out.printf("%s\n%.2f for clustering\n%.1f us per row\n\n",
1:fd24254:         searcher.getClass().getName(), clusterTime,
1:4ca6b86:         clusterTime / syntheticData.getFirst().size() * 1.0e6);
1:fd24254: 
1:fd24254:     // Verify that the total weight of the centroids near each corner is correct.
1:fd24254:     double[] cornerWeights = new double[1 << NUM_DIMENSIONS];
1:fd24254:     Searcher trueFinder = new BruteSearch(new EuclideanDistanceMeasure());
1:fd24254:     for (Vector trueCluster : syntheticData.getSecond()) {
1:fd24254:       trueFinder.add(trueCluster);
1:fd24254:     }
1:fd24254:     for (Centroid centroid : clusterer) {
1:fd24254:       WeightedThing<Vector> closest = trueFinder.search(centroid, 1).get(0);
1:fd24254:       cornerWeights[((Centroid)closest.getValue()).getIndex()] += centroid.getWeight();
1:fd24254:     }
1:fd24254:     int expectedNumPoints = NUM_DATA_POINTS / (1 << NUM_DIMENSIONS);
1:fd24254:     for (double v : cornerWeights) {
1:fd24254:       System.out.printf("%f ", v);
1:fd24254:     }
1:fd24254:     System.out.println();
1:fd24254:     for (double v : cornerWeights) {
1:fd24254:       assertEquals(expectedNumPoints, v, 0);
1:fd24254:     }
1:fd24254:   }
1:fd24254: 
1:fd24254:   @Test
1:fd24254:   public void testInitialization() {
1:fd24254:     // Start with super clusterable data.
1:fd24254:     List<? extends WeightedVector> data = cubishTestData(0.01);
1:fd24254: 
1:fd24254:     // Just do initialization of ball k-means. This should drop a point into each of the clusters.
1:fd24254:     BallKMeans r = new BallKMeans(new BruteSearch(new SquaredEuclideanDistanceMeasure()), 6, 20);
1:fd24254:     r.cluster(data);
1:fd24254: 
1:fd24254:     // Put the centroids into a matrix.
1:fd24254:     Matrix x = new DenseMatrix(6, 5);
1:fd24254:     int row = 0;
1:fd24254:     for (Centroid c : r) {
1:fd24254:       x.viewRow(row).assign(c.viewPart(0, 5));
1:fd24254:       row++;
1:fd24254:     }
1:fd24254: 
1:fd24254:     // Verify that each column looks right. Should contain zeros except for a single 6.
1:fd24254:     final Vector columnNorms = x.aggregateColumns(new VectorFunction() {
1:fd24254:       @Override
1:fd24254:       public double apply(Vector f) {
1:fd24254:         // Return the sum of three discrepancy measures.
1:fd24254:         return Math.abs(f.minValue()) + Math.abs(f.maxValue() - 6) + Math.abs(f.norm(1) - 6);
1:fd24254:       }
1:fd24254:     });
1:fd24254:     // Verify all errors are nearly zero.
1:fd24254:     assertEquals(0, columnNorms.norm(1) / columnNorms.size(), 0.1);
1:fd24254: 
1:fd24254:     // Verify that the centroids are a permutation of the original ones.
1:fd24254:     SingularValueDecomposition svd = new SingularValueDecomposition(x);
1:fd24254:     Vector s = svd.getS().viewDiagonal().assign(Functions.div(6));
1:fd24254:     assertEquals(5, s.getLengthSquared(), 0.05);
1:fd24254:     assertEquals(5, s.norm(1), 0.05);
1:fd24254:   }
1:fd24254: 
1:4ca6b86:   private static List<? extends WeightedVector> cubishTestData(double radius) {
1:fd24254:     List<WeightedVector> data = Lists.newArrayListWithCapacity(K1 + 5000);
1:fd24254:     int row = 0;
1:fd24254: 
1:fd24254:     MultiNormal g = new MultiNormal(radius, new ConstantVector(0, 10));
1:fd24254:     for (int i = 0; i < K1; i++) {
1:fd24254:       data.add(new WeightedVector(g.sample(), 1, row++));
1:fd24254:     }
1:fd24254: 
1:fd24254:     for (int i = 0; i < 5; i++) {
1:fd24254:       Vector m = new DenseVector(10);
1:4ca6b86:       m.set(i, 6); // This was originally i == 0 ? 6 : 6 which can't be right
1:fd24254:       MultiNormal gx = new MultiNormal(radius, m);
1:fd24254:       for (int j = 0; j < 1000; j++) {
1:fd24254:         data.add(new WeightedVector(gx.sample(), 1, row++));
1:fd24254:       }
1:fd24254:     }
1:fd24254:     return data;
1:fd24254:   }
1:fd24254: }
============================================================================
author:pferrel
-------------------------------------------------------------------------------
commit:b988c49
author:frankscholten
-------------------------------------------------------------------------------
commit:1a42d85
author:sslavic
-------------------------------------------------------------------------------
commit:9abf755
/////////////////////////////////////////////////////////////////////////
1: import org.apache.mahout.common.RandomUtils;
/////////////////////////////////////////////////////////////////////////
1: import org.junit.Before;
1: import org.junit.BeforeClass;
/////////////////////////////////////////////////////////////////////////
1:   @BeforeClass
1:   public static void setUp() {
1:     RandomUtils.useTestSeed();
1:     syntheticData = DataUtils.sampleMultiNormalHypercube(NUM_DIMENSIONS, NUM_DATA_POINTS, DISTRIBUTION_RADIUS);
1: 
1:   }
1: 
1:   private static Pair<List<Centroid>, List<Centroid>> syntheticData;
1: 
/////////////////////////////////////////////////////////////////////////
1:     Pair<List<Centroid>, List<Centroid>> data = syntheticData;
1:     clusterer.cluster(data.getFirst());
1:     long hash = 0;
1:     for (Centroid centroid : data.getFirst()) {
1:       for (Vector.Element element : centroid.all()) {
1:         hash = 31 * hash + 17 * element.index() + Double.toHexString(element.get()).hashCode();
1:       }
1:     }
1:     System.out.printf("Hash = %08x\n", hash);
1: 
author:Sean R. Owen
-------------------------------------------------------------------------------
commit:4ca6b86
/////////////////////////////////////////////////////////////////////////
1:     assertEquals("Total weight not preserved", totalWeight(syntheticData.getFirst()), totalWeight(clusterer), 1.0e-9);
/////////////////////////////////////////////////////////////////////////
1:         clusterTime / syntheticData.getFirst().size() * 1.0e6);
/////////////////////////////////////////////////////////////////////////
1:   private static List<? extends WeightedVector> cubishTestData(double radius) {
/////////////////////////////////////////////////////////////////////////
1:       m.set(i, 6); // This was originally i == 0 ? 6 : 6 which can't be right
author:dfilimon
-------------------------------------------------------------------------------
commit:fd24254
/////////////////////////////////////////////////////////////////////////
1: /*
1:  * Licensed to the Apache Software Foundation (ASF) under one or more
1:  * contributor license agreements.  See the NOTICE file distributed with
1:  * this work for additional information regarding copyright ownership.
1:  * The ASF licenses this file to You under the Apache License, Version 2.0
1:  * (the "License"); you may not use this file except in compliance with
1:  * the License.  You may obtain a copy of the License at
1:  *
1:  *     http://www.apache.org/licenses/LICENSE-2.0
1:  *
1:  * Unless required by applicable law or agreed to in writing, software
1:  * distributed under the License is distributed on an "AS IS" BASIS,
1:  * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
1:  * See the License for the specific language governing permissions and
1:  * limitations under the License.
1:  */
1: 
1: package org.apache.mahout.clustering.streaming.cluster;
1: 
1: import java.util.List;
1: 
1: import com.google.common.collect.Lists;
1: import org.apache.mahout.clustering.ClusteringUtils;
1: import org.apache.mahout.common.Pair;
1: import org.apache.mahout.common.distance.EuclideanDistanceMeasure;
1: import org.apache.mahout.common.distance.SquaredEuclideanDistanceMeasure;
1: import org.apache.mahout.math.Centroid;
1: import org.apache.mahout.math.ConstantVector;
1: import org.apache.mahout.math.DenseMatrix;
1: import org.apache.mahout.math.DenseVector;
1: import org.apache.mahout.math.Matrix;
1: import org.apache.mahout.math.SingularValueDecomposition;
1: import org.apache.mahout.math.Vector;
1: import org.apache.mahout.math.WeightedVector;
1: import org.apache.mahout.math.function.Functions;
1: import org.apache.mahout.math.function.VectorFunction;
1: import org.apache.mahout.math.neighborhood.BruteSearch;
1: import org.apache.mahout.math.neighborhood.Searcher;
1: import org.apache.mahout.math.neighborhood.UpdatableSearcher;
1: import org.apache.mahout.math.random.MultiNormal;
1: import org.apache.mahout.math.random.WeightedThing;
1: import org.apache.mahout.math.stats.OnlineSummarizer;
1: import org.junit.Test;
1: 
1: import static org.apache.mahout.clustering.ClusteringUtils.totalWeight;
1: import static org.junit.Assert.assertEquals;
1: import static org.junit.Assert.assertTrue;
1: 
1: public class BallKMeansTest {
1:   private static final int NUM_DATA_POINTS = 10000;
1:   private static final int NUM_DIMENSIONS = 4;
1:   private static final int NUM_ITERATIONS = 20;
1:   private static final double DISTRIBUTION_RADIUS = 0.01;
1: 
0:   private static Pair<List<Centroid>, List<Centroid>> syntheticData =
0:       DataUtils.sampleMultiNormalHypercube(NUM_DIMENSIONS, NUM_DATA_POINTS, DISTRIBUTION_RADIUS);
1:   private static final int K1 = 100;
1: 
1:   @Test
1:   public void testClusteringMultipleRuns() {
1:     for (int i = 1; i <= 10; ++i) {
1:       BallKMeans clusterer = new BallKMeans(new BruteSearch(new SquaredEuclideanDistanceMeasure()),
1:           1 << NUM_DIMENSIONS, NUM_ITERATIONS, true, i);
1:       clusterer.cluster(syntheticData.getFirst());
1:       double costKMeansPlusPlus = ClusteringUtils.totalClusterCost(syntheticData.getFirst(), clusterer);
1: 
1:       clusterer = new BallKMeans(new BruteSearch(new SquaredEuclideanDistanceMeasure()),
1:           1 << NUM_DIMENSIONS, NUM_ITERATIONS, false, i);
1:       clusterer.cluster(syntheticData.getFirst());
1:       double costKMeansRandom = ClusteringUtils.totalClusterCost(syntheticData.getFirst(), clusterer);
1: 
1:       System.out.printf("%d runs; kmeans++: %f; random: %f\n", i, costKMeansPlusPlus, costKMeansRandom);
1:       assertTrue("kmeans++ cost should be less than random cost", costKMeansPlusPlus < costKMeansRandom);
1:     }
1:   }
1: 
1:   @Test
1:   public void testClustering() {
1:     UpdatableSearcher searcher = new BruteSearch(new SquaredEuclideanDistanceMeasure());
1:     BallKMeans clusterer = new BallKMeans(searcher, 1 << NUM_DIMENSIONS, NUM_ITERATIONS);
1: 
1:     long startTime = System.currentTimeMillis();
1:     clusterer.cluster(syntheticData.getFirst());
1:     long endTime = System.currentTimeMillis();
1: 
0:     assertEquals("Total weight not preserved", totalWeight(syntheticData.getFirst()), totalWeight(clusterer), 1e-9);
1: 
1:     // Verify that each corner of the cube has a centroid very nearby.
1:     // This is probably FALSE for large-dimensional spaces!
1:     OnlineSummarizer summarizer = new OnlineSummarizer();
1:     for (Vector mean : syntheticData.getSecond()) {
1:       WeightedThing<Vector> v = searcher.search(mean, 1).get(0);
1:       summarizer.add(v.getWeight());
1:     }
1:     assertTrue(String.format("Median weight [%f] too large [>%f]", summarizer.getMedian(),
1:         DISTRIBUTION_RADIUS), summarizer.getMedian() < DISTRIBUTION_RADIUS);
1: 
1:     double clusterTime = (endTime - startTime) / 1000.0;
1:     System.out.printf("%s\n%.2f for clustering\n%.1f us per row\n\n",
1:         searcher.getClass().getName(), clusterTime,
0:         clusterTime / syntheticData.getFirst().size() * 1e6);
1: 
1:     // Verify that the total weight of the centroids near each corner is correct.
1:     double[] cornerWeights = new double[1 << NUM_DIMENSIONS];
1:     Searcher trueFinder = new BruteSearch(new EuclideanDistanceMeasure());
1:     for (Vector trueCluster : syntheticData.getSecond()) {
1:       trueFinder.add(trueCluster);
1:     }
1:     for (Centroid centroid : clusterer) {
1:       WeightedThing<Vector> closest = trueFinder.search(centroid, 1).get(0);
1:       cornerWeights[((Centroid)closest.getValue()).getIndex()] += centroid.getWeight();
1:     }
1:     int expectedNumPoints = NUM_DATA_POINTS / (1 << NUM_DIMENSIONS);
1:     for (double v : cornerWeights) {
1:       System.out.printf("%f ", v);
1:     }
1:     System.out.println();
1:     for (double v : cornerWeights) {
1:       assertEquals(expectedNumPoints, v, 0);
1:     }
1:   }
1: 
1:   @Test
1:   public void testInitialization() {
1:     // Start with super clusterable data.
1:     List<? extends WeightedVector> data = cubishTestData(0.01);
1: 
1:     // Just do initialization of ball k-means. This should drop a point into each of the clusters.
1:     BallKMeans r = new BallKMeans(new BruteSearch(new SquaredEuclideanDistanceMeasure()), 6, 20);
1:     r.cluster(data);
1: 
1:     // Put the centroids into a matrix.
1:     Matrix x = new DenseMatrix(6, 5);
1:     int row = 0;
1:     for (Centroid c : r) {
1:       x.viewRow(row).assign(c.viewPart(0, 5));
1:       row++;
1:     }
1: 
1:     // Verify that each column looks right. Should contain zeros except for a single 6.
1:     final Vector columnNorms = x.aggregateColumns(new VectorFunction() {
1:       @Override
1:       public double apply(Vector f) {
1:         // Return the sum of three discrepancy measures.
1:         return Math.abs(f.minValue()) + Math.abs(f.maxValue() - 6) + Math.abs(f.norm(1) - 6);
1:       }
1:     });
1:     // Verify all errors are nearly zero.
1:     assertEquals(0, columnNorms.norm(1) / columnNorms.size(), 0.1);
1: 
1:     // Verify that the centroids are a permutation of the original ones.
1:     SingularValueDecomposition svd = new SingularValueDecomposition(x);
1:     Vector s = svd.getS().viewDiagonal().assign(Functions.div(6));
1:     assertEquals(5, s.getLengthSquared(), 0.05);
1:     assertEquals(5, s.norm(1), 0.05);
1:   }
1: 
0:   private List<? extends WeightedVector> cubishTestData(double radius) {
1:     List<WeightedVector> data = Lists.newArrayListWithCapacity(K1 + 5000);
1:     int row = 0;
1: 
1:     MultiNormal g = new MultiNormal(radius, new ConstantVector(0, 10));
1:     for (int i = 0; i < K1; i++) {
1:       data.add(new WeightedVector(g.sample(), 1, row++));
1:     }
1: 
1:     for (int i = 0; i < 5; i++) {
1:       Vector m = new DenseVector(10);
0:       m.set(i, i == 0 ? 6 : 6);
1:       MultiNormal gx = new MultiNormal(radius, m);
1:       for (int j = 0; j < 1000; j++) {
1:         data.add(new WeightedVector(gx.sample(), 1, row++));
1:       }
1:     }
1:     return data;
1:   }
1: }
============================================================================