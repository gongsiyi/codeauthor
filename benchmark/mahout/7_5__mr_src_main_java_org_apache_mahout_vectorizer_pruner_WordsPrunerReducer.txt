1:d8e91f9: package org.apache.mahout.vectorizer.pruner;
1:d8e91f9: /**
1:d8e91f9:  * Licensed to the Apache Software Foundation (ASF) under one or more
1:d8e91f9:  * contributor license agreements.  See the NOTICE file distributed with
1:d8e91f9:  * this work for additional information regarding copyright ownership.
1:d8e91f9:  * The ASF licenses this file to You under the Apache License, Version 2.0
1:d8e91f9:  * (the "License"); you may not use this file except in compliance with
1:d8e91f9:  * the License.  You may obtain a copy of the License at
1:d8e91f9:  *
1:d8e91f9:  *     http://www.apache.org/licenses/LICENSE-2.0
1:d8e91f9:  *
1:d8e91f9:  * Unless required by applicable law or agreed to in writing, software
1:d8e91f9:  * distributed under the License is distributed on an "AS IS" BASIS,
1:d8e91f9:  * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
1:d8e91f9:  * See the License for the specific language governing permissions and
1:d8e91f9:  * limitations under the License.
1:d8e91f9:  */
2:d8e91f9: 
1:d8e91f9: import org.apache.hadoop.conf.Configuration;
1:d8e91f9: import org.apache.hadoop.fs.Path;
1:d8e91f9: import org.apache.hadoop.io.IntWritable;
1:d8e91f9: import org.apache.hadoop.io.LongWritable;
1:d8e91f9: import org.apache.hadoop.io.WritableComparable;
1:d8e91f9: import org.apache.hadoop.mapreduce.Reducer;
1:6d9179e: import org.apache.mahout.common.HadoopUtil;
1:d8e91f9: import org.apache.mahout.common.Pair;
1:d8e91f9: import org.apache.mahout.common.iterator.sequencefile.SequenceFileIterable;
1:d8e91f9: import org.apache.mahout.math.Vector;
1:d8e91f9: import org.apache.mahout.math.VectorWritable;
1:d8e91f9: import org.apache.mahout.math.map.OpenIntLongHashMap;
1:d8e91f9: import org.apache.mahout.vectorizer.HighDFWordsPruner;
1:d8e91f9: 
1:d8e91f9: import java.io.IOException;
1:d8e91f9: import java.util.Iterator;
1:d8e91f9: 
1:d8e91f9: public class WordsPrunerReducer extends
1:d8e91f9:         Reducer<WritableComparable<?>, VectorWritable, WritableComparable<?>, VectorWritable> {
1:d8e91f9: 
1:d8e91f9:   private final OpenIntLongHashMap dictionary = new OpenIntLongHashMap();
1:d1e5295:   private long maxDf = Long.MAX_VALUE;
1:d1e5295:   private long minDf = -1;
1:d8e91f9: 
1:d8e91f9:   @Override
1:d8e91f9:   protected void reduce(WritableComparable<?> key, Iterable<VectorWritable> values, Context context)
1:58cc1ae:     throws IOException, InterruptedException {
1:d8e91f9:     Iterator<VectorWritable> it = values.iterator();
1:d8e91f9:     if (!it.hasNext()) {
1:d8e91f9:       return;
1:25d59aa:     }
1:d8e91f9:     Vector value = it.next().get();
1:d8e91f9:     Vector vector = value.clone();
1:d1e5295:     if (maxDf != Long.MAX_VALUE || minDf > -1) {
1:dc62944:       for (Vector.Element e : value.nonZeroes()) {
1:d8e91f9:         if (!dictionary.containsKey(e.index())) {
1:d8e91f9:           vector.setQuick(e.index(), 0.0);
1:d8e91f9:           continue;
2:d8e91f9:         }
1:d8e91f9:         long df = dictionary.get(e.index());
1:d1e5295:         if (df > maxDf || df < minDf) {
1:d8e91f9:           vector.setQuick(e.index(), 0.0);
1:d8e91f9:         }
1:d8e91f9:       }
1:d8e91f9:     }
1:d8e91f9: 
1:d8e91f9:     VectorWritable vectorWritable = new VectorWritable(vector);
1:d8e91f9:     context.write(key, vectorWritable);
1:d8e91f9:   }
1:d8e91f9: 
1:d8e91f9:   @Override
1:d8e91f9:   protected void setup(Context context) throws IOException, InterruptedException {
1:d8e91f9:     super.setup(context);
1:d8e91f9:     Configuration conf = context.getConfiguration();
1:335a993:     //Path[] localFiles = HadoopUtil.getCachedFiles(conf);
1:d8e91f9: 
1:d1e5295:     maxDf = conf.getLong(HighDFWordsPruner.MAX_DF, Long.MAX_VALUE);
1:d1e5295:     minDf = conf.getLong(HighDFWordsPruner.MIN_DF, -1);
1:6d9179e: 
1:6d9179e:     Path dictionaryFile = HadoopUtil.getSingleCachedFile(conf);
1:6d9179e: 
1:d8e91f9:     // key is feature, value is the document frequency
1:6d16230:     for (Pair<IntWritable, LongWritable> record
1:6d16230:             : new SequenceFileIterable<IntWritable, LongWritable>(dictionaryFile, true, conf)) {
1:d8e91f9:       dictionary.put(record.getFirst().get(), record.getSecond().get());
1:d8e91f9:     }
1:d8e91f9:   }
1:d8e91f9: }
============================================================================
author:pferrel
-------------------------------------------------------------------------------
commit:b988c49
author:frankscholten
-------------------------------------------------------------------------------
commit:1a42d85
author:Sean R. Owen
-------------------------------------------------------------------------------
commit:335a993
/////////////////////////////////////////////////////////////////////////
1:     //Path[] localFiles = HadoopUtil.getCachedFiles(conf);
author:Sebastian Schelter
-------------------------------------------------------------------------------
commit:58cc1ae
/////////////////////////////////////////////////////////////////////////
1:     throws IOException, InterruptedException {
commit:6d9179e
/////////////////////////////////////////////////////////////////////////
1: import org.apache.mahout.common.HadoopUtil;
/////////////////////////////////////////////////////////////////////////
0:     Path[] localFiles = HadoopUtil.getCachedFiles(conf);
1: 
1:     Path dictionaryFile = HadoopUtil.getSingleCachedFile(conf);
1: 
commit:6d16230
/////////////////////////////////////////////////////////////////////////
0:     throws IOException, InterruptedException {
/////////////////////////////////////////////////////////////////////////
1:     for (Pair<IntWritable, LongWritable> record
1:         : new SequenceFileIterable<IntWritable, LongWritable>(dictionaryFile, true, conf)) {
author:Grant Ingersoll
-------------------------------------------------------------------------------
commit:25d59aa
/////////////////////////////////////////////////////////////////////////
0: import org.apache.hadoop.fs.FileSystem;
/////////////////////////////////////////////////////////////////////////
/////////////////////////////////////////////////////////////////////////
0:           throws IOException, InterruptedException {
/////////////////////////////////////////////////////////////////////////
0:     FileSystem fs = FileSystem.getLocal(conf);
0:     Path dictionaryFile;
0:     if (fs.exists(localFiles[0])) {
0:       dictionaryFile = fs.makeQualified(localFiles[0]);
0:     } else {//MAHOUT-992: this seems safe
0:       dictionaryFile = fs.makeQualified(new Path(DistributedCache.getCacheFiles(conf)[0].getPath()));
1:     }
0:             : new SequenceFileIterable<IntWritable, LongWritable>(dictionaryFile, true, conf)) {
commit:4a6453c
/////////////////////////////////////////////////////////////////////////
0:     Path[] localFiles = DistributedCache.getLocalCacheFiles(conf);
0:     Path dictionaryFile = localFiles[0];
commit:d8e91f9
/////////////////////////////////////////////////////////////////////////
1: package org.apache.mahout.vectorizer.pruner;
1: /**
1:  * Licensed to the Apache Software Foundation (ASF) under one or more
1:  * contributor license agreements.  See the NOTICE file distributed with
1:  * this work for additional information regarding copyright ownership.
1:  * The ASF licenses this file to You under the Apache License, Version 2.0
1:  * (the "License"); you may not use this file except in compliance with
1:  * the License.  You may obtain a copy of the License at
1:  *
1:  *     http://www.apache.org/licenses/LICENSE-2.0
1:  *
1:  * Unless required by applicable law or agreed to in writing, software
1:  * distributed under the License is distributed on an "AS IS" BASIS,
1:  * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
1:  * See the License for the specific language governing permissions and
1:  * limitations under the License.
1:  */
1: 
0: import com.google.common.base.Preconditions;
1: import org.apache.hadoop.conf.Configuration;
0: import org.apache.hadoop.filecache.DistributedCache;
1: import org.apache.hadoop.fs.Path;
1: import org.apache.hadoop.io.IntWritable;
1: import org.apache.hadoop.io.LongWritable;
1: import org.apache.hadoop.io.WritableComparable;
1: import org.apache.hadoop.mapreduce.Reducer;
1: import org.apache.mahout.common.Pair;
1: import org.apache.mahout.common.iterator.sequencefile.SequenceFileIterable;
1: import org.apache.mahout.math.Vector;
1: import org.apache.mahout.math.VectorWritable;
1: import org.apache.mahout.math.map.OpenIntLongHashMap;
1: import org.apache.mahout.vectorizer.HighDFWordsPruner;
1: 
1: import java.io.IOException;
0: import java.net.URI;
1: import java.util.Iterator;
1: 
1: public class WordsPrunerReducer extends
1:         Reducer<WritableComparable<?>, VectorWritable, WritableComparable<?>, VectorWritable> {
1: 
1:   private final OpenIntLongHashMap dictionary = new OpenIntLongHashMap();
0:   private long maxDf = -1;
1: 
1:   @Override
1:   protected void reduce(WritableComparable<?> key, Iterable<VectorWritable> values, Context context)
0:           throws IOException, InterruptedException {
1:     Iterator<VectorWritable> it = values.iterator();
1:     if (!it.hasNext()) {
1:       return;
1:     }
1:     Vector value = it.next().get();
1:     Vector vector = value.clone();
0:     if (maxDf > -1) {
0:       Iterator<Vector.Element> it1 = value.iterateNonZero();
0:       while (it1.hasNext()) {
0:         Vector.Element e = it1.next();
1:         if (!dictionary.containsKey(e.index())) {
1:           vector.setQuick(e.index(), 0.0);
1:           continue;
1:         }
1:         long df = dictionary.get(e.index());
0:         if (df > maxDf) {
1:           vector.setQuick(e.index(), 0.0);
1:         }
1:       }
1:     }
1: 
1:     VectorWritable vectorWritable = new VectorWritable(vector);
1:     context.write(key, vectorWritable);
1:   }
1: 
1:   @Override
1:   protected void setup(Context context) throws IOException, InterruptedException {
1:     super.setup(context);
1:     Configuration conf = context.getConfiguration();
0:     URI[] localFiles = DistributedCache.getCacheFiles(conf);
0:     Preconditions.checkArgument(localFiles != null && localFiles.length >= 1,
0:             "missing paths from the DistributedCache");
1: 
0:     maxDf = conf.getLong(HighDFWordsPruner.MAX_DF, -1);
1: 
0:     Path dictionaryFile = new Path(localFiles[0].getPath());
1:     // key is feature, value is the document frequency
0:     for (Pair<IntWritable, LongWritable> record :
0:             new SequenceFileIterable<IntWritable, LongWritable>(dictionaryFile, true, conf)) {
1:       dictionary.put(record.getFirst().get(), record.getSecond().get());
1:     }
1:   }
1: }
author:Robin Anil
-------------------------------------------------------------------------------
commit:d1e5295
/////////////////////////////////////////////////////////////////////////
1:   private long maxDf = Long.MAX_VALUE;
1:   private long minDf = -1;
/////////////////////////////////////////////////////////////////////////
1:     if (maxDf != Long.MAX_VALUE || minDf > -1) {
1:         if (df > maxDf || df < minDf) {
/////////////////////////////////////////////////////////////////////////
1:     maxDf = conf.getLong(HighDFWordsPruner.MAX_DF, Long.MAX_VALUE);
1:     minDf = conf.getLong(HighDFWordsPruner.MIN_DF, -1);
author:Jacob Alexander Mannix
-------------------------------------------------------------------------------
commit:dc62944
/////////////////////////////////////////////////////////////////////////
1:       for (Vector.Element e : value.nonZeroes()) {
============================================================================