1:a704e17: /*
1:a704e17:  * Licensed to the Apache Software Foundation (ASF) under one or more
1:a704e17:  * contributor license agreements.  See the NOTICE file distributed with
1:a704e17:  * this work for additional information regarding copyright ownership.
1:a704e17:  * The ASF licenses this file to You under the Apache License, Version 2.0
1:a704e17:  * (the "License"); you may not use this file except in compliance with
1:a704e17:  * the License.  You may obtain a copy of the License at
1:a704e17:  *
1:a704e17:  *     http://www.apache.org/licenses/LICENSE-2.0
1:a704e17:  *
1:a704e17:  * Unless required by applicable law or agreed to in writing, software
1:a704e17:  * distributed under the License is distributed on an "AS IS" BASIS,
1:a704e17:  * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
1:a704e17:  * See the License for the specific language governing permissions and
1:a704e17:  * limitations under the License.
1:a704e17:  */
1:a704e17: 
1:4b25f31: package org.apache.mahout.classifier.sgd;
3:4b25f31: 
1:ff79ff4: import org.apache.mahout.common.MahoutTestCase;
1:d84e4c0: import org.apache.mahout.common.RandomUtils;
1:4b25f31: import org.apache.mahout.math.DenseVector;
1:4b25f31: import org.apache.mahout.math.Vector;
1:4b25f31: import org.apache.mahout.math.jet.random.Exponential;
1:4b25f31: import org.junit.Test;
1:4b25f31: 
1:c88c240: import com.carrotsearch.randomizedtesting.annotations.ThreadLeakLingering;
1:c88c240: 
1:22726e8: import java.util.Random;
1:22726e8: 
1:ff79ff4: public final class AdaptiveLogisticRegressionTest extends MahoutTestCase {
1:ff79ff4: 
1:c88c240:   @ThreadLeakLingering(linger=1000)
1:4b25f31:   @Test
1:4b25f31:   public void testTrain() {
1:22726e8: 
1:c0c8d9f:     Random gen = RandomUtils.getRandom();
1:c0c8d9f:     Exponential exp = new Exponential(0.5, gen);
1:4b25f31:     Vector beta = new DenseVector(200);
1:dc62944:     for (Vector.Element element : beta.all()) {
1:22726e8:       int sign = 1;
1:22726e8:       if (gen.nextDouble() < 0.5) {
1:22726e8:         sign = -1;
1:22726e8:       }
1:4b25f31:       element.set(sign * exp.nextDouble());
3:4b25f31:     }
1:4b25f31: 
1:4b25f31:     AdaptiveLogisticRegression.Wrapper cl = new AdaptiveLogisticRegression.Wrapper(2, 200, new L1());
1:c0c8d9f:     cl.update(new double[]{1.0e-5, 1});
1:4b25f31: 
1:4b25f31:     for (int i = 0; i < 10000; i++) {
1:4b25f31:       AdaptiveLogisticRegression.TrainingExample r = getExample(i, gen, beta);
1:4b25f31:       cl.train(r);
2:4b25f31:       if (i % 1000 == 0) {
1:4b25f31:         System.out.printf("%10d %10.3f\n", i, cl.getLearner().auc());
1:4b25f31:       }
1:4b25f31:     }
1:2ce6410:     assertEquals(1, cl.getLearner().auc(), 0.1);
1:4b25f31: 
1:c88c240:     AdaptiveLogisticRegression adaptiveLogisticRegression = new AdaptiveLogisticRegression(2, 200, new L1());
1:c88c240:     adaptiveLogisticRegression.setInterval(1000);
1:4b25f31: 
1:4b25f31:     for (int i = 0; i < 20000; i++) {
1:4b25f31:       AdaptiveLogisticRegression.TrainingExample r = getExample(i, gen, beta);
1:c88c240:       adaptiveLogisticRegression.train(r.getKey(), r.getActual(), r.getInstance());
1:c88c240:       if (i % 1000 == 0 && adaptiveLogisticRegression.getBest() != null) {
1:3a1adb7:         System.out.printf("%10d %10.4f %10.8f %.3f\n",
1:c88c240:                           i, adaptiveLogisticRegression.auc(),
1:c88c240:                           Math.log10(adaptiveLogisticRegression.getBest().getMappedParams()[0]), adaptiveLogisticRegression.getBest().getMappedParams()[1]);
1:4b25f31:       }
1:4b25f31:     }
1:c88c240:     assertEquals(1, adaptiveLogisticRegression.auc(), 0.1);
1:c88c240:     adaptiveLogisticRegression.close();
1:4b25f31:   }
1:4b25f31: 
1:c0c8d9f:   private static AdaptiveLogisticRegression.TrainingExample getExample(int i, Random gen, Vector beta) {
3:4b25f31:     Vector data = new DenseVector(200);
1:4b25f31: 
1:dc62944:     for (Vector.Element element : data.all()) {
1:4b25f31:       element.set(gen.nextDouble() < 0.3 ? 1 : 0);
1:4b25f31:     }
1:4b25f31: 
1:4b25f31:     double p = 1 / (1 + Math.exp(1.5 - data.dot(beta)));
1:4b25f31:     int target = 0;
1:4b25f31:     if (gen.nextDouble() < p) {
1:4b25f31:       target = 1;
1:4b25f31:     }
1:e6b4e35:     return new AdaptiveLogisticRegression.TrainingExample(i, null, target, data);
1:4b25f31:   }
1:d84e4c0: 
1:4b25f31:   @Test
1:4b25f31:   public void copyLearnsAsExpected() {
1:c0c8d9f:     Random gen = RandomUtils.getRandom();
1:c0c8d9f:     Exponential exp = new Exponential(0.5, gen);
1:4b25f31:     Vector beta = new DenseVector(200);
1:dc62944:     for (Vector.Element element : beta.all()) {
2:4b25f31:         int sign = 1;
2:4b25f31:         if (gen.nextDouble() < 0.5) {
2:4b25f31:           sign = -1;
1:4b25f31:         }
1:4b25f31:       element.set(sign * exp.nextDouble());
1:4b25f31:     }
1:4b25f31: 
1:4b25f31:     // train one copy of a wrapped learner
1:4b25f31:     AdaptiveLogisticRegression.Wrapper w = new AdaptiveLogisticRegression.Wrapper(2, 200, new L1());
1:4b25f31:     for (int i = 0; i < 3000; i++) {
1:4b25f31:       AdaptiveLogisticRegression.TrainingExample r = getExample(i, gen, beta);
1:4b25f31:       w.train(r);
1:4b25f31:       if (i % 1000 == 0) {
1:4b25f31:         System.out.printf("%10d %.3f\n", i, w.getLearner().auc());
1:4b25f31:       }
1:4b25f31:     }
1:4b25f31:     System.out.printf("%10d %.3f\n", 3000, w.getLearner().auc());
1:4b25f31:     double auc1 = w.getLearner().auc();
1:4b25f31: 
1:4b25f31:     // then switch to a copy of that learner ... progress should continue
1:4b25f31:     AdaptiveLogisticRegression.Wrapper w2 = w.copy();
1:4b25f31: 
1:4b25f31:     for (int i = 0; i < 5000; i++) {
1:4b25f31:       if (i % 1000 == 0) {
1:4b25f31:         if (i == 0) {
1:ff79ff4:           assertEquals("Should have started with no data", 0.5, w2.getLearner().auc(), 0.0001);
1:4b25f31:         }
1:4b25f31:         if (i == 1000) {
1:c0c8d9f:           double auc2 = w2.getLearner().auc();
1:ff79ff4:           assertTrue("Should have had head-start", Math.abs(auc2 - 0.5) > 0.1);
1:ff79ff4:           assertTrue("AUC should improve quickly on copy", auc1 < auc2);
1:4b25f31:         }
1:4b25f31:         System.out.printf("%10d %.3f\n", i, w2.getLearner().auc());
1:4b25f31:       }
1:4b25f31:       AdaptiveLogisticRegression.TrainingExample r = getExample(i, gen, beta);
1:4b25f31:       w2.train(r);
1:4b25f31:     }
1:ff79ff4:     assertEquals("Original should not change after copy is updated", auc1, w.getLearner().auc(), 1.0e-5);
1:d84e4c0: 
1:d84e4c0:     // this improvement is really quite lenient
1:ff79ff4:     assertTrue("AUC should improve significantly on copy", auc1 < w2.getLearner().auc() - 0.05);
1:4b25f31: 
1:4b25f31:     // make sure that the copy didn't lose anything
1:ff79ff4:     assertEquals(auc1, w.getLearner().auc(), 0);
1:4b25f31:   }
1:597db6a: 
1:597db6a:   @Test
1:597db6a:   public void stepSize() {
1:597db6a:     assertEquals(500, AdaptiveLogisticRegression.stepSize(15000, 2));
1:597db6a:     assertEquals(2000, AdaptiveLogisticRegression.stepSize(15000, 2.6));
1:597db6a:     assertEquals(5000, AdaptiveLogisticRegression.stepSize(24000, 2.6));
1:597db6a:     assertEquals(10000, AdaptiveLogisticRegression.stepSize(15000, 3));
1:597db6a:   }
1:597db6a: 
1:597db6a:   @Test
1:c88c240:   @ThreadLeakLingering(linger = 1000)
1:597db6a:   public void constantStep() {
1:597db6a:     AdaptiveLogisticRegression lr = new AdaptiveLogisticRegression(2, 1000, new L1());
1:597db6a:     lr.setInterval(5000);
1:597db6a:     assertEquals(20000, lr.nextStep(15000));
1:597db6a:     assertEquals(20000, lr.nextStep(15001));
1:597db6a:     assertEquals(20000, lr.nextStep(16500));
1:597db6a:     assertEquals(20000, lr.nextStep(19999));
1:c88c240:     lr.close(); 
1:597db6a:   }
1:597db6a:     
1:597db6a: 
1:597db6a:   @Test
1:c88c240:   @ThreadLeakLingering(linger = 1000)
1:597db6a:   public void growingStep() {
1:597db6a:     AdaptiveLogisticRegression lr = new AdaptiveLogisticRegression(2, 1000, new L1());
1:597db6a:     lr.setInterval(2000, 10000);
1:597db6a: 
1:597db6a:     // start with minimum step size
1:e64dd36:     for (int i = 2000; i < 20000; i+=2000) {
1:597db6a:       assertEquals(i + 2000, lr.nextStep(i));
1:597db6a:     }
1:597db6a: 
1:597db6a:     // then level up a bit
1:597db6a:     for (int i = 20000; i < 50000; i += 5000) {
1:597db6a:       assertEquals(i + 5000, lr.nextStep(i));
1:597db6a:     }
1:597db6a: 
1:597db6a:     // and more, but we top out with this step size
1:597db6a:     for (int i = 50000; i < 500000; i += 10000) {
1:597db6a:       assertEquals(i + 10000, lr.nextStep(i));
1:597db6a:     }
1:c88c240:     lr.close();
1:597db6a:   }
1:4b25f31: }
============================================================================
author:pferrel
-------------------------------------------------------------------------------
commit:b988c49
author:frankscholten
-------------------------------------------------------------------------------
commit:1a42d85
author:smarthi
-------------------------------------------------------------------------------
commit:c88c240
/////////////////////////////////////////////////////////////////////////
1: import com.carrotsearch.randomizedtesting.annotations.ThreadLeakLingering;
1: 
1:   @ThreadLeakLingering(linger=1000)
/////////////////////////////////////////////////////////////////////////
1:     AdaptiveLogisticRegression adaptiveLogisticRegression = new AdaptiveLogisticRegression(2, 200, new L1());
1:     adaptiveLogisticRegression.setInterval(1000);
1:       adaptiveLogisticRegression.train(r.getKey(), r.getActual(), r.getInstance());
1:       if (i % 1000 == 0 && adaptiveLogisticRegression.getBest() != null) {
1:                           i, adaptiveLogisticRegression.auc(),
1:                           Math.log10(adaptiveLogisticRegression.getBest().getMappedParams()[0]), adaptiveLogisticRegression.getBest().getMappedParams()[1]);
1:     assertEquals(1, adaptiveLogisticRegression.auc(), 0.1);
1:     adaptiveLogisticRegression.close();
/////////////////////////////////////////////////////////////////////////
1:   @ThreadLeakLingering(linger = 1000)
/////////////////////////////////////////////////////////////////////////
1:     lr.close(); 
1:   @ThreadLeakLingering(linger = 1000)
/////////////////////////////////////////////////////////////////////////
1:     lr.close();
author:Jacob Alexander Mannix
-------------------------------------------------------------------------------
commit:dc62944
/////////////////////////////////////////////////////////////////////////
1:     for (Vector.Element element : beta.all()) {
/////////////////////////////////////////////////////////////////////////
1:     for (Vector.Element element : data.all()) {
/////////////////////////////////////////////////////////////////////////
1:     for (Vector.Element element : beta.all()) {
author:tcp
-------------------------------------------------------------------------------
commit:e64dd36
/////////////////////////////////////////////////////////////////////////
1:     for (int i = 2000; i < 20000; i+=2000) {
author:Sean R. Owen
-------------------------------------------------------------------------------
commit:3a1adb7
/////////////////////////////////////////////////////////////////////////
0:       if (i % 1000 == 0 && x.getBest() != null) {
1:         System.out.printf("%10d %10.4f %10.8f %.3f\n",
0:                           i, x.auc(),
0:                           Math.log10(x.getBest().getMappedParams()[0]), x.getBest().getMappedParams()[1]);
commit:ff79ff4
/////////////////////////////////////////////////////////////////////////
1: import org.apache.mahout.common.MahoutTestCase;
1: public final class AdaptiveLogisticRegressionTest extends MahoutTestCase {
1: 
/////////////////////////////////////////////////////////////////////////
/////////////////////////////////////////////////////////////////////////
1:           assertEquals("Should have started with no data", 0.5, w2.getLearner().auc(), 0.0001);
1:           assertTrue("Should have had head-start", Math.abs(auc2 - 0.5) > 0.1);
1:           assertTrue("AUC should improve quickly on copy", auc1 < auc2);
1:     assertEquals("Original should not change after copy is updated", auc1, w.getLearner().auc(), 1.0e-5);
1:     assertTrue("AUC should improve significantly on copy", auc1 < w2.getLearner().auc() - 0.05);
1:     assertEquals(auc1, w.getLearner().auc(), 0);
commit:c0c8d9f
/////////////////////////////////////////////////////////////////////////
/////////////////////////////////////////////////////////////////////////
1:     Random gen = RandomUtils.getRandom();
1:     Exponential exp = new Exponential(0.5, gen);
/////////////////////////////////////////////////////////////////////////
1:     cl.update(new double[]{1.0e-5, 1});
/////////////////////////////////////////////////////////////////////////
0:           System.out.printf("%10d %10.4f %10.8f %.3f\n",
0:                             i, x.auc(),
0:                             Math.log10(x.getBest().getMappedParams()[0]), x.getBest().getMappedParams()[1]);
1:   private static AdaptiveLogisticRegression.TrainingExample getExample(int i, Random gen, Vector beta) {
/////////////////////////////////////////////////////////////////////////
1:     Random gen = RandomUtils.getRandom();
1:     Exponential exp = new Exponential(0.5, gen);
/////////////////////////////////////////////////////////////////////////
/////////////////////////////////////////////////////////////////////////
1:           double auc2 = w2.getLearner().auc();
/////////////////////////////////////////////////////////////////////////
0:     Assert.assertEquals("Original should not change after copy is updated", auc1, w.getLearner().auc(), 1.0e-5);
author:Ted Dunning
-------------------------------------------------------------------------------
commit:597db6a
/////////////////////////////////////////////////////////////////////////
1: 
1:   @Test
1:   public void stepSize() {
1:     assertEquals(500, AdaptiveLogisticRegression.stepSize(15000, 2));
1:     assertEquals(2000, AdaptiveLogisticRegression.stepSize(15000, 2.6));
1:     assertEquals(5000, AdaptiveLogisticRegression.stepSize(24000, 2.6));
1:     assertEquals(10000, AdaptiveLogisticRegression.stepSize(15000, 3));
1:   }
1: 
1:   @Test
1:   public void constantStep() {
1:     AdaptiveLogisticRegression lr = new AdaptiveLogisticRegression(2, 1000, new L1());
1:     lr.setInterval(5000);
1:     assertEquals(20000, lr.nextStep(15000));
1:     assertEquals(20000, lr.nextStep(15001));
1:     assertEquals(20000, lr.nextStep(16500));
1:     assertEquals(20000, lr.nextStep(19999));
1:   }
1:     
1: 
1:   @Test
1:   public void growingStep() {
1:     AdaptiveLogisticRegression lr = new AdaptiveLogisticRegression(2, 1000, new L1());
1:     lr.setInterval(2000, 10000);
1: 
1:     // start with minimum step size
0:     for (int i = 2000; i < 20000;i+=2000) {
1:       assertEquals(i + 2000, lr.nextStep(i));
1:     }
1: 
1:     // then level up a bit
1:     for (int i = 20000; i < 50000; i += 5000) {
1:       assertEquals(i + 5000, lr.nextStep(i));
1:     }
1: 
1:     // and more, but we top out with this step size
1:     for (int i = 50000; i < 500000; i += 10000) {
1:       assertEquals(i + 10000, lr.nextStep(i));
1:     }
1:   }
commit:e6b4e35
/////////////////////////////////////////////////////////////////////////
1:     return new AdaptiveLogisticRegression.TrainingExample(i, null, target, data);
commit:2ce6410
/////////////////////////////////////////////////////////////////////////
/////////////////////////////////////////////////////////////////////////
1:     assertEquals(1, cl.getLearner().auc(), 0.1);
/////////////////////////////////////////////////////////////////////////
0:     assertEquals(1, x.auc(), 0.1);
commit:22726e8
/////////////////////////////////////////////////////////////////////////
1: import java.util.Random;
1: 
0:     final Random gen = RandomUtils.getRandom();
0:     final Exponential exp = new Exponential(0.5, gen);
1:       int sign = 1;
1:       if (gen.nextDouble() < 0.5) {
1:         sign = -1;
1:       }
/////////////////////////////////////////////////////////////////////////
0:   private AdaptiveLogisticRegression.TrainingExample getExample(int i, Random gen, Vector beta) {
/////////////////////////////////////////////////////////////////////////
1: 
0:     final Random gen = RandomUtils.getRandom();
0:     final Exponential exp = new Exponential(0.5, gen);
commit:f747b51
/////////////////////////////////////////////////////////////////////////
0:     Assert.assertTrue("AUC should improve significantly on copy", auc1 < w2.getLearner().auc() - 0.05);
commit:a704e17
/////////////////////////////////////////////////////////////////////////
1: /*
1:  * Licensed to the Apache Software Foundation (ASF) under one or more
1:  * contributor license agreements.  See the NOTICE file distributed with
1:  * this work for additional information regarding copyright ownership.
1:  * The ASF licenses this file to You under the Apache License, Version 2.0
1:  * (the "License"); you may not use this file except in compliance with
1:  * the License.  You may obtain a copy of the License at
1:  *
1:  *     http://www.apache.org/licenses/LICENSE-2.0
1:  *
1:  * Unless required by applicable law or agreed to in writing, software
1:  * distributed under the License is distributed on an "AS IS" BASIS,
1:  * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
1:  * See the License for the specific language governing permissions and
1:  * limitations under the License.
1:  */
1: 
commit:d84e4c0
/////////////////////////////////////////////////////////////////////////
1: import org.apache.mahout.common.RandomUtils;
/////////////////////////////////////////////////////////////////////////
/////////////////////////////////////////////////////////////////////////
/////////////////////////////////////////////////////////////////////////
0:     return new AdaptiveLogisticRegression.TrainingExample(i, target, data);
0:     RandomUtils.useTestSeed();
1:     
/////////////////////////////////////////////////////////////////////////
0:     double auc2;
/////////////////////////////////////////////////////////////////////////
0:           Assert.assertTrue("AUC should improve quickly on copy", auc1 < auc2);
0:     Assert.assertEquals("Original should not change after copy is updated", auc1, w.getLearner().auc(), 1e-5);
1: 
1:     // this improvement is really quite lenient
0:     Assert.assertTrue("AUC should improve substantially on copy", auc1 < w2.getLearner().auc() - 0.1);
commit:d3ace90
/////////////////////////////////////////////////////////////////////////
0:     AdaptiveLogisticRegression x = new AdaptiveLogisticRegression(2, 200, new L1());
commit:ae086da
/////////////////////////////////////////////////////////////////////////
0:       x.train(r.getKey(), r.getActual(), r.getInstance());
commit:4b25f31
/////////////////////////////////////////////////////////////////////////
1: package org.apache.mahout.classifier.sgd;
1: 
1: import org.apache.mahout.math.DenseVector;
1: import org.apache.mahout.math.Vector;
1: import org.apache.mahout.math.jet.random.Exponential;
0: import org.apache.mahout.math.jet.random.Normal;
0: import org.apache.mahout.math.jet.random.engine.MersenneTwister;
0: import org.junit.Assert;
1: import org.junit.Test;
1: 
0: public class AdaptiveLogisticRegressionTest {
1:   @Test
1:   public void testTrain() {
0:     // we make up data for a simple model
1: 
0:     final MersenneTwister gen = new MersenneTwister(1);
0:     final Exponential exp = new Exponential(.5, gen);
1:     Vector data = new DenseVector(200);
1:     Vector beta = new DenseVector(200);
0:     for (Vector.Element element : beta) {
1:         int sign = 1;
1:         if (gen.nextDouble() < 0.5) {
1:           sign = -1;
1:         }
1:       element.set(sign * exp.nextDouble());
1:     }
1: 
1:     AdaptiveLogisticRegression.Wrapper cl = new AdaptiveLogisticRegression.Wrapper(2, 200, new L1());
0:     cl.update(new double[]{1e-5, 1});
1: 
1:     for (int i = 0; i < 10000; i++) {
1:       AdaptiveLogisticRegression.TrainingExample r = getExample(i, gen, beta);
1:       cl.train(r);
1:       if (i % 1000 == 0) {
0: //        cl.close();
1:         System.out.printf("%10d %10.3f\n", i, cl.getLearner().auc());
1:       }
1:     }
1: 
0:     AdaptiveLogisticRegression x = new AdaptiveLogisticRegression(20, 2, 200, new L1());
0:     x.setInterval(1000);
1: 
0:     final Normal norm = new Normal(0, 1, gen);
1:     for (int i = 0; i < 20000; i++) {
1:       AdaptiveLogisticRegression.TrainingExample r = getExample(i, gen, beta);
0:       x.train(r.key, r.actual, r.instance);
1:       if (i % 1000 == 0) {
0:         if (x.getBest() != null) {
0:           System.out.printf("%10d %10.4f %10.8f %.3f\n", i, x.auc(), Math.log10(x.getBest().getMappedParams()[0]), x.getBest().getMappedParams()[1]);
1:         }
1:       }
1:     }
1:   }
1: 
0:   private AdaptiveLogisticRegression.TrainingExample getExample(int i, MersenneTwister gen, Vector beta) {
1:     Vector data = new DenseVector(200);
1: 
0:     for (Vector.Element element : data) {
1:       element.set(gen.nextDouble() < 0.3 ? 1 : 0);
1:     }
1: 
1:     double p = 1 / (1 + Math.exp(1.5 - data.dot(beta)));
1:     int target = 0;
1:     if (gen.nextDouble() < p) {
1:       target = 1;
1:     }
0:     AdaptiveLogisticRegression.TrainingExample r = new AdaptiveLogisticRegression.TrainingExample(i, target, data);
0:     return r;
1:   }
1: 
1:   @Test
1:   public void copyLearnsAsExpected() {
0:     final MersenneTwister gen = new MersenneTwister(1);
0:     final Exponential exp = new Exponential(.5, gen);
1:     Vector data = new DenseVector(200);
1:     Vector beta = new DenseVector(200);
0:     for (Vector.Element element : beta) {
1:         int sign = 1;
1:         if (gen.nextDouble() < 0.5) {
1:           sign = -1;
1:         }
1:       element.set(sign * exp.nextDouble());
1:     }
1: 
1:     // train one copy of a wrapped learner
1:     AdaptiveLogisticRegression.Wrapper w = new AdaptiveLogisticRegression.Wrapper(2, 200, new L1());
1:     for (int i = 0; i < 3000; i++) {
1:       AdaptiveLogisticRegression.TrainingExample r = getExample(i, gen, beta);
1:       w.train(r);
1:       if (i % 1000 == 0) {
1:         System.out.printf("%10d %.3f\n", i, w.getLearner().auc());
1:       }
1:     }
1:     System.out.printf("%10d %.3f\n", 3000, w.getLearner().auc());
1:     double auc1 = w.getLearner().auc();
1: 
1:     // then switch to a copy of that learner ... progress should continue
1:     AdaptiveLogisticRegression.Wrapper w2 = w.copy();
0:     double auc2 = -1;
1: 
1:     for (int i = 0; i < 5000; i++) {
1:       if (i % 1000 == 0) {
1:         if (i == 0) {
0:           Assert.assertEquals("Should have started with no data", 0.5, w2.getLearner().auc(), 0.0001);
1:         }
1:         if (i == 1000) {
0:           auc2 = w2.getLearner().auc();
0:           Assert.assertTrue("Should have had head-start", Math.abs(auc2 - 0.5) > 0.1);
1:         }
1:         System.out.printf("%10d %.3f\n", i, w2.getLearner().auc());
1:       }
1:       AdaptiveLogisticRegression.TrainingExample r = getExample(i, gen, beta);
1:       w2.train(r);
1:     }
0:     Assert.assertTrue("AUC should improve on copy", auc1 < w2.getLearner().auc() - 0.1);
0:     Assert.assertTrue("AUC should improve on copy", auc1 < auc2);
1: 
1:     // make sure that the copy didn't lose anything
0:     Assert.assertEquals(auc1, w.getLearner().auc(), 0);
1:   }
1: }
============================================================================