1:557bbbc: /*
1:557bbbc:  * Licensed to the Apache Software Foundation (ASF) under one
1:557bbbc:  * or more contributor license agreements.  See the NOTICE file
1:557bbbc:  * distributed with this work for additional information
1:557bbbc:  * regarding copyright ownership.  The ASF licenses this file
1:557bbbc:  * to you under the Apache License, Version 2.0 (the
1:557bbbc:  * "License"); you may not use this file except in compliance
1:557bbbc:  * with the License.  You may obtain a copy of the License at
1:557bbbc:  *
1:557bbbc:  *     http://www.apache.org/licenses/LICENSE-2.0
1:557bbbc:  *
1:557bbbc:  * Unless required by applicable law or agreed to in writing, software
1:557bbbc:  * distributed under the License is distributed on an "AS IS" BASIS,
1:557bbbc:  * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
1:557bbbc:  * See the License for the specific language governing permissions and
1:557bbbc:  * limitations under the License.
1:557bbbc:  */
1:557bbbc: 
1:557bbbc: package org.apache.cassandra.db;
1:557bbbc: 
1:f2d5cd0: import java.nio.ByteBuffer;
1:f2d5cd0: import java.util.ArrayList;
1:557bbbc: import java.util.List;
1:557bbbc: 
1:557bbbc: import org.junit.BeforeClass;
1:557bbbc: import org.junit.Test;
1:557bbbc: 
1:557bbbc: import org.apache.cassandra.SchemaLoader;
1:557bbbc: import org.apache.cassandra.Util;
1:557bbbc: import org.apache.cassandra.config.CFMetaData;
1:9797511: import org.apache.cassandra.config.DatabaseDescriptor;
1:f2d5cd0: import org.apache.cassandra.db.filter.ClusteringIndexSliceFilter;
1:f2d5cd0: import org.apache.cassandra.db.filter.ColumnFilter;
1:f2d5cd0: import org.apache.cassandra.db.filter.DataLimits;
1:f2d5cd0: import org.apache.cassandra.db.filter.RowFilter;
1:557bbbc: import org.apache.cassandra.db.marshal.AsciiType;
1:557bbbc: import org.apache.cassandra.db.marshal.BytesType;
1:557bbbc: import org.apache.cassandra.db.partitions.FilteredPartition;
1:f2d5cd0: import org.apache.cassandra.db.partitions.PartitionIterator;
1:f2d5cd0: import org.apache.cassandra.db.partitions.UnfilteredPartitionIterator;
1:f2d5cd0: import org.apache.cassandra.db.partitions.UnfilteredPartitionIterators;
1:f2d5cd0: import org.apache.cassandra.db.rows.Row;
1:f2d5cd0: import org.apache.cassandra.db.rows.RowIterator;
1:f2d5cd0: import org.apache.cassandra.db.rows.SerializationHelper;
1:f2d5cd0: import org.apache.cassandra.db.rows.UnfilteredRowIterator;
1:f2d5cd0: import org.apache.cassandra.db.rows.UnfilteredRowIterators;
1:557bbbc: import org.apache.cassandra.exceptions.ConfigurationException;
1:f2d5cd0: import org.apache.cassandra.io.util.DataInputBuffer;
1:f2d5cd0: import org.apache.cassandra.io.util.DataOutputBuffer;
1:f2d5cd0: import org.apache.cassandra.net.MessagingService;
1:557bbbc: import org.apache.cassandra.schema.KeyspaceParams;
1:557bbbc: import org.apache.cassandra.utils.ByteBufferUtil;
1:f2d5cd0: import org.apache.cassandra.utils.FBUtilities;
1:557bbbc: 
1:557bbbc: import static org.junit.Assert.assertEquals;
1:557bbbc: 
1:557bbbc: public class ReadCommandTest
1:557bbbc: {
1:557bbbc:     private static final String KEYSPACE = "ReadCommandTest";
1:557bbbc:     private static final String CF1 = "Standard1";
1:557bbbc:     private static final String CF2 = "Standard2";
1:f2d5cd0:     private static final String CF3 = "Standard3";
1:557bbbc: 
1:557bbbc:     @BeforeClass
1:557bbbc:     public static void defineSchema() throws ConfigurationException
1:557bbbc:     {
1:9797511:         DatabaseDescriptor.daemonInitialization();
1:9797511: 
1:557bbbc:         CFMetaData metadata1 = SchemaLoader.standardCFMD(KEYSPACE, CF1);
1:557bbbc: 
1:557bbbc:         CFMetaData metadata2 = CFMetaData.Builder.create(KEYSPACE, CF2)
1:557bbbc:                                                          .addPartitionKey("key", BytesType.instance)
1:557bbbc:                                                          .addClusteringColumn("col", AsciiType.instance)
1:557bbbc:                                                          .addRegularColumn("a", AsciiType.instance)
1:557bbbc:                                                          .addRegularColumn("b", AsciiType.instance).build();
1:557bbbc: 
1:f2d5cd0:         CFMetaData metadata3 = CFMetaData.Builder.create(KEYSPACE, CF3)
1:f2d5cd0:                                                  .addPartitionKey("key", BytesType.instance)
1:f2d5cd0:                                                  .addClusteringColumn("col", AsciiType.instance)
1:f2d5cd0:                                                  .addRegularColumn("a", AsciiType.instance)
1:f2d5cd0:                                                  .addRegularColumn("b", AsciiType.instance)
1:f2d5cd0:                                                  .addRegularColumn("c", AsciiType.instance)
1:f2d5cd0:                                                  .addRegularColumn("d", AsciiType.instance)
1:f2d5cd0:                                                  .addRegularColumn("e", AsciiType.instance)
1:f2d5cd0:                                                  .addRegularColumn("f", AsciiType.instance).build();
1:f2d5cd0: 
1:557bbbc:         SchemaLoader.prepareServer();
1:557bbbc:         SchemaLoader.createKeyspace(KEYSPACE,
1:557bbbc:                                     KeyspaceParams.simple(1),
1:557bbbc:                                     metadata1,
1:f2d5cd0:                                     metadata2,
1:f2d5cd0:                                     metadata3);
1:557bbbc:     }
1:557bbbc: 
1:557bbbc:     @Test
1:557bbbc:     public void testPartitionRangeAbort() throws Exception
1:557bbbc:     {
1:557bbbc:         ColumnFamilyStore cfs = Keyspace.open(KEYSPACE).getColumnFamilyStore(CF1);
1:557bbbc: 
1:557bbbc:         new RowUpdateBuilder(cfs.metadata, 0, ByteBufferUtil.bytes("key1"))
1:557bbbc:                 .clustering("Column1")
1:557bbbc:                 .add("val", ByteBufferUtil.bytes("abcd"))
1:557bbbc:                 .build()
1:557bbbc:                 .apply();
1:557bbbc: 
1:557bbbc:         cfs.forceBlockingFlush();
1:557bbbc: 
1:557bbbc:         new RowUpdateBuilder(cfs.metadata, 0, ByteBufferUtil.bytes("key2"))
1:557bbbc:                 .clustering("Column1")
1:557bbbc:                 .add("val", ByteBufferUtil.bytes("abcd"))
1:557bbbc:                 .build()
1:557bbbc:                 .apply();
1:557bbbc: 
1:557bbbc:         ReadCommand readCommand = Util.cmd(cfs).build();
1:557bbbc:         assertEquals(2, Util.getAll(readCommand).size());
1:557bbbc: 
1:557bbbc:         readCommand.abort();
1:557bbbc:         assertEquals(0, Util.getAll(readCommand).size());
1:557bbbc:     }
1:f2d5cd0: 
1:f2d5cd0:     @Test
1:557bbbc:     public void testSinglePartitionSliceAbort() throws Exception
1:557bbbc:     {
1:557bbbc:         ColumnFamilyStore cfs = Keyspace.open(KEYSPACE).getColumnFamilyStore(CF2);
1:557bbbc: 
1:881e2da:         cfs.truncateBlocking();
1:557bbbc: 
1:557bbbc:         new RowUpdateBuilder(cfs.metadata, 0, ByteBufferUtil.bytes("key"))
1:557bbbc:                 .clustering("cc")
1:557bbbc:                 .add("a", ByteBufferUtil.bytes("abcd"))
1:557bbbc:                 .build()
1:557bbbc:                 .apply();
1:557bbbc: 
1:557bbbc:         cfs.forceBlockingFlush();
1:557bbbc: 
1:557bbbc:         new RowUpdateBuilder(cfs.metadata, 0, ByteBufferUtil.bytes("key"))
1:557bbbc:                 .clustering("dd")
1:557bbbc:                 .add("a", ByteBufferUtil.bytes("abcd"))
1:557bbbc:                 .build()
1:557bbbc:                 .apply();
1:557bbbc: 
1:557bbbc:         ReadCommand readCommand = Util.cmd(cfs, Util.dk("key")).build();
1:557bbbc: 
1:557bbbc:         List<FilteredPartition> partitions = Util.getAll(readCommand);
1:557bbbc:         assertEquals(1, partitions.size());
1:557bbbc:         assertEquals(2, partitions.get(0).rowCount());
1:557bbbc: 
1:557bbbc:         readCommand.abort();
1:557bbbc:         assertEquals(0, Util.getAll(readCommand).size());
1:557bbbc:     }
1:557bbbc: 
1:557bbbc:     @Test
1:557bbbc:     public void testSinglePartitionNamesAbort() throws Exception
1:557bbbc:     {
1:557bbbc:         ColumnFamilyStore cfs = Keyspace.open(KEYSPACE).getColumnFamilyStore(CF2);
1:557bbbc: 
1:881e2da:         cfs.truncateBlocking();
1:557bbbc: 
1:557bbbc:         new RowUpdateBuilder(cfs.metadata, 0, ByteBufferUtil.bytes("key"))
1:557bbbc:                 .clustering("cc")
1:557bbbc:                 .add("a", ByteBufferUtil.bytes("abcd"))
1:557bbbc:                 .build()
1:557bbbc:                 .apply();
1:557bbbc: 
1:557bbbc:         cfs.forceBlockingFlush();
1:557bbbc: 
1:557bbbc:         new RowUpdateBuilder(cfs.metadata, 0, ByteBufferUtil.bytes("key"))
1:881e2da:                 .clustering("dd")
1:557bbbc:                 .add("a", ByteBufferUtil.bytes("abcd"))
1:557bbbc:                 .build()
1:557bbbc:                 .apply();
1:557bbbc: 
1:557bbbc:         ReadCommand readCommand = Util.cmd(cfs, Util.dk("key")).includeRow("cc").includeRow("dd").build();
1:557bbbc: 
1:557bbbc:         List<FilteredPartition> partitions = Util.getAll(readCommand);
1:557bbbc:         assertEquals(1, partitions.size());
1:557bbbc:         assertEquals(2, partitions.get(0).rowCount());
1:881e2da: 
1:557bbbc:         readCommand.abort();
1:557bbbc:         assertEquals(0, Util.getAll(readCommand).size());
1:557bbbc:     }
1:881e2da: 
1:557bbbc:     @Test
1:f2d5cd0:     public void testSinglePartitionGroupMerge() throws Exception
1:f2d5cd0:     {
1:f2d5cd0:         ColumnFamilyStore cfs = Keyspace.open(KEYSPACE).getColumnFamilyStore(CF3);
1:f2d5cd0: 
1:f2d5cd0:         String[][][] groups = new String[][][] {
1:f2d5cd0:             new String[][] {
1:f2d5cd0:                 new String[] { "1", "key1", "aa", "a" }, // "1" indicates to create the data, "-1" to delete the row
1:f2d5cd0:                 new String[] { "1", "key2", "bb", "b" },
1:f2d5cd0:                 new String[] { "1", "key3", "cc", "c" }
1:f2d5cd0:             },
1:f2d5cd0:             new String[][] {
1:f2d5cd0:                 new String[] { "1", "key3", "dd", "d" },
1:f2d5cd0:                 new String[] { "1", "key2", "ee", "e" },
1:f2d5cd0:                 new String[] { "1", "key1", "ff", "f" }
1:f2d5cd0:             },
1:f2d5cd0:             new String[][] {
1:f2d5cd0:                 new String[] { "1", "key6", "aa", "a" },
1:f2d5cd0:                 new String[] { "1", "key5", "bb", "b" },
1:f2d5cd0:                 new String[] { "1", "key4", "cc", "c" }
1:f2d5cd0:             },
1:f2d5cd0:             new String[][] {
1:f2d5cd0:                 new String[] { "-1", "key6", "aa", "a" },
1:f2d5cd0:                 new String[] { "-1", "key2", "bb", "b" }
1:f2d5cd0:             }
1:f2d5cd0:         };
1:f2d5cd0: 
1:f2d5cd0:         // Given the data above, when the keys are sorted and the deletions removed, we should
1:f2d5cd0:         // get these clustering rows in this order
1:f2d5cd0:         String[] expectedRows = new String[] { "aa", "ff", "ee", "cc", "dd", "cc", "bb"};
1:f2d5cd0: 
1:f2d5cd0:         List<ByteBuffer> buffers = new ArrayList<>(groups.length);
1:f2d5cd0:         int nowInSeconds = FBUtilities.nowInSeconds();
1:f2d5cd0:         ColumnFilter columnFilter = ColumnFilter.allColumnsBuilder(cfs.metadata).build();
1:f2d5cd0:         RowFilter rowFilter = RowFilter.create();
1:f2d5cd0:         Slice slice = Slice.make(ClusteringBound.BOTTOM, ClusteringBound.TOP);
1:f2d5cd0:         ClusteringIndexSliceFilter sliceFilter = new ClusteringIndexSliceFilter(Slices.with(cfs.metadata.comparator, slice), false);
1:f2d5cd0: 
1:f2d5cd0:         for (String[][] group : groups)
1:f2d5cd0:         {
1:f2d5cd0:             cfs.truncateBlocking();
1:f2d5cd0: 
1:f2d5cd0:             List<SinglePartitionReadCommand> commands = new ArrayList<>(group.length);
1:f2d5cd0: 
1:f2d5cd0:             for (String[] data : group)
1:f2d5cd0:             {
1:f2d5cd0:                 if (data[0].equals("1"))
1:f2d5cd0:                 {
1:f2d5cd0:                     new RowUpdateBuilder(cfs.metadata, 0, ByteBufferUtil.bytes(data[1]))
1:f2d5cd0:                     .clustering(data[2])
1:f2d5cd0:                     .add(data[3], ByteBufferUtil.bytes("blah"))
1:f2d5cd0:                     .build()
1:f2d5cd0:                     .apply();
1:f2d5cd0:                 }
1:f2d5cd0:                 else
1:f2d5cd0:                 {
1:f2d5cd0:                     RowUpdateBuilder.deleteRow(cfs.metadata, FBUtilities.timestampMicros(), ByteBufferUtil.bytes(data[1]), data[2]).apply();
1:f2d5cd0:                 }
1:f2d5cd0:                 commands.add(SinglePartitionReadCommand.create(cfs.metadata, nowInSeconds, columnFilter, rowFilter, DataLimits.NONE, Util.dk(data[1]), sliceFilter));
1:f2d5cd0:             }
1:f2d5cd0: 
1:f2d5cd0:             cfs.forceBlockingFlush();
1:f2d5cd0: 
1:f2d5cd0:             ReadQuery query = new SinglePartitionReadCommand.Group(commands, DataLimits.NONE);
1:f2d5cd0: 
1:f2d5cd0:             try (ReadExecutionController executionController = query.executionController();
1:f2d5cd0:                  UnfilteredPartitionIterator iter = query.executeLocally(executionController);
1:f2d5cd0:                  DataOutputBuffer buffer = new DataOutputBuffer())
1:f2d5cd0:             {
1:f2d5cd0:                 UnfilteredPartitionIterators.serializerForIntraNode().serialize(iter,
1:f2d5cd0:                                                                                 columnFilter,
1:f2d5cd0:                                                                                 buffer,
1:f2d5cd0:                                                                                 MessagingService.current_version);
1:f2d5cd0:                 buffers.add(buffer.buffer());
1:f2d5cd0:             }
1:f2d5cd0:         }
1:f2d5cd0: 
1:f2d5cd0:         // deserialize, merge and check the results are all there
1:f2d5cd0:         List<UnfilteredPartitionIterator> iterators = new ArrayList<>();
1:f2d5cd0: 
1:f2d5cd0:         for (ByteBuffer buffer : buffers)
1:f2d5cd0:         {
1:f2d5cd0:             try (DataInputBuffer in = new DataInputBuffer(buffer, true))
1:f2d5cd0:             {
1:f2d5cd0:                 iterators.add(UnfilteredPartitionIterators.serializerForIntraNode().deserialize(in,
1:f2d5cd0:                                                                                                 MessagingService.current_version,
1:f2d5cd0:                                                                                                 cfs.metadata,
1:f2d5cd0:                                                                                                 columnFilter,
1:f2d5cd0:                                                                                                 SerializationHelper.Flag.LOCAL));
1:f2d5cd0:             }
1:f2d5cd0:         }
1:f2d5cd0: 
1:f2d5cd0:         try(PartitionIterator partitionIterator = UnfilteredPartitionIterators.mergeAndFilter(iterators,
1:f2d5cd0:                                                                                           nowInSeconds,
1:f2d5cd0:                                                                                           new UnfilteredPartitionIterators.MergeListener()
1:f2d5cd0:         {
1:f2d5cd0:             public UnfilteredRowIterators.MergeListener getRowMergeListener(DecoratedKey partitionKey, List<UnfilteredRowIterator> versions)
1:f2d5cd0:             {
1:f2d5cd0:                 return null;
1:f2d5cd0:             }
1:f2d5cd0: 
1:f2d5cd0:             public void close()
1:f2d5cd0:             {
1:f2d5cd0: 
1:f2d5cd0:             }
1:f2d5cd0:         }))
1:f2d5cd0:         {
1:f2d5cd0: 
1:f2d5cd0:             int i = 0;
1:f2d5cd0:             int numPartitions = 0;
1:f2d5cd0:             while (partitionIterator.hasNext())
1:f2d5cd0:             {
1:f2d5cd0:                 numPartitions++;
1:f2d5cd0:                 try(RowIterator rowIterator = partitionIterator.next())
1:f2d5cd0:                 {
1:f2d5cd0:                     while (rowIterator.hasNext())
1:f2d5cd0:                     {
1:f2d5cd0:                         Row row = rowIterator.next();
1:f2d5cd0:                         assertEquals("col=" + expectedRows[i++], row.clustering().toString(cfs.metadata));
1:f2d5cd0:                         //System.out.print(row.toString(cfs.metadata, true));
1:f2d5cd0:                     }
1:f2d5cd0:                 }
1:f2d5cd0:             }
1:f2d5cd0: 
1:f2d5cd0:             assertEquals(5, numPartitions);
1:f2d5cd0:             assertEquals(expectedRows.length, i);
1:f2d5cd0:         }
1:f2d5cd0:     }
1:557bbbc: }
============================================================================
author:Stefania Alborghetti
-------------------------------------------------------------------------------
commit:f2d5cd0
/////////////////////////////////////////////////////////////////////////
1: import java.nio.ByteBuffer;
1: import java.util.ArrayList;
/////////////////////////////////////////////////////////////////////////
1: import org.apache.cassandra.db.filter.ClusteringIndexSliceFilter;
1: import org.apache.cassandra.db.filter.ColumnFilter;
1: import org.apache.cassandra.db.filter.DataLimits;
1: import org.apache.cassandra.db.filter.RowFilter;
1: import org.apache.cassandra.db.partitions.PartitionIterator;
1: import org.apache.cassandra.db.partitions.UnfilteredPartitionIterator;
1: import org.apache.cassandra.db.partitions.UnfilteredPartitionIterators;
1: import org.apache.cassandra.db.rows.Row;
1: import org.apache.cassandra.db.rows.RowIterator;
1: import org.apache.cassandra.db.rows.SerializationHelper;
1: import org.apache.cassandra.db.rows.UnfilteredRowIterator;
1: import org.apache.cassandra.db.rows.UnfilteredRowIterators;
1: import org.apache.cassandra.io.util.DataInputBuffer;
1: import org.apache.cassandra.io.util.DataOutputBuffer;
1: import org.apache.cassandra.net.MessagingService;
1: import org.apache.cassandra.utils.FBUtilities;
/////////////////////////////////////////////////////////////////////////
1:     private static final String CF3 = "Standard3";
/////////////////////////////////////////////////////////////////////////
1:         CFMetaData metadata3 = CFMetaData.Builder.create(KEYSPACE, CF3)
1:                                                  .addPartitionKey("key", BytesType.instance)
1:                                                  .addClusteringColumn("col", AsciiType.instance)
1:                                                  .addRegularColumn("a", AsciiType.instance)
1:                                                  .addRegularColumn("b", AsciiType.instance)
1:                                                  .addRegularColumn("c", AsciiType.instance)
1:                                                  .addRegularColumn("d", AsciiType.instance)
1:                                                  .addRegularColumn("e", AsciiType.instance)
1:                                                  .addRegularColumn("f", AsciiType.instance).build();
1: 
1:                                     metadata2,
1:                                     metadata3);
/////////////////////////////////////////////////////////////////////////
1: 
1:     @Test
1:     public void testSinglePartitionGroupMerge() throws Exception
1:     {
1:         ColumnFamilyStore cfs = Keyspace.open(KEYSPACE).getColumnFamilyStore(CF3);
1: 
1:         String[][][] groups = new String[][][] {
1:             new String[][] {
1:                 new String[] { "1", "key1", "aa", "a" }, // "1" indicates to create the data, "-1" to delete the row
1:                 new String[] { "1", "key2", "bb", "b" },
1:                 new String[] { "1", "key3", "cc", "c" }
1:             },
1:             new String[][] {
1:                 new String[] { "1", "key3", "dd", "d" },
1:                 new String[] { "1", "key2", "ee", "e" },
1:                 new String[] { "1", "key1", "ff", "f" }
1:             },
1:             new String[][] {
1:                 new String[] { "1", "key6", "aa", "a" },
1:                 new String[] { "1", "key5", "bb", "b" },
1:                 new String[] { "1", "key4", "cc", "c" }
1:             },
1:             new String[][] {
1:                 new String[] { "-1", "key6", "aa", "a" },
1:                 new String[] { "-1", "key2", "bb", "b" }
1:             }
1:         };
1: 
1:         // Given the data above, when the keys are sorted and the deletions removed, we should
1:         // get these clustering rows in this order
1:         String[] expectedRows = new String[] { "aa", "ff", "ee", "cc", "dd", "cc", "bb"};
1: 
1:         List<ByteBuffer> buffers = new ArrayList<>(groups.length);
1:         int nowInSeconds = FBUtilities.nowInSeconds();
1:         ColumnFilter columnFilter = ColumnFilter.allColumnsBuilder(cfs.metadata).build();
1:         RowFilter rowFilter = RowFilter.create();
1:         Slice slice = Slice.make(ClusteringBound.BOTTOM, ClusteringBound.TOP);
1:         ClusteringIndexSliceFilter sliceFilter = new ClusteringIndexSliceFilter(Slices.with(cfs.metadata.comparator, slice), false);
1: 
1:         for (String[][] group : groups)
1:         {
1:             cfs.truncateBlocking();
1: 
1:             List<SinglePartitionReadCommand> commands = new ArrayList<>(group.length);
1: 
1:             for (String[] data : group)
1:             {
1:                 if (data[0].equals("1"))
1:                 {
1:                     new RowUpdateBuilder(cfs.metadata, 0, ByteBufferUtil.bytes(data[1]))
1:                     .clustering(data[2])
1:                     .add(data[3], ByteBufferUtil.bytes("blah"))
1:                     .build()
1:                     .apply();
1:                 }
1:                 else
1:                 {
1:                     RowUpdateBuilder.deleteRow(cfs.metadata, FBUtilities.timestampMicros(), ByteBufferUtil.bytes(data[1]), data[2]).apply();
1:                 }
1:                 commands.add(SinglePartitionReadCommand.create(cfs.metadata, nowInSeconds, columnFilter, rowFilter, DataLimits.NONE, Util.dk(data[1]), sliceFilter));
1:             }
1: 
1:             cfs.forceBlockingFlush();
1: 
1:             ReadQuery query = new SinglePartitionReadCommand.Group(commands, DataLimits.NONE);
1: 
1:             try (ReadExecutionController executionController = query.executionController();
1:                  UnfilteredPartitionIterator iter = query.executeLocally(executionController);
1:                  DataOutputBuffer buffer = new DataOutputBuffer())
1:             {
1:                 UnfilteredPartitionIterators.serializerForIntraNode().serialize(iter,
1:                                                                                 columnFilter,
1:                                                                                 buffer,
1:                                                                                 MessagingService.current_version);
1:                 buffers.add(buffer.buffer());
1:             }
1:         }
1: 
1:         // deserialize, merge and check the results are all there
1:         List<UnfilteredPartitionIterator> iterators = new ArrayList<>();
1: 
1:         for (ByteBuffer buffer : buffers)
1:         {
1:             try (DataInputBuffer in = new DataInputBuffer(buffer, true))
1:             {
1:                 iterators.add(UnfilteredPartitionIterators.serializerForIntraNode().deserialize(in,
1:                                                                                                 MessagingService.current_version,
1:                                                                                                 cfs.metadata,
1:                                                                                                 columnFilter,
1:                                                                                                 SerializationHelper.Flag.LOCAL));
1:             }
1:         }
1: 
1:         try(PartitionIterator partitionIterator = UnfilteredPartitionIterators.mergeAndFilter(iterators,
1:                                                                                           nowInSeconds,
1:                                                                                           new UnfilteredPartitionIterators.MergeListener()
1:         {
1:             public UnfilteredRowIterators.MergeListener getRowMergeListener(DecoratedKey partitionKey, List<UnfilteredRowIterator> versions)
1:             {
1:                 return null;
1:             }
1: 
1:             public void close()
1:             {
1: 
1:             }
1:         }))
1:         {
1: 
1:             int i = 0;
1:             int numPartitions = 0;
1:             while (partitionIterator.hasNext())
1:             {
1:                 numPartitions++;
1:                 try(RowIterator rowIterator = partitionIterator.next())
1:                 {
1:                     while (rowIterator.hasNext())
1:                     {
1:                         Row row = rowIterator.next();
1:                         assertEquals("col=" + expectedRows[i++], row.clustering().toString(cfs.metadata));
1:                         //System.out.print(row.toString(cfs.metadata, true));
1:                     }
1:                 }
1:             }
1: 
1:             assertEquals(5, numPartitions);
1:             assertEquals(expectedRows.length, i);
1:         }
1:     }
commit:557bbbc
/////////////////////////////////////////////////////////////////////////
1: /*
1:  * Licensed to the Apache Software Foundation (ASF) under one
1:  * or more contributor license agreements.  See the NOTICE file
1:  * distributed with this work for additional information
1:  * regarding copyright ownership.  The ASF licenses this file
1:  * to you under the Apache License, Version 2.0 (the
1:  * "License"); you may not use this file except in compliance
1:  * with the License.  You may obtain a copy of the License at
1:  *
1:  *     http://www.apache.org/licenses/LICENSE-2.0
1:  *
1:  * Unless required by applicable law or agreed to in writing, software
1:  * distributed under the License is distributed on an "AS IS" BASIS,
1:  * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
1:  * See the License for the specific language governing permissions and
1:  * limitations under the License.
1:  */
1: 
1: package org.apache.cassandra.db;
1: 
1: import java.util.List;
1: 
1: import org.junit.BeforeClass;
1: import org.junit.Test;
1: 
1: import org.apache.cassandra.SchemaLoader;
1: import org.apache.cassandra.Util;
1: import org.apache.cassandra.config.CFMetaData;
1: import org.apache.cassandra.db.marshal.AsciiType;
1: import org.apache.cassandra.db.marshal.BytesType;
1: import org.apache.cassandra.db.partitions.FilteredPartition;
1: import org.apache.cassandra.exceptions.ConfigurationException;
1: import org.apache.cassandra.schema.KeyspaceParams;
1: import org.apache.cassandra.utils.ByteBufferUtil;
1: 
1: import static org.junit.Assert.assertEquals;
1: 
1: public class ReadCommandTest
1: {
1:     private static final String KEYSPACE = "ReadCommandTest";
1:     private static final String CF1 = "Standard1";
1:     private static final String CF2 = "Standard2";
1: 
1:     @BeforeClass
1:     public static void defineSchema() throws ConfigurationException
1:     {
1:         CFMetaData metadata1 = SchemaLoader.standardCFMD(KEYSPACE, CF1);
1: 
1:         CFMetaData metadata2 = CFMetaData.Builder.create(KEYSPACE, CF2)
1:                                                          .addPartitionKey("key", BytesType.instance)
1:                                                          .addClusteringColumn("col", AsciiType.instance)
1:                                                          .addRegularColumn("a", AsciiType.instance)
1:                                                          .addRegularColumn("b", AsciiType.instance).build();
1: 
1:         SchemaLoader.prepareServer();
1:         SchemaLoader.createKeyspace(KEYSPACE,
1:                                     KeyspaceParams.simple(1),
1:                                     metadata1,
0:                                     metadata2);
1:     }
1: 
1:     @Test
1:     public void testPartitionRangeAbort() throws Exception
1:     {
1:         ColumnFamilyStore cfs = Keyspace.open(KEYSPACE).getColumnFamilyStore(CF1);
1: 
1:         new RowUpdateBuilder(cfs.metadata, 0, ByteBufferUtil.bytes("key1"))
1:                 .clustering("Column1")
1:                 .add("val", ByteBufferUtil.bytes("abcd"))
1:                 .build()
1:                 .apply();
1: 
1:         cfs.forceBlockingFlush();
1: 
1:         new RowUpdateBuilder(cfs.metadata, 0, ByteBufferUtil.bytes("key2"))
1:                 .clustering("Column1")
1:                 .add("val", ByteBufferUtil.bytes("abcd"))
1:                 .build()
1:                 .apply();
1: 
1:         ReadCommand readCommand = Util.cmd(cfs).build();
1:         assertEquals(2, Util.getAll(readCommand).size());
1: 
1:         readCommand.abort();
1:         assertEquals(0, Util.getAll(readCommand).size());
1:     }
1: 
1:     @Test
1:     public void testSinglePartitionSliceAbort() throws Exception
1:     {
1:         ColumnFamilyStore cfs = Keyspace.open(KEYSPACE).getColumnFamilyStore(CF2);
1: 
1:         new RowUpdateBuilder(cfs.metadata, 0, ByteBufferUtil.bytes("key"))
1:                 .clustering("cc")
1:                 .add("a", ByteBufferUtil.bytes("abcd"))
1:                 .build()
1:                 .apply();
1: 
1:         cfs.forceBlockingFlush();
1: 
1:         new RowUpdateBuilder(cfs.metadata, 0, ByteBufferUtil.bytes("key"))
1:                 .clustering("dd")
1:                 .add("a", ByteBufferUtil.bytes("abcd"))
1:                 .build()
1:                 .apply();
1: 
1:         ReadCommand readCommand = Util.cmd(cfs, Util.dk("key")).build();
1: 
1:         List<FilteredPartition> partitions = Util.getAll(readCommand);
1:         assertEquals(1, partitions.size());
1:         assertEquals(2, partitions.get(0).rowCount());
1: 
1:         readCommand.abort();
1:         assertEquals(0, Util.getAll(readCommand).size());
1:     }
1: 
1:     @Test
1:     public void testSinglePartitionNamesAbort() throws Exception
1:     {
1:         ColumnFamilyStore cfs = Keyspace.open(KEYSPACE).getColumnFamilyStore(CF2);
1: 
1:         new RowUpdateBuilder(cfs.metadata, 0, ByteBufferUtil.bytes("key"))
1:                 .clustering("cc")
1:                 .add("a", ByteBufferUtil.bytes("abcd"))
1:                 .build()
1:                 .apply();
1: 
1:         cfs.forceBlockingFlush();
1: 
1:         new RowUpdateBuilder(cfs.metadata, 0, ByteBufferUtil.bytes("key"))
0:                 .clustering("cdd")
1:                 .add("a", ByteBufferUtil.bytes("abcd"))
1:                 .build()
1:                 .apply();
1: 
1:         ReadCommand readCommand = Util.cmd(cfs, Util.dk("key")).includeRow("cc").includeRow("dd").build();
1: 
1:         List<FilteredPartition> partitions = Util.getAll(readCommand);
1:         assertEquals(1, partitions.size());
1:         assertEquals(2, partitions.get(0).rowCount());
1: 
1:         readCommand.abort();
1:         assertEquals(0, Util.getAll(readCommand).size());
1:     }
1: }
author:Robert Stupp
-------------------------------------------------------------------------------
commit:9797511
/////////////////////////////////////////////////////////////////////////
1: import org.apache.cassandra.config.DatabaseDescriptor;
/////////////////////////////////////////////////////////////////////////
1:         DatabaseDescriptor.daemonInitialization();
1: 
author:Jeff Jirsa
-------------------------------------------------------------------------------
commit:881e2da
/////////////////////////////////////////////////////////////////////////
1:         cfs.truncateBlocking();
1: 
/////////////////////////////////////////////////////////////////////////
1:         cfs.truncateBlocking();
1: 
/////////////////////////////////////////////////////////////////////////
1:                 .clustering("dd")
============================================================================