1:56e0ad1: /**
1:56e0ad1:  * Licensed to the Apache Software Foundation (ASF) under one
1:56e0ad1:  * or more contributor license agreements.  See the NOTICE file
1:56e0ad1:  * distributed with this work for additional information
1:56e0ad1:  * regarding copyright ownership.  The ASF licenses this file
1:56e0ad1:  * to you under the Apache License, Version 2.0 (the
1:56e0ad1:  * "License"); you may not use this file except in compliance
1:56e0ad1:  * with the License.  You may obtain a copy of the License at
1:56e0ad1:  *
1:56e0ad1:  *     http://www.apache.org/licenses/LICENSE-2.0
1:56e0ad1:  *
1:56e0ad1:  * Unless required by applicable law or agreed to in writing, software
1:56e0ad1:  * distributed under the License is distributed on an "AS IS" BASIS,
1:56e0ad1:  * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
1:56e0ad1:  * See the License for the specific language governing permissions and
1:56e0ad1:  * limitations under the License.
1:56e0ad1:  */
10:56e0ad1: 
1:56e0ad1: import java.io.IOException;
1:56e0ad1: import java.nio.ByteBuffer;
1:56e0ad1: import java.nio.charset.CharacterCodingException;
1:56e0ad1: import java.util.*;
1:56e0ad1: 
1:56e0ad1: import org.slf4j.Logger;
1:56e0ad1: import org.slf4j.LoggerFactory;
1:56e0ad1: 
1:a004779: import org.apache.cassandra.hadoop.cql3.CqlConfigHelper;
1:d7cb970: import org.apache.cassandra.hadoop.cql3.CqlInputFormat;
1:56e0ad1: import org.apache.hadoop.conf.Configuration;
1:56e0ad1: import org.apache.hadoop.conf.Configured;
1:56e0ad1: import org.apache.hadoop.fs.Path;
1:56e0ad1: import org.apache.hadoop.io.Text;
1:56e0ad1: import org.apache.hadoop.io.LongWritable;
1:56e0ad1: import org.apache.hadoop.mapreduce.Job;
1:56e0ad1: import org.apache.hadoop.mapreduce.Mapper;
1:b4f2ff1: import org.apache.hadoop.mapreduce.Reducer;
1:56e0ad1: import org.apache.hadoop.mapreduce.lib.output.FileOutputFormat;
1:56e0ad1: import org.apache.hadoop.util.Tool;
1:56e0ad1: import org.apache.hadoop.util.ToolRunner;
1:d7cb970: import com.datastax.driver.core.Row;
1:56e0ad1: import org.apache.cassandra.hadoop.ConfigHelper;
1:56e0ad1: import org.apache.cassandra.utils.ByteBufferUtil;
1:b0841d8: 
1:56e0ad1: 
1:56e0ad1: /**
1:56e0ad1:  * This sums the word count stored in the input_words_count ColumnFamily for the key "sum".
1:56e0ad1:  *
1:56e0ad1:  * Output is written to a text file.
1:56e0ad1:  */
1:56e0ad1: public class WordCountCounters extends Configured implements Tool
1:3b708f9: {
1:56e0ad1:     private static final Logger logger = LoggerFactory.getLogger(WordCountCounters.class);
1:b4f2ff1: 
1:d7cb970:     static final String INPUT_MAPPER_VAR = "input_mapper";
1:56e0ad1:     static final String COUNTER_COLUMN_FAMILY = "input_words_count";
1:56e0ad1:     private static final String OUTPUT_PATH_PREFIX = "/tmp/word_count_counters";
1:56e0ad1: 
1:56e0ad1:     public static void main(String[] args) throws Exception
1:3b708f9:     {
1:56e0ad1:         // Let ToolRunner handle generic command-line options
1:56e0ad1:         ToolRunner.run(new Configuration(), new WordCountCounters(), args);
1:56e0ad1:         System.exit(0);
1:3b708f9:     }
1:b0841d8: 
1:d7cb970:     public static class SumNativeMapper extends Mapper<Long, Row, Text, LongWritable>
1:d7cb970:     {
1:d7cb970:         long sum = -1;
1:d7cb970:         public void map(Long key, Row row, Context context) throws IOException, InterruptedException
1:d7cb970:         {   
1:d7cb970:             if (sum < 0)
1:d7cb970:                 sum = 0;
1:d7cb970: 
1:d7cb970:             logger.debug("read " + key + ":count_num from " + context.getInputSplit());
1:d7cb970:             sum += Long.valueOf(row.getString("count_num"));
1:d7cb970:         }
1:d7cb970: 
1:d7cb970:         protected void cleanup(Context context) throws IOException, InterruptedException {
1:d7cb970:             if (sum > 0)
1:d7cb970:                 context.write(new Text("total_count"), new LongWritable(sum));
1:d7cb970:         }
1:d7cb970:     }
1:d7cb970: 
1:56e0ad1:     public static class SumMapper extends Mapper<Map<String, ByteBuffer>, Map<String, ByteBuffer>, Text, LongWritable>
1:3b708f9:     {
1:3b708f9:         long sum = -1;
1:3b708f9: 
1:56e0ad1:         public void map(Map<String, ByteBuffer> key, Map<String, ByteBuffer> columns, Context context) throws IOException, InterruptedException
1:3b708f9:         {   
1:3b708f9:             if (sum < 0)
1:3b708f9:                 sum = 0;
1:3b708f9: 
1:56e0ad1:             logger.debug("read " + toString(key) + ":count_num from " + context.getInputSplit());
1:56e0ad1:             sum += Long.valueOf(ByteBufferUtil.string(columns.get("count_num")));
1:3b708f9:         }
1:3b708f9: 
1:3b708f9:         protected void cleanup(Context context) throws IOException, InterruptedException {
1:3b708f9:             if (sum > 0)
1:3b708f9:                 context.write(new Text("total_count"), new LongWritable(sum));
1:3b708f9:         }
1:56e0ad1: 
1:56e0ad1:         private String toString(Map<String, ByteBuffer> keys)
6:56e0ad1:         {
1:56e0ad1:             String result = "";
1:56e0ad1:             try
1:56e0ad1:             {
1:56e0ad1:                 for (ByteBuffer key : keys.values())
1:56e0ad1:                     result = result + ByteBufferUtil.string(key) + ":";
1:3b708f9:             }
1:56e0ad1:             catch (CharacterCodingException e)
1:56e0ad1:             {
1:56e0ad1:                 logger.error("Failed to print keys", e);
7:56e0ad1:             }
1:56e0ad1:             return result;
1:56e0ad1:         }
1:56e0ad1:     }
1:56e0ad1: 
1:b4f2ff1:     public static class ReducerToFilesystem extends Reducer<Text, LongWritable, Text, LongWritable>
1:b4f2ff1:     {
1:b4f2ff1:         long sum = 0;
1:b4f2ff1: 
1:b4f2ff1:         public void reduce(Text key, Iterable<LongWritable> values, Context context) throws IOException, InterruptedException
1:b4f2ff1:         {
1:b4f2ff1:             for (LongWritable val : values)
1:b4f2ff1:                 sum += val.get();
1:b4f2ff1:             context.write(key, new LongWritable(sum));
1:b4f2ff1:         }
1:b4f2ff1:     }
1:b4f2ff1: 
1:56e0ad1:     public int run(String[] args) throws Exception
1:3b708f9:     {
1:d7cb970:         String inputMapperType = "native";
1:d7cb970:         if (args != null && args[0].startsWith(INPUT_MAPPER_VAR))
1:d7cb970:         {
1:d7cb970:             String[] arg0 = args[0].split("=");
1:d7cb970:             if (arg0 != null && arg0.length == 2)
1:d7cb970:                 inputMapperType = arg0[1];
1:d7cb970:         }
1:56e0ad1:         Job job = new Job(getConf(), "wordcountcounters");
1:56e0ad1: 
1:b4f2ff1:         job.setCombinerClass(ReducerToFilesystem.class);
1:b4f2ff1:         job.setReducerClass(ReducerToFilesystem.class);
1:d7cb970:         job.setJarByClass(WordCountCounters.class); 
1:b0841d8: 
1:56e0ad1:         ConfigHelper.setInputInitialAddress(job.getConfiguration(), "localhost");
1:56e0ad1:         ConfigHelper.setInputPartitioner(job.getConfiguration(), "Murmur3Partitioner");
1:56e0ad1:         ConfigHelper.setInputColumnFamily(job.getConfiguration(), WordCount.KEYSPACE, WordCount.OUTPUT_COLUMN_FAMILY);
1:56e0ad1: 
1:a004779:         CqlConfigHelper.setInputCQLPageRowSize(job.getConfiguration(), "3");
1:d7cb970:         if ("native".equals(inputMapperType))
1:d7cb970:         {
1:d7cb970:             job.setMapperClass(SumNativeMapper.class);
1:e550ea6:             job.setInputFormatClass(CqlInputFormat.class);
1:d7cb970:             CqlConfigHelper.setInputCql(job.getConfiguration(), "select * from " + WordCount.OUTPUT_COLUMN_FAMILY + " where token(word) > ? and token(word) <= ? allow filtering");
1:d7cb970:         }
1:d7cb970:         else
1:d7cb970:         {
1:d7cb970:             job.setMapperClass(SumMapper.class);
1:f6d0f43:             job.setInputFormatClass(CqlInputFormat.class);
1:d7cb970:             ConfigHelper.setInputRpcPort(job.getConfiguration(), "9160");
1:d7cb970:         }
1:56e0ad1: 
1:d7cb970:         job.setOutputKeyClass(Text.class);
1:d7cb970:         job.setOutputValueClass(LongWritable.class);
1:d7cb970:         FileOutputFormat.setOutputPath(job, new Path(OUTPUT_PATH_PREFIX));
1:56e0ad1:         job.waitForCompletion(true);
1:56e0ad1:         return 0;
1:3b708f9:     }
1:3b708f9: }
============================================================================
author:Brandon Williams
-------------------------------------------------------------------------------
commit:f6d0f43
/////////////////////////////////////////////////////////////////////////
/////////////////////////////////////////////////////////////////////////
1:             job.setInputFormatClass(CqlInputFormat.class);
author:Aleksey Yeschenko
-------------------------------------------------------------------------------
commit:e27cdf9
/////////////////////////////////////////////////////////////////////////
0: import org.apache.cassandra.hadoop.cql3.CqlPagingInputFormat;
/////////////////////////////////////////////////////////////////////////
0:             job.setInputFormatClass(CqlPagingInputFormat.class);
author:rekhajoshm
-------------------------------------------------------------------------------
commit:e550ea6
/////////////////////////////////////////////////////////////////////////
/////////////////////////////////////////////////////////////////////////
1:             job.setInputFormatClass(CqlInputFormat.class);
author:Sylvain Lebresne
-------------------------------------------------------------------------------
commit:d7cb970
/////////////////////////////////////////////////////////////////////////
1: import org.apache.cassandra.hadoop.cql3.CqlInputFormat;
/////////////////////////////////////////////////////////////////////////
1: import com.datastax.driver.core.Row;
/////////////////////////////////////////////////////////////////////////
1:     static final String INPUT_MAPPER_VAR = "input_mapper";
/////////////////////////////////////////////////////////////////////////
1:     public static class SumNativeMapper extends Mapper<Long, Row, Text, LongWritable>
1:     {
1:         long sum = -1;
1:         public void map(Long key, Row row, Context context) throws IOException, InterruptedException
1:         {   
1:             if (sum < 0)
1:                 sum = 0;
1: 
1:             logger.debug("read " + key + ":count_num from " + context.getInputSplit());
1:             sum += Long.valueOf(row.getString("count_num"));
1:         }
1: 
1:         protected void cleanup(Context context) throws IOException, InterruptedException {
1:             if (sum > 0)
1:                 context.write(new Text("total_count"), new LongWritable(sum));
1:         }
1:     }
1: 
/////////////////////////////////////////////////////////////////////////
/////////////////////////////////////////////////////////////////////////
1:         String inputMapperType = "native";
1:         if (args != null && args[0].startsWith(INPUT_MAPPER_VAR))
1:         {
1:             String[] arg0 = args[0].split("=");
1:             if (arg0 != null && arg0.length == 2)
1:                 inputMapperType = arg0[1];
1:         }
1:         job.setJarByClass(WordCountCounters.class); 
1:         if ("native".equals(inputMapperType))
1:         {
1:             job.setMapperClass(SumNativeMapper.class);
0:             job.setInputFormatClass(CqlInputFormat.class);
1:             CqlConfigHelper.setInputCql(job.getConfiguration(), "select * from " + WordCount.OUTPUT_COLUMN_FAMILY + " where token(word) > ? and token(word) <= ? allow filtering");
1:         }
1:         else
1:         {
1:             job.setMapperClass(SumMapper.class);
0:             job.setInputFormatClass(CqlPagingInputFormat.class);
1:             ConfigHelper.setInputRpcPort(job.getConfiguration(), "9160");
1:         }
1:         job.setOutputKeyClass(Text.class);
1:         job.setOutputValueClass(LongWritable.class);
1:         FileOutputFormat.setOutputPath(job, new Path(OUTPUT_PATH_PREFIX));
commit:b0841d8
/////////////////////////////////////////////////////////////////////////
/////////////////////////////////////////////////////////////////////////
1: 
/////////////////////////////////////////////////////////////////////////
/////////////////////////////////////////////////////////////////////////
/////////////////////////////////////////////////////////////////////////
1:     
/////////////////////////////////////////////////////////////////////////
0:         job.setJarByClass(WordCountCounters.class);
0:         job.setMapperClass(SumMapper.class);
0:         job.setOutputKeyClass(Text.class);
0:         job.setOutputValueClass(LongWritable.class);
0:         FileOutputFormat.setOutputPath(job, new Path(OUTPUT_PATH_PREFIX));
0:         job.setInputFormatClass(CqlPagingInputFormat.class);
1: 
0:         ConfigHelper.setInputRpcPort(job.getConfiguration(), "9160");
commit:3b708f9
/////////////////////////////////////////////////////////////////////////
0: import org.apache.cassandra.hadoop.cql3.CqlInputFormat;
/////////////////////////////////////////////////////////////////////////
0: import com.datastax.driver.core.Row;
/////////////////////////////////////////////////////////////////////////
0:     static final String INPUT_MAPPER_VAR = "input_mapper";
/////////////////////////////////////////////////////////////////////////
0:     public static class SumNativeMapper extends Mapper<Long, Row, Text, LongWritable>
1:     {
1:         long sum = -1;
0:         public void map(Long key, Row row, Context context) throws IOException, InterruptedException
1:         {   
1:             if (sum < 0)
1:                 sum = 0;
1: 
0:             logger.debug("read " + key + ":count_num from " + context.getInputSplit());
0:             sum += Long.valueOf(row.getString("count_num"));
1:         }
1: 
1:         protected void cleanup(Context context) throws IOException, InterruptedException {
1:             if (sum > 0)
1:                 context.write(new Text("total_count"), new LongWritable(sum));
1:         }
1:     }
1: 
/////////////////////////////////////////////////////////////////////////
/////////////////////////////////////////////////////////////////////////
0:         String inputMapperType = "native";
0:         if (args != null && args[0].startsWith(INPUT_MAPPER_VAR))
1:         {
0:             String[] arg0 = args[0].split("=");
0:             if (arg0 != null && arg0.length == 2)
0:                 inputMapperType = arg0[1];
1:         }
0:         job.setJarByClass(WordCountCounters.class); 
0:         if ("native".equals(inputMapperType))
1:         {
0:             job.setMapperClass(SumNativeMapper.class);
0:             job.setInputFormatClass(CqlInputFormat.class);
0:             CqlConfigHelper.setInputCql(job.getConfiguration(), "select * from " + WordCount.OUTPUT_COLUMN_FAMILY + " where token(word) > ? and token(word) <= ? allow filtering");
1:         }
0:         else
1:         {
0:             job.setMapperClass(SumMapper.class);
0:             job.setInputFormatClass(CqlPagingInputFormat.class);
0:             ConfigHelper.setInputRpcPort(job.getConfiguration(), "9160");
1:         }
0:         job.setOutputKeyClass(Text.class);
0:         job.setOutputValueClass(LongWritable.class);
0:         FileOutputFormat.setOutputPath(job, new Path(OUTPUT_PATH_PREFIX));
author:Jonathan Ellis
-------------------------------------------------------------------------------
commit:b4f2ff1
/////////////////////////////////////////////////////////////////////////
1: import org.apache.hadoop.mapreduce.Reducer;
/////////////////////////////////////////////////////////////////////////
1: 
/////////////////////////////////////////////////////////////////////////
1:     public static class ReducerToFilesystem extends Reducer<Text, LongWritable, Text, LongWritable>
1:     {
1:         long sum = 0;
1: 
1:         public void reduce(Text key, Iterable<LongWritable> values, Context context) throws IOException, InterruptedException
1:         {
1:             for (LongWritable val : values)
1:                 sum += val.get();
1:             context.write(key, new LongWritable(sum));
1:         }
1:     }
1: 
1:         job.setCombinerClass(ReducerToFilesystem.class);
1:         job.setReducerClass(ReducerToFilesystem.class);
commit:a004779
/////////////////////////////////////////////////////////////////////////
1: import org.apache.cassandra.hadoop.cql3.CqlConfigHelper;
0: import org.apache.cassandra.hadoop.cql3.CqlPagingInputFormat;
/////////////////////////////////////////////////////////////////////////
0:         job.setInputFormatClass(CqlPagingInputFormat.class);
1:         CqlConfigHelper.setInputCQLPageRowSize(job.getConfiguration(), "3");
commit:02a7ba8
/////////////////////////////////////////////////////////////////////////
0: import org.apache.cassandra.hadoop.cql3.CqlConfigHelper;
0: import org.apache.cassandra.hadoop.cql3.CqlPagingInputFormat;
/////////////////////////////////////////////////////////////////////////
0:         job.setInputFormatClass(CqlPagingInputFormat.class);
0:         CqlConfigHelper.setInputCQLPageRowSize(job.getConfiguration(), "3");
commit:56e0ad1
/////////////////////////////////////////////////////////////////////////
1: /**
1:  * Licensed to the Apache Software Foundation (ASF) under one
1:  * or more contributor license agreements.  See the NOTICE file
1:  * distributed with this work for additional information
1:  * regarding copyright ownership.  The ASF licenses this file
1:  * to you under the Apache License, Version 2.0 (the
1:  * "License"); you may not use this file except in compliance
1:  * with the License.  You may obtain a copy of the License at
1:  *
1:  *     http://www.apache.org/licenses/LICENSE-2.0
1:  *
1:  * Unless required by applicable law or agreed to in writing, software
1:  * distributed under the License is distributed on an "AS IS" BASIS,
1:  * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
1:  * See the License for the specific language governing permissions and
1:  * limitations under the License.
1:  */
1: 
1: import java.io.IOException;
1: import java.nio.ByteBuffer;
1: import java.nio.charset.CharacterCodingException;
1: import java.util.*;
1: 
1: import org.slf4j.Logger;
1: import org.slf4j.LoggerFactory;
1: 
1: import org.apache.hadoop.conf.Configuration;
1: import org.apache.hadoop.conf.Configured;
1: import org.apache.hadoop.fs.Path;
0: import org.apache.hadoop.io.IntWritable;
1: import org.apache.hadoop.io.Text;
1: import org.apache.hadoop.io.LongWritable;
1: import org.apache.hadoop.mapreduce.Job;
1: import org.apache.hadoop.mapreduce.Mapper;
0: import org.apache.hadoop.mapreduce.Mapper.Context;
1: import org.apache.hadoop.mapreduce.lib.output.FileOutputFormat;
1: import org.apache.hadoop.util.Tool;
1: import org.apache.hadoop.util.ToolRunner;
1: 
0: import org.apache.cassandra.hadoop.cql3.ColumnFamilyInputFormat;
1: import org.apache.cassandra.hadoop.ConfigHelper;
0: import org.apache.cassandra.hadoop.cql3.CQLConfigHelper;
0: import org.apache.cassandra.thrift.*;
1: import org.apache.cassandra.utils.ByteBufferUtil;
1: 
1: 
1: /**
1:  * This sums the word count stored in the input_words_count ColumnFamily for the key "sum".
1:  *
1:  * Output is written to a text file.
1:  */
1: public class WordCountCounters extends Configured implements Tool
1: {
1:     private static final Logger logger = LoggerFactory.getLogger(WordCountCounters.class);
1: 
1:     static final String COUNTER_COLUMN_FAMILY = "input_words_count";
1:     private static final String OUTPUT_PATH_PREFIX = "/tmp/word_count_counters";
1: 
1:     public static void main(String[] args) throws Exception
1:     {
1:         // Let ToolRunner handle generic command-line options
1:         ToolRunner.run(new Configuration(), new WordCountCounters(), args);
1:         System.exit(0);
1:     }
1: 
1:     public static class SumMapper extends Mapper<Map<String, ByteBuffer>, Map<String, ByteBuffer>, Text, LongWritable>
1:     {
0:         long sum = -1;
1:         public void map(Map<String, ByteBuffer> key, Map<String, ByteBuffer> columns, Context context) throws IOException, InterruptedException
1:         {   
0:             if (sum < 0)
0:                 sum = 0;
1: 
1:             logger.debug("read " + toString(key) + ":count_num from " + context.getInputSplit());
1:             sum += Long.valueOf(ByteBufferUtil.string(columns.get("count_num")));
1:         }
1: 
0:         protected void cleanup(Context context) throws IOException, InterruptedException {
0:             if (sum > 0)
0:                 context.write(new Text("total_count"), new LongWritable(sum));
1:         }
1: 
1:         private String toString(Map<String, ByteBuffer> keys)
1:         {
1:             String result = "";
1:             try
1:             {
1:                 for (ByteBuffer key : keys.values())
1:                     result = result + ByteBufferUtil.string(key) + ":";
1:             }
1:             catch (CharacterCodingException e)
1:             {
1:                 logger.error("Failed to print keys", e);
1:             }
1:             return result;
1:         }
1:     }
1: 
1:     
1:     public int run(String[] args) throws Exception
1:     {
1:         Job job = new Job(getConf(), "wordcountcounters");
0:         job.setJarByClass(WordCountCounters.class);
0:         job.setMapperClass(SumMapper.class);
1: 
0:         job.setOutputKeyClass(Text.class);
0:         job.setOutputValueClass(LongWritable.class);
0:         FileOutputFormat.setOutputPath(job, new Path(OUTPUT_PATH_PREFIX));
1: 
0:         job.setInputFormatClass(ColumnFamilyInputFormat.class);
1: 
0:         ConfigHelper.setInputRpcPort(job.getConfiguration(), "9160");
1:         ConfigHelper.setInputInitialAddress(job.getConfiguration(), "localhost");
1:         ConfigHelper.setInputPartitioner(job.getConfiguration(), "Murmur3Partitioner");
1:         ConfigHelper.setInputColumnFamily(job.getConfiguration(), WordCount.KEYSPACE, WordCount.OUTPUT_COLUMN_FAMILY);
1: 
0:         CQLConfigHelper.setInputCQLPageRowSize(job.getConfiguration(), "3");
1: 
1:         job.waitForCompletion(true);
1:         return 0;
1:     }
1: }
============================================================================