1:d7cb970: /*
1:d7cb970:  * Licensed to the Apache Software Foundation (ASF) under one
1:d7cb970:  * or more contributor license agreements.  See the NOTICE file
1:d7cb970:  * distributed with this work for additional information
1:d7cb970:  * regarding copyright ownership.  The ASF licenses this file
1:d7cb970:  * to you under the Apache License, Version 2.0 (the
1:d7cb970:  * "License"); you may not use this file except in compliance
1:d7cb970:  * with the License.  You may obtain a copy of the License at
1:d7cb970:  *
1:d7cb970:  *     http://www.apache.org/licenses/LICENSE-2.0
1:d7cb970:  *
1:d7cb970:  * Unless required by applicable law or agreed to in writing, software
1:d7cb970:  * distributed under the License is distributed on an "AS IS" BASIS,
1:d7cb970:  * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
1:d7cb970:  * See the License for the specific language governing permissions and
1:d7cb970:  * limitations under the License.
1:d7cb970:  */
1:d7cb970: package org.apache.cassandra.hadoop.cql3;
9:d7cb970: 
1:d7cb970: import java.io.IOException;
1:d7cb970: import java.math.BigDecimal;
1:d7cb970: import java.math.BigInteger;
1:d7cb970: import java.net.InetAddress;
1:d7cb970: import java.nio.ByteBuffer;
1:3e9d345: import java.util.*;
1:d7cb970: 
1:52df514: import com.google.common.base.Function;
1:52df514: import com.google.common.base.Joiner;
1:52df514: import com.google.common.base.Splitter;
1:06c130e: 
1:06c130e: import com.datastax.driver.core.TypeCodec;
1:489a9e8: import org.apache.cassandra.utils.AbstractIterator;
1:52df514: import com.google.common.collect.Iterables;
1:d7cb970: import com.google.common.collect.Maps;
1:abbf634: import org.apache.commons.lang3.StringUtils;
1:d7cb970: import org.slf4j.Logger;
1:d7cb970: import org.slf4j.LoggerFactory;
1:52df514: 
1:d7cb970: import com.datastax.driver.core.Cluster;
1:d7cb970: import com.datastax.driver.core.ColumnDefinitions;
1:d7cb970: import com.datastax.driver.core.ColumnMetadata;
1:a133992: import com.datastax.driver.core.LocalDate;
1:f698cc2: import com.datastax.driver.core.Metadata;
1:d7cb970: import com.datastax.driver.core.ResultSet;
1:d7cb970: import com.datastax.driver.core.Row;
1:d7cb970: import com.datastax.driver.core.Session;
1:f698cc2: import com.datastax.driver.core.TableMetadata;
1:f698cc2: import com.datastax.driver.core.Token;
1:b94c87a: import com.datastax.driver.core.TupleValue;
1:b94c87a: import com.datastax.driver.core.UDTValue;
1:f698cc2: import com.google.common.reflect.TypeToken;
1:d7cb970: import org.apache.cassandra.db.marshal.AbstractType;
1:d7cb970: import org.apache.cassandra.dht.IPartitioner;
1:d7cb970: import org.apache.cassandra.hadoop.ColumnFamilySplit;
1:d7cb970: import org.apache.cassandra.hadoop.ConfigHelper;
1:a3b9d56: import org.apache.cassandra.hadoop.HadoopCompat;
1:acf1b18: import org.apache.cassandra.utils.ByteBufferUtil;
1:d7cb970: import org.apache.cassandra.utils.Pair;
1:d7cb970: import org.apache.hadoop.conf.Configuration;
1:d7cb970: import org.apache.hadoop.mapreduce.InputSplit;
1:d7cb970: import org.apache.hadoop.mapreduce.RecordReader;
1:d7cb970: import org.apache.hadoop.mapreduce.TaskAttemptContext;
1:b94c87a: 
1:d7cb970: /**
1:222ea95:  * <p>
1:d7cb970:  * CqlRecordReader reads the rows return from the CQL query
1:d7cb970:  * It uses CQL auto-paging.
1:222ea95:  * </p>
1:222ea95:  * <p>
1:d7cb970:  * Return a Long as a local CQL row key starts from 0;
1:222ea95:  * </p>
1:222ea95:  * {@code
1:d7cb970:  * Row as C* java driver CQL result set row
1:d7cb970:  * 1) select clause must include partition key columns (to calculate the progress based on the actual CF row processed)
1:d7cb970:  * 2) where clause must include token(partition_key1, ...  , partition_keyn) > ? and 
1:d7cb970:  *       token(partition_key1, ... , partition_keyn) <= ?  (in the right order) 
1:222ea95:  * }
1:d7cb970:  */
1:d7cb970: public class CqlRecordReader extends RecordReader<Long, Row>
1:e6668be:         implements org.apache.hadoop.mapred.RecordReader<Long, Row>, AutoCloseable
8:d7cb970: {
1:d7cb970:     private static final Logger logger = LoggerFactory.getLogger(CqlRecordReader.class);
1:52df514: 
1:d7cb970:     private ColumnFamilySplit split;
1:d7cb970:     private RowIterator rowIterator;
1:3e9d345: 
1:d7cb970:     private Pair<Long, Row> currentRow;
1:d7cb970:     private int totalRowCount; // total number of rows to fetch
1:d7cb970:     private String keyspace;
1:d7cb970:     private String cfName;
1:d7cb970:     private String cqlQuery;
1:d7cb970:     private Cluster cluster;
1:d7cb970:     private Session session;
1:d7cb970:     private IPartitioner partitioner;
1:52df514:     private String inputColumns;
1:52df514:     private String userDefinedWhereClauses;
1:52df514: 
1:52df514:     private List<String> partitionKeys = new ArrayList<>();
1:d7cb970: 
1:d7cb970:     // partition keys -- key aliases
1:d7cb970:     private LinkedHashMap<String, Boolean> partitionBoundColumns = Maps.newLinkedHashMap();
1:1cb426b:     protected int nativeProtocolVersion = 1;
1:d7cb970: 
1:d7cb970:     public CqlRecordReader()
1:d7cb970:     {
1:d7cb970:         super();
8:d7cb970:     }
1:52df514: 
1:1d5d303:     @Override
1:d7cb970:     public void initialize(InputSplit split, TaskAttemptContext context) throws IOException
1:0356ee7:     {
1:d7cb970:         this.split = (ColumnFamilySplit) split;
1:a3b9d56:         Configuration conf = HadoopCompat.getConfiguration(context);
1:d7cb970:         totalRowCount = (this.split.getLength() < Long.MAX_VALUE)
1:d7cb970:                       ? (int) this.split.getLength()
1:d7cb970:                       : ConfigHelper.getInputSplitSize(conf);
1:7049ee0:         cfName = ConfigHelper.getInputColumnFamily(conf);
1:7049ee0:         keyspace = ConfigHelper.getInputKeyspace(conf);
1:52df514:         partitioner = ConfigHelper.getInputPartitioner(conf);
1:52df514:         inputColumns = CqlConfigHelper.getInputcolumns(conf);
1:52df514:         userDefinedWhereClauses = CqlConfigHelper.getInputWhereClauses(conf);
1:b10c2ab: 
1:52df514:         try
1:52df514:         {
1:d7cb970:             if (cluster != null)
1:d7cb970:                 return;
1:52df514: 
1:fe39eb7:             // create a Cluster instance
1:d7cb970:             String[] locations = split.getLocations();
1:fe39eb7:             cluster = CqlConfigHelper.getInputCluster(locations, conf);
1:52df514:         }
2:d7cb970:         catch (Exception e)
1:52df514:         {
1:d7cb970:             throw new RuntimeException(e);
1:52df514:         }
1:52df514: 
1:d7cb970:         if (cluster != null)
1:7ddcf3e:             session = cluster.connect(quote(keyspace));
1:d7cb970: 
1:52df514:         if (session == null)
1:52df514:           throw new RuntimeException("Can't create connection session");
1:52df514: 
1:1cb426b:         //get negotiated serialization protocol
1:16b0288:         nativeProtocolVersion = cluster.getConfiguration().getProtocolOptions().getProtocolVersion().toInt();
1:1cb426b: 
1:52df514:         // If the user provides a CQL query then we will use it without validation
1:52df514:         // otherwise we will fall back to building a query using the:
1:52df514:         //   inputColumns
1:52df514:         //   whereClauses
1:52df514:         cqlQuery = CqlConfigHelper.getInputCql(conf);
1:7049ee0:         // validate that the user hasn't tried to give us a custom query along with input columns
1:7049ee0:         // and where clauses
1:7049ee0:         if (StringUtils.isNotEmpty(cqlQuery) && (StringUtils.isNotEmpty(inputColumns) ||
1:7049ee0:                                                  StringUtils.isNotEmpty(userDefinedWhereClauses)))
1:7049ee0:         {
1:7049ee0:             throw new AssertionError("Cannot define a custom query with input columns and / or where clauses");
1:7049ee0:         }
1:7049ee0: 
1:52df514:         if (StringUtils.isEmpty(cqlQuery))
1:52df514:             cqlQuery = buildQuery();
1:4a849ef:         logger.trace("cqlQuery {}", cqlQuery);
1:52df514: 
1:d7cb970:         rowIterator = new RowIterator();
1:4a849ef:         logger.trace("created {}", rowIterator);
1:d7cb970:     }
1:d7cb970: 
1:d7cb970:     public void close()
1:d7cb970:     {
1:d7cb970:         if (session != null)
1:d7cb970:             session.close();
1:541a20d:         if (cluster != null)
1:541a20d:             cluster.close();
1:d7cb970:     }
1:d7cb970: 
1:d7cb970:     public Long getCurrentKey()
1:d7cb970:     {
1:d7cb970:         return currentRow.left;
1:d7cb970:     }
1:d7cb970: 
1:d7cb970:     public Row getCurrentValue()
1:d7cb970:     {
1:d7cb970:         return currentRow.right;
1:d7cb970:     }
1:d7cb970: 
1:d7cb970:     public float getProgress()
1:d7cb970:     {
1:d7cb970:         if (!rowIterator.hasNext())
1:d7cb970:             return 1.0F;
1:d7cb970: 
1:d7cb970:         // the progress is likely to be reported slightly off the actual but close enough
1:d7cb970:         float progress = ((float) rowIterator.totalRead / totalRowCount);
1:d7cb970:         return progress > 1.0F ? 1.0F : progress;
1:d7cb970:     }
1:d7cb970: 
1:d7cb970:     public boolean nextKeyValue() throws IOException
1:d7cb970:     {
1:d7cb970:         if (!rowIterator.hasNext())
1:d7cb970:         {
1:4a849ef:             logger.trace("Finished scanning {} rows (estimate was: {})", rowIterator.totalRead, totalRowCount);
1:d7cb970:             return false;
1:d7cb970:         }
1:d7cb970: 
3:d7cb970:         try
1:d7cb970:         {
1:d7cb970:             currentRow = rowIterator.next();
1:d7cb970:         }
1:d7cb970:         catch (Exception e)
1:d7cb970:         {
1:d7cb970:             // throw it as IOException, so client can catch it and handle it at client side
1:d7cb970:             IOException ioe = new IOException(e.getMessage());
1:d7cb970:             ioe.initCause(ioe.getCause());
1:d7cb970:             throw ioe;
1:d7cb970:         }
1:d7cb970:         return true;
1:d7cb970:     }
1:d7cb970: 
1:d7cb970:     // Because the old Hadoop API wants us to write to the key and value
1:d7cb970:     // and the new asks for them, we need to copy the output of the new API
1:d7cb970:     // to the old. Thus, expect a small performance hit.
1:d7cb970:     // And obviously this wouldn't work for wide rows. But since ColumnFamilyInputFormat
1:d7cb970:     // and ColumnFamilyRecordReader don't support them, it should be fine for now.
1:d7cb970:     public boolean next(Long key, Row value) throws IOException
1:d7cb970:     {
1:d7cb970:         if (nextKeyValue())
1:d7cb970:         {
1:d7cb970:             ((WrappedRow)value).setRow(getCurrentValue());
1:d7cb970:             return true;
1:d7cb970:         }
1:d7cb970:         return false;
1:d7cb970:     }
1:d7cb970: 
1:d7cb970:     public long getPos() throws IOException
1:d7cb970:     {
1:1fab7b7:         return rowIterator.totalRead;
1:d7cb970:     }
1:d7cb970: 
1:d7cb970:     public Long createKey()
1:d7cb970:     {
1:794d68b:         return Long.valueOf(0L);
1:d7cb970:     }
1:d7cb970: 
1:d7cb970:     public Row createValue()
1:d7cb970:     {
1:d7cb970:         return new WrappedRow();
1:d7cb970:     }
1:d7cb970: 
1:1cb426b:     /**
1:1cb426b:      * Return native version protocol of the cluster connection
1:1cb426b:      * @return serialization protocol version.
1:1cb426b:      */
1:68d2526:     public int getNativeProtocolVersion() 
1:68d2526:     {
1:1cb426b:         return nativeProtocolVersion;
1:1cb426b:     }
1:1cb426b: 
1:d7cb970:     /** CQL row iterator 
1:d7cb970:      *  Input cql query  
1:d7cb970:      *  1) select clause must include key columns (if we use partition key based row count)
1:d7cb970:      *  2) where clause must include token(partition_key1 ... partition_keyn) > ? and 
1:d7cb970:      *     token(partition_key1 ... partition_keyn) <= ? 
1:52df514:      */
1:d7cb970:     private class RowIterator extends AbstractIterator<Pair<Long, Row>>
1:d7cb970:     {
1:d7cb970:         private long keyId = 0L;
1:d7cb970:         protected int totalRead = 0; // total number of cf rows read
1:d7cb970:         protected Iterator<Row> rows;
1:d7cb970:         private Map<String, ByteBuffer> previousRowKey = new HashMap<String, ByteBuffer>(); // previous CF row key
1:d7cb970: 
1:d7cb970:         public RowIterator()
1:d7cb970:         {
1:d7cb970:             AbstractType type = partitioner.getTokenValidator();
1:d7cb970:             ResultSet rs = session.execute(cqlQuery, type.compose(type.fromString(split.getStartToken())), type.compose(type.fromString(split.getEndToken())) );
1:7049ee0:             for (ColumnMetadata meta : cluster.getMetadata().getKeyspace(quote(keyspace)).getTable(quote(cfName)).getPartitionKey())
1:d7cb970:                 partitionBoundColumns.put(meta.getName(), Boolean.TRUE);
1:d7cb970:             rows = rs.iterator();
1:d7cb970:         }
1:d7cb970: 
1:d7cb970:         protected Pair<Long, Row> computeNext()
1:d7cb970:         {
1:d7cb970:             if (rows == null || !rows.hasNext())
1:d7cb970:                 return endOfData();
1:d7cb970: 
1:d7cb970:             Row row = rows.next();
1:f096eb6:             Map<String, ByteBuffer> keyColumns = new HashMap<String, ByteBuffer>(partitionBoundColumns.size()); 
1:d7cb970:             for (String column : partitionBoundColumns.keySet())
1:d7cb970:                 keyColumns.put(column, row.getBytesUnsafe(column));
1:d7cb970: 
1:d7cb970:             // increase total CF row read
1:d7cb970:             if (previousRowKey.isEmpty() && !keyColumns.isEmpty())
1:d7cb970:             {
1:d7cb970:                 previousRowKey = keyColumns;
1:d7cb970:                 totalRead++;
1:d7cb970:             }
1:52df514:             else
1:52df514:             {
1:d7cb970:                 for (String column : partitionBoundColumns.keySet())
1:d7cb970:                 {
1:acf1b18:                     // this is not correct - but we don't seem to have easy access to better type information here
1:acf1b18:                     if (ByteBufferUtil.compareUnsigned(keyColumns.get(column), previousRowKey.get(column)) != 0)
1:d7cb970:                     {
1:d7cb970:                         previousRowKey = keyColumns;
1:d7cb970:                         totalRead++;
2:d7cb970:                         break;
1:52df514:                     }
1:d7cb970:                 }
1:d7cb970:             }
1:d7cb970:             keyId ++;
1:d7cb970:             return Pair.create(keyId, row);
1:d7cb970:         }
1:d7cb970:     }
1:d7cb970: 
1:e615960:     private static class WrappedRow implements Row
1:d7cb970:     {
1:d7cb970:         private Row row;
1:d7cb970: 
1:d7cb970:         public void setRow(Row row)
1:d7cb970:         {
1:d7cb970:             this.row = row;
1:d7cb970:         }
1:d7cb970: 
1:d7cb970:         @Override
1:d7cb970:         public ColumnDefinitions getColumnDefinitions()
1:d7cb970:         {
1:d7cb970:             return row.getColumnDefinitions();
1:d7cb970:         }
1:d7cb970: 
1:d7cb970:         @Override
1:d7cb970:         public boolean isNull(int i)
1:d7cb970:         {
1:d7cb970:             return row.isNull(i);
1:d7cb970:         }
1:d7cb970: 
1:d7cb970:         @Override
1:d7cb970:         public boolean isNull(String name)
1:d7cb970:         {
1:d7cb970:             return row.isNull(name);
1:d7cb970:         }
1:d7cb970: 
1:d7cb970:         @Override
1:16b0288:         public Object getObject(int i)
1:16b0288:         {
1:16b0288:             return row.getObject(i);
1:16b0288:         }
1:16b0288: 
1:16b0288:         @Override
1:06c130e:         public <T> T get(int i, Class<T> aClass)
1:06c130e:         {
1:06c130e:             return row.get(i, aClass);
1:06c130e:         }
1:06c130e: 
1:06c130e:         @Override
1:06c130e:         public <T> T get(int i, TypeToken<T> typeToken)
1:06c130e:         {
1:06c130e:             return row.get(i, typeToken);
1:06c130e:         }
1:06c130e: 
1:06c130e:         @Override
1:06c130e:         public <T> T get(int i, TypeCodec<T> typeCodec)
1:06c130e:         {
1:06c130e:             return row.get(i, typeCodec);
1:06c130e:         }
1:06c130e: 
1:06c130e:         @Override
1:16b0288:         public Object getObject(String s)
1:16b0288:         {
1:16b0288:             return row.getObject(s);
1:16b0288:         }
1:16b0288: 
1:16b0288:         @Override
1:06c130e:         public <T> T get(String s, Class<T> aClass)
1:06c130e:         {
1:06c130e:             return row.get(s, aClass);
1:06c130e:         }
1:06c130e: 
1:06c130e:         @Override
1:06c130e:         public <T> T get(String s, TypeToken<T> typeToken)
1:06c130e:         {
1:06c130e:             return row.get(s, typeToken);
1:06c130e:         }
1:06c130e: 
1:06c130e:         @Override
1:06c130e:         public <T> T get(String s, TypeCodec<T> typeCodec)
1:06c130e:         {
1:06c130e:             return row.get(s, typeCodec);
1:06c130e:         }
1:06c130e: 
1:06c130e:         @Override
1:d7cb970:         public boolean getBool(int i)
1:d7cb970:         {
1:d7cb970:             return row.getBool(i);
1:d7cb970:         }
1:d7cb970: 
1:d7cb970:         @Override
1:d7cb970:         public boolean getBool(String name)
1:d7cb970:         {
1:d7cb970:             return row.getBool(name);
1:d7cb970:         }
1:d7cb970: 
1:d7cb970:         @Override
1:16b0288:         public short getShort(int i)
1:16b0288:         {
1:16b0288:             return row.getShort(i);
1:16b0288:         }
1:16b0288: 
1:16b0288:         @Override
1:16b0288:         public short getShort(String s)
1:16b0288:         {
1:16b0288:             return row.getShort(s);
1:16b0288:         }
1:16b0288: 
1:16b0288:         @Override
1:16b0288:         public byte getByte(int i)
1:16b0288:         {
1:16b0288:             return row.getByte(i);
1:16b0288:         }
1:16b0288: 
1:16b0288:         @Override
1:16b0288:         public byte getByte(String s)
1:16b0288:         {
1:16b0288:             return row.getByte(s);
1:16b0288:         }
1:16b0288: 
1:16b0288:         @Override
1:d7cb970:         public int getInt(int i)
1:d7cb970:         {
1:d7cb970:             return row.getInt(i);
1:d7cb970:         }
1:d7cb970: 
1:d7cb970:         @Override
1:d7cb970:         public int getInt(String name)
1:d7cb970:         {
1:d7cb970:             return row.getInt(name);
1:d7cb970:         }
1:d7cb970: 
1:d7cb970:         @Override
1:d7cb970:         public long getLong(int i)
1:d7cb970:         {
1:d7cb970:             return row.getLong(i);
1:d7cb970:         }
1:d7cb970: 
1:d7cb970:         @Override
1:d7cb970:         public long getLong(String name)
1:d7cb970:         {
1:d7cb970:             return row.getLong(name);
1:d7cb970:         }
1:d7cb970: 
1:d7cb970:         @Override
1:16b0288:         public Date getTimestamp(int i)
1:16b0288:         {
1:16b0288:             return row.getTimestamp(i);
1:16b0288:         }
1:16b0288: 
1:16b0288:         @Override
1:16b0288:         public Date getTimestamp(String s)
1:16b0288:         {
1:16b0288:             return row.getTimestamp(s);
1:16b0288:         }
1:16b0288: 
1:16b0288:         @Override
1:a133992:         public LocalDate getDate(int i)
1:d7cb970:         {
1:d7cb970:             return row.getDate(i);
1:d7cb970:         }
1:d7cb970: 
1:d7cb970:         @Override
1:a133992:         public LocalDate getDate(String s)
1:d7cb970:         {
1:16b0288:             return row.getDate(s);
1:16b0288:         }
1:16b0288: 
1:16b0288:         @Override
1:16b0288:         public long getTime(int i)
1:16b0288:         {
1:16b0288:             return row.getTime(i);
1:16b0288:         }
1:16b0288: 
1:16b0288:         @Override
1:16b0288:         public long getTime(String s)
1:16b0288:         {
1:16b0288:             return row.getTime(s);
1:d7cb970:         }
1:d7cb970: 
1:d7cb970:         @Override
1:d7cb970:         public float getFloat(int i)
1:d7cb970:         {
1:d7cb970:             return row.getFloat(i);
1:d7cb970:         }
1:d7cb970: 
1:d7cb970:         @Override
1:d7cb970:         public float getFloat(String name)
1:d7cb970:         {
1:d7cb970:             return row.getFloat(name);
1:d7cb970:         }
1:d7cb970: 
1:d7cb970:         @Override
1:d7cb970:         public double getDouble(int i)
1:d7cb970:         {
1:d7cb970:             return row.getDouble(i);
1:d7cb970:         }
1:d7cb970: 
1:d7cb970:         @Override
1:d7cb970:         public double getDouble(String name)
1:d7cb970:         {
1:d7cb970:             return row.getDouble(name);
1:d7cb970:         }
1:d7cb970: 
1:d7cb970:         @Override
1:d7cb970:         public ByteBuffer getBytesUnsafe(int i)
1:d7cb970:         {
1:d7cb970:             return row.getBytesUnsafe(i);
1:d7cb970:         }
1:d7cb970: 
1:d7cb970:         @Override
1:d7cb970:         public ByteBuffer getBytesUnsafe(String name)
1:d7cb970:         {
1:d7cb970:             return row.getBytesUnsafe(name);
1:d7cb970:         }
1:d7cb970: 
1:d7cb970:         @Override
1:d7cb970:         public ByteBuffer getBytes(int i)
1:d7cb970:         {
1:d7cb970:             return row.getBytes(i);
1:d7cb970:         }
1:d7cb970: 
1:d7cb970:         @Override
1:d7cb970:         public ByteBuffer getBytes(String name)
1:d7cb970:         {
1:d7cb970:             return row.getBytes(name);
1:d7cb970:         }
1:d7cb970: 
1:d7cb970:         @Override
1:d7cb970:         public String getString(int i)
1:d7cb970:         {
1:d7cb970:             return row.getString(i);
1:d7cb970:         }
1:d7cb970: 
1:d7cb970:         @Override
1:d7cb970:         public String getString(String name)
1:d7cb970:         {
1:d7cb970:             return row.getString(name);
1:d7cb970:         }
1:d7cb970: 
1:d7cb970:         @Override
1:d7cb970:         public BigInteger getVarint(int i)
1:d7cb970:         {
1:d7cb970:             return row.getVarint(i);
1:d7cb970:         }
1:d7cb970: 
1:d7cb970:         @Override
1:d7cb970:         public BigInteger getVarint(String name)
1:d7cb970:         {
1:d7cb970:             return row.getVarint(name);
1:d7cb970:         }
1:d7cb970: 
1:d7cb970:         @Override
1:d7cb970:         public BigDecimal getDecimal(int i)
1:d7cb970:         {
1:d7cb970:             return row.getDecimal(i);
1:d7cb970:         }
1:d7cb970: 
1:d7cb970:         @Override
1:d7cb970:         public BigDecimal getDecimal(String name)
1:d7cb970:         {
1:d7cb970:             return row.getDecimal(name);
1:d7cb970:         }
1:d7cb970: 
1:d7cb970:         @Override
1:d7cb970:         public UUID getUUID(int i)
1:d7cb970:         {
1:d7cb970:             return row.getUUID(i);
1:d7cb970:         }
1:d7cb970: 
1:d7cb970:         @Override
1:d7cb970:         public UUID getUUID(String name)
1:d7cb970:         {
1:d7cb970:             return row.getUUID(name);
1:d7cb970:         }
1:d7cb970: 
1:d7cb970:         @Override
1:d7cb970:         public InetAddress getInet(int i)
1:d7cb970:         {
1:d7cb970:             return row.getInet(i);
1:d7cb970:         }
1:d7cb970: 
1:d7cb970:         @Override
1:d7cb970:         public InetAddress getInet(String name)
1:d7cb970:         {
1:d7cb970:             return row.getInet(name);
1:d7cb970:         }
1:d7cb970: 
1:d7cb970:         @Override
1:d7cb970:         public <T> List<T> getList(int i, Class<T> elementsClass)
1:d7cb970:         {
1:d7cb970:             return row.getList(i, elementsClass);
1:d7cb970:         }
1:d7cb970: 
1:d7cb970:         @Override
1:f698cc2:         public <T> List<T> getList(int i, TypeToken<T> typeToken)
1:f698cc2:         {
1:f698cc2:             return row.getList(i, typeToken);
1:f698cc2:         }
1:f698cc2: 
1:f698cc2:         @Override
1:d7cb970:         public <T> List<T> getList(String name, Class<T> elementsClass)
1:d7cb970:         {
1:d7cb970:             return row.getList(name, elementsClass);
1:d7cb970:         }
1:d7cb970: 
1:d7cb970:         @Override
1:f698cc2:         public <T> List<T> getList(String s, TypeToken<T> typeToken)
1:f698cc2:         {
1:f698cc2:             return row.getList(s, typeToken);
1:f698cc2:         }
1:f698cc2: 
1:f698cc2:         @Override
1:d7cb970:         public <T> Set<T> getSet(int i, Class<T> elementsClass)
1:d7cb970:         {
1:d7cb970:             return row.getSet(i, elementsClass);
1:d7cb970:         }
1:d7cb970: 
1:d7cb970:         @Override
1:f698cc2:         public <T> Set<T> getSet(int i, TypeToken<T> typeToken)
1:f698cc2:         {
1:f698cc2:             return row.getSet(i, typeToken);
1:f698cc2:         }
1:f698cc2: 
1:f698cc2:         @Override
1:d7cb970:         public <T> Set<T> getSet(String name, Class<T> elementsClass)
1:d7cb970:         {
1:d7cb970:             return row.getSet(name, elementsClass);
1:d7cb970:         }
1:d7cb970: 
1:d7cb970:         @Override
1:f698cc2:         public <T> Set<T> getSet(String s, TypeToken<T> typeToken)
1:f698cc2:         {
1:f698cc2:             return row.getSet(s, typeToken);
1:f698cc2:         }
1:f698cc2: 
1:f698cc2:         @Override
1:d7cb970:         public <K, V> Map<K, V> getMap(int i, Class<K> keysClass, Class<V> valuesClass)
1:d7cb970:         {
1:d7cb970:             return row.getMap(i, keysClass, valuesClass);
1:d7cb970:         }
1:d7cb970: 
1:d7cb970:         @Override
1:f698cc2:         public <K, V> Map<K, V> getMap(int i, TypeToken<K> typeToken, TypeToken<V> typeToken1)
1:f698cc2:         {
1:f698cc2:             return row.getMap(i, typeToken, typeToken1);
1:f698cc2:         }
1:f698cc2: 
1:f698cc2:         @Override
1:d7cb970:         public <K, V> Map<K, V> getMap(String name, Class<K> keysClass, Class<V> valuesClass)
1:d7cb970:         {
1:d7cb970:             return row.getMap(name, keysClass, valuesClass);
1:d7cb970:         }
1:b94c87a: 
1:b94c87a:         @Override
1:f698cc2:         public <K, V> Map<K, V> getMap(String s, TypeToken<K> typeToken, TypeToken<V> typeToken1)
1:f698cc2:         {
1:f698cc2:             return row.getMap(s, typeToken, typeToken1);
1:f698cc2:         }
1:f698cc2: 
1:f698cc2:         @Override
1:b94c87a:         public UDTValue getUDTValue(int i)
1:b94c87a:         {
1:b94c87a:             return row.getUDTValue(i);
1:b94c87a:         }
1:b94c87a: 
1:b94c87a:         @Override
1:b94c87a:         public UDTValue getUDTValue(String name)
1:b94c87a:         {
1:b94c87a:             return row.getUDTValue(name);
1:b94c87a:         }
1:b94c87a: 
1:b94c87a:         @Override
1:b94c87a:         public TupleValue getTupleValue(int i)
1:b94c87a:         {
1:b94c87a:             return row.getTupleValue(i);
1:b94c87a:         }
1:b94c87a: 
1:b94c87a:         @Override
1:b94c87a:         public TupleValue getTupleValue(String name)
1:b94c87a:         {
1:b94c87a:             return row.getTupleValue(name);
1:b94c87a:         }
1:f698cc2: 
1:f698cc2:         @Override
1:f698cc2:         public Token getToken(int i)
1:f698cc2:         {
1:f698cc2:             return row.getToken(i);
1:f698cc2:         }
1:f698cc2: 
1:f698cc2:         @Override
1:f698cc2:         public Token getToken(String name)
1:f698cc2:         {
1:f698cc2:             return row.getToken(name);
1:f698cc2:         }
1:f698cc2: 
1:f698cc2:         @Override
1:f698cc2:         public Token getPartitionKeyToken()
1:f698cc2:         {
1:f698cc2:             return row.getPartitionKeyToken();
1:f698cc2:         }
1:d7cb970:     }
1:0356ee7: 
1:52df514:     /**
1:52df514:      * Build a query for the reader of the form:
1:52df514:      *
1:b10c2ab:      * SELECT * FROM ks>cf token(pk1,...pkn)>? AND token(pk1,...pkn)<=? [AND user where clauses] [ALLOW FILTERING]
1:d7cb970:      */
1:52df514:     private String buildQuery()
1:52df514:     {
1:52df514:         fetchKeys();
1:52df514: 
1:7049ee0:         List<String> columns = getSelectColumns();
1:7049ee0:         String selectColumnList = columns.size() == 0 ? "*" : makeColumnList(columns);
1:52df514:         String partitionKeyList = makeColumnList(partitionKeys);
1:52df514: 
1:52df514:         return String.format("SELECT %s FROM %s.%s WHERE token(%s)>? AND token(%s)<=?" + getAdditionalWhereClauses(),
1:6e73a51:                              selectColumnList, quote(keyspace), quote(cfName), partitionKeyList, partitionKeyList);
1:52df514:     }
1:52df514: 
1:52df514:     private String getAdditionalWhereClauses()
1:52df514:     {
1:52df514:         String whereClause = "";
1:52df514:         if (StringUtils.isNotEmpty(userDefinedWhereClauses))
1:52df514:             whereClause += " AND " + userDefinedWhereClauses;
1:52df514:         if (StringUtils.isNotEmpty(userDefinedWhereClauses))
1:52df514:             whereClause += " ALLOW FILTERING";
1:52df514:         return whereClause;
1:52df514:     }
1:52df514: 
1:52df514:     private List<String> getSelectColumns()
1:52df514:     {
1:52df514:         List<String> selectColumns = new ArrayList<>();
1:52df514: 
1:7049ee0:         if (StringUtils.isNotEmpty(inputColumns))
1:52df514:         {
1:52df514:             // We must select all the partition keys plus any other columns the user wants
1:52df514:             selectColumns.addAll(partitionKeys);
1:52df514:             for (String column : Splitter.on(',').split(inputColumns))
1:52df514:             {
1:52df514:                 if (!partitionKeys.contains(column))
1:52df514:                     selectColumns.add(column);
1:52df514:             }
1:52df514:         }
1:52df514:         return selectColumns;
1:52df514:     }
1:52df514: 
1:52df514:     private String makeColumnList(Collection<String> columns)
1:52df514:     {
1:52df514:         return Joiner.on(',').join(Iterables.transform(columns, new Function<String, String>()
1:52df514:         {
1:52df514:             public String apply(String column)
1:52df514:             {
1:52df514:                 return quote(column);
1:52df514:             }
1:52df514:         }));
1:52df514:     }
1:52df514: 
1:52df514:     private void fetchKeys()
1:52df514:     {
1:3e9d345:         // get CF meta data
1:f698cc2:         TableMetadata tableMetadata = session.getCluster()
1:f698cc2:                                              .getMetadata()
1:f698cc2:                                              .getKeyspace(Metadata.quote(keyspace))
1:f698cc2:                                              .getTable(Metadata.quote(cfName));
1:f698cc2:         if (tableMetadata == null)
1:52df514:         {
1:52df514:             throw new RuntimeException("No table metadata found for " + keyspace + "." + cfName);
1:52df514:         }
1:f698cc2:         //Here we assume that tableMetadata.getPartitionKey() always
1:f698cc2:         //returns the list of columns in order of component_index
1:f698cc2:         for (ColumnMetadata partitionKey : tableMetadata.getPartitionKey())
1:52df514:         {
1:f698cc2:             partitionKeys.add(partitionKey.getName());
1:52df514:         }
1:52df514:     }
1:52df514: 
1:0356ee7:     private String quote(String identifier)
1:52df514:     {
1:0356ee7:         return "\"" + identifier.replaceAll("\"", "\"\"") + "\"";
1:52df514:     }
1:0356ee7: }
============================================================================
author:Dave Brosius
-------------------------------------------------------------------------------
commit:68d2526
/////////////////////////////////////////////////////////////////////////
1:     public int getNativeProtocolVersion() 
1:     {
commit:1fab7b7
/////////////////////////////////////////////////////////////////////////
1:         return rowIterator.totalRead;
commit:222ea95
/////////////////////////////////////////////////////////////////////////
1:  * <p>
1:  * </p>
1:  * <p>
1:  * </p>
1:  * {@code
1:  * }
commit:f096eb6
/////////////////////////////////////////////////////////////////////////
1:             Map<String, ByteBuffer> keyColumns = new HashMap<String, ByteBuffer>(partitionBoundColumns.size()); 
commit:452dd10
commit:abbf634
/////////////////////////////////////////////////////////////////////////
1: import org.apache.commons.lang3.StringUtils;
commit:70bf377
commit:1d5d303
/////////////////////////////////////////////////////////////////////////
1:     @Override
/////////////////////////////////////////////////////////////////////////
commit:e6668be
/////////////////////////////////////////////////////////////////////////
/////////////////////////////////////////////////////////////////////////
1:         implements org.apache.hadoop.mapred.RecordReader<Long, Row>, AutoCloseable
commit:7269e46
/////////////////////////////////////////////////////////////////////////
0: import org.apache.commons.lang3.StringUtils;
commit:1def02f
/////////////////////////////////////////////////////////////////////////
author:blerer
-------------------------------------------------------------------------------
commit:aa60cde
author:Paulo Motta
-------------------------------------------------------------------------------
commit:4a849ef
/////////////////////////////////////////////////////////////////////////
1:         logger.trace("cqlQuery {}", cqlQuery);
1:         logger.trace("created {}", rowIterator);
/////////////////////////////////////////////////////////////////////////
1:             logger.trace("Finished scanning {} rows (estimate was: {})", rowIterator.totalRead, totalRowCount);
author:Sam Tunnicliffe
-------------------------------------------------------------------------------
commit:06c130e
/////////////////////////////////////////////////////////////////////////
1: 
1: import com.datastax.driver.core.TypeCodec;
/////////////////////////////////////////////////////////////////////////
1:         public <T> T get(int i, Class<T> aClass)
1:         {
1:             return row.get(i, aClass);
1:         }
1: 
1:         @Override
1:         public <T> T get(int i, TypeToken<T> typeToken)
1:         {
1:             return row.get(i, typeToken);
1:         }
1: 
1:         @Override
1:         public <T> T get(int i, TypeCodec<T> typeCodec)
1:         {
1:             return row.get(i, typeCodec);
1:         }
1: 
1:         @Override
1:         public <T> T get(String s, Class<T> aClass)
1:         {
1:             return row.get(s, aClass);
1:         }
1: 
1:         @Override
1:         public <T> T get(String s, TypeToken<T> typeToken)
1:         {
1:             return row.get(s, typeToken);
1:         }
1: 
1:         @Override
1:         public <T> T get(String s, TypeCodec<T> typeCodec)
1:         {
1:             return row.get(s, typeCodec);
1:         }
1: 
1:         @Override
author:Benedict Elliott Smith
-------------------------------------------------------------------------------
commit:489a9e8
/////////////////////////////////////////////////////////////////////////
1: import org.apache.cassandra.utils.AbstractIterator;
author:Robert Stupp
-------------------------------------------------------------------------------
commit:a133992
/////////////////////////////////////////////////////////////////////////
1: import com.datastax.driver.core.LocalDate;
/////////////////////////////////////////////////////////////////////////
1:         public LocalDate getDate(int i)
1:         public LocalDate getDate(String s)
commit:16b0288
/////////////////////////////////////////////////////////////////////////
0: import com.datastax.driver.core.DateWithoutTime;
/////////////////////////////////////////////////////////////////////////
1:         nativeProtocolVersion = cluster.getConfiguration().getProtocolOptions().getProtocolVersion().toInt();
/////////////////////////////////////////////////////////////////////////
1:         public Object getObject(int i)
1:         {
1:             return row.getObject(i);
1:         }
1: 
1:         @Override
1:         public Object getObject(String s)
1:         {
1:             return row.getObject(s);
1:         }
1: 
1:         @Override
/////////////////////////////////////////////////////////////////////////
1:         public short getShort(int i)
1:         {
1:             return row.getShort(i);
1:         }
1: 
1:         @Override
1:         public short getShort(String s)
1:         {
1:             return row.getShort(s);
1:         }
1: 
1:         @Override
1:         public byte getByte(int i)
1:         {
1:             return row.getByte(i);
1:         }
1: 
1:         @Override
1:         public byte getByte(String s)
1:         {
1:             return row.getByte(s);
1:         }
1: 
1:         @Override
/////////////////////////////////////////////////////////////////////////
1:         public Date getTimestamp(int i)
1:         {
1:             return row.getTimestamp(i);
1:         }
1: 
1:         @Override
1:         public Date getTimestamp(String s)
1:         {
1:             return row.getTimestamp(s);
1:         }
1: 
1:         @Override
0:         public DateWithoutTime getDate(int i)
0:         public DateWithoutTime getDate(String s)
1:             return row.getDate(s);
1:         }
1: 
1:         @Override
1:         public long getTime(int i)
1:         {
1:             return row.getTime(i);
1:         }
1: 
1:         @Override
1:         public long getTime(String s)
1:         {
1:             return row.getTime(s);
commit:794d68b
/////////////////////////////////////////////////////////////////////////
1:         return Long.valueOf(0L);
commit:b94c87a
/////////////////////////////////////////////////////////////////////////
1: import com.datastax.driver.core.TupleValue;
1: import com.datastax.driver.core.UDTValue;
1: 
/////////////////////////////////////////////////////////////////////////
1: 
1:         @Override
1:         public UDTValue getUDTValue(int i)
1:         {
1:             return row.getUDTValue(i);
1:         }
1: 
1:         @Override
1:         public UDTValue getUDTValue(String name)
1:         {
1:             return row.getUDTValue(name);
1:         }
1: 
1:         @Override
1:         public TupleValue getTupleValue(int i)
1:         {
1:             return row.getTupleValue(i);
1:         }
1: 
1:         @Override
1:         public TupleValue getTupleValue(String name)
1:         {
1:             return row.getTupleValue(name);
1:         }
author:Philip Thompson
-------------------------------------------------------------------------------
commit:f698cc2
/////////////////////////////////////////////////////////////////////////
1: import com.datastax.driver.core.Metadata;
1: import com.datastax.driver.core.TableMetadata;
1: import com.datastax.driver.core.Token;
1: import com.google.common.reflect.TypeToken;
/////////////////////////////////////////////////////////////////////////
1:         public <T> List<T> getList(int i, TypeToken<T> typeToken)
1:         {
1:             return row.getList(i, typeToken);
1:         }
1: 
1:         @Override
1:         public <T> List<T> getList(String s, TypeToken<T> typeToken)
1:         {
1:             return row.getList(s, typeToken);
1:         }
1: 
1:         @Override
1:         public <T> Set<T> getSet(int i, TypeToken<T> typeToken)
1:         {
1:             return row.getSet(i, typeToken);
1:         }
1: 
1:         @Override
1:         public <T> Set<T> getSet(String s, TypeToken<T> typeToken)
1:         {
1:             return row.getSet(s, typeToken);
1:         }
1: 
1:         @Override
1:         public <K, V> Map<K, V> getMap(int i, TypeToken<K> typeToken, TypeToken<V> typeToken1)
1:         {
1:             return row.getMap(i, typeToken, typeToken1);
1:         }
1: 
1:         @Override
1:         public <K, V> Map<K, V> getMap(String s, TypeToken<K> typeToken, TypeToken<V> typeToken1)
1:         {
1:             return row.getMap(s, typeToken, typeToken1);
1:         }
1: 
1:         @Override
/////////////////////////////////////////////////////////////////////////
1: 
1:         @Override
1:         public Token getToken(int i)
1:         {
1:             return row.getToken(i);
1:         }
1: 
1:         @Override
1:         public Token getToken(String name)
1:         {
1:             return row.getToken(name);
1:         }
1: 
1:         @Override
1:         public Token getPartitionKeyToken()
1:         {
1:             return row.getPartitionKeyToken();
1:         }
/////////////////////////////////////////////////////////////////////////
1:         TableMetadata tableMetadata = session.getCluster()
1:                                              .getMetadata()
1:                                              .getKeyspace(Metadata.quote(keyspace))
1:                                              .getTable(Metadata.quote(cfName));
1:         if (tableMetadata == null)
1:         //Here we assume that tableMetadata.getPartitionKey() always
1:         //returns the list of columns in order of component_index
1:         for (ColumnMetadata partitionKey : tableMetadata.getPartitionKey())
1:             partitionKeys.add(partitionKey.getName());
author:Brandon Williams
-------------------------------------------------------------------------------
commit:5839a4d
commit:7e6d9eb
/////////////////////////////////////////////////////////////////////////
commit:42e483a
commit:1cb426b
/////////////////////////////////////////////////////////////////////////
1:     protected int nativeProtocolVersion = 1;
/////////////////////////////////////////////////////////////////////////
1:         //get negotiated serialization protocol
0:         nativeProtocolVersion = cluster.getConfiguration().getProtocolOptions().getProtocolVersion();
1: 
/////////////////////////////////////////////////////////////////////////
1:     /**
1:      * Return native version protocol of the cluster connection
1:      * @return serialization protocol version.
1:      */
0:     public int getNativeProtocolVersion() {
1:         return nativeProtocolVersion;
1:     }
1: 
commit:60c0cad
commit:0126760
commit:b10c2ab
/////////////////////////////////////////////////////////////////////////
/////////////////////////////////////////////////////////////////////////
1: 
/////////////////////////////////////////////////////////////////////////
/////////////////////////////////////////////////////////////////////////
1:      * SELECT * FROM ks>cf token(pk1,...pkn)>? AND token(pk1,...pkn)<=? [AND user where clauses] [ALLOW FILTERING]
/////////////////////////////////////////////////////////////////////////
commit:df8b568
commit:397c0b7
commit:6e73a51
/////////////////////////////////////////////////////////////////////////
1:                              selectColumnList, quote(keyspace), quote(cfName), partitionKeyList, partitionKeyList);
commit:26bde8d
commit:5172860
commit:7ddcf3e
/////////////////////////////////////////////////////////////////////////
1:             session = cluster.connect(quote(keyspace));
commit:7f1671c
commit:dbc4582
commit:7049ee0
/////////////////////////////////////////////////////////////////////////
/////////////////////////////////////////////////////////////////////////
1:         cfName = ConfigHelper.getInputColumnFamily(conf);
1:         keyspace = ConfigHelper.getInputKeyspace(conf);
/////////////////////////////////////////////////////////////////////////
1:         // validate that the user hasn't tried to give us a custom query along with input columns
1:         // and where clauses
1:         if (StringUtils.isNotEmpty(cqlQuery) && (StringUtils.isNotEmpty(inputColumns) ||
1:                                                  StringUtils.isNotEmpty(userDefinedWhereClauses)))
1:         {
1:             throw new AssertionError("Cannot define a custom query with input columns and / or where clauses");
1:         }
1: 
/////////////////////////////////////////////////////////////////////////
1:             for (ColumnMetadata meta : cluster.getMetadata().getKeyspace(quote(keyspace)).getTable(quote(cfName)).getPartitionKey())
/////////////////////////////////////////////////////////////////////////
1:         List<String> columns = getSelectColumns();
1:         String selectColumnList = columns.size() == 0 ? "*" : makeColumnList(columns);
/////////////////////////////////////////////////////////////////////////
1:         if (StringUtils.isNotEmpty(inputColumns))
/////////////////////////////////////////////////////////////////////////
commit:755d345
commit:1d744b5
commit:52df514
/////////////////////////////////////////////////////////////////////////
1: import com.google.common.base.Function;
1: import com.google.common.base.Joiner;
0: import com.google.common.base.Optional;
1: import com.google.common.base.Splitter;
1: import com.google.common.collect.Iterables;
0: import org.apache.commons.collections.CollectionUtils;
0: import org.apache.commons.lang.StringUtils;
1: 
/////////////////////////////////////////////////////////////////////////
0:     public static final int DEFAULT_CQL_PAGE_LIMIT = 1000;
1: 
/////////////////////////////////////////////////////////////////////////
1:     private String inputColumns;
1:     private String userDefinedWhereClauses;
0:     private int pageRowSize;
1: 
1:     private List<String> partitionKeys = new ArrayList<>();
0:     private List<String> clusteringKeys = new ArrayList<>();
/////////////////////////////////////////////////////////////////////////
1:         partitioner = ConfigHelper.getInputPartitioner(conf);
1:         inputColumns = CqlConfigHelper.getInputcolumns(conf);
1:         userDefinedWhereClauses = CqlConfigHelper.getInputWhereClauses(conf);
0:         Optional<Integer> pageRowSizeOptional = CqlConfigHelper.getInputPageRowSize(conf);
1:         try
1:         {
0:             pageRowSize = pageRowSizeOptional.isPresent() ? pageRowSizeOptional.get() : DEFAULT_CQL_PAGE_LIMIT;
1:         }
0:         catch(NumberFormatException e)
1:         {
0:             pageRowSize = DEFAULT_CQL_PAGE_LIMIT;
1:         }
/////////////////////////////////////////////////////////////////////////
1: 
1:         if (session == null)
1:           throw new RuntimeException("Can't create connection session");
1: 
1:         // If the user provides a CQL query then we will use it without validation
1:         // otherwise we will fall back to building a query using the:
1:         //   inputColumns
1:         //   whereClauses
0:         //   pageRowSize
1:         cqlQuery = CqlConfigHelper.getInputCql(conf);
1:         if (StringUtils.isEmpty(cqlQuery))
1:             cqlQuery = buildQuery();
0:         logger.debug("cqlQuery {}", cqlQuery);
1: 
/////////////////////////////////////////////////////////////////////////
/////////////////////////////////////////////////////////////////////////
1:     /**
1:      * Build a query for the reader of the form:
1:      *
0:      * SELECT * FROM ks>cf token(pk1,...pkn)>? AND token(pk1,...pkn)<=? [AND user where clauses]
0:      * LIMIT pageRowSize [ALLOW FILTERING]
1:      */
1:     private String buildQuery()
1:     {
1:         fetchKeys();
1: 
0:         String selectColumnList = makeColumnList(getSelectColumns());
1:         String partitionKeyList = makeColumnList(partitionKeys);
1: 
1:         return String.format("SELECT %s FROM %s.%s WHERE token(%s)>? AND token(%s)<=?" + getAdditionalWhereClauses(),
0:                              selectColumnList, keyspace, cfName, partitionKeyList, partitionKeyList);
1:     }
1: 
1:     private String getAdditionalWhereClauses()
1:     {
1:         String whereClause = "";
1:         if (StringUtils.isNotEmpty(userDefinedWhereClauses))
1:             whereClause += " AND " + userDefinedWhereClauses;
0:         whereClause += " LIMIT " + pageRowSize;
1:         if (StringUtils.isNotEmpty(userDefinedWhereClauses))
1:             whereClause += " ALLOW FILTERING";
1:         return whereClause;
1:     }
1: 
1:     private List<String> getSelectColumns()
1:     {
1:         List<String> selectColumns = new ArrayList<>();
1: 
0:         if (StringUtils.isEmpty(inputColumns))
0:             selectColumns.add("*");
1:         else
1:         {
1:             // We must select all the partition keys plus any other columns the user wants
1:             selectColumns.addAll(partitionKeys);
1:             for (String column : Splitter.on(',').split(inputColumns))
1:             {
1:                 if (!partitionKeys.contains(column))
1:                     selectColumns.add(column);
1:             }
1:         }
1:         return selectColumns;
1:     }
1: 
1:     private String makeColumnList(Collection<String> columns)
1:     {
1:         return Joiner.on(',').join(Iterables.transform(columns, new Function<String, String>()
1:         {
1:             public String apply(String column)
1:             {
1:                 return quote(column);
1:             }
1:         }));
1:     }
1: 
1:     private void fetchKeys()
1:     {
0:         String query = "SELECT column_name, component_index, type FROM system.schema_columns WHERE keyspace_name='" +
0:                        keyspace + "' and columnfamily_name='" + cfName + "'";
0:         List<Row> rows = session.execute(query).all();
0:         if (CollectionUtils.isEmpty(rows))
1:         {
1:             throw new RuntimeException("No table metadata found for " + keyspace + "." + cfName);
1:         }
0:         int numberOfPartitionKeys = 0;
0:         for (Row row : rows)
0:             if (row.getString(2).equals("partition_key"))
0:                 numberOfPartitionKeys++;
0:         String[] partitionKeyArray = new String[numberOfPartitionKeys];
0:         for (Row row : rows)
1:         {
0:             String type = row.getString(2);
0:             String column = row.getString(0);
0:             if (type.equals("partition_key"))
1:             {
0:                 int componentIndex = row.isNull(1) ? 0 : row.getInt(1);
0:                 partitionKeyArray[componentIndex] = column;
1:             }
0:             else if (type.equals("clustering_key"))
1:             {
0:                 clusteringKeys.add(column);
1:             }
1:         }
0:         partitionKeys.addAll(Arrays.asList(partitionKeyArray));
1:     }
1: 
1: 
1: 
commit:4a295b6
commit:a3b9d56
/////////////////////////////////////////////////////////////////////////
1: import org.apache.cassandra.hadoop.HadoopCompat;
/////////////////////////////////////////////////////////////////////////
1:         Configuration conf = HadoopCompat.getConfiguration(context);
0:         partitioner = ConfigHelper.getInputPartitioner(HadoopCompat.getConfiguration(context));
commit:cf60ca8
commit:786396e
/////////////////////////////////////////////////////////////////////////
0:         cfName = quote(ConfigHelper.getInputColumnFamily(conf));
0:         keyspace = quote(ConfigHelper.getInputKeyspace(conf));
/////////////////////////////////////////////////////////////////////////
0:             session = cluster.connect(keyspace);
commit:86cd423
commit:0356ee7
/////////////////////////////////////////////////////////////////////////
/////////////////////////////////////////////////////////////////////////
0:             session = cluster.connect(quote(keyspace));
/////////////////////////////////////////////////////////////////////////
1: 
1:     private String quote(String identifier)
1:     {
1:         return "\"" + identifier.replaceAll("\"", "\"\"") + "\"";
1:     }
author:Aleksey Yeschenko
-------------------------------------------------------------------------------
commit:3e9d345
/////////////////////////////////////////////////////////////////////////
1: import java.util.*;
/////////////////////////////////////////////////////////////////////////
/////////////////////////////////////////////////////////////////////////
0: import org.apache.cassandra.schema.LegacySchemaTables;
0: import org.apache.cassandra.db.SystemKeyspace;
/////////////////////////////////////////////////////////////////////////
0:         String query = String.format("SELECT column_name, component_index, type " +
0:                                      "FROM %s.%s " +
0:                                      "WHERE keyspace_name = '%s' AND columnfamily_name = '%s'",
0:                                      SystemKeyspace.NAME,
0:                                      LegacySchemaTables.COLUMNS,
0:                                      keyspace,
0:                                      cfName);
1: 
1:         // get CF meta data
commit:314afb8
commit:ce747d7
author:Jonathan Ellis
-------------------------------------------------------------------------------
commit:49833b9
commit:abde62d
/////////////////////////////////////////////////////////////////////////
/////////////////////////////////////////////////////////////////////////
0:         if (rows.isEmpty())
commit:8a5365e
commit:541a20d
/////////////////////////////////////////////////////////////////////////
1:         if (cluster != null)
1:             cluster.close();
commit:e615960
/////////////////////////////////////////////////////////////////////////
1:     private static class WrappedRow implements Row
author:Jacek Lewandowski
-------------------------------------------------------------------------------
commit:fe39eb7
/////////////////////////////////////////////////////////////////////////
1:             // create a Cluster instance
1:             cluster = CqlConfigHelper.getInputCluster(locations, conf);
author:belliottsmith
-------------------------------------------------------------------------------
commit:acf1b18
/////////////////////////////////////////////////////////////////////////
1: import org.apache.cassandra.utils.ByteBufferUtil;
/////////////////////////////////////////////////////////////////////////
1:                     // this is not correct - but we don't seem to have easy access to better type information here
1:                     if (ByteBufferUtil.compareUnsigned(keyColumns.get(column), previousRowKey.get(column)) != 0)
author:Sylvain Lebresne
-------------------------------------------------------------------------------
commit:d7cb970
/////////////////////////////////////////////////////////////////////////
1: /*
1:  * Licensed to the Apache Software Foundation (ASF) under one
1:  * or more contributor license agreements.  See the NOTICE file
1:  * distributed with this work for additional information
1:  * regarding copyright ownership.  The ASF licenses this file
1:  * to you under the Apache License, Version 2.0 (the
1:  * "License"); you may not use this file except in compliance
1:  * with the License.  You may obtain a copy of the License at
1:  *
1:  *     http://www.apache.org/licenses/LICENSE-2.0
1:  *
1:  * Unless required by applicable law or agreed to in writing, software
1:  * distributed under the License is distributed on an "AS IS" BASIS,
1:  * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
1:  * See the License for the specific language governing permissions and
1:  * limitations under the License.
1:  */
1: package org.apache.cassandra.hadoop.cql3;
1: 
1: import java.io.IOException;
1: import java.math.BigDecimal;
1: import java.math.BigInteger;
1: import java.net.InetAddress;
1: import java.nio.ByteBuffer;
0: import java.util.*;
1: 
0: import com.google.common.collect.AbstractIterator;
1: import com.google.common.collect.Maps;
1: 
1: import org.slf4j.Logger;
1: import org.slf4j.LoggerFactory;
1: 
1: import org.apache.cassandra.db.marshal.AbstractType;
0: import org.apache.cassandra.db.marshal.BytesType;
1: import org.apache.cassandra.dht.IPartitioner;
1: import org.apache.cassandra.hadoop.ColumnFamilySplit;
1: import org.apache.cassandra.hadoop.ConfigHelper;
1: import org.apache.cassandra.utils.Pair;
1: import org.apache.hadoop.conf.Configuration;
1: import org.apache.hadoop.mapreduce.InputSplit;
1: import org.apache.hadoop.mapreduce.RecordReader;
1: import org.apache.hadoop.mapreduce.TaskAttemptContext;
1: 
1: import com.datastax.driver.core.Cluster;
1: import com.datastax.driver.core.ColumnDefinitions;
1: import com.datastax.driver.core.ColumnMetadata;
1: import com.datastax.driver.core.ResultSet;
1: import com.datastax.driver.core.Row;
1: import com.datastax.driver.core.Session;
1: /**
1:  * CqlRecordReader reads the rows return from the CQL query
1:  * It uses CQL auto-paging.
0:  * <p/>
1:  * Return a Long as a local CQL row key starts from 0;
0:  * <p/>
1:  * Row as C* java driver CQL result set row
1:  * 1) select clause must include partition key columns (to calculate the progress based on the actual CF row processed)
1:  * 2) where clause must include token(partition_key1, ...  , partition_keyn) > ? and 
1:  *       token(partition_key1, ... , partition_keyn) <= ?  (in the right order) 
1:  */
1: public class CqlRecordReader extends RecordReader<Long, Row>
0:         implements org.apache.hadoop.mapred.RecordReader<Long, Row>
1: {
1:     private static final Logger logger = LoggerFactory.getLogger(CqlRecordReader.class);
1: 
1:     private ColumnFamilySplit split;
1:     private RowIterator rowIterator;
1: 
1:     private Pair<Long, Row> currentRow;
1:     private int totalRowCount; // total number of rows to fetch
1:     private String keyspace;
1:     private String cfName;
1:     private String cqlQuery;
1:     private Cluster cluster;
1:     private Session session;
1:     private IPartitioner partitioner;
1: 
1:     // partition keys -- key aliases
1:     private LinkedHashMap<String, Boolean> partitionBoundColumns = Maps.newLinkedHashMap();
1: 
1:     public CqlRecordReader()
1:     {
1:         super();
1:     }
1: 
1:     public void initialize(InputSplit split, TaskAttemptContext context) throws IOException
1:     {
1:         this.split = (ColumnFamilySplit) split;
0:         Configuration conf = context.getConfiguration();
1:         totalRowCount = (this.split.getLength() < Long.MAX_VALUE)
1:                       ? (int) this.split.getLength()
1:                       : ConfigHelper.getInputSplitSize(conf);
0:         cfName = ConfigHelper.getInputColumnFamily(conf);
0:         keyspace = ConfigHelper.getInputKeyspace(conf);              
0:         cqlQuery = CqlConfigHelper.getInputCql(conf);
0:         partitioner = ConfigHelper.getInputPartitioner(context.getConfiguration());
1:         try
1:         {
1:             if (cluster != null)
1:                 return;
1: 
0:             // create connection using thrift
1:             String[] locations = split.getLocations();
0:             Exception lastException = null;
0:             for (String location : locations)
1:             {
1:                 try
1:                 {
0:                     cluster = CqlConfigHelper.getInputCluster(location, conf);
1:                     break;
1:                 }
1:                 catch (Exception e)
1:                 {
0:                     lastException = e;
0:                     logger.warn("Failed to create authenticated client to {}", location);
1:                 }
1:             }
0:             if (cluster == null && lastException != null)
0:                 throw lastException;
1:         }
1:         catch (Exception e)
1:         {
1:             throw new RuntimeException(e);
1:         }
1: 
1:         if (cluster != null)
0:             session = cluster.connect(keyspace);
1:         rowIterator = new RowIterator();
0:         logger.debug("created {}", rowIterator);
1:     }
1: 
1:     public void close()
1:     {
1:         if (session != null)
1:             session.close();
1:     }
1: 
1:     public Long getCurrentKey()
1:     {
1:         return currentRow.left;
1:     }
1: 
1:     public Row getCurrentValue()
1:     {
1:         return currentRow.right;
1:     }
1: 
1:     public float getProgress()
1:     {
1:         if (!rowIterator.hasNext())
1:             return 1.0F;
1: 
1:         // the progress is likely to be reported slightly off the actual but close enough
1:         float progress = ((float) rowIterator.totalRead / totalRowCount);
1:         return progress > 1.0F ? 1.0F : progress;
1:     }
1: 
1:     public boolean nextKeyValue() throws IOException
1:     {
1:         if (!rowIterator.hasNext())
1:         {
0:             logger.debug("Finished scanning {} rows (estimate was: {})", rowIterator.totalRead, totalRowCount);
1:             return false;
1:         }
1: 
1:         try
1:         {
1:             currentRow = rowIterator.next();
1:         }
1:         catch (Exception e)
1:         {
1:             // throw it as IOException, so client can catch it and handle it at client side
1:             IOException ioe = new IOException(e.getMessage());
1:             ioe.initCause(ioe.getCause());
1:             throw ioe;
1:         }
1:         return true;
1:     }
1: 
1:     // Because the old Hadoop API wants us to write to the key and value
1:     // and the new asks for them, we need to copy the output of the new API
1:     // to the old. Thus, expect a small performance hit.
1:     // And obviously this wouldn't work for wide rows. But since ColumnFamilyInputFormat
1:     // and ColumnFamilyRecordReader don't support them, it should be fine for now.
1:     public boolean next(Long key, Row value) throws IOException
1:     {
1:         if (nextKeyValue())
1:         {
1:             ((WrappedRow)value).setRow(getCurrentValue());
1:             return true;
1:         }
1:         return false;
1:     }
1: 
1:     public long getPos() throws IOException
1:     {
0:         return (long) rowIterator.totalRead;
1:     }
1: 
1:     public Long createKey()
1:     {
0:         return new Long(0L);
1:     }
1: 
1:     public Row createValue()
1:     {
1:         return new WrappedRow();
1:     }
1: 
1:     /** CQL row iterator 
1:      *  Input cql query  
1:      *  1) select clause must include key columns (if we use partition key based row count)
1:      *  2) where clause must include token(partition_key1 ... partition_keyn) > ? and 
1:      *     token(partition_key1 ... partition_keyn) <= ? 
1:      */
1:     private class RowIterator extends AbstractIterator<Pair<Long, Row>>
1:     {
1:         private long keyId = 0L;
1:         protected int totalRead = 0; // total number of cf rows read
1:         protected Iterator<Row> rows;
1:         private Map<String, ByteBuffer> previousRowKey = new HashMap<String, ByteBuffer>(); // previous CF row key
1: 
1:         public RowIterator()
1:         {
0:             if (session == null)
0:                 throw new RuntimeException("Can't create connection session");
1: 
1:             AbstractType type = partitioner.getTokenValidator();
1:             ResultSet rs = session.execute(cqlQuery, type.compose(type.fromString(split.getStartToken())), type.compose(type.fromString(split.getEndToken())) );
0:             for (ColumnMetadata meta : cluster.getMetadata().getKeyspace(keyspace).getTable(cfName).getPartitionKey())
1:                 partitionBoundColumns.put(meta.getName(), Boolean.TRUE);
1:             rows = rs.iterator();
1:         }
1: 
1:         protected Pair<Long, Row> computeNext()
1:         {
1:             if (rows == null || !rows.hasNext())
1:                 return endOfData();
1: 
1:             Row row = rows.next();
0:             Map<String, ByteBuffer> keyColumns = new HashMap<String, ByteBuffer>(); 
1:             for (String column : partitionBoundColumns.keySet())
1:                 keyColumns.put(column, row.getBytesUnsafe(column));
1: 
1:             // increase total CF row read
1:             if (previousRowKey.isEmpty() && !keyColumns.isEmpty())
1:             {
1:                 previousRowKey = keyColumns;
1:                 totalRead++;
1:             }
0:             else
1:             {
1:                 for (String column : partitionBoundColumns.keySet())
1:                 {
0:                     if (BytesType.bytesCompare(keyColumns.get(column), previousRowKey.get(column)) != 0)
1:                     {
1:                         previousRowKey = keyColumns;
1:                         totalRead++;
1:                         break;
1:                     }
1:                 }
1:             }
1:             keyId ++;
1:             return Pair.create(keyId, row);
1:         }
1:     }
1: 
0:     private class WrappedRow implements Row
1:     {
1:         private Row row;
1: 
1:         public void setRow(Row row)
1:         {
1:             this.row = row;
1:         }
1: 
1:         @Override
1:         public ColumnDefinitions getColumnDefinitions()
1:         {
1:             return row.getColumnDefinitions();
1:         }
1: 
1:         @Override
1:         public boolean isNull(int i)
1:         {
1:             return row.isNull(i);
1:         }
1: 
1:         @Override
1:         public boolean isNull(String name)
1:         {
1:             return row.isNull(name);
1:         }
1: 
1:         @Override
1:         public boolean getBool(int i)
1:         {
1:             return row.getBool(i);
1:         }
1: 
1:         @Override
1:         public boolean getBool(String name)
1:         {
1:             return row.getBool(name);
1:         }
1: 
1:         @Override
1:         public int getInt(int i)
1:         {
1:             return row.getInt(i);
1:         }
1: 
1:         @Override
1:         public int getInt(String name)
1:         {
1:             return row.getInt(name);
1:         }
1: 
1:         @Override
1:         public long getLong(int i)
1:         {
1:             return row.getLong(i);
1:         }
1: 
1:         @Override
1:         public long getLong(String name)
1:         {
1:             return row.getLong(name);
1:         }
1: 
1:         @Override
0:         public Date getDate(int i)
1:         {
1:             return row.getDate(i);
1:         }
1: 
1:         @Override
0:         public Date getDate(String name)
1:         {
0:             return row.getDate(name);
1:         }
1: 
1:         @Override
1:         public float getFloat(int i)
1:         {
1:             return row.getFloat(i);
1:         }
1: 
1:         @Override
1:         public float getFloat(String name)
1:         {
1:             return row.getFloat(name);
1:         }
1: 
1:         @Override
1:         public double getDouble(int i)
1:         {
1:             return row.getDouble(i);
1:         }
1: 
1:         @Override
1:         public double getDouble(String name)
1:         {
1:             return row.getDouble(name);
1:         }
1: 
1:         @Override
1:         public ByteBuffer getBytesUnsafe(int i)
1:         {
1:             return row.getBytesUnsafe(i);
1:         }
1: 
1:         @Override
1:         public ByteBuffer getBytesUnsafe(String name)
1:         {
1:             return row.getBytesUnsafe(name);
1:         }
1: 
1:         @Override
1:         public ByteBuffer getBytes(int i)
1:         {
1:             return row.getBytes(i);
1:         }
1: 
1:         @Override
1:         public ByteBuffer getBytes(String name)
1:         {
1:             return row.getBytes(name);
1:         }
1: 
1:         @Override
1:         public String getString(int i)
1:         {
1:             return row.getString(i);
1:         }
1: 
1:         @Override
1:         public String getString(String name)
1:         {
1:             return row.getString(name);
1:         }
1: 
1:         @Override
1:         public BigInteger getVarint(int i)
1:         {
1:             return row.getVarint(i);
1:         }
1: 
1:         @Override
1:         public BigInteger getVarint(String name)
1:         {
1:             return row.getVarint(name);
1:         }
1: 
1:         @Override
1:         public BigDecimal getDecimal(int i)
1:         {
1:             return row.getDecimal(i);
1:         }
1: 
1:         @Override
1:         public BigDecimal getDecimal(String name)
1:         {
1:             return row.getDecimal(name);
1:         }
1: 
1:         @Override
1:         public UUID getUUID(int i)
1:         {
1:             return row.getUUID(i);
1:         }
1: 
1:         @Override
1:         public UUID getUUID(String name)
1:         {
1:             return row.getUUID(name);
1:         }
1: 
1:         @Override
1:         public InetAddress getInet(int i)
1:         {
1:             return row.getInet(i);
1:         }
1: 
1:         @Override
1:         public InetAddress getInet(String name)
1:         {
1:             return row.getInet(name);
1:         }
1: 
1:         @Override
1:         public <T> List<T> getList(int i, Class<T> elementsClass)
1:         {
1:             return row.getList(i, elementsClass);
1:         }
1: 
1:         @Override
1:         public <T> List<T> getList(String name, Class<T> elementsClass)
1:         {
1:             return row.getList(name, elementsClass);
1:         }
1: 
1:         @Override
1:         public <T> Set<T> getSet(int i, Class<T> elementsClass)
1:         {
1:             return row.getSet(i, elementsClass);
1:         }
1: 
1:         @Override
1:         public <T> Set<T> getSet(String name, Class<T> elementsClass)
1:         {
1:             return row.getSet(name, elementsClass);
1:         }
1: 
1:         @Override
1:         public <K, V> Map<K, V> getMap(int i, Class<K> keysClass, Class<V> valuesClass)
1:         {
1:             return row.getMap(i, keysClass, valuesClass);
1:         }
1: 
1:         @Override
1:         public <K, V> Map<K, V> getMap(String name, Class<K> keysClass, Class<V> valuesClass)
1:         {
1:             return row.getMap(name, keysClass, valuesClass);
1:         }
1:     }
1: }
commit:b0841d8
/////////////////////////////////////////////////////////////////////////
commit:3b708f9
/////////////////////////////////////////////////////////////////////////
0: /*
0:  * Licensed to the Apache Software Foundation (ASF) under one
0:  * or more contributor license agreements.  See the NOTICE file
0:  * distributed with this work for additional information
0:  * regarding copyright ownership.  The ASF licenses this file
0:  * to you under the Apache License, Version 2.0 (the
0:  * "License"); you may not use this file except in compliance
0:  * with the License.  You may obtain a copy of the License at
0:  *
0:  *     http://www.apache.org/licenses/LICENSE-2.0
0:  *
0:  * Unless required by applicable law or agreed to in writing, software
0:  * distributed under the License is distributed on an "AS IS" BASIS,
0:  * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
0:  * See the License for the specific language governing permissions and
0:  * limitations under the License.
0:  */
0: package org.apache.cassandra.hadoop.cql3;
0: 
0: import java.io.IOException;
0: import java.nio.ByteBuffer;
0: import java.util.*;
0: 
0: import com.google.common.collect.AbstractIterator;
0: import com.google.common.collect.Maps;
0: 
0: import org.slf4j.Logger;
0: import org.slf4j.LoggerFactory;
0: 
0: import org.apache.cassandra.db.marshal.AbstractType;
0: import org.apache.cassandra.db.marshal.BytesType;
0: import org.apache.cassandra.dht.IPartitioner;
0: import org.apache.cassandra.hadoop.ColumnFamilySplit;
0: import org.apache.cassandra.hadoop.ConfigHelper;
0: import org.apache.cassandra.utils.Pair;
0: import org.apache.hadoop.conf.Configuration;
0: import org.apache.hadoop.mapreduce.InputSplit;
0: import org.apache.hadoop.mapreduce.RecordReader;
0: import org.apache.hadoop.mapreduce.TaskAttemptContext;
0: 
0: import com.datastax.driver.core.Cluster;
0: import com.datastax.driver.core.ColumnMetadata;
0: import com.datastax.driver.core.ResultSet;
0: import com.datastax.driver.core.Row;
0: import com.datastax.driver.core.Session;
0: /**
0:  * CqlRecordReader reads the rows return from the CQL query
0:  * It uses CQL auto-paging.
0:  * <p/>
0:  * Return a Long as a local CQL row key starts from 0;
0:  * <p/>
0:  * Row as C* java driver CQL result set row
0:  * 1) select clause must include partition key columns (to calculate the progress based on the actual CF row processed)
0:  * 2) where clause must include token(partition_key1, ...  , partition_keyn) > ? and 
0:  *       token(partition_key1, ... , partition_keyn) <= ?  (in the right order) 
0:  */
0: public class CqlRecordReader extends RecordReader<Long, Row>
0:         implements org.apache.hadoop.mapred.RecordReader<Long, Row>
0: {
0:     private static final Logger logger = LoggerFactory.getLogger(CqlRecordReader.class);
0: 
0:     private ColumnFamilySplit split;
0:     private RowIterator rowIterator;
0: 
0:     private Pair<Long, Row> currentRow;
0:     private int totalRowCount; // total number of rows to fetch
0:     private String keyspace;
0:     private String cfName;
0:     private String cqlQuery;
0:     private Cluster cluster;
0:     private Session session;
0:     private IPartitioner partitioner;
0: 
0:     // partition keys -- key aliases
0:     private LinkedHashMap<String, Boolean> partitionBoundColumns = Maps.newLinkedHashMap();
0: 
0:     public CqlRecordReader()
0:     {
0:         super();
0:     }
0: 
0:     public void initialize(InputSplit split, TaskAttemptContext context) throws IOException
0:     {
0:         this.split = (ColumnFamilySplit) split;
0:         Configuration conf = context.getConfiguration();
0:         totalRowCount = (this.split.getLength() < Long.MAX_VALUE)
0:                       ? (int) this.split.getLength()
0:                       : ConfigHelper.getInputSplitSize(conf);
0:         cfName = ConfigHelper.getInputColumnFamily(conf);
0:         keyspace = ConfigHelper.getInputKeyspace(conf);              
0:         cqlQuery = CqlConfigHelper.getInputCql(conf);
0:         partitioner = ConfigHelper.getInputPartitioner(context.getConfiguration());
0:         try
0:         {
0:             if (cluster != null)
0:                 return;
0: 
0:             // create connection using thrift
0:             String[] locations = split.getLocations();
0:             Exception lastException = null;
0:             for (String location : locations)
0:             {
0:                 try
0:                 {
0:                     cluster = CqlConfigHelper.getInputCluster(location, conf);
0:                     break;
0:                 }
0:                 catch (Exception e)
0:                 {
0:                     lastException = e;
0:                     logger.warn("Failed to create authenticated client to {}", location);
0:                 }
0:             }
0:             if (cluster == null && lastException != null)
0:                 throw lastException;
0:         }
0:         catch (Exception e)
0:         {
0:             throw new RuntimeException(e);
0:         }
0: 
0:         session = cluster.connect(keyspace);
0:         rowIterator = new RowIterator();
0:         logger.debug("created {}", rowIterator);
0:     }
0: 
0:     public void close()
0:     {
0:         if (session != null)
0:             session.close();
0:     }
0: 
0:     public Long getCurrentKey()
0:     {
0:         return currentRow.left;
0:     }
0: 
0:     public Row getCurrentValue()
0:     {
0:         return currentRow.right;
0:     }
0: 
0:     public float getProgress()
0:     {
0:         if (!rowIterator.hasNext())
0:             return 1.0F;
0: 
0:         // the progress is likely to be reported slightly off the actual but close enough
0:         float progress = ((float) rowIterator.totalRead / totalRowCount);
0:         return progress > 1.0F ? 1.0F : progress;
0:     }
0: 
0:     public boolean nextKeyValue() throws IOException
0:     {
0:         if (!rowIterator.hasNext())
0:         {
0:             logger.debug("Finished scanning {} rows (estimate was: {})", rowIterator.totalRead, totalRowCount);
0:             return false;
0:         }
0: 
0:         try
0:         {
0:             currentRow = rowIterator.next();
0:         }
0:         catch (Exception e)
0:         {
0:             // throw it as IOException, so client can catch it and handle it at client side
0:             IOException ioe = new IOException(e.getMessage());
0:             ioe.initCause(ioe.getCause());
0:             throw ioe;
0:         }
0:         return true;
0:     }
0: 
0:     // Because the old Hadoop API wants us to write to the key and value
0:     // and the new asks for them, we need to copy the output of the new API
0:     // to the old. Thus, expect a small performance hit.
0:     // And obviously this wouldn't work for wide rows. But since ColumnFamilyInputFormat
0:     // and ColumnFamilyRecordReader don't support them, it should be fine for now.
0:     public boolean next(Long key, Row value) throws IOException
0:     {
0:         if (nextKeyValue())
0:         {
0:             key = getCurrentKey();
0:             value = getCurrentValue();
0:             return true;
0:         }
0:         return false;
0:     }
0: 
0:     public long getPos() throws IOException
0:     {
0:         return (long) rowIterator.totalRead;
0:     }
0: 
0:     public Long createKey()
0:     {
0:         return null;
0:     }
0: 
0:     public Row createValue()
0:     {
0:         return null;
0:     }
0: 
0:     /** CQL row iterator 
0:      *  Input cql query  
0:      *  1) select clause must include key columns (if we use partition key based row count)
0:      *  2) where clause must include token(partition_key1 ... partition_keyn) > ? and 
0:      *     token(partition_key1 ... partition_keyn) <= ? 
0:      */
0:     private class RowIterator extends AbstractIterator<Pair<Long, Row>>
0:     {
0:         private long keyId = 0L;
0:         protected int totalRead = 0; // total number of cf rows read
0:         protected Iterator<Row> rows;
0:         private Map<String, ByteBuffer> previousRowKey = new HashMap<String, ByteBuffer>(); // previous CF row key
0: 
0:         public RowIterator()
0:         {
0:             AbstractType type = partitioner.getTokenValidator();
0:             ResultSet rs = session.execute(cqlQuery, type.compose(type.fromString(split.getStartToken())), type.compose(type.fromString(split.getEndToken())) );
0:             for (ColumnMetadata meta : cluster.getMetadata().getKeyspace(keyspace).getTable(cfName).getPartitionKey())
0:                 partitionBoundColumns.put(meta.getName(), Boolean.TRUE);
0:             rows = rs.iterator();
0:         }
0: 
0:         protected Pair<Long, Row> computeNext()
0:         {
0:             if (rows == null || !rows.hasNext())
0:                 return endOfData();
0: 
0:             Row row = rows.next();
0:             Map<String, ByteBuffer> keyColumns = new HashMap<String, ByteBuffer>(); 
0:             for (String column : partitionBoundColumns.keySet())
0:                 keyColumns.put(column, row.getBytesUnsafe(column));
0: 
0:             // increase total CF row read
0:             if (previousRowKey.isEmpty() && !keyColumns.isEmpty())
0:             {
0:                 previousRowKey = keyColumns;
0:                 totalRead++;
0:             }
0:             else
0:             {
0:                 for (String column : partitionBoundColumns.keySet())
0:                 {
0:                     if (BytesType.bytesCompare(keyColumns.get(column), previousRowKey.get(column)) != 0)
0:                     {
0:                         previousRowKey = keyColumns;
0:                         totalRead++;
0:                         break;
0:                     }
0:                 }
0:             }
0:             keyId ++;
0:             return Pair.create(keyId, row);
0:         }
0:     }
0: }
============================================================================