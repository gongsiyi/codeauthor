1:f2c38f8: /*
1:f2c38f8:  * Licensed to the Apache Software Foundation (ASF) under one
1:f2c38f8:  * or more contributor license agreements.  See the NOTICE file
1:f2c38f8:  * distributed with this work for additional information
1:f2c38f8:  * regarding copyright ownership.  The ASF licenses this file
1:f2c38f8:  * to you under the Apache License, Version 2.0 (the
1:f2c38f8:  * "License"); you may not use this file except in compliance
1:f2c38f8:  * with the License.  You may obtain a copy of the License at
1:f2c38f8:  *
1:07cf56f:  *     http://www.apache.org/licenses/LICENSE-2.0
1:f2c38f8:  *
1:07cf56f:  * Unless required by applicable law or agreed to in writing, software
1:07cf56f:  * distributed under the License is distributed on an "AS IS" BASIS,
1:07cf56f:  * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
1:07cf56f:  * See the License for the specific language governing permissions and
1:07cf56f:  * limitations under the License.
1:f2c38f8:  */
1:f2c38f8: package org.apache.cassandra.utils;
1:f2c38f8: 
1:ef25537: import java.io.IOException;
1:ef25537: import java.util.Arrays;
1:ef25537: import java.util.concurrent.atomic.AtomicLongArray;
1:f2c38f8: 
1:6ea00a3: import com.google.common.base.Objects;
1:6ea00a3: 
1:2ae5272: import org.apache.cassandra.db.TypeSizes;
1:84eeb28: import org.apache.cassandra.io.ISerializer;
1:03f72ac: import org.apache.cassandra.io.util.DataInputPlus;
1:75508ec: import org.apache.cassandra.io.util.DataOutputPlus;
1:18f0234: import org.slf4j.Logger;
1:75508ec: 
1:f2c38f8: public class EstimatedHistogram
1:f2c38f8: {
1:5a6e2b0:     public static final EstimatedHistogramSerializer serializer = new EstimatedHistogramSerializer();
1:f2c38f8: 
1:f2c38f8:     /**
1:c00a5c1:      * The series of values to which the counts in `buckets` correspond:
1:09a1dd5:      * 1, 2, 3, 4, 5, 6, 7, 8, 10, 12, 14, 17, 20, etc.
1:c00a5c1:      * Thus, a `buckets` of [0, 0, 1, 10] would mean we had seen one value of 3 and 10 values of 4.
1:07cf56f:      *
1:c00a5c1:      * The series starts at 1 and grows by 1.2 each time (rounding and removing duplicates). It goes from 1
1:c00a5c1:      * to around 36M by default (creating 90+1 buckets), which will give us timing resolution from microseconds to
1:c00a5c1:      * 36 seconds, with less precision as the numbers get larger.
1:07cf56f:      *
1:09a1dd5:      * Each bucket represents values from (previous bucket offset, current offset].
1:f2c38f8:      */
1:16f99c5:     private final long[] bucketOffsets;
1:f2c38f8: 
1:09a1dd5:     // buckets is one element longer than bucketOffsets -- the last element is values greater than the last offset
1:f2c38f8:     final AtomicLongArray buckets;
1:f2c38f8: 
1:f2c38f8:     public EstimatedHistogram()
1:f2c38f8:     {
1:5ac1aab:         this(90);
1:c8136b9:     }
1:c8136b9: 
1:c00a5c1:     public EstimatedHistogram(int bucketCount)
1:c8136b9:     {
1:5aa19cb:         this(bucketCount, false);
1:5aa19cb:     }
1:5aa19cb: 
1:5aa19cb:     public EstimatedHistogram(int bucketCount, boolean considerZeroes)
1:5aa19cb:     {
1:5aa19cb:         bucketOffsets = newOffsets(bucketCount, considerZeroes);
1:09a1dd5:         buckets = new AtomicLongArray(bucketOffsets.length + 1);
1:c8136b9:     }
1:c8136b9: 
1:6e0395e:     /**
1:6e0395e:      * Create EstimatedHistogram from only bucket data.
1:6e0395e:      *
1:6e0395e:      * @param bucketData bucket data
1:6e0395e:      */
1:6e0395e:     public EstimatedHistogram(long[] bucketData)
1:6e0395e:     {
1:6e0395e:         assert bucketData != null && bucketData.length > 0 : "Bucket data must be an array of size more than 0";
1:6e0395e:         bucketOffsets = newOffsets(bucketData.length - 1, false);
1:6e0395e:         buckets = new AtomicLongArray(bucketData);
1:6e0395e:     }
1:6e0395e: 
1:2893b7f:     public EstimatedHistogram(long[] offsets, long[] bucketData)
1:18f0234:     {
1:2893b7f:         assert bucketData.length == offsets.length +1;
1:2893b7f:         bucketOffsets = offsets;
1:2893b7f:         buckets = new AtomicLongArray(bucketData);
1:f2c38f8:     }
1:f2c38f8: 
1:2e90259:     public static long[] newOffsets(int size, boolean considerZeroes)
1:2893b7f:     {
1:5aa19cb:         long[] result = new long[size + (considerZeroes ? 1 : 0)];
1:5aa19cb:         int i = 0;
1:5aa19cb:         if (considerZeroes)
1:5aa19cb:             result[i++] = 0;
1:c00a5c1:         long last = 1;
1:5aa19cb:         result[i++] = last;
1:5aa19cb:         for (; i < result.length; i++)
1:2893b7f:         {
1:c00a5c1:             long next = Math.round(last * 1.2);
1:c00a5c1:             if (next == last)
1:c00a5c1:                 next++;
1:16f99c5:             result[i] = next;
1:c00a5c1:             last = next;
1:2893b7f:         }
1:16f99c5: 
1:16f99c5:         return result;
1:c00a5c1:     }
1:f2c38f8: 
1:c8136b9:     /**
1:09a1dd5:      * @return the histogram values corresponding to each bucket index
1:07cf56f:      */
1:c00a5c1:     public long[] getBucketOffsets()
1:2893b7f:     {
1:c00a5c1:         return bucketOffsets;
1:c00a5c1:     }
1:18f0234: 
1:09a1dd5:     /**
1:09a1dd5:      * Increments the count of the bucket closest to n, rounding UP.
1:09a1dd5:      * @param n
1:09a1dd5:      */
1:f2c38f8:     public void add(long n)
1:f2c38f8:     {
1:f2c38f8:         int index = Arrays.binarySearch(bucketOffsets, n);
1:f2c38f8:         if (index < 0)
1:f2c38f8:         {
1:09a1dd5:             // inexact match, take the first bucket higher than n
1:f2c38f8:             index = -index - 1;
1:f2c38f8:         }
1:09a1dd5:         // else exact match; we're good
1:f2c38f8:         buckets.incrementAndGet(index);
1:f2c38f8:     }
1:f2c38f8: 
1:09a1dd5:     /**
1:09a1dd5:      * @return the count in the given bucket
1:09a1dd5:      */
1:09a1dd5:     long get(int bucket)
1:f2c38f8:     {
1:09a1dd5:         return buckets.get(bucket);
1:f2c38f8:     }
1:f2c38f8: 
1:09a1dd5:     /**
1:5001907:      * @param reset zero out buckets afterwards if true
1:09a1dd5:      * @return a long[] containing the current histogram buckets
1:09a1dd5:      */
1:09a1dd5:     public long[] getBuckets(boolean reset)
1:f2c38f8:     {
1:16f99c5:         final int len = buckets.length();
1:16f99c5:         long[] rv = new long[len];
1:07cdfd0: 
1:f2c38f8:         if (reset)
1:16f99c5:             for (int i = 0; i < len; i++)
1:16f99c5:                 rv[i] = buckets.getAndSet(i, 0L);
1:16f99c5:         else
1:16f99c5:             for (int i = 0; i < len; i++)
1:16f99c5:                 rv[i] = buckets.get(i);
1:f2c38f8: 
1:f2c38f8:         return rv;
1:f2c38f8:     }
1:2fd3268: 
1:09a1dd5:     /**
1:09a1dd5:      * @return the smallest value that could have been added to this histogram
1:09a1dd5:      */
1:c00a5c1:     public long min()
1:09a1dd5:     {
1:07cdfd0:         for (int i = 0; i < buckets.length(); i++)
1:c00a5c1:         {
1:c00a5c1:             if (buckets.get(i) > 0)
1:09a1dd5:                 return i == 0 ? 0 : 1 + bucketOffsets[i - 1];
1:f2c38f8:         }
1:c00a5c1:         return 0;
1:09a1dd5:     }
1:09a1dd5: 
1:09a1dd5:     /**
1:09a1dd5:      * @return the largest value that could have been added to this histogram.  If the histogram
1:09a1dd5:      * overflowed, returns Long.MAX_VALUE.
1:09a1dd5:      */
1:c00a5c1:     public long max()
1:c00a5c1:     {
1:09a1dd5:         int lastBucket = buckets.length() - 1;
1:5ac1aab:         if (buckets.get(lastBucket) > 0)
1:09a1dd5:             return Long.MAX_VALUE;
1:5ac1aab: 
1:5ac1aab:         for (int i = lastBucket - 1; i >= 0; i--)
1:c00a5c1:         {
1:c00a5c1:             if (buckets.get(i) > 0)
1:5ac1aab:                 return bucketOffsets[i];
1:c00a5c1:         }
1:c00a5c1:         return 0;
1:c00a5c1:     }
1:09a1dd5: 
1:09a1dd5:     /**
1:772c4f1:      * @param percentile
1:772c4f1:      * @return estimated value at given percentile
1:772c4f1:      */
1:772c4f1:     public long percentile(double percentile)
1:772c4f1:     {
1:772c4f1:         assert percentile >= 0 && percentile <= 1.0;
1:772c4f1:         int lastBucket = buckets.length() - 1;
1:772c4f1:         if (buckets.get(lastBucket) > 0)
1:772c4f1:             throw new IllegalStateException("Unable to compute when histogram overflowed");
1:772c4f1: 
1:d4e3786:         long pcount = (long) Math.ceil(count() * percentile);
1:772c4f1:         if (pcount == 0)
1:772c4f1:             return 0;
1:772c4f1: 
1:772c4f1:         long elements = 0;
1:772c4f1:         for (int i = 0; i < lastBucket; i++)
1:772c4f1:         {
1:772c4f1:             elements += buckets.get(i);
1:772c4f1:             if (elements >= pcount)
1:772c4f1:                 return bucketOffsets[i];
1:772c4f1:         }
1:772c4f1:         return 0;
1:772c4f1:     }
1:772c4f1: 
1:772c4f1:     /**
1:5aa19cb:      * @return the ceil of mean histogram value (average of bucket offsets, weighted by count)
1:c8136b9:      * @throws IllegalStateException if any values were greater than the largest bucket threshold
1:c8136b9:      */
1:09a1dd5:     public long mean()
1:c8136b9:     {
1:5aa19cb:         return (long) Math.ceil(rawMean());
1:5aa19cb:     }
1:5aa19cb: 
1:5aa19cb:     /**
1:5aa19cb:      * @return the mean histogram value (average of bucket offsets, weighted by count)
1:5aa19cb:      * @throws IllegalStateException if any values were greater than the largest bucket threshold
1:5aa19cb:      */
1:5aa19cb:     public double rawMean()
1:5aa19cb:     {
1:09a1dd5:         int lastBucket = buckets.length() - 1;
1:09a1dd5:         if (buckets.get(lastBucket) > 0)
1:09a1dd5:             throw new IllegalStateException("Unable to compute ceiling for max when histogram overflowed");
1:09a1dd5: 
1:09a1dd5:         long elements = 0;
1:09a1dd5:         long sum = 0;
1:09a1dd5:         for (int i = 0; i < lastBucket; i++)
1:c00a5c1:         {
1:16f99c5:             long bCount = buckets.get(i);
1:16f99c5:             elements += bCount;
1:16f99c5:             sum += bCount * bucketOffsets[i];
1:09a1dd5:         }
1:09a1dd5: 
1:5aa19cb:         return (double) sum / elements;
1:c00a5c1:     }
1:09a1dd5: 
2:09a1dd5:     /**
1:23785c3:      * @return the total number of non-zero values
1:23785c3:      */
1:23785c3:     public long count()
1:23785c3:     {
1:23785c3:        long sum = 0L;
1:23785c3:        for (int i = 0; i < buckets.length(); i++)
1:23785c3:            sum += buckets.get(i);
1:23785c3:        return sum;
1:23785c3:     }
1:23785c3: 
1:23785c3:     /**
1:6e0395e:      * @return the largest bucket offset
1:6e0395e:      */
1:6e0395e:     public long getLargestBucketOffset()
1:6e0395e:     {
1:6e0395e:         return bucketOffsets[bucketOffsets.length - 1];
1:6e0395e:     }
1:6e0395e: 
1:6e0395e:     /**
1:09a1dd5:      * @return true if this histogram has overflowed -- that is, a value larger than our largest bucket could bound was added
1:09a1dd5:      */
1:09a1dd5:     public boolean isOverflowed()
1:09a1dd5:     {
1:09a1dd5:         return buckets.get(buckets.length() - 1) > 0;
1:da6369b:     }
1:da6369b: 
1:18f0234:     /**
1:18f0234:      * log.debug() every record in the histogram
1:18f0234:      *
1:18f0234:      * @param log
1:18f0234:      */
1:18f0234:     public void log(Logger log)
1:da6369b:     {
1:18f0234:         // only print overflow if there is any
1:18f0234:         int nameCount;
1:18f0234:         if (buckets.get(buckets.length() - 1) == 0)
1:18f0234:             nameCount = buckets.length() - 1;
1:f2c38f8:         else
1:18f0234:             nameCount = buckets.length();
1:18f0234:         String[] names = new String[nameCount];
1:18f0234: 
1:18f0234:         int maxNameLength = 0;
1:18f0234:         for (int i = 0; i < nameCount; i++)
1:18f0234:         {
1:18f0234:             names[i] = nameOfRange(bucketOffsets, i);
1:18f0234:             maxNameLength = Math.max(maxNameLength, names[i].length());
1:18f0234:         }
1:18f0234: 
1:18f0234:         // emit log records
1:18f0234:         String formatstr = "%" + maxNameLength + "s: %d";
1:18f0234:         for (int i = 0; i < nameCount; i++)
1:18f0234:         {
1:18f0234:             long count = buckets.get(i);
1:18f0234:             // sort-of-hack to not print empty ranges at the start that are only used to demarcate the
1:18f0234:             // first populated range. for code clarity we don't omit this record from the maxNameLength
1:18f0234:             // calculation, and accept the unnecessary whitespace prefixes that will occasionally occur
1:18f0234:             if (i == 0 && count == 0)
1:18f0234:                 continue;
1:18f0234:             log.debug(String.format(formatstr, names[i], count));
1:18f0234:         }
1:18f0234:     }
1:18f0234: 
1:18f0234:     private static String nameOfRange(long[] bucketOffsets, int index)
1:18f0234:     {
1:18f0234:         StringBuilder sb = new StringBuilder();
1:18f0234:         appendRange(sb, bucketOffsets, index);
1:18f0234:         return sb.toString();
1:18f0234:     }
1:18f0234: 
1:18f0234:     private static void appendRange(StringBuilder sb, long[] bucketOffsets, int index)
1:18f0234:     {
1:18f0234:         sb.append("[");
1:18f0234:         if (index == 0)
1:18f0234:             if (bucketOffsets[0] > 0)
1:18f0234:                 // by original definition, this histogram is for values greater than zero only;
1:18f0234:                 // if values of 0 or less are required, an entry of lb-1 must be inserted at the start
1:18f0234:                 sb.append("1");
2:18f0234:             else
1:18f0234:                 sb.append("-Inf");
1:18f0234:         else
1:18f0234:             sb.append(bucketOffsets[index - 1] + 1);
1:18f0234:         sb.append("..");
1:18f0234:         if (index == bucketOffsets.length)
1:18f0234:             sb.append("Inf");
1:18f0234:         else
1:18f0234:             sb.append(bucketOffsets[index]);
1:18f0234:         sb.append("]");
1:18f0234:     }
1:18f0234: 
1:f44dc94:     @Override
1:f44dc94:     public boolean equals(Object o)
1:c00a5c1:     {
1:f44dc94:         if (this == o)
1:f44dc94:             return true;
1:f44dc94: 
1:f44dc94:         if (!(o instanceof EstimatedHistogram))
1:f44dc94:             return false;
1:f44dc94: 
1:f44dc94:         EstimatedHistogram that = (EstimatedHistogram) o;
1:f44dc94:         return Arrays.equals(getBucketOffsets(), that.getBucketOffsets()) &&
1:f44dc94:                Arrays.equals(getBuckets(false), that.getBuckets(false));
6:c00a5c1:     }
1:07cdfd0: 
1:6ea00a3:     @Override
1:6ea00a3:     public int hashCode()
1:6ea00a3:     {
1:6ea00a3:         return Objects.hashCode(getBucketOffsets(), getBuckets(false));
1:6ea00a3:     }
1:2fd3268: 
1:84eeb28:     public static class EstimatedHistogramSerializer implements ISerializer<EstimatedHistogram>
6:c00a5c1:     {
1:75508ec:         public void serialize(EstimatedHistogram eh, DataOutputPlus out) throws IOException
1:c00a5c1:         {
1:2893b7f:             long[] offsets = eh.getBucketOffsets();
1:09a1dd5:             long[] buckets = eh.getBuckets(false);
1:60d9c7f:             out.writeInt(buckets.length);
1:2893b7f:             for (int i = 0; i < buckets.length; i++)
1:2893b7f:             {
1:60d9c7f:                 out.writeLong(offsets[i == 0 ? 0 : i - 1]);
1:60d9c7f:                 out.writeLong(buckets[i]);
1:2893b7f:             }
1:2893b7f:         }
1:6ea00a3: 
1:03f72ac:         public EstimatedHistogram deserialize(DataInputPlus in) throws IOException
1:2893b7f:         {
1:60d9c7f:             int size = in.readInt();
1:2893b7f:             long[] offsets = new long[size - 1];
1:2893b7f:             long[] buckets = new long[size];
1:2893b7f: 
1:68d2526:             for (int i = 0; i < size; i++)
1:68d2526:             {
1:60d9c7f:                 offsets[i == 0 ? 0 : i - 1] = in.readLong();
1:60d9c7f:                 buckets[i] = in.readLong();
1:2893b7f:             }
1:2893b7f:             return new EstimatedHistogram(offsets, buckets);
1:84eeb28:         }
1:84eeb28: 
1:03f72ac:         public long serializedSize(EstimatedHistogram eh)
1:84eeb28:         {
1:74bf5aa:             int size = 0;
1:74bf5aa: 
1:74bf5aa:             long[] offsets = eh.getBucketOffsets();
1:74bf5aa:             long[] buckets = eh.getBuckets(false);
1:03f72ac:             size += TypeSizes.sizeof(buckets.length);
1:74bf5aa:             for (int i = 0; i < buckets.length; i++)
1:74bf5aa:             {
1:03f72ac:                 size += TypeSizes.sizeof(offsets[i == 0 ? 0 : i - 1]);
1:03f72ac:                 size += TypeSizes.sizeof(buckets[i]);
1:74bf5aa:             }
1:74bf5aa:             return size;
1:2893b7f:         }
1:2893b7f:     }
1:c00a5c1: }
============================================================================
author:Dave Brosius
-------------------------------------------------------------------------------
commit:68d2526
/////////////////////////////////////////////////////////////////////////
1:             for (int i = 0; i < size; i++)
1:             {
author:T Jake Luciani
-------------------------------------------------------------------------------
commit:2691c9e
author:Per Otterstrom
-------------------------------------------------------------------------------
commit:2e90259
/////////////////////////////////////////////////////////////////////////
1:     public static long[] newOffsets(int size, boolean considerZeroes)
author:Yuki Morishita
-------------------------------------------------------------------------------
commit:34b07a7
commit:6e0395e
/////////////////////////////////////////////////////////////////////////
1:     /**
1:      * Create EstimatedHistogram from only bucket data.
1:      *
1:      * @param bucketData bucket data
1:      */
1:     public EstimatedHistogram(long[] bucketData)
1:     {
1:         assert bucketData != null && bucketData.length > 0 : "Bucket data must be an array of size more than 0";
1:         bucketOffsets = newOffsets(bucketData.length - 1, false);
1:         buckets = new AtomicLongArray(bucketData);
1:     }
1: 
/////////////////////////////////////////////////////////////////////////
1:      * @return the largest bucket offset
1:      */
1:     public long getLargestBucketOffset()
1:     {
1:         return bucketOffsets[bucketOffsets.length - 1];
1:     }
1: 
1:     /**
commit:74bf5aa
/////////////////////////////////////////////////////////////////////////
0:         public long serializedSize(EstimatedHistogram eh, TypeSizes typeSizes)
1:             int size = 0;
1: 
1:             long[] offsets = eh.getBucketOffsets();
1:             long[] buckets = eh.getBuckets(false);
0:             size += typeSizes.sizeof(buckets.length);
1:             for (int i = 0; i < buckets.length; i++)
1:             {
0:                 size += typeSizes.sizeof(offsets[i == 0 ? 0 : i - 1]);
0:                 size += typeSizes.sizeof(buckets[i]);
1:             }
1:             return size;
author:Joshua McKenzie
-------------------------------------------------------------------------------
commit:5aa19cb
/////////////////////////////////////////////////////////////////////////
1:         this(bucketCount, false);
1:     }
1: 
1:     public EstimatedHistogram(int bucketCount, boolean considerZeroes)
1:     {
1:         bucketOffsets = newOffsets(bucketCount, considerZeroes);
/////////////////////////////////////////////////////////////////////////
0:     private static long[] newOffsets(int size, boolean considerZeroes)
1:         long[] result = new long[size + (considerZeroes ? 1 : 0)];
1:         int i = 0;
1:         if (considerZeroes)
1:             result[i++] = 0;
1:         result[i++] = last;
1:         for (; i < result.length; i++)
/////////////////////////////////////////////////////////////////////////
1:      * @return the ceil of mean histogram value (average of bucket offsets, weighted by count)
1:         return (long) Math.ceil(rawMean());
1:     }
1: 
1:     /**
1:      * @return the mean histogram value (average of bucket offsets, weighted by count)
1:      * @throws IllegalStateException if any values were greater than the largest bucket threshold
1:      */
1:     public double rawMean()
1:     {
/////////////////////////////////////////////////////////////////////////
1:         return (double) sum / elements;
commit:c8136b9
/////////////////////////////////////////////////////////////////////////
0:         this(bucketCount, false);
1:     }
1: 
0:     public EstimatedHistogram(int bucketCount, boolean considerZeroes)
1:     {
0:         bucketOffsets = newOffsets(bucketCount, considerZeroes);
/////////////////////////////////////////////////////////////////////////
0:     private static long[] newOffsets(int size, boolean considerZeroes)
0:         long[] result = new long[size + (considerZeroes ? 1 : 0)];
0:         int i = 0;
0:         if (considerZeroes)
0:             result[i++] = 0;
0:         result[i++] = last;
0:         for (; i < result.length; i++)
/////////////////////////////////////////////////////////////////////////
0:      * @return the ceil of mean histogram value (average of bucket offsets, weighted by count)
0:         return (long) Math.ceil(rawMean());
1:     }
1: 
1:     /**
0:      * @return the mean histogram value (average of bucket offsets, weighted by count)
1:      * @throws IllegalStateException if any values were greater than the largest bucket threshold
1:      */
0:     public double rawMean()
1:     {
/////////////////////////////////////////////////////////////////////////
0:         return (double) sum / elements;
author:Paulo Motta
-------------------------------------------------------------------------------
commit:4a849ef
/////////////////////////////////////////////////////////////////////////
0:      * log.trace() every record in the histogram
/////////////////////////////////////////////////////////////////////////
0:             log.trace(String.format(formatstr, names[i], count));
author:Ariel Weisberg
-------------------------------------------------------------------------------
commit:03f72ac
/////////////////////////////////////////////////////////////////////////
/////////////////////////////////////////////////////////////////////////
1: import org.apache.cassandra.io.util.DataInputPlus;
/////////////////////////////////////////////////////////////////////////
1:         public EstimatedHistogram deserialize(DataInputPlus in) throws IOException
/////////////////////////////////////////////////////////////////////////
1:         public long serializedSize(EstimatedHistogram eh)
1:             size += TypeSizes.sizeof(buckets.length);
1:                 size += TypeSizes.sizeof(offsets[i == 0 ? 0 : i - 1]);
1:                 size += TypeSizes.sizeof(buckets[i]);
author:Carl Yeksigian
-------------------------------------------------------------------------------
commit:d4e3786
/////////////////////////////////////////////////////////////////////////
1:         long pcount = (long) Math.ceil(count() * percentile);
author:belliottsmith
-------------------------------------------------------------------------------
commit:75508ec
/////////////////////////////////////////////////////////////////////////
/////////////////////////////////////////////////////////////////////////
1: import org.apache.cassandra.io.util.DataOutputPlus;
1: 
/////////////////////////////////////////////////////////////////////////
1:         public void serialize(EstimatedHistogram eh, DataOutputPlus out) throws IOException
author:Jonathan Ellis
-------------------------------------------------------------------------------
commit:de9be79
commit:16f99c5
/////////////////////////////////////////////////////////////////////////
1:     private final long[] bucketOffsets;
/////////////////////////////////////////////////////////////////////////
0:         bucketOffsets = newOffsets(bucketCount);
/////////////////////////////////////////////////////////////////////////
0:     private static long[] newOffsets(int size)
0:         long[] result = new long[size];
0:         result[0] = last;
1:             result[i] = next;
1: 
1:         return result;
/////////////////////////////////////////////////////////////////////////
1:         final int len = buckets.length();
1:         long[] rv = new long[len];
1:             for (int i = 0; i < len; i++)
1:                 rv[i] = buckets.getAndSet(i, 0L);
1:         else
1:             for (int i = 0; i < len; i++)
1:                 rv[i] = buckets.get(i);
/////////////////////////////////////////////////////////////////////////
1:             long bCount = buckets.get(i);
1:             elements += bCount;
1:             sum += bCount * bucketOffsets[i];
commit:c0ba85a
/////////////////////////////////////////////////////////////////////////
commit:60d9c7f
/////////////////////////////////////////////////////////////////////////
0:         public void serialize(EstimatedHistogram eh, DataOutput out) throws IOException
1:             out.writeInt(buckets.length);
1:                 out.writeLong(offsets[i == 0 ? 0 : i - 1]);
1:                 out.writeLong(buckets[i]);
0:         public EstimatedHistogram deserialize(DataInput in) throws IOException
1:             int size = in.readInt();
1:                 offsets[i == 0 ? 0 : i - 1] = in.readLong();
1:                 buckets[i] = in.readLong();
commit:2ae5272
/////////////////////////////////////////////////////////////////////////
1: import org.apache.cassandra.db.TypeSizes;
/////////////////////////////////////////////////////////////////////////
0:         public long serializedSize(EstimatedHistogram object, TypeSizes typeSizes)
commit:772c4f1
/////////////////////////////////////////////////////////////////////////
1:      * @param percentile
1:      * @return estimated value at given percentile
1:      */
1:     public long percentile(double percentile)
1:     {
1:         assert percentile >= 0 && percentile <= 1.0;
1:         int lastBucket = buckets.length() - 1;
1:         if (buckets.get(lastBucket) > 0)
1:             throw new IllegalStateException("Unable to compute when histogram overflowed");
1: 
0:         long pcount = (long) Math.floor(count() * percentile);
1:         if (pcount == 0)
1:             return 0;
1: 
1:         long elements = 0;
1:         for (int i = 0; i < lastBucket; i++)
1:         {
1:             elements += buckets.get(i);
1:             if (elements >= pcount)
1:                 return bucketOffsets[i];
1:         }
1:         return 0;
1:     }
1: 
1:     /**
commit:23785c3
/////////////////////////////////////////////////////////////////////////
1:      * @return the total number of non-zero values
1:      */
1:     public long count()
1:     {
1:        long sum = 0L;
1:        for (int i = 0; i < buckets.length(); i++) 
1:            sum += buckets.get(i);
1:        return sum;
1:     }
1: 
1:     /**
commit:84eeb28
/////////////////////////////////////////////////////////////////////////
1: import org.apache.cassandra.io.ISerializer;
/////////////////////////////////////////////////////////////////////////
1:     public static class EstimatedHistogramSerializer implements ISerializer<EstimatedHistogram>
/////////////////////////////////////////////////////////////////////////
1: 
0:         public long serializedSize(EstimatedHistogram object)
1:         {
0:             throw new UnsupportedOperationException();
1:         }
commit:5001907
/////////////////////////////////////////////////////////////////////////
1:      * @param reset zero out buckets afterwards if true
commit:f44dc94
/////////////////////////////////////////////////////////////////////////
1:     @Override
1:     public boolean equals(Object o)
1:         if (this == o)
1:             return true;
1:         
1:         if (!(o instanceof EstimatedHistogram))
1:             return false;
1:         
1:         EstimatedHistogram that = (EstimatedHistogram) o;
1:         return Arrays.equals(getBucketOffsets(), that.getBucketOffsets()) &&
1:                Arrays.equals(getBuckets(false), that.getBuckets(false));
commit:da6369b
/////////////////////////////////////////////////////////////////////////
0:     public boolean equals(EstimatedHistogram o)
1:     {
0:         return Arrays.equals(getBucketOffsets(), o.getBucketOffsets()) &&
0:                Arrays.equals(getBuckets(false), o.getBuckets(false));
1:     }
1: 
commit:6a0df02
/////////////////////////////////////////////////////////////////////////
commit:a9cbaae
commit:ef25537
/////////////////////////////////////////////////////////////////////////
1: import java.io.IOException;
1: import java.util.Arrays;
1: import java.util.concurrent.atomic.AtomicLongArray;
commit:5ac1aab
/////////////////////////////////////////////////////////////////////////
1:         this(90);
/////////////////////////////////////////////////////////////////////////
0:         int lastBucket = numBuckets - 1;
1:         if (buckets.get(lastBucket) > 0)
0:             throw new IllegalStateException("Unable to compute ceiling for max when all buckets are full");
1: 
1:         for (int i = lastBucket - 1; i >= 0; i--)
1:                 return bucketOffsets[i];
commit:7b3fdb2
/////////////////////////////////////////////////////////////////////////
0:     public long[] get(boolean reset)
commit:f2c38f8
/////////////////////////////////////////////////////////////////////////
1: /*
1: * Licensed to the Apache Software Foundation (ASF) under one
1: * or more contributor license agreements.  See the NOTICE file
1: * distributed with this work for additional information
1: * regarding copyright ownership.  The ASF licenses this file
1: * to you under the Apache License, Version 2.0 (the
1: * "License"); you may not use this file except in compliance
1: * with the License.  You may obtain a copy of the License at
1: *
0: *    http://www.apache.org/licenses/LICENSE-2.0
1: *
0: * Unless required by applicable law or agreed to in writing,
0: * software distributed under the License is distributed on an
0: * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
0: * KIND, either express or implied.  See the License for the
0: * specific language governing permissions and limitations
0: * under the License.
1: */
1: package org.apache.cassandra.utils;
1: 
0: import java.util.concurrent.atomic.AtomicLongArray;
0: import java.util.Arrays;
1: 
1: public class EstimatedHistogram
1: {
1: 
1:     /**
0:      * This series starts at 1 and grows by 1.2 each time (rounding down and removing duplicates). It goes from 1
0:      * to around 30M, which will give us timing resolution from microseconds to 30 seconds, with less precision
0:      * as the numbers get larger.
1:      */
0:     private static final long[] bucketOffsets = {
0:             1, 2, 3, 4, 5, 6, 7, 8, 10, 12, 15, 18, 22, 26, 31, 38, 46, 55, 66, 79, 95, 114, 137, 164, 197, 237, 284, 341, 410, 492, 590,
0:             708, 850, 1020, 1224, 1469, 1763, 2116, 2539, 3047, 3657, 4388, 5266, 6319, 7583, 9100, 10920, 13104, 15725, 18870, 22644,
0:             27173, 32608, 39130, 46956, 56347, 67617, 81140, 97368, 116842, 140210, 168252, 201903, 242283, 290740, 348888, 418666,
0:             502400, 602880, 723456, 868147, 1041776, 1250132, 1500158, 1800190, 2160228, 2592274, 3110728, 3732874, 4479449, 5375339,
0:             6450407, 7740489, 9288586, 11146304, 13375565, 16050678, 19260813, 23112976, 27735572, 33282686
0:     };
1: 
0:     private static final int numBuckets = bucketOffsets.length + 1;
1: 
1:     final AtomicLongArray buckets;
1: 
1:     public EstimatedHistogram()
1:     {
0:         buckets = new AtomicLongArray(numBuckets);
1:     }
1: 
1:     public void add(long n)
1:     {
1:         int index = Arrays.binarySearch(bucketOffsets, n);
1:         if (index < 0)
1:         {
0:             //inexact match, find closest bucket
1:             index = -index - 1;
1:         }
1:         else
1:         {
0:             //exact match, so we want the next highest one
0:             index += 1;
1:         }
1:         buckets.incrementAndGet(index);
1:     }
1: 
0:     public long[] get(Boolean reset)
1:     {
0:         long[] rv = new long[numBuckets];
0:         for (int i = 0; i < numBuckets; i++)
0:             rv[i] = buckets.get(i);
1: 
1:         if (reset)
0:             for (int i = 0; i < numBuckets; i++)
0:                 buckets.set(i, 0L);
1: 
1:         return rv;
1:     }
1: }
author:Benedict
-------------------------------------------------------------------------------
commit:18f0234
/////////////////////////////////////////////////////////////////////////
1: import org.slf4j.Logger;
/////////////////////////////////////////////////////////////////////////
1:     /**
1:      * log.debug() every record in the histogram
1:      *
1:      * @param log
1:      */
1:     public void log(Logger log)
1:     {
1: 
1:         // only print overflow if there is any
1:         int nameCount;
1:         if (buckets.get(buckets.length() - 1) == 0)
1:             nameCount = buckets.length() - 1;
1:         else
1:             nameCount = buckets.length();
1:         String[] names = new String[nameCount];
1: 
1:         int maxNameLength = 0;
1:         for (int i = 0; i < nameCount; i++)
1:         {
1:             names[i] = nameOfRange(bucketOffsets, i);
1:             maxNameLength = Math.max(maxNameLength, names[i].length());
1:         }
1: 
1:         // emit log records
1:         String formatstr = "%" + maxNameLength + "s: %d";
1:         for (int i = 0; i < nameCount; i++)
1:         {
1:             long count = buckets.get(i);
1:             // sort-of-hack to not print empty ranges at the start that are only used to demarcate the
1:             // first populated range. for code clarity we don't omit this record from the maxNameLength
1:             // calculation, and accept the unnecessary whitespace prefixes that will occasionally occur
1:             if (i == 0 && count == 0)
1:                 continue;
1:             log.debug(String.format(formatstr, names[i], count));
1:         }
1:     }
1: 
1:     private static String nameOfRange(long[] bucketOffsets, int index)
1:     {
1:         StringBuilder sb = new StringBuilder();
1:         appendRange(sb, bucketOffsets, index);
1:         return sb.toString();
1:     }
1: 
1:     private static void appendRange(StringBuilder sb, long[] bucketOffsets, int index)
1:     {
1:         sb.append("[");
1:         if (index == 0)
1:             if (bucketOffsets[0] > 0)
1:                 // by original definition, this histogram is for values greater than zero only;
1:                 // if values of 0 or less are required, an entry of lb-1 must be inserted at the start
1:                 sb.append("1");
1:             else
1:                 sb.append("-Inf");
1:         else
1:             sb.append(bucketOffsets[index - 1] + 1);
1:         sb.append("..");
1:         if (index == bucketOffsets.length)
1:             sb.append("Inf");
1:         else
1:             sb.append(bucketOffsets[index]);
1:         sb.append("]");
1:     }
1: 
author:Vijay Parthasarathy
-------------------------------------------------------------------------------
commit:cb25a8f
/////////////////////////////////////////////////////////////////////////
0: import org.apache.cassandra.db.DBTypeSizes;
/////////////////////////////////////////////////////////////////////////
0:         public long serializedSize(EstimatedHistogram object, DBTypeSizes typeSizes)
author:Sylvain Lebresne
-------------------------------------------------------------------------------
commit:07cdfd0
/////////////////////////////////////////////////////////////////////////
1:        for (int i = 0; i < buckets.length(); i++)
/////////////////////////////////////////////////////////////////////////
1: 
1: 
commit:2fd3268
/////////////////////////////////////////////////////////////////////////
0:        for (int i = 0; i < buckets.length(); i++)
/////////////////////////////////////////////////////////////////////////
1: 
1: 
commit:5a6e2b0
/////////////////////////////////////////////////////////////////////////
1:     public static final EstimatedHistogramSerializer serializer = new EstimatedHistogramSerializer();
commit:07cf56f
/////////////////////////////////////////////////////////////////////////
0:  * Licensed to the Apache Software Foundation (ASF) under one
0:  * or more contributor license agreements.  See the NOTICE file
0:  * distributed with this work for additional information
0:  * regarding copyright ownership.  The ASF licenses this file
0:  * to you under the Apache License, Version 2.0 (the
0:  * "License"); you may not use this file except in compliance
0:  * with the License.  You may obtain a copy of the License at
1:  *
1:  *     http://www.apache.org/licenses/LICENSE-2.0
1:  *
1:  * Unless required by applicable law or agreed to in writing, software
1:  * distributed under the License is distributed on an "AS IS" BASIS,
1:  * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
1:  * See the License for the specific language governing permissions and
1:  * limitations under the License.
1:  */
commit:6ea00a3
/////////////////////////////////////////////////////////////////////////
1: import com.google.common.base.Objects;
1: 
/////////////////////////////////////////////////////////////////////////
1:     @Override
1:     public int hashCode()
1:     {
1:         return Objects.hashCode(getBucketOffsets(), getBuckets(false));
1:     }
1: 
author:Brandon Williams
-------------------------------------------------------------------------------
commit:09a1dd5
/////////////////////////////////////////////////////////////////////////
0:     public static EstimatedHistogramSerializer serializer = new EstimatedHistogramSerializer();
1:      * 1, 2, 3, 4, 5, 6, 7, 8, 10, 12, 14, 17, 20, etc.
0:      *
1:      * Each bucket represents values from (previous bucket offset, current offset].
1:     // buckets is one element longer than bucketOffsets -- the last element is values greater than the last offset
/////////////////////////////////////////////////////////////////////////
1:         buckets = new AtomicLongArray(bucketOffsets.length + 1);
/////////////////////////////////////////////////////////////////////////
/////////////////////////////////////////////////////////////////////////
0:         for (int i = 1; i < size; i++)
/////////////////////////////////////////////////////////////////////////
1:     /**
1:      * @return the histogram values corresponding to each bucket index
1:      */
1: 
1:     /**
1:      * Increments the count of the bucket closest to n, rounding UP.
1:      * @param n
1:      */
1:             // inexact match, take the first bucket higher than n
1:         // else exact match; we're good
1:     /**
1:      * @return the count in the given bucket
1:      */
1:     long get(int bucket)
1:         return buckets.get(bucket);
1:     }
1: 
1:     /**
0:      * @param reset: zero out buckets afterwards if true
1:      * @return a long[] containing the current histogram buckets
1:      */
1:     public long[] getBuckets(boolean reset)
1:     {
0:         long[] rv = new long[buckets.length()];
0:         for (int i = 0; i < buckets.length(); i++)
0:             for (int i = 0; i < buckets.length(); i++)
1:     /**
1:      * @return the smallest value that could have been added to this histogram
1:      */
0:         for (int i = 0; i < buckets.length(); i++)
1:                 return i == 0 ? 0 : 1 + bucketOffsets[i - 1];
1:     /**
1:      * @return the largest value that could have been added to this histogram.  If the histogram
1:      * overflowed, returns Long.MAX_VALUE.
1:      */
1:         int lastBucket = buckets.length() - 1;
1:             return Long.MAX_VALUE;
/////////////////////////////////////////////////////////////////////////
1:     /**
0:      * @return the mean histogram value (average of bucket offsets, weighted by count)
0:      * @throws IllegalStateException if any values were greater than the largest bucket threshold
0:      */
1:     public long mean()
1:         int lastBucket = buckets.length() - 1;
1:         if (buckets.get(lastBucket) > 0)
1:             throw new IllegalStateException("Unable to compute ceiling for max when histogram overflowed");
1: 
1:         long elements = 0;
1:         long sum = 0;
1:         for (int i = 0; i < lastBucket; i++)
0:             elements += buckets.get(i);
0:             sum += buckets.get(i) * bucketOffsets[i];
1: 
0:         return (long) Math.ceil((double) sum / elements);
1:     }
1: 
1:     /**
1:      * @return true if this histogram has overflowed -- that is, a value larger than our largest bucket could bound was added
0:      */
1:     public boolean isOverflowed()
1:     {
1:         return buckets.get(buckets.length() - 1) > 0;
/////////////////////////////////////////////////////////////////////////
1:             long[] buckets = eh.getBuckets(false);
commit:2893b7f
/////////////////////////////////////////////////////////////////////////
0: import java.io.IOException;
0: import java.io.DataInputStream;
0: import java.io.DataOutputStream;
1: 
0: import org.apache.cassandra.io.ICompactSerializer;
/////////////////////////////////////////////////////////////////////////
0:     public static EstimatedHistogramSerializer serializer = new EstimatedHistogramSerializer();
0: 
/////////////////////////////////////////////////////////////////////////
1:     public EstimatedHistogram(long[] offsets, long[] bucketData)
1:     {
1:         assert bucketData.length == offsets.length +1;
1:         bucketOffsets = offsets;
1:         buckets = new AtomicLongArray(bucketData);
0:         numBuckets = bucketData.length;
1:     }
0: 
/////////////////////////////////////////////////////////////////////////
0: 
0:     public static class EstimatedHistogramSerializer implements ICompactSerializer<EstimatedHistogram>
1:     {
0:         public void serialize(EstimatedHistogram eh, DataOutputStream dos) throws IOException
1:         {
1:             long[] offsets = eh.getBucketOffsets();
0:             long[] buckets = eh.get(false);
0:             dos.writeInt(buckets.length);
1:             for (int i = 0; i < buckets.length; i++)
1:             {
0:                 dos.writeLong(offsets[i == 0 ? 0 : i - 1]);
0:                 dos.writeLong(buckets[i]);
1:             }
1:         }
0: 
0:         public EstimatedHistogram deserialize(DataInputStream dis) throws IOException
1:         {
0:             int size = dis.readInt();
1:             long[] offsets = new long[size - 1];
1:             long[] buckets = new long[size];
0: 
0:             for (int i = 0; i < size; i++) {
0:                 offsets[i == 0 ? 0 : i - 1] = dis.readLong();
0:                 buckets[i] = dis.readLong();
1:             }
1:             return new EstimatedHistogram(offsets, buckets);
1:         }
1:     }
commit:c00a5c1
/////////////////////////////////////////////////////////////////////////
1:      * The series of values to which the counts in `buckets` correspond:
0:      * 1, 2, 3, 4, 5, 6, 7, 8, 10, 12, 15, 18, 22, etc.
1:      * Thus, a `buckets` of [0, 0, 1, 10] would mean we had seen one value of 3 and 10 values of 4.
0:      *
1:      * The series starts at 1 and grows by 1.2 each time (rounding and removing duplicates). It goes from 1
1:      * to around 36M by default (creating 90+1 buckets), which will give us timing resolution from microseconds to
1:      * 36 seconds, with less precision as the numbers get larger.
0:     private long[] bucketOffsets;
0:     private int numBuckets;
0:         makeOffsets(90);
1:     public EstimatedHistogram(int bucketCount)
1:     {
0:         makeOffsets(bucketCount);
0:         buckets = new AtomicLongArray(numBuckets);
1:     }
0: 
0:     public EstimatedHistogram(long[] bucketData)
1:     {
0:         makeOffsets(bucketData.length - 1);
0:         buckets = new AtomicLongArray(bucketData);
1:     }
0: 
0:     private void makeOffsets(int size)
1:     {
0:         bucketOffsets = new long[size];
1:         long last = 1;
0:         bucketOffsets[0] = last;
0:         for(int i = 1; i < size; i++)
1:         {
1:             long next = Math.round(last * 1.2);
1:             if (next == last)
1:                 next++;
0:             bucketOffsets[i] = next;
1:             last = next;
1:         }
0:         numBuckets = bucketOffsets.length + 1;
1:     }
0: 
1:     public long[] getBucketOffsets()
1:     {
1:         return bucketOffsets;
1:     }
0:     
/////////////////////////////////////////////////////////////////////////
0: 
1:     public long min()
1:     {
0:         for (int i = 0; i < numBuckets; i++)
1:         {
1:             if (buckets.get(i) > 0)
0:                 return bucketOffsets[i == 0 ? 0 : i - 1];
1:         }
1:         return 0;
1:     }
0: 
1:     public long max()
1:     {
0:         for (int i = numBuckets - 1; i >= 0; i--)
1:         {
1:             if (buckets.get(i) > 0)
0:                 return bucketOffsets[i == 0 ? 0 : i - 1];
1:         }
1:         return 0;
1:     }
0: 
0:     public long median()
1:     {
0:         long max = 0;
0:         long median = 0;
0:         for (int i = 0; i < numBuckets; i++)
1:         {
0:             if (max < 1 || buckets.get(i) > max)
1:             {
0:                 max = buckets.get(i);
0:                 if (max > 0)
0:                     median = bucketOffsets[i == 0 ? 0 : i - 1];
1:             }
1:         }
0:         return median;
1:     }
author:Gary Dusbabek
-------------------------------------------------------------------------------
commit:1ecdd7f
/////////////////////////////////////////////////////////////////////////
0: import java.io.DataInput;
0: import java.io.DataOutput;
0: import org.apache.cassandra.io.ICompactSerializer2;
/////////////////////////////////////////////////////////////////////////
0:     public static class EstimatedHistogramSerializer implements ICompactSerializer2<EstimatedHistogram>
0:         public void serialize(EstimatedHistogram eh, DataOutput dos) throws IOException
/////////////////////////////////////////////////////////////////////////
0:         public EstimatedHistogram deserialize(DataInput dis) throws IOException
commit:6c06aa9
/////////////////////////////////////////////////////////////////////////
============================================================================