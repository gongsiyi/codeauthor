1:72790dc: /*
1:72790dc:  * Licensed to the Apache Software Foundation (ASF) under one
1:72790dc:  * or more contributor license agreements.  See the NOTICE file
1:72790dc:  * distributed with this work for additional information
1:72790dc:  * regarding copyright ownership.  The ASF licenses this file
1:72790dc:  * to you under the Apache License, Version 2.0 (the
1:72790dc:  * "License"); you may not use this file except in compliance
1:72790dc:  * with the License.  You may obtain a copy of the License at
1:72790dc:  *
1:72790dc:  *     http://www.apache.org/licenses/LICENSE-2.0
1:72790dc:  *
1:72790dc:  * Unless required by applicable law or agreed to in writing, software
1:72790dc:  * distributed under the License is distributed on an "AS IS" BASIS,
1:72790dc:  * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
1:72790dc:  * See the License for the specific language governing permissions and
1:72790dc:  * limitations under the License.
1:72790dc:  */
1:72790dc: package org.apache.cassandra.index.sasi.disk;
1:72790dc: 
1:72790dc: import java.io.File;
1:72790dc: import java.nio.ByteBuffer;
1:72790dc: import java.util.HashMap;
1:72790dc: import java.util.HashSet;
1:72790dc: import java.util.Map;
1:72790dc: import java.util.Set;
1:72790dc: import java.util.concurrent.*;
1:72790dc: 
1:72790dc: import org.apache.cassandra.concurrent.JMXEnabledThreadPoolExecutor;
1:72790dc: import org.apache.cassandra.concurrent.NamedThreadFactory;
1:72790dc: import org.apache.cassandra.config.ColumnDefinition;
1:72790dc: import org.apache.cassandra.db.DecoratedKey;
1:72790dc: import org.apache.cassandra.db.compaction.OperationType;
1:72790dc: import org.apache.cassandra.db.rows.Row;
1:72790dc: import org.apache.cassandra.db.rows.Unfiltered;
1:72790dc: import org.apache.cassandra.index.sasi.analyzer.AbstractAnalyzer;
1:72790dc: import org.apache.cassandra.index.sasi.conf.ColumnIndex;
1:72790dc: import org.apache.cassandra.index.sasi.utils.CombinedTermIterator;
1:72790dc: import org.apache.cassandra.index.sasi.utils.TypeUtil;
1:72790dc: import org.apache.cassandra.db.marshal.AbstractType;
1:5c4d5c7: import org.apache.cassandra.io.FSError;
1:72790dc: import org.apache.cassandra.io.sstable.Descriptor;
1:72790dc: import org.apache.cassandra.io.sstable.format.SSTableFlushObserver;
1:72790dc: import org.apache.cassandra.io.util.FileUtils;
1:72790dc: import org.apache.cassandra.utils.FBUtilities;
1:72790dc: import org.apache.cassandra.utils.Pair;
1:72790dc: 
1:72790dc: import com.google.common.annotations.VisibleForTesting;
1:72790dc: import com.google.common.util.concurrent.Futures;
1:72790dc: import com.google.common.util.concurrent.Uninterruptibles;
1:72790dc: 
1:72790dc: import org.slf4j.Logger;
1:72790dc: import org.slf4j.LoggerFactory;
1:72790dc: 
1:72790dc: public class PerSSTableIndexWriter implements SSTableFlushObserver
1:72790dc: {
1:72790dc:     private static final Logger logger = LoggerFactory.getLogger(PerSSTableIndexWriter.class);
1:72790dc: 
1:72790dc:     private static final ThreadPoolExecutor INDEX_FLUSHER_MEMTABLE;
1:72790dc:     private static final ThreadPoolExecutor INDEX_FLUSHER_GENERAL;
1:72790dc: 
1:72790dc:     static
1:72790dc:     {
1:72790dc:         INDEX_FLUSHER_GENERAL = new JMXEnabledThreadPoolExecutor(1, 8, 60, TimeUnit.SECONDS,
1:72790dc:                                                                  new LinkedBlockingQueue<>(),
1:72790dc:                                                                  new NamedThreadFactory("SASI-General"),
1:72790dc:                                                                  "internal");
1:72790dc:         INDEX_FLUSHER_GENERAL.allowCoreThreadTimeOut(true);
1:72790dc: 
1:72790dc:         INDEX_FLUSHER_MEMTABLE = new JMXEnabledThreadPoolExecutor(1, 8, 60, TimeUnit.SECONDS,
1:72790dc:                                                                   new LinkedBlockingQueue<>(),
1:72790dc:                                                                   new NamedThreadFactory("SASI-Memtable"),
1:72790dc:                                                                   "internal");
1:72790dc:         INDEX_FLUSHER_MEMTABLE.allowCoreThreadTimeOut(true);
1:72790dc:     }
1:72790dc: 
1:72790dc:     private final int nowInSec = FBUtilities.nowInSeconds();
1:72790dc: 
1:72790dc:     private final Descriptor descriptor;
1:72790dc:     private final OperationType source;
1:72790dc: 
1:72790dc:     private final AbstractType<?> keyValidator;
1:72790dc:     private final Map<ColumnDefinition, ColumnIndex> supportedIndexes;
1:72790dc: 
1:72790dc:     @VisibleForTesting
1:72790dc:     protected final Map<ColumnDefinition, Index> indexes;
1:72790dc: 
1:72790dc:     private DecoratedKey currentKey;
1:72790dc:     private long currentKeyPosition;
1:72790dc:     private boolean isComplete;
1:72790dc: 
1:72790dc:     public PerSSTableIndexWriter(AbstractType<?> keyValidator,
1:72790dc:                                  Descriptor descriptor,
1:72790dc:                                  OperationType source,
1:72790dc:                                  Map<ColumnDefinition, ColumnIndex> supportedIndexes)
1:72790dc:     {
1:72790dc:         this.keyValidator = keyValidator;
1:72790dc:         this.descriptor = descriptor;
1:72790dc:         this.source = source;
1:72790dc:         this.supportedIndexes = supportedIndexes;
1:72790dc:         this.indexes = new HashMap<>();
1:72790dc:     }
1:72790dc: 
1:72790dc:     public void begin()
1:72790dc:     {}
1:72790dc: 
1:72790dc:     public void startPartition(DecoratedKey key, long curPosition)
1:72790dc:     {
1:72790dc:         currentKey = key;
1:72790dc:         currentKeyPosition = curPosition;
1:72790dc:     }
1:72790dc: 
1:7d857b4:     public void nextUnfilteredCluster(Unfiltered unfiltered, long currentRowOffset)
1:72790dc:     {
1:72790dc:         if (!unfiltered.isRow())
1:72790dc:             return;
1:72790dc: 
1:72790dc:         Row row = (Row) unfiltered;
1:72790dc: 
1:72790dc:         supportedIndexes.keySet().forEach((column) -> {
1:72790dc:             ByteBuffer value = ColumnIndex.getValueOf(column, row, nowInSec);
1:72790dc:             if (value == null)
1:72790dc:                 return;
1:72790dc: 
1:72790dc:             ColumnIndex columnIndex = supportedIndexes.get(column);
1:72790dc:             if (columnIndex == null)
1:72790dc:                 return;
1:72790dc: 
1:72790dc:             Index index = indexes.get(column);
1:72790dc:             if (index == null)
1:5c4d5c7:                 indexes.put(column, (index = newIndex(columnIndex)));
1:72790dc: 
1:7d857b4:             index.add(value.duplicate(), currentKey, currentKeyPosition, currentRowOffset);
1:72790dc:         });
1:72790dc:     }
1:72790dc: 
1:7d857b4:     public void nextUnfilteredCluster(Unfiltered unfilteredCluster)
1:7d857b4:     {
1:7d857b4:         throw new UnsupportedOperationException("SASI Index does not support direct row access.");
1:7d857b4:     }
1:7d857b4: 
1:72790dc:     public void complete()
1:72790dc:     {
1:72790dc:         if (isComplete)
1:72790dc:             return;
1:72790dc: 
1:72790dc:         currentKey = null;
1:72790dc: 
1:72790dc:         try
1:72790dc:         {
1:72790dc:             CountDownLatch latch = new CountDownLatch(indexes.size());
1:72790dc:             for (Index index : indexes.values())
1:72790dc:                 index.complete(latch);
1:72790dc: 
1:72790dc:             Uninterruptibles.awaitUninterruptibly(latch);
1:72790dc:         }
1:72790dc:         finally
1:72790dc:         {
1:72790dc:             indexes.clear();
1:72790dc:             isComplete = true;
1:72790dc:         }
1:72790dc:     }
1:72790dc: 
1:72790dc:     public Index getIndex(ColumnDefinition columnDef)
1:72790dc:     {
1:72790dc:         return indexes.get(columnDef);
1:72790dc:     }
1:72790dc: 
1:72790dc:     public Descriptor getDescriptor()
1:72790dc:     {
1:72790dc:         return descriptor;
1:72790dc:     }
1:72790dc: 
1:72790dc:     @VisibleForTesting
1:5c4d5c7:     protected Index newIndex(ColumnIndex columnIndex)
1:5c4d5c7:     {
1:5c4d5c7:         return new Index(columnIndex);
1:5c4d5c7:     }
1:5c4d5c7: 
1:5c4d5c7:     @VisibleForTesting
1:72790dc:     protected class Index
1:72790dc:     {
1:5c4d5c7:         @VisibleForTesting
1:5c4d5c7:         protected final String outputFile;
1:5c4d5c7: 
1:72790dc:         private final ColumnIndex columnIndex;
1:72790dc:         private final AbstractAnalyzer analyzer;
1:72790dc:         private final long maxMemorySize;
1:72790dc: 
1:72790dc:         @VisibleForTesting
1:72790dc:         protected final Set<Future<OnDiskIndex>> segments;
1:72790dc:         private int segmentNumber = 0;
1:72790dc: 
1:72790dc:         private OnDiskIndexBuilder currentBuilder;
1:72790dc: 
1:72790dc:         public Index(ColumnIndex columnIndex)
1:72790dc:         {
1:72790dc:             this.columnIndex = columnIndex;
1:72790dc:             this.outputFile = descriptor.filenameFor(columnIndex.getComponent());
1:72790dc:             this.analyzer = columnIndex.getAnalyzer();
1:72790dc:             this.segments = new HashSet<>();
1:72790dc:             this.maxMemorySize = maxMemorySize(columnIndex);
1:72790dc:             this.currentBuilder = newIndexBuilder();
1:72790dc:         }
1:72790dc: 
1:7d857b4:         public void add(ByteBuffer term, DecoratedKey key, long partitoinOffset, long rowOffset)
1:72790dc:         {
1:72790dc:             if (term.remaining() == 0)
1:72790dc:                 return;
1:72790dc: 
1:72790dc:             boolean isAdded = false;
1:72790dc: 
1:72790dc:             analyzer.reset(term);
1:72790dc:             while (analyzer.hasNext())
1:72790dc:             {
1:72790dc:                 ByteBuffer token = analyzer.next();
1:72790dc:                 int size = token.remaining();
1:72790dc: 
1:72790dc:                 if (token.remaining() >= OnDiskIndexBuilder.MAX_TERM_SIZE)
1:72790dc:                 {
1:db68ac9:                     logger.info("Rejecting value (size {}, maximum {}) for column {} (analyzed {}) at {} SSTable.",
1:db68ac9:                             FBUtilities.prettyPrintMemory(term.remaining()),
1:db68ac9:                             FBUtilities.prettyPrintMemory(OnDiskIndexBuilder.MAX_TERM_SIZE),
1:72790dc:                             columnIndex.getColumnName(),
1:72790dc:                             columnIndex.getMode().isAnalyzed,
1:72790dc:                             descriptor);
2:72790dc:                     continue;
1:72790dc:                 }
1:72790dc: 
1:72790dc:                 if (!TypeUtil.isValid(token, columnIndex.getValidator()))
1:72790dc:                 {
1:72790dc:                     if ((token = TypeUtil.tryUpcast(token, columnIndex.getValidator())) == null)
1:72790dc:                     {
1:db68ac9:                         logger.info("({}) Failed to add {} to index for key: {}, value size was {}, validator is {}.",
1:72790dc:                                     outputFile,
1:72790dc:                                     columnIndex.getColumnName(),
1:72790dc:                                     keyValidator.getString(key.getKey()),
1:db68ac9:                                     FBUtilities.prettyPrintMemory(size),
1:72790dc:                                     columnIndex.getValidator());
1:72790dc:                         continue;
1:72790dc:                     }
1:72790dc:                 }
1:72790dc: 
1:7d857b4:                 currentBuilder.add(token, key, partitoinOffset, rowOffset);
1:72790dc:                 isAdded = true;
1:72790dc:             }
1:72790dc: 
1:72790dc:             if (!isAdded || currentBuilder.estimatedMemoryUse() < maxMemorySize)
1:72790dc:                 return; // non of the generated tokens were added to the index or memory size wasn't reached
1:72790dc: 
1:72790dc:             segments.add(getExecutor().submit(scheduleSegmentFlush(false)));
1:72790dc:         }
1:72790dc: 
1:72790dc:         @VisibleForTesting
1:72790dc:         protected Callable<OnDiskIndex> scheduleSegmentFlush(final boolean isFinal)
1:72790dc:         {
1:72790dc:             final OnDiskIndexBuilder builder = currentBuilder;
1:72790dc:             currentBuilder = newIndexBuilder();
1:72790dc: 
1:72790dc:             final String segmentFile = filename(isFinal);
1:72790dc: 
1:72790dc:             return () -> {
1:5c4d5c7:                 long start = System.nanoTime();
1:72790dc: 
1:72790dc:                 try
1:72790dc:                 {
1:72790dc:                     File index = new File(segmentFile);
1:72790dc:                     return builder.finish(index) ? new OnDiskIndex(index, columnIndex.getValidator(), null) : null;
1:72790dc:                 }
1:5c4d5c7:                 catch (Exception | FSError e)
1:72790dc:                 {
1:5c4d5c7:                     logger.error("Failed to build index segment {}", segmentFile, e);
1:5c4d5c7:                     return null;
1:5c4d5c7:                 }
1:72790dc:                 finally
1:5c4d5c7:                 {
1:72790dc:                     if (!isFinal)
1:5c4d5c7:                         logger.info("Flushed index segment {}, took {} ms.", segmentFile, TimeUnit.NANOSECONDS.toMillis(System.nanoTime() - start));
1:72790dc:                 }
1:72790dc:             };
1:72790dc:         }
1:72790dc: 
1:72790dc:         public void complete(final CountDownLatch latch)
1:72790dc:         {
1:72790dc:             logger.info("Scheduling index flush to {}", outputFile);
1:72790dc: 
1:72790dc:             getExecutor().submit((Runnable) () -> {
2:72790dc:                 long start1 = System.nanoTime();
1:72790dc: 
1:72790dc:                 OnDiskIndex[] parts = new OnDiskIndex[segments.size() + 1];
1:72790dc: 
1:72790dc:                 try
1:72790dc:                 {
1:72790dc:                     // no parts present, build entire index from memory
1:72790dc:                     if (segments.isEmpty())
1:72790dc:                     {
1:72790dc:                         scheduleSegmentFlush(true).call();
1:72790dc:                         return;
1:72790dc:                     }
1:72790dc: 
1:72790dc:                     // parts are present but there is something still in memory, let's flush that inline
1:72790dc:                     if (!currentBuilder.isEmpty())
1:72790dc:                     {
1:733d1ee:                         @SuppressWarnings("resource")
1:72790dc:                         OnDiskIndex last = scheduleSegmentFlush(false).call();
1:72790dc:                         segments.add(Futures.immediateFuture(last));
1:72790dc:                     }
1:72790dc: 
1:72790dc:                     int index = 0;
1:72790dc:                     ByteBuffer combinedMin = null, combinedMax = null;
1:72790dc: 
1:72790dc:                     for (Future<OnDiskIndex> f : segments)
1:72790dc:                     {
1:05660a5:                         @SuppressWarnings("resource")
1:5c4d5c7:                         OnDiskIndex part = f.get();
2:72790dc:                         if (part == null)
1:72790dc:                             continue;
1:72790dc: 
1:72790dc:                         parts[index++] = part;
1:72790dc:                         combinedMin = (combinedMin == null || keyValidator.compare(combinedMin, part.minKey()) > 0) ? part.minKey() : combinedMin;
1:72790dc:                         combinedMax = (combinedMax == null || keyValidator.compare(combinedMax, part.maxKey()) < 0) ? part.maxKey() : combinedMax;
1:72790dc:                     }
1:72790dc: 
1:72790dc:                     OnDiskIndexBuilder builder = newIndexBuilder();
1:72790dc:                     builder.finish(Pair.create(combinedMin, combinedMax),
1:72790dc:                                    new File(outputFile),
1:72790dc:                                    new CombinedTermIterator(parts));
1:72790dc:                 }
1:5c4d5c7:                 catch (Exception | FSError e)
1:72790dc:                 {
1:72790dc:                     logger.error("Failed to flush index {}.", outputFile, e);
1:72790dc:                     FileUtils.delete(outputFile);
1:72790dc:                 }
1:72790dc:                 finally
1:72790dc:                 {
1:72790dc:                     logger.info("Index flush to {} took {} ms.", outputFile, TimeUnit.NANOSECONDS.toMillis(System.nanoTime() - start1));
1:72790dc: 
1:5c4d5c7:                     for (int segment = 0; segment < segmentNumber; segment++)
1:72790dc:                     {
1:05660a5:                         @SuppressWarnings("resource")
1:5c4d5c7:                         OnDiskIndex part = parts[segment];
1:72790dc: 
1:5c4d5c7:                         if (part != null)
1:72790dc:                             FileUtils.closeQuietly(part);
1:72790dc: 
1:5c4d5c7:                         FileUtils.delete(outputFile + "_" + segment);
1:72790dc:                     }
1:5c4d5c7: 
1:72790dc:                     latch.countDown();
1:72790dc:                 }
1:72790dc:             });
1:72790dc:         }
1:72790dc: 
1:72790dc:         private ExecutorService getExecutor()
1:72790dc:         {
1:72790dc:             return source == OperationType.FLUSH ? INDEX_FLUSHER_MEMTABLE : INDEX_FLUSHER_GENERAL;
1:72790dc:         }
1:72790dc: 
1:72790dc:         private OnDiskIndexBuilder newIndexBuilder()
1:72790dc:         {
1:72790dc:             return new OnDiskIndexBuilder(keyValidator, columnIndex.getValidator(), columnIndex.getMode().mode);
1:72790dc:         }
1:72790dc: 
1:72790dc:         public String filename(boolean isFinal)
1:72790dc:         {
1:72790dc:             return outputFile + (isFinal ? "" : "_" + segmentNumber++);
1:72790dc:         }
1:72790dc:     }
1:72790dc: 
1:72790dc:     protected long maxMemorySize(ColumnIndex columnIndex)
1:72790dc:     {
1:72790dc:         // 1G for memtable and configuration for compaction
1:72790dc:         return source == OperationType.FLUSH ? 1073741824L : columnIndex.getMode().maxCompactionFlushMemoryInMb;
1:72790dc:     }
1:72790dc: 
1:72790dc:     public int hashCode()
1:72790dc:     {
1:72790dc:         return descriptor.hashCode();
1:72790dc:     }
1:72790dc: 
1:72790dc:     public boolean equals(Object o)
1:72790dc:     {
1:72790dc:         return !(o == null || !(o instanceof PerSSTableIndexWriter)) && descriptor.equals(((PerSSTableIndexWriter) o).descriptor);
1:72790dc:     }
1:72790dc: }
============================================================================
author:Alex Petrov
-------------------------------------------------------------------------------
commit:7d857b4
/////////////////////////////////////////////////////////////////////////
1:     public void nextUnfilteredCluster(Unfiltered unfiltered, long currentRowOffset)
/////////////////////////////////////////////////////////////////////////
1:             index.add(value.duplicate(), currentKey, currentKeyPosition, currentRowOffset);
1:     public void nextUnfilteredCluster(Unfiltered unfilteredCluster)
1:     {
1:         throw new UnsupportedOperationException("SASI Index does not support direct row access.");
1:     }
1: 
/////////////////////////////////////////////////////////////////////////
1:         public void add(ByteBuffer term, DecoratedKey key, long partitoinOffset, long rowOffset)
/////////////////////////////////////////////////////////////////////////
1:                 currentBuilder.add(token, key, partitoinOffset, rowOffset);
author:Sam Tunnicliffe
-------------------------------------------------------------------------------
commit:05660a5
/////////////////////////////////////////////////////////////////////////
1:                         @SuppressWarnings("resource")
/////////////////////////////////////////////////////////////////////////
1:                         @SuppressWarnings("resource")
author:Giampaolo Trapasso
-------------------------------------------------------------------------------
commit:db68ac9
/////////////////////////////////////////////////////////////////////////
1:                     logger.info("Rejecting value (size {}, maximum {}) for column {} (analyzed {}) at {} SSTable.",
1:                             FBUtilities.prettyPrintMemory(term.remaining()),
1:                             FBUtilities.prettyPrintMemory(OnDiskIndexBuilder.MAX_TERM_SIZE),
/////////////////////////////////////////////////////////////////////////
1:                         logger.info("({}) Failed to add {} to index for key: {}, value size was {}, validator is {}.",
1:                                     FBUtilities.prettyPrintMemory(size),
author:Pavel Yaskevich
-------------------------------------------------------------------------------
commit:b6ff7f6
commit:72790dc
/////////////////////////////////////////////////////////////////////////
1: /*
1:  * Licensed to the Apache Software Foundation (ASF) under one
1:  * or more contributor license agreements.  See the NOTICE file
1:  * distributed with this work for additional information
1:  * regarding copyright ownership.  The ASF licenses this file
1:  * to you under the Apache License, Version 2.0 (the
1:  * "License"); you may not use this file except in compliance
1:  * with the License.  You may obtain a copy of the License at
1:  *
1:  *     http://www.apache.org/licenses/LICENSE-2.0
1:  *
1:  * Unless required by applicable law or agreed to in writing, software
1:  * distributed under the License is distributed on an "AS IS" BASIS,
1:  * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
1:  * See the License for the specific language governing permissions and
1:  * limitations under the License.
1:  */
1: package org.apache.cassandra.index.sasi.disk;
1: 
1: import java.io.File;
1: import java.nio.ByteBuffer;
1: import java.util.HashMap;
1: import java.util.HashSet;
1: import java.util.Map;
1: import java.util.Set;
1: import java.util.concurrent.*;
1: 
1: import org.apache.cassandra.concurrent.JMXEnabledThreadPoolExecutor;
1: import org.apache.cassandra.concurrent.NamedThreadFactory;
1: import org.apache.cassandra.config.ColumnDefinition;
1: import org.apache.cassandra.db.DecoratedKey;
1: import org.apache.cassandra.db.compaction.OperationType;
1: import org.apache.cassandra.db.rows.Row;
1: import org.apache.cassandra.db.rows.Unfiltered;
1: import org.apache.cassandra.index.sasi.analyzer.AbstractAnalyzer;
1: import org.apache.cassandra.index.sasi.conf.ColumnIndex;
1: import org.apache.cassandra.index.sasi.utils.CombinedTermIterator;
1: import org.apache.cassandra.index.sasi.utils.TypeUtil;
1: import org.apache.cassandra.db.marshal.AbstractType;
1: import org.apache.cassandra.io.sstable.Descriptor;
1: import org.apache.cassandra.io.sstable.format.SSTableFlushObserver;
1: import org.apache.cassandra.io.util.FileUtils;
1: import org.apache.cassandra.utils.FBUtilities;
1: import org.apache.cassandra.utils.Pair;
1: 
1: import com.google.common.annotations.VisibleForTesting;
1: import com.google.common.util.concurrent.Futures;
1: import com.google.common.util.concurrent.Uninterruptibles;
1: 
1: import org.slf4j.Logger;
1: import org.slf4j.LoggerFactory;
1: 
1: public class PerSSTableIndexWriter implements SSTableFlushObserver
1: {
1:     private static final Logger logger = LoggerFactory.getLogger(PerSSTableIndexWriter.class);
1: 
1:     private static final ThreadPoolExecutor INDEX_FLUSHER_MEMTABLE;
1:     private static final ThreadPoolExecutor INDEX_FLUSHER_GENERAL;
1: 
1:     static
1:     {
1:         INDEX_FLUSHER_GENERAL = new JMXEnabledThreadPoolExecutor(1, 8, 60, TimeUnit.SECONDS,
1:                                                                  new LinkedBlockingQueue<>(),
1:                                                                  new NamedThreadFactory("SASI-General"),
1:                                                                  "internal");
1:         INDEX_FLUSHER_GENERAL.allowCoreThreadTimeOut(true);
1: 
1:         INDEX_FLUSHER_MEMTABLE = new JMXEnabledThreadPoolExecutor(1, 8, 60, TimeUnit.SECONDS,
1:                                                                   new LinkedBlockingQueue<>(),
1:                                                                   new NamedThreadFactory("SASI-Memtable"),
1:                                                                   "internal");
1:         INDEX_FLUSHER_MEMTABLE.allowCoreThreadTimeOut(true);
1:     }
1: 
1:     private final int nowInSec = FBUtilities.nowInSeconds();
1: 
1:     private final Descriptor descriptor;
1:     private final OperationType source;
1: 
1:     private final AbstractType<?> keyValidator;
1:     private final Map<ColumnDefinition, ColumnIndex> supportedIndexes;
1: 
1:     @VisibleForTesting
1:     protected final Map<ColumnDefinition, Index> indexes;
1: 
1:     private DecoratedKey currentKey;
1:     private long currentKeyPosition;
1:     private boolean isComplete;
1: 
1:     public PerSSTableIndexWriter(AbstractType<?> keyValidator,
1:                                  Descriptor descriptor,
1:                                  OperationType source,
1:                                  Map<ColumnDefinition, ColumnIndex> supportedIndexes)
1:     {
1:         this.keyValidator = keyValidator;
1:         this.descriptor = descriptor;
1:         this.source = source;
1:         this.supportedIndexes = supportedIndexes;
1:         this.indexes = new HashMap<>();
1:     }
1: 
1:     public void begin()
1:     {}
1: 
1:     public void startPartition(DecoratedKey key, long curPosition)
1:     {
1:         currentKey = key;
1:         currentKeyPosition = curPosition;
1:     }
1: 
0:     public void nextUnfilteredCluster(Unfiltered unfiltered)
1:     {
1:         if (!unfiltered.isRow())
1:             return;
1: 
1:         Row row = (Row) unfiltered;
1: 
1:         supportedIndexes.keySet().forEach((column) -> {
1:             ByteBuffer value = ColumnIndex.getValueOf(column, row, nowInSec);
1:             if (value == null)
1:                 return;
1: 
1:             ColumnIndex columnIndex = supportedIndexes.get(column);
1:             if (columnIndex == null)
1:                 return;
1: 
1:             Index index = indexes.get(column);
1:             if (index == null)
0:                 indexes.put(column, (index = new Index(columnIndex)));
1: 
0:             index.add(value.duplicate(), currentKey, currentKeyPosition);
1:         });
1:     }
1: 
1:     public void complete()
1:     {
1:         if (isComplete)
1:             return;
1: 
1:         currentKey = null;
1: 
1:         try
1:         {
1:             CountDownLatch latch = new CountDownLatch(indexes.size());
1:             for (Index index : indexes.values())
1:                 index.complete(latch);
1: 
1:             Uninterruptibles.awaitUninterruptibly(latch);
1:         }
1:         finally
1:         {
1:             indexes.clear();
1:             isComplete = true;
1:         }
1:     }
1: 
1:     public Index getIndex(ColumnDefinition columnDef)
1:     {
1:         return indexes.get(columnDef);
1:     }
1: 
1:     public Descriptor getDescriptor()
1:     {
1:         return descriptor;
1:     }
1: 
1:     @VisibleForTesting
1:     protected class Index
1:     {
1:         private final ColumnIndex columnIndex;
0:         private final String outputFile;
1:         private final AbstractAnalyzer analyzer;
1:         private final long maxMemorySize;
1: 
1:         @VisibleForTesting
1:         protected final Set<Future<OnDiskIndex>> segments;
1:         private int segmentNumber = 0;
1: 
1:         private OnDiskIndexBuilder currentBuilder;
1: 
1:         public Index(ColumnIndex columnIndex)
1:         {
1:             this.columnIndex = columnIndex;
1:             this.outputFile = descriptor.filenameFor(columnIndex.getComponent());
1:             this.analyzer = columnIndex.getAnalyzer();
1:             this.segments = new HashSet<>();
1:             this.maxMemorySize = maxMemorySize(columnIndex);
1:             this.currentBuilder = newIndexBuilder();
1:         }
1: 
0:         public void add(ByteBuffer term, DecoratedKey key, long keyPosition)
1:         {
1:             if (term.remaining() == 0)
1:                 return;
1: 
1:             boolean isAdded = false;
1: 
1:             analyzer.reset(term);
1:             while (analyzer.hasNext())
1:             {
1:                 ByteBuffer token = analyzer.next();
1:                 int size = token.remaining();
1: 
1:                 if (token.remaining() >= OnDiskIndexBuilder.MAX_TERM_SIZE)
1:                 {
0:                     logger.info("Rejecting value (size {}, maximum {} bytes) for column {} (analyzed {}) at {} SSTable.",
0:                             term.remaining(),
0:                             OnDiskIndexBuilder.MAX_TERM_SIZE,
1:                             columnIndex.getColumnName(),
1:                             columnIndex.getMode().isAnalyzed,
1:                             descriptor);
1:                     continue;
1:                 }
1: 
1:                 if (!TypeUtil.isValid(token, columnIndex.getValidator()))
1:                 {
1:                     if ((token = TypeUtil.tryUpcast(token, columnIndex.getValidator())) == null)
1:                     {
0:                         logger.info("({}) Failed to add {} to index for key: {}, value size was {} bytes, validator is {}.",
1:                                     outputFile,
1:                                     columnIndex.getColumnName(),
1:                                     keyValidator.getString(key.getKey()),
0:                                     size,
1:                                     columnIndex.getValidator());
1:                         continue;
1:                     }
1:                 }
1: 
0:                 currentBuilder.add(token, key, keyPosition);
1:                 isAdded = true;
1:             }
1: 
1:             if (!isAdded || currentBuilder.estimatedMemoryUse() < maxMemorySize)
1:                 return; // non of the generated tokens were added to the index or memory size wasn't reached
1: 
1:             segments.add(getExecutor().submit(scheduleSegmentFlush(false)));
1:         }
1: 
1:         @VisibleForTesting
1:         protected Callable<OnDiskIndex> scheduleSegmentFlush(final boolean isFinal)
1:         {
1:             final OnDiskIndexBuilder builder = currentBuilder;
1:             currentBuilder = newIndexBuilder();
1: 
1:             final String segmentFile = filename(isFinal);
1: 
1:             return () -> {
1:                 long start1 = System.nanoTime();
1: 
1:                 try
1:                 {
1:                     File index = new File(segmentFile);
1:                     return builder.finish(index) ? new OnDiskIndex(index, columnIndex.getValidator(), null) : null;
1:                 }
1:                 finally
1:                 {
1:                     if (!isFinal)
0:                         logger.info("Flushed index segment {}, took {} ms.", segmentFile, TimeUnit.NANOSECONDS.toMillis(System.nanoTime() - start1));
1:                 }
1:             };
1:         }
1: 
1:         public void complete(final CountDownLatch latch)
1:         {
1:             logger.info("Scheduling index flush to {}", outputFile);
1: 
1:             getExecutor().submit((Runnable) () -> {
1:                 long start1 = System.nanoTime();
1: 
1:                 OnDiskIndex[] parts = new OnDiskIndex[segments.size() + 1];
1: 
1:                 try
1:                 {
1:                     // no parts present, build entire index from memory
1:                     if (segments.isEmpty())
1:                     {
1:                         scheduleSegmentFlush(true).call();
1:                         return;
1:                     }
1: 
1:                     // parts are present but there is something still in memory, let's flush that inline
1:                     if (!currentBuilder.isEmpty())
1:                     {
1:                         OnDiskIndex last = scheduleSegmentFlush(false).call();
1:                         segments.add(Futures.immediateFuture(last));
1:                     }
1: 
1:                     int index = 0;
1:                     ByteBuffer combinedMin = null, combinedMax = null;
1: 
1:                     for (Future<OnDiskIndex> f : segments)
1:                     {
0:                         OnDiskIndex part = Futures.getUnchecked(f);
1:                         if (part == null)
1:                             continue;
1: 
1:                         parts[index++] = part;
1:                         combinedMin = (combinedMin == null || keyValidator.compare(combinedMin, part.minKey()) > 0) ? part.minKey() : combinedMin;
1:                         combinedMax = (combinedMax == null || keyValidator.compare(combinedMax, part.maxKey()) < 0) ? part.maxKey() : combinedMax;
1:                     }
1: 
1:                     OnDiskIndexBuilder builder = newIndexBuilder();
1:                     builder.finish(Pair.create(combinedMin, combinedMax),
1:                                    new File(outputFile),
1:                                    new CombinedTermIterator(parts));
1:                 }
0:                 catch (Exception e)
1:                 {
1:                     logger.error("Failed to flush index {}.", outputFile, e);
1:                     FileUtils.delete(outputFile);
1:                 }
1:                 finally
1:                 {
1:                     logger.info("Index flush to {} took {} ms.", outputFile, TimeUnit.NANOSECONDS.toMillis(System.nanoTime() - start1));
1: 
0:                     for (OnDiskIndex part : parts)
1:                     {
1:                         if (part == null)
1:                             continue;
1: 
1:                         FileUtils.closeQuietly(part);
0:                         FileUtils.delete(part.getIndexPath());
1:                     }
1: 
1:                     latch.countDown();
1:                 }
1:             });
1:         }
1: 
1:         private ExecutorService getExecutor()
1:         {
1:             return source == OperationType.FLUSH ? INDEX_FLUSHER_MEMTABLE : INDEX_FLUSHER_GENERAL;
1:         }
1: 
1:         private OnDiskIndexBuilder newIndexBuilder()
1:         {
1:             return new OnDiskIndexBuilder(keyValidator, columnIndex.getValidator(), columnIndex.getMode().mode);
1:         }
1: 
1:         public String filename(boolean isFinal)
1:         {
1:             return outputFile + (isFinal ? "" : "_" + segmentNumber++);
1:         }
1:     }
1: 
1:     protected long maxMemorySize(ColumnIndex columnIndex)
1:     {
1:         // 1G for memtable and configuration for compaction
1:         return source == OperationType.FLUSH ? 1073741824L : columnIndex.getMode().maxCompactionFlushMemoryInMb;
1:     }
1: 
1:     public int hashCode()
1:     {
1:         return descriptor.hashCode();
1:     }
1: 
1:     public boolean equals(Object o)
1:     {
1:         return !(o == null || !(o instanceof PerSSTableIndexWriter)) && descriptor.equals(((PerSSTableIndexWriter) o).descriptor);
1:     }
1: }
author:Jordan West
-------------------------------------------------------------------------------
commit:5c4d5c7
/////////////////////////////////////////////////////////////////////////
1: import org.apache.cassandra.io.FSError;
/////////////////////////////////////////////////////////////////////////
1:                 indexes.put(column, (index = newIndex(columnIndex)));
/////////////////////////////////////////////////////////////////////////
1:     protected Index newIndex(ColumnIndex columnIndex)
1:     {
1:         return new Index(columnIndex);
1:     }
1: 
1:     @VisibleForTesting
1:         @VisibleForTesting
1:         protected final String outputFile;
1: 
/////////////////////////////////////////////////////////////////////////
1:                 long start = System.nanoTime();
1:                 catch (Exception | FSError e)
1:                 {
1:                     logger.error("Failed to build index segment {}", segmentFile, e);
1:                     return null;
1:                 }
1:                         logger.info("Flushed index segment {}, took {} ms.", segmentFile, TimeUnit.NANOSECONDS.toMillis(System.nanoTime() - start));
/////////////////////////////////////////////////////////////////////////
1:                         OnDiskIndex part = f.get();
/////////////////////////////////////////////////////////////////////////
1:                 catch (Exception | FSError e)
/////////////////////////////////////////////////////////////////////////
1:                     for (int segment = 0; segment < segmentNumber; segment++)
1:                         OnDiskIndex part = parts[segment];
1:                         if (part != null)
0:                             FileUtils.closeQuietly(part);
1: 
1:                         FileUtils.delete(outputFile + "_" + segment);
author:Jason Brown
-------------------------------------------------------------------------------
commit:733d1ee
/////////////////////////////////////////////////////////////////////////
1:                         @SuppressWarnings("resource")
/////////////////////////////////////////////////////////////////////////
0:                         @SuppressWarnings("resource")
============================================================================