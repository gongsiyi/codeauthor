1:72790dc: /*
1:72790dc:  * Licensed to the Apache Software Foundation (ASF) under one
1:72790dc:  * or more contributor license agreements.  See the NOTICE file
1:72790dc:  * distributed with this work for additional information
1:72790dc:  * regarding copyright ownership.  The ASF licenses this file
1:72790dc:  * to you under the Apache License, Version 2.0 (the
1:72790dc:  * "License"); you may not use this file except in compliance
1:72790dc:  * with the License.  You may obtain a copy of the License at
1:72790dc:  *
1:72790dc:  *     http://www.apache.org/licenses/LICENSE-2.0
1:72790dc:  *
1:72790dc:  * Unless required by applicable law or agreed to in writing, software
1:72790dc:  * distributed under the License is distributed on an "AS IS" BASIS,
1:72790dc:  * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
1:72790dc:  * See the License for the specific language governing permissions and
1:72790dc:  * limitations under the License.
1:72790dc:  */
1:72790dc: package org.apache.cassandra.index.sasi.disk;
137:72790dc: 
1:72790dc: import java.io.IOException;
1:72790dc: import java.util.*;
1:72790dc: 
1:7d857b4: import org.apache.cassandra.io.util.*;
1:7d857b4: import org.apache.cassandra.utils.*;
1:7d857b4: import org.apache.cassandra.utils.obs.BitUtil;
1:72790dc: 
1:7d857b4: public interface TokenTreeBuilder extends Iterable<Pair<Long, KeyOffsets>>
105:72790dc: {
1:7d857b4:     final static int BLOCK_BYTES = 4096;
1:7d857b4: 
1:7d857b4:     final static int LEAF_ENTRY_TYPE_BYTES = Short.BYTES;
1:7d857b4:     final static int TOKEN_OFFSET_BYTES = LEAF_ENTRY_TYPE_BYTES;
1:7d857b4:     final static int LEAF_PARTITON_OFFSET_BYTES = Long.BYTES;
1:7d857b4:     final static int LEAF_ROW_OFFSET_BYTES = Long.BYTES;
1:7d857b4: 
1:7d857b4:     final static int LEAF_PARTITON_OFFSET_PACKED_BYTES = Integer.BYTES;
1:7d857b4:     final static int LEAF_ROW_OFFSET_PACKED_BYTES = Integer.BYTES;
1:7d857b4:     final static int COLLISION_ENTRY_BYTES = LEAF_PARTITON_OFFSET_BYTES + LEAF_ROW_OFFSET_BYTES;
1:7d857b4: 
1:7d857b4:     final static int HEADER_INFO_BYTE_BYTES = Byte.BYTES;
1:7d857b4:     final static int HEADER_TOKEN_COUNT_BYTES = Short.BYTES;
1:7d857b4: 
1:7d857b4:     final static int ROOT_HEADER_MAGIC_SIZE = Short.BYTES;
1:7d857b4:     final static int ROOT_HEADER_TOKEN_COUNT_SIZE = Long.BYTES;
1:7d857b4: 
1:7d857b4:     // Partitioner token size in bytes
1:7d857b4:     final static int TOKEN_BYTES = Long.BYTES;
1:7d857b4: 
1:7d857b4:     // Leaf entry size in bytes, see {@class SimpleLeafEntry} for a full description
1:7d857b4:     final static int LEAF_ENTRY_BYTES = LEAF_ENTRY_TYPE_BYTES + TOKEN_BYTES + LEAF_PARTITON_OFFSET_BYTES + LEAF_ROW_OFFSET_BYTES;
1:7d857b4:     // Shared header size in bytes, see {@class AbstractTreeBuilder$Header} for a full description
1:7d857b4:     final static int SHARED_HEADER_BYTES = HEADER_INFO_BYTE_BYTES + HEADER_TOKEN_COUNT_BYTES + 2 * TOKEN_BYTES;
1:7d857b4:     // Block header size in bytes, see {@class AbstractTreeBuilder$RootHeader}
1:7d857b4:     final static int BLOCK_HEADER_BYTES = BitUtil.nextHighestPowerOfTwo(SHARED_HEADER_BYTES + ROOT_HEADER_MAGIC_SIZE + ROOT_HEADER_TOKEN_COUNT_SIZE + 2 * TOKEN_BYTES);
1:7d857b4: 
1:7d857b4:     // Overflow trailer capacity is currently 8 overflow items. Each overflow item consists of two longs.
1:7d857b4:     final static int OVERFLOW_TRAILER_CAPACITY = 8;
1:7d857b4:     final static int OVERFLOW_TRAILER_BYTES = OVERFLOW_TRAILER_CAPACITY * COLLISION_ENTRY_BYTES;;
1:7d857b4:     final static int TOKENS_PER_BLOCK = (TokenTreeBuilder.BLOCK_BYTES - BLOCK_HEADER_BYTES - OVERFLOW_TRAILER_BYTES) / LEAF_ENTRY_BYTES;
1:7d857b4: 
1:7d857b4:     final static int LEGACY_LEAF_ENTRY_BYTES = Short.BYTES + Short.BYTES + TOKEN_BYTES + Integer.BYTES;
1:7d857b4:     final static int LEGACY_TOKEN_OFFSET_BYTES = 2 * Short.BYTES;
1:7d857b4:     final static byte LAST_LEAF_SHIFT = 1;
1:7d857b4: 
1:7d857b4:     /**
1:7d857b4:      * {@code Header} size in bytes.
1:7d857b4:      */
1:7d857b4:     final byte ENTRY_TYPE_MASK = 0x03;
1:7d857b4:     final short AB_MAGIC = 0x5A51;
1:7d857b4:     final short AC_MAGIC = 0x7C63;
1:5c4d5c7: 
1:72790dc:     // note: ordinal positions are used here, do not change order
1:72790dc:     enum EntryType
1:72790dc:     {
1:7d857b4:         SIMPLE,
1:7d857b4:         FACTORED,
1:7d857b4:         PACKED,
1:7d857b4:         OVERFLOW;
1:72790dc: 
1:72790dc:         public static EntryType of(int ordinal)
1:72790dc:         {
1:72790dc:             if (ordinal == SIMPLE.ordinal())
1:72790dc:                 return SIMPLE;
1:72790dc: 
1:72790dc:             if (ordinal == FACTORED.ordinal())
1:72790dc:                 return FACTORED;
1:72790dc: 
1:72790dc:             if (ordinal == PACKED.ordinal())
1:72790dc:                 return PACKED;
1:72790dc: 
1:72790dc:             if (ordinal == OVERFLOW.ordinal())
1:72790dc:                 return OVERFLOW;
1:72790dc: 
1:72790dc:             throw new IllegalArgumentException("Unknown ordinal: " + ordinal);
107:72790dc:         }
1:72790dc:     }
1:72790dc: 
1:7d857b4:     void add(Long token, long partitionOffset, long rowOffset);
1:7d857b4:     void add(SortedMap<Long, KeyOffsets> data);
1:7d857b4:     void add(Iterator<Pair<Long, KeyOffsets>> data);
1:5c4d5c7:     void add(TokenTreeBuilder ttb);
1:72790dc: 
1:5c4d5c7:     boolean isEmpty();
1:5c4d5c7:     long getTokenCount();
1:72790dc: 
1:5c4d5c7:     TokenTreeBuilder finish();
1:72790dc: 
1:5c4d5c7:     int serializedSize();
1:5c4d5c7:     void write(DataOutputPlus out) throws IOException;
1:72790dc: }
============================================================================
author:Alex Petrov
-------------------------------------------------------------------------------
commit:7d857b4
/////////////////////////////////////////////////////////////////////////
1: import org.apache.cassandra.io.util.*;
1: import org.apache.cassandra.utils.*;
1: import org.apache.cassandra.utils.obs.BitUtil;
1: public interface TokenTreeBuilder extends Iterable<Pair<Long, KeyOffsets>>
1:     final static int BLOCK_BYTES = 4096;
1: 
1:     final static int LEAF_ENTRY_TYPE_BYTES = Short.BYTES;
1:     final static int TOKEN_OFFSET_BYTES = LEAF_ENTRY_TYPE_BYTES;
1:     final static int LEAF_PARTITON_OFFSET_BYTES = Long.BYTES;
1:     final static int LEAF_ROW_OFFSET_BYTES = Long.BYTES;
1: 
1:     final static int LEAF_PARTITON_OFFSET_PACKED_BYTES = Integer.BYTES;
1:     final static int LEAF_ROW_OFFSET_PACKED_BYTES = Integer.BYTES;
1:     final static int COLLISION_ENTRY_BYTES = LEAF_PARTITON_OFFSET_BYTES + LEAF_ROW_OFFSET_BYTES;
1: 
1:     final static int HEADER_INFO_BYTE_BYTES = Byte.BYTES;
1:     final static int HEADER_TOKEN_COUNT_BYTES = Short.BYTES;
1: 
1:     final static int ROOT_HEADER_MAGIC_SIZE = Short.BYTES;
1:     final static int ROOT_HEADER_TOKEN_COUNT_SIZE = Long.BYTES;
1: 
1:     // Partitioner token size in bytes
1:     final static int TOKEN_BYTES = Long.BYTES;
1: 
1:     // Leaf entry size in bytes, see {@class SimpleLeafEntry} for a full description
1:     final static int LEAF_ENTRY_BYTES = LEAF_ENTRY_TYPE_BYTES + TOKEN_BYTES + LEAF_PARTITON_OFFSET_BYTES + LEAF_ROW_OFFSET_BYTES;
1:     // Shared header size in bytes, see {@class AbstractTreeBuilder$Header} for a full description
1:     final static int SHARED_HEADER_BYTES = HEADER_INFO_BYTE_BYTES + HEADER_TOKEN_COUNT_BYTES + 2 * TOKEN_BYTES;
1:     // Block header size in bytes, see {@class AbstractTreeBuilder$RootHeader}
1:     final static int BLOCK_HEADER_BYTES = BitUtil.nextHighestPowerOfTwo(SHARED_HEADER_BYTES + ROOT_HEADER_MAGIC_SIZE + ROOT_HEADER_TOKEN_COUNT_SIZE + 2 * TOKEN_BYTES);
1: 
1:     // Overflow trailer capacity is currently 8 overflow items. Each overflow item consists of two longs.
1:     final static int OVERFLOW_TRAILER_CAPACITY = 8;
1:     final static int OVERFLOW_TRAILER_BYTES = OVERFLOW_TRAILER_CAPACITY * COLLISION_ENTRY_BYTES;;
1:     final static int TOKENS_PER_BLOCK = (TokenTreeBuilder.BLOCK_BYTES - BLOCK_HEADER_BYTES - OVERFLOW_TRAILER_BYTES) / LEAF_ENTRY_BYTES;
1: 
1:     final static int LEGACY_LEAF_ENTRY_BYTES = Short.BYTES + Short.BYTES + TOKEN_BYTES + Integer.BYTES;
1:     final static int LEGACY_TOKEN_OFFSET_BYTES = 2 * Short.BYTES;
1:     final static byte LAST_LEAF_SHIFT = 1;
1: 
1:     /**
1:      * {@code Header} size in bytes.
1:      */
1:     final byte ENTRY_TYPE_MASK = 0x03;
1:     final short AB_MAGIC = 0x5A51;
1:     final short AC_MAGIC = 0x7C63;
1:         SIMPLE,
1:         FACTORED,
1:         PACKED,
1:         OVERFLOW;
/////////////////////////////////////////////////////////////////////////
1:     void add(Long token, long partitionOffset, long rowOffset);
1:     void add(SortedMap<Long, KeyOffsets> data);
1:     void add(Iterator<Pair<Long, KeyOffsets>> data);
author:Jordan West
-------------------------------------------------------------------------------
commit:5c4d5c7
/////////////////////////////////////////////////////////////////////////
0: public interface TokenTreeBuilder extends Iterable<Pair<Long, LongSet>>
0:     int BLOCK_BYTES = 4096;
0:     int BLOCK_HEADER_BYTES = 64;
0:     int OVERFLOW_TRAILER_BYTES = 64;
0:     int OVERFLOW_TRAILER_CAPACITY = OVERFLOW_TRAILER_BYTES / 8;
0:     int TOKENS_PER_BLOCK = (BLOCK_BYTES - BLOCK_HEADER_BYTES - OVERFLOW_TRAILER_BYTES) / 16;
0:     long MAX_OFFSET = (1L << 47) - 1; // 48 bits for (signed) offset
0:     byte LAST_LEAF_SHIFT = 1;
0:     byte SHARED_HEADER_BYTES = 19;
0:     byte ENTRY_TYPE_MASK = 0x03;
0:     short AB_MAGIC = 0x5A51;
1: 
/////////////////////////////////////////////////////////////////////////
0:     void add(Long token, long keyPosition);
0:     void add(SortedMap<Long, LongSet> data);
0:     void add(Iterator<Pair<Long, LongSet>> data);
1:     void add(TokenTreeBuilder ttb);
1:     boolean isEmpty();
1:     long getTokenCount();
1:     TokenTreeBuilder finish();
1:     int serializedSize();
1:     void write(DataOutputPlus out) throws IOException;
author:Pavel Yaskevich
-------------------------------------------------------------------------------
commit:72790dc
/////////////////////////////////////////////////////////////////////////
1: /*
1:  * Licensed to the Apache Software Foundation (ASF) under one
1:  * or more contributor license agreements.  See the NOTICE file
1:  * distributed with this work for additional information
1:  * regarding copyright ownership.  The ASF licenses this file
1:  * to you under the Apache License, Version 2.0 (the
1:  * "License"); you may not use this file except in compliance
1:  * with the License.  You may obtain a copy of the License at
1:  *
1:  *     http://www.apache.org/licenses/LICENSE-2.0
1:  *
1:  * Unless required by applicable law or agreed to in writing, software
1:  * distributed under the License is distributed on an "AS IS" BASIS,
1:  * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
1:  * See the License for the specific language governing permissions and
1:  * limitations under the License.
1:  */
1: package org.apache.cassandra.index.sasi.disk;
1: 
1: import java.io.IOException;
0: import java.nio.ByteBuffer;
1: import java.util.*;
1: 
0: import org.apache.cassandra.io.util.DataOutputPlus;
0: import org.apache.cassandra.utils.FBUtilities;
0: import org.apache.cassandra.utils.Pair;
1: 
0: import com.carrotsearch.hppc.LongArrayList;
0: import com.carrotsearch.hppc.LongSet;
0: import com.carrotsearch.hppc.cursors.LongCursor;
0: import com.carrotsearch.hppc.LongOpenHashSet;
0: import com.google.common.collect.AbstractIterator;
1: 
0: public class TokenTreeBuilder
1: {
1:     // note: ordinal positions are used here, do not change order
1:     enum EntryType
1:     {
0:         SIMPLE, FACTORED, PACKED, OVERFLOW;
1: 
1:         public static EntryType of(int ordinal)
1:         {
1:             if (ordinal == SIMPLE.ordinal())
1:                 return SIMPLE;
1: 
1:             if (ordinal == FACTORED.ordinal())
1:                 return FACTORED;
1: 
1:             if (ordinal == PACKED.ordinal())
1:                 return PACKED;
1: 
1:             if (ordinal == OVERFLOW.ordinal())
1:                 return OVERFLOW;
1: 
1:             throw new IllegalArgumentException("Unknown ordinal: " + ordinal);
1:         }
1:     }
1: 
0:     public static final int BLOCK_BYTES = 4096;
0:     public static final int BLOCK_HEADER_BYTES = 64;
0:     public static final int OVERFLOW_TRAILER_BYTES = 64;
0:     public static final int OVERFLOW_TRAILER_CAPACITY = OVERFLOW_TRAILER_BYTES / 8;
0:     public static final int TOKENS_PER_BLOCK = (BLOCK_BYTES - BLOCK_HEADER_BYTES - OVERFLOW_TRAILER_BYTES) / 16;
0:     public static final long MAX_OFFSET = (1L << 47) - 1; // 48 bits for (signed) offset
0:     public static final byte LAST_LEAF_SHIFT = 1;
0:     public static final byte SHARED_HEADER_BYTES = 19;
0:     public static final byte ENTRY_TYPE_MASK = 0x03;
0:     public static final short AB_MAGIC = 0x5A51;
1: 
0:     private final SortedMap<Long, LongSet> tokens = new TreeMap<>();
0:     private int numBlocks;
1: 
0:     private Node root;
0:     private InteriorNode rightmostParent;
0:     private Leaf leftmostLeaf;
0:     private Leaf rightmostLeaf;
0:     private long tokenCount = 0;
0:     private long treeMinToken;
0:     private long treeMaxToken;
1: 
0:     public TokenTreeBuilder()
0:     {}
1: 
0:     public TokenTreeBuilder(SortedMap<Long, LongSet> data)
1:     {
0:         add(data);
1:     }
1: 
0:     public void add(Long token, long keyPosition)
1:     {
0:         LongSet found = tokens.get(token);
0:         if (found == null)
0:             tokens.put(token, (found = new LongOpenHashSet(2)));
1: 
0:         found.add(keyPosition);
1:     }
1: 
0:     public void add(SortedMap<Long, LongSet> data)
1:     {
0:         for (Map.Entry<Long, LongSet> newEntry : data.entrySet())
1:         {
0:             LongSet found = tokens.get(newEntry.getKey());
0:             if (found == null)
0:                 tokens.put(newEntry.getKey(), (found = new LongOpenHashSet(4)));
1: 
0:             for (LongCursor offset : newEntry.getValue())
0:                 found.add(offset.value);
1:         }
1:     }
1: 
0:     public TokenTreeBuilder finish()
1:     {
0:         maybeBulkLoad();
0:         return this;
1:     }
1: 
0:     public SortedMap<Long, LongSet> getTokens()
1:     {
0:         return tokens;
1:     }
1: 
0:     public long getTokenCount()
1:     {
0:         return tokenCount;
1:     }
1: 
0:     public int serializedSize()
1:     {
0:         if (numBlocks == 1)
0:             return (BLOCK_HEADER_BYTES + ((int) tokenCount * 16));
0:         else
0:             return numBlocks * BLOCK_BYTES;
1:     }
1: 
0:     public void write(DataOutputPlus out) throws IOException
1:     {
0:         ByteBuffer blockBuffer = ByteBuffer.allocate(BLOCK_BYTES);
0:         Iterator<Node> levelIterator = root.levelIterator();
0:         long childBlockIndex = 1;
1: 
0:         while (levelIterator != null)
1:         {
1: 
0:             Node firstChild = null;
0:             while (levelIterator.hasNext())
1:             {
0:                 Node block = levelIterator.next();
1: 
0:                 if (firstChild == null && !block.isLeaf())
0:                     firstChild = ((InteriorNode) block).children.get(0);
1: 
0:                 block.serialize(childBlockIndex, blockBuffer);
0:                 flushBuffer(blockBuffer, out, numBlocks != 1);
1: 
0:                 childBlockIndex += block.childCount();
1:             }
1: 
0:             levelIterator = (firstChild == null) ? null : firstChild.levelIterator();
1:         }
1:     }
1: 
0:     public Iterator<Pair<Long, LongSet>> iterator()
1:     {
0:         return new TokenIterator(leftmostLeaf.levelIterator());
1:     }
1: 
0:     private void maybeBulkLoad()
1:     {
0:         if (root == null)
0:             bulkLoad();
1:     }
1: 
0:     private void flushBuffer(ByteBuffer buffer, DataOutputPlus o, boolean align) throws IOException
1:     {
0:         // seek to end of last block before flushing
0:         if (align)
0:             alignBuffer(buffer, BLOCK_BYTES);
1: 
0:         buffer.flip();
0:         o.write(buffer);
0:         buffer.clear();
1:     }
1: 
0:     private static void alignBuffer(ByteBuffer buffer, int blockSize)
1:     {
0:         long curPos = buffer.position();
0:         if ((curPos & (blockSize - 1)) != 0) // align on the block boundary if needed
0:             buffer.position((int) FBUtilities.align(curPos, blockSize));
1:     }
1: 
0:     private void bulkLoad()
1:     {
0:         tokenCount = tokens.size();
0:         treeMinToken = tokens.firstKey();
0:         treeMaxToken = tokens.lastKey();
0:         numBlocks = 1;
1: 
0:         // special case the tree that only has a single block in it (so we don't create a useless root)
0:         if (tokenCount <= TOKENS_PER_BLOCK)
1:         {
0:             leftmostLeaf = new Leaf(tokens);
0:             rightmostLeaf = leftmostLeaf;
0:             root = leftmostLeaf;
1:         }
0:         else
1:         {
0:             root = new InteriorNode();
0:             rightmostParent = (InteriorNode) root;
1: 
0:             int i = 0;
0:             Leaf lastLeaf = null;
0:             Long firstToken = tokens.firstKey();
0:             Long finalToken = tokens.lastKey();
0:             Long lastToken;
0:             for (Long token : tokens.keySet())
1:             {
0:                 if (i == 0 || (i % TOKENS_PER_BLOCK != 0 && i != (tokenCount - 1)))
1:                 {
0:                     i++;
0:                     continue;
1:                 }
1: 
0:                 lastToken = token;
0:                 Leaf leaf = (i != (tokenCount - 1) || token.equals(finalToken)) ?
0:                         new Leaf(tokens.subMap(firstToken, lastToken)) : new Leaf(tokens.tailMap(firstToken));
1: 
0:                 if (i == TOKENS_PER_BLOCK)
0:                     leftmostLeaf = leaf;
0:                 else
0:                     lastLeaf.next = leaf;
1: 
0:                 rightmostParent.add(leaf);
0:                 lastLeaf = leaf;
0:                 rightmostLeaf = leaf;
0:                 firstToken = lastToken;
0:                 i++;
0:                 numBlocks++;
1: 
0:                 if (token.equals(finalToken))
1:                 {
0:                     Leaf finalLeaf = new Leaf(tokens.tailMap(token));
0:                     lastLeaf.next = finalLeaf;
0:                     rightmostParent.add(finalLeaf);
0:                     rightmostLeaf = finalLeaf;
0:                     numBlocks++;
1:                 }
1:             }
1: 
1:         }
1:     }
1: 
0:     private abstract class Node
1:     {
0:         protected InteriorNode parent;
0:         protected Node next;
0:         protected Long nodeMinToken, nodeMaxToken;
1: 
0:         public abstract void serialize(long childBlockIndex, ByteBuffer buf);
0:         public abstract int childCount();
0:         public abstract int tokenCount();
0:         public abstract Long smallestToken();
1: 
0:         public Iterator<Node> levelIterator()
1:         {
0:             return new LevelIterator(this);
1:         }
1: 
0:         public boolean isLeaf()
1:         {
0:             return (this instanceof Leaf);
1:         }
1: 
0:         protected boolean isLastLeaf()
1:         {
0:             return this == rightmostLeaf;
1:         }
1: 
0:         protected boolean isRoot()
1:         {
0:             return this == root;
1:         }
1: 
0:         protected void updateTokenRange(long token)
1:         {
0:             nodeMinToken = nodeMinToken == null ? token : Math.min(nodeMinToken, token);
0:             nodeMaxToken = nodeMaxToken == null ? token : Math.max(nodeMaxToken, token);
1:         }
1: 
0:         protected void serializeHeader(ByteBuffer buf)
1:         {
0:             Header header;
0:             if (isRoot())
0:                 header = new RootHeader();
0:             else if (!isLeaf())
0:                 header = new InteriorNodeHeader();
0:             else
0:                 header = new LeafHeader();
1: 
0:             header.serialize(buf);
0:             alignBuffer(buf, BLOCK_HEADER_BYTES);
1:         }
1: 
0:         private abstract class Header
1:         {
0:             public void serialize(ByteBuffer buf)
1:             {
0:                 buf.put(infoByte())
0:                         .putShort((short) (tokenCount()))
0:                         .putLong(nodeMinToken)
0:                         .putLong(nodeMaxToken);
1:             }
1: 
0:             protected abstract byte infoByte();
1:         }
1: 
0:         private class RootHeader extends Header
1:         {
0:             public void serialize(ByteBuffer buf)
1:             {
0:                 super.serialize(buf);
0:                 writeMagic(buf);
0:                 buf.putLong(tokenCount)
0:                         .putLong(treeMinToken)
0:                         .putLong(treeMaxToken);
1:             }
1: 
0:             protected byte infoByte()
1:             {
0:                 // if leaf, set leaf indicator and last leaf indicator (bits 0 & 1)
0:                 // if not leaf, clear both bits
0:                 return (byte) ((isLeaf()) ? 3 : 0);
1:             }
1: 
0:             protected void writeMagic(ByteBuffer buf)
1:             {
0:                 switch (Descriptor.CURRENT_VERSION)
1:                 {
0:                     case Descriptor.VERSION_AB:
0:                         buf.putShort(AB_MAGIC);
0:                         break;
0:                     default:
0:                         break;
1:                 }
1: 
1:             }
1:         }
1: 
0:         private class InteriorNodeHeader extends Header
1:         {
0:             // bit 0 (leaf indicator) & bit 1 (last leaf indicator) cleared
0:             protected byte infoByte()
1:             {
0:                 return 0;
1:             }
1:         }
1: 
0:         private class LeafHeader extends Header
1:         {
0:             // bit 0 set as leaf indicator
0:             // bit 1 set if this is last leaf of data
0:             protected byte infoByte()
1:             {
0:                 byte infoByte = 1;
0:                 infoByte |= (isLastLeaf()) ? (1 << LAST_LEAF_SHIFT) : 0;
1: 
0:                 return infoByte;
1:             }
1:         }
1: 
1:     }
1: 
0:     private class Leaf extends Node
1:     {
0:         private final SortedMap<Long, LongSet> tokens;
0:         private LongArrayList overflowCollisions;
1: 
0:         Leaf(SortedMap<Long, LongSet> data)
1:         {
0:             nodeMinToken = data.firstKey();
0:             nodeMaxToken = data.lastKey();
0:             tokens = data;
1:         }
1: 
0:         public Long largestToken()
1:         {
0:             return nodeMaxToken;
1:         }
1: 
0:         public void serialize(long childBlockIndex, ByteBuffer buf)
1:         {
0:             serializeHeader(buf);
0:             serializeData(buf);
0:             serializeOverflowCollisions(buf);
1:         }
1: 
0:         public int childCount()
1:         {
0:             return 0;
1:         }
1: 
0:         public int tokenCount()
1:         {
0:             return tokens.size();
1:         }
1: 
0:         public Long smallestToken()
1:         {
0:             return nodeMinToken;
1:         }
1: 
0:         public Iterator<Map.Entry<Long, LongSet>> tokenIterator()
1:         {
0:             return tokens.entrySet().iterator();
1:         }
1: 
0:         private void serializeData(ByteBuffer buf)
1:         {
0:             for (Map.Entry<Long, LongSet> entry : tokens.entrySet())
0:                 createEntry(entry.getKey(), entry.getValue()).serialize(buf);
1:         }
1: 
0:         private void serializeOverflowCollisions(ByteBuffer buf)
1:         {
0:             if (overflowCollisions != null)
0:                 for (LongCursor offset : overflowCollisions)
0:                     buf.putLong(offset.value);
1:         }
1: 
1: 
0:         private LeafEntry createEntry(final long tok, final LongSet offsets)
1:         {
0:             int offsetCount = offsets.size();
0:             switch (offsetCount)
1:             {
0:                 case 0:
0:                     throw new AssertionError("no offsets for token " + tok);
0:                 case 1:
0:                     long offset = offsets.toArray()[0];
0:                     if (offset > MAX_OFFSET)
0:                         throw new AssertionError("offset " + offset + " cannot be greater than " + MAX_OFFSET);
0:                     else if (offset <= Integer.MAX_VALUE)
0:                         return new SimpleLeafEntry(tok, offset);
0:                     else
0:                         return new FactoredOffsetLeafEntry(tok, offset);
0:                 case 2:
0:                     long[] rawOffsets = offsets.toArray();
0:                     if (rawOffsets[0] <= Integer.MAX_VALUE && rawOffsets[1] <= Integer.MAX_VALUE &&
0:                             (rawOffsets[0] <= Short.MAX_VALUE || rawOffsets[1] <= Short.MAX_VALUE))
0:                         return new PackedCollisionLeafEntry(tok, rawOffsets);
0:                     else
0:                         return createOverflowEntry(tok, offsetCount, offsets);
0:                 default:
0:                     return createOverflowEntry(tok, offsetCount, offsets);
1:             }
1:         }
1: 
0:         private LeafEntry createOverflowEntry(final long tok, final int offsetCount, final LongSet offsets)
1:         {
0:             if (overflowCollisions == null)
0:                 overflowCollisions = new LongArrayList();
1: 
0:             LeafEntry entry = new OverflowCollisionLeafEntry(tok, (short) overflowCollisions.size(), (short) offsetCount);
0:             for (LongCursor o : offsets) {
0:                 if (overflowCollisions.size() == OVERFLOW_TRAILER_CAPACITY)
0:                     throw new AssertionError("cannot have more than " + OVERFLOW_TRAILER_CAPACITY + " overflow collisions per leaf");
0:                 else
0:                     overflowCollisions.add(o.value);
1:             }
0:             return entry;
1:         }
1: 
0:         private abstract class LeafEntry
1:         {
0:             protected final long token;
1: 
0:             abstract public EntryType type();
0:             abstract public int offsetData();
0:             abstract public short offsetExtra();
1: 
0:             public LeafEntry(final long tok)
1:             {
0:                 token = tok;
1:             }
1: 
0:             public void serialize(ByteBuffer buf)
1:             {
0:                 buf.putShort((short) type().ordinal())
0:                         .putShort(offsetExtra())
0:                         .putLong(token)
0:                         .putInt(offsetData());
1:             }
1: 
1:         }
1: 
1: 
0:         // assumes there is a single offset and the offset is <= Integer.MAX_VALUE
0:         private class SimpleLeafEntry extends LeafEntry
1:         {
0:             private final long offset;
1: 
0:             public SimpleLeafEntry(final long tok, final long off)
1:             {
0:                 super(tok);
0:                 offset = off;
1:             }
1: 
0:             public EntryType type()
1:             {
0:                 return EntryType.SIMPLE;
1:             }
1: 
0:             public int offsetData()
1:             {
0:                 return (int) offset;
1:             }
1: 
0:             public short offsetExtra()
1:             {
0:                 return 0;
1:             }
1:         }
1: 
0:         // assumes there is a single offset and Integer.MAX_VALUE < offset <= MAX_OFFSET
0:         // take the middle 32 bits of offset (or the top 32 when considering offset is max 48 bits)
0:         // and store where offset is normally stored. take bottom 16 bits of offset and store in entry header
0:         private class FactoredOffsetLeafEntry extends LeafEntry
1:         {
0:             private final long offset;
1: 
0:             public FactoredOffsetLeafEntry(final long tok, final long off)
1:             {
0:                 super(tok);
0:                 offset = off;
1:             }
1: 
0:             public EntryType type()
1:             {
0:                 return EntryType.FACTORED;
1:             }
1: 
0:             public int offsetData()
1:             {
0:                 return (int) (offset >>> Short.SIZE);
1:             }
1: 
0:             public short offsetExtra()
1:             {
0:                 return (short) offset;
1:             }
1:         }
1: 
0:         // holds an entry with two offsets that can be packed in an int & a short
0:         // the int offset is stored where offset is normally stored. short offset is
0:         // stored in entry header
0:         private class PackedCollisionLeafEntry extends LeafEntry
1:         {
0:             private short smallerOffset;
0:             private int largerOffset;
1: 
0:             public PackedCollisionLeafEntry(final long tok, final long[] offs)
1:             {
0:                 super(tok);
1: 
0:                 smallerOffset = (short) Math.min(offs[0], offs[1]);
0:                 largerOffset = (int) Math.max(offs[0], offs[1]);
1:             }
1: 
0:             public EntryType type()
1:             {
0:                 return EntryType.PACKED;
1:             }
1: 
0:             public int offsetData()
1:             {
0:                 return largerOffset;
1:             }
1: 
0:             public short offsetExtra()
1:             {
0:                 return smallerOffset;
1:             }
1:         }
1: 
0:         // holds an entry with three or more offsets, or two offsets that cannot
0:         // be packed into an int & a short. the index into the overflow list
0:         // is stored where the offset is normally stored. the number of overflowed offsets
0:         // for the entry is stored in the entry header
0:         private class OverflowCollisionLeafEntry extends LeafEntry
1:         {
0:             private final short startIndex;
0:             private final short count;
1: 
0:             public OverflowCollisionLeafEntry(final long tok, final short collisionStartIndex, final short collisionCount)
1:             {
0:                 super(tok);
0:                 startIndex = collisionStartIndex;
0:                 count = collisionCount;
1:             }
1: 
0:             public EntryType type()
1:             {
0:                 return EntryType.OVERFLOW;
1:             }
1: 
0:             public int offsetData()
1:             {
0:                 return startIndex;
1:             }
1: 
0:             public short offsetExtra()
1:             {
0:                 return count;
1:             }
1: 
1:         }
1: 
1:     }
1: 
0:     private class InteriorNode extends Node
1:     {
0:         private List<Long> tokens = new ArrayList<>(TOKENS_PER_BLOCK);
0:         private List<Node> children = new ArrayList<>(TOKENS_PER_BLOCK + 1);
0:         private int position = 0; // TODO (jwest): can get rid of this and use array size
1: 
1: 
0:         public void serialize(long childBlockIndex, ByteBuffer buf)
1:         {
0:             serializeHeader(buf);
0:             serializeTokens(buf);
0:             serializeChildOffsets(childBlockIndex, buf);
1:         }
1: 
0:         public int childCount()
1:         {
0:             return children.size();
1:         }
1: 
0:         public int tokenCount()
1:         {
0:             return tokens.size();
1:         }
1: 
0:         public Long smallestToken()
1:         {
0:             return tokens.get(0);
1:         }
1: 
0:         protected void add(Long token, InteriorNode leftChild, InteriorNode rightChild)
1:         {
0:             int pos = tokens.size();
0:             if (pos == TOKENS_PER_BLOCK)
1:             {
0:                 InteriorNode sibling = split();
0:                 sibling.add(token, leftChild, rightChild);
1: 
1:             }
0:             else {
0:                 if (leftChild != null)
0:                     children.add(pos, leftChild);
1: 
0:                 if (rightChild != null)
1:                 {
0:                     children.add(pos + 1, rightChild);
0:                     rightChild.parent = this;
1:                 }
1: 
0:                 updateTokenRange(token);
0:                 tokens.add(pos, token);
1:             }
1:         }
1: 
0:         protected void add(Leaf node)
1:         {
1: 
0:             if (position == (TOKENS_PER_BLOCK + 1))
1:             {
0:                 rightmostParent = split();
0:                 rightmostParent.add(node);
1:             }
0:             else
1:             {
1: 
0:                 node.parent = this;
0:                 children.add(position, node);
0:                 position++;
1: 
0:                 // the first child is referenced only during bulk load. we don't take a value
0:                 // to store into the tree, one is subtracted since position has already been incremented
0:                 // for the next node to be added
0:                 if (position - 1 == 0)
0:                     return;
1: 
1: 
0:                 // tokens are inserted one behind the current position, but 2 is subtracted because
0:                 // position has already been incremented for the next add
0:                 Long smallestToken = node.smallestToken();
0:                 updateTokenRange(smallestToken);
0:                 tokens.add(position - 2, smallestToken);
1:             }
1: 
1:         }
1: 
0:         protected InteriorNode split()
1:         {
0:             Pair<Long, InteriorNode> splitResult = splitBlock();
0:             Long middleValue = splitResult.left;
0:             InteriorNode sibling = splitResult.right;
0:             InteriorNode leftChild = null;
1: 
0:             // create a new root if necessary
0:             if (parent == null)
1:             {
0:                 parent = new InteriorNode();
0:                 root = parent;
0:                 sibling.parent = parent;
0:                 leftChild = this;
0:                 numBlocks++;
1:             }
1: 
0:             parent.add(middleValue, leftChild, sibling);
1: 
0:             return sibling;
1:         }
1: 
0:         protected Pair<Long, InteriorNode> splitBlock()
1:         {
0:             final int splitPosition = TOKENS_PER_BLOCK - 2;
0:             InteriorNode sibling = new InteriorNode();
0:             sibling.parent = parent;
0:             next = sibling;
1: 
0:             Long middleValue = tokens.get(splitPosition);
1: 
0:             for (int i = splitPosition; i < TOKENS_PER_BLOCK; i++)
1:             {
0:                 if (i != TOKENS_PER_BLOCK && i != splitPosition)
1:                 {
0:                     long token = tokens.get(i);
0:                     sibling.updateTokenRange(token);
0:                     sibling.tokens.add(token);
1:                 }
1: 
0:                 Node child = children.get(i + 1);
0:                 child.parent = sibling;
0:                 sibling.children.add(child);
0:                 sibling.position++;
1:             }
1: 
0:             for (int i = TOKENS_PER_BLOCK; i >= splitPosition; i--)
1:             {
0:                 if (i != TOKENS_PER_BLOCK)
0:                     tokens.remove(i);
1: 
0:                 if (i != splitPosition)
0:                     children.remove(i);
1:             }
1: 
0:             nodeMinToken = smallestToken();
0:             nodeMaxToken = tokens.get(tokens.size() - 1);
0:             numBlocks++;
1: 
0:             return Pair.create(middleValue, sibling);
1:         }
1: 
0:         protected boolean isFull()
1:         {
0:             return (position >= TOKENS_PER_BLOCK + 1);
1:         }
1: 
0:         private void serializeTokens(ByteBuffer buf)
1:         {
0:             for (Long token : tokens)
0:                 buf.putLong(token);
1:         }
1: 
1: 
0:         private void serializeChildOffsets(long childBlockIndex, ByteBuffer buf)
1:         {
0:             for (int i = 0; i < children.size(); i++)
0:                 buf.putLong((childBlockIndex + i) * BLOCK_BYTES);
1:         }
1:     }
1: 
0:     public static class LevelIterator extends AbstractIterator<Node>
1:     {
0:         private Node currentNode;
1: 
0:         LevelIterator(Node first)
1:         {
0:             currentNode = first;
1:         }
1: 
0:         public Node computeNext()
1:         {
0:             if (currentNode == null)
0:                 return endOfData();
1: 
0:             Node returnNode = currentNode;
0:             currentNode = returnNode.next;
1: 
0:             return returnNode;
1:         }
1: 
1: 
1:     }
1: 
0:     public static class TokenIterator extends AbstractIterator<Pair<Long, LongSet>>
1:     {
0:         private Iterator<Node> levelIterator;
0:         private Iterator<Map.Entry<Long, LongSet>> currentIterator;
1: 
0:         TokenIterator(Iterator<Node> level)
1:         {
0:             levelIterator = level;
0:             if (levelIterator.hasNext())
0:                 currentIterator = ((Leaf) levelIterator.next()).tokenIterator();
1:         }
1: 
0:         public Pair<Long, LongSet> computeNext()
1:         {
0:             if (currentIterator != null && currentIterator.hasNext())
1:             {
0:                 Map.Entry<Long, LongSet> next = currentIterator.next();
0:                 return Pair.create(next.getKey(), next.getValue());
1:             }
0:             else
1:             {
0:                 if (!levelIterator.hasNext())
0:                     return endOfData();
0:                 else
1:                 {
0:                     currentIterator = ((Leaf) levelIterator.next()).tokenIterator();
0:                     return computeNext();
1:                 }
1:             }
1: 
1:         }
1:     }
1: }
============================================================================