1:72790dc: /*
1:72790dc:  * Licensed to the Apache Software Foundation (ASF) under one
1:72790dc:  * or more contributor license agreements.  See the NOTICE file
1:72790dc:  * distributed with this work for additional information
1:72790dc:  * regarding copyright ownership.  The ASF licenses this file
1:72790dc:  * to you under the Apache License, Version 2.0 (the
1:72790dc:  * "License"); you may not use this file except in compliance
1:72790dc:  * with the License.  You may obtain a copy of the License at
1:72790dc:  *
1:72790dc:  *     http://www.apache.org/licenses/LICENSE-2.0
1:72790dc:  *
1:72790dc:  * Unless required by applicable law or agreed to in writing, software
1:72790dc:  * distributed under the License is distributed on an "AS IS" BASIS,
1:72790dc:  * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
1:72790dc:  * See the License for the specific language governing permissions and
1:72790dc:  * limitations under the License.
1:72790dc:  */
1:72790dc: package org.apache.cassandra.index.sasi.analyzer;
1:72790dc: 
1:72790dc: import java.io.InputStream;
1:72790dc: import java.nio.ByteBuffer;
1:72790dc: import java.util.ArrayList;
1:72790dc: import java.util.List;
1:72790dc: import java.util.Locale;
1:72790dc: 
1:72790dc: import org.junit.Test;
1:72790dc: 
1:eb82861: import org.apache.cassandra.serializers.UTF8Serializer;
1:eb82861: 
1:72790dc: import static org.junit.Assert.assertEquals;
1:72790dc: 
1:72790dc: public class StandardAnalyzerTest
1:72790dc: {
1:72790dc:     @Test
1:72790dc:     public void testTokenizationAscii() throws Exception
1:72790dc:     {
1:72790dc:         InputStream is = StandardAnalyzerTest.class.getClassLoader()
1:72790dc:                 .getResourceAsStream("tokenization/apache_license_header.txt");
1:72790dc: 
1:72790dc:         StandardTokenizerOptions options = new StandardTokenizerOptions.OptionsBuilder()
1:72790dc:                 .maxTokenLength(5).build();
1:72790dc:         StandardAnalyzer tokenizer = new StandardAnalyzer();
1:72790dc:         tokenizer.init(options);
1:72790dc: 
1:72790dc:         List<ByteBuffer> tokens = new ArrayList<>();
1:72790dc:         tokenizer.reset(is);
1:72790dc:         while (tokenizer.hasNext())
1:72790dc:             tokens.add(tokenizer.next());
1:72790dc: 
1:72790dc:         assertEquals(67, tokens.size());
1:72790dc:     }
1:72790dc: 
1:72790dc:     @Test
1:72790dc:     public void testTokenizationLoremIpsum() throws Exception
1:72790dc:     {
1:72790dc:         InputStream is = StandardAnalyzerTest.class.getClassLoader()
1:72790dc:                 .getResourceAsStream("tokenization/lorem_ipsum.txt");
1:72790dc: 
1:72790dc:         StandardAnalyzer tokenizer = new StandardAnalyzer();
1:72790dc:         tokenizer.init(StandardTokenizerOptions.getDefaultOptions());
1:72790dc: 
1:72790dc:         List<ByteBuffer> tokens = new ArrayList<>();
1:72790dc:         tokenizer.reset(is);
1:72790dc:         while (tokenizer.hasNext())
1:72790dc:             tokens.add(tokenizer.next());
1:72790dc: 
1:72790dc:         assertEquals(62, tokens.size());
1:72790dc: 
1:72790dc:     }
1:72790dc: 
1:72790dc:     @Test
1:72790dc:     public void testTokenizationJaJp1() throws Exception
1:72790dc:     {
1:72790dc:         InputStream is = StandardAnalyzerTest.class.getClassLoader()
1:72790dc:                 .getResourceAsStream("tokenization/ja_jp_1.txt");
1:72790dc: 
1:72790dc:         StandardAnalyzer tokenizer = new StandardAnalyzer();
1:72790dc:         tokenizer.init(StandardTokenizerOptions.getDefaultOptions());
1:72790dc: 
1:72790dc:         tokenizer.reset(is);
1:72790dc:         List<ByteBuffer> tokens = new ArrayList<>();
1:72790dc:         while (tokenizer.hasNext())
1:72790dc:             tokens.add(tokenizer.next());
1:72790dc: 
1:72790dc:         assertEquals(210, tokens.size());
1:72790dc:     }
1:72790dc: 
1:72790dc:     @Test
1:72790dc:     public void testTokenizationJaJp2() throws Exception
1:72790dc:     {
1:72790dc:         InputStream is = StandardAnalyzerTest.class.getClassLoader()
1:72790dc:                 .getResourceAsStream("tokenization/ja_jp_2.txt");
1:72790dc: 
1:72790dc:         StandardTokenizerOptions options = new StandardTokenizerOptions.OptionsBuilder().stemTerms(true)
1:72790dc:                 .ignoreStopTerms(true).alwaysLowerCaseTerms(true).build();
1:72790dc:         StandardAnalyzer tokenizer = new StandardAnalyzer();
1:72790dc:         tokenizer.init(options);
1:72790dc: 
1:72790dc:         tokenizer.reset(is);
1:72790dc:         List<ByteBuffer> tokens = new ArrayList<>();
1:72790dc:         while (tokenizer.hasNext())
1:72790dc:             tokens.add(tokenizer.next());
1:72790dc: 
1:72790dc:         assertEquals(57, tokens.size());
1:72790dc:     }
1:72790dc: 
1:72790dc:     @Test
1:72790dc:     public void testTokenizationRuRu1() throws Exception
1:72790dc:     {
1:72790dc:         InputStream is = StandardAnalyzerTest.class.getClassLoader()
1:72790dc:                 .getResourceAsStream("tokenization/ru_ru_1.txt");
1:72790dc:         StandardAnalyzer tokenizer = new StandardAnalyzer();
1:72790dc:         tokenizer.init(StandardTokenizerOptions.getDefaultOptions());
1:72790dc: 
1:72790dc:         List<ByteBuffer> tokens = new ArrayList<>();
1:72790dc:         tokenizer.reset(is);
1:72790dc:         while (tokenizer.hasNext())
1:72790dc:             tokens.add(tokenizer.next());
1:72790dc: 
1:72790dc:         assertEquals(456, tokens.size());
1:72790dc:     }
1:72790dc: 
1:72790dc:     @Test
1:72790dc:     public void testTokenizationZnTw1() throws Exception
1:72790dc:     {
1:72790dc:         InputStream is = StandardAnalyzerTest.class.getClassLoader()
1:72790dc:                 .getResourceAsStream("tokenization/zn_tw_1.txt");
1:72790dc:         StandardAnalyzer tokenizer = new StandardAnalyzer();
1:72790dc:         tokenizer.init(StandardTokenizerOptions.getDefaultOptions());
1:72790dc: 
1:72790dc:         List<ByteBuffer> tokens = new ArrayList<>();
1:72790dc:         tokenizer.reset(is);
1:72790dc:         while (tokenizer.hasNext())
1:72790dc:             tokens.add(tokenizer.next());
1:72790dc: 
1:72790dc:         assertEquals(963, tokens.size());
1:72790dc:     }
1:72790dc: 
1:72790dc:     @Test
1:72790dc:     public void testTokenizationAdventuresOfHuckFinn() throws Exception
1:72790dc:     {
1:72790dc:         InputStream is = StandardAnalyzerTest.class.getClassLoader()
1:72790dc:                 .getResourceAsStream("tokenization/adventures_of_huckleberry_finn_mark_twain.txt");
1:72790dc: 
1:72790dc:         StandardTokenizerOptions options = new StandardTokenizerOptions.OptionsBuilder().stemTerms(true)
1:72790dc:                 .ignoreStopTerms(true).useLocale(Locale.ENGLISH)
1:72790dc:                 .alwaysLowerCaseTerms(true).build();
1:72790dc:         StandardAnalyzer tokenizer = new StandardAnalyzer();
1:72790dc:         tokenizer.init(options);
1:72790dc: 
1:72790dc:         List<ByteBuffer> tokens = new ArrayList<>();
1:72790dc:         tokenizer.reset(is);
1:72790dc:         while (tokenizer.hasNext())
1:72790dc:             tokens.add(tokenizer.next());
1:72790dc: 
1:eb82861:         assertEquals(37739, tokens.size());
1:eb82861:     }
1:eb82861: 
1:eb82861:     @Test
1:eb82861:     public void testSkipStopWordBeforeStemmingFrench() throws Exception
1:eb82861:     {
1:eb82861:         InputStream is = StandardAnalyzerTest.class.getClassLoader()
1:eb82861:                .getResourceAsStream("tokenization/french_skip_stop_words_before_stemming.txt");
1:eb82861: 
1:eb82861:         StandardTokenizerOptions options = new StandardTokenizerOptions.OptionsBuilder().stemTerms(true)
1:eb82861:                 .ignoreStopTerms(true).useLocale(Locale.FRENCH)
1:eb82861:                 .alwaysLowerCaseTerms(true).build();
1:eb82861:         StandardAnalyzer tokenizer = new StandardAnalyzer();
1:eb82861:         tokenizer.init(options);
1:eb82861: 
1:eb82861:         List<ByteBuffer> tokens = new ArrayList<>();
1:eb82861:         List<String> words = new ArrayList<>();
1:eb82861:         tokenizer.reset(is);
1:eb82861:         while (tokenizer.hasNext())
1:eb82861:         {
1:eb82861:             final ByteBuffer nextToken = tokenizer.next();
1:eb82861:             tokens.add(nextToken);
1:eb82861:             words.add(UTF8Serializer.instance.deserialize(nextToken.duplicate()));
1:eb82861:         }
1:eb82861: 
1:eb82861:         assertEquals(4, tokens.size());
1:eb82861:         assertEquals("dans", words.get(0));
1:eb82861:         assertEquals("plui", words.get(1));
1:eb82861:         assertEquals("chanson", words.get(2));
1:eb82861:         assertEquals("connu", words.get(3));
1:72790dc:     }
1:72790dc: 
1:72790dc:     @Test
1:72790dc:     public void tokenizeDomainNamesAndUrls() throws Exception
1:72790dc:     {
1:72790dc:         InputStream is = StandardAnalyzerTest.class.getClassLoader()
1:72790dc:                 .getResourceAsStream("tokenization/top_visited_domains.txt");
1:72790dc: 
1:72790dc:         StandardAnalyzer tokenizer = new StandardAnalyzer();
1:72790dc:         tokenizer.init(StandardTokenizerOptions.getDefaultOptions());
1:72790dc:         tokenizer.reset(is);
1:72790dc: 
1:72790dc:         List<ByteBuffer> tokens = new ArrayList<>();
1:72790dc:         while (tokenizer.hasNext())
1:72790dc:             tokens.add(tokenizer.next());
1:72790dc: 
1:72790dc:         assertEquals(15, tokens.size());
1:72790dc:     }
1:72790dc: 
1:72790dc:     @Test
1:72790dc:     public void testReuseAndResetTokenizerInstance() throws Exception
1:72790dc:     {
1:72790dc:         List<ByteBuffer> bbToTokenize = new ArrayList<>();
1:72790dc:         bbToTokenize.add(ByteBuffer.wrap("Nip it in the bud".getBytes()));
1:72790dc:         bbToTokenize.add(ByteBuffer.wrap("I couldn’t care less".getBytes()));
1:72790dc:         bbToTokenize.add(ByteBuffer.wrap("One and the same".getBytes()));
1:72790dc:         bbToTokenize.add(ByteBuffer.wrap("The squeaky wheel gets the grease.".getBytes()));
1:72790dc:         bbToTokenize.add(ByteBuffer.wrap("The pen is mightier than the sword.".getBytes()));
1:72790dc: 
1:72790dc:         StandardAnalyzer tokenizer = new StandardAnalyzer();
1:72790dc:         tokenizer.init(StandardTokenizerOptions.getDefaultOptions());
1:72790dc: 
1:72790dc:         List<ByteBuffer> tokens = new ArrayList<>();
1:72790dc:         for (ByteBuffer bb : bbToTokenize)
1:72790dc:         {
1:72790dc:             tokenizer.reset(bb);
1:72790dc:             while (tokenizer.hasNext())
1:72790dc:                 tokens.add(tokenizer.next());
1:72790dc:         }
1:72790dc:         assertEquals(10, tokens.size());
1:72790dc:     }
1:72790dc: }
============================================================================
author:Pavel Yaskevich
-------------------------------------------------------------------------------
commit:eb82861
/////////////////////////////////////////////////////////////////////////
1: import org.apache.cassandra.serializers.UTF8Serializer;
1: 
/////////////////////////////////////////////////////////////////////////
1:         assertEquals(37739, tokens.size());
1:     }
1: 
1:     @Test
1:     public void testSkipStopWordBeforeStemmingFrench() throws Exception
1:     {
1:         InputStream is = StandardAnalyzerTest.class.getClassLoader()
1:                .getResourceAsStream("tokenization/french_skip_stop_words_before_stemming.txt");
1: 
1:         StandardTokenizerOptions options = new StandardTokenizerOptions.OptionsBuilder().stemTerms(true)
1:                 .ignoreStopTerms(true).useLocale(Locale.FRENCH)
1:                 .alwaysLowerCaseTerms(true).build();
1:         StandardAnalyzer tokenizer = new StandardAnalyzer();
1:         tokenizer.init(options);
1: 
1:         List<ByteBuffer> tokens = new ArrayList<>();
1:         List<String> words = new ArrayList<>();
1:         tokenizer.reset(is);
1:         while (tokenizer.hasNext())
1:         {
1:             final ByteBuffer nextToken = tokenizer.next();
1:             tokens.add(nextToken);
1:             words.add(UTF8Serializer.instance.deserialize(nextToken.duplicate()));
1:         }
1: 
1:         assertEquals(4, tokens.size());
1:         assertEquals("dans", words.get(0));
1:         assertEquals("plui", words.get(1));
1:         assertEquals("chanson", words.get(2));
1:         assertEquals("connu", words.get(3));
commit:72790dc
/////////////////////////////////////////////////////////////////////////
1: /*
1:  * Licensed to the Apache Software Foundation (ASF) under one
1:  * or more contributor license agreements.  See the NOTICE file
1:  * distributed with this work for additional information
1:  * regarding copyright ownership.  The ASF licenses this file
1:  * to you under the Apache License, Version 2.0 (the
1:  * "License"); you may not use this file except in compliance
1:  * with the License.  You may obtain a copy of the License at
1:  *
1:  *     http://www.apache.org/licenses/LICENSE-2.0
1:  *
1:  * Unless required by applicable law or agreed to in writing, software
1:  * distributed under the License is distributed on an "AS IS" BASIS,
1:  * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
1:  * See the License for the specific language governing permissions and
1:  * limitations under the License.
1:  */
1: package org.apache.cassandra.index.sasi.analyzer;
1: 
1: import java.io.InputStream;
1: import java.nio.ByteBuffer;
1: import java.util.ArrayList;
1: import java.util.List;
1: import java.util.Locale;
1: 
1: import org.junit.Test;
1: 
1: import static org.junit.Assert.assertEquals;
1: 
1: public class StandardAnalyzerTest
1: {
1:     @Test
1:     public void testTokenizationAscii() throws Exception
1:     {
1:         InputStream is = StandardAnalyzerTest.class.getClassLoader()
1:                 .getResourceAsStream("tokenization/apache_license_header.txt");
1: 
1:         StandardTokenizerOptions options = new StandardTokenizerOptions.OptionsBuilder()
1:                 .maxTokenLength(5).build();
1:         StandardAnalyzer tokenizer = new StandardAnalyzer();
1:         tokenizer.init(options);
1: 
1:         List<ByteBuffer> tokens = new ArrayList<>();
1:         tokenizer.reset(is);
1:         while (tokenizer.hasNext())
1:             tokens.add(tokenizer.next());
1: 
1:         assertEquals(67, tokens.size());
1:     }
1: 
1:     @Test
1:     public void testTokenizationLoremIpsum() throws Exception
1:     {
1:         InputStream is = StandardAnalyzerTest.class.getClassLoader()
1:                 .getResourceAsStream("tokenization/lorem_ipsum.txt");
1: 
1:         StandardAnalyzer tokenizer = new StandardAnalyzer();
1:         tokenizer.init(StandardTokenizerOptions.getDefaultOptions());
1: 
1:         List<ByteBuffer> tokens = new ArrayList<>();
1:         tokenizer.reset(is);
1:         while (tokenizer.hasNext())
1:             tokens.add(tokenizer.next());
1: 
1:         assertEquals(62, tokens.size());
1: 
1:     }
1: 
1:     @Test
1:     public void testTokenizationJaJp1() throws Exception
1:     {
1:         InputStream is = StandardAnalyzerTest.class.getClassLoader()
1:                 .getResourceAsStream("tokenization/ja_jp_1.txt");
1: 
1:         StandardAnalyzer tokenizer = new StandardAnalyzer();
1:         tokenizer.init(StandardTokenizerOptions.getDefaultOptions());
1: 
1:         tokenizer.reset(is);
1:         List<ByteBuffer> tokens = new ArrayList<>();
1:         while (tokenizer.hasNext())
1:             tokens.add(tokenizer.next());
1: 
1:         assertEquals(210, tokens.size());
1:     }
1: 
1:     @Test
1:     public void testTokenizationJaJp2() throws Exception
1:     {
1:         InputStream is = StandardAnalyzerTest.class.getClassLoader()
1:                 .getResourceAsStream("tokenization/ja_jp_2.txt");
1: 
1:         StandardTokenizerOptions options = new StandardTokenizerOptions.OptionsBuilder().stemTerms(true)
1:                 .ignoreStopTerms(true).alwaysLowerCaseTerms(true).build();
1:         StandardAnalyzer tokenizer = new StandardAnalyzer();
1:         tokenizer.init(options);
1: 
1:         tokenizer.reset(is);
1:         List<ByteBuffer> tokens = new ArrayList<>();
1:         while (tokenizer.hasNext())
1:             tokens.add(tokenizer.next());
1: 
1:         assertEquals(57, tokens.size());
1:     }
1: 
1:     @Test
1:     public void testTokenizationRuRu1() throws Exception
1:     {
1:         InputStream is = StandardAnalyzerTest.class.getClassLoader()
1:                 .getResourceAsStream("tokenization/ru_ru_1.txt");
1:         StandardAnalyzer tokenizer = new StandardAnalyzer();
1:         tokenizer.init(StandardTokenizerOptions.getDefaultOptions());
1: 
1:         List<ByteBuffer> tokens = new ArrayList<>();
1:         tokenizer.reset(is);
1:         while (tokenizer.hasNext())
1:             tokens.add(tokenizer.next());
1: 
1:         assertEquals(456, tokens.size());
1:     }
1: 
1:     @Test
1:     public void testTokenizationZnTw1() throws Exception
1:     {
1:         InputStream is = StandardAnalyzerTest.class.getClassLoader()
1:                 .getResourceAsStream("tokenization/zn_tw_1.txt");
1:         StandardAnalyzer tokenizer = new StandardAnalyzer();
1:         tokenizer.init(StandardTokenizerOptions.getDefaultOptions());
1: 
1:         List<ByteBuffer> tokens = new ArrayList<>();
1:         tokenizer.reset(is);
1:         while (tokenizer.hasNext())
1:             tokens.add(tokenizer.next());
1: 
1:         assertEquals(963, tokens.size());
1:     }
1: 
1:     @Test
1:     public void testTokenizationAdventuresOfHuckFinn() throws Exception
1:     {
1:         InputStream is = StandardAnalyzerTest.class.getClassLoader()
1:                 .getResourceAsStream("tokenization/adventures_of_huckleberry_finn_mark_twain.txt");
1: 
1:         StandardTokenizerOptions options = new StandardTokenizerOptions.OptionsBuilder().stemTerms(true)
1:                 .ignoreStopTerms(true).useLocale(Locale.ENGLISH)
1:                 .alwaysLowerCaseTerms(true).build();
1:         StandardAnalyzer tokenizer = new StandardAnalyzer();
1:         tokenizer.init(options);
1: 
1:         List<ByteBuffer> tokens = new ArrayList<>();
1:         tokenizer.reset(is);
1:         while (tokenizer.hasNext())
1:             tokens.add(tokenizer.next());
1: 
0:         assertEquals(40249, tokens.size());
1:     }
1: 
1:     @Test
1:     public void tokenizeDomainNamesAndUrls() throws Exception
1:     {
1:         InputStream is = StandardAnalyzerTest.class.getClassLoader()
1:                 .getResourceAsStream("tokenization/top_visited_domains.txt");
1: 
1:         StandardAnalyzer tokenizer = new StandardAnalyzer();
1:         tokenizer.init(StandardTokenizerOptions.getDefaultOptions());
1:         tokenizer.reset(is);
1: 
1:         List<ByteBuffer> tokens = new ArrayList<>();
1:         while (tokenizer.hasNext())
1:             tokens.add(tokenizer.next());
1: 
1:         assertEquals(15, tokens.size());
1:     }
1: 
1:     @Test
1:     public void testReuseAndResetTokenizerInstance() throws Exception
1:     {
1:         List<ByteBuffer> bbToTokenize = new ArrayList<>();
1:         bbToTokenize.add(ByteBuffer.wrap("Nip it in the bud".getBytes()));
1:         bbToTokenize.add(ByteBuffer.wrap("I couldn’t care less".getBytes()));
1:         bbToTokenize.add(ByteBuffer.wrap("One and the same".getBytes()));
1:         bbToTokenize.add(ByteBuffer.wrap("The squeaky wheel gets the grease.".getBytes()));
1:         bbToTokenize.add(ByteBuffer.wrap("The pen is mightier than the sword.".getBytes()));
1: 
1:         StandardAnalyzer tokenizer = new StandardAnalyzer();
1:         tokenizer.init(StandardTokenizerOptions.getDefaultOptions());
1: 
1:         List<ByteBuffer> tokens = new ArrayList<>();
1:         for (ByteBuffer bb : bbToTokenize)
1:         {
1:             tokenizer.reset(bb);
1:             while (tokenizer.hasNext())
1:                 tokens.add(tokenizer.next());
1:         }
1:         assertEquals(10, tokens.size());
1:     }
1: }
============================================================================