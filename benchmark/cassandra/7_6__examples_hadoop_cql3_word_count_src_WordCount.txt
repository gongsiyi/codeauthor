1:56e0ad1: /**
1:56e0ad1:  * Licensed to the Apache Software Foundation (ASF) under one
1:56e0ad1:  * or more contributor license agreements.  See the NOTICE file
1:56e0ad1:  * distributed with this work for additional information
1:56e0ad1:  * regarding copyright ownership.  The ASF licenses this file
1:56e0ad1:  * to you under the Apache License, Version 2.0 (the
1:56e0ad1:  * "License"); you may not use this file except in compliance
1:56e0ad1:  * with the License.  You may obtain a copy of the License at
1:56e0ad1:  *
1:56e0ad1:  *     http://www.apache.org/licenses/LICENSE-2.0
1:56e0ad1:  *
1:56e0ad1:  * Unless required by applicable law or agreed to in writing, software
1:56e0ad1:  * distributed under the License is distributed on an "AS IS" BASIS,
1:56e0ad1:  * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
1:56e0ad1:  * See the License for the specific language governing permissions and
1:56e0ad1:  * limitations under the License.
1:56e0ad1:  */
17:56e0ad1: 
1:56e0ad1: import java.io.IOException;
1:56e0ad1: import java.nio.ByteBuffer;
1:56e0ad1: import java.util.*;
1:56e0ad1: import java.util.Map.Entry;
1:56e0ad1: 
1:a004779: import org.apache.cassandra.hadoop.cql3.CqlConfigHelper;
1:a004779: import org.apache.cassandra.hadoop.cql3.CqlOutputFormat;
1:56e0ad1: import org.slf4j.Logger;
1:56e0ad1: import org.slf4j.LoggerFactory;
1:56e0ad1: 
1:d7cb970: import org.apache.cassandra.hadoop.cql3.CqlInputFormat;
1:56e0ad1: import org.apache.cassandra.hadoop.ConfigHelper;
1:56e0ad1: import org.apache.cassandra.utils.ByteBufferUtil;
1:56e0ad1: import org.apache.hadoop.conf.Configuration;
1:56e0ad1: import org.apache.hadoop.conf.Configured;
1:56e0ad1: import org.apache.hadoop.fs.Path;
1:56e0ad1: import org.apache.hadoop.io.IntWritable;
1:56e0ad1: import org.apache.hadoop.io.Text;
1:56e0ad1: import org.apache.hadoop.mapreduce.Job;
1:56e0ad1: import org.apache.hadoop.mapreduce.Mapper;
1:56e0ad1: import org.apache.hadoop.mapreduce.Reducer;
1:56e0ad1: import org.apache.hadoop.mapreduce.lib.output.FileOutputFormat;
1:56e0ad1: import org.apache.hadoop.util.Tool;
1:56e0ad1: import org.apache.hadoop.util.ToolRunner;
1:d7cb970: import com.datastax.driver.core.Row;
1:b0841d8: 
1:56e0ad1: /**
1:56e0ad1:  * This counts the occurrences of words in ColumnFamily
1:1936657:  *   cql3_wordcount ( id uuid,
1:b4f2ff1:  *                   line  text,
1:b4f2ff1:  *                   PRIMARY KEY (id))
1:56e0ad1:  *
1:56e0ad1:  * For each word, we output the total number of occurrences across all body texts.
1:56e0ad1:  *
1:56e0ad1:  * When outputting to Cassandra, we write the word counts to column family
1:b4f2ff1:  *  output_words ( word text,
1:56e0ad1:  *                 count_num text,
1:b4f2ff1:  *                 PRIMARY KEY (word))
1:56e0ad1:  * as a {word, count} to columns: word, count_num with a row key of "word sum"
1:56e0ad1:  */
1:56e0ad1: public class WordCount extends Configured implements Tool
13:56e0ad1: {
1:56e0ad1:     private static final Logger logger = LoggerFactory.getLogger(WordCount.class);
1:d7cb970:     static final String INPUT_MAPPER_VAR = "input_mapper";
1:1936657:     static final String KEYSPACE = "cql3_wordcount";
1:56e0ad1:     static final String COLUMN_FAMILY = "inputs";
1:b0841d8: 
1:56e0ad1:     static final String OUTPUT_REDUCER_VAR = "output_reducer";
1:56e0ad1:     static final String OUTPUT_COLUMN_FAMILY = "output_words";
1:b0841d8: 
1:56e0ad1:     private static final String OUTPUT_PATH_PREFIX = "/tmp/word_count";
1:56e0ad1:     private static final String PRIMARY_KEY = "row_key";
1:3b708f9: 
1:56e0ad1:     public static void main(String[] args) throws Exception
1:56e0ad1:     {
1:56e0ad1:         // Let ToolRunner handle generic command-line options
1:56e0ad1:         ToolRunner.run(new Configuration(), new WordCount(), args);
1:56e0ad1:         System.exit(0);
14:56e0ad1:     }
1:3b708f9: 
1:56e0ad1:     public static class TokenizerMapper extends Mapper<Map<String, ByteBuffer>, Map<String, ByteBuffer>, Text, IntWritable>
1:3b708f9:     {
1:3b708f9:         private final static IntWritable one = new IntWritable(1);
1:3b708f9:         private Text word = new Text();
1:3b708f9:         private ByteBuffer sourceColumn;
1:d7cb970: 
1:3b708f9:         protected void setup(org.apache.hadoop.mapreduce.Mapper.Context context)
1:3b708f9:         throws IOException, InterruptedException
1:3b708f9:         {
1:3b708f9:         }
1:3b708f9: 
1:56e0ad1:         public void map(Map<String, ByteBuffer> keys, Map<String, ByteBuffer> columns, Context context) throws IOException, InterruptedException
1:3b708f9:         {
1:56e0ad1:             for (Entry<String, ByteBuffer> column : columns.entrySet())
1:3b708f9:             {
1:b4f2ff1:                 if (!"line".equalsIgnoreCase(column.getKey()))
1:56e0ad1:                     continue;
1:3b708f9: 
1:56e0ad1:                 String value = ByteBufferUtil.string(column.getValue());
1:56e0ad1: 
1:3b708f9:                 StringTokenizer itr = new StringTokenizer(value);
1:3b708f9:                 while (itr.hasMoreTokens())
1:3b708f9:                 {
1:3b708f9:                     word.set(itr.nextToken());
1:3b708f9:                     context.write(word, one);
1:3b708f9:                 }
1:56e0ad1:             }
1:56e0ad1:         }
1:56e0ad1:     }
1:56e0ad1: 
1:d7cb970:     public static class NativeTokenizerMapper extends Mapper<Long, Row, Text, IntWritable>
1:d7cb970:     {
1:d7cb970:         private final static IntWritable one = new IntWritable(1);
1:d7cb970:         private Text word = new Text();
1:d7cb970:         private ByteBuffer sourceColumn;
1:d7cb970: 
1:d7cb970:         protected void setup(org.apache.hadoop.mapreduce.Mapper.Context context)
1:d7cb970:         throws IOException, InterruptedException
1:d7cb970:         {
1:d7cb970:         }
1:3b708f9: 
1:d7cb970:         public void map(Long key, Row row, Context context) throws IOException, InterruptedException
1:d7cb970:         {
1:d7cb970:             String value = row.getString("line");
1:cfee3da:             logger.debug("read {}:{}={} from {}", key, "line", value, context.getInputSplit());
1:d7cb970:             StringTokenizer itr = new StringTokenizer(value);
1:d7cb970:             while (itr.hasMoreTokens())
1:d7cb970:             {
1:d7cb970:                 word.set(itr.nextToken());
1:d7cb970:                 context.write(word, one);
1:d7cb970:             }
1:d7cb970:         }
1:d7cb970:     }
1:d7cb970: 
1:56e0ad1:     public static class ReducerToFilesystem extends Reducer<Text, IntWritable, Text, IntWritable>
1:3b708f9:     {
1:56e0ad1:         public void reduce(Text key, Iterable<IntWritable> values, Context context) throws IOException, InterruptedException
1:56e0ad1:         {
1:56e0ad1:             int sum = 0;
1:56e0ad1:             for (IntWritable val : values)
1:56e0ad1:                 sum += val.get();
1:56e0ad1:             context.write(key, new IntWritable(sum));
1:3b708f9:         }
1:3b708f9:     }
1:56e0ad1: 
1:56e0ad1:     public static class ReducerToCassandra extends Reducer<Text, IntWritable, Map<String, ByteBuffer>, List<ByteBuffer>>
1:56e0ad1:     {
1:56e0ad1:         private Map<String, ByteBuffer> keys;
1:56e0ad1:         private ByteBuffer key;
1:56e0ad1:         protected void setup(org.apache.hadoop.mapreduce.Reducer.Context context)
2:56e0ad1:         throws IOException, InterruptedException
1:56e0ad1:         {
1:56e0ad1:             keys = new LinkedHashMap<String, ByteBuffer>();
1:3b708f9:         }
1:56e0ad1: 
1:56e0ad1:         public void reduce(Text word, Iterable<IntWritable> values, Context context) throws IOException, InterruptedException
1:56e0ad1:         {
1:56e0ad1:             int sum = 0;
1:56e0ad1:             for (IntWritable val : values)
1:56e0ad1:                 sum += val.get();
1:b4f2ff1:             keys.put("word", ByteBufferUtil.bytes(word.toString()));
1:56e0ad1:             context.write(keys, getBindVariables(word, sum));
1:56e0ad1:         }
1:56e0ad1: 
1:56e0ad1:         private List<ByteBuffer> getBindVariables(Text word, int sum)
1:56e0ad1:         {
1:56e0ad1:             List<ByteBuffer> variables = new ArrayList<ByteBuffer>();
1:56e0ad1:             variables.add(ByteBufferUtil.bytes(String.valueOf(sum)));         
1:56e0ad1:             return variables;
1:56e0ad1:         }
1:56e0ad1:     }
1:56e0ad1: 
1:56e0ad1:     public int run(String[] args) throws Exception
1:56e0ad1:     {
1:56e0ad1:         String outputReducerType = "filesystem";
1:d7cb970:         String inputMapperType = "native";
1:d7cb970:         String outputReducer = null;
1:d7cb970:         String inputMapper = null;
1:d7cb970: 
1:d7cb970:         if (args != null)
1:3b708f9:         {
1:d7cb970:             if(args[0].startsWith(OUTPUT_REDUCER_VAR))
1:d7cb970:                 outputReducer = args[0];
1:d7cb970:             if(args[0].startsWith(INPUT_MAPPER_VAR))
1:d7cb970:                 inputMapper = args[0];
1:d7cb970:             
1:d7cb970:             if (args.length == 2)
1:d7cb970:             {
1:d7cb970:                 if(args[1].startsWith(OUTPUT_REDUCER_VAR))
1:d7cb970:                     outputReducer = args[1];
1:d7cb970:                 if(args[1].startsWith(INPUT_MAPPER_VAR))
1:d7cb970:                     inputMapper = args[1]; 
1:d7cb970:             }
1:d7cb970:         }
1:d7cb970: 
1:d7cb970:         if (outputReducer != null)
1:d7cb970:         {
1:d7cb970:             String[] s = outputReducer.split("=");
1:3b708f9:             if (s != null && s.length == 2)
1:56e0ad1:                 outputReducerType = s[1];
1:3b708f9:         }
1:56e0ad1:         logger.info("output reducer type: " + outputReducerType);
1:d7cb970:         if (inputMapper != null)
1:d7cb970:         {
1:d7cb970:             String[] s = inputMapper.split("=");
1:d7cb970:             if (s != null && s.length == 2)
1:d7cb970:                 inputMapperType = s[1];
1:d7cb970:         }
1:56e0ad1:         Job job = new Job(getConf(), "wordcount");
1:56e0ad1:         job.setJarByClass(WordCount.class);
1:b0841d8: 
1:56e0ad1:         if (outputReducerType.equalsIgnoreCase("filesystem"))
1:3b708f9:         {
1:56e0ad1:             job.setCombinerClass(ReducerToFilesystem.class);
1:56e0ad1:             job.setReducerClass(ReducerToFilesystem.class);
1:56e0ad1:             job.setOutputKeyClass(Text.class);
1:56e0ad1:             job.setOutputValueClass(IntWritable.class);
1:56e0ad1:             FileOutputFormat.setOutputPath(job, new Path(OUTPUT_PATH_PREFIX));
1:3b708f9:         }
1:3b708f9:         else
1:3b708f9:         {
1:56e0ad1:             job.setReducerClass(ReducerToCassandra.class);
1:3b708f9: 
1:56e0ad1:             job.setMapOutputKeyClass(Text.class);
1:56e0ad1:             job.setMapOutputValueClass(IntWritable.class);
1:56e0ad1:             job.setOutputKeyClass(Map.class);
1:56e0ad1:             job.setOutputValueClass(List.class);
1:56e0ad1: 
1:a004779:             job.setOutputFormatClass(CqlOutputFormat.class);
1:56e0ad1: 
1:56e0ad1:             ConfigHelper.setOutputColumnFamily(job.getConfiguration(), KEYSPACE, OUTPUT_COLUMN_FAMILY);
1:56e0ad1:             job.getConfiguration().set(PRIMARY_KEY, "word,sum");
1:f1004e9:             String query = "UPDATE " + KEYSPACE + "." + OUTPUT_COLUMN_FAMILY +
1:f1004e9:                            " SET count_num = ? ";
1:a004779:             CqlConfigHelper.setOutputCql(job.getConfiguration(), query);
1:56e0ad1:             ConfigHelper.setOutputInitialAddress(job.getConfiguration(), "localhost");
1:56e0ad1:             ConfigHelper.setOutputPartitioner(job.getConfiguration(), "Murmur3Partitioner");
1:3b708f9:         }
1:56e0ad1: 
1:d7cb970:         if (inputMapperType.equalsIgnoreCase("native"))
1:d7cb970:         {
1:d7cb970:             job.setMapperClass(NativeTokenizerMapper.class);
1:e550ea6:             job.setInputFormatClass(CqlInputFormat.class);
1:d7cb970:             CqlConfigHelper.setInputCql(job.getConfiguration(), "select * from " + COLUMN_FAMILY + " where token(id) > ? and token(id) <= ? allow filtering");
1:d7cb970:         }
1:d7cb970:         else
1:d7cb970:         {
1:d7cb970:             job.setMapperClass(TokenizerMapper.class);
1:f6d0f43:             job.setInputFormatClass(CqlInputFormat.class);
1:d7cb970:             ConfigHelper.setInputRpcPort(job.getConfiguration(), "9160");
1:d7cb970:         }
1:56e0ad1: 
1:56e0ad1:         ConfigHelper.setInputInitialAddress(job.getConfiguration(), "localhost");
1:56e0ad1:         ConfigHelper.setInputColumnFamily(job.getConfiguration(), KEYSPACE, COLUMN_FAMILY);
1:56e0ad1:         ConfigHelper.setInputPartitioner(job.getConfiguration(), "Murmur3Partitioner");
1:56e0ad1: 
1:a004779:         CqlConfigHelper.setInputCQLPageRowSize(job.getConfiguration(), "3");
1:56e0ad1:         job.waitForCompletion(true);
1:56e0ad1:         return 0;
1:3b708f9:     }
1:b4f2ff1: }
============================================================================
author:Sam Tunnicliffe
-------------------------------------------------------------------------------
commit:1936657
/////////////////////////////////////////////////////////////////////////
1:  *   cql3_wordcount ( id uuid,
/////////////////////////////////////////////////////////////////////////
1:     static final String KEYSPACE = "cql3_wordcount";
author:Brandon Williams
-------------------------------------------------------------------------------
commit:3b94548
commit:f6d0f43
/////////////////////////////////////////////////////////////////////////
/////////////////////////////////////////////////////////////////////////
1:             job.setInputFormatClass(CqlInputFormat.class);
author:Aleksey Yeschenko
-------------------------------------------------------------------------------
commit:e1e28d0
commit:e27cdf9
/////////////////////////////////////////////////////////////////////////
0: import org.apache.cassandra.hadoop.cql3.CqlPagingInputFormat;
/////////////////////////////////////////////////////////////////////////
0:             job.setInputFormatClass(CqlPagingInputFormat.class);
author:Dave Brosius
-------------------------------------------------------------------------------
commit:cfee3da
/////////////////////////////////////////////////////////////////////////
/////////////////////////////////////////////////////////////////////////
1:             logger.debug("read {}:{}={} from {}", key, "line", value, context.getInputSplit());
author:rekhajoshm
-------------------------------------------------------------------------------
commit:e550ea6
/////////////////////////////////////////////////////////////////////////
/////////////////////////////////////////////////////////////////////////
1:             job.setInputFormatClass(CqlInputFormat.class);
author:Sylvain Lebresne
-------------------------------------------------------------------------------
commit:d7cb970
/////////////////////////////////////////////////////////////////////////
1: import org.apache.cassandra.hadoop.cql3.CqlInputFormat;
/////////////////////////////////////////////////////////////////////////
0: import org.apache.hadoop.mapreduce.Mapper.Context;
1: import com.datastax.driver.core.Row;
/////////////////////////////////////////////////////////////////////////
1:     static final String INPUT_MAPPER_VAR = "input_mapper";
/////////////////////////////////////////////////////////////////////////
/////////////////////////////////////////////////////////////////////////
1:     public static class NativeTokenizerMapper extends Mapper<Long, Row, Text, IntWritable>
1:     {
1:         private final static IntWritable one = new IntWritable(1);
1:         private Text word = new Text();
1:         private ByteBuffer sourceColumn;
1: 
1:         protected void setup(org.apache.hadoop.mapreduce.Mapper.Context context)
1:         throws IOException, InterruptedException
1:         {
1:         }
1: 
1:         public void map(Long key, Row row, Context context) throws IOException, InterruptedException
1:         {
1:             String value = row.getString("line");
0:             logger.debug("read {}:{}={} from {}", new Object[] {key, "line", value, context.getInputSplit()});
1:             StringTokenizer itr = new StringTokenizer(value);
1:             while (itr.hasMoreTokens())
1:             {
1:                 word.set(itr.nextToken());
1:                 context.write(word, one);
1:             }
1:         }
1:     }
1: 
/////////////////////////////////////////////////////////////////////////
1:         String inputMapperType = "native";
1:         String outputReducer = null;
1:         String inputMapper = null;
1: 
1:         if (args != null)
1:             if(args[0].startsWith(OUTPUT_REDUCER_VAR))
1:                 outputReducer = args[0];
1:             if(args[0].startsWith(INPUT_MAPPER_VAR))
1:                 inputMapper = args[0];
1:             
1:             if (args.length == 2)
1:             {
1:                 if(args[1].startsWith(OUTPUT_REDUCER_VAR))
1:                     outputReducer = args[1];
1:                 if(args[1].startsWith(INPUT_MAPPER_VAR))
1:                     inputMapper = args[1]; 
1:             }
1:         }
1: 
1:         if (outputReducer != null)
1:         {
1:             String[] s = outputReducer.split("=");
1:         if (inputMapper != null)
1:         {
1:             String[] s = inputMapper.split("=");
1:             if (s != null && s.length == 2)
1:                 inputMapperType = s[1];
1:         }
/////////////////////////////////////////////////////////////////////////
1:         if (inputMapperType.equalsIgnoreCase("native"))
1:         {
1:             job.setMapperClass(NativeTokenizerMapper.class);
0:             job.setInputFormatClass(CqlInputFormat.class);
1:             CqlConfigHelper.setInputCql(job.getConfiguration(), "select * from " + COLUMN_FAMILY + " where token(id) > ? and token(id) <= ? allow filtering");
1:         }
1:         else
1:         {
1:             job.setMapperClass(TokenizerMapper.class);
0:             job.setInputFormatClass(CqlPagingInputFormat.class);
1:             ConfigHelper.setInputRpcPort(job.getConfiguration(), "9160");
1:         }
commit:b0841d8
/////////////////////////////////////////////////////////////////////////
/////////////////////////////////////////////////////////////////////////
1: 
/////////////////////////////////////////////////////////////////////////
1: 
/////////////////////////////////////////////////////////////////////////
1: 
/////////////////////////////////////////////////////////////////////////
/////////////////////////////////////////////////////////////////////////
0:         if (args != null && args[0].startsWith(OUTPUT_REDUCER_VAR))
0:             String[] s = args[0].split("=");
1: 
0:         job.setMapperClass(TokenizerMapper.class);
/////////////////////////////////////////////////////////////////////////
0:         job.setInputFormatClass(CqlPagingInputFormat.class);
0:         ConfigHelper.setInputRpcPort(job.getConfiguration(), "9160");
commit:3b708f9
/////////////////////////////////////////////////////////////////////////
0: import org.apache.cassandra.hadoop.cql3.CqlInputFormat;
/////////////////////////////////////////////////////////////////////////
0: import org.apache.hadoop.mapreduce.Mapper.Context;
0: import com.datastax.driver.core.Row;
/////////////////////////////////////////////////////////////////////////
0:     static final String INPUT_MAPPER_VAR = "input_mapper";
/////////////////////////////////////////////////////////////////////////
/////////////////////////////////////////////////////////////////////////
0:     public static class NativeTokenizerMapper extends Mapper<Long, Row, Text, IntWritable>
1:     {
1:         private final static IntWritable one = new IntWritable(1);
1:         private Text word = new Text();
1:         private ByteBuffer sourceColumn;
1: 
1:         protected void setup(org.apache.hadoop.mapreduce.Mapper.Context context)
1:         throws IOException, InterruptedException
1:         {
1:         }
1: 
0:         public void map(Long key, Row row, Context context) throws IOException, InterruptedException
1:         {
0:             String value = row.getString("line");
0:             logger.debug("read {}:{}={} from {}", new Object[] {key, "line", value, context.getInputSplit()});
1:             StringTokenizer itr = new StringTokenizer(value);
1:             while (itr.hasMoreTokens())
1:             {
1:                 word.set(itr.nextToken());
1:                 context.write(word, one);
1:             }
1:         }
1:     }
1: 
/////////////////////////////////////////////////////////////////////////
0:         String inputMapperType = "native";
0:         String outputReducer = null;
0:         String inputMapper = null;
1: 
0:         if (args != null)
0:             if(args[0].startsWith(OUTPUT_REDUCER_VAR))
0:                 outputReducer = args[0];
0:             if(args[0].startsWith(INPUT_MAPPER_VAR))
0:                 inputMapper = args[0];
1:             
0:             if (args.length == 2)
1:             {
0:                 if(args[1].startsWith(OUTPUT_REDUCER_VAR))
0:                     outputReducer = args[1];
0:                 if(args[1].startsWith(INPUT_MAPPER_VAR))
0:                     inputMapper = args[1]; 
1:             }
1:         }
1: 
0:         if (outputReducer != null)
1:         {
0:             String[] s = outputReducer.split("=");
0:         if (inputMapper != null)
1:         {
0:             String[] s = inputMapper.split("=");
1:             if (s != null && s.length == 2)
0:                 inputMapperType = s[1];
1:         }
/////////////////////////////////////////////////////////////////////////
0:         if (inputMapperType.equalsIgnoreCase("native"))
1:         {
0:             job.setMapperClass(NativeTokenizerMapper.class);
0:             job.setInputFormatClass(CqlInputFormat.class);
0:             CqlConfigHelper.setInputCql(job.getConfiguration(), "select * from " + COLUMN_FAMILY + " where token(id) > ? and token(id) <= ? allow filtering");
1:         }
1:         else
1:         {
0:             job.setMapperClass(TokenizerMapper.class);
0:             job.setInputFormatClass(CqlPagingInputFormat.class);
0:             ConfigHelper.setInputRpcPort(job.getConfiguration(), "9160");
1:         }
author:Jonathan Ellis
-------------------------------------------------------------------------------
commit:b4f2ff1
/////////////////////////////////////////////////////////////////////////
0:  *   cql3_worldcount ( id uuid,
1:  *                   line  text,
1:  *                   PRIMARY KEY (id))
1:  *  output_words ( word text,
1:  *                 PRIMARY KEY (word))
/////////////////////////////////////////////////////////////////////////
1:                 if (!"line".equalsIgnoreCase(column.getKey()))
/////////////////////////////////////////////////////////////////////////
/////////////////////////////////////////////////////////////////////////
/////////////////////////////////////////////////////////////////////////
1:             keys.put("word", ByteBufferUtil.bytes(word.toString()));
/////////////////////////////////////////////////////////////////////////
1: }
commit:f1004e9
/////////////////////////////////////////////////////////////////////////
0:             keys.put("word", ByteBufferUtil.bytes(word.toString()));
/////////////////////////////////////////////////////////////////////////
1:             String query = "UPDATE " + KEYSPACE + "." + OUTPUT_COLUMN_FAMILY +
1:                            " SET count_num = ? ";
commit:a004779
/////////////////////////////////////////////////////////////////////////
1: import org.apache.cassandra.hadoop.cql3.CqlConfigHelper;
1: import org.apache.cassandra.hadoop.cql3.CqlOutputFormat;
0: import org.apache.cassandra.hadoop.cql3.CqlPagingInputFormat;
/////////////////////////////////////////////////////////////////////////
/////////////////////////////////////////////////////////////////////////
1:             job.setOutputFormatClass(CqlOutputFormat.class);
1:             CqlConfigHelper.setOutputCql(job.getConfiguration(), query);
0:         job.setInputFormatClass(CqlPagingInputFormat.class);
1:         CqlConfigHelper.setInputCQLPageRowSize(job.getConfiguration(), "3");
0:         CqlConfigHelper.setInputWhereClauses(job.getConfiguration(), "title='A'");
commit:56e0ad1
/////////////////////////////////////////////////////////////////////////
1: /**
1:  * Licensed to the Apache Software Foundation (ASF) under one
1:  * or more contributor license agreements.  See the NOTICE file
1:  * distributed with this work for additional information
1:  * regarding copyright ownership.  The ASF licenses this file
1:  * to you under the Apache License, Version 2.0 (the
1:  * "License"); you may not use this file except in compliance
1:  * with the License.  You may obtain a copy of the License at
1:  *
1:  *     http://www.apache.org/licenses/LICENSE-2.0
1:  *
1:  * Unless required by applicable law or agreed to in writing, software
1:  * distributed under the License is distributed on an "AS IS" BASIS,
1:  * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
1:  * See the License for the specific language governing permissions and
1:  * limitations under the License.
1:  */
1: 
1: import java.io.IOException;
1: import java.nio.ByteBuffer;
1: import java.util.*;
1: import java.util.Map.Entry;
1: 
0: import org.apache.cassandra.thrift.*;
0: import org.apache.cassandra.hadoop.cql3.ColumnFamilyOutputFormat;
1: import org.slf4j.Logger;
1: import org.slf4j.LoggerFactory;
1: 
0: import org.apache.cassandra.hadoop.cql3.CQLConfigHelper;
0: import org.apache.cassandra.hadoop.cql3.ColumnFamilyInputFormat;
1: import org.apache.cassandra.hadoop.ConfigHelper;
1: import org.apache.cassandra.utils.ByteBufferUtil;
1: import org.apache.hadoop.conf.Configuration;
1: import org.apache.hadoop.conf.Configured;
1: import org.apache.hadoop.fs.Path;
1: import org.apache.hadoop.io.IntWritable;
1: import org.apache.hadoop.io.Text;
1: import org.apache.hadoop.mapreduce.Job;
1: import org.apache.hadoop.mapreduce.Mapper;
1: import org.apache.hadoop.mapreduce.Reducer;
0: import org.apache.hadoop.mapreduce.Reducer.Context;
1: import org.apache.hadoop.mapreduce.lib.output.FileOutputFormat;
1: import org.apache.hadoop.util.Tool;
1: import org.apache.hadoop.util.ToolRunner;
1: 
0: import java.nio.charset.CharacterCodingException;
1: 
1: /**
1:  * This counts the occurrences of words in ColumnFamily
0:  *   cql3_worldcount ( user_id text,
0:  *                   category_id text,
0:  *                   sub_category_id text,
0:  *                   title  text,
0:  *                   body  text,
0:  *                   PRIMARY KEY (user_id, category_id, sub_category_id))
1:  *
1:  * For each word, we output the total number of occurrences across all body texts.
1:  *
1:  * When outputting to Cassandra, we write the word counts to column family
0:  *  output_words ( row_id1 text,
0:  *                 row_id2 text,
0:  *                 word text,
1:  *                 count_num text,
0:  *                 PRIMARY KEY ((row_id1, row_id2), word))
1:  * as a {word, count} to columns: word, count_num with a row key of "word sum"
1:  */
1: public class WordCount extends Configured implements Tool
1: {
1:     private static final Logger logger = LoggerFactory.getLogger(WordCount.class);
1: 
0:     static final String KEYSPACE = "cql3_worldcount";
1:     static final String COLUMN_FAMILY = "inputs";
1: 
1:     static final String OUTPUT_REDUCER_VAR = "output_reducer";
1:     static final String OUTPUT_COLUMN_FAMILY = "output_words";
1: 
1:     private static final String OUTPUT_PATH_PREFIX = "/tmp/word_count";
1: 
1:     private static final String PRIMARY_KEY = "row_key";
1: 
1:     public static void main(String[] args) throws Exception
1:     {
1:         // Let ToolRunner handle generic command-line options
1:         ToolRunner.run(new Configuration(), new WordCount(), args);
1:         System.exit(0);
1:     }
1: 
1:     public static class TokenizerMapper extends Mapper<Map<String, ByteBuffer>, Map<String, ByteBuffer>, Text, IntWritable>
1:     {
0:         private final static IntWritable one = new IntWritable(1);
0:         private Text word = new Text();
0:         private ByteBuffer sourceColumn;
1: 
0:         protected void setup(org.apache.hadoop.mapreduce.Mapper.Context context)
1:         throws IOException, InterruptedException
1:         {
1:         }
1: 
1:         public void map(Map<String, ByteBuffer> keys, Map<String, ByteBuffer> columns, Context context) throws IOException, InterruptedException
1:         {
1:             for (Entry<String, ByteBuffer> column : columns.entrySet())
1:             {
0:                 if (!"body".equalsIgnoreCase(column.getKey()))
1:                     continue;
1: 
1:                 String value = ByteBufferUtil.string(column.getValue());
1: 
0:                 logger.debug("read {}:{}={} from {}",
0:                              new Object[] {toString(keys), column.getKey(), value, context.getInputSplit()});
1: 
0:                 StringTokenizer itr = new StringTokenizer(value);
0:                 while (itr.hasMoreTokens())
1:                 {
0:                     word.set(itr.nextToken());
0:                     context.write(word, one);
1:                 }
1:             }
1:         }
1: 
0:         private String toString(Map<String, ByteBuffer> keys)
1:         {
0:             String result = "";
0:             try
1:             {
0:                 for (ByteBuffer key : keys.values())
0:                     result = result + ByteBufferUtil.string(key) + ":";
1:             }
0:             catch (CharacterCodingException e)
1:             {
0:                 logger.error("Failed to print keys", e);
1:             }
0:             return result;
1:         }
1:     }
1: 
1:     public static class ReducerToFilesystem extends Reducer<Text, IntWritable, Text, IntWritable>
1:     {
1:         public void reduce(Text key, Iterable<IntWritable> values, Context context) throws IOException, InterruptedException
1:         {
1:             int sum = 0;
1:             for (IntWritable val : values)
1:                 sum += val.get();
1:             context.write(key, new IntWritable(sum));
1:         }
1:     }
1: 
1:     public static class ReducerToCassandra extends Reducer<Text, IntWritable, Map<String, ByteBuffer>, List<ByteBuffer>>
1:     {
1:         private Map<String, ByteBuffer> keys;
1:         private ByteBuffer key;
1:         protected void setup(org.apache.hadoop.mapreduce.Reducer.Context context)
1:         throws IOException, InterruptedException
1:         {
1:             keys = new LinkedHashMap<String, ByteBuffer>();
0:             String[] partitionKeys = context.getConfiguration().get(PRIMARY_KEY).split(",");
0:             keys.put("row_id1", ByteBufferUtil.bytes(partitionKeys[0]));
0:             keys.put("row_id2", ByteBufferUtil.bytes(partitionKeys[1]));
1:         }
1: 
1:         public void reduce(Text word, Iterable<IntWritable> values, Context context) throws IOException, InterruptedException
1:         {
1:             int sum = 0;
1:             for (IntWritable val : values)
1:                 sum += val.get();
1:             context.write(keys, getBindVariables(word, sum));
1:         }
1: 
1:         private List<ByteBuffer> getBindVariables(Text word, int sum)
1:         {
1:             List<ByteBuffer> variables = new ArrayList<ByteBuffer>();
0:             variables.add(keys.get("row_id1"));
0:             variables.add(keys.get("row_id2"));
0:             variables.add(ByteBufferUtil.bytes(word.toString()));
1:             variables.add(ByteBufferUtil.bytes(String.valueOf(sum)));         
1:             return variables;
1:         }
1:     }
1: 
1:     public int run(String[] args) throws Exception
1:     {
1:         String outputReducerType = "filesystem";
0:         if (args != null && args[0].startsWith(OUTPUT_REDUCER_VAR))
1:         {
0:             String[] s = args[0].split("=");
0:             if (s != null && s.length == 2)
1:                 outputReducerType = s[1];
1:         }
1:         logger.info("output reducer type: " + outputReducerType);
1: 
1:         Job job = new Job(getConf(), "wordcount");
1:         job.setJarByClass(WordCount.class);
0:         job.setMapperClass(TokenizerMapper.class);
1: 
1:         if (outputReducerType.equalsIgnoreCase("filesystem"))
1:         {
1:             job.setCombinerClass(ReducerToFilesystem.class);
1:             job.setReducerClass(ReducerToFilesystem.class);
1:             job.setOutputKeyClass(Text.class);
1:             job.setOutputValueClass(IntWritable.class);
1:             FileOutputFormat.setOutputPath(job, new Path(OUTPUT_PATH_PREFIX));
1:         }
0:         else
1:         {
1:             job.setReducerClass(ReducerToCassandra.class);
1: 
1:             job.setMapOutputKeyClass(Text.class);
1:             job.setMapOutputValueClass(IntWritable.class);
1:             job.setOutputKeyClass(Map.class);
1:             job.setOutputValueClass(List.class);
1: 
0:             job.setOutputFormatClass(ColumnFamilyOutputFormat.class);
1: 
1:             ConfigHelper.setOutputColumnFamily(job.getConfiguration(), KEYSPACE, OUTPUT_COLUMN_FAMILY);
1:             job.getConfiguration().set(PRIMARY_KEY, "word,sum");
0:             String query = "INSERT INTO " + KEYSPACE + "." + OUTPUT_COLUMN_FAMILY +
0:                            " (row_id1, row_id2, word, count_num) " +
0:                            " values (?, ?, ?, ?)";
0:             CQLConfigHelper.setOutputCql(job.getConfiguration(), query);
1:             ConfigHelper.setOutputInitialAddress(job.getConfiguration(), "localhost");
1:             ConfigHelper.setOutputPartitioner(job.getConfiguration(), "Murmur3Partitioner");
1:         }
1: 
0:         job.setInputFormatClass(ColumnFamilyInputFormat.class);
1: 
0:         ConfigHelper.setInputRpcPort(job.getConfiguration(), "9160");
1:         ConfigHelper.setInputInitialAddress(job.getConfiguration(), "localhost");
1:         ConfigHelper.setInputColumnFamily(job.getConfiguration(), KEYSPACE, COLUMN_FAMILY);
1:         ConfigHelper.setInputPartitioner(job.getConfiguration(), "Murmur3Partitioner");
1: 
0:         CQLConfigHelper.setInputCQLPageRowSize(job.getConfiguration(), "3");
0:         //this is the user defined filter clauses, you can comment it out if you want count all titles
0:         CQLConfigHelper.setInputWhereClauses(job.getConfiguration(), "title='A'");
1:         job.waitForCompletion(true);
1:         return 0;
1:     }
1: }
============================================================================