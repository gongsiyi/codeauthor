1:5151169: /*
1:5151169:  * Licensed to the Apache Software Foundation (ASF) under one
1:5151169:  * or more contributor license agreements.  See the NOTICE file
1:5151169:  * distributed with this work for additional information
1:5151169:  * regarding copyright ownership.  The ASF licenses this file
1:5151169:  * to you under the Apache License, Version 2.0 (the
1:5151169:  * "License"); you may not use this file except in compliance
1:5151169:  * with the License.  You may obtain a copy of the License at
1:5151169:  *
1:5151169:  *     http://www.apache.org/licenses/LICENSE-2.0
1:5151169:  *
1:5151169:  * Unless required by applicable law or agreed to in writing, software
1:5151169:  * distributed under the License is distributed on an "AS IS" BASIS,
1:5151169:  * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
1:5151169:  * See the License for the specific language governing permissions and
1:5151169:  * limitations under the License.
1:5151169:  */
1:5151169: package org.apache.cassandra.streaming.compress;
1:5151169: 
1:5151169: import java.io.IOException;
1:5151169: import java.util.ArrayList;
1:5151169: import java.util.Collection;
1:5151169: import java.util.List;
1:5151169: 
1:582bdba: import org.slf4j.Logger;
1:582bdba: import org.slf4j.LoggerFactory;
1:582bdba: 
1:5151169: import org.apache.cassandra.io.compress.CompressionMetadata;
1:e2c6341: import org.apache.cassandra.io.sstable.Component;
1:0368e97: import org.apache.cassandra.io.sstable.format.SSTableReader;
1:4e29b7a: import org.apache.cassandra.io.util.ChannelProxy;
1:16499ca: import org.apache.cassandra.io.util.DataOutputStreamPlus;
1:5151169: import org.apache.cassandra.streaming.ProgressInfo;
1:5151169: import org.apache.cassandra.streaming.StreamSession;
1:5151169: import org.apache.cassandra.streaming.StreamWriter;
1:db68ac9: import org.apache.cassandra.utils.FBUtilities;
1:5151169: import org.apache.cassandra.utils.Pair;
1:5151169: 
1:5151169: /**
1:5151169:  * StreamWriter for compressed SSTable.
1:5151169:  */
1:5151169: public class CompressedStreamWriter extends StreamWriter
2:5151169: {
1:5151169:     public static final int CHUNK_SIZE = 10 * 1024 * 1024;
1:5151169: 
1:582bdba:     private static final Logger logger = LoggerFactory.getLogger(CompressedStreamWriter.class);
1:582bdba: 
1:5151169:     private final CompressionInfo compressionInfo;
1:5151169: 
1:5151169:     public CompressedStreamWriter(SSTableReader sstable, Collection<Pair<Long, Long>> sections, CompressionInfo compressionInfo, StreamSession session)
1:5151169:     {
1:5151169:         super(sstable, sections, session);
1:5151169:         this.compressionInfo = compressionInfo;
2:5151169:     }
1:5151169: 
1:5151169:     @Override
1:16499ca:     public void write(DataOutputStreamPlus out) throws IOException
1:5151169:     {
1:5151169:         long totalSize = totalSize();
1:582bdba:         logger.debug("[Stream #{}] Start streaming file {} to {}, repairedAt = {}, totalSize = {}", session.planId(),
1:582bdba:                      sstable.getFilename(), session.peer, sstable.getSSTableMetadata().repairedAt, totalSize);
1:ce63ccc:         try (ChannelProxy fc = sstable.getDataChannel().sharedCopy())
1:5151169:         {
1:5151169:             long progress = 0L;
1:5151169:             // calculate chunks to transfer. we want to send continuous chunks altogether.
1:5151169:             List<Pair<Long, Long>> sections = getTransferSections(compressionInfo.chunks);
1:5151169: 
1:582bdba:             int sectionIdx = 0;
3:582bdba: 
1:5151169:             // stream each of the required sections of the file
1:16499ca:             for (final Pair<Long, Long> section : sections)
1:5151169:             {
1:5151169:                 // length of the section to stream
1:5151169:                 long length = section.right - section.left;
1:5151169: 
1:582bdba:                 logger.trace("[Stream #{}] Writing section {} with length {} to stream.", session.planId(), sectionIdx++, length);
1:582bdba: 
1:5151169:                 // tracks write progress
1:5151169:                 long bytesTransferred = 0;
1:5151169:                 while (bytesTransferred < length)
1:5151169:                 {
1:16499ca:                     final long bytesTransferredFinal = bytesTransferred;
1:16499ca:                     final int toTransfer = (int) Math.min(CHUNK_SIZE, length - bytesTransferred);
1:5151169:                     limiter.acquire(toTransfer);
1:ce63ccc:                     long lastWrite = out.applyToChannel((wbc) -> fc.transferTo(section.left + bytesTransferredFinal, toTransfer, wbc));
1:5151169:                     bytesTransferred += lastWrite;
1:5151169:                     progress += lastWrite;
1:e2c6341:                     session.progress(sstable.descriptor.filenameFor(Component.DATA), ProgressInfo.Direction.OUT, progress, totalSize);
1:5151169:                 }
1:5151169:             }
1:582bdba:             logger.debug("[Stream #{}] Finished streaming file {} to {}, bytesTransferred = {}, totalSize = {}",
1:db68ac9:                          session.planId(), sstable.getFilename(), session.peer, FBUtilities.prettyPrintMemory(progress), FBUtilities.prettyPrintMemory(totalSize));
1:5151169:         }
1:5151169:     }
1:5151169: 
1:5151169:     @Override
1:5151169:     protected long totalSize()
1:5151169:     {
1:5151169:         long size = 0;
1:5151169:         // calculate total length of transferring chunks
1:5151169:         for (CompressionMetadata.Chunk chunk : compressionInfo.chunks)
1:5151169:             size += chunk.length + 4; // 4 bytes for CRC
1:5151169:         return size;
1:5151169:     }
1:5151169: 
1:5151169:     // chunks are assumed to be sorted by offset
1:5151169:     private List<Pair<Long, Long>> getTransferSections(CompressionMetadata.Chunk[] chunks)
1:5151169:     {
1:5151169:         List<Pair<Long, Long>> transferSections = new ArrayList<>();
1:5151169:         Pair<Long, Long> lastSection = null;
1:5151169:         for (CompressionMetadata.Chunk chunk : chunks)
1:5151169:         {
1:5151169:             if (lastSection != null)
1:5151169:             {
1:5151169:                 if (chunk.offset == lastSection.right)
1:5151169:                 {
1:5151169:                     // extend previous section to end of this chunk
1:5151169:                     lastSection = Pair.create(lastSection.left, chunk.offset + chunk.length + 4); // 4 bytes for CRC
1:5151169:                 }
1:5151169:                 else
1:5151169:                 {
1:5151169:                     transferSections.add(lastSection);
1:5151169:                     lastSection = Pair.create(chunk.offset, chunk.offset + chunk.length + 4);
1:5151169:                 }
1:5151169:             }
1:5151169:             else
1:5151169:             {
1:5151169:                 lastSection = Pair.create(chunk.offset, chunk.offset + chunk.length + 4);
1:5151169:             }
1:5151169:         }
1:5151169:         if (lastSection != null)
1:5151169:             transferSections.add(lastSection);
1:5151169:         return transferSections;
1:5151169:     }
1:5151169: }
============================================================================
author:Dave Brosius
-------------------------------------------------------------------------------
commit:087264f
/////////////////////////////////////////////////////////////////////////
/////////////////////////////////////////////////////////////////////////
author:Giampaolo Trapasso
-------------------------------------------------------------------------------
commit:db68ac9
/////////////////////////////////////////////////////////////////////////
1: import org.apache.cassandra.utils.FBUtilities;
/////////////////////////////////////////////////////////////////////////
1:                          session.planId(), sstable.getFilename(), session.peer, FBUtilities.prettyPrintMemory(progress), FBUtilities.prettyPrintMemory(totalSize));
author:Yuki Morishita
-------------------------------------------------------------------------------
commit:a7feb80
commit:0f995a2
commit:04fd84c
/////////////////////////////////////////////////////////////////////////
commit:5151169
/////////////////////////////////////////////////////////////////////////
1: /*
1:  * Licensed to the Apache Software Foundation (ASF) under one
1:  * or more contributor license agreements.  See the NOTICE file
1:  * distributed with this work for additional information
1:  * regarding copyright ownership.  The ASF licenses this file
1:  * to you under the Apache License, Version 2.0 (the
1:  * "License"); you may not use this file except in compliance
1:  * with the License.  You may obtain a copy of the License at
1:  *
1:  *     http://www.apache.org/licenses/LICENSE-2.0
1:  *
1:  * Unless required by applicable law or agreed to in writing, software
1:  * distributed under the License is distributed on an "AS IS" BASIS,
1:  * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
1:  * See the License for the specific language governing permissions and
1:  * limitations under the License.
1:  */
1: package org.apache.cassandra.streaming.compress;
1: 
1: import java.io.IOException;
0: import java.nio.channels.FileChannel;
0: import java.nio.channels.WritableByteChannel;
1: import java.util.ArrayList;
1: import java.util.Collection;
1: import java.util.List;
1: 
1: import org.apache.cassandra.io.compress.CompressionMetadata;
0: import org.apache.cassandra.io.sstable.SSTableReader;
0: import org.apache.cassandra.io.util.FileUtils;
0: import org.apache.cassandra.io.util.RandomAccessReader;
1: import org.apache.cassandra.streaming.ProgressInfo;
1: import org.apache.cassandra.streaming.StreamSession;
1: import org.apache.cassandra.streaming.StreamWriter;
1: import org.apache.cassandra.utils.Pair;
1: 
1: /**
1:  * StreamWriter for compressed SSTable.
1:  */
1: public class CompressedStreamWriter extends StreamWriter
1: {
1:     public static final int CHUNK_SIZE = 10 * 1024 * 1024;
1: 
1:     private final CompressionInfo compressionInfo;
1: 
1:     public CompressedStreamWriter(SSTableReader sstable, Collection<Pair<Long, Long>> sections, CompressionInfo compressionInfo, StreamSession session)
1:     {
1:         super(sstable, sections, session);
1:         this.compressionInfo = compressionInfo;
1:     }
1: 
1:     @Override
0:     public void write(WritableByteChannel channel) throws IOException
1:     {
1:         long totalSize = totalSize();
0:         RandomAccessReader file = sstable.openDataReader();
0:         FileChannel fc = file.getChannel();
1: 
1:         long progress = 0L;
1:         // calculate chunks to transfer. we want to send continuous chunks altogether.
1:         List<Pair<Long, Long>> sections = getTransferSections(compressionInfo.chunks);
0:         try
1:         {
1:             // stream each of the required sections of the file
0:             for (Pair<Long, Long> section : sections)
1:             {
1:                 // length of the section to stream
1:                 long length = section.right - section.left;
1:                 // tracks write progress
1:                 long bytesTransferred = 0;
1:                 while (bytesTransferred < length)
1:                 {
0:                     int toTransfer = (int) Math.min(CHUNK_SIZE, length - bytesTransferred);
1:                     limiter.acquire(toTransfer);
0:                     long lastWrite = fc.transferTo(section.left + bytesTransferred, toTransfer, channel);
1:                     bytesTransferred += lastWrite;
1:                     progress += lastWrite;
0:                     session.progress(sstable.descriptor, ProgressInfo.Direction.OUT, progress, totalSize);
1:                 }
1:             }
1:         }
0:         finally
1:         {
0:             // no matter what happens close file
0:             FileUtils.closeQuietly(file);
1:         }
1: 
0:         sstable.releaseReference();
1:     }
1: 
1:     @Override
1:     protected long totalSize()
1:     {
1:         long size = 0;
1:         // calculate total length of transferring chunks
1:         for (CompressionMetadata.Chunk chunk : compressionInfo.chunks)
1:             size += chunk.length + 4; // 4 bytes for CRC
1:         return size;
1:     }
1: 
1:     // chunks are assumed to be sorted by offset
1:     private List<Pair<Long, Long>> getTransferSections(CompressionMetadata.Chunk[] chunks)
1:     {
1:         List<Pair<Long, Long>> transferSections = new ArrayList<>();
1:         Pair<Long, Long> lastSection = null;
1:         for (CompressionMetadata.Chunk chunk : chunks)
1:         {
1:             if (lastSection != null)
1:             {
1:                 if (chunk.offset == lastSection.right)
1:                 {
1:                     // extend previous section to end of this chunk
1:                     lastSection = Pair.create(lastSection.left, chunk.offset + chunk.length + 4); // 4 bytes for CRC
1:                 }
1:                 else
1:                 {
1:                     transferSections.add(lastSection);
1:                     lastSection = Pair.create(chunk.offset, chunk.offset + chunk.length + 4);
1:                 }
1:             }
1:             else
1:             {
1:                 lastSection = Pair.create(chunk.offset, chunk.offset + chunk.length + 4);
1:             }
1:         }
1:         if (lastSection != null)
1:             transferSections.add(lastSection);
1:         return transferSections;
1:     }
1: }
author:Paulo Motta
-------------------------------------------------------------------------------
commit:582bdba
/////////////////////////////////////////////////////////////////////////
1: import org.slf4j.Logger;
1: import org.slf4j.LoggerFactory;
1: 
/////////////////////////////////////////////////////////////////////////
1:     private static final Logger logger = LoggerFactory.getLogger(CompressedStreamWriter.class);
1: 
/////////////////////////////////////////////////////////////////////////
1:         logger.debug("[Stream #{}] Start streaming file {} to {}, repairedAt = {}, totalSize = {}", session.planId(),
1:                      sstable.getFilename(), session.peer, sstable.getSSTableMetadata().repairedAt, totalSize);
1: 
1:             int sectionIdx = 0;
1: 
1: 
1:                 logger.trace("[Stream #{}] Writing section {} with length {} to stream.", session.planId(), sectionIdx++, length);
1: 
/////////////////////////////////////////////////////////////////////////
1:             logger.debug("[Stream #{}] Finished streaming file {} to {}, bytesTransferred = {}, totalSize = {}",
0:                          session.planId(), sstable.getFilename(), session.peer, progress, totalSize);
author:Marcus Eriksson
-------------------------------------------------------------------------------
commit:e2c6341
/////////////////////////////////////////////////////////////////////////
1: import org.apache.cassandra.io.sstable.Component;
/////////////////////////////////////////////////////////////////////////
1:                     session.progress(sstable.descriptor.filenameFor(Component.DATA), ProgressInfo.Direction.OUT, progress, totalSize);
author:Stefania Alborghetti
-------------------------------------------------------------------------------
commit:ce63ccc
/////////////////////////////////////////////////////////////////////////
1:         try (ChannelProxy fc = sstable.getDataChannel().sharedCopy())
/////////////////////////////////////////////////////////////////////////
1:                     long lastWrite = out.applyToChannel((wbc) -> fc.transferTo(section.left + bytesTransferredFinal, toTransfer, wbc));
commit:4e29b7a
/////////////////////////////////////////////////////////////////////////
/////////////////////////////////////////////////////////////////////////
1: import org.apache.cassandra.io.util.ChannelProxy;
/////////////////////////////////////////////////////////////////////////
0:         final ChannelProxy fc = file.getChannel();
/////////////////////////////////////////////////////////////////////////
0:                             return fc.transferTo(section.left + bytesTransferredFinal, toTransfer, wbc);
author:stefania
-------------------------------------------------------------------------------
commit:17dd4cc
/////////////////////////////////////////////////////////////////////////
0:         try (RandomAccessReader file = sstable.openDataReader(); final ChannelProxy fc = file.getChannel().sharedCopy())
author:T Jake Luciani
-------------------------------------------------------------------------------
commit:7aafe05
/////////////////////////////////////////////////////////////////////////
0:         try (RandomAccessReader file = sstable.openDataReader(); final ChannelProxy fc = file.getChannel())
0:             long progress = 0L;
0:             // calculate chunks to transfer. we want to send continuous chunks altogether.
0:             List<Pair<Long, Long>> sections = getTransferSections(compressionInfo.chunks);
/////////////////////////////////////////////////////////////////////////
0:                     long lastWrite = out.applyToChannel(new Function<WritableByteChannel, Long>()
/////////////////////////////////////////////////////////////////////////
author:Ariel Weisberg
-------------------------------------------------------------------------------
commit:16499ca
/////////////////////////////////////////////////////////////////////////
0: import com.google.common.base.Function;
0: 
0: import org.apache.cassandra.io.FSWriteError;
1: import org.apache.cassandra.io.util.DataOutputStreamPlus;
/////////////////////////////////////////////////////////////////////////
1:     public void write(DataOutputStreamPlus out) throws IOException
0:         final FileChannel fc = file.getChannel();
/////////////////////////////////////////////////////////////////////////
1:             for (final Pair<Long, Long> section : sections)
/////////////////////////////////////////////////////////////////////////
1:                     final long bytesTransferredFinal = bytesTransferred;
1:                     final int toTransfer = (int) Math.min(CHUNK_SIZE, length - bytesTransferred);
0:                     long lastWrite = out.applyToChannel( new Function<WritableByteChannel, Long>()
0:                     {
0:                         public Long apply(WritableByteChannel wbc)
0:                         {
0:                             try
0:                             {
0:                                 return fc.transferTo(section.left + bytesTransferredFinal, toTransfer, wbc);
0:                             }
0:                             catch (IOException e)
0:                             {
0:                                 throw new FSWriteError(e, sstable.getFilename());
0:                             }
0:                         }
0:                     });
author:Jake Luciani
-------------------------------------------------------------------------------
commit:0368e97
/////////////////////////////////////////////////////////////////////////
1: import org.apache.cassandra.io.sstable.format.SSTableReader;
============================================================================