1:72790dc: /*
1:72790dc:  * Licensed to the Apache Software Foundation (ASF) under one
1:72790dc:  * or more contributor license agreements.  See the NOTICE file
1:72790dc:  * distributed with this work for additional information
1:72790dc:  * regarding copyright ownership.  The ASF licenses this file
1:72790dc:  * to you under the Apache License, Version 2.0 (the
1:72790dc:  * "License"); you may not use this file except in compliance
1:72790dc:  * with the License.  You may obtain a copy of the License at
1:72790dc:  *
1:72790dc:  *     http://www.apache.org/licenses/LICENSE-2.0
1:72790dc:  *
1:72790dc:  * Unless required by applicable law or agreed to in writing, software
1:72790dc:  * distributed under the License is distributed on an "AS IS" BASIS,
1:72790dc:  * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
1:72790dc:  * See the License for the specific language governing permissions and
1:72790dc:  * limitations under the License.
1:72790dc:  */
1:72790dc: package org.apache.cassandra.index.sasi.disk;
1:72790dc: 
1:72790dc: import java.io.*;
1:72790dc: import java.nio.ByteBuffer;
1:72790dc: import java.util.*;
1:72790dc: import java.util.stream.Collectors;
1:72790dc: 
1:7d857b4: import org.apache.cassandra.index.sasi.*;
1:72790dc: import org.apache.cassandra.index.sasi.plan.Expression;
1:72790dc: import org.apache.cassandra.index.sasi.plan.Expression.Op;
1:72790dc: import org.apache.cassandra.index.sasi.utils.MappedBuffer;
1:72790dc: import org.apache.cassandra.index.sasi.utils.RangeUnionIterator;
1:72790dc: import org.apache.cassandra.index.sasi.utils.AbstractIterator;
1:72790dc: import org.apache.cassandra.index.sasi.utils.RangeIterator;
1:72790dc: import org.apache.cassandra.db.marshal.AbstractType;
1:72790dc: import org.apache.cassandra.io.FSReadError;
1:72790dc: import org.apache.cassandra.io.util.ChannelProxy;
1:72790dc: import org.apache.cassandra.io.util.FileUtils;
1:72790dc: import org.apache.cassandra.utils.ByteBufferUtil;
1:72790dc: import org.apache.cassandra.utils.FBUtilities;
1:72790dc: 
1:72790dc: import com.google.common.collect.Iterables;
1:72790dc: import com.google.common.collect.Iterators;
1:72790dc: import com.google.common.collect.PeekingIterator;
1:72790dc: 
1:72790dc: import static org.apache.cassandra.index.sasi.disk.OnDiskBlock.SearchResult;
1:7d857b4: import static org.apache.cassandra.index.sasi.disk.TokenTreeBuilder.TOKEN_BYTES;
1:72790dc: 
1:72790dc: public class OnDiskIndex implements Iterable<OnDiskIndex.DataTerm>, Closeable
1:72790dc: {
1:72790dc:     public enum IteratorOrder
1:72790dc:     {
1:72790dc:         DESC(1), ASC(-1);
1:72790dc: 
1:72790dc:         public final int step;
1:72790dc: 
1:72790dc:         IteratorOrder(int step)
1:72790dc:         {
1:72790dc:             this.step = step;
1:72790dc:         }
1:72790dc: 
1:72790dc:         public int startAt(OnDiskBlock<DataTerm> block, Expression e)
1:72790dc:         {
1:72790dc:             switch (this)
1:72790dc:             {
1:72790dc:                 case DESC:
1:72790dc:                     return e.lower == null
1:72790dc:                             ? 0
1:72790dc:                             : startAt(block.search(e.validator, e.lower.value), e.lower.inclusive);
1:72790dc: 
1:72790dc:                 case ASC:
1:72790dc:                     return e.upper == null
1:72790dc:                             ? block.termCount() - 1
1:72790dc:                             : startAt(block.search(e.validator, e.upper.value), e.upper.inclusive);
1:72790dc: 
1:72790dc:                 default:
1:72790dc:                     throw new IllegalArgumentException("Unknown order: " + this);
1:72790dc:             }
1:72790dc:         }
1:72790dc: 
1:72790dc:         public int startAt(SearchResult<DataTerm> found, boolean inclusive)
1:72790dc:         {
1:72790dc:             switch (this)
1:72790dc:             {
1:72790dc:                 case DESC:
1:72790dc:                     if (found.cmp < 0)
1:72790dc:                         return found.index + 1;
1:72790dc: 
1:72790dc:                     return inclusive || found.cmp != 0 ? found.index : found.index + 1;
1:72790dc: 
1:72790dc:                 case ASC:
1:72790dc:                     if (found.cmp < 0) // search term was bigger then whole data set
1:72790dc:                         return found.index;
1:72790dc:                     return inclusive && (found.cmp == 0 || found.cmp < 0) ? found.index : found.index - 1;
1:72790dc: 
1:72790dc:                 default:
1:72790dc:                     throw new IllegalArgumentException("Unknown order: " + this);
1:72790dc:             }
1:72790dc:         }
1:72790dc:     }
1:72790dc: 
1:72790dc:     public final Descriptor descriptor;
1:72790dc:     protected final OnDiskIndexBuilder.Mode mode;
1:72790dc:     protected final OnDiskIndexBuilder.TermSize termSize;
1:72790dc: 
1:72790dc:     protected final AbstractType<?> comparator;
1:72790dc:     protected final MappedBuffer indexFile;
1:72790dc:     protected final long indexSize;
1:2ca2fff:     protected final boolean hasMarkedPartials;
1:72790dc: 
1:7d857b4:     protected final KeyFetcher keyFetcher;
1:72790dc: 
1:72790dc:     protected final String indexPath;
1:72790dc: 
1:72790dc:     protected final PointerLevel[] levels;
1:72790dc:     protected final DataLevel dataLevel;
1:72790dc: 
1:72790dc:     protected final ByteBuffer minTerm, maxTerm, minKey, maxKey;
1:72790dc: 
1:733d1ee:     @SuppressWarnings("resource")
1:7d857b4:     public OnDiskIndex(File index, AbstractType<?> cmp, KeyFetcher keyReader)
1:72790dc:     {
1:72790dc:         keyFetcher = keyReader;
1:72790dc: 
1:72790dc:         comparator = cmp;
1:72790dc:         indexPath = index.getAbsolutePath();
1:72790dc: 
1:72790dc:         RandomAccessFile backingFile = null;
1:72790dc:         try
1:72790dc:         {
1:72790dc:             backingFile = new RandomAccessFile(index, "r");
1:72790dc: 
1:72790dc:             descriptor = new Descriptor(backingFile.readUTF());
1:72790dc: 
1:72790dc:             termSize = OnDiskIndexBuilder.TermSize.of(backingFile.readShort());
1:72790dc: 
1:72790dc:             minTerm = ByteBufferUtil.readWithShortLength(backingFile);
1:72790dc:             maxTerm = ByteBufferUtil.readWithShortLength(backingFile);
1:72790dc: 
1:72790dc:             minKey = ByteBufferUtil.readWithShortLength(backingFile);
1:72790dc:             maxKey = ByteBufferUtil.readWithShortLength(backingFile);
1:72790dc: 
1:72790dc:             mode = OnDiskIndexBuilder.Mode.mode(backingFile.readUTF());
1:2ca2fff:             hasMarkedPartials = backingFile.readBoolean();
1:72790dc: 
1:72790dc:             indexSize = backingFile.length();
1:72790dc:             indexFile = new MappedBuffer(new ChannelProxy(indexPath, backingFile.getChannel()));
1:72790dc: 
1:72790dc:             // start of the levels
1:72790dc:             indexFile.position(indexFile.getLong(indexSize - 8));
1:72790dc: 
1:72790dc:             int numLevels = indexFile.getInt();
1:72790dc:             levels = new PointerLevel[numLevels];
1:72790dc:             for (int i = 0; i < levels.length; i++)
1:72790dc:             {
1:72790dc:                 int blockCount = indexFile.getInt();
1:72790dc:                 levels[i] = new PointerLevel(indexFile.position(), blockCount);
1:72790dc:                 indexFile.position(indexFile.position() + blockCount * 8);
1:72790dc:             }
1:72790dc: 
1:72790dc:             int blockCount = indexFile.getInt();
1:72790dc:             dataLevel = new DataLevel(indexFile.position(), blockCount);
1:72790dc:         }
1:72790dc:         catch (IOException e)
1:72790dc:         {
1:72790dc:             throw new FSReadError(e, index);
1:72790dc:         }
1:72790dc:         finally
1:72790dc:         {
1:72790dc:             FileUtils.closeQuietly(backingFile);
1:72790dc:         }
1:72790dc:     }
1:72790dc: 
1:2ca2fff:     public boolean hasMarkedPartials()
1:2ca2fff:     {
1:2ca2fff:         return hasMarkedPartials;
1:2ca2fff:     }
1:2ca2fff: 
1:2ca2fff:     public OnDiskIndexBuilder.Mode mode()
1:2ca2fff:     {
1:2ca2fff:         return mode;
1:2ca2fff:     }
1:2ca2fff: 
1:72790dc:     public ByteBuffer minTerm()
1:72790dc:     {
1:72790dc:         return minTerm;
1:72790dc:     }
1:72790dc: 
1:72790dc:     public ByteBuffer maxTerm()
1:72790dc:     {
1:72790dc:         return maxTerm;
1:72790dc:     }
1:72790dc: 
1:72790dc:     public ByteBuffer minKey()
1:72790dc:     {
1:72790dc:         return minKey;
1:72790dc:     }
1:72790dc: 
1:72790dc:     public ByteBuffer maxKey()
1:72790dc:     {
1:72790dc:         return maxKey;
1:72790dc:     }
1:72790dc: 
1:72790dc:     public DataTerm min()
1:72790dc:     {
1:72790dc:         return dataLevel.getBlock(0).getTerm(0);
1:72790dc:     }
1:72790dc: 
1:72790dc:     public DataTerm max()
1:72790dc:     {
1:72790dc:         DataBlock block = dataLevel.getBlock(dataLevel.blockCount - 1);
1:72790dc:         return block.getTerm(block.termCount() - 1);
1:72790dc:     }
1:72790dc: 
1:72790dc:     /**
1:72790dc:      * Search for rows which match all of the terms inside the given expression in the index file.
1:72790dc:      *
1:72790dc:      * @param exp The expression to use for the query.
1:72790dc:      *
1:72790dc:      * @return Iterator which contains rows for all of the terms from the given range.
1:72790dc:      */
1:72790dc:     public RangeIterator<Long, Token> search(Expression exp)
1:72790dc:     {
1:3928665:         assert mode.supports(exp.getOp());
1:3928665: 
1:2ca2fff:         if (exp.getOp() == Expression.Op.PREFIX && mode == OnDiskIndexBuilder.Mode.CONTAINS && !hasMarkedPartials)
1:2ca2fff:             throw new UnsupportedOperationException("prefix queries in CONTAINS mode are not supported by this index");
1:2ca2fff: 
1:3928665:         // optimization in case single term is requested from index
1:3928665:         // we don't really need to build additional union iterator
1:3928665:         if (exp.getOp() == Op.EQ)
1:3928665:         {
1:3928665:             DataTerm term = getTerm(exp.lower.value);
1:3928665:             return term == null ? null : term.getTokens();
1:3928665:         }
1:3928665: 
1:72790dc:         // convert single NOT_EQ to range with exclusion
1:72790dc:         final Expression expression = (exp.getOp() != Op.NOT_EQ)
1:72790dc:                                         ? exp
1:72790dc:                                         : new Expression(exp).setOp(Op.RANGE)
1:72790dc:                                                 .setLower(new Expression.Bound(minTerm, true))
1:72790dc:                                                 .setUpper(new Expression.Bound(maxTerm, true))
1:72790dc:                                                 .addExclusion(exp.lower.value);
1:72790dc: 
1:72790dc:         List<ByteBuffer> exclusions = new ArrayList<>(expression.exclusions.size());
1:72790dc: 
1:72790dc:         Iterables.addAll(exclusions, expression.exclusions.stream().filter(exclusion -> {
1:72790dc:             // accept only exclusions which are in the bounds of lower/upper
1:72790dc:             return !(expression.lower != null && comparator.compare(exclusion, expression.lower.value) < 0)
1:72790dc:                 && !(expression.upper != null && comparator.compare(exclusion, expression.upper.value) > 0);
1:72790dc:         }).collect(Collectors.toList()));
1:72790dc: 
1:72790dc:         Collections.sort(exclusions, comparator);
1:72790dc: 
1:72790dc:         if (exclusions.size() == 0)
1:72790dc:             return searchRange(expression);
1:72790dc: 
1:72790dc:         List<Expression> ranges = new ArrayList<>(exclusions.size());
1:72790dc: 
1:72790dc:         // calculate range splits based on the sorted exclusions
1:72790dc:         Iterator<ByteBuffer> exclusionsIterator = exclusions.iterator();
1:72790dc: 
1:72790dc:         Expression.Bound min = expression.lower, max = null;
1:72790dc:         while (exclusionsIterator.hasNext())
1:72790dc:         {
1:72790dc:             max = new Expression.Bound(exclusionsIterator.next(), false);
1:72790dc:             ranges.add(new Expression(expression).setOp(Op.RANGE).setLower(min).setUpper(max));
1:72790dc:             min = max;
1:72790dc:         }
1:72790dc: 
1:72790dc:         assert max != null;
1:72790dc:         ranges.add(new Expression(expression).setOp(Op.RANGE).setLower(max).setUpper(expression.upper));
1:72790dc: 
1:72790dc:         RangeUnionIterator.Builder<Long, Token> builder = RangeUnionIterator.builder();
1:72790dc:         for (Expression e : ranges)
1:72790dc:         {
1:733d1ee:             @SuppressWarnings("resource")
1:72790dc:             RangeIterator<Long, Token> range = searchRange(e);
1:72790dc:             if (range != null)
1:72790dc:                 builder.add(range);
1:72790dc:         }
1:72790dc: 
1:72790dc:         return builder.build();
1:72790dc:     }
1:72790dc: 
1:72790dc:     private RangeIterator<Long, Token> searchRange(Expression range)
1:72790dc:     {
1:72790dc:         Expression.Bound lower = range.lower;
1:72790dc:         Expression.Bound upper = range.upper;
1:72790dc: 
1:72790dc:         int lowerBlock = lower == null ? 0 : getDataBlock(lower.value);
1:72790dc:         int upperBlock = upper == null
1:72790dc:                 ? dataLevel.blockCount - 1
1:72790dc:                 // optimization so we don't have to fetch upperBlock when query has lower == upper
1:72790dc:                 : (lower != null && comparator.compare(lower.value, upper.value) == 0) ? lowerBlock : getDataBlock(upper.value);
1:72790dc: 
1:72790dc:         return (mode != OnDiskIndexBuilder.Mode.SPARSE || lowerBlock == upperBlock || upperBlock - lowerBlock <= 1)
1:72790dc:                 ? searchPoint(lowerBlock, range)
1:72790dc:                 : searchRange(lowerBlock, lower, upperBlock, upper);
1:72790dc:     }
1:72790dc: 
1:72790dc:     private RangeIterator<Long, Token> searchRange(int lowerBlock, Expression.Bound lower, int upperBlock, Expression.Bound upper)
1:72790dc:     {
1:72790dc:         // if lower is at the beginning of the block that means we can just do a single iterator per block
1:72790dc:         SearchResult<DataTerm> lowerPosition = (lower == null) ? null : searchIndex(lower.value, lowerBlock);
1:72790dc:         SearchResult<DataTerm> upperPosition = (upper == null) ? null : searchIndex(upper.value, upperBlock);
1:72790dc: 
1:72790dc:         RangeUnionIterator.Builder<Long, Token> builder = RangeUnionIterator.builder();
1:72790dc: 
1:72790dc:         // optimistically assume that first and last blocks are full block reads, saves at least 3 'else' conditions
1:72790dc:         int firstFullBlockIdx = lowerBlock, lastFullBlockIdx = upperBlock;
1:72790dc: 
1:72790dc:         // 'lower' doesn't cover the whole block so we need to do a partial iteration
1:72790dc:         // Two reasons why that can happen:
1:72790dc:         //   - 'lower' is not the first element of the block
1:72790dc:         //   - 'lower' is first element but it's not inclusive in the query
1:72790dc:         if (lowerPosition != null && (lowerPosition.index > 0 || !lower.inclusive))
1:72790dc:         {
1:72790dc:             DataBlock block = dataLevel.getBlock(lowerBlock);
1:72790dc:             int start = (lower.inclusive || lowerPosition.cmp != 0) ? lowerPosition.index : lowerPosition.index + 1;
1:72790dc: 
1:72790dc:             builder.add(block.getRange(start, block.termCount()));
1:72790dc:             firstFullBlockIdx = lowerBlock + 1;
1:72790dc:         }
1:72790dc: 
1:72790dc:         if (upperPosition != null)
1:72790dc:         {
1:72790dc:             DataBlock block = dataLevel.getBlock(upperBlock);
1:72790dc:             int lastIndex = block.termCount() - 1;
1:72790dc: 
1:72790dc:             // The save as with 'lower' but here we need to check if the upper is the last element of the block,
1:72790dc:             // which means that we only have to get individual results if:
1:72790dc:             //  - if it *is not* the last element, or
1:72790dc:             //  - it *is* but shouldn't be included (dictated by upperInclusive)
1:72790dc:             if (upperPosition.index != lastIndex || !upper.inclusive)
1:72790dc:             {
1:72790dc:                 int end = (upperPosition.cmp < 0 || (upperPosition.cmp == 0 && upper.inclusive))
1:72790dc:                                 ? upperPosition.index + 1 : upperPosition.index;
1:72790dc: 
1:72790dc:                 builder.add(block.getRange(0, end));
1:72790dc:                 lastFullBlockIdx = upperBlock - 1;
1:72790dc:             }
1:72790dc:         }
1:72790dc: 
1:72790dc:         int totalSuperBlocks = (lastFullBlockIdx - firstFullBlockIdx) / OnDiskIndexBuilder.SUPER_BLOCK_SIZE;
1:72790dc: 
1:72790dc:         // if there are no super-blocks, we can simply read all of the block iterators in sequence
1:72790dc:         if (totalSuperBlocks == 0)
1:72790dc:         {
1:72790dc:             for (int i = firstFullBlockIdx; i <= lastFullBlockIdx; i++)
1:72790dc:                 builder.add(dataLevel.getBlock(i).getBlockIndex().iterator(keyFetcher));
1:72790dc: 
1:72790dc:             return builder.build();
1:72790dc:         }
1:72790dc: 
1:72790dc:         // first get all of the blocks which are aligned before the first super-block in the sequence,
1:72790dc:         // e.g. if the block range was (1, 9) and super-block-size = 4, we need to read 1, 2, 3, 4 - 7 is covered by
1:72790dc:         // super-block, 8, 9 is a remainder.
1:72790dc: 
1:72790dc:         int superBlockAlignedStart = firstFullBlockIdx == 0 ? 0 : (int) FBUtilities.align(firstFullBlockIdx, OnDiskIndexBuilder.SUPER_BLOCK_SIZE);
1:72790dc:         for (int blockIdx = firstFullBlockIdx; blockIdx < Math.min(superBlockAlignedStart, lastFullBlockIdx); blockIdx++)
1:72790dc:             builder.add(getBlockIterator(blockIdx));
1:72790dc: 
1:72790dc:         // now read all of the super-blocks matched by the request, from the previous comment
1:72790dc:         // it's a block with index 1 (which covers everything from 4 to 7)
1:72790dc: 
1:72790dc:         int superBlockIdx = superBlockAlignedStart / OnDiskIndexBuilder.SUPER_BLOCK_SIZE;
1:72790dc:         for (int offset = 0; offset < totalSuperBlocks - 1; offset++)
1:72790dc:             builder.add(dataLevel.getSuperBlock(superBlockIdx++).iterator());
1:72790dc: 
1:72790dc:         // now it's time for a remainder read, again from the previous example it's 8, 9 because
1:72790dc:         // we have over-shot previous block but didn't request enough to cover next super-block.
1:72790dc: 
1:72790dc:         int lastCoveredBlock = superBlockIdx * OnDiskIndexBuilder.SUPER_BLOCK_SIZE;
1:72790dc:         for (int offset = 0; offset <= (lastFullBlockIdx - lastCoveredBlock); offset++)
1:72790dc:             builder.add(getBlockIterator(lastCoveredBlock + offset));
1:72790dc: 
1:72790dc:         return builder.build();
1:72790dc:     }
1:72790dc: 
1:72790dc:     private RangeIterator<Long, Token> searchPoint(int lowerBlock, Expression expression)
1:72790dc:     {
1:72790dc:         Iterator<DataTerm> terms = new TermIterator(lowerBlock, expression, IteratorOrder.DESC);
1:72790dc:         RangeUnionIterator.Builder<Long, Token> builder = RangeUnionIterator.builder();
1:72790dc: 
1:72790dc:         while (terms.hasNext())
1:72790dc:         {
1:72790dc:             try
1:72790dc:             {
1:72790dc:                 builder.add(terms.next().getTokens());
1:72790dc:             }
1:72790dc:             finally
1:72790dc:             {
1:72790dc:                 expression.checkpoint();
1:72790dc:             }
1:72790dc:         }
1:72790dc: 
1:72790dc:         return builder.build();
1:72790dc:     }
1:72790dc: 
1:72790dc:     private RangeIterator<Long, Token> getBlockIterator(int blockIdx)
1:72790dc:     {
1:72790dc:         DataBlock block = dataLevel.getBlock(blockIdx);
1:72790dc:         return (block.hasCombinedIndex)
1:72790dc:                 ? block.getBlockIndex().iterator(keyFetcher)
1:72790dc:                 : block.getRange(0, block.termCount());
1:72790dc:     }
1:72790dc: 
1:72790dc:     public Iterator<DataTerm> iteratorAt(ByteBuffer query, IteratorOrder order, boolean inclusive)
1:72790dc:     {
1:72790dc:         Expression e = new Expression("", comparator);
1:72790dc:         Expression.Bound bound = new Expression.Bound(query, inclusive);
1:72790dc: 
1:72790dc:         switch (order)
1:72790dc:         {
1:72790dc:             case DESC:
1:72790dc:                 e.setLower(bound);
1:72790dc:                 break;
1:72790dc: 
1:72790dc:             case ASC:
1:72790dc:                 e.setUpper(bound);
1:72790dc:                 break;
1:72790dc: 
1:72790dc:             default:
1:72790dc:                 throw new IllegalArgumentException("Unknown order: " + order);
1:72790dc:         }
1:72790dc: 
1:72790dc:         return new TermIterator(levels.length == 0 ? 0 : getBlockIdx(findPointer(query), query), e, order);
1:72790dc:     }
1:72790dc: 
1:72790dc:     private int getDataBlock(ByteBuffer query)
1:72790dc:     {
1:72790dc:         return levels.length == 0 ? 0 : getBlockIdx(findPointer(query), query);
1:72790dc:     }
1:72790dc: 
1:72790dc:     public Iterator<DataTerm> iterator()
1:72790dc:     {
1:72790dc:         return new TermIterator(0, new Expression("", comparator), IteratorOrder.DESC);
1:72790dc:     }
1:72790dc: 
1:72790dc:     public void close() throws IOException
1:72790dc:     {
1:72790dc:         FileUtils.closeQuietly(indexFile);
1:72790dc:     }
1:72790dc: 
1:72790dc:     private PointerTerm findPointer(ByteBuffer query)
1:72790dc:     {
1:72790dc:         PointerTerm ptr = null;
1:72790dc:         for (PointerLevel level : levels)
1:72790dc:         {
1:72790dc:             if ((ptr = level.getPointer(ptr, query)) == null)
1:72790dc:                 return null;
1:72790dc:         }
1:72790dc: 
1:72790dc:         return ptr;
1:72790dc:     }
1:72790dc: 
1:3928665:     private DataTerm getTerm(ByteBuffer query)
1:3928665:     {
1:3928665:         SearchResult<DataTerm> term = searchIndex(query, getDataBlock(query));
1:3928665:         return term.cmp == 0 ? term.result : null;
1:3928665:     }
1:3928665: 
1:72790dc:     private SearchResult<DataTerm> searchIndex(ByteBuffer query, int blockIdx)
1:72790dc:     {
1:72790dc:         return dataLevel.getBlock(blockIdx).search(comparator, query);
1:72790dc:     }
1:72790dc: 
1:72790dc:     private int getBlockIdx(PointerTerm ptr, ByteBuffer query)
1:72790dc:     {
1:72790dc:         int blockIdx = 0;
1:72790dc:         if (ptr != null)
1:72790dc:         {
1:72790dc:             int cmp = ptr.compareTo(comparator, query);
1:72790dc:             blockIdx = (cmp == 0 || cmp > 0) ? ptr.getBlock() : ptr.getBlock() + 1;
1:72790dc:         }
1:72790dc: 
1:72790dc:         return blockIdx;
1:72790dc:     }
1:72790dc: 
1:72790dc:     protected class PointerLevel extends Level<PointerBlock>
1:72790dc:     {
1:72790dc:         public PointerLevel(long offset, int count)
1:72790dc:         {
1:72790dc:             super(offset, count);
1:72790dc:         }
1:72790dc: 
1:72790dc:         public PointerTerm getPointer(PointerTerm parent, ByteBuffer query)
1:72790dc:         {
1:72790dc:             return getBlock(getBlockIdx(parent, query)).search(comparator, query).result;
1:72790dc:         }
1:72790dc: 
1:72790dc:         protected PointerBlock cast(MappedBuffer block)
1:72790dc:         {
1:72790dc:             return new PointerBlock(block);
1:72790dc:         }
1:72790dc:     }
1:72790dc: 
1:72790dc:     protected class DataLevel extends Level<DataBlock>
1:72790dc:     {
1:72790dc:         protected final int superBlockCnt;
1:72790dc:         protected final long superBlocksOffset;
1:72790dc: 
1:72790dc:         public DataLevel(long offset, int count)
1:72790dc:         {
1:72790dc:             super(offset, count);
1:72790dc:             long baseOffset = blockOffsets + blockCount * 8;
1:72790dc:             superBlockCnt = indexFile.getInt(baseOffset);
1:72790dc:             superBlocksOffset = baseOffset + 4;
1:72790dc:         }
1:72790dc: 
1:72790dc:         protected DataBlock cast(MappedBuffer block)
1:72790dc:         {
1:72790dc:             return new DataBlock(block);
1:72790dc:         }
1:72790dc: 
1:72790dc:         public OnDiskSuperBlock getSuperBlock(int idx)
1:72790dc:         {
1:72790dc:             assert idx < superBlockCnt : String.format("requested index %d is greater than super block count %d", idx, superBlockCnt);
1:72790dc:             long blockOffset = indexFile.getLong(superBlocksOffset + idx * 8);
1:72790dc:             return new OnDiskSuperBlock(indexFile.duplicate().position(blockOffset));
1:72790dc:         }
1:72790dc:     }
1:72790dc: 
1:72790dc:     protected class OnDiskSuperBlock
1:72790dc:     {
1:72790dc:         private final TokenTree tokenTree;
1:72790dc: 
1:72790dc:         public OnDiskSuperBlock(MappedBuffer buffer)
1:72790dc:         {
1:72790dc:             tokenTree = new TokenTree(descriptor, buffer);
1:72790dc:         }
1:72790dc: 
1:72790dc:         public RangeIterator<Long, Token> iterator()
1:72790dc:         {
1:72790dc:             return tokenTree.iterator(keyFetcher);
1:72790dc:         }
1:72790dc:     }
1:72790dc: 
1:72790dc:     protected abstract class Level<T extends OnDiskBlock>
1:72790dc:     {
1:72790dc:         protected final long blockOffsets;
1:72790dc:         protected final int blockCount;
1:72790dc: 
1:72790dc:         public Level(long offsets, int count)
1:72790dc:         {
1:72790dc:             this.blockOffsets = offsets;
1:72790dc:             this.blockCount = count;
1:72790dc:         }
1:72790dc: 
1:72790dc:         public T getBlock(int idx) throws FSReadError
1:72790dc:         {
1:72790dc:             assert idx >= 0 && idx < blockCount;
1:72790dc: 
1:72790dc:             // calculate block offset and move there
1:72790dc:             // (long is intentional, we'll just need mmap implementation which supports long positions)
1:72790dc:             long blockOffset = indexFile.getLong(blockOffsets + idx * 8);
1:72790dc:             return cast(indexFile.duplicate().position(blockOffset));
1:72790dc:         }
1:72790dc: 
1:72790dc:         protected abstract T cast(MappedBuffer block);
1:72790dc:     }
1:72790dc: 
1:72790dc:     protected class DataBlock extends OnDiskBlock<DataTerm>
1:72790dc:     {
1:72790dc:         public DataBlock(MappedBuffer data)
1:72790dc:         {
1:72790dc:             super(descriptor, data, BlockType.DATA);
1:72790dc:         }
1:72790dc: 
1:72790dc:         protected DataTerm cast(MappedBuffer data)
1:72790dc:         {
1:72790dc:             return new DataTerm(data, termSize, getBlockIndex());
1:72790dc:         }
1:72790dc: 
1:72790dc:         public RangeIterator<Long, Token> getRange(int start, int end)
1:72790dc:         {
1:72790dc:             RangeUnionIterator.Builder<Long, Token> builder = RangeUnionIterator.builder();
1:72790dc:             NavigableMap<Long, Token> sparse = new TreeMap<>();
1:72790dc: 
1:72790dc:             for (int i = start; i < end; i++)
1:72790dc:             {
1:72790dc:                 DataTerm term = getTerm(i);
1:72790dc: 
1:72790dc:                 if (term.isSparse())
1:72790dc:                 {
1:72790dc:                     NavigableMap<Long, Token> tokens = term.getSparseTokens();
1:72790dc:                     for (Map.Entry<Long, Token> t : tokens.entrySet())
1:72790dc:                     {
1:72790dc:                         Token token = sparse.get(t.getKey());
1:72790dc:                         if (token == null)
1:72790dc:                             sparse.put(t.getKey(), t.getValue());
1:72790dc:                         else
1:72790dc:                             token.merge(t.getValue());
1:72790dc:                     }
1:72790dc:                 }
1:72790dc:                 else
1:72790dc:                 {
1:72790dc:                     builder.add(term.getTokens());
1:72790dc:                 }
1:72790dc:             }
1:72790dc: 
1:72790dc:             PrefetchedTokensIterator prefetched = sparse.isEmpty() ? null : new PrefetchedTokensIterator(sparse);
1:72790dc: 
1:72790dc:             if (builder.rangeCount() == 0)
1:72790dc:                 return prefetched;
1:72790dc: 
1:72790dc:             builder.add(prefetched);
1:72790dc:             return builder.build();
1:72790dc:         }
1:72790dc:     }
1:72790dc: 
1:72790dc:     protected class PointerBlock extends OnDiskBlock<PointerTerm>
1:72790dc:     {
1:72790dc:         public PointerBlock(MappedBuffer block)
1:72790dc:         {
1:72790dc:             super(descriptor, block, BlockType.POINTER);
1:72790dc:         }
1:72790dc: 
1:72790dc:         protected PointerTerm cast(MappedBuffer data)
1:72790dc:         {
1:2ca2fff:             return new PointerTerm(data, termSize, hasMarkedPartials);
1:72790dc:         }
1:72790dc:     }
1:72790dc: 
1:72790dc:     public class DataTerm extends Term implements Comparable<DataTerm>
1:72790dc:     {
1:72790dc:         private final TokenTree perBlockIndex;
1:72790dc: 
1:72790dc:         protected DataTerm(MappedBuffer content, OnDiskIndexBuilder.TermSize size, TokenTree perBlockIndex)
1:72790dc:         {
1:2ca2fff:             super(content, size, hasMarkedPartials);
1:72790dc:             this.perBlockIndex = perBlockIndex;
1:72790dc:         }
1:72790dc: 
1:72790dc:         public RangeIterator<Long, Token> getTokens()
1:72790dc:         {
1:72790dc:             final long blockEnd = FBUtilities.align(content.position(), OnDiskIndexBuilder.BLOCK_SIZE);
1:72790dc: 
1:7d857b4:             // ([int] -1 for sparse, offset for non-sparse)
1:72790dc:             if (isSparse())
1:72790dc:                 return new PrefetchedTokensIterator(getSparseTokens());
1:72790dc: 
1:72790dc:             long offset = blockEnd + 4 + content.getInt(getDataOffset() + 1);
1:72790dc:             return new TokenTree(descriptor, indexFile.duplicate().position(offset)).iterator(keyFetcher);
1:72790dc:         }
1:72790dc: 
1:72790dc:         public boolean isSparse()
1:72790dc:         {
1:72790dc:             return content.get(getDataOffset()) > 0;
1:72790dc:         }
1:72790dc: 
1:72790dc:         public NavigableMap<Long, Token> getSparseTokens()
1:72790dc:         {
1:72790dc:             long ptrOffset = getDataOffset();
1:72790dc: 
1:72790dc:             byte size = content.get(ptrOffset);
1:72790dc: 
1:72790dc:             assert size > 0;
1:72790dc: 
1:72790dc:             NavigableMap<Long, Token> individualTokens = new TreeMap<>();
1:72790dc:             for (int i = 0; i < size; i++)
1:72790dc:             {
1:7d857b4:                 Token token = perBlockIndex.get(content.getLong(ptrOffset + 1 + TOKEN_BYTES * i), keyFetcher);
1:72790dc: 
1:72790dc:                 assert token != null;
1:72790dc:                 individualTokens.put(token.get(), token);
1:72790dc:             }
1:72790dc: 
1:72790dc:             return individualTokens;
1:72790dc:         }
1:72790dc: 
1:72790dc:         public int compareTo(DataTerm other)
1:72790dc:         {
1:72790dc:             return other == null ? 1 : compareTo(comparator, other.getTerm());
1:72790dc:         }
1:72790dc:     }
1:72790dc: 
1:72790dc:     protected static class PointerTerm extends Term
1:72790dc:     {
1:2ca2fff:         public PointerTerm(MappedBuffer content, OnDiskIndexBuilder.TermSize size, boolean hasMarkedPartials)
1:72790dc:         {
1:2ca2fff:             super(content, size, hasMarkedPartials);
1:72790dc:         }
1:72790dc: 
1:72790dc:         public int getBlock()
1:72790dc:         {
1:72790dc:             return content.getInt(getDataOffset());
1:72790dc:         }
1:72790dc:     }
1:72790dc: 
1:72790dc:     private static class PrefetchedTokensIterator extends RangeIterator<Long, Token>
1:72790dc:     {
1:72790dc:         private final NavigableMap<Long, Token> tokens;
1:72790dc:         private PeekingIterator<Token> currentIterator;
1:72790dc: 
1:72790dc:         public PrefetchedTokensIterator(NavigableMap<Long, Token> tokens)
1:72790dc:         {
1:72790dc:             super(tokens.firstKey(), tokens.lastKey(), tokens.size());
1:72790dc:             this.tokens = tokens;
1:72790dc:             this.currentIterator = Iterators.peekingIterator(tokens.values().iterator());
1:72790dc:         }
1:72790dc: 
1:72790dc:         protected Token computeNext()
1:72790dc:         {
1:72790dc:             return currentIterator != null && currentIterator.hasNext()
1:72790dc:                     ? currentIterator.next()
1:72790dc:                     : endOfData();
1:72790dc:         }
1:72790dc: 
1:72790dc:         protected void performSkipTo(Long nextToken)
1:72790dc:         {
1:72790dc:             currentIterator = Iterators.peekingIterator(tokens.tailMap(nextToken, true).values().iterator());
1:72790dc:         }
1:72790dc: 
1:72790dc:         public void close() throws IOException
1:72790dc:         {
1:72790dc:             endOfData();
1:72790dc:         }
1:72790dc:     }
1:72790dc: 
1:72790dc:     public AbstractType<?> getComparator()
1:72790dc:     {
1:72790dc:         return comparator;
1:72790dc:     }
1:72790dc: 
1:72790dc:     public String getIndexPath()
1:72790dc:     {
1:72790dc:         return indexPath;
1:72790dc:     }
1:72790dc: 
1:72790dc:     private class TermIterator extends AbstractIterator<DataTerm>
1:72790dc:     {
1:72790dc:         private final Expression e;
1:72790dc:         private final IteratorOrder order;
1:72790dc: 
1:72790dc:         protected OnDiskBlock<DataTerm> currentBlock;
1:72790dc:         protected int blockIndex, offset;
1:72790dc: 
1:72790dc:         private boolean checkLower = true, checkUpper = true;
1:72790dc: 
1:72790dc:         public TermIterator(int startBlock, Expression expression, IteratorOrder order)
1:72790dc:         {
1:72790dc:             this.e = expression;
1:72790dc:             this.order = order;
1:72790dc:             this.blockIndex = startBlock;
1:72790dc: 
1:72790dc:             nextBlock();
1:72790dc:         }
1:72790dc: 
1:72790dc:         protected DataTerm computeNext()
1:72790dc:         {
1:72790dc:             for (;;)
1:72790dc:             {
1:72790dc:                 if (currentBlock == null)
1:72790dc:                     return endOfData();
1:72790dc: 
1:72790dc:                 if (offset >= 0 && offset < currentBlock.termCount())
1:72790dc:                 {
1:72790dc:                     DataTerm currentTerm = currentBlock.getTerm(nextOffset());
1:72790dc: 
1:7107646:                     // we need to step over all of the partial terms, in PREFIX mode,
1:7107646:                     // encountered by the query until upper-bound tells us to stop
1:7107646:                     if (e.getOp() == Op.PREFIX && currentTerm.isPartial())
1:7107646:                         continue;
1:7107646: 
1:7107646:                     // haven't reached the start of the query range yet, let's
1:7107646:                     // keep skip the current term until lower bound is satisfied
1:72790dc:                     if (checkLower && !e.isLowerSatisfiedBy(currentTerm))
1:72790dc:                         continue;
1:72790dc: 
1:72790dc:                     // flip the flag right on the first bounds match
1:72790dc:                     // to avoid expensive comparisons
1:72790dc:                     checkLower = false;
1:72790dc: 
1:72790dc:                     if (checkUpper && !e.isUpperSatisfiedBy(currentTerm))
1:72790dc:                         return endOfData();
1:72790dc: 
1:72790dc:                     return currentTerm;
1:72790dc:                 }
1:72790dc: 
1:72790dc:                 nextBlock();
1:72790dc:             }
1:72790dc:         }
1:72790dc: 
1:72790dc:         protected void nextBlock()
1:72790dc:         {
1:72790dc:             currentBlock = null;
1:72790dc: 
1:72790dc:             if (blockIndex < 0 || blockIndex >= dataLevel.blockCount)
1:72790dc:                 return;
1:72790dc: 
1:72790dc:             currentBlock = dataLevel.getBlock(nextBlockIndex());
1:72790dc:             offset = checkLower ? order.startAt(currentBlock, e) : currentBlock.minOffset(order);
1:72790dc: 
1:72790dc:             // let's check the last term of the new block right away
1:72790dc:             // if expression's upper bound is satisfied by it such means that we can avoid
1:72790dc:             // doing any expensive upper bound checks for that block.
1:72790dc:             checkUpper = e.hasUpper() && !e.isUpperSatisfiedBy(currentBlock.getTerm(currentBlock.maxOffset(order)));
1:72790dc:         }
1:72790dc: 
1:72790dc:         protected int nextBlockIndex()
1:72790dc:         {
1:72790dc:             int current = blockIndex;
1:72790dc:             blockIndex += order.step;
1:72790dc:             return current;
1:72790dc:         }
1:72790dc: 
1:72790dc:         protected int nextOffset()
1:72790dc:         {
1:72790dc:             int current = offset;
1:72790dc:             offset += order.step;
1:72790dc:             return current;
1:72790dc:         }
1:72790dc:     }
1:72790dc: }
============================================================================
author:Alex Petrov
-------------------------------------------------------------------------------
commit:7d857b4
/////////////////////////////////////////////////////////////////////////
1: import org.apache.cassandra.index.sasi.*;
/////////////////////////////////////////////////////////////////////////
1: import static org.apache.cassandra.index.sasi.disk.TokenTreeBuilder.TOKEN_BYTES;
/////////////////////////////////////////////////////////////////////////
1:     protected final KeyFetcher keyFetcher;
/////////////////////////////////////////////////////////////////////////
1:     public OnDiskIndex(File index, AbstractType<?> cmp, KeyFetcher keyReader)
/////////////////////////////////////////////////////////////////////////
1:             // ([int] -1 for sparse, offset for non-sparse)
/////////////////////////////////////////////////////////////////////////
1:                 Token token = perBlockIndex.get(content.getLong(ptrOffset + 1 + TOKEN_BYTES * i), keyFetcher);
author:Pavel Yaskevich
-------------------------------------------------------------------------------
commit:7107646
/////////////////////////////////////////////////////////////////////////
1:                     // we need to step over all of the partial terms, in PREFIX mode,
1:                     // encountered by the query until upper-bound tells us to stop
1:                     if (e.getOp() == Op.PREFIX && currentTerm.isPartial())
1:                         continue;
1: 
1:                     // haven't reached the start of the query range yet, let's
1:                     // keep skip the current term until lower bound is satisfied
commit:3928665
/////////////////////////////////////////////////////////////////////////
1:         assert mode.supports(exp.getOp());
1: 
1:         // optimization in case single term is requested from index
1:         // we don't really need to build additional union iterator
1:         if (exp.getOp() == Op.EQ)
1:         {
1:             DataTerm term = getTerm(exp.lower.value);
1:             return term == null ? null : term.getTokens();
1:         }
1: 
/////////////////////////////////////////////////////////////////////////
1:     private DataTerm getTerm(ByteBuffer query)
1:     {
1:         SearchResult<DataTerm> term = searchIndex(query, getDataBlock(query));
1:         return term.cmp == 0 ? term.result : null;
1:     }
1: 
commit:72790dc
/////////////////////////////////////////////////////////////////////////
1: /*
1:  * Licensed to the Apache Software Foundation (ASF) under one
1:  * or more contributor license agreements.  See the NOTICE file
1:  * distributed with this work for additional information
1:  * regarding copyright ownership.  The ASF licenses this file
1:  * to you under the Apache License, Version 2.0 (the
1:  * "License"); you may not use this file except in compliance
1:  * with the License.  You may obtain a copy of the License at
1:  *
1:  *     http://www.apache.org/licenses/LICENSE-2.0
1:  *
1:  * Unless required by applicable law or agreed to in writing, software
1:  * distributed under the License is distributed on an "AS IS" BASIS,
1:  * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
1:  * See the License for the specific language governing permissions and
1:  * limitations under the License.
1:  */
1: package org.apache.cassandra.index.sasi.disk;
1: 
1: import java.io.*;
1: import java.nio.ByteBuffer;
1: import java.util.*;
1: import java.util.stream.Collectors;
1: 
0: import org.apache.cassandra.db.DecoratedKey;
0: import org.apache.cassandra.index.sasi.Term;
1: import org.apache.cassandra.index.sasi.plan.Expression;
1: import org.apache.cassandra.index.sasi.plan.Expression.Op;
1: import org.apache.cassandra.index.sasi.utils.MappedBuffer;
1: import org.apache.cassandra.index.sasi.utils.RangeUnionIterator;
1: import org.apache.cassandra.index.sasi.utils.AbstractIterator;
1: import org.apache.cassandra.index.sasi.utils.RangeIterator;
1: import org.apache.cassandra.db.marshal.AbstractType;
1: import org.apache.cassandra.io.FSReadError;
1: import org.apache.cassandra.io.util.ChannelProxy;
1: import org.apache.cassandra.io.util.FileUtils;
1: import org.apache.cassandra.utils.ByteBufferUtil;
1: import org.apache.cassandra.utils.FBUtilities;
1: 
0: import com.google.common.base.Function;
1: import com.google.common.collect.Iterables;
1: import com.google.common.collect.Iterators;
1: import com.google.common.collect.PeekingIterator;
1: 
1: import static org.apache.cassandra.index.sasi.disk.OnDiskBlock.SearchResult;
1: 
1: public class OnDiskIndex implements Iterable<OnDiskIndex.DataTerm>, Closeable
1: {
1:     public enum IteratorOrder
1:     {
1:         DESC(1), ASC(-1);
1: 
1:         public final int step;
1: 
1:         IteratorOrder(int step)
1:         {
1:             this.step = step;
1:         }
1: 
1:         public int startAt(OnDiskBlock<DataTerm> block, Expression e)
1:         {
1:             switch (this)
1:             {
1:                 case DESC:
1:                     return e.lower == null
1:                             ? 0
1:                             : startAt(block.search(e.validator, e.lower.value), e.lower.inclusive);
1: 
1:                 case ASC:
1:                     return e.upper == null
1:                             ? block.termCount() - 1
1:                             : startAt(block.search(e.validator, e.upper.value), e.upper.inclusive);
1: 
1:                 default:
1:                     throw new IllegalArgumentException("Unknown order: " + this);
1:             }
1:         }
1: 
1:         public int startAt(SearchResult<DataTerm> found, boolean inclusive)
1:         {
1:             switch (this)
1:             {
1:                 case DESC:
1:                     if (found.cmp < 0)
1:                         return found.index + 1;
1: 
1:                     return inclusive || found.cmp != 0 ? found.index : found.index + 1;
1: 
1:                 case ASC:
1:                     if (found.cmp < 0) // search term was bigger then whole data set
1:                         return found.index;
1:                     return inclusive && (found.cmp == 0 || found.cmp < 0) ? found.index : found.index - 1;
1: 
1:                 default:
1:                     throw new IllegalArgumentException("Unknown order: " + this);
1:             }
1:         }
1:     }
1: 
1:     public final Descriptor descriptor;
1:     protected final OnDiskIndexBuilder.Mode mode;
1:     protected final OnDiskIndexBuilder.TermSize termSize;
1: 
1:     protected final AbstractType<?> comparator;
1:     protected final MappedBuffer indexFile;
1:     protected final long indexSize;
1: 
0:     protected final Function<Long, DecoratedKey> keyFetcher;
1: 
1:     protected final String indexPath;
1: 
1:     protected final PointerLevel[] levels;
1:     protected final DataLevel dataLevel;
1: 
1:     protected final ByteBuffer minTerm, maxTerm, minKey, maxKey;
1: 
0:     public OnDiskIndex(File index, AbstractType<?> cmp, Function<Long, DecoratedKey> keyReader)
1:     {
1:         keyFetcher = keyReader;
1: 
1:         comparator = cmp;
1:         indexPath = index.getAbsolutePath();
1: 
1:         RandomAccessFile backingFile = null;
1:         try
1:         {
1:             backingFile = new RandomAccessFile(index, "r");
1: 
1:             descriptor = new Descriptor(backingFile.readUTF());
1: 
1:             termSize = OnDiskIndexBuilder.TermSize.of(backingFile.readShort());
1: 
1:             minTerm = ByteBufferUtil.readWithShortLength(backingFile);
1:             maxTerm = ByteBufferUtil.readWithShortLength(backingFile);
1: 
1:             minKey = ByteBufferUtil.readWithShortLength(backingFile);
1:             maxKey = ByteBufferUtil.readWithShortLength(backingFile);
1: 
1:             mode = OnDiskIndexBuilder.Mode.mode(backingFile.readUTF());
1: 
1:             indexSize = backingFile.length();
1:             indexFile = new MappedBuffer(new ChannelProxy(indexPath, backingFile.getChannel()));
1: 
1:             // start of the levels
1:             indexFile.position(indexFile.getLong(indexSize - 8));
1: 
1:             int numLevels = indexFile.getInt();
1:             levels = new PointerLevel[numLevels];
1:             for (int i = 0; i < levels.length; i++)
1:             {
1:                 int blockCount = indexFile.getInt();
1:                 levels[i] = new PointerLevel(indexFile.position(), blockCount);
1:                 indexFile.position(indexFile.position() + blockCount * 8);
1:             }
1: 
1:             int blockCount = indexFile.getInt();
1:             dataLevel = new DataLevel(indexFile.position(), blockCount);
1:         }
1:         catch (IOException e)
1:         {
1:             throw new FSReadError(e, index);
1:         }
1:         finally
1:         {
1:             FileUtils.closeQuietly(backingFile);
1:         }
1:     }
1: 
1:     public ByteBuffer minTerm()
1:     {
1:         return minTerm;
1:     }
1: 
1:     public ByteBuffer maxTerm()
1:     {
1:         return maxTerm;
1:     }
1: 
1:     public ByteBuffer minKey()
1:     {
1:         return minKey;
1:     }
1: 
1:     public ByteBuffer maxKey()
1:     {
1:         return maxKey;
1:     }
1: 
1:     public DataTerm min()
1:     {
1:         return dataLevel.getBlock(0).getTerm(0);
1:     }
1: 
1:     public DataTerm max()
1:     {
1:         DataBlock block = dataLevel.getBlock(dataLevel.blockCount - 1);
1:         return block.getTerm(block.termCount() - 1);
1:     }
1: 
1:     /**
1:      * Search for rows which match all of the terms inside the given expression in the index file.
1:      *
1:      * @param exp The expression to use for the query.
1:      *
1:      * @return Iterator which contains rows for all of the terms from the given range.
1:      */
1:     public RangeIterator<Long, Token> search(Expression exp)
1:     {
1:         // convert single NOT_EQ to range with exclusion
1:         final Expression expression = (exp.getOp() != Op.NOT_EQ)
1:                                         ? exp
1:                                         : new Expression(exp).setOp(Op.RANGE)
1:                                                 .setLower(new Expression.Bound(minTerm, true))
1:                                                 .setUpper(new Expression.Bound(maxTerm, true))
1:                                                 .addExclusion(exp.lower.value);
1: 
1:         List<ByteBuffer> exclusions = new ArrayList<>(expression.exclusions.size());
1: 
1:         Iterables.addAll(exclusions, expression.exclusions.stream().filter(exclusion -> {
1:             // accept only exclusions which are in the bounds of lower/upper
1:             return !(expression.lower != null && comparator.compare(exclusion, expression.lower.value) < 0)
1:                 && !(expression.upper != null && comparator.compare(exclusion, expression.upper.value) > 0);
1:         }).collect(Collectors.toList()));
1: 
1:         Collections.sort(exclusions, comparator);
1: 
1:         if (exclusions.size() == 0)
1:             return searchRange(expression);
1: 
1:         List<Expression> ranges = new ArrayList<>(exclusions.size());
1: 
1:         // calculate range splits based on the sorted exclusions
1:         Iterator<ByteBuffer> exclusionsIterator = exclusions.iterator();
1: 
1:         Expression.Bound min = expression.lower, max = null;
1:         while (exclusionsIterator.hasNext())
1:         {
1:             max = new Expression.Bound(exclusionsIterator.next(), false);
1:             ranges.add(new Expression(expression).setOp(Op.RANGE).setLower(min).setUpper(max));
1:             min = max;
1:         }
1: 
1:         assert max != null;
1:         ranges.add(new Expression(expression).setOp(Op.RANGE).setLower(max).setUpper(expression.upper));
1: 
1:         RangeUnionIterator.Builder<Long, Token> builder = RangeUnionIterator.builder();
1:         for (Expression e : ranges)
1:         {
1:             RangeIterator<Long, Token> range = searchRange(e);
1:             if (range != null)
1:                 builder.add(range);
1:         }
1: 
1:         return builder.build();
1:     }
1: 
1:     private RangeIterator<Long, Token> searchRange(Expression range)
1:     {
1:         Expression.Bound lower = range.lower;
1:         Expression.Bound upper = range.upper;
1: 
1:         int lowerBlock = lower == null ? 0 : getDataBlock(lower.value);
1:         int upperBlock = upper == null
1:                 ? dataLevel.blockCount - 1
1:                 // optimization so we don't have to fetch upperBlock when query has lower == upper
1:                 : (lower != null && comparator.compare(lower.value, upper.value) == 0) ? lowerBlock : getDataBlock(upper.value);
1: 
1:         return (mode != OnDiskIndexBuilder.Mode.SPARSE || lowerBlock == upperBlock || upperBlock - lowerBlock <= 1)
1:                 ? searchPoint(lowerBlock, range)
1:                 : searchRange(lowerBlock, lower, upperBlock, upper);
1:     }
1: 
1:     private RangeIterator<Long, Token> searchRange(int lowerBlock, Expression.Bound lower, int upperBlock, Expression.Bound upper)
1:     {
1:         // if lower is at the beginning of the block that means we can just do a single iterator per block
1:         SearchResult<DataTerm> lowerPosition = (lower == null) ? null : searchIndex(lower.value, lowerBlock);
1:         SearchResult<DataTerm> upperPosition = (upper == null) ? null : searchIndex(upper.value, upperBlock);
1: 
1:         RangeUnionIterator.Builder<Long, Token> builder = RangeUnionIterator.builder();
1: 
1:         // optimistically assume that first and last blocks are full block reads, saves at least 3 'else' conditions
1:         int firstFullBlockIdx = lowerBlock, lastFullBlockIdx = upperBlock;
1: 
1:         // 'lower' doesn't cover the whole block so we need to do a partial iteration
1:         // Two reasons why that can happen:
1:         //   - 'lower' is not the first element of the block
1:         //   - 'lower' is first element but it's not inclusive in the query
1:         if (lowerPosition != null && (lowerPosition.index > 0 || !lower.inclusive))
1:         {
1:             DataBlock block = dataLevel.getBlock(lowerBlock);
1:             int start = (lower.inclusive || lowerPosition.cmp != 0) ? lowerPosition.index : lowerPosition.index + 1;
1: 
1:             builder.add(block.getRange(start, block.termCount()));
1:             firstFullBlockIdx = lowerBlock + 1;
1:         }
1: 
1:         if (upperPosition != null)
1:         {
1:             DataBlock block = dataLevel.getBlock(upperBlock);
1:             int lastIndex = block.termCount() - 1;
1: 
1:             // The save as with 'lower' but here we need to check if the upper is the last element of the block,
1:             // which means that we only have to get individual results if:
1:             //  - if it *is not* the last element, or
1:             //  - it *is* but shouldn't be included (dictated by upperInclusive)
1:             if (upperPosition.index != lastIndex || !upper.inclusive)
1:             {
1:                 int end = (upperPosition.cmp < 0 || (upperPosition.cmp == 0 && upper.inclusive))
1:                                 ? upperPosition.index + 1 : upperPosition.index;
1: 
1:                 builder.add(block.getRange(0, end));
1:                 lastFullBlockIdx = upperBlock - 1;
1:             }
1:         }
1: 
1:         int totalSuperBlocks = (lastFullBlockIdx - firstFullBlockIdx) / OnDiskIndexBuilder.SUPER_BLOCK_SIZE;
1: 
1:         // if there are no super-blocks, we can simply read all of the block iterators in sequence
1:         if (totalSuperBlocks == 0)
1:         {
1:             for (int i = firstFullBlockIdx; i <= lastFullBlockIdx; i++)
1:                 builder.add(dataLevel.getBlock(i).getBlockIndex().iterator(keyFetcher));
1: 
1:             return builder.build();
1:         }
1: 
1:         // first get all of the blocks which are aligned before the first super-block in the sequence,
1:         // e.g. if the block range was (1, 9) and super-block-size = 4, we need to read 1, 2, 3, 4 - 7 is covered by
1:         // super-block, 8, 9 is a remainder.
1: 
1:         int superBlockAlignedStart = firstFullBlockIdx == 0 ? 0 : (int) FBUtilities.align(firstFullBlockIdx, OnDiskIndexBuilder.SUPER_BLOCK_SIZE);
1:         for (int blockIdx = firstFullBlockIdx; blockIdx < Math.min(superBlockAlignedStart, lastFullBlockIdx); blockIdx++)
1:             builder.add(getBlockIterator(blockIdx));
1: 
1:         // now read all of the super-blocks matched by the request, from the previous comment
1:         // it's a block with index 1 (which covers everything from 4 to 7)
1: 
1:         int superBlockIdx = superBlockAlignedStart / OnDiskIndexBuilder.SUPER_BLOCK_SIZE;
1:         for (int offset = 0; offset < totalSuperBlocks - 1; offset++)
1:             builder.add(dataLevel.getSuperBlock(superBlockIdx++).iterator());
1: 
1:         // now it's time for a remainder read, again from the previous example it's 8, 9 because
1:         // we have over-shot previous block but didn't request enough to cover next super-block.
1: 
1:         int lastCoveredBlock = superBlockIdx * OnDiskIndexBuilder.SUPER_BLOCK_SIZE;
1:         for (int offset = 0; offset <= (lastFullBlockIdx - lastCoveredBlock); offset++)
1:             builder.add(getBlockIterator(lastCoveredBlock + offset));
1: 
1:         return builder.build();
1:     }
1: 
1:     private RangeIterator<Long, Token> searchPoint(int lowerBlock, Expression expression)
1:     {
1:         Iterator<DataTerm> terms = new TermIterator(lowerBlock, expression, IteratorOrder.DESC);
1:         RangeUnionIterator.Builder<Long, Token> builder = RangeUnionIterator.builder();
1: 
1:         while (terms.hasNext())
1:         {
1:             try
1:             {
1:                 builder.add(terms.next().getTokens());
1:             }
1:             finally
1:             {
1:                 expression.checkpoint();
1:             }
1:         }
1: 
1:         return builder.build();
1:     }
1: 
1:     private RangeIterator<Long, Token> getBlockIterator(int blockIdx)
1:     {
1:         DataBlock block = dataLevel.getBlock(blockIdx);
1:         return (block.hasCombinedIndex)
1:                 ? block.getBlockIndex().iterator(keyFetcher)
1:                 : block.getRange(0, block.termCount());
1:     }
1: 
1:     public Iterator<DataTerm> iteratorAt(ByteBuffer query, IteratorOrder order, boolean inclusive)
1:     {
1:         Expression e = new Expression("", comparator);
1:         Expression.Bound bound = new Expression.Bound(query, inclusive);
1: 
1:         switch (order)
1:         {
1:             case DESC:
1:                 e.setLower(bound);
1:                 break;
1: 
1:             case ASC:
1:                 e.setUpper(bound);
1:                 break;
1: 
1:             default:
1:                 throw new IllegalArgumentException("Unknown order: " + order);
1:         }
1: 
1:         return new TermIterator(levels.length == 0 ? 0 : getBlockIdx(findPointer(query), query), e, order);
1:     }
1: 
1:     private int getDataBlock(ByteBuffer query)
1:     {
1:         return levels.length == 0 ? 0 : getBlockIdx(findPointer(query), query);
1:     }
1: 
1:     public Iterator<DataTerm> iterator()
1:     {
1:         return new TermIterator(0, new Expression("", comparator), IteratorOrder.DESC);
1:     }
1: 
1:     public void close() throws IOException
1:     {
1:         FileUtils.closeQuietly(indexFile);
1:     }
1: 
1:     private PointerTerm findPointer(ByteBuffer query)
1:     {
1:         PointerTerm ptr = null;
1:         for (PointerLevel level : levels)
1:         {
1:             if ((ptr = level.getPointer(ptr, query)) == null)
1:                 return null;
1:         }
1: 
1:         return ptr;
1:     }
1: 
1:     private SearchResult<DataTerm> searchIndex(ByteBuffer query, int blockIdx)
1:     {
1:         return dataLevel.getBlock(blockIdx).search(comparator, query);
1:     }
1: 
1:     private int getBlockIdx(PointerTerm ptr, ByteBuffer query)
1:     {
1:         int blockIdx = 0;
1:         if (ptr != null)
1:         {
1:             int cmp = ptr.compareTo(comparator, query);
1:             blockIdx = (cmp == 0 || cmp > 0) ? ptr.getBlock() : ptr.getBlock() + 1;
1:         }
1: 
1:         return blockIdx;
1:     }
1: 
1:     protected class PointerLevel extends Level<PointerBlock>
1:     {
1:         public PointerLevel(long offset, int count)
1:         {
1:             super(offset, count);
1:         }
1: 
1:         public PointerTerm getPointer(PointerTerm parent, ByteBuffer query)
1:         {
1:             return getBlock(getBlockIdx(parent, query)).search(comparator, query).result;
1:         }
1: 
1:         protected PointerBlock cast(MappedBuffer block)
1:         {
1:             return new PointerBlock(block);
1:         }
1:     }
1: 
1:     protected class DataLevel extends Level<DataBlock>
1:     {
1:         protected final int superBlockCnt;
1:         protected final long superBlocksOffset;
1: 
1:         public DataLevel(long offset, int count)
1:         {
1:             super(offset, count);
1:             long baseOffset = blockOffsets + blockCount * 8;
1:             superBlockCnt = indexFile.getInt(baseOffset);
1:             superBlocksOffset = baseOffset + 4;
1:         }
1: 
1:         protected DataBlock cast(MappedBuffer block)
1:         {
1:             return new DataBlock(block);
1:         }
1: 
1:         public OnDiskSuperBlock getSuperBlock(int idx)
1:         {
1:             assert idx < superBlockCnt : String.format("requested index %d is greater than super block count %d", idx, superBlockCnt);
1:             long blockOffset = indexFile.getLong(superBlocksOffset + idx * 8);
1:             return new OnDiskSuperBlock(indexFile.duplicate().position(blockOffset));
1:         }
1:     }
1: 
1:     protected class OnDiskSuperBlock
1:     {
1:         private final TokenTree tokenTree;
1: 
1:         public OnDiskSuperBlock(MappedBuffer buffer)
1:         {
1:             tokenTree = new TokenTree(descriptor, buffer);
1:         }
1: 
1:         public RangeIterator<Long, Token> iterator()
1:         {
1:             return tokenTree.iterator(keyFetcher);
1:         }
1:     }
1: 
1:     protected abstract class Level<T extends OnDiskBlock>
1:     {
1:         protected final long blockOffsets;
1:         protected final int blockCount;
1: 
1:         public Level(long offsets, int count)
1:         {
1:             this.blockOffsets = offsets;
1:             this.blockCount = count;
1:         }
1: 
1:         public T getBlock(int idx) throws FSReadError
1:         {
1:             assert idx >= 0 && idx < blockCount;
1: 
1:             // calculate block offset and move there
1:             // (long is intentional, we'll just need mmap implementation which supports long positions)
1:             long blockOffset = indexFile.getLong(blockOffsets + idx * 8);
1:             return cast(indexFile.duplicate().position(blockOffset));
1:         }
1: 
1:         protected abstract T cast(MappedBuffer block);
1:     }
1: 
1:     protected class DataBlock extends OnDiskBlock<DataTerm>
1:     {
1:         public DataBlock(MappedBuffer data)
1:         {
1:             super(descriptor, data, BlockType.DATA);
1:         }
1: 
1:         protected DataTerm cast(MappedBuffer data)
1:         {
1:             return new DataTerm(data, termSize, getBlockIndex());
1:         }
1: 
1:         public RangeIterator<Long, Token> getRange(int start, int end)
1:         {
1:             RangeUnionIterator.Builder<Long, Token> builder = RangeUnionIterator.builder();
1:             NavigableMap<Long, Token> sparse = new TreeMap<>();
1: 
1:             for (int i = start; i < end; i++)
1:             {
1:                 DataTerm term = getTerm(i);
1: 
1:                 if (term.isSparse())
1:                 {
1:                     NavigableMap<Long, Token> tokens = term.getSparseTokens();
1:                     for (Map.Entry<Long, Token> t : tokens.entrySet())
1:                     {
1:                         Token token = sparse.get(t.getKey());
1:                         if (token == null)
1:                             sparse.put(t.getKey(), t.getValue());
1:                         else
1:                             token.merge(t.getValue());
1:                     }
1:                 }
1:                 else
1:                 {
1:                     builder.add(term.getTokens());
1:                 }
1:             }
1: 
1:             PrefetchedTokensIterator prefetched = sparse.isEmpty() ? null : new PrefetchedTokensIterator(sparse);
1: 
1:             if (builder.rangeCount() == 0)
1:                 return prefetched;
1: 
1:             builder.add(prefetched);
1:             return builder.build();
1:         }
1:     }
1: 
1:     protected class PointerBlock extends OnDiskBlock<PointerTerm>
1:     {
1:         public PointerBlock(MappedBuffer block)
1:         {
1:             super(descriptor, block, BlockType.POINTER);
1:         }
1: 
1:         protected PointerTerm cast(MappedBuffer data)
1:         {
0:             return new PointerTerm(data, termSize);
1:         }
1:     }
1: 
1:     public class DataTerm extends Term implements Comparable<DataTerm>
1:     {
1:         private final TokenTree perBlockIndex;
1: 
1:         protected DataTerm(MappedBuffer content, OnDiskIndexBuilder.TermSize size, TokenTree perBlockIndex)
1:         {
0:             super(content, size);
1:             this.perBlockIndex = perBlockIndex;
1:         }
1: 
1:         public RangeIterator<Long, Token> getTokens()
1:         {
1:             final long blockEnd = FBUtilities.align(content.position(), OnDiskIndexBuilder.BLOCK_SIZE);
1: 
1:             if (isSparse())
1:                 return new PrefetchedTokensIterator(getSparseTokens());
1: 
1:             long offset = blockEnd + 4 + content.getInt(getDataOffset() + 1);
1:             return new TokenTree(descriptor, indexFile.duplicate().position(offset)).iterator(keyFetcher);
1:         }
1: 
1:         public boolean isSparse()
1:         {
1:             return content.get(getDataOffset()) > 0;
1:         }
1: 
1:         public NavigableMap<Long, Token> getSparseTokens()
1:         {
1:             long ptrOffset = getDataOffset();
1: 
1:             byte size = content.get(ptrOffset);
1: 
1:             assert size > 0;
1: 
1:             NavigableMap<Long, Token> individualTokens = new TreeMap<>();
1:             for (int i = 0; i < size; i++)
1:             {
0:                 Token token = perBlockIndex.get(content.getLong(ptrOffset + 1 + (8 * i)), keyFetcher);
1: 
1:                 assert token != null;
1:                 individualTokens.put(token.get(), token);
1:             }
1: 
1:             return individualTokens;
1:         }
1: 
1:         public int compareTo(DataTerm other)
1:         {
1:             return other == null ? 1 : compareTo(comparator, other.getTerm());
1:         }
1:     }
1: 
1:     protected static class PointerTerm extends Term
1:     {
0:         public PointerTerm(MappedBuffer content, OnDiskIndexBuilder.TermSize size)
1:         {
0:             super(content, size);
1:         }
1: 
1:         public int getBlock()
1:         {
1:             return content.getInt(getDataOffset());
1:         }
1:     }
1: 
1:     private static class PrefetchedTokensIterator extends RangeIterator<Long, Token>
1:     {
1:         private final NavigableMap<Long, Token> tokens;
1:         private PeekingIterator<Token> currentIterator;
1: 
1:         public PrefetchedTokensIterator(NavigableMap<Long, Token> tokens)
1:         {
1:             super(tokens.firstKey(), tokens.lastKey(), tokens.size());
1:             this.tokens = tokens;
1:             this.currentIterator = Iterators.peekingIterator(tokens.values().iterator());
1:         }
1: 
1:         protected Token computeNext()
1:         {
1:             return currentIterator != null && currentIterator.hasNext()
1:                     ? currentIterator.next()
1:                     : endOfData();
1:         }
1: 
1:         protected void performSkipTo(Long nextToken)
1:         {
1:             currentIterator = Iterators.peekingIterator(tokens.tailMap(nextToken, true).values().iterator());
1:         }
1: 
1:         public void close() throws IOException
1:         {
1:             endOfData();
1:         }
1:     }
1: 
1:     public AbstractType<?> getComparator()
1:     {
1:         return comparator;
1:     }
1: 
1:     public String getIndexPath()
1:     {
1:         return indexPath;
1:     }
1: 
1:     private class TermIterator extends AbstractIterator<DataTerm>
1:     {
1:         private final Expression e;
1:         private final IteratorOrder order;
1: 
1:         protected OnDiskBlock<DataTerm> currentBlock;
1:         protected int blockIndex, offset;
1: 
1:         private boolean checkLower = true, checkUpper = true;
1: 
1:         public TermIterator(int startBlock, Expression expression, IteratorOrder order)
1:         {
1:             this.e = expression;
1:             this.order = order;
1:             this.blockIndex = startBlock;
1: 
1:             nextBlock();
1:         }
1: 
1:         protected DataTerm computeNext()
1:         {
1:             for (;;)
1:             {
1:                 if (currentBlock == null)
1:                     return endOfData();
1: 
1:                 if (offset >= 0 && offset < currentBlock.termCount())
1:                 {
1:                     DataTerm currentTerm = currentBlock.getTerm(nextOffset());
1: 
1:                     if (checkLower && !e.isLowerSatisfiedBy(currentTerm))
1:                         continue;
1: 
1:                     // flip the flag right on the first bounds match
1:                     // to avoid expensive comparisons
1:                     checkLower = false;
1: 
1:                     if (checkUpper && !e.isUpperSatisfiedBy(currentTerm))
1:                         return endOfData();
1: 
1:                     return currentTerm;
1:                 }
1: 
1:                 nextBlock();
1:             }
1:         }
1: 
1:         protected void nextBlock()
1:         {
1:             currentBlock = null;
1: 
1:             if (blockIndex < 0 || blockIndex >= dataLevel.blockCount)
1:                 return;
1: 
1:             currentBlock = dataLevel.getBlock(nextBlockIndex());
1:             offset = checkLower ? order.startAt(currentBlock, e) : currentBlock.minOffset(order);
1: 
1:             // let's check the last term of the new block right away
1:             // if expression's upper bound is satisfied by it such means that we can avoid
1:             // doing any expensive upper bound checks for that block.
1:             checkUpper = e.hasUpper() && !e.isUpperSatisfiedBy(currentBlock.getTerm(currentBlock.maxOffset(order)));
1:         }
1: 
1:         protected int nextBlockIndex()
1:         {
1:             int current = blockIndex;
1:             blockIndex += order.step;
1:             return current;
1:         }
1: 
1:         protected int nextOffset()
1:         {
1:             int current = offset;
1:             offset += order.step;
1:             return current;
1:         }
1:     }
1: }
author:Jordan West
-------------------------------------------------------------------------------
commit:2ca2fff
/////////////////////////////////////////////////////////////////////////
1:     protected final boolean hasMarkedPartials;
/////////////////////////////////////////////////////////////////////////
1:             hasMarkedPartials = backingFile.readBoolean();
/////////////////////////////////////////////////////////////////////////
1:     public boolean hasMarkedPartials()
1:     {
1:         return hasMarkedPartials;
1:     }
1: 
1:     public OnDiskIndexBuilder.Mode mode()
1:     {
1:         return mode;
1:     }
1: 
/////////////////////////////////////////////////////////////////////////
1:         if (exp.getOp() == Expression.Op.PREFIX && mode == OnDiskIndexBuilder.Mode.CONTAINS && !hasMarkedPartials)
1:             throw new UnsupportedOperationException("prefix queries in CONTAINS mode are not supported by this index");
1: 
/////////////////////////////////////////////////////////////////////////
1:             return new PointerTerm(data, termSize, hasMarkedPartials);
/////////////////////////////////////////////////////////////////////////
1:             super(content, size, hasMarkedPartials);
/////////////////////////////////////////////////////////////////////////
1:         public PointerTerm(MappedBuffer content, OnDiskIndexBuilder.TermSize size, boolean hasMarkedPartials)
1:             super(content, size, hasMarkedPartials);
author:Jason Brown
-------------------------------------------------------------------------------
commit:733d1ee
/////////////////////////////////////////////////////////////////////////
1:     @SuppressWarnings("resource")
/////////////////////////////////////////////////////////////////////////
1:             @SuppressWarnings("resource")
============================================================================