1:b681244: /*
1:b681244:  * Licensed to the Apache Software Foundation (ASF) under one or more
1:b681244:  * contributor license agreements.  See the NOTICE file distributed with
1:b681244:  * this work for additional information regarding copyright ownership.
1:b681244:  * The ASF licenses this file to You under the Apache License, Version 2.0
1:b681244:  * (the "License"); you may not use this file except in compliance with
1:b681244:  * the License.  You may obtain a copy of the License at
1:6118711:  *
1:b681244:  *    http://www.apache.org/licenses/LICENSE-2.0
1:6118711:  *
1:b681244:  * Unless required by applicable law or agreed to in writing, software
1:b681244:  * distributed under the License is distributed on an "AS IS" BASIS,
1:b681244:  * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
1:b681244:  * See the License for the specific language governing permissions and
1:b681244:  * limitations under the License.
1:6118711:  */
1:b681244: package org.apache.carbondata.core.indexstore.blockletindex;
1:07a77fa: 
1:b681244: import java.io.ByteArrayOutputStream;
1:b681244: import java.io.DataOutput;
1:b681244: import java.io.DataOutputStream;
1:b681244: import java.io.IOException;
1:531ecdf: import java.io.Serializable;
1:b681244: import java.util.List;
1:5ea538f: 
1:500654e: import org.apache.carbondata.core.constants.CarbonCommonConstants;
1:0586146: import org.apache.carbondata.core.datamap.dev.DataMapModel;
1:b681244: import org.apache.carbondata.core.datastore.block.SegmentProperties;
1:f4a58c5: import org.apache.carbondata.core.datastore.block.SegmentPropertiesAndSchemaHolder;
1:b681244: import org.apache.carbondata.core.datastore.block.TableBlockInfo;
1:44ffaf5: import org.apache.carbondata.core.indexstore.BlockMetaInfo;
1:b681244: import org.apache.carbondata.core.indexstore.BlockletDetailInfo;
1:28f78b2: import org.apache.carbondata.core.indexstore.ExtendedBlocklet;
1:b681244: import org.apache.carbondata.core.indexstore.row.DataMapRow;
1:b681244: import org.apache.carbondata.core.indexstore.row.DataMapRowImpl;
1:cc0e6f1: import org.apache.carbondata.core.indexstore.schema.CarbonRowSchema;
1:79feac9: import org.apache.carbondata.core.memory.MemoryException;
1:b681244: import org.apache.carbondata.core.metadata.blocklet.BlockletInfo;
1:b681244: import org.apache.carbondata.core.metadata.blocklet.DataFileFooter;
1:b681244: import org.apache.carbondata.core.metadata.blocklet.index.BlockletMinMaxIndex;
1:dc29319: import org.apache.carbondata.core.metadata.schema.table.column.CarbonColumn;
1:dc29319: import org.apache.carbondata.core.util.BlockletDataMapUtil;
1:5ea538f: 
2:6118711: /**
1:b681244:  * Datamap implementation for blocklet.
1:6118711:  */
1:6118711: public class BlockletDataMap extends BlockDataMap implements Serializable {
1:2bad144: 
1:531ecdf:   private static final long serialVersionUID = -2170289352240810993L;
1:22958d9:   // total block number in this datamap
1:22958d9:   private int blockNum = 0;
1:2bad144: 
1:3cbabcd:   @Override
1:3cbabcd:   public void init(DataMapModel dataMapModel) throws IOException, MemoryException {
1:6118711:     super.init(dataMapModel);
1:6118711:   }
1:2bad144: 
1:6118711:   /**
1:6118711:    * Method to check the cache level and load metadata based on that information
1:6118711:    *
1:6118711:    * @param blockletDataMapInfo
1:6118711:    * @param indexInfo
1:6118711:    * @throws IOException
3:6118711:    * @throws MemoryException
1:6118711:    */
1:3cbabcd:   @Override
1:dc29319:   protected DataMapRowImpl loadMetadata(CarbonRowSchema[] taskSummarySchema,
1:dc29319:       SegmentProperties segmentProperties, BlockletDataMapModel blockletDataMapInfo,
1:dc29319:       List<DataFileFooter> indexInfo) throws IOException, MemoryException {
1:6118711:     if (isLegacyStore) {
1:dc29319:       return loadBlockInfoForOldStore(taskSummarySchema, segmentProperties, blockletDataMapInfo,
1:dc29319:           indexInfo);
1:6118711:     } else {
1:dc29319:       return loadBlockletMetaInfo(taskSummarySchema, segmentProperties, blockletDataMapInfo,
1:dc29319:           indexInfo);
1:6118711:     }
1:6118711:   }
1:2bad144: 
1:3cbabcd:   @Override
1:dc29319:   protected CarbonRowSchema[] getTaskSummarySchema() {
1:bd02656:     if (isLegacyStore) {
1:bd02656:       return super.getTaskSummarySchema();
1:bd02656:     }
1:dc29319:     SegmentPropertiesAndSchemaHolder.SegmentPropertiesWrapper segmentPropertiesWrapper =
1:dc29319:         SegmentPropertiesAndSchemaHolder.getInstance()
1:dc29319:             .getSegmentPropertiesWrapper(segmentPropertiesIndex);
1:dc29319:     try {
1:dc29319:       return segmentPropertiesWrapper.getTaskSummarySchema(false, isFilePathStored);
1:dc29319:     } catch (MemoryException e) {
1:dc29319:       throw new RuntimeException(e);
1:dc29319:     }
1:6118711:   }
1:2bad144: 
1:3cbabcd:   @Override
1:dc29319:   protected CarbonRowSchema[] getFileFooterEntrySchema() {
1:bd02656:     if (isLegacyStore) {
1:bd02656:       return super.getFileFooterEntrySchema();
1:bd02656:     }
1:dc29319:     return SegmentPropertiesAndSchemaHolder.getInstance()
1:dc29319:         .getSegmentPropertiesWrapper(segmentPropertiesIndex).getBlockletFileFooterEntrySchema();
1:6118711:   }
1:2bad144: 
1:6118711:   /**
1:6118711:    * Method to load blocklet metadata information
1:6118711:    *
1:6118711:    * @param blockletDataMapInfo
1:6118711:    * @param indexInfo
1:6118711:    * @throws IOException
1:6118711:    * @throws MemoryException
1:6118711:    */
1:dc29319:   private DataMapRowImpl loadBlockletMetaInfo(CarbonRowSchema[] taskSummarySchema,
1:dc29319:       SegmentProperties segmentProperties, BlockletDataMapModel blockletDataMapInfo,
1:dc29319:       List<DataFileFooter> indexInfo) throws IOException, MemoryException {
1:5ea538f:     String tempFilePath = null;
1:6118711:     DataMapRowImpl summaryRow = null;
1:dc29319:     CarbonRowSchema[] schema = getFileFooterEntrySchema();
1:6118711:     // Relative blocklet ID is the id assigned to a blocklet within a part file
1:5ea538f:     int relativeBlockletId = 0;
1:f089287:     for (DataFileFooter fileFooter : indexInfo) {
1:f089287:       TableBlockInfo blockInfo = fileFooter.getBlockInfo().getTableBlockInfo();
1:44ffaf5:       BlockMetaInfo blockMetaInfo =
1:44ffaf5:           blockletDataMapInfo.getBlockMetaInfoMap().get(blockInfo.getFilePath());
1:41b0074:       // Here it loads info about all blocklets of index
1:41b0074:       // Only add if the file exists physically. There are scenarios which index file exists inside
1:41b0074:       // merge index but related carbondata files are deleted. In that case we first check whether
1:41b0074:       // the file exists physically or not
1:44ffaf5:       if (blockMetaInfo != null) {
1:6118711:         // this case is for CACHE_LEVEL = BLOCKLET
1:6118711:         // blocklet ID will start from 0 again only when part file path is changed
1:6118711:         if (null == tempFilePath || !tempFilePath.equals(blockInfo.getFilePath())) {
1:6118711:           tempFilePath = blockInfo.getFilePath();
1:6118711:           relativeBlockletId = 0;
1:22958d9:           blockNum++;
1:5ea538f:         }
1:dc29319:         summaryRow = loadToUnsafe(schema, taskSummarySchema, fileFooter, segmentProperties,
1:8e78957:             getMinMaxCacheColumns(), blockInfo.getFilePath(), summaryRow,
1:dc29319:             blockMetaInfo, relativeBlockletId);
1:6118711:         // this is done because relative blocklet id need to be incremented based on the
1:6118711:         // total number of blocklets
1:6118711:         relativeBlockletId += fileFooter.getBlockletList().size();
1:d1d726a:       }
1:2bad144:     }
1:6118711:     return summaryRow;
1:2bad144:   }
1:2bad144: 
1:dc29319:   private DataMapRowImpl loadToUnsafe(CarbonRowSchema[] schema, CarbonRowSchema[] taskSummarySchema,
1:dc29319:       DataFileFooter fileFooter, SegmentProperties segmentProperties,
1:dc29319:       List<CarbonColumn> minMaxCacheColumns, String filePath, DataMapRowImpl summaryRow,
1:44ffaf5:       BlockMetaInfo blockMetaInfo, int relativeBlockletId) {
1:b681244:     List<BlockletInfo> blockletList = fileFooter.getBlockletList();
1:2bad144:     // Add one row to maintain task level min max for segment pruning
1:b8a02f3:     if (!blockletList.isEmpty() && summaryRow == null) {
1:f4a58c5:       summaryRow = new DataMapRowImpl(taskSummarySchema);
1:2bad144:     }
1:b681244:     for (int index = 0; index < blockletList.size(); index++) {
1:6094af6:       DataMapRow row = new DataMapRowImpl(schema);
1:6094af6:       int ordinal = 0;
1:2bad144:       int taskMinMaxOrdinal = 0;
1:b681244:       BlockletInfo blockletInfo = blockletList.get(index);
1:b681244:       BlockletMinMaxIndex minMaxIndex = blockletInfo.getBlockletIndex().getMinMaxIndex();
1:dc29319:       // get min max values for columns to be cached
1:dc29319:       byte[][] minValuesForColumnsToBeCached = BlockletDataMapUtil
1:dc29319:           .getMinMaxForColumnsToBeCached(segmentProperties, minMaxCacheColumns,
1:dc29319:               minMaxIndex.getMinValues());
1:dc29319:       byte[][] maxValuesForColumnsToBeCached = BlockletDataMapUtil
1:dc29319:           .getMinMaxForColumnsToBeCached(segmentProperties, minMaxCacheColumns,
1:dc29319:               minMaxIndex.getMaxValues());
1:dc29319:       row.setRow(addMinMax(schema[ordinal], minValuesForColumnsToBeCached), ordinal);
1:2bad144:       // compute and set task level min values
1:dc29319:       addTaskMinMaxValues(summaryRow, taskSummarySchema, taskMinMaxOrdinal,
1:dc29319:           minValuesForColumnsToBeCached, TASK_MIN_VALUES_INDEX, true);
1:5ea538f:       ordinal++;
1:2bad144:       taskMinMaxOrdinal++;
1:dc29319:       row.setRow(addMinMax(schema[ordinal], maxValuesForColumnsToBeCached), ordinal);
1:2bad144:       // compute and set task level max values
1:dc29319:       addTaskMinMaxValues(summaryRow, taskSummarySchema, taskMinMaxOrdinal,
1:dc29319:           maxValuesForColumnsToBeCached, TASK_MAX_VALUES_INDEX, false);
1:5ea538f:       ordinal++;
1:b681244:       row.setInt(blockletInfo.getNumberOfRows(), ordinal++);
1:f4a58c5:       // add file name
1:f4a58c5:       byte[] filePathBytes =
1:f4a58c5:           getFileNameFromPath(filePath).getBytes(CarbonCommonConstants.DEFAULT_CHARSET_CLASS);
1:6094af6:       row.setByteArray(filePathBytes, ordinal++);
1:6094af6:       // add version number
1:6094af6:       row.setShort(fileFooter.getVersionId().number(), ordinal++);
1:6094af6:       // add schema updated time
1:6094af6:       row.setLong(fileFooter.getSchemaUpdatedTimeStamp(), ordinal++);
1:b681244:       byte[] serializedData;
1:2bad144:       try {
1:6118711:         // Add block footer offset, it is used if we need to read footer of block
1:6118711:         row.setLong(fileFooter.getBlockInfo().getTableBlockInfo().getBlockOffset(), ordinal++);
1:6118711:         setLocations(blockMetaInfo.getLocationInfo(), row, ordinal++);
1:6118711:         // Store block size
1:6118711:         row.setLong(blockMetaInfo.getSize(), ordinal++);
1:6118711:         // add blocklet info
1:6094af6:         ByteArrayOutputStream stream = new ByteArrayOutputStream();
1:6094af6:         DataOutput dataOutput = new DataOutputStream(stream);
1:b681244:         blockletInfo.write(dataOutput);
1:b681244:         serializedData = stream.toByteArray();
1:6094af6:         row.setByteArray(serializedData, ordinal++);
1:6118711:         // add pages
1:6118711:         row.setShort((short) blockletInfo.getNumberOfPages(), ordinal++);
1:6118711:         // for relative blocklet id i.e blocklet id that belongs to a particular carbondata file
1:6118711:         row.setShort((short) relativeBlockletId++, ordinal);
1:f4a58c5:         memoryDMStore.addIndexRow(schema, row);
1:2bad144:       } catch (Exception e) {
1:2bad144:         throw new RuntimeException(e);
1:2bad144:       }
1:2bad144:     }
1:6094af6:     return summaryRow;
2:2bad144:   }
1:2bad144: 
1:3cbabcd:   @Override
1:28f78b2:   public ExtendedBlocklet getDetailedBlocklet(String blockletId) {
1:6118711:     if (isLegacyStore) {
1:bd02656:       return super.getDetailedBlocklet(blockletId);
1:2bad144:     }
1:6118711:     int absoluteBlockletId = Integer.parseInt(blockletId);
1:dc29319:     DataMapRow safeRow = memoryDMStore.getDataMapRow(getFileFooterEntrySchema(), absoluteBlockletId)
1:dc29319:         .convertToSafeRow();
1:6118711:     short relativeBlockletId = safeRow.getShort(BLOCKLET_ID_INDEX);
1:f4a58c5:     String filePath = getFilePath();
1:4df335f:     return createBlocklet(safeRow, getFileNameWithFilePath(safeRow, filePath), relativeBlockletId,
1:4df335f:         false);
1:2bad144:   }
1:0bbfa85: 
1:3cbabcd:   @Override
1:6118711:   protected short getBlockletId(DataMapRow dataMapRow) {
1:bd02656:     if (isLegacyStore) {
1:bd02656:       return super.getBlockletId(dataMapRow);
1:bd02656:     }
1:6118711:     return dataMapRow.getShort(BLOCKLET_ID_INDEX);
1:bd02656:   }
1:f4a58c5: 
1:3cbabcd:   @Override
1:4df335f:   protected ExtendedBlocklet createBlocklet(DataMapRow row, String fileName, short blockletId,
1:4df335f:       boolean useMinMaxForPruning) {
1:bd02656:     if (isLegacyStore) {
1:4df335f:       return super.createBlocklet(row, fileName, blockletId, useMinMaxForPruning);
1:f4a58c5:     }
1:f4a58c5:     ExtendedBlocklet blocklet = new ExtendedBlocklet(fileName, blockletId + "");
1:6118711:     BlockletDetailInfo detailInfo = getBlockletDetailInfo(row, blockletId, blocklet);
1:f4a58c5:     detailInfo.setColumnSchemas(getColumnSchema());
1:6118711:     detailInfo.setBlockletInfoBinary(row.getByteArray(BLOCKLET_INFO_INDEX));
1:6118711:     detailInfo.setPagesCount(row.getShort(BLOCKLET_PAGE_COUNT_INDEX));
1:4df335f:     detailInfo.setUseMinMaxForPruning(useMinMaxForPruning);
1:b681244:     blocklet.setDetailInfo(detailInfo);
1:b681244:     return blocklet;
1:2bad144:   }
1:07a77fa: 
1:3cbabcd:   @Override
1:3cbabcd:   protected short getBlockletNumOfEntry(int index) {
1:3cbabcd:     if (isLegacyStore) {
1:3cbabcd:       return super.getBlockletNumOfEntry(index);
1:3cbabcd:     } else {
1:3cbabcd:       //in blocklet datamap, each entry contains info of one blocklet
1:3cbabcd:       return 1;
1:3cbabcd:     }
1:3cbabcd:   }
1:3cbabcd: 
1:3cbabcd:   @Override
1:22958d9:   protected int getTotalBlocks() {
1:22958d9:     if (isLegacyStore) {
1:22958d9:       return super.getTotalBlocklets();
1:22958d9:     } else {
1:22958d9:       return blockNum;
1:22958d9:     }
1:22958d9:   }
1:22958d9: 
1:22958d9:   @Override
1:3cbabcd:   protected int getTotalBlocklets() {
1:3cbabcd:     if (isLegacyStore) {
1:3cbabcd:       return super.getTotalBlocklets();
1:3cbabcd:     } else {
1:3cbabcd:       return memoryDMStore.getRowCount();
1:3cbabcd:     }
1:3cbabcd:   }
1:3cbabcd: 
1:2bad144: }
============================================================================
author:Manhua
-------------------------------------------------------------------------------
commit:22958d9
/////////////////////////////////////////////////////////////////////////
1:   // total block number in this datamap
1:   private int blockNum = 0;
/////////////////////////////////////////////////////////////////////////
1:           blockNum++;
/////////////////////////////////////////////////////////////////////////
1:   protected int getTotalBlocks() {
1:     if (isLegacyStore) {
1:       return super.getTotalBlocklets();
1:     } else {
1:       return blockNum;
1:     }
1:   }
1: 
1:   @Override
commit:3cbabcd
/////////////////////////////////////////////////////////////////////////
1:   @Override
1:   public void init(DataMapModel dataMapModel) throws IOException, MemoryException {
/////////////////////////////////////////////////////////////////////////
1:   @Override
/////////////////////////////////////////////////////////////////////////
1:   @Override
/////////////////////////////////////////////////////////////////////////
1:   @Override
/////////////////////////////////////////////////////////////////////////
1:   @Override
/////////////////////////////////////////////////////////////////////////
1:   @Override
/////////////////////////////////////////////////////////////////////////
1:   @Override
/////////////////////////////////////////////////////////////////////////
1:   @Override
1:   protected short getBlockletNumOfEntry(int index) {
1:     if (isLegacyStore) {
1:       return super.getBlockletNumOfEntry(index);
1:     } else {
1:       //in blocklet datamap, each entry contains info of one blocklet
1:       return 1;
1:     }
1:   }
1: 
1:   @Override
1:   protected int getTotalBlocklets() {
1:     if (isLegacyStore) {
1:       return super.getTotalBlocklets();
1:     } else {
1:       return memoryDMStore.getRowCount();
1:     }
1:   }
1: 
author:manishgupta88
-------------------------------------------------------------------------------
commit:4df335f
/////////////////////////////////////////////////////////////////////////
1:     return createBlocklet(safeRow, getFileNameWithFilePath(safeRow, filePath), relativeBlockletId,
1:         false);
/////////////////////////////////////////////////////////////////////////
1:   protected ExtendedBlocklet createBlocklet(DataMapRow row, String fileName, short blockletId,
1:       boolean useMinMaxForPruning) {
1:       return super.createBlocklet(row, fileName, blockletId, useMinMaxForPruning);
1:     detailInfo.setUseMinMaxForPruning(useMinMaxForPruning);
commit:bd02656
/////////////////////////////////////////////////////////////////////////
1:     if (isLegacyStore) {
1:       return super.getTaskSummarySchema();
1:     }
/////////////////////////////////////////////////////////////////////////
1:     if (isLegacyStore) {
1:       return super.getFileFooterEntrySchema();
1:     }
/////////////////////////////////////////////////////////////////////////
1:       return super.getDetailedBlocklet(blockletId);
/////////////////////////////////////////////////////////////////////////
1:     if (isLegacyStore) {
1:       return super.getBlockletId(dataMapRow);
1:     }
1:     if (isLegacyStore) {
0:       return super.createBlocklet(row, fileName, blockletId);
1:     }
commit:dc29319
/////////////////////////////////////////////////////////////////////////
1: import org.apache.carbondata.core.metadata.schema.table.column.CarbonColumn;
1: import org.apache.carbondata.core.util.BlockletDataMapUtil;
/////////////////////////////////////////////////////////////////////////
1:   protected DataMapRowImpl loadMetadata(CarbonRowSchema[] taskSummarySchema,
1:       SegmentProperties segmentProperties, BlockletDataMapModel blockletDataMapInfo,
1:       List<DataFileFooter> indexInfo) throws IOException, MemoryException {
1:       return loadBlockInfoForOldStore(taskSummarySchema, segmentProperties, blockletDataMapInfo,
1:           indexInfo);
1:       return loadBlockletMetaInfo(taskSummarySchema, segmentProperties, blockletDataMapInfo,
1:           indexInfo);
1:   protected CarbonRowSchema[] getTaskSummarySchema() {
1:     SegmentPropertiesAndSchemaHolder.SegmentPropertiesWrapper segmentPropertiesWrapper =
1:         SegmentPropertiesAndSchemaHolder.getInstance()
1:             .getSegmentPropertiesWrapper(segmentPropertiesIndex);
1:     try {
1:       return segmentPropertiesWrapper.getTaskSummarySchema(false, isFilePathStored);
1:     } catch (MemoryException e) {
1:       throw new RuntimeException(e);
1:     }
1:   protected CarbonRowSchema[] getFileFooterEntrySchema() {
1:     return SegmentPropertiesAndSchemaHolder.getInstance()
1:         .getSegmentPropertiesWrapper(segmentPropertiesIndex).getBlockletFileFooterEntrySchema();
/////////////////////////////////////////////////////////////////////////
1:   private DataMapRowImpl loadBlockletMetaInfo(CarbonRowSchema[] taskSummarySchema,
1:       SegmentProperties segmentProperties, BlockletDataMapModel blockletDataMapInfo,
1:       List<DataFileFooter> indexInfo) throws IOException, MemoryException {
1:     CarbonRowSchema[] schema = getFileFooterEntrySchema();
/////////////////////////////////////////////////////////////////////////
1:         summaryRow = loadToUnsafe(schema, taskSummarySchema, fileFooter, segmentProperties,
0:             blockletDataMapInfo.getMinMaxCacheColumns(), blockInfo.getFilePath(), summaryRow,
1:             blockMetaInfo, relativeBlockletId);
/////////////////////////////////////////////////////////////////////////
1:   private DataMapRowImpl loadToUnsafe(CarbonRowSchema[] schema, CarbonRowSchema[] taskSummarySchema,
1:       DataFileFooter fileFooter, SegmentProperties segmentProperties,
1:       List<CarbonColumn> minMaxCacheColumns, String filePath, DataMapRowImpl summaryRow,
/////////////////////////////////////////////////////////////////////////
1:       // get min max values for columns to be cached
1:       byte[][] minValuesForColumnsToBeCached = BlockletDataMapUtil
1:           .getMinMaxForColumnsToBeCached(segmentProperties, minMaxCacheColumns,
1:               minMaxIndex.getMinValues());
1:       byte[][] maxValuesForColumnsToBeCached = BlockletDataMapUtil
1:           .getMinMaxForColumnsToBeCached(segmentProperties, minMaxCacheColumns,
1:               minMaxIndex.getMaxValues());
1:       row.setRow(addMinMax(schema[ordinal], minValuesForColumnsToBeCached), ordinal);
1:       addTaskMinMaxValues(summaryRow, taskSummarySchema, taskMinMaxOrdinal,
1:           minValuesForColumnsToBeCached, TASK_MIN_VALUES_INDEX, true);
1:       row.setRow(addMinMax(schema[ordinal], maxValuesForColumnsToBeCached), ordinal);
1:       addTaskMinMaxValues(summaryRow, taskSummarySchema, taskMinMaxOrdinal,
1:           maxValuesForColumnsToBeCached, TASK_MAX_VALUES_INDEX, false);
/////////////////////////////////////////////////////////////////////////
1:     DataMapRow safeRow = memoryDMStore.getDataMapRow(getFileFooterEntrySchema(), absoluteBlockletId)
1:         .convertToSafeRow();
/////////////////////////////////////////////////////////////////////////
commit:f4a58c5
/////////////////////////////////////////////////////////////////////////
1: import org.apache.carbondata.core.datastore.block.SegmentPropertiesAndSchemaHolder;
/////////////////////////////////////////////////////////////////////////
0:   protected DataMapRowImpl loadMetadata(SegmentProperties segmentProperties,
0:       BlockletDataMapModel blockletDataMapInfo, List<DataFileFooter> indexInfo)
0:       throws IOException, MemoryException {
0:       return loadBlockInfoForOldStore(segmentProperties, blockletDataMapInfo, indexInfo);
0:       return loadBlockletMetaInfo(segmentProperties, blockletDataMapInfo, indexInfo);
0:   protected void createMemoryDMStore(boolean addToUnsafe) throws MemoryException {
0:     memoryDMStore = getMemoryDMStore(addToUnsafe);
/////////////////////////////////////////////////////////////////////////
0:   protected void createSummarySchema(SegmentProperties segmentProperties, boolean addToUnsafe)
0:     CarbonRowSchema[] taskSummarySchema =
0:         SchemaGenerator.createTaskSummarySchema(segmentProperties, false, isFilePathStored);
0:     SegmentPropertiesAndSchemaHolder.getInstance()
0:         .getSegmentPropertiesWrapper(segmentPropertiesIndex)
0:         .setTaskSummarySchema(taskSummarySchema);
0:     taskSummaryDMStore = getMemoryDMStore(addToUnsafe);
/////////////////////////////////////////////////////////////////////////
0:   private DataMapRowImpl loadBlockletMetaInfo(SegmentProperties segmentProperties,
0:       BlockletDataMapModel blockletDataMapInfo, List<DataFileFooter> indexInfo)
0:       throws IOException, MemoryException {
/////////////////////////////////////////////////////////////////////////
0:     CarbonRowSchema[] schema = getSchema();
0:     CarbonRowSchema[] taskSummarySchema = getTaskSummarySchema();
1:       summaryRow = new DataMapRowImpl(taskSummarySchema);
/////////////////////////////////////////////////////////////////////////
0:       addTaskMinMaxValues(summaryRow, minMaxLen, taskSummarySchema, taskMinMaxOrdinal,
0:       addTaskMinMaxValues(summaryRow, minMaxLen, taskSummarySchema, taskMinMaxOrdinal,
1:       // add file name
1:       byte[] filePathBytes =
1:           getFileNameFromPath(filePath).getBytes(CarbonCommonConstants.DEFAULT_CHARSET_CLASS);
/////////////////////////////////////////////////////////////////////////
1:         memoryDMStore.addIndexRow(schema, row);
/////////////////////////////////////////////////////////////////////////
0:     DataMapRow safeRow =
0:         memoryDMStore.getDataMapRow(getSchema(), absoluteBlockletId).convertToSafeRow();
1:     String filePath = getFilePath();
0:     return createBlocklet(safeRow, getFileNameWithFilePath(safeRow, filePath), relativeBlockletId);
0:   protected CarbonRowSchema[] getSchema() {
0:     return SegmentPropertiesAndSchemaHolder.getInstance()
0:         .getSegmentPropertiesWrapper(segmentPropertiesIndex).getBlocketSchema();
1:   }
1: 
0:   protected ExtendedBlocklet createBlocklet(DataMapRow row, String fileName, short blockletId) {
1:     ExtendedBlocklet blocklet = new ExtendedBlocklet(fileName, blockletId + "");
1:     detailInfo.setColumnSchemas(getColumnSchema());
commit:6118711
/////////////////////////////////////////////////////////////////////////
0: import org.apache.carbondata.core.indexstore.schema.SchemaGenerator;
1: public class BlockletDataMap extends BlockDataMap implements Serializable {
0:   @Override public void init(DataMapModel dataMapModel) throws IOException, MemoryException {
1:     super.init(dataMapModel);
1:   }
1:   /**
1:    * Method to check the cache level and load metadata based on that information
1:    *
1:    * @param blockletDataMapInfo
1:    * @param indexInfo
1:    * @throws IOException
1:    * @throws MemoryException
1:    */
0:   protected DataMapRowImpl loadMetadata(BlockletDataMapModel blockletDataMapInfo,
0:       List<DataFileFooter> indexInfo) throws IOException, MemoryException {
1:     if (isLegacyStore) {
0:       return loadBlockInfoForOldStore(blockletDataMapInfo, indexInfo);
1:     } else {
0:       return loadBlockletMetaInfo(blockletDataMapInfo, indexInfo);
1:     }
1:   }
1:   /**
0:    * Method to create blocklet schema
1:    *
0:    * @param segmentProperties
0:    * @param addToUnsafe
1:    * @throws MemoryException
1:    */
0:   protected void createSchema(SegmentProperties segmentProperties, boolean addToUnsafe)
0:       throws MemoryException {
0:     CarbonRowSchema[] schema = SchemaGenerator.createBlockletSchema(segmentProperties);
0:     memoryDMStore = getMemoryDMStore(schema, addToUnsafe);
1:   }
1:   /**
0:    * Creates the schema to store summary information or the information which can be stored only
0:    * once per datamap. It stores datamap level max/min of each column and partition information of
0:    * datamap
1:    *
0:    * @param segmentProperties
1:    * @throws MemoryException
1:    */
0:   protected void createSummarySchema(SegmentProperties segmentProperties, byte[] schemaBinary,
0:       byte[] filePath, byte[] fileName, byte[] segmentId, boolean addToUnsafe)
0:       throws MemoryException {
0:     CarbonRowSchema[] taskSummarySchema = SchemaGenerator
0:         .createTaskSummarySchema(segmentProperties, schemaBinary, filePath, fileName, segmentId,
0:             false);
0:     taskSummaryDMStore = getMemoryDMStore(taskSummarySchema, addToUnsafe);
1:   }
1:   /**
1:    * Method to load blocklet metadata information
1:    *
1:    * @param blockletDataMapInfo
1:    * @param indexInfo
1:    * @throws IOException
1:    * @throws MemoryException
1:    */
0:   private DataMapRowImpl loadBlockletMetaInfo(BlockletDataMapModel blockletDataMapInfo,
0:       List<DataFileFooter> indexInfo) throws IOException, MemoryException {
1:     DataMapRowImpl summaryRow = null;
1:     // Relative blocklet ID is the id assigned to a blocklet within a part file
/////////////////////////////////////////////////////////////////////////
1:         // this case is for CACHE_LEVEL = BLOCKLET
1:         // blocklet ID will start from 0 again only when part file path is changed
1:         if (null == tempFilePath || !tempFilePath.equals(blockInfo.getFilePath())) {
1:           tempFilePath = blockInfo.getFilePath();
1:           relativeBlockletId = 0;
0:         summaryRow =
0:             loadToUnsafe(fileFooter, segmentProperties, blockInfo.getFilePath(), summaryRow,
0:                 blockMetaInfo, relativeBlockletId);
1:         // this is done because relative blocklet id need to be incremented based on the
1:         // total number of blocklets
1:         relativeBlockletId += fileFooter.getBlockletList().size();
1:     return summaryRow;
/////////////////////////////////////////////////////////////////////////
0:       summaryRow = new DataMapRowImpl(taskSummaryDMStore.getSchema());
0:       row.setRow(addMinMax(minMaxLen, schema[ordinal], minMaxIndex.getMinValues()), ordinal);
0:       addTaskMinMaxValues(summaryRow, minMaxLen, taskSummaryDMStore.getSchema(), taskMinMaxOrdinal,
0:           minMaxIndex.getMinValues(), TASK_MIN_VALUES_INDEX, true);
0:       row.setRow(addMinMax(minMaxLen, schema[ordinal], minMaxIndex.getMaxValues()), ordinal);
0:       addTaskMinMaxValues(summaryRow, minMaxLen, taskSummaryDMStore.getSchema(), taskMinMaxOrdinal,
0:           minMaxIndex.getMaxValues(), TASK_MAX_VALUES_INDEX, false);
1:         // Add block footer offset, it is used if we need to read footer of block
1:         row.setLong(fileFooter.getBlockInfo().getTableBlockInfo().getBlockOffset(), ordinal++);
1:         setLocations(blockMetaInfo.getLocationInfo(), row, ordinal++);
1:         // Store block size
1:         row.setLong(blockMetaInfo.getSize(), ordinal++);
1:         // add blocklet info
1:         // add pages
1:         row.setShort((short) blockletInfo.getNumberOfPages(), ordinal++);
1:         // for relative blocklet id i.e blocklet id that belongs to a particular carbondata file
1:         row.setShort((short) relativeBlockletId++, ordinal);
1:     if (isLegacyStore) {
0:       super.getDetailedBlocklet(blockletId);
1:     int absoluteBlockletId = Integer.parseInt(blockletId);
0:     DataMapRow safeRow = memoryDMStore.getDataMapRow(absoluteBlockletId).convertToSafeRow();
1:     short relativeBlockletId = safeRow.getShort(BLOCKLET_ID_INDEX);
0:     return createBlocklet(safeRow, relativeBlockletId);
1:   protected short getBlockletId(DataMapRow dataMapRow) {
1:     return dataMapRow.getShort(BLOCKLET_ID_INDEX);
0:   protected ExtendedBlocklet createBlocklet(DataMapRow row, short blockletId) {
1:     BlockletDetailInfo detailInfo = getBlockletDetailInfo(row, blockletId, blocklet);
1:     detailInfo.setBlockletInfoBinary(row.getByteArray(BLOCKLET_INFO_INDEX));
1:     detailInfo.setPagesCount(row.getShort(BLOCKLET_PAGE_COUNT_INDEX));
commit:1248bd4
/////////////////////////////////////////////////////////////////////////
/////////////////////////////////////////////////////////////////////////
/////////////////////////////////////////////////////////////////////////
0: import org.apache.carbondata.core.util.CarbonUtil;
/////////////////////////////////////////////////////////////////////////
0:     byte[][] maxValues = updateMaxValues(minMaxIndex.getMaxValues(), minMaxLen);
0:     // update min max values in case of old store
0:     byte[][] updatedMinValues =
0:         CarbonUtil.updateMinMaxValues(fileFooter, maxValues, minValues, true);
0:     byte[][] updatedMaxValues =
0:         CarbonUtil.updateMinMaxValues(fileFooter, maxValues, minValues, false);
0:     row.setRow(addMinMax(minMaxLen, schema[ordinal], updatedMinValues), ordinal);
0:         unsafeMemorySummaryDMStore.getSchema()[taskMinMaxOrdinal], updatedMinValues,
0:     row.setRow(addMinMax(minMaxLen, schema[ordinal], updatedMaxValues), ordinal);
0:         unsafeMemorySummaryDMStore.getSchema()[taskMinMaxOrdinal], updatedMaxValues,
/////////////////////////////////////////////////////////////////////////
/////////////////////////////////////////////////////////////////////////
0:       // Remove B-tree jump logic as start and end key prepared is not
0:       // correct for old store scenarios
0:       int startIndex = 0;
0:       int endIndex = unsafeMemoryDMStore.getRowCount();
0:       while (startIndex < endIndex) {
commit:5ea538f
/////////////////////////////////////////////////////////////////////////
0:   private static int BLOCKLET_ID_INDEX = 11;
1: 
/////////////////////////////////////////////////////////////////////////
0:     // below 2 variables will be used for fetching the relative blocklet id. Relative blocklet ID
0:     // is id assigned to a blocklet within a part file
1:     String tempFilePath = null;
1:     int relativeBlockletId = 0;
/////////////////////////////////////////////////////////////////////////
0:           // blocklet ID will start from 0 again only when part file path is changed
0:           if (null == tempFilePath || !tempFilePath.equals(blockInfo.getFilePath())) {
0:             tempFilePath = blockInfo.getFilePath();
0:             relativeBlockletId = 0;
1:           }
0:                   locations, relativeBlockletId);
0:           // this is done because relative blocklet id need to be incremented based on the
0:           // total number of blocklets
0:           relativeBlockletId += fileFooter.getBlockletList().size();
/////////////////////////////////////////////////////////////////////////
0:       String[] locations, int relativeBlockletId) {
/////////////////////////////////////////////////////////////////////////
1:         ordinal++;
0:         // for relative blockelt id i.e blocklet id that belongs to a particular part file
0:         row.setShort((short) relativeBlockletId++, ordinal);
/////////////////////////////////////////////////////////////////////////
1:       ordinal++;
0:       // for relative blocklet id. Value is -1 because in case of old store blocklet info will
0:       // not be present in the index file and in that case we will not knwo the total number of
0:       // blocklets
0:       row.setShort((short) -1, ordinal);
/////////////////////////////////////////////////////////////////////////
0:     // for relative blocklet id i.e. blocklet id that belongs to a particular part file.
0:     indexSchemas.add(new CarbonRowSchema.FixedCarbonRowSchema(DataTypes.SHORT));
1: 
/////////////////////////////////////////////////////////////////////////
0:         blocklets.add(createBlocklet(safeRow, safeRow.getShort(BLOCKLET_ID_INDEX)));
/////////////////////////////////////////////////////////////////////////
0:         int blockletId = safeRow.getShort(BLOCKLET_ID_INDEX);
0:                 getMinMaxValue(safeRow, MIN_VALUES_INDEX), filePath, blockletId);
0:           blocklets.add(createBlocklet(safeRow, blockletId));
/////////////////////////////////////////////////////////////////////////
0:     return createBlocklet(safeRow, safeRow.getShort(BLOCKLET_ID_INDEX));
commit:d1d726a
/////////////////////////////////////////////////////////////////////////
0:       String uniqueBlockPath = filePath.substring(filePath.lastIndexOf("/Part") + 1);
0:       // this case will come in case of old store where index file does not contain the
0:       // blocklet information
0:       if (blockletId != -1) {
0:         uniqueBlockPath = uniqueBlockPath + CarbonCommonConstants.FILE_SEPARATOR + blockletId;
1:       }
commit:2bad144
/////////////////////////////////////////////////////////////////////////
0: import org.apache.carbondata.core.scan.filter.FilterExpressionProcessor;
0: import org.apache.carbondata.core.util.ByteUtil;
/////////////////////////////////////////////////////////////////////////
0:   private static int TASK_MIN_VALUES_INDEX = 0;
1: 
0:   private static int TASK_MAX_VALUES_INDEX = 1;
1: 
0:   private UnsafeMemoryDMStore unsafeMemoryTaskMinMaxDMStore;
1: 
/////////////////////////////////////////////////////////////////////////
0:         createTaskMinMaxSchema(segmentProperties);
/////////////////////////////////////////////////////////////////////////
0:     if (null != unsafeMemoryTaskMinMaxDMStore) {
0:       unsafeMemoryTaskMinMaxDMStore.finishWriting();
1:     }
/////////////////////////////////////////////////////////////////////////
0:     DataMapRow taskMinMaxRow = null;
1:     // Add one row to maintain task level min max for segment pruning
0:     if (!blockletList.isEmpty()) {
0:       taskMinMaxRow = new DataMapRowImpl(unsafeMemoryTaskMinMaxDMStore.getSchema());
1:     }
1:       int taskMinMaxOrdinal = 0;
0:       byte[][] minValues = updateMinValues(minMaxIndex.getMinValues(), minMaxLen);
0:       row.setRow(addMinMax(minMaxLen, schema[ordinal], minValues), ordinal);
1:       // compute and set task level min values
0:       addTaskMinMaxValues(taskMinMaxRow, minMaxLen,
0:           unsafeMemoryTaskMinMaxDMStore.getSchema()[taskMinMaxOrdinal], minValues,
0:           TASK_MIN_VALUES_INDEX, true);
1:       taskMinMaxOrdinal++;
0:       byte[][] maxValues = updateMaxValues(minMaxIndex.getMaxValues(), minMaxLen);
0:       row.setRow(addMinMax(minMaxLen, schema[ordinal], maxValues), ordinal);
1:       // compute and set task level max values
0:       addTaskMinMaxValues(taskMinMaxRow, minMaxLen,
0:           unsafeMemoryTaskMinMaxDMStore.getSchema()[taskMinMaxOrdinal], maxValues,
0:           TASK_MAX_VALUES_INDEX, false);
0:       byte[] filePathBytes = filePath.getBytes(CarbonCommonConstants.DEFAULT_CHARSET_CLASS);
/////////////////////////////////////////////////////////////////////////
0:     // write the task level min/max row to unsafe memory store
0:     if (null != taskMinMaxRow) {
0:       addTaskMinMaxRowToUnsafeMemoryStore(taskMinMaxRow);
1:     }
1:   }
1: 
0:   private void addTaskMinMaxRowToUnsafeMemoryStore(DataMapRow taskMinMaxRow) {
1:     try {
0:       unsafeMemoryTaskMinMaxDMStore.addIndexRowToUnsafe(taskMinMaxRow);
1:     } catch (Exception e) {
1:       throw new RuntimeException(e);
1:     }
/////////////////////////////////////////////////////////////////////////
0:   /**
0:    * This method will compute min/max values at task level
0:    *
0:    * @param taskMinMaxRow
0:    * @param minMaxLen
0:    * @param carbonRowSchema
0:    * @param minMaxValue
0:    * @param ordinal
0:    * @param isMinValueComparison
0:    */
0:   private void addTaskMinMaxValues(DataMapRow taskMinMaxRow, int[] minMaxLen,
0:       CarbonRowSchema carbonRowSchema, byte[][] minMaxValue, int ordinal,
0:       boolean isMinValueComparison) {
0:     DataMapRow row = taskMinMaxRow.getRow(ordinal);
0:     byte[][] updatedMinMaxValues = minMaxValue;
0:     if (null == row) {
0:       CarbonRowSchema[] minSchemas =
0:           ((CarbonRowSchema.StructCarbonRowSchema) carbonRowSchema).getChildSchemas();
0:       row = new DataMapRowImpl(minSchemas);
0:     } else {
0:       byte[][] existingMinMaxValues = getMinMaxValue(taskMinMaxRow, ordinal);
0:       // Compare and update min max values
0:       for (int i = 0; i < minMaxLen.length; i++) {
0:         int compare =
0:             ByteUtil.UnsafeComparer.INSTANCE.compareTo(existingMinMaxValues[i], minMaxValue[i]);
0:         if (isMinValueComparison) {
0:           if (compare < 0) {
0:             updatedMinMaxValues[i] = existingMinMaxValues[i];
1:           }
0:         } else if (compare > 0) {
0:           updatedMinMaxValues[i] = existingMinMaxValues[i];
1:         }
1:       }
1:     }
0:     int minMaxOrdinal = 0;
0:     // min/max value adding
0:     for (int i = 0; i < minMaxLen.length; i++) {
0:       row.setByteArray(updatedMinMaxValues[i], minMaxOrdinal++);
1:     }
0:     taskMinMaxRow.setRow(row, ordinal);
1:   }
1: 
0:     getMinMaxSchema(segmentProperties, indexSchemas);
/////////////////////////////////////////////////////////////////////////
0:   private void createTaskMinMaxSchema(SegmentProperties segmentProperties) throws MemoryException {
0:     List<CarbonRowSchema> taskMinMaxSchemas = new ArrayList<>(2);
0:     getMinMaxSchema(segmentProperties, taskMinMaxSchemas);
0:     unsafeMemoryTaskMinMaxDMStore = new UnsafeMemoryDMStore(
0:         taskMinMaxSchemas.toArray(new CarbonRowSchema[taskMinMaxSchemas.size()]));
0:   }
1: 
0:   private void getMinMaxSchema(SegmentProperties segmentProperties,
0:       List<CarbonRowSchema> minMaxSchemas) {
0:     // Index key
0:     int[] minMaxLen = segmentProperties.getColumnsValueSize();
0:     // do it 2 times, one for min and one for max.
0:     for (int k = 0; k < 2; k++) {
0:       CarbonRowSchema[] mapSchemas = new CarbonRowSchema[minMaxLen.length];
0:       for (int i = 0; i < minMaxLen.length; i++) {
0:         if (minMaxLen[i] <= 0) {
0:           mapSchemas[i] = new CarbonRowSchema.VariableCarbonRowSchema(DataTypes.BYTE_ARRAY);
0:         } else {
0:           mapSchemas[i] =
0:               new CarbonRowSchema.FixedCarbonRowSchema(DataTypes.BYTE_ARRAY, minMaxLen[i]);
0:         }
0:       }
0:       CarbonRowSchema mapSchema =
0:           new CarbonRowSchema.StructCarbonRowSchema(DataTypes.createDefaultStructType(),
0:               mapSchemas);
0:       minMaxSchemas.add(mapSchema);
0:     }
0:   }
1: 
0:   @Override
0:   public boolean isScanRequired(FilterResolverIntf filterExp) {
0:     FilterExecuter filterExecuter =
0:         FilterUtil.getFilterExecuterTree(filterExp, segmentProperties, null);
0:     for (int i = 0; i < unsafeMemoryTaskMinMaxDMStore.getRowCount(); i++) {
0:       DataMapRow unsafeRow = unsafeMemoryTaskMinMaxDMStore.getUnsafeRow(i);
0:       boolean isScanRequired = FilterExpressionProcessor
0:           .isScanRequired(filterExecuter, getMinMaxValue(unsafeRow, TASK_MAX_VALUES_INDEX),
0:               getMinMaxValue(unsafeRow, TASK_MIN_VALUES_INDEX));
0:       if (isScanRequired) {
0:         return true;
0:       }
0:     }
0:     return false;
0:   }
1: 
/////////////////////////////////////////////////////////////////////////
0:     // clear task min/max unsafe memory
0:     if (null != unsafeMemoryTaskMinMaxDMStore) {
0:       unsafeMemoryTaskMinMaxDMStore.freeMemory();
0:       unsafeMemoryTaskMinMaxDMStore = null;
0:     }
/////////////////////////////////////////////////////////////////////////
0:     long memoryUsed = 0L;
0:       memoryUsed += unsafeMemoryDMStore.getMemoryUsed();
0:     if (null != unsafeMemoryTaskMinMaxDMStore) {
0:       memoryUsed += unsafeMemoryTaskMinMaxDMStore.getMemoryUsed();
0:     }
0:     return memoryUsed;
commit:0bbfa85
/////////////////////////////////////////////////////////////////////////
0: import org.apache.carbondata.core.scan.filter.executer.ImplicitColumnFilterExecutor;
/////////////////////////////////////////////////////////////////////////
0:         String filePath = new String(unsafeRow.getByteArray(FILE_PATH_INDEX),
0:             CarbonCommonConstants.DEFAULT_CHARSET_CLASS);
0:         boolean isValid =
0:             addBlockBasedOnMinMaxValue(filterExecuter, getMinMaxValue(unsafeRow, MAX_VALUES_INDEX),
0:                 getMinMaxValue(unsafeRow, MIN_VALUES_INDEX), filePath);
0:         if (isValid) {
/////////////////////////////////////////////////////////////////////////
0:   /**
0:    * select the blocks based on column min and max value
0:    *
0:    * @param filterExecuter
0:    * @param maxValue
0:    * @param minValue
0:    * @param filePath
0:    * @return
0:    */
0:   private boolean addBlockBasedOnMinMaxValue(FilterExecuter filterExecuter, byte[][] maxValue,
0:       byte[][] minValue, String filePath) {
0:     BitSet bitSet = null;
0:     if (filterExecuter instanceof ImplicitColumnFilterExecutor) {
0:       String uniqueBlockPath = filePath.substring(filePath.lastIndexOf("/Part") + 1);
0:       bitSet = ((ImplicitColumnFilterExecutor) filterExecuter)
0:           .isFilterValuesPresentInBlockOrBlocklet(maxValue, minValue, uniqueBlockPath);
0:     } else {
0:       bitSet = filterExecuter.isScanRequired(maxValue, minValue);
0:     }
0:     if (!bitSet.isEmpty()) {
0:       return true;
0:     } else {
0:       return false;
0:     }
0:   }
1: 
author:m00258959
-------------------------------------------------------------------------------
commit:8e78957
/////////////////////////////////////////////////////////////////////////
1:             getMinMaxCacheColumns(), blockInfo.getFilePath(), summaryRow,
author:xuchuanyin
-------------------------------------------------------------------------------
commit:dc53dee
/////////////////////////////////////////////////////////////////////////
0:           boolean isVarchar = false;
0:           if (i < segmentProperties.getDimensions().size()
0:               && segmentProperties.getDimensions().get(i).getDataType() == DataTypes.VARCHAR) {
0:             isVarchar = true;
0:           }
0:           mapSchemas[i] = new CarbonRowSchema.VariableCarbonRowSchema(DataTypes.BYTE_ARRAY,
0:               isVarchar);
author:akashrn5
-------------------------------------------------------------------------------
commit:07a77fa
/////////////////////////////////////////////////////////////////////////
0:   @Override
0:   public void finish() {
1: 
0:   }
1: 
author:dhatchayani
-------------------------------------------------------------------------------
commit:51db049
/////////////////////////////////////////////////////////////////////////
0:   public byte[] getColumnSchemaBinary() {
commit:531ecdf
/////////////////////////////////////////////////////////////////////////
0: import java.io.DataInput;
1: import java.io.Serializable;
/////////////////////////////////////////////////////////////////////////
0: import org.apache.carbondata.core.indexstore.AbstractMemoryDMStore;
0: import org.apache.carbondata.core.indexstore.SafeMemoryDMStore;
/////////////////////////////////////////////////////////////////////////
0: public class BlockletDataMap extends CoarseGrainDataMap implements Serializable {
1:   private static final long serialVersionUID = -2170289352240810993L;
0: 
/////////////////////////////////////////////////////////////////////////
0:   private AbstractMemoryDMStore memoryDMStore;
0:   private AbstractMemoryDMStore summaryDMStore;
0:   // As it is a heavy object it is not recommended to serialize this object
0:   private transient SegmentProperties segmentProperties;
0:   private long blockletSchemaTime;
0: 
/////////////////////////////////////////////////////////////////////////
0:         blockletSchemaTime = fileFooter.getSchemaUpdatedTimeStamp();
0:         createSchema(segmentProperties, ((BlockletDataMapModel) dataMapModel).isAddToUnsafe());
0:             segmentId, ((BlockletDataMapModel) dataMapModel).isAddToUnsafe());
/////////////////////////////////////////////////////////////////////////
0:     if (memoryDMStore != null) {
0:       memoryDMStore.finishWriting();
0:     if (null != summaryDMStore) {
0:       summaryDMStore.finishWriting();
0:     if (LOGGER.isDebugEnabled()) {
0:       LOGGER.debug(
0:           "Time taken to load blocklet datamap from file : " + dataMapModel.getFilePath() + " is "
0:               + (System.currentTimeMillis() - startTime));
0:     }
/////////////////////////////////////////////////////////////////////////
0:     CarbonRowSchema[] schema = memoryDMStore.getSchema();
0:       summaryRow = new DataMapRowImpl(summaryDMStore.getSchema());
/////////////////////////////////////////////////////////////////////////
0:           summaryDMStore.getSchema()[taskMinMaxOrdinal], minValues,
/////////////////////////////////////////////////////////////////////////
0:           summaryDMStore.getSchema()[taskMinMaxOrdinal], maxValues,
/////////////////////////////////////////////////////////////////////////
0:         memoryDMStore.addIndexRow(row);
/////////////////////////////////////////////////////////////////////////
0:     CarbonRowSchema[] schema = memoryDMStore.getSchema();
0:       summaryRow = new DataMapRowImpl(summaryDMStore.getSchema());
/////////////////////////////////////////////////////////////////////////
0:         summaryDMStore.getSchema()[taskMinMaxOrdinal], updatedMinValues,
0:         summaryDMStore.getSchema()[taskMinMaxOrdinal], updatedMaxValues,
/////////////////////////////////////////////////////////////////////////
0:       memoryDMStore.addIndexRow(row);
/////////////////////////////////////////////////////////////////////////
0:         summaryDMStore.addIndexRow(summaryRow);
/////////////////////////////////////////////////////////////////////////
0:   private void createSchema(SegmentProperties segmentProperties, boolean addToUnsafe)
0:       throws MemoryException {
/////////////////////////////////////////////////////////////////////////
0:     CarbonRowSchema[] schema = indexSchemas.toArray(new CarbonRowSchema[indexSchemas.size()]);
0:     memoryDMStore = getMemoryDMStore(schema, addToUnsafe);
/////////////////////////////////////////////////////////////////////////
0:       byte[] filePath, byte[] fileName, byte[] segmentId, boolean addToUnsafe)
/////////////////////////////////////////////////////////////////////////
0:     CarbonRowSchema[] schema =
0:         taskMinMaxSchemas.toArray(new CarbonRowSchema[taskMinMaxSchemas.size()]);
0:     summaryDMStore = getMemoryDMStore(schema, addToUnsafe);
/////////////////////////////////////////////////////////////////////////
0:     for (int i = 0; i < summaryDMStore.getRowCount(); i++) {
0:       DataMapRow unsafeRow = summaryDMStore.getDataMapRow(i);
/////////////////////////////////////////////////////////////////////////
0:     if (memoryDMStore.getRowCount() == 0) {
0:       numBlocklets = memoryDMStore.getRowCount();
0:         DataMapRow safeRow = memoryDMStore.getDataMapRow(i).convertToSafeRow();
0:       numBlocklets = memoryDMStore.getRowCount();
0:         DataMapRow safeRow = memoryDMStore.getDataMapRow(startIndex).convertToSafeRow();
/////////////////////////////////////////////////////////////////////////
0:     if (memoryDMStore.getRowCount() == 0) {
/////////////////////////////////////////////////////////////////////////
0:     DataMapRow safeRow = memoryDMStore.getDataMapRow(index).convertToSafeRow();
0:   /**
0:    * Get the index file name of the blocklet data map
0:    *
0:    * @return
0:    */
0:   public String getIndexFileName() {
0:     DataMapRow unsafeRow = summaryDMStore.getDataMapRow(0);
0:     try {
0:       return new String(unsafeRow.getByteArray(INDEX_FILE_NAME),
0:           CarbonCommonConstants.DEFAULT_CHARSET);
0:     } catch (UnsupportedEncodingException e) {
0:       // should never happen!
0:       throw new IllegalArgumentException("UTF8 encoding is not supported", e);
0:     }
0:   }
0: 
/////////////////////////////////////////////////////////////////////////
0:     detailInfo.setBlockletInfoBinary(row.getByteArray(BLOCK_INFO_INDEX));
/////////////////////////////////////////////////////////////////////////
0:       DataMapRow unsafeRow = summaryDMStore.getDataMapRow(0);
/////////////////////////////////////////////////////////////////////////
0:     int high = memoryDMStore.getRowCount() - 1;
0:       compareRes = comparator.compare(key, memoryDMStore.getDataMapRow(mid));
/////////////////////////////////////////////////////////////////////////
0:             && comparator.compare(key, memoryDMStore.getDataMapRow(currentPos - 1)) == 0) {
/////////////////////////////////////////////////////////////////////////
0:     int high = memoryDMStore.getRowCount() - 1;
0:       compareRes = comparator.compare(key, memoryDMStore.getDataMapRow(mid));
/////////////////////////////////////////////////////////////////////////
0:         while (currentPos + 1 < memoryDMStore.getRowCount()
0:             && comparator.compare(key, memoryDMStore.getDataMapRow(currentPos + 1)) == 0) {
/////////////////////////////////////////////////////////////////////////
0:     DataMapRowImpl dataMapRow = new DataMapRowImpl(memoryDMStore.getSchema());
0:     DataMapRow unsafeRow = summaryDMStore.getDataMapRow(0);
/////////////////////////////////////////////////////////////////////////
0:     if (memoryDMStore != null) {
0:       memoryDMStore.freeMemory();
0:       memoryDMStore = null;
0:     if (null != summaryDMStore) {
0:       summaryDMStore.freeMemory();
0:       summaryDMStore = null;
0:     if (memoryDMStore != null) {
0:       memoryUsed += memoryDMStore.getMemoryUsed();
0:     if (null != summaryDMStore) {
0:       memoryUsed += summaryDMStore.getMemoryUsed();
/////////////////////////////////////////////////////////////////////////
0:   public void setSegmentProperties(SegmentProperties segmentProperties) {
0:     this.segmentProperties = segmentProperties;
0:   }
0: 
0:   public int[] getColumnCardinality() {
0:     return columnCardinality;
0:   }
0: 
0:   private AbstractMemoryDMStore getMemoryDMStore(CarbonRowSchema[] schema, boolean addToUnsafe)
0:       throws MemoryException {
0:     AbstractMemoryDMStore memoryDMStore;
0:     if (addToUnsafe) {
0:       memoryDMStore = new UnsafeMemoryDMStore(schema);
0:     } else {
0:       memoryDMStore = new SafeMemoryDMStore(schema);
0:     }
0:     return memoryDMStore;
0:   }
0: 
0:   /**
0:    * This method will ocnvert safe to unsafe memory DM store
0:    *
0:    * @throws MemoryException
0:    */
0:   public void convertToUnsafeDMStore() throws MemoryException {
0:     if (memoryDMStore instanceof SafeMemoryDMStore) {
0:       UnsafeMemoryDMStore unsafeMemoryDMStore = memoryDMStore.convertToUnsafeDMStore();
0:       memoryDMStore.freeMemory();
0:       memoryDMStore = unsafeMemoryDMStore;
0:     }
0:     if (summaryDMStore instanceof SafeMemoryDMStore) {
0:       UnsafeMemoryDMStore unsafeSummaryMemoryDMStore = summaryDMStore.convertToUnsafeDMStore();
0:       summaryDMStore.freeMemory();
0:       summaryDMStore = unsafeSummaryMemoryDMStore;
0:     }
0:   }
0: 
0:   /**
0:    * Read column schema from binary
0:    * @param schemaArray
0:    * @throws IOException
0:    */
0:   public List<ColumnSchema> readColumnSchema(byte[] schemaArray) throws IOException {
0:     // uncompress it.
0:     schemaArray = Snappy.uncompress(schemaArray);
0:     ByteArrayInputStream schemaStream = new ByteArrayInputStream(schemaArray);
0:     DataInput schemaInput = new DataInputStream(schemaStream);
0:     List<ColumnSchema> columnSchemas = new ArrayList<>();
0:     int size = schemaInput.readShort();
0:     for (int i = 0; i < size; i++) {
0:       ColumnSchema columnSchema = new ColumnSchema();
0:       columnSchema.readFields(schemaInput);
0:       columnSchemas.add(columnSchema);
0:     }
0:     return columnSchemas;
0:   }
0: 
0:   public long getBlockletSchemaTime() {
0:     return blockletSchemaTime;
0:   }
0: 
author:mohammadshahidkhan
-------------------------------------------------------------------------------
commit:a7926ea
/////////////////////////////////////////////////////////////////////////
0:     // changed segmentProperties to this.segmentProperties to make sure the pruning with its own
0:     // segmentProperties.
0:     // Its a temporary fix. The Interface DataMap.prune(FilterResolverIntf filterExp,
0:     // SegmentProperties segmentProperties, List<PartitionSpec> partitions) should be corrected
0:     return prune(filterExp, this.segmentProperties);
author:Jacky Li
-------------------------------------------------------------------------------
commit:03a735b
/////////////////////////////////////////////////////////////////////////
0: import org.apache.carbondata.core.profiler.ExplainCollector;
/////////////////////////////////////////////////////////////////////////
0:     int numBlocklets = 0;
0:       numBlocklets = unsafeMemoryDMStore.getRowCount();
0:       for (int i = 0; i < numBlocklets; i++) {
/////////////////////////////////////////////////////////////////////////
0:       numBlocklets = unsafeMemoryDMStore.getRowCount();
0:       while (startIndex < numBlocklets) {
/////////////////////////////////////////////////////////////////////////
0:     ExplainCollector.addTotalBlocklets(numBlocklets);
commit:fc2a7eb
/////////////////////////////////////////////////////////////////////////
0: import org.apache.carbondata.core.datamap.dev.cgdatamap.CoarseGrainDataMap;
/////////////////////////////////////////////////////////////////////////
0: public class BlockletDataMap extends CoarseGrainDataMap implements Cacheable {
0:       LogServiceFactory.getLogService(BlockletDataMap.class.getName());
commit:89a12af
/////////////////////////////////////////////////////////////////////////
0: import org.apache.carbondata.core.datamap.dev.cgdatamap.AbstractCoarseGrainIndexDataMap;
/////////////////////////////////////////////////////////////////////////
0: public class BlockletIndexDataMap extends AbstractCoarseGrainIndexDataMap implements Cacheable {
0:       LogServiceFactory.getLogService(BlockletIndexDataMap.class.getName());
commit:933e30c
/////////////////////////////////////////////////////////////////////////
0:       DataMapSchema mapSchema = new DataMapSchema.StructDataMapSchema(
0:           DataTypes.createDefaultStructType(), mapSchemas);
commit:f209e8e
/////////////////////////////////////////////////////////////////////////
0:         } else if (DataTypes.isDecimal(dataType)) {
/////////////////////////////////////////////////////////////////////////
0:         } else if (DataTypes.isDecimal(dataType)) {
commit:956833e
/////////////////////////////////////////////////////////////////////////
0: import org.apache.carbondata.core.metadata.datatype.DataTypes;
/////////////////////////////////////////////////////////////////////////
0:         DataType dataType = measures.get(i).getDataType();
0:         if (dataType == DataTypes.BYTE) {
0:           buffer.putLong(Byte.MIN_VALUE);
0:           updatedValues[minValues.length + i] = buffer.array().clone();
0:         } else if (dataType == DataTypes.SHORT) {
0:           buffer.putLong(Short.MIN_VALUE);
0:           updatedValues[minValues.length + i] = buffer.array().clone();
0:         } else if (dataType == DataTypes.INT) {
0:           buffer.putLong(Integer.MIN_VALUE);
0:           updatedValues[minValues.length + i] = buffer.array().clone();
0:         } else if (dataType == DataTypes.LONG) {
0:           buffer.putLong(Long.MIN_VALUE);
0:           updatedValues[minValues.length + i] = buffer.array().clone();
0:         } else if (dataType == DataTypes.DECIMAL) {
0:           updatedValues[minValues.length + i] =
0:               DataTypeUtil.bigDecimalToByte(BigDecimal.valueOf(Long.MIN_VALUE));
0:         } else {
0:           buffer.putDouble(Double.MIN_VALUE);
0:           updatedValues[minValues.length + i] = buffer.array().clone();
/////////////////////////////////////////////////////////////////////////
0:         DataType dataType = measures.get(i).getDataType();
0:         if (dataType == DataTypes.BYTE) {
0:           buffer.putLong(Byte.MAX_VALUE);
0:           updatedValues[maxValues.length + i] = buffer.array().clone();
0:         } else if (dataType == DataTypes.SHORT) {
0:           buffer.putLong(Short.MAX_VALUE);
0:           updatedValues[maxValues.length + i] = buffer.array().clone();
0:         } else if (dataType == DataTypes.INT) {
0:           buffer.putLong(Integer.MAX_VALUE);
0:           updatedValues[maxValues.length + i] = buffer.array().clone();
0:         } else if (dataType == DataTypes.LONG) {
0:           buffer.putLong(Long.MAX_VALUE);
0:           updatedValues[maxValues.length + i] = buffer.array().clone();
0:         } else if (dataType == DataTypes.DECIMAL) {
0:           updatedValues[maxValues.length + i] =
0:               DataTypeUtil.bigDecimalToByte(BigDecimal.valueOf(Long.MAX_VALUE));
0:         } else {
0:           buffer.putDouble(Double.MAX_VALUE);
0:           updatedValues[maxValues.length + i] = buffer.array().clone();
/////////////////////////////////////////////////////////////////////////
0:     indexSchemas.add(new DataMapSchema.VariableDataMapSchema(DataTypes.BYTE_ARRAY));
0:           mapSchemas[i] = new DataMapSchema.VariableDataMapSchema(DataTypes.BYTE_ARRAY);
0:           mapSchemas[i] = new DataMapSchema.FixedDataMapSchema(DataTypes.BYTE_ARRAY, minMaxLen[i]);
0:       DataMapSchema mapSchema = new DataMapSchema.StructDataMapSchema(DataTypes.STRUCT, mapSchemas);
0:     indexSchemas.add(new DataMapSchema.FixedDataMapSchema(DataTypes.INT));
0:     indexSchemas.add(new DataMapSchema.VariableDataMapSchema(DataTypes.BYTE_ARRAY));
0:     indexSchemas.add(new DataMapSchema.FixedDataMapSchema(DataTypes.SHORT));
0:     indexSchemas.add(new DataMapSchema.FixedDataMapSchema(DataTypes.SHORT));
0:     indexSchemas.add(new DataMapSchema.FixedDataMapSchema(DataTypes.LONG));
0:     indexSchemas.add(new DataMapSchema.VariableDataMapSchema(DataTypes.BYTE_ARRAY));
commit:f089287
/////////////////////////////////////////////////////////////////////////
0: import org.apache.carbondata.core.datamap.DataMapDistributable;
0: import org.apache.carbondata.core.datamap.dev.DataMap;
/////////////////////////////////////////////////////////////////////////
0:   public static final String NAME = "clustered.btree.blocklet";
0: 
/////////////////////////////////////////////////////////////////////////
0:   @Override public void init(String path) throws IOException, MemoryException {
0:     List<DataFileFooter> indexInfo = fileFooterConverter.getIndexInfo(path);
1:     for (DataFileFooter fileFooter : indexInfo) {
0:       List<ColumnSchema> columnInTable = fileFooter.getColumnInTable();
0:       if (segmentProperties == null) {
0:         columnCardinality = fileFooter.getSegmentInfo().getColumnCardinality();
0:         segmentProperties = new SegmentProperties(columnInTable, columnCardinality);
0:         createSchema(segmentProperties);
0:       }
1:       TableBlockInfo blockInfo = fileFooter.getBlockInfo().getTableBlockInfo();
0:       fileFooter = CarbonUtil.readMetadatFile(blockInfo);
0:       loadToUnsafe(fileFooter, segmentProperties, blockInfo.getFilePath());
0:     }
0:     if (unsafeMemoryDMStore != null) {
0:       unsafeMemoryDMStore.finishWriting();
author:Ravindra Pesala
-------------------------------------------------------------------------------
commit:56330ae
/////////////////////////////////////////////////////////////////////////
commit:133b303
/////////////////////////////////////////////////////////////////////////
0: import java.math.BigDecimal;
/////////////////////////////////////////////////////////////////////////
0: import org.apache.carbondata.core.metadata.schema.table.column.CarbonMeasure;
0: import org.apache.carbondata.core.util.DataTypeUtil;
/////////////////////////////////////////////////////////////////////////
0:       row.setRow(addMinMax(minMaxLen, schema[ordinal],
0:           updateMinValues(minMaxIndex.getMinValues(), minMaxLen)), ordinal);
0:       row.setRow(addMinMax(minMaxLen, schema[ordinal],
0:           updateMaxValues(minMaxIndex.getMaxValues(), minMaxLen)), ordinal);
/////////////////////////////////////////////////////////////////////////
0:   /**
0:    * Fill the measures min values with minimum , this is needed for backward version compatability
0:    * as older versions don't store min values for measures
0:    */
0:   private byte[][] updateMinValues(byte[][] minValues, int[] minMaxLen) {
0:     byte[][] updatedValues = minValues;
0:     if (minValues.length < minMaxLen.length) {
0:       updatedValues = new byte[minMaxLen.length][];
0:       System.arraycopy(minValues, 0, updatedValues, 0, minValues.length);
0:       List<CarbonMeasure> measures = segmentProperties.getMeasures();
0:       ByteBuffer buffer = ByteBuffer.allocate(8);
0:       for (int i = 0; i < measures.size(); i++) {
0:         buffer.rewind();
0:         switch (measures.get(i).getDataType()) {
0:           case BYTE:
0:             buffer.putLong(Byte.MIN_VALUE);
0:             updatedValues[minValues.length + i] = buffer.array().clone();
0:             break;
0:           case SHORT:
0:             buffer.putLong(Short.MIN_VALUE);
0:             updatedValues[minValues.length + i] = buffer.array().clone();
0:             break;
0:           case INT:
0:             buffer.putLong(Integer.MIN_VALUE);
0:             updatedValues[minValues.length + i] = buffer.array().clone();
0:             break;
0:           case LONG:
0:             buffer.putLong(Long.MIN_VALUE);
0:             updatedValues[minValues.length + i] = buffer.array().clone();
0:             break;
0:           case DECIMAL:
0:             updatedValues[minValues.length + i] =
0:                 DataTypeUtil.bigDecimalToByte(BigDecimal.valueOf(Long.MIN_VALUE));
0:             break;
0:           default:
0:             buffer.putDouble(Double.MIN_VALUE);
0:             updatedValues[minValues.length + i] = buffer.array().clone();
0:         }
0:       }
0:     }
0:     return updatedValues;
0:   }
0: 
0:   /**
0:    * Fill the measures max values with maximum , this is needed for backward version compatability
0:    * as older versions don't store max values for measures
0:    */
0:   private byte[][] updateMaxValues(byte[][] maxValues, int[] minMaxLen) {
0:     byte[][] updatedValues = maxValues;
0:     if (maxValues.length < minMaxLen.length) {
0:       updatedValues = new byte[minMaxLen.length][];
0:       System.arraycopy(maxValues, 0, updatedValues, 0, maxValues.length);
0:       List<CarbonMeasure> measures = segmentProperties.getMeasures();
0:       ByteBuffer buffer = ByteBuffer.allocate(8);
0:       for (int i = 0; i < measures.size(); i++) {
0:         buffer.rewind();
0:         switch (measures.get(i).getDataType()) {
0:           case BYTE:
0:             buffer.putLong(Byte.MAX_VALUE);
0:             updatedValues[maxValues.length + i] = buffer.array().clone();
0:             break;
0:           case SHORT:
0:             buffer.putLong(Short.MAX_VALUE);
0:             updatedValues[maxValues.length + i] = buffer.array().clone();
0:             break;
0:           case INT:
0:             buffer.putLong(Integer.MAX_VALUE);
0:             updatedValues[maxValues.length + i] = buffer.array().clone();
0:             break;
0:           case LONG:
0:             buffer.putLong(Long.MAX_VALUE);
0:             updatedValues[maxValues.length + i] = buffer.array().clone();
0:             break;
0:           case DECIMAL:
0:             updatedValues[maxValues.length + i] =
0:                 DataTypeUtil.bigDecimalToByte(BigDecimal.valueOf(Long.MAX_VALUE));
0:             break;
0:           default:
0:             buffer.putDouble(Double.MAX_VALUE);
0:             updatedValues[maxValues.length + i] = buffer.array().clone();
0:         }
0:       }
0:     }
0:     return updatedValues;
0:   }
0: 
commit:28f78b2
/////////////////////////////////////////////////////////////////////////
1: import org.apache.carbondata.core.indexstore.ExtendedBlocklet;
/////////////////////////////////////////////////////////////////////////
1:   public ExtendedBlocklet getDetailedBlocklet(String blockletId) {
0:     int index = Integer.parseInt(blockletId);
0:     DataMapRow unsafeRow = unsafeMemoryDMStore.getUnsafeRow(index);
0:     return createBlocklet(unsafeRow, index);
0:   }
0: 
/////////////////////////////////////////////////////////////////////////
0:   private ExtendedBlocklet createBlocklet(DataMapRow row, int blockletId) {
0:     ExtendedBlocklet blocklet = new ExtendedBlocklet(
commit:eb771f5
/////////////////////////////////////////////////////////////////////////
0:     if (unsafeMemoryDMStore != null) {
0:       unsafeMemoryDMStore.freeMemory();
0:       unsafeMemoryDMStore = null;
0:       segmentProperties = null;
0:     }
/////////////////////////////////////////////////////////////////////////
0:     if (unsafeMemoryDMStore != null) {
0:       return unsafeMemoryDMStore.getMemoryUsed();
0:     } else {
0:       return 0;
0:     }
commit:1e21cd1
/////////////////////////////////////////////////////////////////////////
/////////////////////////////////////////////////////////////////////////
0:   @Override
0:   public void init(String filePath) throws IOException, MemoryException {
0:     long startTime = System.currentTimeMillis();
0:     List<DataFileFooter> indexInfo = fileFooterConverter.getIndexInfo(filePath);
/////////////////////////////////////////////////////////////////////////
0:       if (fileFooter.getBlockletList() == null || fileFooter.getBlockletList().size() == 0) {
0:         LOGGER
0:             .info("Reading carbondata file footer to get blocklet info " + blockInfo.getFilePath());
0:         fileFooter = CarbonUtil.readMetadatFile(blockInfo);
0:       }
0:     LOGGER.info("Time taken to load blocklet datamap from file : " + filePath + "is " +
0:         (System.currentTimeMillis() - startTime));
/////////////////////////////////////////////////////////////////////////
0:   @Override
0:   public List<Blocklet> prune(FilterResolverIntf filterExp) {
/////////////////////////////////////////////////////////////////////////
0:   @Override
0:   public void clear() {
0:   @Override
0:   public long getFileTimeStamp() {
0:   @Override
0:   public int getAccessCount() {
0:   @Override
0:   public long getMemorySize() {
commit:4e83509
/////////////////////////////////////////////////////////////////////////
0:     int[] minMaxLen = segmentProperties.getColumnsValueSize();
/////////////////////////////////////////////////////////////////////////
0:     int[] minMaxLen = segmentProperties.getColumnsValueSize();
/////////////////////////////////////////////////////////////////////////
0:         new BlockletDMComparator(segmentProperties.getColumnsValueSize(),
author:ravipesala
-------------------------------------------------------------------------------
commit:d35fbaf
/////////////////////////////////////////////////////////////////////////
0: import org.apache.carbondata.core.datamap.dev.cgdatamap.AbstractCoarseGrainDataMap;
/////////////////////////////////////////////////////////////////////////
0: public class BlockletDataMap extends AbstractCoarseGrainDataMap implements Cacheable {
/////////////////////////////////////////////////////////////////////////
0:       boolean isScanRequired = FilterExpressionProcessor.isScanRequired(
0:           filterExecuter, getMinMaxValue(unsafeRow, TASK_MAX_VALUES_INDEX),
0:           getMinMaxValue(unsafeRow, TASK_MIN_VALUES_INDEX));
/////////////////////////////////////////////////////////////////////////
0:   public List<Blocklet> prune(FilterResolverIntf filterExp, SegmentProperties segmentProperties,
0:       List<PartitionSpec> partitions) {
/////////////////////////////////////////////////////////////////////////
0:   public SegmentProperties getSegmentProperties() {
0:     return segmentProperties;
0:   }
0: 
commit:8d3c774
/////////////////////////////////////////////////////////////////////////
0: import org.apache.carbondata.core.indexstore.PartitionSpec;
/////////////////////////////////////////////////////////////////////////
0: import org.apache.carbondata.core.util.path.CarbonTablePath;
0: import org.apache.hadoop.fs.Path;
/////////////////////////////////////////////////////////////////////////
0:   private static int INDEX_PATH = 3;
0: 
0:   private static int INDEX_FILE_NAME = 4;
0: 
0:   private static int SEGMENTID = 5;
/////////////////////////////////////////////////////////////////////////
/////////////////////////////////////////////////////////////////////////
0:     Path path = new Path(blockletDataMapInfo.getFilePath());
0:     byte[] filePath = path.getParent().toString().getBytes(CarbonCommonConstants.DEFAULT_CHARSET);
0:     byte[] fileName = path.getName().toString().getBytes(CarbonCommonConstants.DEFAULT_CHARSET);
0:     byte[] segmentId =
0:         blockletDataMapInfo.getSegmentId().getBytes(CarbonCommonConstants.DEFAULT_CHARSET);
/////////////////////////////////////////////////////////////////////////
0:         createSummarySchema(segmentProperties, schemaBinary, filePath, fileName,
0:             segmentId);
/////////////////////////////////////////////////////////////////////////
0:           schemaBinary,
0:           filePath,
0:           fileName,
0:           segmentId);
/////////////////////////////////////////////////////////////////////////
0:   private void addTaskSummaryRowToUnsafeMemoryStore(DataMapRow summaryRow, byte[] schemaBinary,
0:       byte[] filePath, byte[] fileName, byte[] segmentId) {
/////////////////////////////////////////////////////////////////////////
0:       summaryRow.setByteArray(filePath, INDEX_PATH);
0:       summaryRow.setByteArray(fileName, INDEX_FILE_NAME);
0:       summaryRow.setByteArray(segmentId, SEGMENTID);
/////////////////////////////////////////////////////////////////////////
0:   private void createSummarySchema(SegmentProperties segmentProperties, byte[] schemaBinary,
0:       byte[] filePath, byte[] fileName, byte[] segmentId)
0:     // for storing file path
0:     taskMinMaxSchemas.add(
0:         new CarbonRowSchema.FixedCarbonRowSchema(DataTypes.BYTE_ARRAY, filePath.length));
0:     // for storing file name
0:     taskMinMaxSchemas.add(
0:         new CarbonRowSchema.FixedCarbonRowSchema(DataTypes.BYTE_ARRAY, fileName.length));
0:     // for storing segmentid
0:     taskMinMaxSchemas.add(
0:         new CarbonRowSchema.FixedCarbonRowSchema(DataTypes.BYTE_ARRAY, segmentId.length));
/////////////////////////////////////////////////////////////////////////
0:   @Override
0:   public List<Blocklet> prune(FilterResolverIntf filterExp, List<PartitionSpec> partitions) {
0:     if (partitions != null) {
0:       // First get the partitions which are stored inside datamap.
0:       String[] fileDetails = getFileDetails();
0:       Path folderPath = new Path(fileDetails[0]);
0:       for (PartitionSpec spec : partitions) {
0:         if (folderPath.equals(spec.getLocation()) && isCorrectUUID(fileDetails, spec)) {
0:           found = true;
0:           break;
0:         }
/////////////////////////////////////////////////////////////////////////
0:   private boolean isCorrectUUID(String[] fileDetails, PartitionSpec spec) {
0:     boolean needToScan = false;
0:     if (spec.getUuid() != null) {
0:       String[] split = spec.getUuid().split("_");
0:       if (split[0].equals(fileDetails[2]) && CarbonTablePath.DataFileUtil
0:           .getTimeStampFromFileName(fileDetails[1]).equals(split[1])) {
0:         needToScan = true;
0:       }
0:     } else {
0:       needToScan = true;
0:     }
0:     return needToScan;
0:   }
0: 
/////////////////////////////////////////////////////////////////////////
0:   private String[] getFileDetails() {
0:     try {
0:       String[] fileDetails = new String[3];
0:       DataMapRow unsafeRow = unsafeMemorySummaryDMStore.getUnsafeRow(0);
0:       fileDetails[0] =
0:           new String(unsafeRow.getByteArray(INDEX_PATH), CarbonCommonConstants.DEFAULT_CHARSET);
0:       fileDetails[1] = new String(unsafeRow.getByteArray(INDEX_FILE_NAME),
0:           CarbonCommonConstants.DEFAULT_CHARSET);
0:       fileDetails[2] = new String(unsafeRow.getByteArray(SEGMENTID),
0:           CarbonCommonConstants.DEFAULT_CHARSET);
0:       return fileDetails;
0:     } catch (Exception e) {
0:       throw new RuntimeException(e);
0:     }
0:   }
0: 
0: 
/////////////////////////////////////////////////////////////////////////
commit:44ffaf5
/////////////////////////////////////////////////////////////////////////
1: import org.apache.carbondata.core.indexstore.BlockMetaInfo;
/////////////////////////////////////////////////////////////////////////
0:   private static int BLOCK_LENGTH = 12;
0: 
/////////////////////////////////////////////////////////////////////////
1:       BlockMetaInfo blockMetaInfo =
1:           blockletDataMapInfo.getBlockMetaInfoMap().get(blockInfo.getFilePath());
1:       if (blockMetaInfo != null) {
0:                   blockMetaInfo);
/////////////////////////////////////////////////////////////////////////
0:                   blockMetaInfo, relativeBlockletId);
/////////////////////////////////////////////////////////////////////////
1:       BlockMetaInfo blockMetaInfo, int relativeBlockletId) {
/////////////////////////////////////////////////////////////////////////
0:         setLocations(blockMetaInfo.getLocationInfo(), row, ordinal);
0:         row.setShort((short) relativeBlockletId++, ordinal++);
0:         // Store block size
0:         row.setLong(blockMetaInfo.getSize(), ordinal);
/////////////////////////////////////////////////////////////////////////
0:       BlockMetaInfo blockMetaInfo) {
/////////////////////////////////////////////////////////////////////////
0:       setLocations(blockMetaInfo.getLocationInfo(), row, ordinal);
0:       row.setShort((short) -1, ordinal++);
0: 
0:       // store block size
0:       row.setLong(blockMetaInfo.getSize(), ordinal);
/////////////////////////////////////////////////////////////////////////
0:     // for storing block length.
0:     indexSchemas.add(new CarbonRowSchema.FixedCarbonRowSchema(DataTypes.LONG));
0: 
/////////////////////////////////////////////////////////////////////////
0:     detailInfo.setBlockSize(row.getLong(BLOCK_LENGTH));
commit:41b0074
/////////////////////////////////////////////////////////////////////////
0: import java.io.UnsupportedEncodingException;
/////////////////////////////////////////////////////////////////////////
0: import org.apache.commons.lang3.StringUtils;
/////////////////////////////////////////////////////////////////////////
0:   private static int LOCATIONS = 10;
0: 
/////////////////////////////////////////////////////////////////////////
0:       String[] locations = blockletDataMapInfo.getLocationMap().get(blockInfo.getFilePath());
1:       // Here it loads info about all blocklets of index
1:       // Only add if the file exists physically. There are scenarios which index file exists inside
1:       // merge index but related carbondata files are deleted. In that case we first check whether
1:       // the file exists physically or not
0:       if (locations != null) {
0:         if (fileFooter.getBlockletList() == null) {
0:           // This is old store scenario, here blocklet information is not available in index file so
0:           // load only block info
0:           summaryRow =
0:               loadToUnsafeBlock(fileFooter, segmentProperties, blockInfo.getFilePath(), summaryRow,
0:                   locations);
0:         } else {
0:           summaryRow =
0:               loadToUnsafe(fileFooter, segmentProperties, blockInfo.getFilePath(), summaryRow,
0:                   locations);
0:         }
/////////////////////////////////////////////////////////////////////////
0:       SegmentProperties segmentProperties, String filePath, DataMapRowImpl summaryRow,
0:       String[] locations) {
/////////////////////////////////////////////////////////////////////////
0:         row.setLong(fileFooter.getBlockInfo().getTableBlockInfo().getBlockOffset(), ordinal++);
0:         setLocations(locations, row, ordinal);
0: 
/////////////////////////////////////////////////////////////////////////
0:   private void setLocations(String[] locations, DataMapRow row, int ordinal)
0:       throws UnsupportedEncodingException {
0:     // Add location info
0:     String locationStr = StringUtils.join(locations, ',');
0:     row.setByteArray(locationStr.getBytes(CarbonCommonConstants.DEFAULT_CHARSET), ordinal);
0:   }
0: 
0:       SegmentProperties segmentProperties, String filePath, DataMapRowImpl summaryRow,
0:       String[] locations) {
/////////////////////////////////////////////////////////////////////////
0:     row.setLong(fileFooter.getBlockInfo().getTableBlockInfo().getBlockOffset(), ordinal++);
0:       setLocations(locations, row, ordinal);
/////////////////////////////////////////////////////////////////////////
0:     // for locations
0:     indexSchemas.add(new CarbonRowSchema.VariableCarbonRowSchema(DataTypes.BYTE_ARRAY));
0: 
/////////////////////////////////////////////////////////////////////////
0:     if (unsafeMemoryDMStore.getRowCount() == 0) {
0:       return new ArrayList<>();
0:     }
/////////////////////////////////////////////////////////////////////////
0:         DataMapRow safeRow = unsafeMemoryDMStore.getUnsafeRow(i).convertToSafeRow();
0:         blocklets.add(createBlocklet(safeRow, i));
/////////////////////////////////////////////////////////////////////////
0:         DataMapRow safeRow = unsafeMemoryDMStore.getUnsafeRow(startIndex).convertToSafeRow();
0:         String filePath = new String(safeRow.getByteArray(FILE_PATH_INDEX),
0:             addBlockBasedOnMinMaxValue(filterExecuter, getMinMaxValue(safeRow, MAX_VALUES_INDEX),
0:                 getMinMaxValue(safeRow, MIN_VALUES_INDEX), filePath, startIndex);
0:           blocklets.add(createBlocklet(safeRow, startIndex));
/////////////////////////////////////////////////////////////////////////
0:     if (unsafeMemoryDMStore.getRowCount() == 0) {
0:       return new ArrayList<>();
0:     }
/////////////////////////////////////////////////////////////////////////
0:     DataMapRow safeRow = unsafeMemoryDMStore.getUnsafeRow(index).convertToSafeRow();
0:     return createBlocklet(safeRow, index);
/////////////////////////////////////////////////////////////////////////
0:     try {
0:       if (byteArray.length > 0) {
0:       blocklet.setLocation(
0:           new String(row.getByteArray(LOCATIONS), CarbonCommonConstants.DEFAULT_CHARSET)
0:               .split(","));
0:     } catch (IOException e) {
0:       throw new RuntimeException(e);
commit:6094af6
/////////////////////////////////////////////////////////////////////////
0: import org.apache.carbondata.core.metadata.blocklet.index.BlockletIndex;
/////////////////////////////////////////////////////////////////////////
0: import org.xerial.snappy.Snappy;
0: 
/////////////////////////////////////////////////////////////////////////
0:   private static int BLOCK_FOOTER_OFFSET = 9;
0: 
0:   private static int SCHEMA = 2;
0: 
0:   private static int PARTITION_INFO = 3;
0: 
/////////////////////////////////////////////////////////////////////////
0:     byte[] schemaBinary = null;
0:         List<ColumnSchema> columnInTable = fileFooter.getColumnInTable();
0:         schemaBinary = convertSchemaToBinary(columnInTable);
0:         createSummarySchema(segmentProperties, blockletDataMapInfo.getPartitions(), schemaBinary);
0:       if (fileFooter.getBlockletList() == null) {
0:         // This is old store scenario, here blocklet information is not available in  index file so
0:         // load only block info
0:         summaryRow =
0:             loadToUnsafeBlock(fileFooter, segmentProperties, blockInfo.getFilePath(), summaryRow);
0:       } else {
0:         // Here it loads info about all blocklets of index
0:         summaryRow =
0:             loadToUnsafe(fileFooter, segmentProperties, blockInfo.getFilePath(), summaryRow);
0:       addTaskSummaryRowToUnsafeMemoryStore(
0:           summaryRow,
0:           blockletDataMapInfo.getPartitions(),
0:           schemaBinary);
/////////////////////////////////////////////////////////////////////////
1:         row.setByteArray(serializedData, ordinal++);
0:         // Add block footer offset, it is used if we need to read footer of block
0:         row.setLong(fileFooter.getBlockInfo().getTableBlockInfo().getBlockOffset(), ordinal);
/////////////////////////////////////////////////////////////////////////
0:   /**
0:    * Load information for the block.It is the case can happen only for old stores
0:    * where blocklet information is not available in index file. So load only block information
0:    * and read blocklet information in executor.
0:    */
0:   private DataMapRowImpl loadToUnsafeBlock(DataFileFooter fileFooter,
0:       SegmentProperties segmentProperties, String filePath, DataMapRowImpl summaryRow) {
0:     int[] minMaxLen = segmentProperties.getColumnsValueSize();
0:     BlockletIndex blockletIndex = fileFooter.getBlockletIndex();
0:     CarbonRowSchema[] schema = unsafeMemoryDMStore.getSchema();
0:     // Add one row to maintain task level min max for segment pruning
0:     if (summaryRow == null) {
0:       summaryRow = new DataMapRowImpl(unsafeMemorySummaryDMStore.getSchema());
0:     }
1:     DataMapRow row = new DataMapRowImpl(schema);
1:     int ordinal = 0;
0:     int taskMinMaxOrdinal = 0;
0:     // add start key as index key
0:     row.setByteArray(blockletIndex.getBtreeIndex().getStartKey(), ordinal++);
0: 
0:     BlockletMinMaxIndex minMaxIndex = blockletIndex.getMinMaxIndex();
0:     byte[][] minValues = updateMinValues(minMaxIndex.getMinValues(), minMaxLen);
0:     row.setRow(addMinMax(minMaxLen, schema[ordinal], minValues), ordinal);
0:     // compute and set task level min values
0:     addTaskMinMaxValues(summaryRow, minMaxLen,
0:         unsafeMemorySummaryDMStore.getSchema()[taskMinMaxOrdinal], minValues,
0:         TASK_MIN_VALUES_INDEX, true);
0:     ordinal++;
0:     taskMinMaxOrdinal++;
0:     byte[][] maxValues = updateMaxValues(minMaxIndex.getMaxValues(), minMaxLen);
0:     row.setRow(addMinMax(minMaxLen, schema[ordinal], maxValues), ordinal);
0:     // compute and set task level max values
0:     addTaskMinMaxValues(summaryRow, minMaxLen,
0:         unsafeMemorySummaryDMStore.getSchema()[taskMinMaxOrdinal], maxValues,
0:         TASK_MAX_VALUES_INDEX, false);
0:     ordinal++;
0: 
0:     row.setInt((int)fileFooter.getNumberOfRows(), ordinal++);
0: 
0:     // add file path
0:     byte[] filePathBytes = filePath.getBytes(CarbonCommonConstants.DEFAULT_CHARSET_CLASS);
1:     row.setByteArray(filePathBytes, ordinal++);
0: 
0:     // add pages
0:     row.setShort((short) 0, ordinal++);
0: 
1:     // add version number
1:     row.setShort(fileFooter.getVersionId().number(), ordinal++);
0: 
1:     // add schema updated time
1:     row.setLong(fileFooter.getSchemaUpdatedTimeStamp(), ordinal++);
0: 
0:     // add blocklet info
0:     row.setByteArray(new byte[0], ordinal++);
0: 
0:     row.setLong(fileFooter.getBlockInfo().getTableBlockInfo().getBlockOffset(), ordinal);
0:     try {
0:       unsafeMemoryDMStore.addIndexRowToUnsafe(row);
0:     } catch (Exception e) {
0:       throw new RuntimeException(e);
0:     }
0: 
1:     return summaryRow;
0:   }
0: 
0:       List<String> partitions, byte[] schemaBinary) throws IOException {
0:       // Add column schema , it is useful to generate segment properties in executor.
0:       // So we no need to read footer again there.
0:       if (schemaBinary != null) {
0:         summaryRow.setByteArray(schemaBinary, SCHEMA);
0:       }
0:             ((CarbonRowSchema.StructCarbonRowSchema) unsafeMemorySummaryDMStore
0:                 .getSchema()[PARTITION_INFO]).getChildSchemas();
0:         summaryRow.setRow(partitionRow, PARTITION_INFO);
/////////////////////////////////////////////////////////////////////////
0:     // for block footer offset.
0:     indexSchemas.add(new CarbonRowSchema.FixedCarbonRowSchema(DataTypes.LONG));
0: 
/////////////////////////////////////////////////////////////////////////
0:   private void createSummarySchema(SegmentProperties segmentProperties, List<String> partitions,
0:       byte[] schemaBinary)
0:     List<CarbonRowSchema> taskMinMaxSchemas = new ArrayList<>();
0:     // for storing column schema
0:     taskMinMaxSchemas.add(
0:         new CarbonRowSchema.FixedCarbonRowSchema(DataTypes.BYTE_ARRAY, schemaBinary.length));
/////////////////////////////////////////////////////////////////////////
0:     byte[] byteArray = row.getByteArray(BLOCK_INFO_INDEX);
0:     BlockletInfo blockletInfo = null;
0:     if (byteArray.length > 0) {
0:       try {
0:         blockletInfo = new BlockletInfo();
0:         ByteArrayInputStream stream = new ByteArrayInputStream(byteArray);
0:         DataInputStream inputStream = new DataInputStream(stream);
0:         blockletInfo.readFields(inputStream);
0:         inputStream.close();
0:       } catch (IOException e) {
0:         throw new RuntimeException(e);
0:       }
0:     detailInfo.setBlockFooterOffset(row.getLong(BLOCK_FOOTER_OFFSET));
0:     detailInfo.setColumnSchemaBinary(getColumnSchemaBinary());
/////////////////////////////////////////////////////////////////////////
0:     if (unsafeRow.getColumnCount() > PARTITION_INFO) {
0:       DataMapRow row = unsafeRow.getRow(PARTITION_INFO);
/////////////////////////////////////////////////////////////////////////
0:   private byte[] getColumnSchemaBinary() {
0:     DataMapRow unsafeRow = unsafeMemorySummaryDMStore.getUnsafeRow(0);
0:     return unsafeRow.getByteArray(SCHEMA);
0:   }
0: 
0:   /**
0:    * Convert schema to binary
0:    */
0:   private byte[] convertSchemaToBinary(List<ColumnSchema> columnSchemas) throws IOException {
1:     ByteArrayOutputStream stream = new ByteArrayOutputStream();
1:     DataOutput dataOutput = new DataOutputStream(stream);
0:     dataOutput.writeShort(columnSchemas.size());
0:     for (ColumnSchema columnSchema : columnSchemas) {
0:       if (columnSchema.getColumnReferenceId() == null) {
0:         columnSchema.setColumnReferenceId(columnSchema.getColumnUniqueId());
0:       }
0:       columnSchema.write(dataOutput);
0:     }
0:     byte[] byteArray = stream.toByteArray();
0:     // Compress with snappy to reduce the size of schema
0:     return Snappy.rawCompress(byteArray, byteArray.length);
0:   }
0: 
commit:3ff55a2
/////////////////////////////////////////////////////////////////////////
0:   private boolean isPartitionedSegment;
0: 
/////////////////////////////////////////////////////////////////////////
0:     isPartitionedSegment = blockletDataMapInfo.isPartitionedSegment();
/////////////////////////////////////////////////////////////////////////
0:   /**
0:    * Creates the schema to store summary information or the information which can be stored only
0:    * once per datamap. It stores datamap level max/min of each column and partition information of
0:    * datamap
0:    * @param segmentProperties
0:    * @param partitions
0:    * @throws MemoryException
0:    */
/////////////////////////////////////////////////////////////////////////
0:     // First get the partitions which are stored inside datamap.
0:     // if it has partitioned datamap but there is no partitioned information stored, it means
0:     // partitions are dropped so return empty list.
0:     if (isPartitionedSegment && (storedPartitions == null || storedPartitions.size() == 0)) {
0:       return new ArrayList<>();
0:     }
0:     if (storedPartitions != null && storedPartitions.size() > 0) {
0:       // Check the exact match of partition information inside the stored partitions.
/////////////////////////////////////////////////////////////////////////
0:     // Prune with filters if the partitions are existed in this datamap
/////////////////////////////////////////////////////////////////////////
0:       List<String> partitions = new ArrayList<>();
0:       return partitions;
0:     return null;
commit:b8a02f3
/////////////////////////////////////////////////////////////////////////
0:   private UnsafeMemoryDMStore unsafeMemorySummaryDMStore;
/////////////////////////////////////////////////////////////////////////
0:     DataMapRowImpl summaryRow = null;
0:         createSummarySchema(segmentProperties, blockletDataMapInfo.getPartitions());
/////////////////////////////////////////////////////////////////////////
0:       summaryRow = loadToUnsafe(fileFooter, segmentProperties, blockInfo.getFilePath(), summaryRow);
0:     if (null != unsafeMemorySummaryDMStore) {
0:       addTaskSummaryRowToUnsafeMemoryStore(summaryRow, blockletDataMapInfo.getPartitions());
0:       unsafeMemorySummaryDMStore.finishWriting();
0:   private DataMapRowImpl loadToUnsafe(DataFileFooter fileFooter,
0:       SegmentProperties segmentProperties, String filePath, DataMapRowImpl summaryRow) {
1:     if (!blockletList.isEmpty() && summaryRow == null) {
0:       summaryRow = new DataMapRowImpl(unsafeMemorySummaryDMStore.getSchema());
/////////////////////////////////////////////////////////////////////////
0:       addTaskMinMaxValues(summaryRow, minMaxLen,
0:           unsafeMemorySummaryDMStore.getSchema()[taskMinMaxOrdinal], minValues,
0:       addTaskMinMaxValues(summaryRow, minMaxLen,
0:           unsafeMemorySummaryDMStore.getSchema()[taskMinMaxOrdinal], maxValues,
/////////////////////////////////////////////////////////////////////////
0: 
0:     return summaryRow;
0:   private void addTaskSummaryRowToUnsafeMemoryStore(DataMapRow summaryRow,
0:       List<String> partitions) {
0:     // write the task summary info to unsafe memory store
0:     if (null != summaryRow) {
0:       if (partitions != null && partitions.size() > 0) {
0:         CarbonRowSchema[] minSchemas =
0:             ((CarbonRowSchema.StructCarbonRowSchema) unsafeMemorySummaryDMStore.getSchema()[2])
0:                 .getChildSchemas();
0:         DataMapRow partitionRow = new DataMapRowImpl(minSchemas);
0:         for (int i = 0; i < partitions.size(); i++) {
0:           partitionRow
0:               .setByteArray(partitions.get(i).getBytes(CarbonCommonConstants.DEFAULT_CHARSET_CLASS),
0:                   i);
0:         }
0:         summaryRow.setRow(partitionRow, 2);
0:       }
0:       try {
0:         unsafeMemorySummaryDMStore.addIndexRowToUnsafe(summaryRow);
0:       } catch (Exception e) {
0:         throw new RuntimeException(e);
0:       }
/////////////////////////////////////////////////////////////////////////
0:   private void createSummarySchema(SegmentProperties segmentProperties, List<String> partitions)
0:       throws MemoryException {
0:     if (partitions != null && partitions.size() > 0) {
0:       CarbonRowSchema[] mapSchemas = new CarbonRowSchema[partitions.size()];
0:       for (int i = 0; i < mapSchemas.length; i++) {
0:         mapSchemas[i] = new CarbonRowSchema.VariableCarbonRowSchema(DataTypes.BYTE_ARRAY);
0:       }
0:       CarbonRowSchema mapSchema =
0:           new CarbonRowSchema.StructCarbonRowSchema(DataTypes.createDefaultStructType(),
0:               mapSchemas);
0:       taskMinMaxSchemas.add(mapSchema);
0:     }
0:     unsafeMemorySummaryDMStore = new UnsafeMemoryDMStore(
/////////////////////////////////////////////////////////////////////////
0:     for (int i = 0; i < unsafeMemorySummaryDMStore.getRowCount(); i++) {
0:       DataMapRow unsafeRow = unsafeMemorySummaryDMStore.getUnsafeRow(i);
/////////////////////////////////////////////////////////////////////////
0:   @Override public List<Blocklet> prune(FilterResolverIntf filterExp, List<String> partitions) {
0:     List<String> storedPartitions = getPartitions();
0:     if (storedPartitions != null && storedPartitions.size() > 0 && filterExp != null) {
0:       boolean found = false;
0:       if (partitions != null && partitions.size() > 0) {
0:         found = partitions.containsAll(storedPartitions);
0:       }
0:       if (!found) {
0:         return new ArrayList<>();
0:       }
0:     }
0:     return prune(filterExp);
0:   }
0: 
/////////////////////////////////////////////////////////////////////////
0:   private List<String> getPartitions() {
0:     List<String> partitions = new ArrayList<>();
0:     DataMapRow unsafeRow = unsafeMemorySummaryDMStore.getUnsafeRow(0);
0:     if (unsafeRow.getColumnCount() > 2) {
0:       DataMapRow row = unsafeRow.getRow(2);
0:       for (int i = 0; i < row.getColumnCount(); i++) {
0:         partitions.add(
0:             new String(row.getByteArray(i), CarbonCommonConstants.DEFAULT_CHARSET_CLASS));
0:       }
0:     }
0:     return partitions;
0:   }
0: 
/////////////////////////////////////////////////////////////////////////
0:     if (null != unsafeMemorySummaryDMStore) {
0:       unsafeMemorySummaryDMStore.freeMemory();
0:       unsafeMemorySummaryDMStore = null;
/////////////////////////////////////////////////////////////////////////
0:     if (null != unsafeMemorySummaryDMStore) {
0:       memoryUsed += unsafeMemorySummaryDMStore.getMemoryUsed();
commit:0586146
/////////////////////////////////////////////////////////////////////////
1: import org.apache.carbondata.core.datamap.dev.DataMapModel;
/////////////////////////////////////////////////////////////////////////
0:   public void init(DataMapModel dataMapModel) throws IOException, MemoryException {
0:     assert (dataMapModel instanceof BlockletDataMapModel);
0:     BlockletDataMapModel blockletDataMapInfo = (BlockletDataMapModel) dataMapModel;
0:     List<DataFileFooter> indexInfo = fileFooterConverter
0:         .getIndexInfo(blockletDataMapInfo.getFilePath(), blockletDataMapInfo.getFileData());
/////////////////////////////////////////////////////////////////////////
0:     LOGGER.info(
0:         "Time taken to load blocklet datamap from file : " + dataMapModel.getFilePath() + "is " + (
0:             System.currentTimeMillis() - startTime));
commit:b681244
/////////////////////////////////////////////////////////////////////////
1: /*
1:  * Licensed to the Apache Software Foundation (ASF) under one or more
1:  * contributor license agreements.  See the NOTICE file distributed with
1:  * this work for additional information regarding copyright ownership.
1:  * The ASF licenses this file to You under the Apache License, Version 2.0
1:  * (the "License"); you may not use this file except in compliance with
1:  * the License.  You may obtain a copy of the License at
0:  *
1:  *    http://www.apache.org/licenses/LICENSE-2.0
0:  *
1:  * Unless required by applicable law or agreed to in writing, software
1:  * distributed under the License is distributed on an "AS IS" BASIS,
1:  * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
1:  * See the License for the specific language governing permissions and
1:  * limitations under the License.
0:  */
1: package org.apache.carbondata.core.indexstore.blockletindex;
0: 
0: import java.io.ByteArrayInputStream;
1: import java.io.ByteArrayOutputStream;
0: import java.io.DataInputStream;
1: import java.io.DataOutput;
1: import java.io.DataOutputStream;
1: import java.io.IOException;
0: import java.nio.ByteBuffer;
0: import java.util.ArrayList;
0: import java.util.BitSet;
0: import java.util.Comparator;
1: import java.util.List;
0: 
0: import org.apache.carbondata.common.logging.LogService;
0: import org.apache.carbondata.common.logging.LogServiceFactory;
0: import org.apache.carbondata.core.cache.Cacheable;
0: import org.apache.carbondata.core.datastore.IndexKey;
1: import org.apache.carbondata.core.datastore.block.SegmentProperties;
1: import org.apache.carbondata.core.datastore.block.TableBlockInfo;
0: import org.apache.carbondata.core.indexstore.Blocklet;
1: import org.apache.carbondata.core.indexstore.BlockletDetailInfo;
0: import org.apache.carbondata.core.indexstore.DataMap;
0: import org.apache.carbondata.core.indexstore.DataMapDistributable;
0: import org.apache.carbondata.core.indexstore.DataMapWriter;
0: import org.apache.carbondata.core.indexstore.UnsafeMemoryDMStore;
1: import org.apache.carbondata.core.indexstore.row.DataMapRow;
1: import org.apache.carbondata.core.indexstore.row.DataMapRowImpl;
0: import org.apache.carbondata.core.indexstore.schema.DataMapSchema;
0: import org.apache.carbondata.core.keygenerator.KeyGenException;
1: import org.apache.carbondata.core.metadata.blocklet.BlockletInfo;
1: import org.apache.carbondata.core.metadata.blocklet.DataFileFooter;
1: import org.apache.carbondata.core.metadata.blocklet.index.BlockletMinMaxIndex;
0: import org.apache.carbondata.core.metadata.datatype.DataType;
0: import org.apache.carbondata.core.metadata.schema.table.column.ColumnSchema;
0: import org.apache.carbondata.core.scan.filter.FilterUtil;
0: import org.apache.carbondata.core.scan.filter.executer.FilterExecuter;
0: import org.apache.carbondata.core.scan.filter.resolver.FilterResolverIntf;
0: import org.apache.carbondata.core.util.CarbonUtil;
0: import org.apache.carbondata.core.util.DataFileFooterConverter;
0: 
0: /**
1:  * Datamap implementation for blocklet.
0:  */
0: public class BlockletDataMap implements DataMap, Cacheable {
0: 
0:   private static final LogService LOGGER =
0:       LogServiceFactory.getLogService(BlockletDataMap.class.getName());
0: 
0:   private static int KEY_INDEX = 0;
0: 
0:   private static int MIN_VALUES_INDEX = 1;
0: 
0:   private static int MAX_VALUES_INDEX = 2;
0: 
0:   private static int ROW_COUNT_INDEX = 3;
0: 
0:   private static int FILE_PATH_INDEX = 4;
0: 
0:   private static int PAGE_COUNT_INDEX = 5;
0: 
0:   private static int VERSION_INDEX = 6;
0: 
0:   private static int SCHEMA_UPADATED_TIME_INDEX = 7;
0: 
0:   private static int BLOCK_INFO_INDEX = 8;
0: 
0:   private UnsafeMemoryDMStore unsafeMemoryDMStore;
0: 
0:   private SegmentProperties segmentProperties;
0: 
0:   private int[] columnCardinality;
0: 
0:   @Override public DataMapWriter getWriter() {
0:     return null;
0:   }
0: 
0:   @Override public void init(String path) {
0:     DataFileFooterConverter fileFooterConverter = new DataFileFooterConverter();
0:     try {
0:       List<DataFileFooter> indexInfo = fileFooterConverter.getIndexInfo(path);
0:       for (DataFileFooter fileFooter : indexInfo) {
0:         List<ColumnSchema> columnInTable = fileFooter.getColumnInTable();
0:         if (segmentProperties == null) {
0:           columnCardinality = fileFooter.getSegmentInfo().getColumnCardinality();
0:           segmentProperties = new SegmentProperties(columnInTable, columnCardinality);
0:           createSchema(segmentProperties);
0:         }
0:         TableBlockInfo blockInfo = fileFooter.getBlockInfo().getTableBlockInfo();
0:         fileFooter = CarbonUtil.readMetadatFile(blockInfo);
0: 
0:         loadToUnsafe(fileFooter, segmentProperties, blockInfo.getFilePath());
0:       }
0:       if (unsafeMemoryDMStore != null) {
0:         unsafeMemoryDMStore.finishWriting();
0:       }
0:     } catch (IOException e) {
0:       throw new RuntimeException(e);
0:     }
0:   }
0: 
0:   private void loadToUnsafe(DataFileFooter fileFooter, SegmentProperties segmentProperties,
0:       String filePath) {
0:     int[] minMaxLen = segmentProperties.getEachDimColumnValueSize();
1:     List<BlockletInfo> blockletList = fileFooter.getBlockletList();
0:     DataMapSchema[] schema = unsafeMemoryDMStore.getSchema();
1:     for (int index = 0; index < blockletList.size(); index++) {
0:       DataMapRow row = new DataMapRowImpl(schema);
0:       int ordinal = 0;
1:       BlockletInfo blockletInfo = blockletList.get(index);
0: 
0:       // add start key as index key
0:       row.setByteArray(blockletInfo.getBlockletIndex().getBtreeIndex().getStartKey(), ordinal++);
0: 
1:       BlockletMinMaxIndex minMaxIndex = blockletInfo.getBlockletIndex().getMinMaxIndex();
0:       row.setRow(addMinMax(minMaxLen, schema[ordinal], minMaxIndex.getMinValues()), ordinal);
0:       ordinal++;
0:       row.setRow(addMinMax(minMaxLen, schema[ordinal], minMaxIndex.getMaxValues()), ordinal);
0:       ordinal++;
0: 
1:       row.setInt(blockletInfo.getNumberOfRows(), ordinal++);
0: 
0:       // add file path
0:       byte[] filePathBytes = filePath.getBytes();
0:       row.setByteArray(filePathBytes, ordinal++);
0: 
0:       // add pages
0:       row.setShort((short) blockletInfo.getNumberOfPages(), ordinal++);
0: 
0:       // add version number
0:       row.setShort(fileFooter.getVersionId().number(), ordinal++);
0: 
0:       // add schema updated time
0:       row.setLong(fileFooter.getSchemaUpdatedTimeStamp(), ordinal++);
0: 
0:       // add blocklet info
1:       byte[] serializedData;
0:       try {
0:         ByteArrayOutputStream stream = new ByteArrayOutputStream();
0:         DataOutput dataOutput = new DataOutputStream(stream);
1:         blockletInfo.write(dataOutput);
1:         serializedData = stream.toByteArray();
0:       } catch (IOException e) {
0:         throw new RuntimeException(e);
0:       }
0:       row.setByteArray(serializedData, ordinal);
0:       unsafeMemoryDMStore.addIndexRowToUnsafe(row);
0:     }
0:   }
0: 
0:   private DataMapRow addMinMax(int[] minMaxLen, DataMapSchema dataMapSchema, byte[][] minValues) {
0:     DataMapSchema[] minSchemas =
0:         ((DataMapSchema.StructDataMapSchema) dataMapSchema).getChildSchemas();
0:     DataMapRow minRow = new DataMapRowImpl(minSchemas);
0:     int minOrdinal = 0;
0:     // min value adding
0:     for (int i = 0; i < minMaxLen.length; i++) {
0:       minRow.setByteArray(minValues[i], minOrdinal++);
0:     }
0:     return minRow;
0:   }
0: 
0:   private void createSchema(SegmentProperties segmentProperties) {
0:     List<DataMapSchema> indexSchemas = new ArrayList<>();
0: 
0:     // Index key
0:     indexSchemas.add(new DataMapSchema.VariableDataMapSchema(DataType.BYTE_ARRAY));
0:     int[] minMaxLen = segmentProperties.getEachDimColumnValueSize();
0:     // do it 2 times, one for min and one for max.
0:     for (int k = 0; k < 2; k++) {
0:       DataMapSchema[] mapSchemas = new DataMapSchema[minMaxLen.length];
0:       for (int i = 0; i < minMaxLen.length; i++) {
0:         if (minMaxLen[i] <= 0) {
0:           mapSchemas[i] = new DataMapSchema.VariableDataMapSchema(DataType.BYTE_ARRAY);
0:         } else {
0:           mapSchemas[i] = new DataMapSchema.FixedDataMapSchema(DataType.BYTE_ARRAY, minMaxLen[i]);
0:         }
0:       }
0:       DataMapSchema mapSchema = new DataMapSchema.StructDataMapSchema(DataType.STRUCT, mapSchemas);
0:       indexSchemas.add(mapSchema);
0:     }
0: 
0:     // for number of rows.
0:     indexSchemas.add(new DataMapSchema.FixedDataMapSchema(DataType.INT));
0: 
0:     // for table block path
0:     indexSchemas.add(new DataMapSchema.VariableDataMapSchema(DataType.BYTE_ARRAY));
0: 
0:     // for number of pages.
0:     indexSchemas.add(new DataMapSchema.FixedDataMapSchema(DataType.SHORT));
0: 
0:     // for version number.
0:     indexSchemas.add(new DataMapSchema.FixedDataMapSchema(DataType.SHORT));
0: 
0:     // for schema updated time.
0:     indexSchemas.add(new DataMapSchema.FixedDataMapSchema(DataType.LONG));
0: 
0:     //for blocklet info
0:     indexSchemas.add(new DataMapSchema.VariableDataMapSchema(DataType.BYTE_ARRAY));
0: 
0:     unsafeMemoryDMStore =
0:         new UnsafeMemoryDMStore(indexSchemas.toArray(new DataMapSchema[indexSchemas.size()]));
0:   }
0: 
0:   @Override public List<Blocklet> prune(FilterResolverIntf filterExp) {
0: 
0:     // getting the start and end index key based on filter for hitting the
0:     // selected block reference nodes based on filter resolver tree.
0:     if (LOGGER.isDebugEnabled()) {
0:       LOGGER.debug("preparing the start and end key for finding"
0:           + "start and end block as per filter resolver");
0:     }
0:     List<Blocklet> blocklets = new ArrayList<>();
0:     Comparator<DataMapRow> comparator =
0:         new BlockletDMComparator(segmentProperties.getEachDimColumnValueSize(),
0:             segmentProperties.getNumberOfSortColumns(),
0:             segmentProperties.getNumberOfNoDictSortColumns());
0:     List<IndexKey> listOfStartEndKeys = new ArrayList<IndexKey>(2);
0:     FilterUtil
0:         .traverseResolverTreeAndGetStartAndEndKey(segmentProperties, filterExp, listOfStartEndKeys);
0:     // reading the first value from list which has start key
0:     IndexKey searchStartKey = listOfStartEndKeys.get(0);
0:     // reading the last value from list which has end key
0:     IndexKey searchEndKey = listOfStartEndKeys.get(1);
0:     if (null == searchStartKey && null == searchEndKey) {
0:       try {
0:         // TODO need to handle for no dictionary dimensions
0:         searchStartKey = FilterUtil.prepareDefaultStartIndexKey(segmentProperties);
0:         // TODO need to handle for no dictionary dimensions
0:         searchEndKey = FilterUtil.prepareDefaultEndIndexKey(segmentProperties);
0:       } catch (KeyGenException e) {
0:         return null;
0:       }
0:     }
0:     if (LOGGER.isDebugEnabled()) {
0:       LOGGER.debug(
0:           "Successfully retrieved the start and end key" + "Dictionary Start Key: " + searchStartKey
0:               .getDictionaryKeys() + "No Dictionary Start Key " + searchStartKey
0:               .getNoDictionaryKeys() + "Dictionary End Key: " + searchEndKey.getDictionaryKeys()
0:               + "No Dictionary End Key " + searchEndKey.getNoDictionaryKeys());
0:     }
0:     if (filterExp == null) {
0:       int rowCount = unsafeMemoryDMStore.getRowCount();
0:       for (int i = 0; i < rowCount; i++) {
0:         DataMapRow unsafeRow = unsafeMemoryDMStore.getUnsafeRow(i);
0:         blocklets.add(createBlocklet(unsafeRow, i));
0:       }
0:     } else {
0:       int startIndex = findStartIndex(convertToRow(searchStartKey), comparator);
0:       int endIndex = findEndIndex(convertToRow(searchEndKey), comparator);
0:       FilterExecuter filterExecuter =
0:           FilterUtil.getFilterExecuterTree(filterExp, segmentProperties, null);
0:       while (startIndex <= endIndex) {
0:         DataMapRow unsafeRow = unsafeMemoryDMStore.getUnsafeRow(startIndex);
0:         BitSet bitSet = filterExecuter.isScanRequired(getMinMaxValue(unsafeRow, MAX_VALUES_INDEX),
0:             getMinMaxValue(unsafeRow, MIN_VALUES_INDEX));
0:         if (!bitSet.isEmpty()) {
0:           blocklets.add(createBlocklet(unsafeRow, startIndex));
0:         }
0:         startIndex++;
0:       }
0:     }
0: 
0:     return blocklets;
0:   }
0: 
0:   private byte[][] getMinMaxValue(DataMapRow row, int index) {
0:     DataMapRow minMaxRow = row.getRow(index);
0:     byte[][] minMax = new byte[minMaxRow.getColumnCount()][];
0:     for (int i = 0; i < minMax.length; i++) {
0:       minMax[i] = minMaxRow.getByteArray(i);
0:     }
0:     return minMax;
0:   }
0: 
0:   private Blocklet createBlocklet(DataMapRow row, int blockletId) {
0:     Blocklet blocklet =
0:         new Blocklet(new String(row.getByteArray(FILE_PATH_INDEX)), blockletId + "");
0:     BlockletDetailInfo detailInfo = new BlockletDetailInfo();
0:     detailInfo.setRowCount(row.getInt(ROW_COUNT_INDEX));
0:     detailInfo.setPagesCount(row.getShort(PAGE_COUNT_INDEX));
0:     detailInfo.setVersionNumber(row.getShort(VERSION_INDEX));
0:     detailInfo.setDimLens(columnCardinality);
0:     detailInfo.setSchemaUpdatedTimeStamp(row.getLong(SCHEMA_UPADATED_TIME_INDEX));
0:     BlockletInfo blockletInfo = new BlockletInfo();
0:     try {
0:       byte[] byteArray = row.getByteArray(BLOCK_INFO_INDEX);
0:       ByteArrayInputStream stream = new ByteArrayInputStream(byteArray);
0:       DataInputStream inputStream = new DataInputStream(stream);
0:       blockletInfo.readFields(inputStream);
0:       inputStream.close();
0:     } catch (IOException e) {
0:       throw new RuntimeException(e);
0:     }
0:     detailInfo.setBlockletInfo(blockletInfo);
1:     blocklet.setDetailInfo(detailInfo);
1:     return blocklet;
0:   }
0: 
0:   /**
0:    * Binary search used to get the first tentative index row based on
0:    * search key
0:    *
0:    * @param key search key
0:    * @return first tentative block
0:    */
0:   private int findStartIndex(DataMapRow key, Comparator<DataMapRow> comparator) {
0:     int childNodeIndex;
0:     int low = 0;
0:     int high = unsafeMemoryDMStore.getRowCount() - 1;
0:     int mid = 0;
0:     int compareRes = -1;
0:     //
0:     while (low <= high) {
0:       mid = (low + high) >>> 1;
0:       // compare the entries
0:       compareRes = comparator.compare(key, unsafeMemoryDMStore.getUnsafeRow(mid));
0:       if (compareRes < 0) {
0:         high = mid - 1;
0:       } else if (compareRes > 0) {
0:         low = mid + 1;
0:       } else {
0:         // if key is matched then get the first entry
0:         int currentPos = mid;
0:         while (currentPos - 1 >= 0
0:             && comparator.compare(key, unsafeMemoryDMStore.getUnsafeRow(currentPos - 1)) == 0) {
0:           currentPos--;
0:         }
0:         mid = currentPos;
0:         break;
0:       }
0:     }
0:     // if compare result is less than zero then we
0:     // and mid is more than 0 then we need to previous block as duplicates
0:     // record can be present
0:     if (compareRes < 0) {
0:       if (mid > 0) {
0:         mid--;
0:       }
0:       childNodeIndex = mid;
0:     } else {
0:       childNodeIndex = mid;
0:     }
0:     // get the leaf child
0:     return childNodeIndex;
0:   }
0: 
0:   /**
0:    * Binary search used to get the last tentative block  based on
0:    * search key
0:    *
0:    * @param key search key
0:    * @return first tentative block
0:    */
0:   private int findEndIndex(DataMapRow key, Comparator<DataMapRow> comparator) {
0:     int childNodeIndex;
0:     int low = 0;
0:     int high = unsafeMemoryDMStore.getRowCount() - 1;
0:     int mid = 0;
0:     int compareRes = -1;
0:     //
0:     while (low <= high) {
0:       mid = (low + high) >>> 1;
0:       // compare the entries
0:       compareRes = comparator.compare(key, unsafeMemoryDMStore.getUnsafeRow(mid));
0:       if (compareRes < 0) {
0:         high = mid - 1;
0:       } else if (compareRes > 0) {
0:         low = mid + 1;
0:       } else {
0:         int currentPos = mid;
0:         // if key is matched then get the first entry
0:         while (currentPos + 1 < unsafeMemoryDMStore.getRowCount()
0:             && comparator.compare(key, unsafeMemoryDMStore.getUnsafeRow(currentPos + 1)) == 0) {
0:           currentPos++;
0:         }
0:         mid = currentPos;
0:         break;
0:       }
0:     }
0:     // if compare result is less than zero then we
0:     // and mid is more than 0 then we need to previous block as duplicates
0:     // record can be present
0:     if (compareRes < 0) {
0:       if (mid > 0) {
0:         mid--;
0:       }
0:       childNodeIndex = mid;
0:     } else {
0:       childNodeIndex = mid;
0:     }
0:     return childNodeIndex;
0:   }
0: 
0:   private DataMapRow convertToRow(IndexKey key) {
0:     ByteBuffer buffer =
0:         ByteBuffer.allocate(key.getDictionaryKeys().length + key.getNoDictionaryKeys().length + 8);
0:     buffer.putInt(key.getDictionaryKeys().length);
0:     buffer.putInt(key.getNoDictionaryKeys().length);
0:     buffer.put(key.getDictionaryKeys());
0:     buffer.put(key.getNoDictionaryKeys());
0:     DataMapRowImpl dataMapRow = new DataMapRowImpl(unsafeMemoryDMStore.getSchema());
0:     dataMapRow.setByteArray(buffer.array(), 0);
0:     return dataMapRow;
0:   }
0: 
0:   @Override public void clear() {
0:     unsafeMemoryDMStore.freeMemory();
0:     unsafeMemoryDMStore = null;
0:     segmentProperties = null;
0:   }
0: 
0:   @Override public long getFileTimeStamp() {
0:     return 0;
0:   }
0: 
0:   @Override public int getAccessCount() {
0:     return 0;
0:   }
0: 
0:   @Override public long getMemorySize() {
0:     return unsafeMemoryDMStore.getMemoryUsed();
0:   }
0: 
0:   @Override public DataMapDistributable toDistributable() {
0:     // TODO
0:     return null;
0:   }
0: }
author:sounakr
-------------------------------------------------------------------------------
commit:ca7e2e3
/////////////////////////////////////////////////////////////////////////
0:   private List<Blocklet> prune(FilterResolverIntf filterExp, SegmentProperties segmentProperties) {
/////////////////////////////////////////////////////////////////////////
0:     return prune(filterExp, segmentProperties);
author:rahulforallp
-------------------------------------------------------------------------------
commit:aee5213
/////////////////////////////////////////////////////////////////////////
0:                 getMinMaxValue(unsafeRow, MIN_VALUES_INDEX), filePath, startIndex);
/////////////////////////////////////////////////////////////////////////
0:    * @param blockletId
0:       byte[][] minValue, String filePath, int blockletId) {
0:       String uniqueBlockPath = filePath.substring(filePath.lastIndexOf("/Part") + 1)
0:           + CarbonCommonConstants.FILE_SEPARATOR + blockletId;
author:anubhav100
-------------------------------------------------------------------------------
commit:0c8fa59
/////////////////////////////////////////////////////////////////////////
0:     detailInfo.setBlockletId((short) blockletId);
author:kunal642
-------------------------------------------------------------------------------
commit:cc0e6f1
/////////////////////////////////////////////////////////////////////////
1: import org.apache.carbondata.core.indexstore.schema.CarbonRowSchema;
/////////////////////////////////////////////////////////////////////////
0:     CarbonRowSchema[] schema = unsafeMemoryDMStore.getSchema();
/////////////////////////////////////////////////////////////////////////
0:   private DataMapRow addMinMax(int[] minMaxLen, CarbonRowSchema carbonRowSchema,
0:       byte[][] minValues) {
0:     CarbonRowSchema[] minSchemas =
0:         ((CarbonRowSchema.StructCarbonRowSchema) carbonRowSchema).getChildSchemas();
/////////////////////////////////////////////////////////////////////////
0:     List<CarbonRowSchema> indexSchemas = new ArrayList<>();
0:     indexSchemas.add(new CarbonRowSchema.VariableCarbonRowSchema(DataTypes.BYTE_ARRAY));
0:       CarbonRowSchema[] mapSchemas = new CarbonRowSchema[minMaxLen.length];
0:           mapSchemas[i] = new CarbonRowSchema.VariableCarbonRowSchema(DataTypes.BYTE_ARRAY);
0:           mapSchemas[i] =
0:               new CarbonRowSchema.FixedCarbonRowSchema(DataTypes.BYTE_ARRAY, minMaxLen[i]);
0:       CarbonRowSchema mapSchema =
0:           new CarbonRowSchema.StructCarbonRowSchema(DataTypes.createDefaultStructType(),
0:               mapSchemas);
0:     indexSchemas.add(new CarbonRowSchema.FixedCarbonRowSchema(DataTypes.INT));
0:     indexSchemas.add(new CarbonRowSchema.VariableCarbonRowSchema(DataTypes.BYTE_ARRAY));
0:     indexSchemas.add(new CarbonRowSchema.FixedCarbonRowSchema(DataTypes.SHORT));
0:     indexSchemas.add(new CarbonRowSchema.FixedCarbonRowSchema(DataTypes.SHORT));
0:     indexSchemas.add(new CarbonRowSchema.FixedCarbonRowSchema(DataTypes.LONG));
0:     indexSchemas.add(new CarbonRowSchema.VariableCarbonRowSchema(DataTypes.BYTE_ARRAY));
0:         new UnsafeMemoryDMStore(indexSchemas.toArray(new CarbonRowSchema[indexSchemas.size()]));
author:sraghunandan
-------------------------------------------------------------------------------
commit:500654e
/////////////////////////////////////////////////////////////////////////
0: import java.util.Arrays;
/////////////////////////////////////////////////////////////////////////
1: import org.apache.carbondata.core.constants.CarbonCommonConstants;
/////////////////////////////////////////////////////////////////////////
0:       byte[] filePathBytes =
0:           filePath.getBytes(CarbonCommonConstants.DEFAULT_CHARSET_CLASS);
/////////////////////////////////////////////////////////////////////////
0:           "Successfully retrieved the start and end key" + "Dictionary Start Key: " + Arrays
0:               .toString(searchStartKey.getDictionaryKeys()) + "No Dictionary Start Key " + Arrays
0:               .toString(searchStartKey.getNoDictionaryKeys()) + "Dictionary End Key: " + Arrays
0:               .toString(searchEndKey.getDictionaryKeys()) + "No Dictionary End Key " + Arrays
0:               .toString(searchEndKey.getNoDictionaryKeys()));
/////////////////////////////////////////////////////////////////////////
0:     Blocklet blocklet = new Blocklet(
0:         new String(row.getByteArray(FILE_PATH_INDEX), CarbonCommonConstants.DEFAULT_CHARSET_CLASS),
0:         blockletId + "");
author:Raghunandan S
-------------------------------------------------------------------------------
commit:79feac9
/////////////////////////////////////////////////////////////////////////
1: import org.apache.carbondata.core.memory.MemoryException;
/////////////////////////////////////////////////////////////////////////
0:     } catch (Exception e) {
/////////////////////////////////////////////////////////////////////////
0:         row.setByteArray(serializedData, ordinal);
0:         unsafeMemoryDMStore.addIndexRowToUnsafe(row);
0:       } catch (Exception e) {
/////////////////////////////////////////////////////////////////////////
0:   private void createSchema(SegmentProperties segmentProperties) throws MemoryException {
============================================================================