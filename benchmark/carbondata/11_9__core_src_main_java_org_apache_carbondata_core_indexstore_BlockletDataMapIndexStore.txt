1:b681244: /*
1:b681244:  * Licensed to the Apache Software Foundation (ASF) under one or more
1:b681244:  * contributor license agreements.  See the NOTICE file distributed with
1:b681244:  * this work for additional information regarding copyright ownership.
1:b681244:  * The ASF licenses this file to You under the Apache License, Version 2.0
1:b681244:  * (the "License"); you may not use this file except in compliance with
1:b681244:  * the License.  You may obtain a copy of the License at
1:b681244:  *
1:b681244:  *    http://www.apache.org/licenses/LICENSE-2.0
1:b681244:  *
1:b681244:  * Unless required by applicable law or agreed to in writing, software
1:b681244:  * distributed under the License is distributed on an "AS IS" BASIS,
1:b681244:  * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
1:b681244:  * See the License for the specific language governing permissions and
1:b681244:  * limitations under the License.
3:b681244:  */
1:b681244: package org.apache.carbondata.core.indexstore;
1:41b0074: 
1:b681244: import java.io.IOException;
1:b681244: import java.util.ArrayList;
1:e580d64: import java.util.HashMap;
1:f5cdd5c: import java.util.HashSet;
1:b681244: import java.util.List;
1:b681244: import java.util.Map;
1:f5cdd5c: import java.util.Set;
1:b681244: import java.util.concurrent.ConcurrentHashMap;
9:b681244: 
1:b681244: import org.apache.carbondata.common.logging.LogService;
1:b681244: import org.apache.carbondata.common.logging.LogServiceFactory;
1:b681244: import org.apache.carbondata.core.cache.Cache;
1:b681244: import org.apache.carbondata.core.cache.CarbonLRUCache;
1:8d3c774: import org.apache.carbondata.core.constants.CarbonCommonConstants;
1:531ecdf: import org.apache.carbondata.core.datamap.dev.DataMap;
1:f4a58c5: import org.apache.carbondata.core.datastore.block.SegmentPropertiesAndSchemaHolder;
1:6118711: import org.apache.carbondata.core.indexstore.blockletindex.BlockDataMap;
1:6118711: import org.apache.carbondata.core.indexstore.blockletindex.BlockletDataMapFactory;
1:0586146: import org.apache.carbondata.core.indexstore.blockletindex.BlockletDataMapModel;
1:0586146: import org.apache.carbondata.core.indexstore.blockletindex.SegmentIndexFileStore;
1:f089287: import org.apache.carbondata.core.memory.MemoryException;
1:6118711: import org.apache.carbondata.core.metadata.schema.table.CarbonTable;
1:531ecdf: import org.apache.carbondata.core.util.BlockletDataMapUtil;
1:8d3c774: 
1:8f1a029: import org.apache.hadoop.conf.Configuration;
1:8f1a029: 
3:b681244: /**
1:b681244:  * Class to handle loading, unloading,clearing,storing of the table
1:b681244:  * blocks
1:b681244:  */
1:b681244: public class BlockletDataMapIndexStore
1:5f68a79:     implements Cache<TableBlockIndexUniqueIdentifierWrapper, BlockletDataMapIndexWrapper> {
1:b681244:   private static final LogService LOGGER =
1:b681244:       LogServiceFactory.getLogService(BlockletDataMapIndexStore.class.getName());
1:b681244:   /**
1:b681244:    * CarbonLRU cache
1:b681244:    */
1:b681244:   protected CarbonLRUCache lruCache;
1:b681244: 
1:b681244:   /**
1:b681244:    * map of block info to lock object map, while loading the btree this will be filled
1:b681244:    * and removed after loading the tree for that particular block info, this will be useful
1:b681244:    * while loading the tree concurrently so only block level lock will be applied another
1:b681244:    * block can be loaded concurrently
1:b681244:    */
1:b681244:   private Map<String, Object> segmentLockMap;
1:b681244: 
1:b681244:   /**
1:b681244:    * constructor to initialize the SegmentTaskIndexStore
1:b681244:    *
1:b681244:    * @param lruCache
1:b681244:    */
1:1155d4d:   public BlockletDataMapIndexStore(CarbonLRUCache lruCache) {
1:b681244:     this.lruCache = lruCache;
1:b681244:     segmentLockMap = new ConcurrentHashMap<String, Object>();
1:8d3c774:   }
1:8d3c774: 
1:531ecdf:   @Override
1:5f68a79:   public BlockletDataMapIndexWrapper get(TableBlockIndexUniqueIdentifierWrapper identifierWrapper)
1:b681244:       throws IOException {
1:e580d64:     return get(identifierWrapper, null);
1:e580d64:   }
1:e580d64: 
1:e580d64:   private BlockletDataMapIndexWrapper get(TableBlockIndexUniqueIdentifierWrapper identifierWrapper,
1:e580d64:       Map<String, Map<String, BlockMetaInfo>> segInfoCache) throws IOException {
1:5f68a79:     TableBlockIndexUniqueIdentifier identifier =
1:5f68a79:         identifierWrapper.getTableBlockIndexUniqueIdentifier();
1:0586146:     String lruCacheKey = identifier.getUniqueTableSegmentIdentifier();
1:531ecdf:     BlockletDataMapIndexWrapper blockletDataMapIndexWrapper =
1:531ecdf:         (BlockletDataMapIndexWrapper) lruCache.get(lruCacheKey);
1:6118711:     List<BlockDataMap> dataMaps = new ArrayList<>();
1:531ecdf:     if (blockletDataMapIndexWrapper == null) {
1:531ecdf:       try {
1:8f1a029:         SegmentIndexFileStore indexFileStore =
1:8f1a029:             new SegmentIndexFileStore(identifierWrapper.getConfiguration());
1:f5cdd5c:         Set<String> filesRead = new HashSet<>();
1:531ecdf:         String segmentFilePath = identifier.getIndexFilePath();
1:e580d64:         if (segInfoCache == null) {
1:e580d64:           segInfoCache = new HashMap<String, Map<String, BlockMetaInfo>>();
1:e580d64:         }
1:e580d64:         Map<String, BlockMetaInfo> carbonDataFileBlockMetaInfoMapping =
1:e580d64:             segInfoCache.get(segmentFilePath);
1:e580d64:         if (carbonDataFileBlockMetaInfoMapping == null) {
1:e580d64:           carbonDataFileBlockMetaInfoMapping =
1:8f1a029:               BlockletDataMapUtil.createCarbonDataFileBlockMetaInfoMapping(segmentFilePath,
1:8f1a029:                   identifierWrapper.getConfiguration());
1:e580d64:           segInfoCache.put(segmentFilePath, carbonDataFileBlockMetaInfoMapping);
1:e580d64:         }
1:531ecdf:         // if the identifier is not a merge file we can directly load the datamaps
1:531ecdf:         if (identifier.getMergeIndexFileName() == null) {
2:531ecdf:           Map<String, BlockMetaInfo> blockMetaInfoMap = BlockletDataMapUtil
1:5f68a79:               .getBlockMetaInfoMap(identifierWrapper, indexFileStore, filesRead,
1:531ecdf:                   carbonDataFileBlockMetaInfoMapping);
1:6118711:           BlockDataMap blockletDataMap =
1:6118711:               loadAndGetDataMap(identifier, indexFileStore, blockMetaInfoMap,
1:8f1a029:                   identifierWrapper.getCarbonTable(), identifierWrapper.isAddTableBlockToUnsafe(),
1:8f1a029:                   identifierWrapper.getConfiguration());
2:531ecdf:           dataMaps.add(blockletDataMap);
1:e580d64:           blockletDataMapIndexWrapper =
1:8f1a029:               new BlockletDataMapIndexWrapper(identifier.getSegmentId(), dataMaps,
1:8f1a029:                   identifierWrapper.getConfiguration());
1:531ecdf:         } else {
1:531ecdf:           // if the identifier is a merge file then collect the index files and load the datamaps
1:531ecdf:           List<TableBlockIndexUniqueIdentifier> tableBlockIndexUniqueIdentifiers =
1:531ecdf:               BlockletDataMapUtil.getIndexFileIdentifiersFromMergeFile(identifier, indexFileStore);
1:531ecdf:           for (TableBlockIndexUniqueIdentifier blockIndexUniqueIdentifier :
1:531ecdf:               tableBlockIndexUniqueIdentifiers) {
1:5f68a79:             Map<String, BlockMetaInfo> blockMetaInfoMap = BlockletDataMapUtil.getBlockMetaInfoMap(
1:5f68a79:                 new TableBlockIndexUniqueIdentifierWrapper(blockIndexUniqueIdentifier,
1:5f68a79:                     identifierWrapper.getCarbonTable()), indexFileStore, filesRead,
1:531ecdf:                 carbonDataFileBlockMetaInfoMapping);
1:7341907:             if (!blockMetaInfoMap.isEmpty()) {
1:6118711:               BlockDataMap blockletDataMap =
1:6118711:                   loadAndGetDataMap(blockIndexUniqueIdentifier, indexFileStore, blockMetaInfoMap,
1:bd02656:                       identifierWrapper.getCarbonTable(),
1:8f1a029:                       identifierWrapper.isAddTableBlockToUnsafe(),
1:8f1a029:                       identifierWrapper.getConfiguration());
1:7341907:               dataMaps.add(blockletDataMap);
1:7341907:             }
1:531ecdf:           }
1:e580d64:           blockletDataMapIndexWrapper =
1:8f1a029:               new BlockletDataMapIndexWrapper(identifier.getSegmentId(), dataMaps,
1:8f1a029:                   identifierWrapper.getConfiguration());
1:531ecdf:         }
1:531ecdf:         lruCache.put(identifier.getUniqueTableSegmentIdentifier(), blockletDataMapIndexWrapper,
1:16ed99a:             blockletDataMapIndexWrapper.getMemorySize());
1:531ecdf:       } catch (Throwable e) {
1:531ecdf:         // clear all the memory used by datamaps loaded
1:531ecdf:         for (DataMap dataMap : dataMaps) {
1:531ecdf:           dataMap.clear();
1:531ecdf:         }
1:f089287:         LOGGER.error("memory exception when loading datamap: " + e.getMessage());
1:f089287:         throw new RuntimeException(e.getMessage(), e);
1:531ecdf:       }
1:5daae95:     }
1:531ecdf:     return blockletDataMapIndexWrapper;
1:8d3c774:   }
1:8d3c774: 
1:5f68a79:   @Override public List<BlockletDataMapIndexWrapper> getAll(
1:5f68a79:       List<TableBlockIndexUniqueIdentifierWrapper> tableSegmentUniqueIdentifiers)
1:5f68a79:       throws IOException {
1:e580d64:     Map<String, Map<String, BlockMetaInfo>> segInfoCache
1:e580d64:         = new HashMap<String, Map<String, BlockMetaInfo>>();
1:e580d64: 
1:531ecdf:     List<BlockletDataMapIndexWrapper> blockletDataMapIndexWrappers =
1:89a12af:         new ArrayList<>(tableSegmentUniqueIdentifiers.size());
1:5f68a79:     List<TableBlockIndexUniqueIdentifierWrapper> missedIdentifiersWrapper = new ArrayList<>();
1:531ecdf:     BlockletDataMapIndexWrapper blockletDataMapIndexWrapper = null;
1:0586146:     // Get the datamaps for each indexfile from cache.
2:b681244:     try {
1:5f68a79:       for (TableBlockIndexUniqueIdentifierWrapper
1:5f68a79:                identifierWrapper : tableSegmentUniqueIdentifiers) {
1:5f68a79:         BlockletDataMapIndexWrapper dataMapIndexWrapper =
1:5f68a79:             getIfPresent(identifierWrapper);
1:531ecdf:         if (dataMapIndexWrapper != null) {
1:531ecdf:           blockletDataMapIndexWrappers.add(dataMapIndexWrapper);
1:5daae95:         } else {
1:5f68a79:           missedIdentifiersWrapper.add(identifierWrapper);
1:8d3c774:         }
1:8d3c774:       }
1:5f68a79:       if (missedIdentifiersWrapper.size() > 0) {
1:5f68a79:         for (TableBlockIndexUniqueIdentifierWrapper identifierWrapper : missedIdentifiersWrapper) {
1:e580d64:           blockletDataMapIndexWrapper = get(identifierWrapper, segInfoCache);
1:531ecdf:           blockletDataMapIndexWrappers.add(blockletDataMapIndexWrapper);
1:41b0074:         }
1:41b0074:       }
1:531ecdf:     } catch (Throwable e) {
1:531ecdf:       if (null != blockletDataMapIndexWrapper) {
1:6118711:         List<BlockDataMap> dataMaps = blockletDataMapIndexWrapper.getDataMaps();
1:531ecdf:         for (DataMap dataMap : dataMaps) {
1:531ecdf:           dataMap.clear();
1:531ecdf:         }
1:6094af6:       }
1:b681244:       throw new IOException("Problem in loading segment blocks.", e);
1:6094af6:     }
1:5f68a79: 
1:531ecdf:     return blockletDataMapIndexWrappers;
1:6094af6:   }
1:6094af6: 
1:6094af6:   /**
1:b681244:    * returns the SegmentTaskIndexWrapper
1:b681244:    *
1:5f68a79:    * @param tableSegmentUniqueIdentifierWrapper
1:b681244:    * @return
1:6094af6:    */
1:5f68a79:   @Override public BlockletDataMapIndexWrapper getIfPresent(
1:5f68a79:       TableBlockIndexUniqueIdentifierWrapper tableSegmentUniqueIdentifierWrapper) {
1:531ecdf:     return (BlockletDataMapIndexWrapper) lruCache.get(
1:5f68a79:         tableSegmentUniqueIdentifierWrapper.getTableBlockIndexUniqueIdentifier()
1:5f68a79:             .getUniqueTableSegmentIdentifier());
1:6094af6:   }
1:6094af6: 
1:b681244:   /**
1:b681244:    * method invalidate the segment cache for segment
1:b681244:    *
1:5f68a79:    * @param tableSegmentUniqueIdentifierWrapper
1:b681244:    */
1:5f68a79:   @Override public void invalidate(
1:5f68a79:       TableBlockIndexUniqueIdentifierWrapper tableSegmentUniqueIdentifierWrapper) {
1:f4a58c5:     BlockletDataMapIndexWrapper blockletDataMapIndexWrapper =
1:f4a58c5:         getIfPresent(tableSegmentUniqueIdentifierWrapper);
1:f4a58c5:     if (null != blockletDataMapIndexWrapper) {
1:f4a58c5:       // clear the segmentProperties cache
1:f4a58c5:       List<BlockDataMap> dataMaps = blockletDataMapIndexWrapper.getDataMaps();
1:dc29319:       if (null != dataMaps && !dataMaps.isEmpty()) {
1:f4a58c5:         String segmentId =
1:f4a58c5:             tableSegmentUniqueIdentifierWrapper.getTableBlockIndexUniqueIdentifier().getSegmentId();
1:dc29319:         // as segmentId will be same for all the dataMaps and segmentProperties cache is
1:dc29319:         // maintained at segment level so it need to be called only once for clearing
1:dc29319:         SegmentPropertiesAndSchemaHolder.getInstance()
1:8e78957:             .invalidate(segmentId, dataMaps.get(0).getSegmentPropertiesIndex(),
1:8e78957:                 tableSegmentUniqueIdentifierWrapper.isAddTableBlockToUnsafe());
1:f4a58c5:       }
1:f4a58c5:     }
1:5f68a79:     lruCache.remove(tableSegmentUniqueIdentifierWrapper.getTableBlockIndexUniqueIdentifier()
1:5f68a79:         .getUniqueTableSegmentIdentifier());
1:f4a58c5:   }
1:6094af6: 
1:0586146:   @Override
1:5f68a79:   public void put(TableBlockIndexUniqueIdentifierWrapper tableBlockIndexUniqueIdentifierWrapper,
1:531ecdf:       BlockletDataMapIndexWrapper wrapper) throws IOException, MemoryException {
1:531ecdf:     // As dataMap will use unsafe memory, it is not recommended to overwrite an existing entry
1:531ecdf:     // as in that case clearing unsafe memory need to be taken card. If at all datamap entry
1:531ecdf:     // in the cache need to be overwritten then use the invalidate interface
1:531ecdf:     // and then use the put interface
2:5f68a79:     if (null == getIfPresent(tableBlockIndexUniqueIdentifierWrapper)) {
1:a20f22e:       List<BlockDataMap> dataMaps = wrapper.getDataMaps();
1:a20f22e:       try {
1:a20f22e:         for (BlockDataMap blockletDataMap : dataMaps) {
1:a20f22e:           blockletDataMap.convertToUnsafeDMStore();
1:531ecdf:         }
1:a20f22e:         // Locking is not required here because in LRU cache map add method is synchronized to add
1:a20f22e:         // only one entry at a time and if a key already exists it will not overwrite the entry
1:a20f22e:         lruCache.put(tableBlockIndexUniqueIdentifierWrapper.getTableBlockIndexUniqueIdentifier()
1:a20f22e:             .getUniqueTableSegmentIdentifier(), wrapper, wrapper.getMemorySize());
1:a20f22e:       } catch (Throwable e) {
1:a20f22e:         // clear all the memory acquired by data map in case of any failure
1:a20f22e:         for (DataMap blockletDataMap : dataMaps) {
1:a20f22e:           blockletDataMap.clear();
1:a20f22e:         }
1:a20f22e:         throw new IOException("Problem in adding datamap to cache.", e);
1:531ecdf:       }
1:6094af6:     }
1:b8a02f3:   }
1:6094af6: 
1:b681244: 
1:b681244:   /**
1:b681244:    * Below method will be used to load the segment of segments
1:b681244:    * One segment may have multiple task , so  table segment will be loaded
1:b681244:    * based on task id and will return the map of taksId to table segment
1:b681244:    * map
1:b681244:    *
1:b681244:    * @return map of taks id to segment mapping
1:b681244:    * @throws IOException
1:b681244:    */
1:6118711:   private BlockDataMap loadAndGetDataMap(TableBlockIndexUniqueIdentifier identifier,
1:6118711:       SegmentIndexFileStore indexFileStore, Map<String, BlockMetaInfo> blockMetaInfoMap,
1:8f1a029:       CarbonTable carbonTable, boolean addTableBlockToUnsafe, Configuration configuration)
1:f089287:       throws IOException, MemoryException {
1:531ecdf:     String uniqueTableSegmentIdentifier =
1:0586146:         identifier.getUniqueTableSegmentIdentifier();
1:531ecdf:     Object lock = segmentLockMap.get(uniqueTableSegmentIdentifier);
1:531ecdf:     if (lock == null) {
1:531ecdf:       lock = addAndGetSegmentLock(uniqueTableSegmentIdentifier);
1:531ecdf:     }
1:6118711:     BlockDataMap dataMap;
1:531ecdf:     synchronized (lock) {
1:6118711:       dataMap = (BlockDataMap) BlockletDataMapFactory.createDataMap(carbonTable);
1:f4a58c5:       dataMap.init(new BlockletDataMapModel(carbonTable,
3:8d3c774:           identifier.getIndexFilePath() + CarbonCommonConstants.FILE_SEPARATOR + identifier
1:8d3c774:               .getIndexFileName(), indexFileStore.getFileData(identifier.getIndexFileName()),
1:8f1a029:           blockMetaInfoMap, identifier.getSegmentId(), addTableBlockToUnsafe, configuration));
1:531ecdf:     }
3:b681244:     return dataMap;
1:531ecdf:   }
1:531ecdf: 
1:b681244:   /**
1:b681244:    * Below method will be used to get the segment level lock object
1:b681244:    *
1:b681244:    * @param uniqueIdentifier
1:b681244:    * @return lock object
1:b681244:    */
1:b681244:   private synchronized Object addAndGetSegmentLock(String uniqueIdentifier) {
1:b681244:     // get the segment lock object if it is present then return
1:b681244:     // otherwise add the new lock and return
1:b681244:     Object segmentLoderLockObject = segmentLockMap.get(uniqueIdentifier);
1:b681244:     if (null == segmentLoderLockObject) {
1:b681244:       segmentLoderLockObject = new Object();
1:b681244:       segmentLockMap.put(uniqueIdentifier, segmentLoderLockObject);
1:531ecdf:     }
1:b681244:     return segmentLoderLockObject;
1:531ecdf:   }
1:531ecdf: 
1:b681244:   /**
1:b681244:    * The method clears the access count of table segments
1:b681244:    *
1:5f68a79:    * @param tableSegmentUniqueIdentifiersWrapper
1:b681244:    */
1:5f68a79:   @Override public void clearAccessCount(
1:5f68a79:       List<TableBlockIndexUniqueIdentifierWrapper> tableSegmentUniqueIdentifiersWrapper) {
1:5f68a79:     for (TableBlockIndexUniqueIdentifierWrapper
1:5f68a79:              identifierWrapper : tableSegmentUniqueIdentifiersWrapper) {
1:6118711:       BlockDataMap cacheable = (BlockDataMap) lruCache.get(
1:5f68a79:           identifierWrapper.getTableBlockIndexUniqueIdentifier().getUniqueTableSegmentIdentifier());
1:b681244:       cacheable.clear();
1:8d3c774:     }
1:8d3c774:   }
1:8d3c774: }
============================================================================
author:kunal642
-------------------------------------------------------------------------------
commit:8f1a029
/////////////////////////////////////////////////////////////////////////
1: import org.apache.hadoop.conf.Configuration;
1: 
/////////////////////////////////////////////////////////////////////////
1:         SegmentIndexFileStore indexFileStore =
1:             new SegmentIndexFileStore(identifierWrapper.getConfiguration());
/////////////////////////////////////////////////////////////////////////
1:               BlockletDataMapUtil.createCarbonDataFileBlockMetaInfoMapping(segmentFilePath,
1:                   identifierWrapper.getConfiguration());
/////////////////////////////////////////////////////////////////////////
1:                   identifierWrapper.getCarbonTable(), identifierWrapper.isAddTableBlockToUnsafe(),
1:                   identifierWrapper.getConfiguration());
1:               new BlockletDataMapIndexWrapper(identifier.getSegmentId(), dataMaps,
1:                   identifierWrapper.getConfiguration());
/////////////////////////////////////////////////////////////////////////
1:                       identifierWrapper.isAddTableBlockToUnsafe(),
1:                       identifierWrapper.getConfiguration());
1:               new BlockletDataMapIndexWrapper(identifier.getSegmentId(), dataMaps,
1:                   identifierWrapper.getConfiguration());
/////////////////////////////////////////////////////////////////////////
1:       CarbonTable carbonTable, boolean addTableBlockToUnsafe, Configuration configuration)
/////////////////////////////////////////////////////////////////////////
1:           blockMetaInfoMap, identifier.getSegmentId(), addTableBlockToUnsafe, configuration));
author:rahul
-------------------------------------------------------------------------------
commit:e580d64
/////////////////////////////////////////////////////////////////////////
1: import java.util.HashMap;
/////////////////////////////////////////////////////////////////////////
1:     return get(identifierWrapper, null);
1:   }
1: 
1:   private BlockletDataMapIndexWrapper get(TableBlockIndexUniqueIdentifierWrapper identifierWrapper,
1:       Map<String, Map<String, BlockMetaInfo>> segInfoCache) throws IOException {
/////////////////////////////////////////////////////////////////////////
1:         if (segInfoCache == null) {
1:           segInfoCache = new HashMap<String, Map<String, BlockMetaInfo>>();
1:         }
1:         Map<String, BlockMetaInfo> carbonDataFileBlockMetaInfoMapping =
1:             segInfoCache.get(segmentFilePath);
1:         if (carbonDataFileBlockMetaInfoMapping == null) {
1:           carbonDataFileBlockMetaInfoMapping =
0:               BlockletDataMapUtil.createCarbonDataFileBlockMetaInfoMapping(segmentFilePath);
1:           segInfoCache.put(segmentFilePath, carbonDataFileBlockMetaInfoMapping);
1:         }
/////////////////////////////////////////////////////////////////////////
1:           blockletDataMapIndexWrapper =
0:               new BlockletDataMapIndexWrapper(identifier.getSegmentId(), dataMaps);
/////////////////////////////////////////////////////////////////////////
1:           blockletDataMapIndexWrapper =
0:               new BlockletDataMapIndexWrapper(identifier.getSegmentId(), dataMaps);
/////////////////////////////////////////////////////////////////////////
1:     Map<String, Map<String, BlockMetaInfo>> segInfoCache
1:         = new HashMap<String, Map<String, BlockMetaInfo>>();
1: 
/////////////////////////////////////////////////////////////////////////
1:           blockletDataMapIndexWrapper = get(identifierWrapper, segInfoCache);
author:manishgupta88
-------------------------------------------------------------------------------
commit:a20f22e
/////////////////////////////////////////////////////////////////////////
1:       List<BlockDataMap> dataMaps = wrapper.getDataMaps();
1:       try {
1:         for (BlockDataMap blockletDataMap : dataMaps) {
1:           blockletDataMap.convertToUnsafeDMStore();
1:         // Locking is not required here because in LRU cache map add method is synchronized to add
1:         // only one entry at a time and if a key already exists it will not overwrite the entry
1:         lruCache.put(tableBlockIndexUniqueIdentifierWrapper.getTableBlockIndexUniqueIdentifier()
1:             .getUniqueTableSegmentIdentifier(), wrapper, wrapper.getMemorySize());
1:       } catch (Throwable e) {
1:         // clear all the memory acquired by data map in case of any failure
1:         for (DataMap blockletDataMap : dataMaps) {
1:           blockletDataMap.clear();
1:         }
1:         throw new IOException("Problem in adding datamap to cache.", e);
commit:bd02656
/////////////////////////////////////////////////////////////////////////
0:                   identifierWrapper.getCarbonTable(), identifierWrapper.isAddTableBlockToUnsafe());
/////////////////////////////////////////////////////////////////////////
1:                     identifierWrapper.getCarbonTable(),
0:                     identifierWrapper.isAddTableBlockToUnsafe());
/////////////////////////////////////////////////////////////////////////
0:       CarbonTable carbonTable, boolean addTableBlockToUnsafe)
/////////////////////////////////////////////////////////////////////////
0:           blockMetaInfoMap, identifier.getSegmentId(), addTableBlockToUnsafe));
commit:dc29319
/////////////////////////////////////////////////////////////////////////
1:       if (null != dataMaps && !dataMaps.isEmpty()) {
1:         // as segmentId will be same for all the dataMaps and segmentProperties cache is
1:         // maintained at segment level so it need to be called only once for clearing
1:         SegmentPropertiesAndSchemaHolder.getInstance()
0:             .invalidate(segmentId, dataMaps.get(0).getSegmentPropertiesIndex());
commit:f4a58c5
/////////////////////////////////////////////////////////////////////////
1: import org.apache.carbondata.core.datastore.block.SegmentPropertiesAndSchemaHolder;
/////////////////////////////////////////////////////////////////////////
1:     BlockletDataMapIndexWrapper blockletDataMapIndexWrapper =
1:         getIfPresent(tableSegmentUniqueIdentifierWrapper);
1:     if (null != blockletDataMapIndexWrapper) {
1:       // clear the segmentProperties cache
1:       List<BlockDataMap> dataMaps = blockletDataMapIndexWrapper.getDataMaps();
0:       if (null != dataMaps) {
1:         String segmentId =
1:             tableSegmentUniqueIdentifierWrapper.getTableBlockIndexUniqueIdentifier().getSegmentId();
0:         for (BlockDataMap dataMap : dataMaps) {
0:           // as segmentId will be same for all the dataMaps and segmentProperties cache is
0:           // maintained at segment level so it need to be called only once for clearing
0:           SegmentPropertiesAndSchemaHolder.getInstance()
0:               .invalidate(segmentId, dataMap.getSegmentPropertiesIndex());
0:           break;
1:         }
1:       }
1:     }
/////////////////////////////////////////////////////////////////////////
1:       dataMap.init(new BlockletDataMapModel(carbonTable,
commit:6118711
/////////////////////////////////////////////////////////////////////////
1: import org.apache.carbondata.core.indexstore.blockletindex.BlockDataMap;
1: import org.apache.carbondata.core.indexstore.blockletindex.BlockletDataMapFactory;
1: import org.apache.carbondata.core.metadata.schema.table.CarbonTable;
/////////////////////////////////////////////////////////////////////////
1:     List<BlockDataMap> dataMaps = new ArrayList<>();
/////////////////////////////////////////////////////////////////////////
1:           BlockDataMap blockletDataMap =
1:               loadAndGetDataMap(identifier, indexFileStore, blockMetaInfoMap,
0:                   identifierWrapper.getCarbonTable());
/////////////////////////////////////////////////////////////////////////
1:             BlockDataMap blockletDataMap =
1:                 loadAndGetDataMap(blockIndexUniqueIdentifier, indexFileStore, blockMetaInfoMap,
0:                     identifierWrapper.getCarbonTable());
/////////////////////////////////////////////////////////////////////////
1:         List<BlockDataMap> dataMaps = blockletDataMapIndexWrapper.getDataMaps();
/////////////////////////////////////////////////////////////////////////
0:           List<BlockDataMap> dataMaps = wrapper.getDataMaps();
0:             for (BlockDataMap blockletDataMap: dataMaps) {
/////////////////////////////////////////////////////////////////////////
1:   private BlockDataMap loadAndGetDataMap(TableBlockIndexUniqueIdentifier identifier,
1:       SegmentIndexFileStore indexFileStore, Map<String, BlockMetaInfo> blockMetaInfoMap,
0:       CarbonTable carbonTable)
/////////////////////////////////////////////////////////////////////////
1:     BlockDataMap dataMap;
1:       dataMap = (BlockDataMap) BlockletDataMapFactory.createDataMap(carbonTable);
/////////////////////////////////////////////////////////////////////////
1:       BlockDataMap cacheable = (BlockDataMap) lruCache.get(
author:m00258959
-------------------------------------------------------------------------------
commit:8e78957
/////////////////////////////////////////////////////////////////////////
1:             .invalidate(segmentId, dataMaps.get(0).getSegmentPropertiesIndex(),
1:                 tableSegmentUniqueIdentifierWrapper.isAddTableBlockToUnsafe());
author:dhatchayani
-------------------------------------------------------------------------------
commit:7341907
/////////////////////////////////////////////////////////////////////////
1:             if (!blockMetaInfoMap.isEmpty()) {
0:               BlockDataMap blockletDataMap =
0:                   loadAndGetDataMap(blockIndexUniqueIdentifier, indexFileStore, blockMetaInfoMap,
0:                       identifierWrapper.getCarbonTable(),
0:                       identifierWrapper.isAddTableBlockToUnsafe());
1:               dataMaps.add(blockletDataMap);
1:             }
commit:16ed99a
/////////////////////////////////////////////////////////////////////////
/////////////////////////////////////////////////////////////////////////
/////////////////////////////////////////////////////////////////////////
1:             blockletDataMapIndexWrapper.getMemorySize());
/////////////////////////////////////////////////////////////////////////
/////////////////////////////////////////////////////////////////////////
0:                 wrapper.getMemorySize());
commit:531ecdf
/////////////////////////////////////////////////////////////////////////
/////////////////////////////////////////////////////////////////////////
1: import org.apache.carbondata.core.datamap.dev.DataMap;
1: import org.apache.carbondata.core.util.BlockletDataMapUtil;
0:     implements Cache<TableBlockIndexUniqueIdentifier, BlockletDataMapIndexWrapper> {
/////////////////////////////////////////////////////////////////////////
0:   public BlockletDataMapIndexWrapper get(TableBlockIndexUniqueIdentifier identifier)
1:     BlockletDataMapIndexWrapper blockletDataMapIndexWrapper =
1:         (BlockletDataMapIndexWrapper) lruCache.get(lruCacheKey);
0:     List<BlockletDataMap> dataMaps = new ArrayList<>();
1:     if (blockletDataMapIndexWrapper == null) {
0:         long memorySize = 0L;
1:         String segmentFilePath = identifier.getIndexFilePath();
0:         Map<String, BlockMetaInfo> carbonDataFileBlockMetaInfoMapping = BlockletDataMapUtil
0:             .createCarbonDataFileBlockMetaInfoMapping(segmentFilePath);
1:         // if the identifier is not a merge file we can directly load the datamaps
1:         if (identifier.getMergeIndexFileName() == null) {
1:           Map<String, BlockMetaInfo> blockMetaInfoMap = BlockletDataMapUtil
0:               .getBlockMetaInfoMap(identifier, indexFileStore, filesRead,
1:                   carbonDataFileBlockMetaInfoMapping);
0:           BlockletDataMap blockletDataMap =
0:               loadAndGetDataMap(identifier, indexFileStore, blockMetaInfoMap);
0:           memorySize += blockletDataMap.getMemorySize();
1:           dataMaps.add(blockletDataMap);
0:           blockletDataMapIndexWrapper = new BlockletDataMapIndexWrapper(dataMaps);
1:         } else {
1:           // if the identifier is a merge file then collect the index files and load the datamaps
1:           List<TableBlockIndexUniqueIdentifier> tableBlockIndexUniqueIdentifiers =
1:               BlockletDataMapUtil.getIndexFileIdentifiersFromMergeFile(identifier, indexFileStore);
1:           for (TableBlockIndexUniqueIdentifier blockIndexUniqueIdentifier :
1:               tableBlockIndexUniqueIdentifiers) {
1:             Map<String, BlockMetaInfo> blockMetaInfoMap = BlockletDataMapUtil
0:                 .getBlockMetaInfoMap(blockIndexUniqueIdentifier, indexFileStore, filesRead,
1:                     carbonDataFileBlockMetaInfoMapping);
0:             BlockletDataMap blockletDataMap =
0:                 loadAndGetDataMap(blockIndexUniqueIdentifier, indexFileStore, blockMetaInfoMap);
0:             memorySize += blockletDataMap.getMemorySize();
1:             dataMaps.add(blockletDataMap);
1:           }
0:           blockletDataMapIndexWrapper = new BlockletDataMapIndexWrapper(dataMaps);
1:         }
1:         lruCache.put(identifier.getUniqueTableSegmentIdentifier(), blockletDataMapIndexWrapper,
0:             memorySize);
1:       } catch (Throwable e) {
1:         // clear all the memory used by datamaps loaded
1:         for (DataMap dataMap : dataMaps) {
1:           dataMap.clear();
1:         }
1:     return blockletDataMapIndexWrapper;
0:   public List<BlockletDataMapIndexWrapper> getAll(
1:     List<BlockletDataMapIndexWrapper> blockletDataMapIndexWrappers =
1:     BlockletDataMapIndexWrapper blockletDataMapIndexWrapper = null;
0:         BlockletDataMapIndexWrapper dataMapIndexWrapper = getIfPresent(identifier);
1:         if (dataMapIndexWrapper != null) {
1:           blockletDataMapIndexWrappers.add(dataMapIndexWrapper);
0:         for (TableBlockIndexUniqueIdentifier identifier : missedIdentifiers) {
0:           blockletDataMapIndexWrapper = get(identifier);
1:           blockletDataMapIndexWrappers.add(blockletDataMapIndexWrapper);
1:       if (null != blockletDataMapIndexWrapper) {
0:         List<BlockletDataMap> dataMaps = blockletDataMapIndexWrapper.getDataMaps();
1:         for (DataMap dataMap : dataMaps) {
1:           dataMap.clear();
1:         }
1:     return blockletDataMapIndexWrappers;
/////////////////////////////////////////////////////////////////////////
0:   public BlockletDataMapIndexWrapper getIfPresent(
1:     return (BlockletDataMapIndexWrapper) lruCache.get(
/////////////////////////////////////////////////////////////////////////
1:   @Override
0:   public void put(TableBlockIndexUniqueIdentifier tableBlockIndexUniqueIdentifier,
1:       BlockletDataMapIndexWrapper wrapper) throws IOException, MemoryException {
1:     String uniqueTableSegmentIdentifier =
0:         tableBlockIndexUniqueIdentifier.getUniqueTableSegmentIdentifier();
1:     Object lock = segmentLockMap.get(uniqueTableSegmentIdentifier);
1:     if (lock == null) {
1:       lock = addAndGetSegmentLock(uniqueTableSegmentIdentifier);
1:     }
0:     long memorySize = 0L;
1:     // As dataMap will use unsafe memory, it is not recommended to overwrite an existing entry
1:     // as in that case clearing unsafe memory need to be taken card. If at all datamap entry
1:     // in the cache need to be overwritten then use the invalidate interface
1:     // and then use the put interface
0:     if (null == getIfPresent(tableBlockIndexUniqueIdentifier)) {
1:       synchronized (lock) {
0:         if (null == getIfPresent(tableBlockIndexUniqueIdentifier)) {
0:           List<BlockletDataMap> dataMaps = wrapper.getDataMaps();
1:           try {
0:             for (BlockletDataMap blockletDataMap: dataMaps) {
0:               blockletDataMap.convertToUnsafeDMStore();
0:               memorySize += blockletDataMap.getMemorySize();
1:             }
0:             lruCache.put(tableBlockIndexUniqueIdentifier.getUniqueTableSegmentIdentifier(), wrapper,
0:                 memorySize);
1:           } catch (Throwable e) {
0:             // clear all the memory acquired by data map in case of any failure
0:             for (DataMap blockletDataMap : dataMaps) {
0:               blockletDataMap.clear();
1:             }
0:             throw new IOException("Problem in adding datamap to cache.", e);
1:           }
1:         }
1:       }
1:     }
1:   }
1: 
1: 
/////////////////////////////////////////////////////////////////////////
commit:f5cdd5c
/////////////////////////////////////////////////////////////////////////
1: import java.util.HashSet;
1: import java.util.Set;
/////////////////////////////////////////////////////////////////////////
1:         Set<String> filesRead = new HashSet<>();
0:             getBlockMetaInfoMap(identifier, indexFileStore, filesRead);
/////////////////////////////////////////////////////////////////////////
0:       SegmentIndexFileStore indexFileStore, Set<String> filesRead) throws IOException {
0:       if (indexMergeFile.exists() && !filesRead.contains(indexMergeFile.getPath())) {
0:         filesRead.add(indexMergeFile.getPath());
/////////////////////////////////////////////////////////////////////////
0:         Set<String> filesRead = new HashSet<>();
0:               getBlockMetaInfoMap(identifier, indexFileStore, filesRead);
author:ajantha-bhat
-------------------------------------------------------------------------------
commit:5f68a79
/////////////////////////////////////////////////////////////////////////
1:     implements Cache<TableBlockIndexUniqueIdentifierWrapper, BlockletDataMapIndexWrapper> {
/////////////////////////////////////////////////////////////////////////
1:   public BlockletDataMapIndexWrapper get(TableBlockIndexUniqueIdentifierWrapper identifierWrapper)
1:     TableBlockIndexUniqueIdentifier identifier =
1:         identifierWrapper.getTableBlockIndexUniqueIdentifier();
/////////////////////////////////////////////////////////////////////////
1:               .getBlockMetaInfoMap(identifierWrapper, indexFileStore, filesRead,
/////////////////////////////////////////////////////////////////////////
1:             Map<String, BlockMetaInfo> blockMetaInfoMap = BlockletDataMapUtil.getBlockMetaInfoMap(
1:                 new TableBlockIndexUniqueIdentifierWrapper(blockIndexUniqueIdentifier,
1:                     identifierWrapper.getCarbonTable()), indexFileStore, filesRead,
0:                 carbonDataFileBlockMetaInfoMapping);
/////////////////////////////////////////////////////////////////////////
1:   @Override public List<BlockletDataMapIndexWrapper> getAll(
1:       List<TableBlockIndexUniqueIdentifierWrapper> tableSegmentUniqueIdentifiers)
1:       throws IOException {
1:     List<TableBlockIndexUniqueIdentifierWrapper> missedIdentifiersWrapper = new ArrayList<>();
1:       for (TableBlockIndexUniqueIdentifierWrapper
1:                identifierWrapper : tableSegmentUniqueIdentifiers) {
1:         BlockletDataMapIndexWrapper dataMapIndexWrapper =
1:             getIfPresent(identifierWrapper);
1:           missedIdentifiersWrapper.add(identifierWrapper);
1:       if (missedIdentifiersWrapper.size() > 0) {
1:         for (TableBlockIndexUniqueIdentifierWrapper identifierWrapper : missedIdentifiersWrapper) {
0:           blockletDataMapIndexWrapper = get(identifierWrapper);
/////////////////////////////////////////////////////////////////////////
1: 
1:    * @param tableSegmentUniqueIdentifierWrapper
1:   @Override public BlockletDataMapIndexWrapper getIfPresent(
1:       TableBlockIndexUniqueIdentifierWrapper tableSegmentUniqueIdentifierWrapper) {
1:         tableSegmentUniqueIdentifierWrapper.getTableBlockIndexUniqueIdentifier()
1:             .getUniqueTableSegmentIdentifier());
1:    * @param tableSegmentUniqueIdentifierWrapper
1:   @Override public void invalidate(
1:       TableBlockIndexUniqueIdentifierWrapper tableSegmentUniqueIdentifierWrapper) {
1:     lruCache.remove(tableSegmentUniqueIdentifierWrapper.getTableBlockIndexUniqueIdentifier()
1:         .getUniqueTableSegmentIdentifier());
1:   public void put(TableBlockIndexUniqueIdentifierWrapper tableBlockIndexUniqueIdentifierWrapper,
0:         tableBlockIndexUniqueIdentifierWrapper.getTableBlockIndexUniqueIdentifier()
0:             .getUniqueTableSegmentIdentifier();
/////////////////////////////////////////////////////////////////////////
1:     if (null == getIfPresent(tableBlockIndexUniqueIdentifierWrapper)) {
1:         if (null == getIfPresent(tableBlockIndexUniqueIdentifierWrapper)) {
0:             lruCache.put(tableBlockIndexUniqueIdentifierWrapper.getTableBlockIndexUniqueIdentifier()
0:                 .getUniqueTableSegmentIdentifier(), wrapper, wrapper.getMemorySize());
/////////////////////////////////////////////////////////////////////////
1:    * @param tableSegmentUniqueIdentifiersWrapper
1:   @Override public void clearAccessCount(
1:       List<TableBlockIndexUniqueIdentifierWrapper> tableSegmentUniqueIdentifiersWrapper) {
1:     for (TableBlockIndexUniqueIdentifierWrapper
1:              identifierWrapper : tableSegmentUniqueIdentifiersWrapper) {
0:       BlockletDataMap cacheable = (BlockletDataMap) lruCache.get(
1:           identifierWrapper.getTableBlockIndexUniqueIdentifier().getUniqueTableSegmentIdentifier());
author:mohammadshahidkhan
-------------------------------------------------------------------------------
commit:5daae95
/////////////////////////////////////////////////////////////////////////
0:       if (FileFactory.isFileExist(blockPath)) {
0:         blockMetaInfoMap.put(blockPath, createBlockMetaInfo(blockPath));
1:       } else {
0:         LOGGER.warn("Skipping invalid block " + footer.getBlockInfo().getBlockUniqueName()
0:             + " The block does not exist. The block might be got deleted due to clean up post"
0:             + " update/delete operation over table.");
1:       }
commit:1155d4d
/////////////////////////////////////////////////////////////////////////
/////////////////////////////////////////////////////////////////////////
1:   public BlockletDataMapIndexStore(CarbonLRUCache lruCache) {
author:Jacky Li
-------------------------------------------------------------------------------
commit:fc2a7eb
/////////////////////////////////////////////////////////////////////////
0: import org.apache.carbondata.core.indexstore.blockletindex.BlockletDataMap;
/////////////////////////////////////////////////////////////////////////
0:     implements Cache<TableBlockIndexUniqueIdentifier, BlockletDataMap> {
/////////////////////////////////////////////////////////////////////////
0:   public BlockletDataMap get(TableBlockIndexUniqueIdentifier identifier)
0:     BlockletDataMap dataMap = (BlockletDataMap) lruCache.get(lruCacheKey);
/////////////////////////////////////////////////////////////////////////
0:   public List<BlockletDataMap> getAll(
0:     List<BlockletDataMap> blockletDataMaps =
0:         BlockletDataMap ifPresent = getIfPresent(identifier);
/////////////////////////////////////////////////////////////////////////
0:       for (BlockletDataMap dataMap : blockletDataMaps) {
/////////////////////////////////////////////////////////////////////////
0:   public BlockletDataMap getIfPresent(
0:     return (BlockletDataMap) lruCache.get(
/////////////////////////////////////////////////////////////////////////
0:   private BlockletDataMap loadAndGetDataMap(
/////////////////////////////////////////////////////////////////////////
0:     BlockletDataMap dataMap;
0:       dataMap = new BlockletDataMap();
/////////////////////////////////////////////////////////////////////////
0:       BlockletDataMap cacheable =
0:           (BlockletDataMap) lruCache.get(identifier.getUniqueTableSegmentIdentifier());
commit:89a12af
/////////////////////////////////////////////////////////////////////////
0: import org.apache.carbondata.core.indexstore.blockletindex.BlockletIndexDataMap;
/////////////////////////////////////////////////////////////////////////
0:     implements Cache<TableBlockIndexUniqueIdentifier, BlockletIndexDataMap> {
/////////////////////////////////////////////////////////////////////////
0:   public BlockletIndexDataMap get(TableBlockIndexUniqueIdentifier identifier)
0:     BlockletIndexDataMap dataMap = (BlockletIndexDataMap) lruCache.get(lruCacheKey);
/////////////////////////////////////////////////////////////////////////
0:   public List<BlockletIndexDataMap> getAll(
0:     List<BlockletIndexDataMap> blockletDataMaps =
1:         new ArrayList<>(tableSegmentUniqueIdentifiers.size());
0:         BlockletIndexDataMap ifPresent = getIfPresent(identifier);
/////////////////////////////////////////////////////////////////////////
0:       for (BlockletIndexDataMap dataMap : blockletDataMaps) {
/////////////////////////////////////////////////////////////////////////
0:   public BlockletIndexDataMap getIfPresent(
0:     return (BlockletIndexDataMap) lruCache.get(
/////////////////////////////////////////////////////////////////////////
0:   private BlockletIndexDataMap loadAndGetDataMap(
/////////////////////////////////////////////////////////////////////////
0:     BlockletIndexDataMap dataMap;
0:       dataMap = new BlockletIndexDataMap();
/////////////////////////////////////////////////////////////////////////
0:     for (TableBlockIndexUniqueIdentifier identifier : tableSegmentUniqueIdentifiers) {
0:       BlockletIndexDataMap cacheable =
0:           (BlockletIndexDataMap) lruCache.get(identifier.getUniqueTableSegmentIdentifier());
commit:f089287
/////////////////////////////////////////////////////////////////////////
1: import org.apache.carbondata.core.memory.MemoryException;
/////////////////////////////////////////////////////////////////////////
0:       } catch (MemoryException e) {
1:         LOGGER.error("memory exception when loading datamap: " + e.getMessage());
1:         throw new RuntimeException(e.getMessage(), e);
/////////////////////////////////////////////////////////////////////////
0:       e.printStackTrace();
/////////////////////////////////////////////////////////////////////////
0:       TableBlockIndexUniqueIdentifier tableSegmentUniqueIdentifier)
1:       throws IOException, MemoryException {
author:ravipesala
-------------------------------------------------------------------------------
commit:d35fbaf
/////////////////////////////////////////////////////////////////////////
/////////////////////////////////////////////////////////////////////////
/////////////////////////////////////////////////////////////////////////
commit:8d3c774
/////////////////////////////////////////////////////////////////////////
1: import org.apache.carbondata.core.constants.CarbonCommonConstants;
0: import org.apache.carbondata.core.datastore.filesystem.AbstractDFSCarbonFile;
0: import org.apache.carbondata.core.metadata.blocklet.DataFileFooter;
0: import org.apache.carbondata.core.util.DataFileFooterConverter;
1: 
0: import org.apache.hadoop.fs.LocatedFileStatus;
0: import org.apache.hadoop.fs.Path;
0: import org.apache.hadoop.fs.RemoteIterator;
/////////////////////////////////////////////////////////////////////////
0:         Map<String, BlockMetaInfo> blockMetaInfoMap =
0:             getBlockMetaInfoMap(identifier, indexFileStore);
0:         dataMap = loadAndGetDataMap(identifier, indexFileStore, blockMetaInfoMap);
/////////////////////////////////////////////////////////////////////////
0:   private Map<String, BlockMetaInfo> getBlockMetaInfoMap(TableBlockIndexUniqueIdentifier identifier,
0:       SegmentIndexFileStore indexFileStore) throws IOException {
0:     if (identifier.getMergeIndexFileName() != null) {
0:       CarbonFile indexMergeFile = FileFactory.getCarbonFile(
1:           identifier.getIndexFilePath() + CarbonCommonConstants.FILE_SEPARATOR + identifier
0:               .getMergeIndexFileName());
0:       if (indexMergeFile.exists()) {
0:         indexFileStore.readAllIIndexOfSegment(new CarbonFile[] { indexMergeFile });
1:       }
1:     }
0:     if (indexFileStore.getFileData(identifier.getIndexFileName()) == null) {
0:       indexFileStore.readAllIIndexOfSegment(new CarbonFile[] { FileFactory.getCarbonFile(
1:           identifier.getIndexFilePath() + CarbonCommonConstants.FILE_SEPARATOR + identifier
0:               .getIndexFileName()) });
1:     }
0:     DataFileFooterConverter fileFooterConverter = new DataFileFooterConverter();
0:     Map<String, BlockMetaInfo> blockMetaInfoMap = new HashMap<>();
0:     List<DataFileFooter> indexInfo = fileFooterConverter.getIndexInfo(
1:         identifier.getIndexFilePath() + CarbonCommonConstants.FILE_SEPARATOR + identifier
0:             .getIndexFileName(), indexFileStore.getFileData(identifier.getIndexFileName()));
0:     for (DataFileFooter footer : indexInfo) {
0:       String blockPath = footer.getBlockInfo().getTableBlockInfo().getFilePath();
0:       blockMetaInfoMap.put(blockPath, createBlockMetaInfo(blockPath));
1:     }
0:     return blockMetaInfoMap;
1:   }
1: 
0:   private BlockMetaInfo createBlockMetaInfo(String carbonDataFile) throws IOException {
0:     CarbonFile carbonFile = FileFactory.getCarbonFile(carbonDataFile);
0:     if (carbonFile instanceof AbstractDFSCarbonFile) {
0:       RemoteIterator<LocatedFileStatus> iter =
0:           ((AbstractDFSCarbonFile)carbonFile).fs.listLocatedStatus(new Path(carbonDataFile));
0:       LocatedFileStatus fileStatus = iter.next();
0:       String[] location = fileStatus.getBlockLocations()[0].getHosts();
0:       long len = fileStatus.getLen();
0:       return new BlockMetaInfo(location, len);
0:     } else {
0:       return new BlockMetaInfo(new String[]{"localhost"}, carbonFile.getSize());
1:     }
1:   }
1: 
/////////////////////////////////////////////////////////////////////////
0:         SegmentIndexFileStore indexFileStore = new SegmentIndexFileStore();
0:           Map<String, BlockMetaInfo> blockMetaInfoMap =
0:               getBlockMetaInfoMap(identifier, indexFileStore);
0:               loadAndGetDataMap(identifier, indexFileStore, blockMetaInfoMap));
/////////////////////////////////////////////////////////////////////////
/////////////////////////////////////////////////////////////////////////
0:       dataMap.init(new BlockletDataMapModel(
0:           identifier.getIndexFilePath() + CarbonCommonConstants.FILE_SEPARATOR + identifier
1:               .getIndexFileName(), indexFileStore.getFileData(identifier.getIndexFileName()),
0:           blockMetaInfoMap, identifier.getSegmentId()));
commit:44ffaf5
/////////////////////////////////////////////////////////////////////////
0:         Map<String, BlockMetaInfo> blockMetaInfoMap = new HashMap<>();
/////////////////////////////////////////////////////////////////////////
0:           blockMetaInfoMap
0:               .put(file.getAbsolutePath(), new BlockMetaInfo(file.getLocations(), file.getSize()));
0:         dataMap =
0:             loadAndGetDataMap(identifier, indexFileStore, partitionFileStore, blockMetaInfoMap);
/////////////////////////////////////////////////////////////////////////
0:         Map<String, BlockMetaInfo> blockMetaInfoMap = new HashMap<>();
/////////////////////////////////////////////////////////////////////////
0:               blockMetaInfoMap.put(file.getAbsolutePath(),
0:                   new BlockMetaInfo(file.getLocations(), file.getSize()));
0:               loadAndGetDataMap(identifier, indexFileStore, partitionFileStore, blockMetaInfoMap));
/////////////////////////////////////////////////////////////////////////
0:       Map<String, BlockMetaInfo> blockMetaInfoMap)
/////////////////////////////////////////////////////////////////////////
0:           partitionFileStore.isPartionedSegment(), blockMetaInfoMap));
commit:41b0074
/////////////////////////////////////////////////////////////////////////
0: import org.apache.carbondata.core.datastore.filesystem.CarbonFile;
0: import org.apache.carbondata.core.datastore.impl.FileFactory;
/////////////////////////////////////////////////////////////////////////
0:         Map<String, String[]> locationMap = new HashMap<>();
0:         CarbonFile carbonFile = FileFactory.getCarbonFile(segmentPath);
0:         CarbonFile[] carbonFiles = carbonFile.locationAwareListFiles();
0:         indexFileStore.readAllIIndexOfSegment(carbonFiles);
0:         partitionFileStore.readAllPartitionsOfSegment(carbonFiles, segmentPath);
0:         for (CarbonFile file : carbonFiles) {
0:           locationMap.put(file.getAbsolutePath(), file.getLocations());
1:         }
0:         dataMap = loadAndGetDataMap(identifier, indexFileStore, partitionFileStore, locationMap);
/////////////////////////////////////////////////////////////////////////
0:         Map<String, String[]> locationMap = new HashMap<>();
1: 
/////////////////////////////////////////////////////////////////////////
0:             CarbonFile carbonFile = FileFactory.getCarbonFile(segmentPath);
0:             CarbonFile[] carbonFiles = carbonFile.locationAwareListFiles();
0:             indexFileStore.readAllIIndexOfSegment(carbonFiles);
0:             partitionFileStore.readAllPartitionsOfSegment(carbonFiles, segmentPath);
0:             for (CarbonFile file : carbonFiles) {
0:               locationMap.put(file.getAbsolutePath(), file.getLocations());
1:             }
0:           blockletDataMaps.add(
0:               loadAndGetDataMap(identifier, indexFileStore, partitionFileStore, locationMap));
/////////////////////////////////////////////////////////////////////////
/////////////////////////////////////////////////////////////////////////
0:       PartitionMapFileStore partitionFileStore,
0:       Map<String, String[]> locationMap)
/////////////////////////////////////////////////////////////////////////
0:           partitionFileStore.isPartionedSegment(), locationMap));
commit:6094af6
/////////////////////////////////////////////////////////////////////////
0: import java.util.concurrent.Callable;
0: import java.util.concurrent.ExecutorService;
0: import java.util.concurrent.Executors;
0: import java.util.concurrent.Future;
/////////////////////////////////////////////////////////////////////////
0: import org.apache.carbondata.core.util.CarbonThreadFactory;
/////////////////////////////////////////////////////////////////////////
0:     ExecutorService service = null;
/////////////////////////////////////////////////////////////////////////
0:         service =
0:             Executors.newCachedThreadPool(
0:                 new CarbonThreadFactory("BlockletDataMapIndexStore:" + missedIdentifiers.get(0)
0:                     .getAbsoluteTableIdentifier().getTableName()));
0:         List<Future<BlockletDataMap>> futureList = new ArrayList<>();
/////////////////////////////////////////////////////////////////////////
0:           BlockletDataMapLoader blockletDataMapLoader =
0:               new BlockletDataMapLoader(identifier, indexFileStore, partitionFileStore);
0:           futureList.add(service.submit(blockletDataMapLoader));
1:         }
0:         for (Future<BlockletDataMap> dataMapFuture : futureList) {
0:           blockletDataMaps.add(dataMapFuture.get());
/////////////////////////////////////////////////////////////////////////
0:     } finally {
0:       if (service != null) {
0:         service.shutdownNow();
1:       }
/////////////////////////////////////////////////////////////////////////
0:    * This class is used to parallelize reading of index files.
1:    */
0:   private class BlockletDataMapLoader implements Callable<BlockletDataMap> {
1: 
0:     private TableBlockIndexUniqueIdentifier identifier;
0:     private SegmentIndexFileStore indexFileStore;
0:     private PartitionMapFileStore partitionFileStore;
1: 
0:     public BlockletDataMapLoader(TableBlockIndexUniqueIdentifier identifier,
0:         SegmentIndexFileStore indexFileStore, PartitionMapFileStore partitionFileStore) {
0:       this.identifier = identifier;
0:       this.indexFileStore = indexFileStore;
0:       this.partitionFileStore = partitionFileStore;
1:     }
1: 
0:     @Override public BlockletDataMap call() throws Exception {
0:       return loadAndGetDataMap(identifier, indexFileStore, partitionFileStore);
1:     }
1:   }
1: 
1:   /**
commit:3ff55a2
/////////////////////////////////////////////////////////////////////////
0:         String segmentPath = CarbonTablePath.getSegmentPath(
0:             identifier.getAbsoluteTableIdentifier().getTablePath(),
0:             identifier.getSegmentId());
/////////////////////////////////////////////////////////////////////////
0:           String segmentPath = CarbonTablePath.getSegmentPath(
0:               identifier.getAbsoluteTableIdentifier().getTablePath(),
0:               identifier.getSegmentId());
/////////////////////////////////////////////////////////////////////////
0:           partitionFileStore.getPartitions(identifier.getCarbonIndexFileName()),
0:           partitionFileStore.isPartionedSegment()));
commit:b8a02f3
/////////////////////////////////////////////////////////////////////////
0: import org.apache.carbondata.core.metadata.PartitionMapFileStore;
/////////////////////////////////////////////////////////////////////////
0:         String segmentPath = CarbonTablePath
0:             .getSegmentPath(identifier.getAbsoluteTableIdentifier().getTablePath(),
0:                 identifier.getSegmentId());
0:         indexFileStore.readAllIIndexOfSegment(segmentPath);
0:         PartitionMapFileStore partitionFileStore = new PartitionMapFileStore();
0:         partitionFileStore.readAllPartitionsOfSegment(segmentPath);
0:         dataMap = loadAndGetDataMap(identifier, indexFileStore, partitionFileStore);
/////////////////////////////////////////////////////////////////////////
0:         Map<String, PartitionMapFileStore> partitionFileStoreMap = new HashMap<>();
0:           PartitionMapFileStore partitionFileStore =
0:               partitionFileStoreMap.get(identifier.getSegmentId());
0:           String segmentPath = CarbonTablePath
0:               .getSegmentPath(identifier.getAbsoluteTableIdentifier().getTablePath(),
0:                   identifier.getSegmentId());
0:           if (partitionFileStore == null) {
0:             partitionFileStore = new PartitionMapFileStore();
0:             partitionFileStore.readAllPartitionsOfSegment(segmentPath);
0:             partitionFileStoreMap.put(identifier.getSegmentId(), partitionFileStore);
1:           }
0:           blockletDataMaps.add(loadAndGetDataMap(identifier, indexFileStore, partitionFileStore));
/////////////////////////////////////////////////////////////////////////
0:       SegmentIndexFileStore indexFileStore,
0:       PartitionMapFileStore partitionFileStore)
/////////////////////////////////////////////////////////////////////////
0:     BlockletDataMap dataMap;
0:           indexFileStore.getFileData(identifier.getCarbonIndexFileName()),
0:           partitionFileStore.getPartitions(identifier.getCarbonIndexFileName())));
commit:0586146
/////////////////////////////////////////////////////////////////////////
0: import java.util.HashMap;
/////////////////////////////////////////////////////////////////////////
1: import org.apache.carbondata.core.indexstore.blockletindex.BlockletDataMapModel;
1: import org.apache.carbondata.core.indexstore.blockletindex.SegmentIndexFileStore;
0: import org.apache.carbondata.core.util.path.CarbonTablePath;
/////////////////////////////////////////////////////////////////////////
1:   @Override
0:   public BlockletDataMap get(TableBlockIndexUniqueIdentifier identifier)
1:     String lruCacheKey = identifier.getUniqueTableSegmentIdentifier();
0:         SegmentIndexFileStore indexFileStore = new SegmentIndexFileStore();
0:         indexFileStore.readAllIIndexOfSegment(
0:             CarbonTablePath.getSegmentPath(
0:                 identifier.getAbsoluteTableIdentifier().getTablePath(),
0:                 identifier.getSegmentId()));
0:         dataMap = loadAndGetDataMap(identifier, indexFileStore);
/////////////////////////////////////////////////////////////////////////
0:   @Override
0:   public List<BlockletDataMap> getAll(
0:     List<TableBlockIndexUniqueIdentifier> missedIdentifiers = new ArrayList<>();
1:     // Get the datamaps for each indexfile from cache.
0:         BlockletDataMap ifPresent = getIfPresent(identifier);
0:         if (ifPresent != null) {
0:           blockletDataMaps.add(ifPresent);
0:         } else {
0:           missedIdentifiers.add(identifier);
0:         }
0:       }
0:       if (missedIdentifiers.size() > 0) {
0:         Map<String, SegmentIndexFileStore> segmentIndexFileStoreMap = new HashMap<>();
0:         for (TableBlockIndexUniqueIdentifier identifier: missedIdentifiers) {
0:           SegmentIndexFileStore indexFileStore =
0:               segmentIndexFileStoreMap.get(identifier.getSegmentId());
0:           if (indexFileStore == null) {
0:             String segmentPath = CarbonTablePath
0:                 .getSegmentPath(identifier.getAbsoluteTableIdentifier().getTablePath(),
0:                     identifier.getSegmentId());
0:             indexFileStore = new SegmentIndexFileStore();
0:             indexFileStore.readAllIIndexOfSegment(segmentPath);
0:             segmentIndexFileStoreMap.put(identifier.getSegmentId(), indexFileStore);
0:           }
0:           blockletDataMaps.add(loadAndGetDataMap(identifier, indexFileStore));
0:         }
/////////////////////////////////////////////////////////////////////////
0:   @Override
0:   public BlockletDataMap getIfPresent(
/////////////////////////////////////////////////////////////////////////
0:   @Override
0:   public void invalidate(TableBlockIndexUniqueIdentifier tableSegmentUniqueIdentifier) {
/////////////////////////////////////////////////////////////////////////
0:       TableBlockIndexUniqueIdentifier identifier,
0:       SegmentIndexFileStore indexFileStore)
1:         identifier.getUniqueTableSegmentIdentifier();
/////////////////////////////////////////////////////////////////////////
0:       dataMap.init(new BlockletDataMapModel(identifier.getFilePath(),
0:           indexFileStore.getFileData(identifier.getCarbonIndexFileName())));
0:       lruCache.put(identifier.getUniqueTableSegmentIdentifier(), dataMap,
/////////////////////////////////////////////////////////////////////////
0:   @Override
0:   public void clearAccessCount(
commit:b681244
/////////////////////////////////////////////////////////////////////////
1: /*
1:  * Licensed to the Apache Software Foundation (ASF) under one or more
1:  * contributor license agreements.  See the NOTICE file distributed with
1:  * this work for additional information regarding copyright ownership.
1:  * The ASF licenses this file to You under the Apache License, Version 2.0
1:  * (the "License"); you may not use this file except in compliance with
1:  * the License.  You may obtain a copy of the License at
1:  *
1:  *    http://www.apache.org/licenses/LICENSE-2.0
1:  *
1:  * Unless required by applicable law or agreed to in writing, software
1:  * distributed under the License is distributed on an "AS IS" BASIS,
1:  * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
1:  * See the License for the specific language governing permissions and
1:  * limitations under the License.
1:  */
1: package org.apache.carbondata.core.indexstore;
1: 
1: import java.io.IOException;
1: import java.util.ArrayList;
1: import java.util.List;
1: import java.util.Map;
1: import java.util.concurrent.ConcurrentHashMap;
1: 
1: import org.apache.carbondata.common.logging.LogService;
1: import org.apache.carbondata.common.logging.LogServiceFactory;
1: import org.apache.carbondata.core.cache.Cache;
1: import org.apache.carbondata.core.cache.CarbonLRUCache;
0: import org.apache.carbondata.core.datastore.exception.IndexBuilderException;
0: import org.apache.carbondata.core.indexstore.blockletindex.BlockletDataMap;
1: 
1: /**
1:  * Class to handle loading, unloading,clearing,storing of the table
1:  * blocks
1:  */
1: public class BlockletDataMapIndexStore
0:     implements Cache<TableBlockIndexUniqueIdentifier, BlockletDataMap> {
1:   private static final LogService LOGGER =
1:       LogServiceFactory.getLogService(BlockletDataMapIndexStore.class.getName());
1:   /**
0:    * carbon store path
1:    */
0:   protected String carbonStorePath;
1:   /**
1:    * CarbonLRU cache
1:    */
1:   protected CarbonLRUCache lruCache;
1: 
1:   /**
1:    * map of block info to lock object map, while loading the btree this will be filled
1:    * and removed after loading the tree for that particular block info, this will be useful
1:    * while loading the tree concurrently so only block level lock will be applied another
1:    * block can be loaded concurrently
1:    */
1:   private Map<String, Object> segmentLockMap;
1: 
1:   /**
1:    * constructor to initialize the SegmentTaskIndexStore
1:    *
0:    * @param carbonStorePath
1:    * @param lruCache
1:    */
0:   public BlockletDataMapIndexStore(String carbonStorePath, CarbonLRUCache lruCache) {
0:     this.carbonStorePath = carbonStorePath;
1:     this.lruCache = lruCache;
1:     segmentLockMap = new ConcurrentHashMap<String, Object>();
0:   }
1: 
0:   @Override public BlockletDataMap get(TableBlockIndexUniqueIdentifier tableSegmentUniqueIdentifier)
1:       throws IOException {
0:     String lruCacheKey = tableSegmentUniqueIdentifier.getUniqueTableSegmentIdentifier();
0:     BlockletDataMap dataMap = (BlockletDataMap) lruCache.get(lruCacheKey);
0:     if (dataMap == null) {
1:       try {
0:         dataMap = loadAndGetDataMap(tableSegmentUniqueIdentifier);
0:       } catch (IndexBuilderException e) {
0:         throw new IOException(e.getMessage(), e);
0:       } catch (Throwable e) {
0:         throw new IOException("Problem in loading segment block.", e);
0:       }
0:     }
1:     return dataMap;
0:   }
1: 
0:   @Override public List<BlockletDataMap> getAll(
0:       List<TableBlockIndexUniqueIdentifier> tableSegmentUniqueIdentifiers) throws IOException {
0:     List<BlockletDataMap> blockletDataMaps = new ArrayList<>(tableSegmentUniqueIdentifiers.size());
1:     try {
0:       for (TableBlockIndexUniqueIdentifier identifier : tableSegmentUniqueIdentifiers) {
0:         blockletDataMaps.add(get(identifier));
0:       }
0:     } catch (Throwable e) {
0:       for (BlockletDataMap dataMap : blockletDataMaps) {
0:         dataMap.clear();
0:       }
1:       throw new IOException("Problem in loading segment blocks.", e);
0:     }
0:     return blockletDataMaps;
0:   }
1: 
1:   /**
1:    * returns the SegmentTaskIndexWrapper
1:    *
0:    * @param tableSegmentUniqueIdentifier
1:    * @return
1:    */
0:   @Override public BlockletDataMap getIfPresent(
0:       TableBlockIndexUniqueIdentifier tableSegmentUniqueIdentifier) {
0:     BlockletDataMap dataMap = (BlockletDataMap) lruCache
0:         .get(tableSegmentUniqueIdentifier.getUniqueTableSegmentIdentifier());
1:     return dataMap;
0:   }
1: 
1:   /**
1:    * method invalidate the segment cache for segment
1:    *
0:    * @param tableSegmentUniqueIdentifier
1:    */
0:   @Override public void invalidate(TableBlockIndexUniqueIdentifier tableSegmentUniqueIdentifier) {
0:     lruCache.remove(tableSegmentUniqueIdentifier.getUniqueTableSegmentIdentifier());
0:   }
1: 
1:   /**
1:    * Below method will be used to load the segment of segments
1:    * One segment may have multiple task , so  table segment will be loaded
1:    * based on task id and will return the map of taksId to table segment
1:    * map
1:    *
1:    * @return map of taks id to segment mapping
1:    * @throws IOException
1:    */
0:   private BlockletDataMap loadAndGetDataMap(
0:       TableBlockIndexUniqueIdentifier tableSegmentUniqueIdentifier) throws IOException {
0:     String uniqueTableSegmentIdentifier =
0:         tableSegmentUniqueIdentifier.getUniqueTableSegmentIdentifier();
0:     Object lock = segmentLockMap.get(uniqueTableSegmentIdentifier);
0:     if (lock == null) {
0:       lock = addAndGetSegmentLock(uniqueTableSegmentIdentifier);
0:     }
0:     BlockletDataMap dataMap = null;
0:     synchronized (lock) {
0:       dataMap = new BlockletDataMap();
0:       dataMap.init(tableSegmentUniqueIdentifier.getFilePath());
0:       lruCache.put(tableSegmentUniqueIdentifier.getUniqueTableSegmentIdentifier(), dataMap,
0:           dataMap.getMemorySize());
0:     }
1:     return dataMap;
0:   }
1: 
1:   /**
1:    * Below method will be used to get the segment level lock object
1:    *
1:    * @param uniqueIdentifier
1:    * @return lock object
1:    */
1:   private synchronized Object addAndGetSegmentLock(String uniqueIdentifier) {
1:     // get the segment lock object if it is present then return
1:     // otherwise add the new lock and return
1:     Object segmentLoderLockObject = segmentLockMap.get(uniqueIdentifier);
1:     if (null == segmentLoderLockObject) {
1:       segmentLoderLockObject = new Object();
1:       segmentLockMap.put(uniqueIdentifier, segmentLoderLockObject);
0:     }
1:     return segmentLoderLockObject;
0:   }
1: 
1:   /**
1:    * The method clears the access count of table segments
1:    *
0:    * @param tableSegmentUniqueIdentifiers
1:    */
0:   @Override public void clearAccessCount(
0:       List<TableBlockIndexUniqueIdentifier> tableSegmentUniqueIdentifiers) {
0:     for (TableBlockIndexUniqueIdentifier segmentUniqueIdentifier : tableSegmentUniqueIdentifiers) {
0:       BlockletDataMap cacheable =
0:           (BlockletDataMap) lruCache.get(segmentUniqueIdentifier.getUniqueTableSegmentIdentifier());
1:       cacheable.clear();
0:     }
0:   }
0: }
author:kumarvishal
-------------------------------------------------------------------------------
commit:3a6136d
/////////////////////////////////////////////////////////////////////////
0:               blockMetaInfoMap.put(FileFactory.getUpdatedFilePath(file.getAbsolutePath()),
author:xuchuanyin
-------------------------------------------------------------------------------
commit:910d496
/////////////////////////////////////////////////////////////////////////
0:     return (BlockletDataMap) lruCache.get(
0:         tableSegmentUniqueIdentifier.getUniqueTableSegmentIdentifier());
============================================================================