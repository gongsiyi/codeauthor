1:72cb415: /*
1:72cb415:  * Licensed to the Apache Software Foundation (ASF) under one or more
1:72cb415:  * contributor license agreements.  See the NOTICE file distributed with
1:72cb415:  * this work for additional information regarding copyright ownership.
1:72cb415:  * The ASF licenses this file to You under the Apache License, Version 2.0
1:72cb415:  * (the "License"); you may not use this file except in compliance with
1:72cb415:  * the License.  You may obtain a copy of the License at
1:72cb415:  *
1:72cb415:  *    http://www.apache.org/licenses/LICENSE-2.0
1:72cb415:  *
1:72cb415:  * Unless required by applicable law or agreed to in writing, software
1:72cb415:  * distributed under the License is distributed on an "AS IS" BASIS,
1:72cb415:  * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
1:72cb415:  * See the License for the specific language governing permissions and
1:72cb415:  * limitations under the License.
1:72cb415:  */
1:72cb415: package org.apache.carbondata.core.datastore.chunk.impl;
1:72cb415: 
1:72cb415: import java.io.IOException;
1:72cb415: import java.nio.ByteBuffer;
1:3a4b881: import java.util.BitSet;
1:3a4b881: import java.util.List;
1:72cb415: 
1:3a4b881: import org.apache.carbondata.core.constants.CarbonCommonConstants;
1:daa6465: import org.apache.carbondata.core.datastore.FileReader;
1:72cb415: import org.apache.carbondata.core.datastore.chunk.AbstractRawColumnChunk;
1:daa6465: import org.apache.carbondata.core.datastore.chunk.DimensionColumnPage;
1:72cb415: import org.apache.carbondata.core.datastore.chunk.reader.DimensionColumnChunkReader;
1:8f08c4a: import org.apache.carbondata.core.datastore.compression.Compressor;
1:3a4b881: import org.apache.carbondata.core.datastore.compression.CompressorFactory;
1:3a4b881: import org.apache.carbondata.core.datastore.page.ColumnPage;
1:3a4b881: import org.apache.carbondata.core.datastore.page.encoding.ColumnPageDecoder;
1:3a4b881: import org.apache.carbondata.core.datastore.page.encoding.DefaultEncodingFactory;
1:e6a4f64: import org.apache.carbondata.core.memory.MemoryException;
1:3a4b881: import org.apache.carbondata.core.scan.result.vector.CarbonDictionary;
1:3a4b881: import org.apache.carbondata.core.scan.result.vector.impl.CarbonDictionaryImpl;
1:8f08c4a: import org.apache.carbondata.core.util.CarbonMetadataUtil;
1:3a4b881: import org.apache.carbondata.format.Encoding;
1:3a4b881: import org.apache.carbondata.format.LocalDictionaryChunk;
1:72cb415: 
1:72cb415: /**
1:72cb415:  * Contains raw dimension data,
1:72cb415:  * 1. The read uncompressed raw data of column chunk with all pages is stored in this instance.
1:daa6465:  * 2. The raw data can be converted to processed chunk using decodeColumnPage method
1:72cb415:  *  by specifying page number.
1:72cb415:  */
1:72cb415: public class DimensionRawColumnChunk extends AbstractRawColumnChunk {
1:72cb415: 
1:daa6465:   private DimensionColumnPage[] dataChunks;
1:72cb415: 
1:72cb415:   private DimensionColumnChunkReader chunkReader;
1:72cb415: 
1:daa6465:   private FileReader fileReader;
1:72cb415: 
1:3a4b881:   private CarbonDictionary localDictionary;
1:3a4b881: 
1:d509f17:   public DimensionRawColumnChunk(int columnIndex, ByteBuffer rawData, long offSet, int length,
1:72cb415:       DimensionColumnChunkReader columnChunkReader) {
1:8c1ddbf:     super(columnIndex, rawData, offSet, length);
1:72cb415:     this.chunkReader = columnChunkReader;
1:72cb415:   }
1:72cb415: 
1:d509f17:   /**
1:daa6465:    * Convert all raw data with all pages to processed DimensionColumnPage's
1:72cb415:    * @return
1:72cb415:    */
1:daa6465:   public DimensionColumnPage[] decodeAllColumnPages() {
1:72cb415:     if (dataChunks == null) {
1:daa6465:       dataChunks = new DimensionColumnPage[pagesCount];
1:72cb415:     }
1:72cb415:     for (int i = 0; i < pagesCount; i++) {
1:72cb415:       try {
1:72cb415:         if (dataChunks[i] == null) {
1:daa6465:           dataChunks[i] = chunkReader.decodeColumnPage(this, i);
1:72cb415:         }
1:e6a4f64:       } catch (IOException | MemoryException e) {
1:72cb415:         throw new RuntimeException(e);
1:72cb415:       }
1:72cb415:     }
1:72cb415:     return dataChunks;
1:72cb415:   }
1:72cb415: 
1:72cb415:   /**
1:daa6465:    * Convert raw data with specified page number processed to DimensionColumnPage
1:daa6465:    * @param pageNumber
1:72cb415:    * @return
1:72cb415:    */
1:daa6465:   public DimensionColumnPage decodeColumnPage(int pageNumber) {
1:daa6465:     assert pageNumber < pagesCount;
1:72cb415:     if (dataChunks == null) {
1:daa6465:       dataChunks = new DimensionColumnPage[pagesCount];
1:72cb415:     }
1:daa6465:     if (dataChunks[pageNumber] == null) {
1:72cb415:       try {
1:daa6465:         dataChunks[pageNumber] = chunkReader.decodeColumnPage(this, pageNumber);
1:e6a4f64:       } catch (IOException | MemoryException e) {
1:72cb415:         throw new RuntimeException(e);
1:72cb415:       }
1:72cb415:     }
1:72cb415: 
1:daa6465:     return dataChunks[pageNumber];
1:72cb415:   }
1:72cb415: 
1:72cb415:   /**
1:d509f17:    * Convert raw data with specified page number processed to DimensionColumnDataChunk
1:d509f17:    *
1:d509f17:    * @param index
1:d509f17:    * @return
1:d509f17:    */
1:daa6465:   public DimensionColumnPage convertToDimColDataChunkWithOutCache(int index) {
1:d509f17:     assert index < pagesCount;
1:ceac8ab:     // in case of filter query filter column if filter column is decoded and stored.
1:ceac8ab:     // then return the same
1:ceac8ab:     if (dataChunks != null && null != dataChunks[index]) {
1:ceac8ab:       return dataChunks[index];
1:ceac8ab:     }
1:d509f17:     try {
1:daa6465:       return chunkReader.decodeColumnPage(this, index);
1:d509f17:     } catch (Exception e) {
1:d509f17:       throw new RuntimeException(e);
1:d509f17:     }
1:d509f17:   }
1:d509f17: 
1:72cb415:   @Override public void freeMemory() {
1:4c9bed8:     super.freeMemory();
1:72cb415:     if (null != dataChunks) {
1:72cb415:       for (int i = 0; i < dataChunks.length; i++) {
1:72cb415:         if (dataChunks[i] != null) {
1:72cb415:           dataChunks[i].freeMemory();
1:4c9bed8:           dataChunks[i] = null;
1:72cb415:         }
1:72cb415:       }
1:72cb415:     }
1:ceac8ab:     rawData = null;
1:72cb415:   }
1:72cb415: 
1:daa6465:   public void setFileReader(FileReader fileReader) {
1:daa6465:     this.fileReader = fileReader;
1:72cb415:   }
1:72cb415: 
1:daa6465:   public FileReader getFileReader() {
1:daa6465:     return fileReader;
1:72cb415:   }
1:3a4b881: 
1:3a4b881:   public CarbonDictionary getLocalDictionary() {
1:4a37e05:     if (null != getDataChunkV3() && null != getDataChunkV3().local_dictionary
1:4a37e05:         && null == localDictionary) {
1:3a4b881:       try {
1:8f08c4a:         String compressorName = CarbonMetadataUtil.getCompressorNameFromChunkMeta(
1:8f08c4a:             getDataChunkV3().data_chunk_list.get(0).chunk_meta);
1:8f08c4a: 
1:8f08c4a:         Compressor compressor = CompressorFactory.getInstance().getCompressor(compressorName);
1:8f08c4a:         localDictionary = getDictionary(getDataChunkV3().local_dictionary, compressor);
1:3a4b881:       } catch (IOException | MemoryException e) {
1:3a4b881:         throw new RuntimeException(e);
1:3a4b881:       }
1:3a4b881:     }
1:3a4b881:     return localDictionary;
1:3a4b881:   }
1:3a4b881: 
1:3a4b881:   /**
1:3a4b881:    * Below method will be used to get the local dictionary for a blocklet
1:3a4b881:    * @param localDictionaryChunk
1:3a4b881:    * local dictionary chunk thrift object
1:3a4b881:    * @return local dictionary
1:3a4b881:    * @throws IOException
1:3a4b881:    * @throws MemoryException
1:3a4b881:    */
1:8f08c4a:   private CarbonDictionary getDictionary(LocalDictionaryChunk localDictionaryChunk,
1:8f08c4a:       Compressor compressor) throws IOException, MemoryException {
1:3a4b881:     if (null != localDictionaryChunk) {
1:3a4b881:       List<Encoding> encodings = localDictionaryChunk.getDictionary_meta().getEncoders();
1:3a4b881:       List<ByteBuffer> encoderMetas = localDictionaryChunk.getDictionary_meta().getEncoder_meta();
1:8f08c4a:       ColumnPageDecoder decoder = DefaultEncodingFactory.getInstance().createDecoder(
1:8f08c4a:           encodings, encoderMetas, compressor.getName());
1:3a4b881:       ColumnPage decode = decoder.decode(localDictionaryChunk.getDictionary_data(), 0,
1:3a4b881:           localDictionaryChunk.getDictionary_data().length);
1:8f08c4a:       BitSet usedDictionary = BitSet.valueOf(compressor.unCompressByte(
1:8f08c4a:           localDictionaryChunk.getDictionary_values()));
1:3a4b881:       int length = usedDictionary.length();
1:3a4b881:       int index = 0;
1:3a4b881:       byte[][] dictionary = new byte[length][];
1:3a4b881:       for (int i = 0; i < length; i++) {
1:3a4b881:         if (usedDictionary.get(i)) {
1:3a4b881:           dictionary[i] = decode.getBytes(index++);
1:3a4b881:         } else {
1:3a4b881:           dictionary[i] = null;
1:3a4b881:         }
1:3a4b881:       }
1:3a4b881:       decode.freeMemory();
1:3a4b881:       // as dictionary values starts from 1 setting null default value
1:3a4b881:       dictionary[1] = CarbonCommonConstants.MEMBER_DEFAULT_VAL_ARRAY;
1:3a4b881:       return new CarbonDictionaryImpl(dictionary, usedDictionary.cardinality());
1:3a4b881:     }
1:3a4b881:     return null;
1:3a4b881:   }
1:3a4b881: 
1:72cb415: }
============================================================================
author:xuchuanyin
-------------------------------------------------------------------------------
commit:8f08c4a
/////////////////////////////////////////////////////////////////////////
1: import org.apache.carbondata.core.datastore.compression.Compressor;
/////////////////////////////////////////////////////////////////////////
1: import org.apache.carbondata.core.util.CarbonMetadataUtil;
/////////////////////////////////////////////////////////////////////////
1:         String compressorName = CarbonMetadataUtil.getCompressorNameFromChunkMeta(
1:             getDataChunkV3().data_chunk_list.get(0).chunk_meta);
1: 
1:         Compressor compressor = CompressorFactory.getInstance().getCompressor(compressorName);
1:         localDictionary = getDictionary(getDataChunkV3().local_dictionary, compressor);
/////////////////////////////////////////////////////////////////////////
1:   private CarbonDictionary getDictionary(LocalDictionaryChunk localDictionaryChunk,
1:       Compressor compressor) throws IOException, MemoryException {
1:       ColumnPageDecoder decoder = DefaultEncodingFactory.getInstance().createDecoder(
1:           encodings, encoderMetas, compressor.getName());
1:       BitSet usedDictionary = BitSet.valueOf(compressor.unCompressByte(
1:           localDictionaryChunk.getDictionary_values()));
author:kumarvishal09
-------------------------------------------------------------------------------
commit:4a37e05
/////////////////////////////////////////////////////////////////////////
1:     if (null != getDataChunkV3() && null != getDataChunkV3().local_dictionary
1:         && null == localDictionary) {
commit:3a4b881
/////////////////////////////////////////////////////////////////////////
1: import java.util.BitSet;
1: import java.util.List;
1: import org.apache.carbondata.core.constants.CarbonCommonConstants;
1: import org.apache.carbondata.core.datastore.compression.CompressorFactory;
1: import org.apache.carbondata.core.datastore.page.ColumnPage;
1: import org.apache.carbondata.core.datastore.page.encoding.ColumnPageDecoder;
1: import org.apache.carbondata.core.datastore.page.encoding.DefaultEncodingFactory;
1: import org.apache.carbondata.core.scan.result.vector.CarbonDictionary;
1: import org.apache.carbondata.core.scan.result.vector.impl.CarbonDictionaryImpl;
1: import org.apache.carbondata.format.Encoding;
1: import org.apache.carbondata.format.LocalDictionaryChunk;
/////////////////////////////////////////////////////////////////////////
1:   private CarbonDictionary localDictionary;
1: 
/////////////////////////////////////////////////////////////////////////
1: 
1:   public CarbonDictionary getLocalDictionary() {
0:     if (null != getDataChunkV3().local_dictionary && null == localDictionary) {
1:       try {
0:         localDictionary = getDictionary(getDataChunkV3().local_dictionary);
1:       } catch (IOException | MemoryException e) {
1:         throw new RuntimeException(e);
1:       }
1:     }
1:     return localDictionary;
1:   }
1: 
1:   /**
1:    * Below method will be used to get the local dictionary for a blocklet
1:    * @param localDictionaryChunk
1:    * local dictionary chunk thrift object
1:    * @return local dictionary
1:    * @throws IOException
1:    * @throws MemoryException
1:    */
0:   private CarbonDictionary getDictionary(LocalDictionaryChunk localDictionaryChunk)
0:       throws IOException, MemoryException {
1:     if (null != localDictionaryChunk) {
1:       List<Encoding> encodings = localDictionaryChunk.getDictionary_meta().getEncoders();
1:       List<ByteBuffer> encoderMetas = localDictionaryChunk.getDictionary_meta().getEncoder_meta();
0:       ColumnPageDecoder decoder =
0:           DefaultEncodingFactory.getInstance().createDecoder(encodings, encoderMetas);
1:       ColumnPage decode = decoder.decode(localDictionaryChunk.getDictionary_data(), 0,
1:           localDictionaryChunk.getDictionary_data().length);
0:       BitSet usedDictionary = BitSet.valueOf(CompressorFactory.getInstance().getCompressor()
0:           .unCompressByte(localDictionaryChunk.getDictionary_values()));
1:       int length = usedDictionary.length();
1:       int index = 0;
1:       byte[][] dictionary = new byte[length][];
1:       for (int i = 0; i < length; i++) {
1:         if (usedDictionary.get(i)) {
1:           dictionary[i] = decode.getBytes(index++);
1:         } else {
1:           dictionary[i] = null;
1:         }
1:       }
1:       decode.freeMemory();
1:       // as dictionary values starts from 1 setting null default value
1:       dictionary[1] = CarbonCommonConstants.MEMBER_DEFAULT_VAL_ARRAY;
1:       return new CarbonDictionaryImpl(dictionary, usedDictionary.cardinality());
1:     }
1:     return null;
1:   }
1: 
author:kumarvishal
-------------------------------------------------------------------------------
commit:ceac8ab
/////////////////////////////////////////////////////////////////////////
1:     // in case of filter query filter column if filter column is decoded and stored.
1:     // then return the same
1:     if (dataChunks != null && null != dataChunks[index]) {
1:       return dataChunks[index];
1:     }
/////////////////////////////////////////////////////////////////////////
1:     rawData = null;
author:Jin Zhou
-------------------------------------------------------------------------------
commit:4c9bed8
/////////////////////////////////////////////////////////////////////////
1:     super.freeMemory();
1:           dataChunks[i] = null;
author:Jacky Li
-------------------------------------------------------------------------------
commit:daa6465
/////////////////////////////////////////////////////////////////////////
1: import org.apache.carbondata.core.datastore.FileReader;
1: import org.apache.carbondata.core.datastore.chunk.DimensionColumnPage;
1:  * 2. The raw data can be converted to processed chunk using decodeColumnPage method
1:   private DimensionColumnPage[] dataChunks;
1:   private FileReader fileReader;
/////////////////////////////////////////////////////////////////////////
1:    * Convert all raw data with all pages to processed DimensionColumnPage's
1:   public DimensionColumnPage[] decodeAllColumnPages() {
1:       dataChunks = new DimensionColumnPage[pagesCount];
1:           dataChunks[i] = chunkReader.decodeColumnPage(this, i);
/////////////////////////////////////////////////////////////////////////
1:    * Convert raw data with specified page number processed to DimensionColumnPage
1:    * @param pageNumber
1:   public DimensionColumnPage decodeColumnPage(int pageNumber) {
1:     assert pageNumber < pagesCount;
1:       dataChunks = new DimensionColumnPage[pagesCount];
1:     if (dataChunks[pageNumber] == null) {
1:         dataChunks[pageNumber] = chunkReader.decodeColumnPage(this, pageNumber);
1:     return dataChunks[pageNumber];
/////////////////////////////////////////////////////////////////////////
1:   public DimensionColumnPage convertToDimColDataChunkWithOutCache(int index) {
1:       return chunkReader.decodeColumnPage(this, index);
/////////////////////////////////////////////////////////////////////////
1:   public void setFileReader(FileReader fileReader) {
1:     this.fileReader = fileReader;
1:   public FileReader getFileReader() {
1:     return fileReader;
commit:8c1ddbf
/////////////////////////////////////////////////////////////////////////
0:   public DimensionRawColumnChunk(int columnIndex, ByteBuffer rawData, int offSet, int length,
1:     super(columnIndex, rawData, offSet, length);
commit:e6a4f64
/////////////////////////////////////////////////////////////////////////
1: import org.apache.carbondata.core.memory.MemoryException;
/////////////////////////////////////////////////////////////////////////
1:       } catch (IOException | MemoryException e) {
/////////////////////////////////////////////////////////////////////////
1:       } catch (IOException | MemoryException e) {
author:ravipesala
-------------------------------------------------------------------------------
commit:d509f17
/////////////////////////////////////////////////////////////////////////
1:   public DimensionRawColumnChunk(int columnIndex, ByteBuffer rawData, long offSet, int length,
/////////////////////////////////////////////////////////////////////////
1:   /**
1:    * Convert raw data with specified page number processed to DimensionColumnDataChunk
1:    *
1:    * @param index
1:    * @return
1:    */
0:   public DimensionColumnDataChunk convertToDimColDataChunkWithOutCache(int index) {
1:     assert index < pagesCount;
1:     try {
0:       return chunkReader.convertToDimensionChunk(this, index);
1:     } catch (Exception e) {
1:       throw new RuntimeException(e);
1:     }
1:   }
1: 
commit:72cb415
/////////////////////////////////////////////////////////////////////////
1: /*
1:  * Licensed to the Apache Software Foundation (ASF) under one or more
1:  * contributor license agreements.  See the NOTICE file distributed with
1:  * this work for additional information regarding copyright ownership.
1:  * The ASF licenses this file to You under the Apache License, Version 2.0
1:  * (the "License"); you may not use this file except in compliance with
1:  * the License.  You may obtain a copy of the License at
1:  *
1:  *    http://www.apache.org/licenses/LICENSE-2.0
1:  *
1:  * Unless required by applicable law or agreed to in writing, software
1:  * distributed under the License is distributed on an "AS IS" BASIS,
1:  * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
1:  * See the License for the specific language governing permissions and
1:  * limitations under the License.
1:  */
1: package org.apache.carbondata.core.datastore.chunk.impl;
1: 
1: import java.io.IOException;
1: import java.nio.ByteBuffer;
1: 
0: import org.apache.carbondata.core.datastore.FileHolder;
1: import org.apache.carbondata.core.datastore.chunk.AbstractRawColumnChunk;
0: import org.apache.carbondata.core.datastore.chunk.DimensionColumnDataChunk;
1: import org.apache.carbondata.core.datastore.chunk.reader.DimensionColumnChunkReader;
1: 
1: /**
1:  * Contains raw dimension data,
1:  * 1. The read uncompressed raw data of column chunk with all pages is stored in this instance.
0:  * 2. The raw data can be converted to processed chunk using convertToDimColDataChunk method
1:  *  by specifying page number.
1:  */
1: public class DimensionRawColumnChunk extends AbstractRawColumnChunk {
1: 
0:   private DimensionColumnDataChunk[] dataChunks;
1: 
1:   private DimensionColumnChunkReader chunkReader;
1: 
0:   private FileHolder fileHolder;
1: 
0:   public DimensionRawColumnChunk(int blockletId, ByteBuffer rawData, int offSet, int length,
1:       DimensionColumnChunkReader columnChunkReader) {
0:     super(blockletId, rawData, offSet, length);
1:     this.chunkReader = columnChunkReader;
1:   }
1: 
1:   /**
0:    * Convert all raw data with all pages to processed DimensionColumnDataChunk's
1:    * @return
1:    */
0:   public DimensionColumnDataChunk[] convertToDimColDataChunks() {
1:     if (dataChunks == null) {
0:       dataChunks = new DimensionColumnDataChunk[pagesCount];
1:     }
1:     for (int i = 0; i < pagesCount; i++) {
1:       try {
1:         if (dataChunks[i] == null) {
0:           dataChunks[i] = chunkReader.convertToDimensionChunk(this, i);
1:         }
0:       } catch (Exception e) {
1:         throw new RuntimeException(e);
1:       }
1:     }
1:     return dataChunks;
1:   }
1: 
1:   /**
0:    * Convert raw data with specified page number processed to DimensionColumnDataChunk
0:    * @param index
1:    * @return
1:    */
0:   public DimensionColumnDataChunk convertToDimColDataChunk(int index) {
0:     assert index < pagesCount;
1:     if (dataChunks == null) {
0:       dataChunks = new DimensionColumnDataChunk[pagesCount];
1:     }
0:     if (dataChunks[index] == null) {
1:       try {
0:         dataChunks[index] = chunkReader.convertToDimensionChunk(this, index);
0:       } catch (IOException e) {
1:         throw new RuntimeException(e);
1:       }
1:     }
1: 
0:     return dataChunks[index];
1:   }
1: 
1:   @Override public void freeMemory() {
1:     if (null != dataChunks) {
1:       for (int i = 0; i < dataChunks.length; i++) {
1:         if (dataChunks[i] != null) {
1:           dataChunks[i].freeMemory();
1:         }
1:       }
1:     }
1:   }
1: 
0:   public void setFileHolder(FileHolder fileHolder) {
0:     this.fileHolder = fileHolder;
1:   }
1: 
0:   public FileHolder getFileReader() {
0:     return fileHolder;
1:   }
1: }
============================================================================