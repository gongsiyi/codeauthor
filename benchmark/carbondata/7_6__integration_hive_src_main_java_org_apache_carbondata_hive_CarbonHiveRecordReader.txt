1:a700f83: /*
1:a700f83:  * Licensed to the Apache Software Foundation (ASF) under one or more
1:a700f83:  * contributor license agreements.  See the NOTICE file distributed with
1:a700f83:  * this work for additional information regarding copyright ownership.
1:a700f83:  * The ASF licenses this file to You under the Apache License, Version 2.0
1:a700f83:  * (the "License"); you may not use this file except in compliance with
1:a700f83:  * the License.  You may obtain a copy of the License at
1:a700f83:  *
1:a700f83:  *    http://www.apache.org/licenses/LICENSE-2.0
1:a700f83:  *
1:a700f83:  * Unless required by applicable law or agreed to in writing, software
1:a700f83:  * distributed under the License is distributed on an "AS IS" BASIS,
1:a700f83:  * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
1:a700f83:  * See the License for the specific language governing permissions and
1:a700f83:  * limitations under the License.
1:a700f83:  */
1:a700f83: package org.apache.carbondata.hive;
7:a700f83: 
1:a700f83: import java.io.IOException;
1:a700f83: import java.sql.Date;
1:a700f83: import java.sql.Timestamp;
1:a700f83: import java.util.ArrayList;
1:a700f83: import java.util.Iterator;
1:a700f83: import java.util.List;
1:a700f83: 
1:a700f83: import org.apache.carbondata.core.datastore.block.TableBlockInfo;
1:a700f83: import org.apache.carbondata.core.scan.executor.exception.QueryExecutionException;
1:a700f83: import org.apache.carbondata.core.scan.model.QueryModel;
1:a700f83: import org.apache.carbondata.core.scan.result.iterator.ChunkRowIterator;
1:a700f83: import org.apache.carbondata.hadoop.CarbonRecordReader;
1:a700f83: import org.apache.carbondata.hadoop.readsupport.CarbonReadSupport;
1:a700f83: 
1:a700f83: import org.apache.hadoop.conf.Configuration;
1:a700f83: import org.apache.hadoop.hive.common.type.HiveDecimal;
1:a700f83: import org.apache.hadoop.hive.serde.serdeConstants;
1:a700f83: import org.apache.hadoop.hive.serde2.SerDeException;
1:a700f83: import org.apache.hadoop.hive.serde2.io.DateWritable;
1:a700f83: import org.apache.hadoop.hive.serde2.io.DoubleWritable;
1:a700f83: import org.apache.hadoop.hive.serde2.io.HiveDecimalWritable;
1:a700f83: import org.apache.hadoop.hive.serde2.io.ShortWritable;
1:a700f83: import org.apache.hadoop.hive.serde2.io.TimestampWritable;
1:eb0405d: import org.apache.hadoop.hive.serde2.objectinspector.ListObjectInspector;
1:eb0405d: import org.apache.hadoop.hive.serde2.objectinspector.ObjectInspector;
1:eb0405d: import org.apache.hadoop.hive.serde2.objectinspector.PrimitiveObjectInspector;
1:eb0405d: import org.apache.hadoop.hive.serde2.objectinspector.StructField;
1:eb0405d: import org.apache.hadoop.hive.serde2.objectinspector.StructObjectInspector;
1:a700f83: import org.apache.hadoop.hive.serde2.typeinfo.TypeInfo;
1:a700f83: import org.apache.hadoop.hive.serde2.typeinfo.TypeInfoUtils;
1:a700f83: import org.apache.hadoop.io.ArrayWritable;
1:a700f83: import org.apache.hadoop.io.IntWritable;
1:a700f83: import org.apache.hadoop.io.LongWritable;
1:a700f83: import org.apache.hadoop.io.Text;
1:a700f83: import org.apache.hadoop.io.Writable;
1:a700f83: import org.apache.hadoop.mapred.InputSplit;
1:a700f83: import org.apache.hadoop.mapred.JobConf;
1:a700f83: 
1:d408a8d: class CarbonHiveRecordReader extends CarbonRecordReader<ArrayWritable>
1:a700f83:     implements org.apache.hadoop.mapred.RecordReader<Void, ArrayWritable> {
1:a700f83: 
1:d408a8d:   private ArrayWritable valueObj = null;
1:cbe1419:   private long recordReaderCounter = 0;
1:cbe1419:   private int[] columnIds;
1:a700f83: 
1:a700f83:   public CarbonHiveRecordReader(QueryModel queryModel, CarbonReadSupport<ArrayWritable> readSupport,
1:eb0405d:       InputSplit inputSplit, JobConf jobConf) throws IOException {
1:2a9604c:     super(queryModel, readSupport, jobConf);
1:a700f83:     initialize(inputSplit, jobConf);
7:a700f83:   }
1:a700f83: 
1:d408a8d:   private void initialize(InputSplit inputSplit, Configuration conf) throws IOException {
1:a700f83:     // The input split can contain single HDFS block or multiple blocks, so firstly get all the
1:a700f83:     // blocks and then set them in the query model.
1:a700f83:     List<CarbonHiveInputSplit> splitList;
1:a700f83:     if (inputSplit instanceof CarbonHiveInputSplit) {
1:a700f83:       splitList = new ArrayList<>(1);
1:a700f83:       splitList.add((CarbonHiveInputSplit) inputSplit);
5:a700f83:     } else {
1:a700f83:       throw new RuntimeException("unsupported input split type: " + inputSplit);
1:a700f83:     }
1:a700f83:     List<TableBlockInfo> tableBlockInfoList = CarbonHiveInputSplit.createBlocks(splitList);
1:a700f83:     queryModel.setTableBlockInfos(tableBlockInfoList);
1:eb0405d:     readSupport
1:29dc302:         .initialize(queryModel.getProjectionColumns(), queryModel.getTable());
2:a700f83:     try {
1:a700f83:       carbonIterator = new ChunkRowIterator(queryExecutor.execute(queryModel));
1:a700f83:     } catch (QueryExecutionException e) {
1:a700f83:       throw new IOException(e.getMessage(), e.getCause());
1:a700f83:     }
1:a700f83:     final TypeInfo rowTypeInfo;
1:a700f83:     final List<String> columnNames;
1:a700f83:     List<TypeInfo> columnTypes;
1:a700f83:     // Get column names and sort order
1:25a8ac6:     final String colIds = conf.get("hive.io.file.readcolumn.ids");
1:a700f83:     final String columnTypeProperty = conf.get(serdeConstants.LIST_COLUMN_TYPES);
1:25a8ac6: 
1:a700f83:     if (columnTypeProperty.length() == 0) {
1:a700f83:       columnTypes = new ArrayList<TypeInfo>();
1:a700f83:     } else {
1:a700f83:       columnTypes = TypeInfoUtils.getTypeInfosFromTypeString(columnTypeProperty);
1:a700f83:     }
1:cbe1419: 
1:cbe1419:     if (valueObj == null) {
1:cbe1419:       valueObj = new ArrayWritable(Writable.class, new Writable[columnTypes.size()]);
1:cbe1419:     }
1:cbe1419: 
1:0da86b6:     if (!colIds.equals("")) {
1:0da86b6:       String[] arraySelectedColId = colIds.split(",");
1:cbe1419:       columnIds = new int[arraySelectedColId.length];
1:cbe1419:       int columnId = 0;
1:cbe1419:       for (int j = 0; j < arraySelectedColId.length; j++) {
1:cbe1419:         columnId = Integer.parseInt(arraySelectedColId[j]);
1:cbe1419:         columnIds[j] = columnId;
1:0da86b6:       }
1:25a8ac6:     }
1:cbe1419: 
1:a700f83:   }
1:25a8ac6: 
1:eb0405d:   @Override public boolean next(Void aVoid, ArrayWritable value) throws IOException {
1:a700f83:     if (carbonIterator.hasNext()) {
1:a700f83:       Object obj = readSupport.readRow(carbonIterator.next());
1:cbe1419:       recordReaderCounter++;
1:cbe1419:       Writable[] objArray = (Writable[]) obj;
1:cbe1419:       Writable[] sysArray = new Writable[value.get().length];
1:cbe1419:       if (columnIds != null && columnIds.length > 0 && objArray.length == columnIds.length) {
1:cbe1419:         for (int i = 0; i < columnIds.length; i++) {
1:cbe1419:           sysArray[columnIds[i]] = objArray[i];
1:a700f83:         }
1:cbe1419:         value.set(sysArray);
1:cbe1419:       } else {
1:cbe1419:         value.set(objArray);
1:a700f83:       }
1:a700f83:       return true;
1:0da86b6:     } else {
1:a700f83:       return false;
1:a700f83:     }
1:a700f83:   }
1:a700f83: 
1:eb0405d:   @Override public Void createKey() {
1:a700f83:     return null;
1:a700f83:   }
1:a700f83: 
1:eb0405d:   @Override public ArrayWritable createValue() {
1:a700f83:     return valueObj;
1:a700f83:   }
1:a700f83: 
1:eb0405d:   @Override public long getPos() throws IOException {
1:cbe1419:     return recordReaderCounter;
1:a700f83:   }
1:a700f83: 
1:eb0405d:   @Override public float getProgress() throws IOException {
2:a700f83:     return 0;
1:a700f83:   }
1:a700f83: 
1:d408a8d:   private ArrayWritable createStruct(Object obj, StructObjectInspector inspector)
2:a700f83:       throws SerDeException {
1:a700f83:     List fields = inspector.getAllStructFieldRefs();
1:a700f83:     Writable[] arr = new Writable[fields.size()];
1:a700f83:     for (int i = 0; i < fields.size(); i++) {
1:a700f83:       StructField field = (StructField) fields.get(i);
1:a700f83:       Object subObj = inspector.getStructFieldData(obj, field);
1:a700f83:       ObjectInspector subInspector = field.getFieldObjectInspector();
1:a700f83:       arr[i] = createObject(subObj, subInspector);
1:a700f83:     }
1:a700f83:     return new ArrayWritable(Writable.class, arr);
1:a700f83:   }
1:a700f83: 
1:a700f83:   private ArrayWritable createArray(Object obj, ListObjectInspector inspector)
1:eb0405d:       throws SerDeException {
1:a700f83:     List sourceArray = inspector.getList(obj);
1:a700f83:     ObjectInspector subInspector = inspector.getListElementObjectInspector();
1:a700f83:     List array = new ArrayList();
1:a700f83:     Iterator iterator;
1:a700f83:     if (sourceArray != null) {
1:a700f83:       for (iterator = sourceArray.iterator(); iterator.hasNext(); ) {
1:a700f83:         Object curObj = iterator.next();
1:a700f83:         Writable newObj = createObject(curObj, subInspector);
1:a700f83:         if (newObj != null) {
1:a700f83:           array.add(newObj);
1:a700f83:         }
1:a700f83:       }
1:a700f83:     }
1:a700f83:     if (array.size() > 0) {
1:a700f83:       ArrayWritable subArray = new ArrayWritable(((Writable) array.get(0)).getClass(),
1:a700f83:           (Writable[]) array.toArray(new Writable[array.size()]));
1:a700f83: 
1:eb0405d:       return new ArrayWritable(Writable.class, new Writable[] { subArray });
1:a700f83:     }
1:a700f83:     return null;
1:a700f83:   }
1:a700f83: 
1:a700f83:   private Writable createPrimitive(Object obj, PrimitiveObjectInspector inspector)
1:a700f83:       throws SerDeException {
1:a700f83:     if (obj == null) {
1:a700f83:       return null;
1:a700f83:     }
1:a700f83:     switch (inspector.getPrimitiveCategory()) {
1:a700f83:       case VOID:
1:a700f83:         return null;
1:a700f83:       case DOUBLE:
1:a700f83:         return new DoubleWritable((double) obj);
1:a700f83:       case INT:
1:eb0405d:         return new IntWritable((int) obj);
1:a700f83:       case LONG:
1:a700f83:         return new LongWritable((long) obj);
1:a700f83:       case SHORT:
1:0da86b6:         return new ShortWritable((short) obj);
1:a700f83:       case DATE:
1:b4f65b2:         return new DateWritable(new Date(Long.parseLong(String.valueOf(obj.toString()))));
1:a700f83:       case TIMESTAMP:
1:a700f83:         return new TimestampWritable(new Timestamp((long) obj));
1:a700f83:       case STRING:
1:a700f83:         return new Text(obj.toString());
1:5ff2c50:       case CHAR:
1:5ff2c50:         return new Text(obj.toString());
1:a700f83:       case DECIMAL:
1:eb0405d:         return new HiveDecimalWritable(
1:09f7cdd:             HiveDecimal.create((java.math.BigDecimal) obj));
1:a700f83:     }
1:a700f83:     throw new SerDeException("Unknown primitive : " + inspector.getPrimitiveCategory());
1:a700f83:   }
1:a700f83: 
1:a700f83:   private Writable createObject(Object obj, ObjectInspector inspector) throws SerDeException {
1:a700f83:     switch (inspector.getCategory()) {
1:a700f83:       case STRUCT:
1:a700f83:         return createStruct(obj, (StructObjectInspector) inspector);
1:a700f83:       case LIST:
1:a700f83:         return createArray(obj, (ListObjectInspector) inspector);
1:a700f83:       case PRIMITIVE:
1:a700f83:         return createPrimitive(obj, (PrimitiveObjectInspector) inspector);
1:a700f83:     }
1:a700f83:     throw new SerDeException("Unknown data type" + inspector.getCategory());
1:0da86b6:   }
1:a700f83: }
============================================================================
author:kunal642
-------------------------------------------------------------------------------
commit:2a9604c
/////////////////////////////////////////////////////////////////////////
1:     super(queryModel, readSupport, jobConf);
author:manishgupta88
-------------------------------------------------------------------------------
commit:29dc302
/////////////////////////////////////////////////////////////////////////
1:         .initialize(queryModel.getProjectionColumns(), queryModel.getTable());
author:Raghunandan S
-------------------------------------------------------------------------------
commit:2d24e18
/////////////////////////////////////////////////////////////////////////
/////////////////////////////////////////////////////////////////////////
/////////////////////////////////////////////////////////////////////////
/////////////////////////////////////////////////////////////////////////
/////////////////////////////////////////////////////////////////////////
/////////////////////////////////////////////////////////////////////////
author:chenliang613
-------------------------------------------------------------------------------
commit:09f7cdd
/////////////////////////////////////////////////////////////////////////
1:             HiveDecimal.create((java.math.BigDecimal) obj));
commit:d408a8d
/////////////////////////////////////////////////////////////////////////
1: class CarbonHiveRecordReader extends CarbonRecordReader<ArrayWritable>
1:   private ArrayWritable valueObj = null;
/////////////////////////////////////////////////////////////////////////
1:   private void initialize(InputSplit inputSplit, Configuration conf) throws IOException {
/////////////////////////////////////////////////////////////////////////
0:       ArrayWritable tmpValue;
/////////////////////////////////////////////////////////////////////////
0:   private ArrayWritable createArrayWritable(Object obj) throws SerDeException {
/////////////////////////////////////////////////////////////////////////
1:   private ArrayWritable createStruct(Object obj, StructObjectInspector inspector)
commit:53267c8
/////////////////////////////////////////////////////////////////////////
0:     readSupport.initialize(queryModel.getProjectionColumns(),
0:                            queryModel.getAbsoluteTableIdentifier());
0:       valueObj = new ArrayWritable(Writable.class,
0:                                    new Writable[queryModel.getProjectionColumns().length]);
author:Bhavya
-------------------------------------------------------------------------------
commit:cbe1419
/////////////////////////////////////////////////////////////////////////
1:   private long recordReaderCounter = 0;
1:   private int[] columnIds;
/////////////////////////////////////////////////////////////////////////
0:     final String columnNameProperty = conf.get(serdeConstants.LIST_COLUMNS);
/////////////////////////////////////////////////////////////////////////
1: 
1:     if (valueObj == null) {
1:       valueObj = new ArrayWritable(Writable.class, new Writable[columnTypes.size()]);
1:     }
1: 
1:       columnIds = new int[arraySelectedColId.length];
1:       int columnId = 0;
1:       for (int j = 0; j < arraySelectedColId.length; j++) {
1:         columnId = Integer.parseInt(arraySelectedColId[j]);
1:         columnIds[j] = columnId;
1: 
0:     rowTypeInfo = TypeInfoFactory.getStructTypeInfo(columnNames, columnTypes);
0:     this.objInspector = new CarbonObjectInspector((StructTypeInfo) rowTypeInfo);
1:       recordReaderCounter++;
1:       Writable[] objArray = (Writable[]) obj;
1:       Writable[] sysArray = new Writable[value.get().length];
1:       if (columnIds != null && columnIds.length > 0 && objArray.length == columnIds.length) {
1:         for (int i = 0; i < columnIds.length; i++) {
1:           sysArray[columnIds[i]] = objArray[i];
1:         value.set(sysArray);
1:       } else {
1:         value.set(objArray);
/////////////////////////////////////////////////////////////////////////
/////////////////////////////////////////////////////////////////////////
1:     return recordReaderCounter;
author:anubhav100
-------------------------------------------------------------------------------
commit:0da86b6
/////////////////////////////////////////////////////////////////////////
1:     if (!colIds.equals("")) {
1:       String[] arraySelectedColId = colIds.split(",");
0:       List<TypeInfo> reqColTypes = new ArrayList<TypeInfo>();
0:       for (String anArrayColId : arraySelectedColId) {
0:         reqColTypes.add(columnTypes.get(Integer.parseInt(anArrayColId)));
1:       }
0:       // Create row related objects
0:       rowTypeInfo = TypeInfoFactory.getStructTypeInfo(columnNames, reqColTypes);
0:       this.objInspector = new CarbonObjectInspector((StructTypeInfo) rowTypeInfo);
1:     } else {
0:       rowTypeInfo = TypeInfoFactory.getStructTypeInfo(columnNames, columnTypes);
0:       this.objInspector = new CarbonObjectInspector((StructTypeInfo) rowTypeInfo);
/////////////////////////////////////////////////////////////////////////
1:         return new ShortWritable((short) obj);
/////////////////////////////////////////////////////////////////////////
1: }
commit:5ff2c50
/////////////////////////////////////////////////////////////////////////
1:       case CHAR:
1:         return new Text(obj.toString());
commit:eb0405d
/////////////////////////////////////////////////////////////////////////
/////////////////////////////////////////////////////////////////////////
1: import org.apache.hadoop.hive.serde2.objectinspector.ListObjectInspector;
1: import org.apache.hadoop.hive.serde2.objectinspector.ObjectInspector;
1: import org.apache.hadoop.hive.serde2.objectinspector.PrimitiveObjectInspector;
1: import org.apache.hadoop.hive.serde2.objectinspector.StructField;
1: import org.apache.hadoop.hive.serde2.objectinspector.StructObjectInspector;
/////////////////////////////////////////////////////////////////////////
1:       InputSplit inputSplit, JobConf jobConf) throws IOException {
/////////////////////////////////////////////////////////////////////////
1:     readSupport
0:         .initialize(queryModel.getProjectionColumns(), queryModel.getAbsoluteTableIdentifier());
0:       valueObj =
0:           new ArrayWritable(Writable.class, new Writable[queryModel.getProjectionColumns().length]);
/////////////////////////////////////////////////////////////////////////
1:   @Override public boolean next(Void aVoid, ArrayWritable value) throws IOException {
/////////////////////////////////////////////////////////////////////////
0:             throw new IOException(
0:                 "CarbonHiveInput : size of object differs. Value" + " size :  " + arrValue.length
0:                     + ", Current Object size : " + arrCurrent.length);
0:             throw new IOException("CarbonHiveInput can not support RecordReaders that"
0:                 + " don't return same key & value & value is null");
/////////////////////////////////////////////////////////////////////////
1:   @Override public Void createKey() {
1:   @Override public ArrayWritable createValue() {
1:   @Override public long getPos() throws IOException {
1:   @Override public float getProgress() throws IOException {
/////////////////////////////////////////////////////////////////////////
1:       throws SerDeException {
/////////////////////////////////////////////////////////////////////////
1:       return new ArrayWritable(Writable.class, new Writable[] { subArray });
/////////////////////////////////////////////////////////////////////////
1:         return new IntWritable((int) obj);
0:         return new ShortWritable((Short) obj);
/////////////////////////////////////////////////////////////////////////
1:         return new HiveDecimalWritable(
0:             HiveDecimal.create(((org.apache.spark.sql.types.Decimal) obj).toJavaBigDecimal()));
commit:25a8ac6
/////////////////////////////////////////////////////////////////////////
0:     readSupport.initialize(queryModel.getProjectionColumns(), queryModel.getAbsoluteTableIdentifier());
0:       valueObj = new ArrayWritable(Writable.class, new Writable[queryModel.getProjectionColumns().length]);
1:     final String colIds = conf.get("hive.io.file.readcolumn.ids");
/////////////////////////////////////////////////////////////////////////
1: 
0:     String[] arraySelectedColId = colIds.split(",");
0:     List<TypeInfo> reqColTypes = new ArrayList<TypeInfo>();
1: 
0:     for (String anArrayColId : arraySelectedColId) {
0:       reqColTypes.add(columnTypes.get(Integer.parseInt(anArrayColId)));
1:     }
0:     rowTypeInfo = TypeInfoFactory.getStructTypeInfo(columnNames, reqColTypes);
commit:b4f65b2
/////////////////////////////////////////////////////////////////////////
1:         return new DateWritable(new Date(Long.parseLong(String.valueOf(obj.toString()))));
author:cenyuhai
-------------------------------------------------------------------------------
commit:a700f83
/////////////////////////////////////////////////////////////////////////
1: /*
1:  * Licensed to the Apache Software Foundation (ASF) under one or more
1:  * contributor license agreements.  See the NOTICE file distributed with
1:  * this work for additional information regarding copyright ownership.
1:  * The ASF licenses this file to You under the Apache License, Version 2.0
1:  * (the "License"); you may not use this file except in compliance with
1:  * the License.  You may obtain a copy of the License at
1:  *
1:  *    http://www.apache.org/licenses/LICENSE-2.0
1:  *
1:  * Unless required by applicable law or agreed to in writing, software
1:  * distributed under the License is distributed on an "AS IS" BASIS,
1:  * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
1:  * See the License for the specific language governing permissions and
1:  * limitations under the License.
1:  */
1: package org.apache.carbondata.hive;
1: 
1: 
1: import java.io.IOException;
1: import java.sql.Date;
1: import java.sql.Timestamp;
1: import java.util.ArrayList;
0: import java.util.Arrays;
1: import java.util.Iterator;
1: import java.util.List;
1: 
1: import org.apache.carbondata.core.datastore.block.TableBlockInfo;
1: import org.apache.carbondata.core.scan.executor.exception.QueryExecutionException;
1: import org.apache.carbondata.core.scan.model.QueryModel;
1: import org.apache.carbondata.core.scan.result.iterator.ChunkRowIterator;
1: import org.apache.carbondata.hadoop.CarbonRecordReader;
1: import org.apache.carbondata.hadoop.readsupport.CarbonReadSupport;
1: 
1: import org.apache.hadoop.conf.Configuration;
1: import org.apache.hadoop.hive.common.type.HiveDecimal;
1: import org.apache.hadoop.hive.serde.serdeConstants;
1: import org.apache.hadoop.hive.serde2.SerDeException;
1: import org.apache.hadoop.hive.serde2.io.DateWritable;
1: import org.apache.hadoop.hive.serde2.io.DoubleWritable;
1: import org.apache.hadoop.hive.serde2.io.HiveDecimalWritable;
1: import org.apache.hadoop.hive.serde2.io.ShortWritable;
1: import org.apache.hadoop.hive.serde2.io.TimestampWritable;
0: import org.apache.hadoop.hive.serde2.objectinspector.*;
0: import org.apache.hadoop.hive.serde2.typeinfo.StructTypeInfo;
1: import org.apache.hadoop.hive.serde2.typeinfo.TypeInfo;
0: import org.apache.hadoop.hive.serde2.typeinfo.TypeInfoFactory;
1: import org.apache.hadoop.hive.serde2.typeinfo.TypeInfoUtils;
1: import org.apache.hadoop.io.ArrayWritable;
1: import org.apache.hadoop.io.IntWritable;
1: import org.apache.hadoop.io.LongWritable;
1: import org.apache.hadoop.io.Text;
1: import org.apache.hadoop.io.Writable;
1: import org.apache.hadoop.mapred.InputSplit;
1: import org.apache.hadoop.mapred.JobConf;
1: 
0: public class CarbonHiveRecordReader extends CarbonRecordReader<ArrayWritable>
1:     implements org.apache.hadoop.mapred.RecordReader<Void, ArrayWritable> {
1: 
0:   ArrayWritable valueObj = null;
0:   private CarbonObjectInspector objInspector;
1: 
1:   public CarbonHiveRecordReader(QueryModel queryModel, CarbonReadSupport<ArrayWritable> readSupport,
0:                                 InputSplit inputSplit, JobConf jobConf) throws IOException {
0:     super(queryModel, readSupport);
1:     initialize(inputSplit, jobConf);
1:   }
1: 
0:   public void initialize(InputSplit inputSplit, Configuration conf) throws IOException {
1:     // The input split can contain single HDFS block or multiple blocks, so firstly get all the
1:     // blocks and then set them in the query model.
1:     List<CarbonHiveInputSplit> splitList;
1:     if (inputSplit instanceof CarbonHiveInputSplit) {
1:       splitList = new ArrayList<>(1);
1:       splitList.add((CarbonHiveInputSplit) inputSplit);
1:     } else {
1:       throw new RuntimeException("unsupported input split type: " + inputSplit);
1:     }
1:     List<TableBlockInfo> tableBlockInfoList = CarbonHiveInputSplit.createBlocks(splitList);
1:     queryModel.setTableBlockInfos(tableBlockInfoList);
0:     readSupport.initialize(queryModel.getProjectionColumns(),
0:         queryModel.getAbsoluteTableIdentifier());
1:     try {
1:       carbonIterator = new ChunkRowIterator(queryExecutor.execute(queryModel));
1:     } catch (QueryExecutionException e) {
1:       throw new IOException(e.getMessage(), e.getCause());
1:     }
0:     if (valueObj == null) {
0:       valueObj = new ArrayWritable(Writable.class,
0:           new Writable[queryModel.getProjectionColumns().length]);
1:     }
1: 
1:     final TypeInfo rowTypeInfo;
1:     final List<String> columnNames;
1:     List<TypeInfo> columnTypes;
1:     // Get column names and sort order
0:     final String columnNameProperty = conf.get("hive.io.file.readcolumn.names");
1:     final String columnTypeProperty = conf.get(serdeConstants.LIST_COLUMN_TYPES);
1: 
0:     if (columnNameProperty.length() == 0) {
0:       columnNames = new ArrayList<String>();
1:     } else {
0:       columnNames = Arrays.asList(columnNameProperty.split(","));
1:     }
1:     if (columnTypeProperty.length() == 0) {
1:       columnTypes = new ArrayList<TypeInfo>();
1:     } else {
1:       columnTypes = TypeInfoUtils.getTypeInfosFromTypeString(columnTypeProperty);
1:     }
0:     columnTypes = columnTypes.subList(0, columnNames.size());
0:     // Create row related objects
0:     rowTypeInfo = TypeInfoFactory.getStructTypeInfo(columnNames, columnTypes);
0:     this.objInspector = new CarbonObjectInspector((StructTypeInfo) rowTypeInfo);
1:   }
1: 
0:   @Override
0:   public boolean next(Void aVoid, ArrayWritable value) throws IOException {
1:     if (carbonIterator.hasNext()) {
1:       Object obj = readSupport.readRow(carbonIterator.next());
0:       ArrayWritable tmpValue = null;
1:       try {
0:         tmpValue = createArrayWritable(obj);
0:       } catch (SerDeException se) {
0:         throw new IOException(se.getMessage(), se.getCause());
1:       }
1: 
0:       if (value != tmpValue) {
0:         final Writable[] arrValue = value.get();
0:         final Writable[] arrCurrent = tmpValue.get();
0:         if (valueObj != null && arrValue.length == arrCurrent.length) {
0:           System.arraycopy(arrCurrent, 0, arrValue, 0, arrCurrent.length);
1:         } else {
0:           if (arrValue.length != arrCurrent.length) {
0:             throw new IOException("CarbonHiveInput : size of object differs. Value" +
0:               " size :  " + arrValue.length + ", Current Object size : " + arrCurrent.length);
1:           } else {
0:             throw new IOException("CarbonHiveInput can not support RecordReaders that" +
0:               " don't return same key & value & value is null");
1:           }
1:         }
1:       }
1:       return true;
1:     } else {
1:       return false;
1:     }
1:   }
1: 
0:   public ArrayWritable createArrayWritable(Object obj) throws SerDeException {
0:     return createStruct(obj, objInspector);
1:   }
1: 
0:   @Override
0:   public Void createKey() {
1:     return null;
1:   }
1: 
0:   @Override
0:   public ArrayWritable createValue() {
1:     return valueObj;
1:   }
1: 
0:   @Override
0:   public long getPos() throws IOException {
1:     return 0;
1:   }
1: 
0:   @Override
0:   public float getProgress() throws IOException {
1:     return 0;
1:   }
1: 
0:   public ArrayWritable createStruct(Object obj, StructObjectInspector inspector)
1:       throws SerDeException {
1:     List fields = inspector.getAllStructFieldRefs();
1:     Writable[] arr = new Writable[fields.size()];
1:     for (int i = 0; i < fields.size(); i++) {
1:       StructField field = (StructField) fields.get(i);
1:       Object subObj = inspector.getStructFieldData(obj, field);
1:       ObjectInspector subInspector = field.getFieldObjectInspector();
1:       arr[i] = createObject(subObj, subInspector);
1:     }
1:     return new ArrayWritable(Writable.class, arr);
1:   }
1: 
1:   private ArrayWritable createArray(Object obj, ListObjectInspector inspector)
1:     throws SerDeException {
1:     List sourceArray = inspector.getList(obj);
1:     ObjectInspector subInspector = inspector.getListElementObjectInspector();
1:     List array = new ArrayList();
1:     Iterator iterator;
1:     if (sourceArray != null) {
1:       for (iterator = sourceArray.iterator(); iterator.hasNext(); ) {
1:         Object curObj = iterator.next();
1:         Writable newObj = createObject(curObj, subInspector);
1:         if (newObj != null) {
1:           array.add(newObj);
1:         }
1:       }
1:     }
1:     if (array.size() > 0) {
1:       ArrayWritable subArray = new ArrayWritable(((Writable) array.get(0)).getClass(),
1:           (Writable[]) array.toArray(new Writable[array.size()]));
1: 
0:       return new ArrayWritable(Writable.class, new Writable[]{subArray});
1:     }
1:     return null;
1:   }
1: 
1:   private Writable createPrimitive(Object obj, PrimitiveObjectInspector inspector)
1:       throws SerDeException {
1:     if (obj == null) {
1:       return null;
1:     }
1:     switch (inspector.getPrimitiveCategory()) {
1:       case VOID:
1:         return null;
1:       case DOUBLE:
1:         return new DoubleWritable((double) obj);
1:       case INT:
0:         return new IntWritable(((Long) obj).intValue());
1:       case LONG:
1:         return new LongWritable((long) obj);
1:       case SHORT:
0:         return new ShortWritable(((Long) obj).shortValue());
1:       case DATE:
0:         return new DateWritable(new Date(((long) obj)));
1:       case TIMESTAMP:
1:         return new TimestampWritable(new Timestamp((long) obj));
1:       case STRING:
1:         return new Text(obj.toString());
1:       case DECIMAL:
0:         return new HiveDecimalWritable(HiveDecimal.create(
0:           ((org.apache.spark.sql.types.Decimal) obj).toJavaBigDecimal()));
1:     }
1:     throw new SerDeException("Unknown primitive : " + inspector.getPrimitiveCategory());
1:   }
1: 
1:   private Writable createObject(Object obj, ObjectInspector inspector) throws SerDeException {
1:     switch (inspector.getCategory()) {
1:       case STRUCT:
1:         return createStruct(obj, (StructObjectInspector) inspector);
1:       case LIST:
1:         return createArray(obj, (ListObjectInspector) inspector);
1:       case PRIMITIVE:
1:         return createPrimitive(obj, (PrimitiveObjectInspector) inspector);
1:     }
1:     throw new SerDeException("Unknown data type" + inspector.getCategory());
1:   }
1: }
============================================================================