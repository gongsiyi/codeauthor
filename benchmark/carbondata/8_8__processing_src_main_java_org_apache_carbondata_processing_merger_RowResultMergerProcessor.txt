1:cc59b24: /*
1:cc59b24:  * Licensed to the Apache Software Foundation (ASF) under one or more
1:cc59b24:  * contributor license agreements.  See the NOTICE file distributed with
1:cc59b24:  * this work for additional information regarding copyright ownership.
1:cc59b24:  * The ASF licenses this file to You under the Apache License, Version 2.0
1:cc59b24:  * (the "License"); you may not use this file except in compliance with
1:cc59b24:  * the License.  You may obtain a copy of the License at
1:cc59b24:  *
1:cc59b24:  *    http://www.apache.org/licenses/LICENSE-2.0
1:cc59b24:  *
1:cc59b24:  * Unless required by applicable law or agreed to in writing, software
1:cc59b24:  * distributed under the License is distributed on an "AS IS" BASIS,
1:cc59b24:  * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
1:cc59b24:  * See the License for the specific language governing permissions and
1:cc59b24:  * limitations under the License.
1:cc59b24:  */
1:cc59b24: package org.apache.carbondata.processing.merger;
1:cc59b24: 
1:5ed39de: import java.io.IOException;
1:cc59b24: import java.util.AbstractQueue;
1:cc59b24: import java.util.Comparator;
1:cc59b24: import java.util.List;
1:cc59b24: import java.util.PriorityQueue;
1:cc59b24: 
1:cc59b24: import org.apache.carbondata.common.logging.LogService;
1:cc59b24: import org.apache.carbondata.common.logging.LogServiceFactory;
1:8d3c774: import org.apache.carbondata.core.constants.CarbonCommonConstants;
1:cc59b24: import org.apache.carbondata.core.datastore.block.SegmentProperties;
1:dc83b2a: import org.apache.carbondata.core.datastore.exception.CarbonDataWriterException;
1:dc83b2a: import org.apache.carbondata.core.datastore.row.CarbonRow;
1:dc83b2a: import org.apache.carbondata.core.datastore.row.WriteStepRowUtil;
1:8d3c774: import org.apache.carbondata.core.indexstore.PartitionSpec;
1:cc59b24: import org.apache.carbondata.core.keygenerator.KeyGenException;
1:cc59b24: import org.apache.carbondata.core.metadata.CarbonMetadata;
1:8d3c774: import org.apache.carbondata.core.metadata.SegmentFileStore;
1:cc59b24: import org.apache.carbondata.core.metadata.schema.table.CarbonTable;
1:cc59b24: import org.apache.carbondata.core.scan.result.iterator.RawResultIterator;
1:cc59b24: import org.apache.carbondata.core.scan.wrappers.ByteArrayWrapper;
1:cc59b24: import org.apache.carbondata.core.util.ByteUtil;
1:349c59c: import org.apache.carbondata.processing.exception.SliceMergerException;
1:349c59c: import org.apache.carbondata.processing.loading.model.CarbonLoadModel;
1:cc59b24: import org.apache.carbondata.processing.store.CarbonFactDataHandlerColumnar;
1:cc59b24: import org.apache.carbondata.processing.store.CarbonFactDataHandlerModel;
1:cc59b24: import org.apache.carbondata.processing.store.CarbonFactHandler;
1:ded8b41: import org.apache.carbondata.processing.util.CarbonDataProcessorUtil;
1:cc59b24: 
1:cc59b24: /**
1:cc59b24:  * This is the Merger class responsible for the merging of the segments.
1:cc59b24:  */
1:cc59b24: public class RowResultMergerProcessor extends AbstractResultProcessor {
1:cc59b24: 
1:cc59b24:   private CarbonFactHandler dataHandler;
1:cc59b24:   private SegmentProperties segprop;
1:5ed39de:   private CarbonLoadModel loadModel;
1:8d3c774:   private PartitionSpec partitionSpec;
1:cc59b24:   /**
1:cc59b24:    * record holder heap
1:cc59b24:    */
1:cc59b24:   private AbstractQueue<RawResultIterator> recordHolderHeap;
1:cc59b24: 
1:cc59b24:   private static final LogService LOGGER =
1:cc59b24:       LogServiceFactory.getLogService(RowResultMergerProcessor.class.getName());
1:cc59b24: 
1:cc59b24:   public RowResultMergerProcessor(String databaseName,
1:ded8b41:       String tableName, SegmentProperties segProp, String[] tempStoreLocation,
1:60dfdd3:       CarbonLoadModel loadModel, CompactionType compactionType, PartitionSpec partitionSpec)
1:60dfdd3:       throws IOException {
1:cc59b24:     this.segprop = segProp;
1:8d3c774:     this.partitionSpec = partitionSpec;
1:5ed39de:     this.loadModel = loadModel;
1:ded8b41:     CarbonDataProcessorUtil.createLocations(tempStoreLocation);
1:cc59b24: 
1:7e124f4:     CarbonTable carbonTable = CarbonMetadata.getInstance().getCarbonTable(databaseName, tableName);
1:8d3c774:     String carbonStoreLocation;
1:8d3c774:     if (partitionSpec != null) {
1:8d3c774:       carbonStoreLocation =
1:8d3c774:           partitionSpec.getLocation().toString() + CarbonCommonConstants.FILE_SEPARATOR + loadModel
1:8d3c774:               .getFactTimeStamp() + ".tmp";
1:8d3c774:     } else {
1:8d8b589:       carbonStoreLocation = CarbonDataProcessorUtil.createCarbonStoreLocation(
1:8d8b589:           loadModel.getDatabaseName(), tableName, loadModel.getSegmentId());
1:8d3c774:     }
1:cc59b24:     CarbonFactDataHandlerModel carbonFactDataHandlerModel = CarbonFactDataHandlerModel
1:cc59b24:         .getCarbonFactDataHandlerModel(loadModel, carbonTable, segProp, tableName,
1:8d3c774:             tempStoreLocation, carbonStoreLocation);
1:bf6c471:     setDataFileAttributesInModel(loadModel, compactionType, carbonFactDataHandlerModel);
1:cc59b24:     carbonFactDataHandlerModel.setCompactionFlow(true);
1:60dfdd3:     carbonFactDataHandlerModel.setSegmentId(loadModel.getSegmentId());
1:cc59b24:     dataHandler = new CarbonFactDataHandlerColumnar(carbonFactDataHandlerModel);
1:cc59b24:   }
1:cc59b24: 
1:cc59b24:   private void initRecordHolderHeap(List<RawResultIterator> rawResultIteratorList) {
1:cc59b24:     // create the List of RawResultIterator.
1:cc59b24:     recordHolderHeap = new PriorityQueue<RawResultIterator>(rawResultIteratorList.size(),
1:cc59b24:         new RowResultMergerProcessor.CarbonMdkeyComparator());
1:cc59b24:   }
1:cc59b24: 
1:cc59b24:   /**
1:cc59b24:    * Merge function
1:cc59b24:    *
1:cc59b24:    */
1:f1d8464:   public boolean execute(List<RawResultIterator> resultIteratorList) throws Exception {
1:cc59b24:     initRecordHolderHeap(resultIteratorList);
1:cc59b24:     boolean mergeStatus = false;
1:cc59b24:     int index = 0;
1:cc59b24:     boolean isDataPresent = false;
1:cc59b24:     try {
1:cc59b24: 
1:cc59b24:       // add all iterators to the queue
1:cc59b24:       for (RawResultIterator leaftTupleIterator : resultIteratorList) {
1:cc59b24:         this.recordHolderHeap.add(leaftTupleIterator);
1:cc59b24:         index++;
1:cc59b24:       }
1:cc59b24:       RawResultIterator iterator = null;
1:cc59b24:       while (index > 1) {
1:cc59b24:         // iterator the top record
1:cc59b24:         iterator = this.recordHolderHeap.poll();
2:cc59b24:         Object[] convertedRow = iterator.next();
2:cc59b24:         if (null == convertedRow) {
1:cc59b24:           index--;
1:88279ca:           iterator.close();
1:cc59b24:           continue;
1:cc59b24:         }
2:cc59b24:         if (!isDataPresent) {
2:cc59b24:           dataHandler.initialise();
2:cc59b24:           isDataPresent = true;
1:cc59b24:         }
1:cc59b24:         // get the mdkey
2:cc59b24:         addRow(convertedRow);
1:cc59b24:         // if there is no record in the leaf and all then decrement the
1:cc59b24:         // index
2:cc59b24:         if (!iterator.hasNext()) {
1:cc59b24:           index--;
1:88279ca:           iterator.close();
1:cc59b24:           continue;
1:cc59b24:         }
1:cc59b24:         // add record to heap
1:cc59b24:         this.recordHolderHeap.add(iterator);
1:cc59b24:       }
1:cc59b24:       // if record holder is not empty then iterator the slice holder from
1:cc59b24:       // heap
1:cc59b24:       iterator = this.recordHolderHeap.poll();
1:7ef9164:       if (null != iterator) {
1:7ef9164:         while (true) {
1:7ef9164:           Object[] convertedRow = iterator.next();
1:7ef9164:           if (null == convertedRow) {
1:7ef9164:             iterator.close();
1:7ef9164:             break;
1:7ef9164:           }
1:7ef9164:           // do it only once
1:7ef9164:           if (!isDataPresent) {
1:7ef9164:             dataHandler.initialise();
1:7ef9164:             isDataPresent = true;
1:7ef9164:           }
1:7ef9164:           addRow(convertedRow);
1:7ef9164:           // check if leaf contains no record
1:7ef9164:           if (!iterator.hasNext()) {
1:7ef9164:             break;
1:7ef9164:           }
1:cc59b24:         }
1:cc59b24:       }
1:cc59b24:       if (isDataPresent)
1:cc59b24:       {
1:cc59b24:         this.dataHandler.finish();
1:cc59b24:       }
1:cc59b24:       mergeStatus = true;
1:cc59b24:     } catch (Exception e) {
1:cc59b24:       mergeStatus = false;
1:f1d8464:       throw e;
1:cc59b24:     } finally {
1:cc59b24:       try {
1:cc59b24:         if (isDataPresent) {
1:cc59b24:           this.dataHandler.closeHandler();
1:5ed39de:         }
1:8d3c774:         if (partitionSpec != null) {
1:8d3c774:           SegmentFileStore.writeSegmentFile(loadModel.getTablePath(), loadModel.getTaskNo(),
1:8d3c774:               partitionSpec.getLocation().toString(), loadModel.getFactTimeStamp() + "",
1:8d3c774:               partitionSpec.getPartitions());
1:cc59b24:         }
1:5ed39de:       } catch (CarbonDataWriterException | IOException e) {
1:cc59b24:         mergeStatus = false;
1:f1d8464:         throw e;
1:cc59b24:       }
1:cc59b24:     }
1:cc59b24: 
1:cc59b24:     return mergeStatus;
1:cc59b24:   }
1:cc59b24: 
1:7978b97:   @Override
1:7978b97:   public void close() {
1:7978b97:     // close data handler
1:7978b97:     if (null != dataHandler) {
1:7978b97:       dataHandler.closeHandler();
1:7978b97:     }
1:7978b97:   }
1:7978b97: 
1:cc59b24:   /**
1:cc59b24:    * Below method will be used to add sorted row
1:cc59b24:    *
1:cc59b24:    * @throws SliceMergerException
1:cc59b24:    */
1:cc59b24:   private void addRow(Object[] carbonTuple) throws SliceMergerException {
1:353272e:     CarbonRow row = WriteStepRowUtil.fromMergerRow(carbonTuple, segprop);
1:cc59b24:     try {
1:353272e:       this.dataHandler.addDataToStore(row);
2:cc59b24:     } catch (CarbonDataWriterException e) {
1:cc59b24:       throw new SliceMergerException("Problem in merging the slice", e);
1:cc59b24:     }
1:cc59b24:   }
1:cc59b24: 
1:cc59b24:   /**
1:cc59b24:    * Comparator class for comparing 2 raw row result.
1:cc59b24:    */
1:cc59b24:   private class CarbonMdkeyComparator implements Comparator<RawResultIterator> {
1:cc59b24:     int[] columnValueSizes = segprop.getEachDimColumnValueSize();
1:9f94529:     public CarbonMdkeyComparator() {
1:9f94529:       initSortColumns();
1:cc59b24:     }
1:cc59b24: 
1:9f94529:     private void initSortColumns() {
1:9f94529:       int numberOfSortColumns = segprop.getNumberOfSortColumns();
1:9f94529:       if (numberOfSortColumns != columnValueSizes.length) {
1:9f94529:         int[] sortColumnValueSizes = new int[numberOfSortColumns];
1:9f94529:         System.arraycopy(columnValueSizes, 0, sortColumnValueSizes, 0, numberOfSortColumns);
1:9f94529:         this.columnValueSizes = sortColumnValueSizes;
1:cc59b24:       }
1:cc59b24:     }
1:cc59b24: 
1:cc59b24:     @Override public int compare(RawResultIterator o1, RawResultIterator o2) {
1:cc59b24: 
1:cc59b24:       Object[] row1 = new Object[0];
1:cc59b24:       Object[] row2 = new Object[0];
1:cc59b24:       try {
1:cc59b24:         row1 = o1.fetchConverted();
1:cc59b24:         row2 = o2.fetchConverted();
1:cc59b24:       } catch (KeyGenException e) {
1:cc59b24:         LOGGER.error(e.getMessage());
1:cc59b24:       }
1:cc59b24:       if (null == row1 || null == row2) {
1:cc59b24:         return 0;
1:cc59b24:       }
1:cc59b24:       ByteArrayWrapper key1 = (ByteArrayWrapper) row1[0];
1:cc59b24:       ByteArrayWrapper key2 = (ByteArrayWrapper) row2[0];
1:cc59b24:       int compareResult = 0;
1:cc59b24:       int dictionaryKeyOffset = 0;
1:cc59b24:       byte[] dimCols1 = key1.getDictionaryKey();
1:cc59b24:       byte[] dimCols2 = key2.getDictionaryKey();
1:cc59b24:       int noDicIndex = 0;
1:cc59b24:       for (int eachColumnValueSize : columnValueSizes) {
1:cc59b24:         // case of dictionary cols
1:cc59b24:         if (eachColumnValueSize > 0) {
1:cc59b24: 
1:cc59b24:           compareResult = ByteUtil.UnsafeComparer.INSTANCE
1:cc59b24:               .compareTo(dimCols1, dictionaryKeyOffset, eachColumnValueSize, dimCols2,
1:cc59b24:                   dictionaryKeyOffset, eachColumnValueSize);
1:cc59b24:           dictionaryKeyOffset += eachColumnValueSize;
1:cc59b24:         } else { // case of no dictionary
1:cc59b24: 
1:cc59b24:           byte[] noDictionaryDim1 = key1.getNoDictionaryKeyByIndex(noDicIndex);
1:cc59b24:           byte[] noDictionaryDim2 = key2.getNoDictionaryKeyByIndex(noDicIndex);
1:cc59b24:           compareResult =
1:cc59b24:               ByteUtil.UnsafeComparer.INSTANCE.compareTo(noDictionaryDim1, noDictionaryDim2);
1:cc59b24:           noDicIndex++;
1:cc59b24: 
1:cc59b24:         }
1:cc59b24:         if (0 != compareResult) {
1:cc59b24:           return compareResult;
1:cc59b24:         }
1:cc59b24:       }
1:cc59b24:       return 0;
1:cc59b24:     }
1:cc59b24:   }
1:cc59b24: 
1:cc59b24: }
============================================================================
author:ravipesala
-------------------------------------------------------------------------------
commit:60dfdd3
/////////////////////////////////////////////////////////////////////////
1:       CarbonLoadModel loadModel, CompactionType compactionType, PartitionSpec partitionSpec)
1:       throws IOException {
/////////////////////////////////////////////////////////////////////////
1:     carbonFactDataHandlerModel.setSegmentId(loadModel.getSegmentId());
commit:8d3c774
/////////////////////////////////////////////////////////////////////////
1: import org.apache.carbondata.core.constants.CarbonCommonConstants;
1: import org.apache.carbondata.core.indexstore.PartitionSpec;
1: import org.apache.carbondata.core.metadata.SegmentFileStore;
/////////////////////////////////////////////////////////////////////////
1:   private PartitionSpec partitionSpec;
/////////////////////////////////////////////////////////////////////////
0:       CarbonLoadModel loadModel, CompactionType compactionType, PartitionSpec partitionSpec) {
1:     this.partitionSpec = partitionSpec;
1:     String carbonStoreLocation;
1:     if (partitionSpec != null) {
1:       carbonStoreLocation =
1:           partitionSpec.getLocation().toString() + CarbonCommonConstants.FILE_SEPARATOR + loadModel
1:               .getFactTimeStamp() + ".tmp";
1:     } else {
0:       carbonStoreLocation = CarbonDataProcessorUtil
0:           .createCarbonStoreLocation(carbonTable.getTablePath(), loadModel.getDatabaseName(),
0:               tableName, loadModel.getPartitionId(), loadModel.getSegmentId());
1:     }
1:             tempStoreLocation, carbonStoreLocation);
/////////////////////////////////////////////////////////////////////////
1:         if (partitionSpec != null) {
1:           SegmentFileStore.writeSegmentFile(loadModel.getTablePath(), loadModel.getTaskNo(),
1:               partitionSpec.getLocation().toString(), loadModel.getFactTimeStamp() + "",
1:               partitionSpec.getPartitions());
0:         LOGGER.error(e, "Exception in compaction merger");
commit:5ed39de
/////////////////////////////////////////////////////////////////////////
1: import java.io.IOException;
/////////////////////////////////////////////////////////////////////////
0: import org.apache.carbondata.core.metadata.PartitionMapFileStore;
0: import org.apache.carbondata.core.util.path.CarbonTablePath;
/////////////////////////////////////////////////////////////////////////
1:   private CarbonLoadModel loadModel;
0:   private List<String> partitionNames;
/////////////////////////////////////////////////////////////////////////
0:       CarbonLoadModel loadModel, CompactionType compactionType, List<String> partitionNames) {
0:     this.partitionNames = partitionNames;
1:     this.loadModel = loadModel;
/////////////////////////////////////////////////////////////////////////
0:         if (partitionNames != null) {
0:           new PartitionMapFileStore().writePartitionMapFile(
0:               CarbonTablePath.getSegmentPath(loadModel.getTablePath(), loadModel.getSegmentId()),
0:               loadModel.getTaskNo(),
0:               partitionNames);
1:         }
1:       } catch (CarbonDataWriterException | IOException e) {
0:         LOGGER.error(e,"Exception in compaction merger");
author:Raghunandan S
-------------------------------------------------------------------------------
commit:7ef9164
/////////////////////////////////////////////////////////////////////////
1:       if (null != iterator) {
1:         while (true) {
1:           Object[] convertedRow = iterator.next();
1:           if (null == convertedRow) {
1:             iterator.close();
1:             break;
1:           }
1:           // do it only once
1:           if (!isDataPresent) {
1:             dataHandler.initialise();
1:             isDataPresent = true;
1:           }
1:           addRow(convertedRow);
1:           // check if leaf contains no record
1:           if (!iterator.hasNext()) {
1:             break;
1:           }
author:manishgupta88
-------------------------------------------------------------------------------
commit:88279ca
/////////////////////////////////////////////////////////////////////////
1:           iterator.close();
/////////////////////////////////////////////////////////////////////////
1:           iterator.close();
commit:7978b97
/////////////////////////////////////////////////////////////////////////
1:   @Override
1:   public void close() {
1:     // close data handler
1:     if (null != dataHandler) {
1:       dataHandler.closeHandler();
1:     }
1:   }
1: 
commit:cc59b24
/////////////////////////////////////////////////////////////////////////
1: /*
1:  * Licensed to the Apache Software Foundation (ASF) under one or more
1:  * contributor license agreements.  See the NOTICE file distributed with
1:  * this work for additional information regarding copyright ownership.
1:  * The ASF licenses this file to You under the Apache License, Version 2.0
1:  * (the "License"); you may not use this file except in compliance with
1:  * the License.  You may obtain a copy of the License at
1:  *
1:  *    http://www.apache.org/licenses/LICENSE-2.0
1:  *
1:  * Unless required by applicable law or agreed to in writing, software
1:  * distributed under the License is distributed on an "AS IS" BASIS,
1:  * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
1:  * See the License for the specific language governing permissions and
1:  * limitations under the License.
1:  */
1: package org.apache.carbondata.processing.merger;
1: 
0: import java.io.File;
1: import java.util.AbstractQueue;
1: import java.util.Comparator;
1: import java.util.List;
1: import java.util.PriorityQueue;
1: 
1: import org.apache.carbondata.common.logging.LogService;
1: import org.apache.carbondata.common.logging.LogServiceFactory;
0: import org.apache.carbondata.core.constants.CarbonCommonConstants;
1: import org.apache.carbondata.core.datastore.block.SegmentProperties;
1: import org.apache.carbondata.core.keygenerator.KeyGenException;
1: import org.apache.carbondata.core.metadata.CarbonMetadata;
1: import org.apache.carbondata.core.metadata.schema.table.CarbonTable;
1: import org.apache.carbondata.core.scan.result.iterator.RawResultIterator;
1: import org.apache.carbondata.core.scan.wrappers.ByteArrayWrapper;
1: import org.apache.carbondata.core.util.ByteUtil;
0: import org.apache.carbondata.processing.merger.exeception.SliceMergerException;
0: import org.apache.carbondata.processing.model.CarbonLoadModel;
1: import org.apache.carbondata.processing.store.CarbonFactDataHandlerColumnar;
1: import org.apache.carbondata.processing.store.CarbonFactDataHandlerModel;
1: import org.apache.carbondata.processing.store.CarbonFactHandler;
0: import org.apache.carbondata.processing.store.writer.exception.CarbonDataWriterException;
1: 
1: /**
1:  * This is the Merger class responsible for the merging of the segments.
1:  */
1: public class RowResultMergerProcessor extends AbstractResultProcessor {
1: 
1:   private CarbonFactHandler dataHandler;
1:   private SegmentProperties segprop;
1:   /**
1:    * record holder heap
1:    */
1:   private AbstractQueue<RawResultIterator> recordHolderHeap;
1: 
0:   private TupleConversionAdapter tupleConvertor;
1: 
1:   private static final LogService LOGGER =
1:       LogServiceFactory.getLogService(RowResultMergerProcessor.class.getName());
1: 
1:   public RowResultMergerProcessor(String databaseName,
0:       String tableName, SegmentProperties segProp, String tempStoreLocation,
0:       CarbonLoadModel loadModel, CompactionType compactionType) {
1:     this.segprop = segProp;
0:     if (!new File(tempStoreLocation).mkdirs()) {
0:       LOGGER.error("Error while new File(tempStoreLocation).mkdirs() ");
1:     }
0:     CarbonTable carbonTable = CarbonMetadata.getInstance()
0:             .getCarbonTable(databaseName + CarbonCommonConstants.UNDERSCORE + tableName);
1:     CarbonFactDataHandlerModel carbonFactDataHandlerModel = CarbonFactDataHandlerModel
1:         .getCarbonFactDataHandlerModel(loadModel, carbonTable, segProp, tableName,
0:             tempStoreLocation);
0:     setDataFileAttributesInModel(loadModel, compactionType, carbonTable,
0:         carbonFactDataHandlerModel);
1:     carbonFactDataHandlerModel.setCompactionFlow(true);
1:     dataHandler = new CarbonFactDataHandlerColumnar(carbonFactDataHandlerModel);
0:     tupleConvertor = new TupleConversionAdapter(segProp);
1:   }
1: 
1:   private void initRecordHolderHeap(List<RawResultIterator> rawResultIteratorList) {
1:     // create the List of RawResultIterator.
1:     recordHolderHeap = new PriorityQueue<RawResultIterator>(rawResultIteratorList.size(),
1:         new RowResultMergerProcessor.CarbonMdkeyComparator());
1:   }
1: 
1:   /**
1:    * Merge function
1:    *
1:    */
0:   public boolean execute(List<RawResultIterator> resultIteratorList) {
1:     initRecordHolderHeap(resultIteratorList);
1:     boolean mergeStatus = false;
1:     int index = 0;
1:     boolean isDataPresent = false;
1:     try {
1: 
1:       // add all iterators to the queue
1:       for (RawResultIterator leaftTupleIterator : resultIteratorList) {
1:         this.recordHolderHeap.add(leaftTupleIterator);
1:         index++;
1:       }
1:       RawResultIterator iterator = null;
1:       while (index > 1) {
1:         // iterator the top record
1:         iterator = this.recordHolderHeap.poll();
1:         Object[] convertedRow = iterator.next();
1:         if (null == convertedRow) {
1:           index--;
1:           continue;
1:         }
1:         if (!isDataPresent) {
1:           dataHandler.initialise();
1:           isDataPresent = true;
1:         }
1:         // get the mdkey
1:         addRow(convertedRow);
1:         // if there is no record in the leaf and all then decrement the
1:         // index
1:         if (!iterator.hasNext()) {
1:           index--;
1:           continue;
1:         }
1:         // add record to heap
1:         this.recordHolderHeap.add(iterator);
1:       }
1:       // if record holder is not empty then iterator the slice holder from
1:       // heap
1:       iterator = this.recordHolderHeap.poll();
0:       while (true) {
1:         Object[] convertedRow = iterator.next();
1:         if (null == convertedRow) {
0:           break;
1:         }
0:         // do it only once
1:         if (!isDataPresent) {
1:           dataHandler.initialise();
1:           isDataPresent = true;
1:         }
1:         addRow(convertedRow);
0:         // check if leaf contains no record
1:         if (!iterator.hasNext()) {
0:           break;
1:         }
1:       }
1:       if (isDataPresent)
1:       {
1:         this.dataHandler.finish();
1:       }
1:       mergeStatus = true;
1:     } catch (Exception e) {
0:       LOGGER.error(e, e.getMessage());
0:       LOGGER.error("Exception in compaction merger " + e.getMessage());
1:       mergeStatus = false;
1:     } finally {
1:       try {
1:         if (isDataPresent) {
1:           this.dataHandler.closeHandler();
1:         }
1:       } catch (CarbonDataWriterException e) {
0:         LOGGER.error("Exception while closing the handler in compaction merger " + e.getMessage());
1:         mergeStatus = false;
1:       }
1:     }
1: 
1:     return mergeStatus;
1:   }
1: 
1:   /**
1:    * Below method will be used to add sorted row
1:    *
1:    * @throws SliceMergerException
1:    */
1:   private void addRow(Object[] carbonTuple) throws SliceMergerException {
0:     Object[] rowInWritableFormat;
1: 
0:     rowInWritableFormat = tupleConvertor.getObjectArray(carbonTuple);
1:     try {
0:       this.dataHandler.addDataToStore(rowInWritableFormat);
1:     } catch (CarbonDataWriterException e) {
1:       throw new SliceMergerException("Problem in merging the slice", e);
1:     }
1:   }
1: 
1:   /**
1:    * Comparator class for comparing 2 raw row result.
1:    */
1:   private class CarbonMdkeyComparator implements Comparator<RawResultIterator> {
1: 
1:     @Override public int compare(RawResultIterator o1, RawResultIterator o2) {
1: 
1:       Object[] row1 = new Object[0];
1:       Object[] row2 = new Object[0];
1:       try {
1:         row1 = o1.fetchConverted();
1:         row2 = o2.fetchConverted();
1:       } catch (KeyGenException e) {
1:         LOGGER.error(e.getMessage());
1:       }
1:       if (null == row1 || null == row2) {
1:         return 0;
1:       }
1:       ByteArrayWrapper key1 = (ByteArrayWrapper) row1[0];
1:       ByteArrayWrapper key2 = (ByteArrayWrapper) row2[0];
1:       int compareResult = 0;
1:       int[] columnValueSizes = segprop.getEachDimColumnValueSize();
1:       int dictionaryKeyOffset = 0;
1:       byte[] dimCols1 = key1.getDictionaryKey();
1:       byte[] dimCols2 = key2.getDictionaryKey();
1:       int noDicIndex = 0;
1:       for (int eachColumnValueSize : columnValueSizes) {
1:         // case of dictionary cols
1:         if (eachColumnValueSize > 0) {
1: 
1:           compareResult = ByteUtil.UnsafeComparer.INSTANCE
1:               .compareTo(dimCols1, dictionaryKeyOffset, eachColumnValueSize, dimCols2,
1:                   dictionaryKeyOffset, eachColumnValueSize);
1:           dictionaryKeyOffset += eachColumnValueSize;
1:         } else { // case of no dictionary
1: 
1:           byte[] noDictionaryDim1 = key1.getNoDictionaryKeyByIndex(noDicIndex);
1:           byte[] noDictionaryDim2 = key2.getNoDictionaryKeyByIndex(noDicIndex);
1:           compareResult =
1:               ByteUtil.UnsafeComparer.INSTANCE.compareTo(noDictionaryDim1, noDictionaryDim2);
1:           noDicIndex++;
1: 
1:         }
1:         if (0 != compareResult) {
1:           return compareResult;
1:         }
1:       }
1:       return 0;
1:     }
1:   }
1: 
1: }
author:xuchuanyin
-------------------------------------------------------------------------------
commit:e26cccc
/////////////////////////////////////////////////////////////////////////
0:           iterator.close();
commit:8d8b589
/////////////////////////////////////////////////////////////////////////
1:       carbonStoreLocation = CarbonDataProcessorUtil.createCarbonStoreLocation(
1:           loadModel.getDatabaseName(), tableName, loadModel.getSegmentId());
commit:ded8b41
/////////////////////////////////////////////////////////////////////////
/////////////////////////////////////////////////////////////////////////
1: import org.apache.carbondata.processing.util.CarbonDataProcessorUtil;
/////////////////////////////////////////////////////////////////////////
1:       String tableName, SegmentProperties segProp, String[] tempStoreLocation,
1:     CarbonDataProcessorUtil.createLocations(tempStoreLocation);
0: 
author:rahulforallp
-------------------------------------------------------------------------------
commit:f1d8464
/////////////////////////////////////////////////////////////////////////
1:   public boolean execute(List<RawResultIterator> resultIteratorList) throws Exception {
/////////////////////////////////////////////////////////////////////////
1:       throw e;
/////////////////////////////////////////////////////////////////////////
1:         throw e;
author:Jacky Li
-------------------------------------------------------------------------------
commit:bf6c471
/////////////////////////////////////////////////////////////////////////
1:     setDataFileAttributesInModel(loadModel, compactionType, carbonFactDataHandlerModel);
commit:5bedd77
/////////////////////////////////////////////////////////////////////////
0:               tableName, loadModel.getSegmentId());
commit:349c59c
/////////////////////////////////////////////////////////////////////////
1: import org.apache.carbondata.processing.exception.SliceMergerException;
1: import org.apache.carbondata.processing.loading.model.CarbonLoadModel;
author:QiangCai
-------------------------------------------------------------------------------
commit:7e124f4
/////////////////////////////////////////////////////////////////////////
/////////////////////////////////////////////////////////////////////////
1:     CarbonTable carbonTable = CarbonMetadata.getInstance().getCarbonTable(databaseName, tableName);
commit:9f94529
/////////////////////////////////////////////////////////////////////////
0:     int[] columnValueSizes = segprop.getEachDimColumnValueSize();
1:     public CarbonMdkeyComparator() {
1:       initSortColumns();
0:     }
0: 
1:     private void initSortColumns() {
1:       int numberOfSortColumns = segprop.getNumberOfSortColumns();
1:       if (numberOfSortColumns != columnValueSizes.length) {
1:         int[] sortColumnValueSizes = new int[numberOfSortColumns];
1:         System.arraycopy(columnValueSizes, 0, sortColumnValueSizes, 0, numberOfSortColumns);
1:         this.columnValueSizes = sortColumnValueSizes;
0:       }
0:     }
/////////////////////////////////////////////////////////////////////////
author:jackylk
-------------------------------------------------------------------------------
commit:dc83b2a
/////////////////////////////////////////////////////////////////////////
1: import org.apache.carbondata.core.datastore.exception.CarbonDataWriterException;
1: import org.apache.carbondata.core.datastore.row.CarbonRow;
1: import org.apache.carbondata.core.datastore.row.WriteStepRowUtil;
/////////////////////////////////////////////////////////////////////////
commit:353272e
/////////////////////////////////////////////////////////////////////////
0: import org.apache.carbondata.processing.newflow.row.CarbonRow;
0: import org.apache.carbondata.processing.newflow.row.WriteStepRowUtil;
/////////////////////////////////////////////////////////////////////////
/////////////////////////////////////////////////////////////////////////
/////////////////////////////////////////////////////////////////////////
1:     CarbonRow row = WriteStepRowUtil.fromMergerRow(carbonTuple, segprop);
1:       this.dataHandler.addDataToStore(row);
============================================================================