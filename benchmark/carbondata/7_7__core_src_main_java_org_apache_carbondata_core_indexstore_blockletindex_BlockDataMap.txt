1:6118711: /*
1:6118711:  * Licensed to the Apache Software Foundation (ASF) under one or more
1:6118711:  * contributor license agreements.  See the NOTICE file distributed with
1:6118711:  * this work for additional information regarding copyright ownership.
1:6118711:  * The ASF licenses this file to You under the Apache License, Version 2.0
1:6118711:  * (the "License"); you may not use this file except in compliance with
1:6118711:  * the License.  You may obtain a copy of the License at
2:6118711:  *
1:6118711:  *    http://www.apache.org/licenses/LICENSE-2.0
1:6118711:  *
1:6118711:  * Unless required by applicable law or agreed to in writing, software
1:6118711:  * distributed under the License is distributed on an "AS IS" BASIS,
1:6118711:  * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
1:6118711:  * See the License for the specific language governing permissions and
1:6118711:  * limitations under the License.
3:6118711:  */
1:6118711: package org.apache.carbondata.core.indexstore.blockletindex;
10:6118711: 
1:dc29319: import java.io.*;
1:1cea4d3: import java.nio.ByteBuffer;
1:3cbabcd: import java.util.*;
1:6118711: 
1:6118711: import org.apache.carbondata.common.logging.LogService;
1:6118711: import org.apache.carbondata.common.logging.LogServiceFactory;
1:6118711: import org.apache.carbondata.core.constants.CarbonCommonConstants;
1:6118711: import org.apache.carbondata.core.datamap.dev.DataMapModel;
1:6118711: import org.apache.carbondata.core.datamap.dev.cgdatamap.CoarseGrainDataMap;
1:6118711: import org.apache.carbondata.core.datastore.block.SegmentProperties;
1:f4a58c5: import org.apache.carbondata.core.datastore.block.SegmentPropertiesAndSchemaHolder;
1:6118711: import org.apache.carbondata.core.datastore.block.TableBlockInfo;
1:6118711: import org.apache.carbondata.core.indexstore.AbstractMemoryDMStore;
1:6118711: import org.apache.carbondata.core.indexstore.BlockMetaInfo;
1:6118711: import org.apache.carbondata.core.indexstore.Blocklet;
1:6118711: import org.apache.carbondata.core.indexstore.BlockletDetailInfo;
1:6118711: import org.apache.carbondata.core.indexstore.ExtendedBlocklet;
1:6118711: import org.apache.carbondata.core.indexstore.PartitionSpec;
1:6118711: import org.apache.carbondata.core.indexstore.SafeMemoryDMStore;
1:6118711: import org.apache.carbondata.core.indexstore.UnsafeMemoryDMStore;
1:6118711: import org.apache.carbondata.core.indexstore.row.DataMapRow;
1:6118711: import org.apache.carbondata.core.indexstore.row.DataMapRowImpl;
1:6118711: import org.apache.carbondata.core.indexstore.schema.CarbonRowSchema;
1:6118711: import org.apache.carbondata.core.memory.MemoryException;
1:3894e1d: import org.apache.carbondata.core.metadata.AbsoluteTableIdentifier;
1:6118711: import org.apache.carbondata.core.metadata.blocklet.DataFileFooter;
1:6118711: import org.apache.carbondata.core.metadata.blocklet.index.BlockletIndex;
1:6118711: import org.apache.carbondata.core.metadata.blocklet.index.BlockletMinMaxIndex;
1:3894e1d: import org.apache.carbondata.core.metadata.schema.table.CarbonTable;
1:dc29319: import org.apache.carbondata.core.metadata.schema.table.column.CarbonColumn;
1:3894e1d: import org.apache.carbondata.core.metadata.schema.table.column.CarbonDimension;
1:6118711: import org.apache.carbondata.core.metadata.schema.table.column.ColumnSchema;
1:6118711: import org.apache.carbondata.core.profiler.ExplainCollector;
1:3894e1d: import org.apache.carbondata.core.scan.expression.Expression;
1:6118711: import org.apache.carbondata.core.scan.filter.FilterExpressionProcessor;
1:6118711: import org.apache.carbondata.core.scan.filter.FilterUtil;
1:6118711: import org.apache.carbondata.core.scan.filter.executer.FilterExecuter;
1:6118711: import org.apache.carbondata.core.scan.filter.executer.ImplicitColumnFilterExecutor;
1:3894e1d: import org.apache.carbondata.core.scan.filter.intf.FilterOptimizer;
1:3894e1d: import org.apache.carbondata.core.scan.filter.optimizer.RangeFilterOptmizer;
1:6118711: import org.apache.carbondata.core.scan.filter.resolver.FilterResolverIntf;
1:3894e1d: import org.apache.carbondata.core.scan.model.QueryModel;
1:6118711: import org.apache.carbondata.core.util.BlockletDataMapUtil;
1:6118711: import org.apache.carbondata.core.util.ByteUtil;
1:6118711: import org.apache.carbondata.core.util.CarbonUtil;
1:6118711: import org.apache.carbondata.core.util.DataFileFooterConverter;
1:6118711: import org.apache.carbondata.core.util.path.CarbonTablePath;
1:6118711: 
1:6118711: import org.apache.commons.lang3.StringUtils;
1:6118711: import org.apache.hadoop.fs.Path;
1:6118711: 
3:6118711: /**
1:6118711:  * Datamap implementation for block.
1:6118711:  */
1:6118711: public class BlockDataMap extends CoarseGrainDataMap
1:6118711:     implements BlockletDataMapRowIndexes, Serializable {
1:6118711: 
1:6118711:   private static final LogService LOGGER =
1:6118711:       LogServiceFactory.getLogService(BlockDataMap.class.getName());
1:6118711: 
1:6118711:   protected static final long serialVersionUID = -2170289352240810993L;
1:6118711:   /**
1:6118711:    * for CACHE_LEVEL=BLOCK and legacy store default blocklet id will be -1
1:6118711:    */
1:6118711:   private static final short BLOCK_DEFAULT_BLOCKLET_ID = -1;
1:f4a58c5:   /**
1:f4a58c5:    * store which will hold all the block or blocklet entries in one task
1:f4a58c5:    */
1:6118711:   protected AbstractMemoryDMStore memoryDMStore;
1:f4a58c5:   /**
1:f4a58c5:    * task summary holder store
1:f4a58c5:    */
1:6118711:   protected AbstractMemoryDMStore taskSummaryDMStore;
1:f4a58c5:   /**
1:f4a58c5:    * index of segmentProperties in the segmentProperties holder
1:f4a58c5:    */
1:f4a58c5:   protected int segmentPropertiesIndex;
1:6118711:   /**
1:6118711:    * flag to check for store from 1.1 or any prior version
1:6118711:    */
1:6118711:   protected boolean isLegacyStore;
1:f4a58c5:   /**
1:f4a58c5:    * flag to be used for forming the complete file path from file name. It will be true in case of
1:f4a58c5:    * partition table and non transactional table
1:f4a58c5:    */
1:f4a58c5:   protected boolean isFilePathStored;
1:6118711: 
1:8f1a029:   @Override public void init(DataMapModel dataMapModel)
1:8f1a029:       throws IOException, MemoryException {
1:6118711:     long startTime = System.currentTimeMillis();
1:6118711:     assert (dataMapModel instanceof BlockletDataMapModel);
1:6118711:     BlockletDataMapModel blockletDataMapInfo = (BlockletDataMapModel) dataMapModel;
1:8f1a029:     DataFileFooterConverter fileFooterConverter =
1:8f1a029:         new DataFileFooterConverter(dataMapModel.getConfiguration());
1:6118711:     List<DataFileFooter> indexInfo = fileFooterConverter
1:3894e1d:         .getIndexInfo(blockletDataMapInfo.getFilePath(), blockletDataMapInfo.getFileData(),
1:3894e1d:             blockletDataMapInfo.getCarbonTable().isTransactionalTable());
1:6118711:     Path path = new Path(blockletDataMapInfo.getFilePath());
1:f4a58c5:     // store file path only in case of partition table, non transactional table and flat folder
1:f4a58c5:     // structure
1:f4a58c5:     byte[] filePath = null;
1:f4a58c5:     boolean isPartitionTable = blockletDataMapInfo.getCarbonTable().isHivePartitionTable();
1:f4a58c5:     if (isPartitionTable || !blockletDataMapInfo.getCarbonTable().isTransactionalTable()
1:f4a58c5:         || blockletDataMapInfo.getCarbonTable().isSupportFlatFolder()) {
1:f4a58c5:       filePath = path.getParent().toString().getBytes(CarbonCommonConstants.DEFAULT_CHARSET);
1:f4a58c5:       isFilePathStored = true;
1:f4a58c5:     }
1:6118711:     byte[] fileName = path.getName().toString().getBytes(CarbonCommonConstants.DEFAULT_CHARSET);
1:6118711:     byte[] segmentId =
1:6118711:         blockletDataMapInfo.getSegmentId().getBytes(CarbonCommonConstants.DEFAULT_CHARSET);
1:6118711:     if (!indexInfo.isEmpty()) {
1:6118711:       DataFileFooter fileFooter = indexInfo.get(0);
1:6118711:       // store for 1.1 or any prior version will not have any blocklet information in file footer
1:6118711:       isLegacyStore = fileFooter.getBlockletList() == null;
1:6118711:       // init segment properties and create schema
1:f4a58c5:       SegmentProperties segmentProperties = initSegmentProperties(blockletDataMapInfo, fileFooter);
1:dc29319:       createMemorySchema(blockletDataMapInfo);
1:dc29319:       createSummaryDMStore(blockletDataMapInfo);
1:dc29319:       CarbonRowSchema[] taskSummarySchema = getTaskSummarySchema();
1:f4a58c5:       // check for legacy store and load the metadata
1:dc29319:       DataMapRowImpl summaryRow =
1:dc29319:           loadMetadata(taskSummarySchema, segmentProperties, blockletDataMapInfo, indexInfo);
1:dc29319:       finishWriting(taskSummarySchema, filePath, fileName, segmentId, summaryRow);
8:6118711:     }
1:6118711:     if (LOGGER.isDebugEnabled()) {
1:6118711:       LOGGER.debug(
1:6118711:           "Time taken to load blocklet datamap from file : " + dataMapModel.getFilePath() + " is "
1:6118711:               + (System.currentTimeMillis() - startTime));
1:6118711:     }
1:6118711:   }
1:6118711: 
1:dc29319:   private void finishWriting(CarbonRowSchema[] taskSummarySchema, byte[] filePath, byte[] fileName,
1:dc29319:       byte[] segmentId, DataMapRowImpl summaryRow) throws MemoryException {
1:6118711:     if (memoryDMStore != null) {
1:6118711:       memoryDMStore.finishWriting();
1:6118711:     }
1:6118711:     if (null != taskSummaryDMStore) {
1:dc29319:       addTaskSummaryRowToUnsafeMemoryStore(taskSummarySchema, summaryRow, filePath, fileName,
1:dc29319:           segmentId);
1:6118711:       taskSummaryDMStore.finishWriting();
1:6118711:     }
1:6118711:   }
1:6118711: 
1:6118711:   /**
1:6118711:    * Method to check the cache level and load metadata based on that information
1:6118711:    *
1:f4a58c5:    * @param segmentProperties
1:6118711:    * @param blockletDataMapInfo
1:6118711:    * @param indexInfo
2:6118711:    * @throws IOException
1:6118711:    * @throws MemoryException
1:6118711:    */
1:dc29319:   protected DataMapRowImpl loadMetadata(CarbonRowSchema[] taskSummarySchema,
1:dc29319:       SegmentProperties segmentProperties, BlockletDataMapModel blockletDataMapInfo,
1:dc29319:       List<DataFileFooter> indexInfo) throws IOException, MemoryException {
1:6118711:     if (isLegacyStore) {
1:dc29319:       return loadBlockInfoForOldStore(taskSummarySchema, segmentProperties, blockletDataMapInfo,
1:dc29319:           indexInfo);
1:6118711:     } else {
1:dc29319:       return loadBlockMetaInfo(taskSummarySchema, segmentProperties, blockletDataMapInfo,
1:dc29319:           indexInfo);
1:6118711:     }
1:6118711:   }
1:6118711: 
1:6118711:   /**
1:6118711:    * initialise segment properties
1:6118711:    *
1:6118711:    * @param fileFooter
1:6118711:    * @throws IOException
1:6118711:    */
1:f4a58c5:   private SegmentProperties initSegmentProperties(BlockletDataMapModel blockletDataMapInfo,
1:f4a58c5:       DataFileFooter fileFooter) throws IOException {
1:6118711:     List<ColumnSchema> columnInTable = fileFooter.getColumnInTable();
1:f4a58c5:     int[] columnCardinality = fileFooter.getSegmentInfo().getColumnCardinality();
1:f4a58c5:     segmentPropertiesIndex = SegmentPropertiesAndSchemaHolder.getInstance()
1:dc29319:         .addSegmentProperties(blockletDataMapInfo.getCarbonTable(),
1:f4a58c5:             columnInTable, columnCardinality, blockletDataMapInfo.getSegmentId());
1:f4a58c5:     return getSegmentProperties();
1:6118711:   }
1:6118711: 
1:6118711:   /**
1:6118711:    * This is old store scenario, here blocklet information is not available in index
1:6118711:    * file so load only block info. Old store refers to store in 1.1 or prior to 1.1 version
1:6118711:    *
1:6118711:    * @param blockletDataMapInfo
1:6118711:    * @param indexInfo
1:6118711:    * @throws IOException
1:6118711:    * @throws MemoryException
1:6118711:    */
1:dc29319:   protected DataMapRowImpl loadBlockInfoForOldStore(CarbonRowSchema[] taskSummarySchema,
1:dc29319:       SegmentProperties segmentProperties, BlockletDataMapModel blockletDataMapInfo,
1:dc29319:       List<DataFileFooter> indexInfo) throws IOException, MemoryException {
1:6118711:     DataMapRowImpl summaryRow = null;
1:dc29319:     CarbonRowSchema[] schema = getFileFooterEntrySchema();
1:6118711:     for (DataFileFooter fileFooter : indexInfo) {
1:6118711:       TableBlockInfo blockInfo = fileFooter.getBlockInfo().getTableBlockInfo();
1:6118711:       BlockMetaInfo blockMetaInfo =
1:6118711:           blockletDataMapInfo.getBlockMetaInfoMap().get(blockInfo.getFilePath());
1:6118711:       // Here it loads info about all blocklets of index
1:6118711:       // Only add if the file exists physically. There are scenarios which index file exists inside
1:6118711:       // merge index but related carbondata files are deleted. In that case we first check whether
1:6118711:       // the file exists physically or not
1:6118711:       if (null != blockMetaInfo) {
1:6118711:         BlockletIndex blockletIndex = fileFooter.getBlockletIndex();
1:6118711:         BlockletMinMaxIndex minMaxIndex = blockletIndex.getMinMaxIndex();
1:6118711:         byte[][] minValues =
1:6118711:             BlockletDataMapUtil.updateMinValues(segmentProperties, minMaxIndex.getMinValues());
1:6118711:         byte[][] maxValues =
1:6118711:             BlockletDataMapUtil.updateMaxValues(segmentProperties, minMaxIndex.getMaxValues());
1:6118711:         // update min max values in case of old store for measures as measure min/max in
1:6118711:         // old stores in written opposite
1:6118711:         byte[][] updatedMinValues =
1:6118711:             CarbonUtil.updateMinMaxValues(fileFooter, maxValues, minValues, true);
1:6118711:         byte[][] updatedMaxValues =
1:6118711:             CarbonUtil.updateMinMaxValues(fileFooter, maxValues, minValues, false);
1:dc29319:         summaryRow = loadToUnsafeBlock(schema, taskSummarySchema, fileFooter, segmentProperties,
1:8e78957:             getMinMaxCacheColumns(), blockInfo.getFilePath(), summaryRow,
1:dc29319:             blockMetaInfo, updatedMinValues, updatedMaxValues);
1:6118711:       }
1:6118711:     }
1:6118711:     return summaryRow;
1:6118711:   }
1:6118711: 
1:6118711:   /**
1:6118711:    * Method to load block metadata information
1:6118711:    *
1:6118711:    * @param blockletDataMapInfo
1:6118711:    * @param indexInfo
1:6118711:    * @throws IOException
1:6118711:    * @throws MemoryException
1:6118711:    */
1:dc29319:   private DataMapRowImpl loadBlockMetaInfo(CarbonRowSchema[] taskSummarySchema,
1:dc29319:       SegmentProperties segmentProperties, BlockletDataMapModel blockletDataMapInfo,
1:dc29319:       List<DataFileFooter> indexInfo) throws IOException, MemoryException {
1:6118711:     String tempFilePath = null;
1:6118711:     DataFileFooter previousDataFileFooter = null;
1:6118711:     int footerCounter = 0;
1:6118711:     byte[][] blockMinValues = null;
1:6118711:     byte[][] blockMaxValues = null;
1:6118711:     DataMapRowImpl summaryRow = null;
1:1cea4d3:     List<Short> blockletCountInEachBlock = new ArrayList<>(indexInfo.size());
1:1cea4d3:     short totalBlockletsInOneBlock = 0;
1:6118711:     boolean isLastFileFooterEntryNeedToBeAdded = false;
1:dc29319:     CarbonRowSchema[] schema = getFileFooterEntrySchema();
1:6118711:     for (DataFileFooter fileFooter : indexInfo) {
1:6118711:       TableBlockInfo blockInfo = fileFooter.getBlockInfo().getTableBlockInfo();
1:6118711:       BlockMetaInfo blockMetaInfo =
1:6118711:           blockletDataMapInfo.getBlockMetaInfoMap().get(blockInfo.getFilePath());
1:6118711:       footerCounter++;
1:6118711:       if (blockMetaInfo != null) {
1:6118711:         // this variable will be used for adding the DataMapRow entry every time a unique block
1:6118711:         // path is encountered
1:6118711:         if (null == tempFilePath) {
1:6118711:           tempFilePath = blockInfo.getFilePath();
1:6118711:           // 1st time assign the min and max values from the current file footer
1:6118711:           blockMinValues = fileFooter.getBlockletIndex().getMinMaxIndex().getMinValues();
1:6118711:           blockMaxValues = fileFooter.getBlockletIndex().getMinMaxIndex().getMaxValues();
1:6118711:           previousDataFileFooter = fileFooter;
1:6118711:           totalBlockletsInOneBlock++;
1:6118711:         } else if (blockInfo.getFilePath().equals(tempFilePath)) {
1:6118711:           // After iterating over all the blocklets that belong to one block we need to compute the
1:6118711:           // min and max at block level. So compare min and max values and update if required
1:6118711:           BlockletMinMaxIndex currentFooterMinMaxIndex =
1:6118711:               fileFooter.getBlockletIndex().getMinMaxIndex();
1:dc29319:           blockMinValues =
1:dc29319:               compareAndUpdateMinMax(currentFooterMinMaxIndex.getMinValues(), blockMinValues, true);
1:dc29319:           blockMaxValues =
1:dc29319:               compareAndUpdateMinMax(currentFooterMinMaxIndex.getMaxValues(), blockMaxValues,
1:dc29319:                   false);
1:6118711:           totalBlockletsInOneBlock++;
1:6118711:         }
1:6118711:         // as one task contains entries for all the blocklets we need iterate and load only the
1:6118711:         // with unique file path because each unique path will correspond to one
1:6118711:         // block in the task. OR condition is to handle the loading of last file footer
1:6118711:         if (!blockInfo.getFilePath().equals(tempFilePath) || footerCounter == indexInfo.size()) {
1:6118711:           TableBlockInfo previousBlockInfo =
1:6118711:               previousDataFileFooter.getBlockInfo().getTableBlockInfo();
1:dc29319:           summaryRow = loadToUnsafeBlock(schema, taskSummarySchema, previousDataFileFooter,
1:8e78957:               segmentProperties, getMinMaxCacheColumns(), previousBlockInfo.getFilePath(),
1:8e78957:               summaryRow,
1:6118711:               blockletDataMapInfo.getBlockMetaInfoMap().get(previousBlockInfo.getFilePath()),
2:6118711:               blockMinValues, blockMaxValues);
1:6118711:           // flag to check whether last file footer entry is different from previous entry.
1:6118711:           // If yes then it need to be added at last
1:6118711:           isLastFileFooterEntryNeedToBeAdded =
1:6118711:               (footerCounter == indexInfo.size()) && (!blockInfo.getFilePath()
1:6118711:                   .equals(tempFilePath));
1:6118711:           // assign local variables values using the current file footer
1:6118711:           tempFilePath = blockInfo.getFilePath();
1:6118711:           blockMinValues = fileFooter.getBlockletIndex().getMinMaxIndex().getMinValues();
1:6118711:           blockMaxValues = fileFooter.getBlockletIndex().getMinMaxIndex().getMaxValues();
1:6118711:           previousDataFileFooter = fileFooter;
1:6118711:           blockletCountInEachBlock.add(totalBlockletsInOneBlock);
1:6118711:           // for next block count will start from 1 because a row is created whenever a new file
1:6118711:           // path comes. Here already a new file path has come so the count should start from 1
1:6118711:           totalBlockletsInOneBlock = 1;
1:6118711:         }
1:6118711:       }
1:6118711:     }
1:6118711:     // add the last file footer entry
1:6118711:     if (isLastFileFooterEntryNeedToBeAdded) {
1:dc29319:       summaryRow =
1:dc29319:           loadToUnsafeBlock(schema, taskSummarySchema, previousDataFileFooter, segmentProperties,
1:8e78957:               getMinMaxCacheColumns(),
1:dc29319:               previousDataFileFooter.getBlockInfo().getTableBlockInfo().getFilePath(), summaryRow,
1:dc29319:               blockletDataMapInfo.getBlockMetaInfoMap()
1:dc29319:                   .get(previousDataFileFooter.getBlockInfo().getTableBlockInfo().getFilePath()),
1:dc29319:               blockMinValues, blockMaxValues);
1:6118711:       blockletCountInEachBlock.add(totalBlockletsInOneBlock);
1:6118711:     }
1:1cea4d3:     byte[] blockletCount = convertRowCountFromShortToByteArray(blockletCountInEachBlock);
1:f4a58c5:     // blocklet count index is the last index
1:dc29319:     summaryRow.setByteArray(blockletCount, taskSummarySchema.length - 1);
1:6118711:     return summaryRow;
1:6118711:   }
1:6118711: 
1:1cea4d3:   private byte[] convertRowCountFromShortToByteArray(List<Short> blockletCountInEachBlock) {
1:1cea4d3:     int bufferSize = blockletCountInEachBlock.size() * 2;
1:1cea4d3:     ByteBuffer byteBuffer = ByteBuffer.allocate(bufferSize);
1:1cea4d3:     for (Short blockletCount : blockletCountInEachBlock) {
1:1cea4d3:       byteBuffer.putShort(blockletCount);
1:1cea4d3:     }
1:1cea4d3:     byteBuffer.rewind();
1:1cea4d3:     return byteBuffer.array();
1:1cea4d3:   }
1:1cea4d3: 
1:6118711:   protected void setLocations(String[] locations, DataMapRow row, int ordinal)
1:6118711:       throws UnsupportedEncodingException {
1:6118711:     // Add location info
1:6118711:     String locationStr = StringUtils.join(locations, ',');
1:6118711:     row.setByteArray(locationStr.getBytes(CarbonCommonConstants.DEFAULT_CHARSET), ordinal);
1:6118711:   }
1:6118711: 
1:6118711:   /**
1:6118711:    * Load information for the block.It is the case can happen only for old stores
1:6118711:    * where blocklet information is not available in index file. So load only block information
1:6118711:    * and read blocklet information in executor.
1:6118711:    */
1:dc29319:   protected DataMapRowImpl loadToUnsafeBlock(CarbonRowSchema[] schema,
1:dc29319:       CarbonRowSchema[] taskSummarySchema, DataFileFooter fileFooter,
1:dc29319:       SegmentProperties segmentProperties, List<CarbonColumn> minMaxCacheColumns, String filePath,
1:dc29319:       DataMapRowImpl summaryRow, BlockMetaInfo blockMetaInfo, byte[][] minValues,
1:dc29319:       byte[][] maxValues) {
1:6118711:     // Add one row to maintain task level min max for segment pruning
1:6118711:     if (summaryRow == null) {
1:f4a58c5:       summaryRow = new DataMapRowImpl(taskSummarySchema);
1:6118711:     }
1:6118711:     DataMapRow row = new DataMapRowImpl(schema);
1:6118711:     int ordinal = 0;
1:6118711:     int taskMinMaxOrdinal = 0;
1:dc29319:     // get min max values for columns to be cached
1:dc29319:     byte[][] minValuesForColumnsToBeCached = BlockletDataMapUtil
1:dc29319:         .getMinMaxForColumnsToBeCached(segmentProperties, minMaxCacheColumns, minValues);
1:dc29319:     byte[][] maxValuesForColumnsToBeCached = BlockletDataMapUtil
1:dc29319:         .getMinMaxForColumnsToBeCached(segmentProperties, minMaxCacheColumns, maxValues);
1:dc29319:     row.setRow(addMinMax(schema[ordinal], minValuesForColumnsToBeCached), ordinal);
1:6118711:     // compute and set task level min values
1:dc29319:     addTaskMinMaxValues(summaryRow, taskSummarySchema, taskMinMaxOrdinal,
1:dc29319:         minValuesForColumnsToBeCached, TASK_MIN_VALUES_INDEX, true);
1:6118711:     ordinal++;
1:6118711:     taskMinMaxOrdinal++;
1:dc29319:     row.setRow(addMinMax(schema[ordinal], maxValuesForColumnsToBeCached), ordinal);
1:6118711:     // compute and set task level max values
1:dc29319:     addTaskMinMaxValues(summaryRow, taskSummarySchema, taskMinMaxOrdinal,
1:dc29319:         maxValuesForColumnsToBeCached, TASK_MAX_VALUES_INDEX, false);
1:6118711:     ordinal++;
1:6118711:     // add total rows in one carbondata file
1:6118711:     row.setInt((int) fileFooter.getNumberOfRows(), ordinal++);
1:f4a58c5:     // add file name
1:f4a58c5:     byte[] filePathBytes =
1:f4a58c5:         getFileNameFromPath(filePath).getBytes(CarbonCommonConstants.DEFAULT_CHARSET_CLASS);
1:6118711:     row.setByteArray(filePathBytes, ordinal++);
1:6118711:     // add version number
1:6118711:     row.setShort(fileFooter.getVersionId().number(), ordinal++);
1:6118711:     // add schema updated time
1:6118711:     row.setLong(fileFooter.getSchemaUpdatedTimeStamp(), ordinal++);
1:6118711:     // add block offset
1:6118711:     row.setLong(fileFooter.getBlockInfo().getTableBlockInfo().getBlockOffset(), ordinal++);
1:6118711:     try {
1:6118711:       setLocations(blockMetaInfo.getLocationInfo(), row, ordinal++);
1:6118711:       // store block size
1:6118711:       row.setLong(blockMetaInfo.getSize(), ordinal);
1:f4a58c5:       memoryDMStore.addIndexRow(schema, row);
1:6118711:     } catch (Exception e) {
1:6118711:       throw new RuntimeException(e);
1:6118711:     }
1:6118711:     return summaryRow;
1:6118711:   }
1:6118711: 
1:f4a58c5:   protected String getFileNameFromPath(String filePath) {
1:f4a58c5:     return CarbonTablePath.getCarbonDataFileName(filePath);
1:f4a58c5:   }
1:f4a58c5: 
1:f4a58c5:   protected String getFilePath() {
1:f4a58c5:     if (isFilePathStored) {
1:f4a58c5:       return getTableTaskInfo(SUMMARY_INDEX_PATH);
1:f4a58c5:     }
1:f4a58c5:     // create the segment directory path
1:f4a58c5:     String tablePath = SegmentPropertiesAndSchemaHolder.getInstance()
1:f4a58c5:         .getSegmentPropertiesWrapper(segmentPropertiesIndex).getTableIdentifier().getTablePath();
1:f4a58c5:     String segmentId = getTableTaskInfo(SUMMARY_SEGMENTID);
1:f4a58c5:     return CarbonTablePath.getSegmentPath(tablePath, segmentId);
1:f4a58c5:   }
1:f4a58c5: 
1:f4a58c5:   protected String getFileNameWithFilePath(DataMapRow dataMapRow, String filePath) {
1:f4a58c5:     String fileName = filePath + CarbonCommonConstants.FILE_SEPARATOR + new String(
1:f4a58c5:         dataMapRow.getByteArray(FILE_PATH_INDEX), CarbonCommonConstants.DEFAULT_CHARSET_CLASS)
1:f4a58c5:         + CarbonTablePath.getCarbonDataExtension();
1:f4a58c5:     return fileName;
1:f4a58c5:   }
1:f4a58c5: 
1:dc29319:   private void addTaskSummaryRowToUnsafeMemoryStore(CarbonRowSchema[] taskSummarySchema,
1:dc29319:       DataMapRow summaryRow, byte[] filePath, byte[] fileName, byte[] segmentId) {
1:6118711:     // write the task summary info to unsafe memory store
1:6118711:     if (null != summaryRow) {
1:6118711:       summaryRow.setByteArray(fileName, SUMMARY_INDEX_FILE_NAME);
1:6118711:       summaryRow.setByteArray(segmentId, SUMMARY_SEGMENTID);
1:f4a58c5:       if (null != filePath) {
1:f4a58c5:         summaryRow.setByteArray(filePath, SUMMARY_INDEX_PATH);
1:f4a58c5:       }
1:6118711:       try {
1:dc29319:         taskSummaryDMStore.addIndexRow(taskSummarySchema, summaryRow);
1:6118711:       } catch (Exception e) {
1:6118711:         throw new RuntimeException(e);
1:6118711:       }
1:6118711:     }
1:6118711:   }
1:6118711: 
1:dc29319:   protected DataMapRow addMinMax(CarbonRowSchema carbonRowSchema,
1:6118711:       byte[][] minValues) {
1:6118711:     CarbonRowSchema[] minSchemas =
1:6118711:         ((CarbonRowSchema.StructCarbonRowSchema) carbonRowSchema).getChildSchemas();
1:6118711:     DataMapRow minRow = new DataMapRowImpl(minSchemas);
1:6118711:     int minOrdinal = 0;
1:6118711:     // min value adding
1:dc29319:     for (int i = 0; i < minValues.length; i++) {
1:6118711:       minRow.setByteArray(minValues[i], minOrdinal++);
1:6118711:     }
1:6118711:     return minRow;
1:6118711:   }
1:6118711: 
1:6118711:   /**
1:6118711:    * This method will compute min/max values at task level
1:6118711:    *
1:6118711:    * @param taskMinMaxRow
1:6118711:    * @param carbonRowSchema
1:6118711:    * @param taskMinMaxOrdinal
1:6118711:    * @param minMaxValue
1:6118711:    * @param ordinal
1:6118711:    * @param isMinValueComparison
1:6118711:    */
1:dc29319:   protected void addTaskMinMaxValues(DataMapRow taskMinMaxRow, CarbonRowSchema[] carbonRowSchema,
1:dc29319:       int taskMinMaxOrdinal, byte[][] minMaxValue, int ordinal, boolean isMinValueComparison) {
1:6118711:     DataMapRow row = taskMinMaxRow.getRow(ordinal);
1:6118711:     byte[][] updatedMinMaxValues = null;
1:6118711:     if (null == row) {
1:6118711:       CarbonRowSchema[] minSchemas =
1:6118711:           ((CarbonRowSchema.StructCarbonRowSchema) carbonRowSchema[taskMinMaxOrdinal])
1:6118711:               .getChildSchemas();
1:6118711:       row = new DataMapRowImpl(minSchemas);
1:6118711:       updatedMinMaxValues = minMaxValue;
1:6118711:     } else {
1:6118711:       byte[][] existingMinMaxValues = getMinMaxValue(taskMinMaxRow, ordinal);
1:dc29319:       updatedMinMaxValues =
1:dc29319:           compareAndUpdateMinMax(minMaxValue, existingMinMaxValues, isMinValueComparison);
1:6118711:     }
1:6118711:     int minMaxOrdinal = 0;
1:6118711:     // min/max value adding
1:dc29319:     for (int i = 0; i < updatedMinMaxValues.length; i++) {
1:6118711:       row.setByteArray(updatedMinMaxValues[i], minMaxOrdinal++);
1:6118711:     }
1:6118711:     taskMinMaxRow.setRow(row, ordinal);
1:6118711:   }
1:6118711: 
1:6118711:   /**
1:6118711:    * This method will do min/max comparison of values and update if required
1:6118711:    *
1:6118711:    * @param minMaxValueCompare1
1:6118711:    * @param minMaxValueCompare2
1:6118711:    * @param isMinValueComparison
1:6118711:    */
1:dc29319:   private byte[][] compareAndUpdateMinMax(byte[][] minMaxValueCompare1,
1:6118711:       byte[][] minMaxValueCompare2, boolean isMinValueComparison) {
1:6118711:     // Compare and update min max values
1:dc29319:     byte[][] updatedMinMaxValues = new byte[minMaxValueCompare1.length][];
1:6118711:     System.arraycopy(minMaxValueCompare1, 0, updatedMinMaxValues, 0, minMaxValueCompare1.length);
1:dc29319:     for (int i = 0; i < minMaxValueCompare1.length; i++) {
1:6118711:       int compare = ByteUtil.UnsafeComparer.INSTANCE
1:6118711:           .compareTo(minMaxValueCompare2[i], minMaxValueCompare1[i]);
1:6118711:       if (isMinValueComparison) {
1:6118711:         if (compare < 0) {
1:6118711:           updatedMinMaxValues[i] = minMaxValueCompare2[i];
1:6118711:         }
1:6118711:       } else if (compare > 0) {
1:6118711:         updatedMinMaxValues[i] = minMaxValueCompare2[i];
1:6118711:       }
1:6118711:     }
1:6118711:     return updatedMinMaxValues;
1:6118711:   }
1:6118711: 
1:dc29319:   protected void createMemorySchema(BlockletDataMapModel blockletDataMapModel)
1:dc29319:       throws MemoryException {
1:dc29319:     memoryDMStore = getMemoryDMStore(blockletDataMapModel.isAddToUnsafe());
1:6118711:   }
1:6118711: 
1:6118711:   /**
1:6118711:    * Creates the schema to store summary information or the information which can be stored only
1:6118711:    * once per datamap. It stores datamap level max/min of each column and partition information of
1:6118711:    * datamap
1:6118711:    *
1:6118711:    * @throws MemoryException
1:6118711:    */
1:dc29319:   protected void createSummaryDMStore(BlockletDataMapModel blockletDataMapModel)
2:6118711:       throws MemoryException {
1:dc29319:     taskSummaryDMStore = getMemoryDMStore(blockletDataMapModel.isAddToUnsafe());
1:6118711:   }
1:6118711: 
1:6118711:   @Override public boolean isScanRequired(FilterResolverIntf filterExp) {
1:dc29319:     FilterExecuter filterExecuter = FilterUtil
1:dc29319:         .getFilterExecuterTree(filterExp, getSegmentProperties(), null, getMinMaxCacheColumns());
1:f4a58c5:     DataMapRow unsafeRow = taskSummaryDMStore
1:f4a58c5:         .getDataMapRow(getTaskSummarySchema(), taskSummaryDMStore.getRowCount() - 1);
1:6118711:     boolean isScanRequired = FilterExpressionProcessor
1:6118711:         .isScanRequired(filterExecuter, getMinMaxValue(unsafeRow, TASK_MAX_VALUES_INDEX),
1:6118711:             getMinMaxValue(unsafeRow, TASK_MIN_VALUES_INDEX));
1:6118711:     if (isScanRequired) {
1:6118711:       return true;
1:6118711:     }
1:6118711:     return false;
1:6118711:   }
1:6118711: 
1:8e78957:   protected List<CarbonColumn> getMinMaxCacheColumns() {
1:dc29319:     return SegmentPropertiesAndSchemaHolder.getInstance()
1:dc29319:         .getSegmentPropertiesWrapper(segmentPropertiesIndex).getMinMaxCacheColumns();
1:dc29319:   }
1:dc29319: 
1:3cbabcd:   /**
1:3cbabcd:    * for CACHE_LEVEL=BLOCK, each entry in memoryDMStore is for a block
1:3cbabcd:    * if data is not legacy store, we can get blocklet count from taskSummaryDMStore
1:3cbabcd:    */
1:3cbabcd:   protected short getBlockletNumOfEntry(int index) {
1:3cbabcd:     if (isLegacyStore) {
1:3cbabcd:       // dummy value
1:3cbabcd:       return 0;
1:3cbabcd:     } else {
1:f04850f:       return ByteBuffer.wrap(getBlockletRowCountForEachBlock()).getShort(
1:f04850f:           index * CarbonCommonConstants.SHORT_SIZE_IN_BYTE);
1:3cbabcd:     }
1:3cbabcd:   }
1:3cbabcd: 
1:22958d9:   // get total block number in this datamap
1:22958d9:   protected int getTotalBlocks() {
1:22958d9:     if (isLegacyStore) {
1:22958d9:       // dummy value
1:22958d9:       return 0;
1:22958d9:     } else {
1:22958d9:       return memoryDMStore.getRowCount();
1:22958d9:     }
1:22958d9:   }
1:22958d9: 
1:3cbabcd:   // get total blocklet number in this datamap
1:3cbabcd:   protected int getTotalBlocklets() {
1:3cbabcd:     if (isLegacyStore) {
1:3cbabcd:       // dummy value
1:3cbabcd:       return 0;
1:3cbabcd:     } else {
1:3cbabcd:       ByteBuffer byteBuffer = ByteBuffer.wrap(getBlockletRowCountForEachBlock());
1:3cbabcd:       int sum = 0;
1:3cbabcd:       while (byteBuffer.hasRemaining()) {
1:3cbabcd:         sum += byteBuffer.getShort();
1:3cbabcd:       }
1:3cbabcd:       return sum;
1:3cbabcd:     }
1:3cbabcd:   }
1:3cbabcd: 
1:f4a58c5:   private List<Blocklet> prune(FilterResolverIntf filterExp) {
1:6118711:     if (memoryDMStore.getRowCount() == 0) {
1:6118711:       return new ArrayList<>();
1:6118711:     }
1:6118711:     List<Blocklet> blocklets = new ArrayList<>();
1:dc29319:     CarbonRowSchema[] schema = getFileFooterEntrySchema();
1:f4a58c5:     String filePath = getFilePath();
1:3cbabcd:     int numEntries = memoryDMStore.getRowCount();
1:3cbabcd:     int totalBlocklets = getTotalBlocklets();
1:3cbabcd:     int hitBlocklets = 0;
1:6118711:     if (filterExp == null) {
1:3cbabcd:       for (int i = 0; i < numEntries; i++) {
1:f4a58c5:         DataMapRow safeRow = memoryDMStore.getDataMapRow(schema, i).convertToSafeRow();
1:f4a58c5:         blocklets.add(createBlocklet(safeRow, getFileNameWithFilePath(safeRow, filePath),
1:4df335f:             getBlockletId(safeRow), false));
1:6118711:       }
1:3cbabcd:       hitBlocklets = totalBlocklets;
1:6118711:     } else {
1:6118711:       // Remove B-tree jump logic as start and end key prepared is not
1:6118711:       // correct for old store scenarios
1:3cbabcd:       int entryIndex = 0;
1:dc29319:       FilterExecuter filterExecuter = FilterUtil
1:dc29319:           .getFilterExecuterTree(filterExp, getSegmentProperties(), null, getMinMaxCacheColumns());
1:4df335f:       // flag to be used for deciding whether use min/max in executor pruning for BlockletDataMap
1:4df335f:       boolean useMinMaxForPruning = useMinMaxForExecutorPruning(filterExp);
1:4df335f:       // min and max for executor pruning
1:3cbabcd:       while (entryIndex < numEntries) {
1:3cbabcd:         DataMapRow safeRow = memoryDMStore.getDataMapRow(schema, entryIndex).convertToSafeRow();
1:f4a58c5:         String fileName = getFileNameWithFilePath(safeRow, filePath);
1:6118711:         short blockletId = getBlockletId(safeRow);
1:6118711:         boolean isValid =
1:6118711:             addBlockBasedOnMinMaxValue(filterExecuter, getMinMaxValue(safeRow, MAX_VALUES_INDEX),
1:f4a58c5:                 getMinMaxValue(safeRow, MIN_VALUES_INDEX), fileName, blockletId);
1:6118711:         if (isValid) {
1:4df335f:           blocklets.add(createBlocklet(safeRow, fileName, blockletId, useMinMaxForPruning));
1:3cbabcd:           hitBlocklets += getBlockletNumOfEntry(entryIndex);
1:6118711:         }
1:3cbabcd:         entryIndex++;
1:6118711:       }
1:6118711:     }
1:3cbabcd: 
1:3cbabcd:     if (isLegacyStore) {
1:3cbabcd:       ExplainCollector.setShowPruningInfo(false);
1:3cbabcd:     } else {
1:3cbabcd:       ExplainCollector.setShowPruningInfo(true);
1:3cbabcd:       ExplainCollector.addTotalBlocklets(totalBlocklets);
1:22958d9:       ExplainCollector.addTotalBlocks(getTotalBlocks());
1:3cbabcd:       ExplainCollector.addDefaultDataMapPruningHit(hitBlocklets);
1:3cbabcd:     }
1:6118711:     return blocklets;
1:6118711:   }
1:6118711: 
1:4df335f:   private boolean useMinMaxForExecutorPruning(FilterResolverIntf filterResolverIntf) {
1:4df335f:     boolean useMinMaxForPruning = false;
1:7e93d7b:     if (!isLegacyStore && this instanceof BlockletDataMap) {
1:4df335f:       useMinMaxForPruning = BlockletDataMapUtil
1:4df335f:           .useMinMaxForBlockletPruning(filterResolverIntf, getMinMaxCacheColumns());
1:4df335f:     }
1:4df335f:     return useMinMaxForPruning;
1:4df335f:   }
1:4df335f: 
1:6118711:   @Override
1:3894e1d:   public List<Blocklet> prune(Expression expression, SegmentProperties properties,
1:3894e1d:       List<PartitionSpec> partitions, AbsoluteTableIdentifier identifier) throws IOException {
1:3894e1d:     FilterResolverIntf filterResolverIntf = null;
1:3894e1d:     if (expression != null) {
1:3894e1d:       QueryModel.FilterProcessVO processVO =
1:3894e1d:           new QueryModel.FilterProcessVO(properties.getDimensions(), properties.getMeasures(),
1:3894e1d:               new ArrayList<CarbonDimension>());
1:3894e1d:       QueryModel.processFilterExpression(processVO, expression, null, null);
1:3894e1d:       // Optimize Filter Expression and fit RANGE filters is conditions apply.
1:3894e1d:       FilterOptimizer rangeFilterOptimizer = new RangeFilterOptmizer(expression);
1:3894e1d:       rangeFilterOptimizer.optimizeFilter();
1:3894e1d:       filterResolverIntf = CarbonTable.resolveFilter(expression, identifier);
1:3894e1d:     }
1:3894e1d:     return prune(filterResolverIntf, properties, partitions);
1:3894e1d:   }
1:3894e1d: 
1:3894e1d:   @Override
1:6118711:   public List<Blocklet> prune(FilterResolverIntf filterExp, SegmentProperties segmentProperties,
1:6118711:       List<PartitionSpec> partitions) {
1:6118711:     if (memoryDMStore.getRowCount() == 0) {
1:6118711:       return new ArrayList<>();
1:6118711:     }
1:6118711:     // if it has partitioned datamap but there is no partitioned information stored, it means
1:6118711:     // partitions are dropped so return empty list.
1:6118711:     if (partitions != null) {
1:6118711:       if (!validatePartitionInfo(partitions)) {
1:6118711:         return new ArrayList<>();
1:6118711:       }
1:6118711:     }
1:6118711:     // Prune with filters if the partitions are existed in this datamap
1:6118711:     // changed segmentProperties to this.segmentProperties to make sure the pruning with its own
1:6118711:     // segmentProperties.
1:6118711:     // Its a temporary fix. The Interface DataMap.prune(FilterResolverIntf filterExp,
1:6118711:     // SegmentProperties segmentProperties, List<PartitionSpec> partitions) should be corrected
1:f4a58c5:     return prune(filterExp);
1:6118711:   }
1:6118711: 
1:6118711:   private boolean validatePartitionInfo(List<PartitionSpec> partitions) {
1:6118711:     // First get the partitions which are stored inside datamap.
1:6118711:     String[] fileDetails = getFileDetails();
1:6118711:     // Check the exact match of partition information inside the stored partitions.
1:6118711:     boolean found = false;
1:6118711:     Path folderPath = new Path(fileDetails[0]);
1:6118711:     for (PartitionSpec spec : partitions) {
1:6118711:       if (folderPath.equals(spec.getLocation()) && isCorrectUUID(fileDetails, spec)) {
1:6118711:         found = true;
1:6118711:         break;
1:6118711:       }
1:6118711:     }
1:6118711:     return found;
1:6118711:   }
1:6118711: 
1:6118711:   @Override public void finish() {
1:6118711: 
1:6118711:   }
1:6118711: 
1:6118711:   private boolean isCorrectUUID(String[] fileDetails, PartitionSpec spec) {
1:6118711:     boolean needToScan = false;
1:6118711:     if (spec.getUuid() != null) {
1:6118711:       String[] split = spec.getUuid().split("_");
1:6118711:       if (split[0].equals(fileDetails[2]) && CarbonTablePath.DataFileUtil
1:6118711:           .getTimeStampFromFileName(fileDetails[1]).equals(split[1])) {
1:6118711:         needToScan = true;
1:6118711:       }
1:6118711:     } else {
1:6118711:       needToScan = true;
1:6118711:     }
1:6118711:     return needToScan;
1:6118711:   }
1:6118711: 
1:6118711:   /**
1:6118711:    * select the blocks based on column min and max value
1:6118711:    *
1:6118711:    * @param filterExecuter
1:6118711:    * @param maxValue
1:6118711:    * @param minValue
1:6118711:    * @param filePath
1:6118711:    * @param blockletId
1:6118711:    * @return
1:6118711:    */
1:6118711:   private boolean addBlockBasedOnMinMaxValue(FilterExecuter filterExecuter, byte[][] maxValue,
1:6118711:       byte[][] minValue, String filePath, int blockletId) {
1:6118711:     BitSet bitSet = null;
1:6118711:     if (filterExecuter instanceof ImplicitColumnFilterExecutor) {
1:6118711:       String uniqueBlockPath = filePath.substring(filePath.lastIndexOf("/Part") + 1);
1:6118711:       // this case will come in case of old store where index file does not contain the
1:6118711:       // blocklet information
1:6118711:       if (blockletId != -1) {
1:6118711:         uniqueBlockPath = uniqueBlockPath + CarbonCommonConstants.FILE_SEPARATOR + blockletId;
1:6118711:       }
1:6118711:       bitSet = ((ImplicitColumnFilterExecutor) filterExecuter)
1:6118711:           .isFilterValuesPresentInBlockOrBlocklet(maxValue, minValue, uniqueBlockPath);
1:6118711:     } else {
1:6118711:       bitSet = filterExecuter.isScanRequired(maxValue, minValue);
1:6118711:     }
1:6118711:     if (!bitSet.isEmpty()) {
1:6118711:       return true;
1:6118711:     } else {
1:6118711:       return false;
1:6118711:     }
1:6118711:   }
1:6118711: 
1:6118711:   public ExtendedBlocklet getDetailedBlocklet(String blockletId) {
1:6118711:     if (isLegacyStore) {
1:6118711:       throw new UnsupportedOperationException("With legacy store only BlockletDataMap is allowed."
1:6118711:           + " In order to use other dataMaps upgrade to new store.");
1:6118711:     }
1:6118711:     int absoluteBlockletId = Integer.parseInt(blockletId);
1:6118711:     return createBlockletFromRelativeBlockletId(absoluteBlockletId);
1:6118711:   }
1:6118711: 
1:6118711:   /**
1:6118711:    * Method to get the relative blocklet ID. Absolute blocklet ID is the blocklet Id as per
1:6118711:    * task level but relative blocklet ID is id as per carbondata file/block level
1:6118711:    *
1:6118711:    * @param absoluteBlockletId
1:6118711:    * @return
1:6118711:    */
1:6118711:   private ExtendedBlocklet createBlockletFromRelativeBlockletId(int absoluteBlockletId) {
1:6118711:     short relativeBlockletId = -1;
1:6118711:     int rowIndex = 0;
1:6118711:     // return 0 if absoluteBlockletId is 0
1:6118711:     if (absoluteBlockletId == 0) {
1:6118711:       relativeBlockletId = (short) absoluteBlockletId;
1:6118711:     } else {
1:6118711:       int diff = absoluteBlockletId;
1:1cea4d3:       ByteBuffer byteBuffer = ByteBuffer.wrap(getBlockletRowCountForEachBlock());
1:6118711:       // Example: absoluteBlockletID = 17, blockletRowCountForEachBlock = {4,3,2,5,7}
1:6118711:       // step1: diff = 17-4, diff = 13
1:6118711:       // step2: diff = 13-3, diff = 10
1:6118711:       // step3: diff = 10-2, diff = 8
1:6118711:       // step4: diff = 8-5, diff = 3
1:6118711:       // step5: diff = 3-7, diff = -4 (satisfies <= 0)
1:6118711:       // step6: relativeBlockletId = -4+7, relativeBlockletId = 3 (4th index starting from 0)
1:1cea4d3:       while (byteBuffer.hasRemaining()) {
1:1cea4d3:         short blockletCount = byteBuffer.getShort();
1:6118711:         diff = diff - blockletCount;
1:6118711:         if (diff < 0) {
1:6118711:           relativeBlockletId = (short) (diff + blockletCount);
1:6118711:           break;
1:6118711:         }
1:6118711:         rowIndex++;
1:6118711:       }
1:6118711:     }
1:dc29319:     DataMapRow safeRow =
1:dc29319:         memoryDMStore.getDataMapRow(getFileFooterEntrySchema(), rowIndex).convertToSafeRow();
1:f4a58c5:     String filePath = getFilePath();
1:4df335f:     return createBlocklet(safeRow, getFileNameWithFilePath(safeRow, filePath), relativeBlockletId,
1:4df335f:         false);
1:6118711:   }
1:6118711: 
1:6118711:   private byte[] getBlockletRowCountForEachBlock() {
1:6118711:     // taskSummary DM store will  have only one row
1:f4a58c5:     CarbonRowSchema[] taskSummarySchema = getTaskSummarySchema();
1:f4a58c5:     return taskSummaryDMStore
1:f4a58c5:         .getDataMapRow(taskSummarySchema, taskSummaryDMStore.getRowCount() - 1)
1:f4a58c5:         .getByteArray(taskSummarySchema.length - 1);
1:6118711:   }
1:6118711: 
1:6118711:   /**
1:6118711:    * Get the index file name of the blocklet data map
1:6118711:    *
1:6118711:    * @return
1:6118711:    */
1:f4a58c5:   public String getTableTaskInfo(int index) {
1:f4a58c5:     DataMapRow unsafeRow = taskSummaryDMStore.getDataMapRow(getTaskSummarySchema(), 0);
1:6118711:     try {
1:f4a58c5:       return new String(unsafeRow.getByteArray(index), CarbonCommonConstants.DEFAULT_CHARSET);
1:6118711:     } catch (UnsupportedEncodingException e) {
1:6118711:       // should never happen!
1:6118711:       throw new IllegalArgumentException("UTF8 encoding is not supported", e);
1:6118711:     }
1:6118711:   }
1:6118711: 
1:6118711:   private byte[][] getMinMaxValue(DataMapRow row, int index) {
1:6118711:     DataMapRow minMaxRow = row.getRow(index);
1:6118711:     byte[][] minMax = new byte[minMaxRow.getColumnCount()][];
1:6118711:     for (int i = 0; i < minMax.length; i++) {
1:6118711:       minMax[i] = minMaxRow.getByteArray(i);
1:6118711:     }
1:6118711:     return minMax;
1:6118711:   }
1:6118711: 
1:6118711:   protected short getBlockletId(DataMapRow dataMapRow) {
1:6118711:     return BLOCK_DEFAULT_BLOCKLET_ID;
1:6118711:   }
1:6118711: 
1:4df335f:   protected ExtendedBlocklet createBlocklet(DataMapRow row, String fileName, short blockletId,
1:4df335f:       boolean useMinMaxForPruning) {
1:f4a58c5:     ExtendedBlocklet blocklet = new ExtendedBlocklet(fileName, blockletId + "", false);
1:6118711:     BlockletDetailInfo detailInfo = getBlockletDetailInfo(row, blockletId, blocklet);
1:6118711:     detailInfo.setBlockletInfoBinary(new byte[0]);
1:6118711:     blocklet.setDetailInfo(detailInfo);
1:6118711:     return blocklet;
1:6118711:   }
1:6118711: 
1:6118711:   protected BlockletDetailInfo getBlockletDetailInfo(DataMapRow row, short blockletId,
1:6118711:       ExtendedBlocklet blocklet) {
1:6118711:     BlockletDetailInfo detailInfo = new BlockletDetailInfo();
1:6118711:     detailInfo.setRowCount(row.getInt(ROW_COUNT_INDEX));
1:6118711:     detailInfo.setVersionNumber(row.getShort(VERSION_INDEX));
1:6118711:     detailInfo.setBlockletId(blockletId);
1:f4a58c5:     detailInfo.setDimLens(getColumnCardinality());
1:6118711:     detailInfo.setSchemaUpdatedTimeStamp(row.getLong(SCHEMA_UPADATED_TIME_INDEX));
1:6118711:     try {
1:6118711:       blocklet.setLocation(
1:6118711:           new String(row.getByteArray(LOCATIONS), CarbonCommonConstants.DEFAULT_CHARSET)
1:6118711:               .split(","));
1:6118711:     } catch (IOException e) {
1:6118711:       throw new RuntimeException(e);
1:6118711:     }
1:6118711:     detailInfo.setBlockFooterOffset(row.getLong(BLOCK_FOOTER_OFFSET));
1:6118711:     detailInfo.setBlockSize(row.getLong(BLOCK_LENGTH));
1:6118711:     detailInfo.setLegacyStore(isLegacyStore);
1:6118711:     return detailInfo;
1:6118711:   }
1:6118711: 
1:6118711:   private String[] getFileDetails() {
1:6118711:     try {
1:6118711:       String[] fileDetails = new String[3];
1:f4a58c5:       DataMapRow unsafeRow = taskSummaryDMStore.getDataMapRow(getTaskSummarySchema(), 0);
1:6118711:       fileDetails[0] = new String(unsafeRow.getByteArray(SUMMARY_INDEX_PATH),
2:6118711:           CarbonCommonConstants.DEFAULT_CHARSET);
1:6118711:       fileDetails[1] = new String(unsafeRow.getByteArray(SUMMARY_INDEX_FILE_NAME),
1:6118711:           CarbonCommonConstants.DEFAULT_CHARSET);
1:6118711:       fileDetails[2] = new String(unsafeRow.getByteArray(SUMMARY_SEGMENTID),
1:6118711:           CarbonCommonConstants.DEFAULT_CHARSET);
1:6118711:       return fileDetails;
1:6118711:     } catch (Exception e) {
1:6118711:       throw new RuntimeException(e);
1:6118711:     }
1:6118711:   }
1:6118711: 
1:6118711:   @Override public void clear() {
1:6118711:     if (memoryDMStore != null) {
1:6118711:       memoryDMStore.freeMemory();
1:6118711:       memoryDMStore = null;
1:6118711:     }
1:6118711:     // clear task min/max unsafe memory
1:6118711:     if (null != taskSummaryDMStore) {
1:6118711:       taskSummaryDMStore.freeMemory();
1:6118711:       taskSummaryDMStore = null;
1:6118711:     }
1:6118711:   }
1:6118711: 
1:6118711:   public long getMemorySize() {
1:6118711:     long memoryUsed = 0L;
1:6118711:     if (memoryDMStore != null) {
1:6118711:       memoryUsed += memoryDMStore.getMemoryUsed();
1:6118711:     }
1:6118711:     if (null != taskSummaryDMStore) {
1:6118711:       memoryUsed += taskSummaryDMStore.getMemoryUsed();
1:6118711:     }
1:6118711:     return memoryUsed;
1:6118711:   }
1:6118711: 
1:f4a58c5:   protected SegmentProperties getSegmentProperties() {
2:f4a58c5:     return SegmentPropertiesAndSchemaHolder.getInstance()
1:f4a58c5:         .getSegmentProperties(segmentPropertiesIndex);
1:6118711:   }
1:6118711: 
1:6118711:   public int[] getColumnCardinality() {
1:f4a58c5:     return SegmentPropertiesAndSchemaHolder.getInstance()
1:f4a58c5:         .getSegmentPropertiesWrapper(segmentPropertiesIndex).getColumnCardinality();
1:6118711:   }
1:6118711: 
1:f4a58c5:   public List<ColumnSchema> getColumnSchema() {
1:f4a58c5:     return SegmentPropertiesAndSchemaHolder.getInstance()
1:f4a58c5:         .getSegmentPropertiesWrapper(segmentPropertiesIndex).getColumnsInTable();
1:f4a58c5:   }
1:f4a58c5: 
1:f4a58c5:   protected AbstractMemoryDMStore getMemoryDMStore(boolean addToUnsafe)
1:6118711:       throws MemoryException {
1:6118711:     AbstractMemoryDMStore memoryDMStore;
1:6118711:     if (addToUnsafe) {
1:f4a58c5:       memoryDMStore = new UnsafeMemoryDMStore();
1:6118711:     } else {
1:f4a58c5:       memoryDMStore = new SafeMemoryDMStore();
1:6118711:     }
1:6118711:     return memoryDMStore;
1:f4a58c5:   }
1:f4a58c5: 
1:dc29319:   protected CarbonRowSchema[] getFileFooterEntrySchema() {
1:f4a58c5:     return SegmentPropertiesAndSchemaHolder.getInstance()
1:dc29319:         .getSegmentPropertiesWrapper(segmentPropertiesIndex).getBlockFileFooterEntrySchema();
1:6118711:   }
1:6118711: 
1:f4a58c5:   protected CarbonRowSchema[] getTaskSummarySchema() {
1:dc29319:     SegmentPropertiesAndSchemaHolder.SegmentPropertiesWrapper segmentPropertiesWrapper =
1:dc29319:         SegmentPropertiesAndSchemaHolder.getInstance()
1:dc29319:             .getSegmentPropertiesWrapper(segmentPropertiesIndex);
1:dc29319:     try {
1:dc29319:       return segmentPropertiesWrapper.getTaskSummarySchema(!isLegacyStore, isFilePathStored);
1:dc29319:     } catch (MemoryException e) {
1:dc29319:       throw new RuntimeException(e);
1:dc29319:     }
1:f4a58c5:   }
1:f4a58c5: 
1:6118711:   /**
1:6118711:    * This method will ocnvert safe to unsafe memory DM store
1:6118711:    *
1:6118711:    * @throws MemoryException
1:6118711:    */
1:6118711:   public void convertToUnsafeDMStore() throws MemoryException {
1:6118711:     if (memoryDMStore instanceof SafeMemoryDMStore) {
1:dc29319:       UnsafeMemoryDMStore unsafeMemoryDMStore = memoryDMStore.convertToUnsafeDMStore(
1:dc29319:           getFileFooterEntrySchema());
1:6118711:       memoryDMStore.freeMemory();
1:6118711:       memoryDMStore = unsafeMemoryDMStore;
1:6118711:     }
1:6118711:     if (taskSummaryDMStore instanceof SafeMemoryDMStore) {
1:f4a58c5:       UnsafeMemoryDMStore unsafeSummaryMemoryDMStore =
1:f4a58c5:           taskSummaryDMStore.convertToUnsafeDMStore(getTaskSummarySchema());
1:6118711:       taskSummaryDMStore.freeMemory();
1:6118711:       taskSummaryDMStore = unsafeSummaryMemoryDMStore;
1:6118711:     }
1:6118711:   }
1:6118711: 
1:f4a58c5:   public void setSegmentPropertiesIndex(int segmentPropertiesIndex) {
1:f4a58c5:     this.segmentPropertiesIndex = segmentPropertiesIndex;
1:6118711:   }
1:6118711: 
1:f4a58c5:   public int getSegmentPropertiesIndex() {
1:f4a58c5:     return segmentPropertiesIndex;
1:6118711:   }
1:6118711: }
============================================================================
author:Manhua
-------------------------------------------------------------------------------
commit:22958d9
/////////////////////////////////////////////////////////////////////////
1:   // get total block number in this datamap
1:   protected int getTotalBlocks() {
1:     if (isLegacyStore) {
1:       // dummy value
1:       return 0;
1:     } else {
1:       return memoryDMStore.getRowCount();
1:     }
1:   }
1: 
/////////////////////////////////////////////////////////////////////////
1:       ExplainCollector.addTotalBlocks(getTotalBlocks());
commit:f04850f
/////////////////////////////////////////////////////////////////////////
1:       return ByteBuffer.wrap(getBlockletRowCountForEachBlock()).getShort(
1:           index * CarbonCommonConstants.SHORT_SIZE_IN_BYTE);
commit:3cbabcd
/////////////////////////////////////////////////////////////////////////
1: import java.util.*;
/////////////////////////////////////////////////////////////////////////
1:   /**
1:    * for CACHE_LEVEL=BLOCK, each entry in memoryDMStore is for a block
1:    * if data is not legacy store, we can get blocklet count from taskSummaryDMStore
1:    */
1:   protected short getBlockletNumOfEntry(int index) {
1:     if (isLegacyStore) {
1:       // dummy value
1:       return 0;
1:     } else {
0:       return ByteBuffer.wrap(getBlockletRowCountForEachBlock()).getShort(index);
1:     }
1:   }
1: 
1:   // get total blocklet number in this datamap
1:   protected int getTotalBlocklets() {
1:     if (isLegacyStore) {
1:       // dummy value
1:       return 0;
1:     } else {
1:       ByteBuffer byteBuffer = ByteBuffer.wrap(getBlockletRowCountForEachBlock());
1:       int sum = 0;
1:       while (byteBuffer.hasRemaining()) {
1:         sum += byteBuffer.getShort();
1:       }
1:       return sum;
1:     }
1:   }
1: 
/////////////////////////////////////////////////////////////////////////
1:     int numEntries = memoryDMStore.getRowCount();
1:     int totalBlocklets = getTotalBlocklets();
1:     int hitBlocklets = 0;
1:       for (int i = 0; i < numEntries; i++) {
1:       hitBlocklets = totalBlocklets;
1:       int entryIndex = 0;
1:       while (entryIndex < numEntries) {
1:         DataMapRow safeRow = memoryDMStore.getDataMapRow(schema, entryIndex).convertToSafeRow();
/////////////////////////////////////////////////////////////////////////
1:           hitBlocklets += getBlockletNumOfEntry(entryIndex);
1:         entryIndex++;
1: 
1:     if (isLegacyStore) {
1:       ExplainCollector.setShowPruningInfo(false);
1:     } else {
1:       ExplainCollector.setShowPruningInfo(true);
1:       ExplainCollector.addTotalBlocklets(totalBlocklets);
1:       ExplainCollector.addDefaultDataMapPruningHit(hitBlocklets);
1:     }
author:kunal642
-------------------------------------------------------------------------------
commit:8f1a029
/////////////////////////////////////////////////////////////////////////
1:   @Override public void init(DataMapModel dataMapModel)
1:       throws IOException, MemoryException {
1:     DataFileFooterConverter fileFooterConverter =
1:         new DataFileFooterConverter(dataMapModel.getConfiguration());
author:ravipesala
-------------------------------------------------------------------------------
commit:3894e1d
/////////////////////////////////////////////////////////////////////////
1: import org.apache.carbondata.core.metadata.AbsoluteTableIdentifier;
1: import org.apache.carbondata.core.metadata.schema.table.CarbonTable;
1: import org.apache.carbondata.core.metadata.schema.table.column.CarbonDimension;
1: import org.apache.carbondata.core.scan.expression.Expression;
1: import org.apache.carbondata.core.scan.filter.intf.FilterOptimizer;
1: import org.apache.carbondata.core.scan.filter.optimizer.RangeFilterOptmizer;
1: import org.apache.carbondata.core.scan.model.QueryModel;
/////////////////////////////////////////////////////////////////////////
1:         .getIndexInfo(blockletDataMapInfo.getFilePath(), blockletDataMapInfo.getFileData(),
1:             blockletDataMapInfo.getCarbonTable().isTransactionalTable());
/////////////////////////////////////////////////////////////////////////
1:   public List<Blocklet> prune(Expression expression, SegmentProperties properties,
1:       List<PartitionSpec> partitions, AbsoluteTableIdentifier identifier) throws IOException {
1:     FilterResolverIntf filterResolverIntf = null;
1:     if (expression != null) {
1:       QueryModel.FilterProcessVO processVO =
1:           new QueryModel.FilterProcessVO(properties.getDimensions(), properties.getMeasures(),
1:               new ArrayList<CarbonDimension>());
1:       QueryModel.processFilterExpression(processVO, expression, null, null);
1:       // Optimize Filter Expression and fit RANGE filters is conditions apply.
1:       FilterOptimizer rangeFilterOptimizer = new RangeFilterOptmizer(expression);
1:       rangeFilterOptimizer.optimizeFilter();
1:       filterResolverIntf = CarbonTable.resolveFilter(expression, identifier);
1:     }
1:     return prune(filterResolverIntf, properties, partitions);
1:   }
1: 
1:   @Override
author:akashrn5
-------------------------------------------------------------------------------
commit:7e93d7b
/////////////////////////////////////////////////////////////////////////
1:     if (!isLegacyStore && this instanceof BlockletDataMap) {
author:xuchuanyin
-------------------------------------------------------------------------------
commit:1cea4d3
/////////////////////////////////////////////////////////////////////////
1: import java.nio.ByteBuffer;
/////////////////////////////////////////////////////////////////////////
/////////////////////////////////////////////////////////////////////////
1:     List<Short> blockletCountInEachBlock = new ArrayList<>(indexInfo.size());
1:     short totalBlockletsInOneBlock = 0;
/////////////////////////////////////////////////////////////////////////
1:     byte[] blockletCount = convertRowCountFromShortToByteArray(blockletCountInEachBlock);
1:   private byte[] convertRowCountFromShortToByteArray(List<Short> blockletCountInEachBlock) {
1:     int bufferSize = blockletCountInEachBlock.size() * 2;
1:     ByteBuffer byteBuffer = ByteBuffer.allocate(bufferSize);
1:     for (Short blockletCount : blockletCountInEachBlock) {
1:       byteBuffer.putShort(blockletCount);
1:     }
1:     byteBuffer.rewind();
1:     return byteBuffer.array();
1:   }
1: 
/////////////////////////////////////////////////////////////////////////
1:       ByteBuffer byteBuffer = ByteBuffer.wrap(getBlockletRowCountForEachBlock());
/////////////////////////////////////////////////////////////////////////
1:       while (byteBuffer.hasRemaining()) {
1:         short blockletCount = byteBuffer.getShort();
author:manishgupta88
-------------------------------------------------------------------------------
commit:4df335f
/////////////////////////////////////////////////////////////////////////
1:             getBlockletId(safeRow), false));
/////////////////////////////////////////////////////////////////////////
1:       // flag to be used for deciding whether use min/max in executor pruning for BlockletDataMap
1:       boolean useMinMaxForPruning = useMinMaxForExecutorPruning(filterExp);
1:       // min and max for executor pruning
/////////////////////////////////////////////////////////////////////////
1:           blocklets.add(createBlocklet(safeRow, fileName, blockletId, useMinMaxForPruning));
/////////////////////////////////////////////////////////////////////////
1:   private boolean useMinMaxForExecutorPruning(FilterResolverIntf filterResolverIntf) {
1:     boolean useMinMaxForPruning = false;
0:     if (this instanceof BlockletDataMap) {
1:       useMinMaxForPruning = BlockletDataMapUtil
1:           .useMinMaxForBlockletPruning(filterResolverIntf, getMinMaxCacheColumns());
1:     }
1:     return useMinMaxForPruning;
1:   }
1: 
/////////////////////////////////////////////////////////////////////////
1:     return createBlocklet(safeRow, getFileNameWithFilePath(safeRow, filePath), relativeBlockletId,
1:         false);
/////////////////////////////////////////////////////////////////////////
1:   protected ExtendedBlocklet createBlocklet(DataMapRow row, String fileName, short blockletId,
1:       boolean useMinMaxForPruning) {
commit:dc29319
/////////////////////////////////////////////////////////////////////////
1: import java.io.*;
/////////////////////////////////////////////////////////////////////////
1: import org.apache.carbondata.core.metadata.schema.table.column.CarbonColumn;
/////////////////////////////////////////////////////////////////////////
1:       createMemorySchema(blockletDataMapInfo);
1:       createSummaryDMStore(blockletDataMapInfo);
1:       CarbonRowSchema[] taskSummarySchema = getTaskSummarySchema();
1:       DataMapRowImpl summaryRow =
1:           loadMetadata(taskSummarySchema, segmentProperties, blockletDataMapInfo, indexInfo);
1:       finishWriting(taskSummarySchema, filePath, fileName, segmentId, summaryRow);
/////////////////////////////////////////////////////////////////////////
1:   private void finishWriting(CarbonRowSchema[] taskSummarySchema, byte[] filePath, byte[] fileName,
1:       byte[] segmentId, DataMapRowImpl summaryRow) throws MemoryException {
1:       addTaskSummaryRowToUnsafeMemoryStore(taskSummarySchema, summaryRow, filePath, fileName,
1:           segmentId);
/////////////////////////////////////////////////////////////////////////
1:   protected DataMapRowImpl loadMetadata(CarbonRowSchema[] taskSummarySchema,
1:       SegmentProperties segmentProperties, BlockletDataMapModel blockletDataMapInfo,
1:       List<DataFileFooter> indexInfo) throws IOException, MemoryException {
1:       return loadBlockInfoForOldStore(taskSummarySchema, segmentProperties, blockletDataMapInfo,
1:           indexInfo);
1:       return loadBlockMetaInfo(taskSummarySchema, segmentProperties, blockletDataMapInfo,
1:           indexInfo);
/////////////////////////////////////////////////////////////////////////
1:         .addSegmentProperties(blockletDataMapInfo.getCarbonTable(),
/////////////////////////////////////////////////////////////////////////
1:   protected DataMapRowImpl loadBlockInfoForOldStore(CarbonRowSchema[] taskSummarySchema,
1:       SegmentProperties segmentProperties, BlockletDataMapModel blockletDataMapInfo,
1:       List<DataFileFooter> indexInfo) throws IOException, MemoryException {
1:     CarbonRowSchema[] schema = getFileFooterEntrySchema();
/////////////////////////////////////////////////////////////////////////
1:         summaryRow = loadToUnsafeBlock(schema, taskSummarySchema, fileFooter, segmentProperties,
0:             blockletDataMapInfo.getMinMaxCacheColumns(), blockInfo.getFilePath(), summaryRow,
1:             blockMetaInfo, updatedMinValues, updatedMaxValues);
/////////////////////////////////////////////////////////////////////////
1:   private DataMapRowImpl loadBlockMetaInfo(CarbonRowSchema[] taskSummarySchema,
1:       SegmentProperties segmentProperties, BlockletDataMapModel blockletDataMapInfo,
1:       List<DataFileFooter> indexInfo) throws IOException, MemoryException {
/////////////////////////////////////////////////////////////////////////
1:     CarbonRowSchema[] schema = getFileFooterEntrySchema();
/////////////////////////////////////////////////////////////////////////
1:           blockMinValues =
1:               compareAndUpdateMinMax(currentFooterMinMaxIndex.getMinValues(), blockMinValues, true);
1:           blockMaxValues =
1:               compareAndUpdateMinMax(currentFooterMinMaxIndex.getMaxValues(), blockMaxValues,
1:                   false);
/////////////////////////////////////////////////////////////////////////
1:           summaryRow = loadToUnsafeBlock(schema, taskSummarySchema, previousDataFileFooter,
0:               segmentProperties, blockletDataMapInfo.getMinMaxCacheColumns(),
/////////////////////////////////////////////////////////////////////////
1:       summaryRow =
1:           loadToUnsafeBlock(schema, taskSummarySchema, previousDataFileFooter, segmentProperties,
0:               blockletDataMapInfo.getMinMaxCacheColumns(),
1:               previousDataFileFooter.getBlockInfo().getTableBlockInfo().getFilePath(), summaryRow,
1:               blockletDataMapInfo.getBlockMetaInfoMap()
1:                   .get(previousDataFileFooter.getBlockInfo().getTableBlockInfo().getFilePath()),
1:               blockMinValues, blockMaxValues);
1:     summaryRow.setByteArray(blockletCount, taskSummarySchema.length - 1);
/////////////////////////////////////////////////////////////////////////
1:   protected DataMapRowImpl loadToUnsafeBlock(CarbonRowSchema[] schema,
1:       CarbonRowSchema[] taskSummarySchema, DataFileFooter fileFooter,
1:       SegmentProperties segmentProperties, List<CarbonColumn> minMaxCacheColumns, String filePath,
1:       DataMapRowImpl summaryRow, BlockMetaInfo blockMetaInfo, byte[][] minValues,
1:       byte[][] maxValues) {
/////////////////////////////////////////////////////////////////////////
1:     // get min max values for columns to be cached
1:     byte[][] minValuesForColumnsToBeCached = BlockletDataMapUtil
1:         .getMinMaxForColumnsToBeCached(segmentProperties, minMaxCacheColumns, minValues);
1:     byte[][] maxValuesForColumnsToBeCached = BlockletDataMapUtil
1:         .getMinMaxForColumnsToBeCached(segmentProperties, minMaxCacheColumns, maxValues);
1:     row.setRow(addMinMax(schema[ordinal], minValuesForColumnsToBeCached), ordinal);
1:     addTaskMinMaxValues(summaryRow, taskSummarySchema, taskMinMaxOrdinal,
1:         minValuesForColumnsToBeCached, TASK_MIN_VALUES_INDEX, true);
1:     row.setRow(addMinMax(schema[ordinal], maxValuesForColumnsToBeCached), ordinal);
1:     addTaskMinMaxValues(summaryRow, taskSummarySchema, taskMinMaxOrdinal,
1:         maxValuesForColumnsToBeCached, TASK_MAX_VALUES_INDEX, false);
/////////////////////////////////////////////////////////////////////////
1:   private void addTaskSummaryRowToUnsafeMemoryStore(CarbonRowSchema[] taskSummarySchema,
1:       DataMapRow summaryRow, byte[] filePath, byte[] fileName, byte[] segmentId) {
/////////////////////////////////////////////////////////////////////////
1:         taskSummaryDMStore.addIndexRow(taskSummarySchema, summaryRow);
1:   protected DataMapRow addMinMax(CarbonRowSchema carbonRowSchema,
1:     for (int i = 0; i < minValues.length; i++) {
/////////////////////////////////////////////////////////////////////////
1:   protected void addTaskMinMaxValues(DataMapRow taskMinMaxRow, CarbonRowSchema[] carbonRowSchema,
1:       int taskMinMaxOrdinal, byte[][] minMaxValue, int ordinal, boolean isMinValueComparison) {
/////////////////////////////////////////////////////////////////////////
1:       updatedMinMaxValues =
1:           compareAndUpdateMinMax(minMaxValue, existingMinMaxValues, isMinValueComparison);
1:     for (int i = 0; i < updatedMinMaxValues.length; i++) {
/////////////////////////////////////////////////////////////////////////
1:   private byte[][] compareAndUpdateMinMax(byte[][] minMaxValueCompare1,
1:     byte[][] updatedMinMaxValues = new byte[minMaxValueCompare1.length][];
1:     for (int i = 0; i < minMaxValueCompare1.length; i++) {
/////////////////////////////////////////////////////////////////////////
1:   protected void createMemorySchema(BlockletDataMapModel blockletDataMapModel)
1:       throws MemoryException {
1:     memoryDMStore = getMemoryDMStore(blockletDataMapModel.isAddToUnsafe());
/////////////////////////////////////////////////////////////////////////
1:   protected void createSummaryDMStore(BlockletDataMapModel blockletDataMapModel)
1:     taskSummaryDMStore = getMemoryDMStore(blockletDataMapModel.isAddToUnsafe());
1:     FilterExecuter filterExecuter = FilterUtil
1:         .getFilterExecuterTree(filterExp, getSegmentProperties(), null, getMinMaxCacheColumns());
/////////////////////////////////////////////////////////////////////////
0:   private List<CarbonColumn> getMinMaxCacheColumns() {
1:     return SegmentPropertiesAndSchemaHolder.getInstance()
1:         .getSegmentPropertiesWrapper(segmentPropertiesIndex).getMinMaxCacheColumns();
1:   }
1: 
1:     CarbonRowSchema[] schema = getFileFooterEntrySchema();
/////////////////////////////////////////////////////////////////////////
1:       FilterExecuter filterExecuter = FilterUtil
1:           .getFilterExecuterTree(filterExp, getSegmentProperties(), null, getMinMaxCacheColumns());
/////////////////////////////////////////////////////////////////////////
1:     DataMapRow safeRow =
1:         memoryDMStore.getDataMapRow(getFileFooterEntrySchema(), rowIndex).convertToSafeRow();
/////////////////////////////////////////////////////////////////////////
1:   protected CarbonRowSchema[] getFileFooterEntrySchema() {
1:         .getSegmentPropertiesWrapper(segmentPropertiesIndex).getBlockFileFooterEntrySchema();
1:     SegmentPropertiesAndSchemaHolder.SegmentPropertiesWrapper segmentPropertiesWrapper =
1:         SegmentPropertiesAndSchemaHolder.getInstance()
1:             .getSegmentPropertiesWrapper(segmentPropertiesIndex);
1:     try {
1:       return segmentPropertiesWrapper.getTaskSummarySchema(!isLegacyStore, isFilePathStored);
1:     } catch (MemoryException e) {
1:       throw new RuntimeException(e);
1:     }
/////////////////////////////////////////////////////////////////////////
1:       UnsafeMemoryDMStore unsafeMemoryDMStore = memoryDMStore.convertToUnsafeDMStore(
1:           getFileFooterEntrySchema());
commit:f4a58c5
/////////////////////////////////////////////////////////////////////////
0: import java.io.IOException;
0: import java.io.Serializable;
0: import java.io.UnsupportedEncodingException;
/////////////////////////////////////////////////////////////////////////
1: import org.apache.carbondata.core.datastore.block.SegmentPropertiesAndSchemaHolder;
/////////////////////////////////////////////////////////////////////////
/////////////////////////////////////////////////////////////////////////
1:   /**
1:    * store which will hold all the block or blocklet entries in one task
1:    */
1:   /**
1:    * task summary holder store
1:    */
1:   /**
1:    * index of segmentProperties in the segmentProperties holder
1:    */
1:   protected int segmentPropertiesIndex;
1:   /**
1:    * flag to be used for forming the complete file path from file name. It will be true in case of
1:    * partition table and non transactional table
1:    */
1:   protected boolean isFilePathStored;
/////////////////////////////////////////////////////////////////////////
1:     // store file path only in case of partition table, non transactional table and flat folder
1:     // structure
1:     byte[] filePath = null;
1:     boolean isPartitionTable = blockletDataMapInfo.getCarbonTable().isHivePartitionTable();
1:     if (isPartitionTable || !blockletDataMapInfo.getCarbonTable().isTransactionalTable()
1:         || blockletDataMapInfo.getCarbonTable().isSupportFlatFolder()) {
1:       filePath = path.getParent().toString().getBytes(CarbonCommonConstants.DEFAULT_CHARSET);
1:       isFilePathStored = true;
1:     }
1:       SegmentProperties segmentProperties = initSegmentProperties(blockletDataMapInfo, fileFooter);
0:       createMemoryDMStore(blockletDataMapInfo.isAddToUnsafe());
0:       createSummarySchema(segmentProperties, blockletDataMapInfo.isAddToUnsafe());
1:       // check for legacy store and load the metadata
0:       DataMapRowImpl summaryRow = loadMetadata(segmentProperties, blockletDataMapInfo, indexInfo);
0:       finishWriting(filePath, fileName, segmentId, summaryRow);
/////////////////////////////////////////////////////////////////////////
0:       DataMapRowImpl summaryRow) throws MemoryException {
0:       addTaskSummaryRowToUnsafeMemoryStore(summaryRow, filePath, fileName, segmentId);
/////////////////////////////////////////////////////////////////////////
1:    * @param segmentProperties
0:   protected DataMapRowImpl loadMetadata(SegmentProperties segmentProperties,
0:       BlockletDataMapModel blockletDataMapInfo, List<DataFileFooter> indexInfo)
0:       throws IOException, MemoryException {
0:       return loadBlockInfoForOldStore(segmentProperties, blockletDataMapInfo, indexInfo);
0:       return loadBlockMetaInfo(segmentProperties, blockletDataMapInfo, indexInfo);
/////////////////////////////////////////////////////////////////////////
1:   private SegmentProperties initSegmentProperties(BlockletDataMapModel blockletDataMapInfo,
1:       DataFileFooter fileFooter) throws IOException {
1:     int[] columnCardinality = fileFooter.getSegmentInfo().getColumnCardinality();
1:     segmentPropertiesIndex = SegmentPropertiesAndSchemaHolder.getInstance()
0:         .addSegmentProperties(blockletDataMapInfo.getCarbonTable().getAbsoluteTableIdentifier(),
1:             columnInTable, columnCardinality, blockletDataMapInfo.getSegmentId());
1:     return getSegmentProperties();
/////////////////////////////////////////////////////////////////////////
0:   protected DataMapRowImpl loadBlockInfoForOldStore(SegmentProperties segmentProperties,
0:       BlockletDataMapModel blockletDataMapInfo, List<DataFileFooter> indexInfo)
0:       throws IOException, MemoryException {
/////////////////////////////////////////////////////////////////////////
0:   private DataMapRowImpl loadBlockMetaInfo(SegmentProperties segmentProperties,
0:       BlockletDataMapModel blockletDataMapInfo, List<DataFileFooter> indexInfo)
0:       throws IOException, MemoryException {
/////////////////////////////////////////////////////////////////////////
1:     // blocklet count index is the last index
0:     summaryRow.setByteArray(blockletCount, getTaskSummarySchema().length - 1);
/////////////////////////////////////////////////////////////////////////
0:     CarbonRowSchema[] schema = getSchema();
1:     CarbonRowSchema[] taskSummarySchema = getTaskSummarySchema();
1:       summaryRow = new DataMapRowImpl(taskSummarySchema);
0:     addTaskMinMaxValues(summaryRow, minMaxLen, taskSummarySchema, taskMinMaxOrdinal,
0:     addTaskMinMaxValues(summaryRow, minMaxLen, taskSummarySchema, taskMinMaxOrdinal,
1:     // add file name
1:     byte[] filePathBytes =
1:         getFileNameFromPath(filePath).getBytes(CarbonCommonConstants.DEFAULT_CHARSET_CLASS);
/////////////////////////////////////////////////////////////////////////
1:       memoryDMStore.addIndexRow(schema, row);
1:   protected String getFileNameFromPath(String filePath) {
1:     return CarbonTablePath.getCarbonDataFileName(filePath);
1:   }
1: 
1:   protected String getFilePath() {
1:     if (isFilePathStored) {
1:       return getTableTaskInfo(SUMMARY_INDEX_PATH);
1:     }
1:     // create the segment directory path
1:     String tablePath = SegmentPropertiesAndSchemaHolder.getInstance()
1:         .getSegmentPropertiesWrapper(segmentPropertiesIndex).getTableIdentifier().getTablePath();
1:     String segmentId = getTableTaskInfo(SUMMARY_SEGMENTID);
1:     return CarbonTablePath.getSegmentPath(tablePath, segmentId);
1:   }
1: 
1:   protected String getFileNameWithFilePath(DataMapRow dataMapRow, String filePath) {
1:     String fileName = filePath + CarbonCommonConstants.FILE_SEPARATOR + new String(
1:         dataMapRow.getByteArray(FILE_PATH_INDEX), CarbonCommonConstants.DEFAULT_CHARSET_CLASS)
1:         + CarbonTablePath.getCarbonDataExtension();
1:     return fileName;
1:   }
1: 
0:   private void addTaskSummaryRowToUnsafeMemoryStore(DataMapRow summaryRow, byte[] filePath,
0:       byte[] fileName, byte[] segmentId) {
1:       if (null != filePath) {
1:         summaryRow.setByteArray(filePath, SUMMARY_INDEX_PATH);
1:       }
0:         taskSummaryDMStore.addIndexRow(getTaskSummarySchema(), summaryRow);
/////////////////////////////////////////////////////////////////////////
0:   protected void createMemoryDMStore(boolean addToUnsafe) throws MemoryException {
0:     memoryDMStore = getMemoryDMStore(addToUnsafe);
/////////////////////////////////////////////////////////////////////////
0:   protected void createSummarySchema(SegmentProperties segmentProperties, boolean addToUnsafe)
0:         .createTaskSummarySchema(segmentProperties, storeBlockletCount, isFilePathStored);
0:     SegmentPropertiesAndSchemaHolder.getInstance()
0:         .getSegmentPropertiesWrapper(segmentPropertiesIndex)
0:         .setTaskSummarySchema(taskSummarySchema);
0:     taskSummaryDMStore = getMemoryDMStore(addToUnsafe);
0:         FilterUtil.getFilterExecuterTree(filterExp, getSegmentProperties(), null);
1:     DataMapRow unsafeRow = taskSummaryDMStore
1:         .getDataMapRow(getTaskSummarySchema(), taskSummaryDMStore.getRowCount() - 1);
/////////////////////////////////////////////////////////////////////////
1:   private List<Blocklet> prune(FilterResolverIntf filterExp) {
0:     CarbonRowSchema[] schema = getSchema();
1:     String filePath = getFilePath();
1:         DataMapRow safeRow = memoryDMStore.getDataMapRow(schema, i).convertToSafeRow();
1:         blocklets.add(createBlocklet(safeRow, getFileNameWithFilePath(safeRow, filePath),
0:             getBlockletId(safeRow)));
/////////////////////////////////////////////////////////////////////////
0:           FilterUtil.getFilterExecuterTree(filterExp, getSegmentProperties(), null);
0:         DataMapRow safeRow = memoryDMStore.getDataMapRow(schema, startIndex).convertToSafeRow();
1:         String fileName = getFileNameWithFilePath(safeRow, filePath);
1:                 getMinMaxValue(safeRow, MIN_VALUES_INDEX), fileName, blockletId);
0:           blocklets.add(createBlocklet(safeRow, fileName, blockletId));
/////////////////////////////////////////////////////////////////////////
1:     return prune(filterExp);
/////////////////////////////////////////////////////////////////////////
0:     DataMapRow safeRow = memoryDMStore.getDataMapRow(getSchema(), rowIndex).convertToSafeRow();
1:     String filePath = getFilePath();
0:     return createBlocklet(safeRow, getFileNameWithFilePath(safeRow, filePath), relativeBlockletId);
0:     CarbonRowSchema[] taskSummarySchema = getTaskSummarySchema();
1:     return taskSummaryDMStore
1:         .getDataMapRow(taskSummarySchema, taskSummaryDMStore.getRowCount() - 1)
1:         .getByteArray(taskSummarySchema.length - 1);
/////////////////////////////////////////////////////////////////////////
1:   public String getTableTaskInfo(int index) {
1:     DataMapRow unsafeRow = taskSummaryDMStore.getDataMapRow(getTaskSummarySchema(), 0);
1:       return new String(unsafeRow.getByteArray(index), CarbonCommonConstants.DEFAULT_CHARSET);
/////////////////////////////////////////////////////////////////////////
0:   protected ExtendedBlocklet createBlocklet(DataMapRow row, String fileName, short blockletId) {
1:     ExtendedBlocklet blocklet = new ExtendedBlocklet(fileName, blockletId + "", false);
/////////////////////////////////////////////////////////////////////////
1:     detailInfo.setDimLens(getColumnCardinality());
/////////////////////////////////////////////////////////////////////////
/////////////////////////////////////////////////////////////////////////
1:       DataMapRow unsafeRow = taskSummaryDMStore.getDataMapRow(getTaskSummarySchema(), 0);
/////////////////////////////////////////////////////////////////////////
/////////////////////////////////////////////////////////////////////////
1:   protected SegmentProperties getSegmentProperties() {
1:     return SegmentPropertiesAndSchemaHolder.getInstance()
1:         .getSegmentProperties(segmentPropertiesIndex);
1:     return SegmentPropertiesAndSchemaHolder.getInstance()
1:         .getSegmentPropertiesWrapper(segmentPropertiesIndex).getColumnCardinality();
1:   public List<ColumnSchema> getColumnSchema() {
1:     return SegmentPropertiesAndSchemaHolder.getInstance()
1:         .getSegmentPropertiesWrapper(segmentPropertiesIndex).getColumnsInTable();
1:   }
1: 
1:   protected AbstractMemoryDMStore getMemoryDMStore(boolean addToUnsafe)
1:       memoryDMStore = new UnsafeMemoryDMStore();
1:       memoryDMStore = new SafeMemoryDMStore();
0:   protected CarbonRowSchema[] getSchema() {
1:     return SegmentPropertiesAndSchemaHolder.getInstance()
0:         .getSegmentPropertiesWrapper(segmentPropertiesIndex).getBlockSchema();
1:   }
1: 
1:   protected CarbonRowSchema[] getTaskSummarySchema() {
1:     return SegmentPropertiesAndSchemaHolder.getInstance()
0:         .getSegmentPropertiesWrapper(segmentPropertiesIndex).getTaskSummarySchema();
1:   }
1: 
/////////////////////////////////////////////////////////////////////////
0:       UnsafeMemoryDMStore unsafeMemoryDMStore = memoryDMStore.convertToUnsafeDMStore(getSchema());
1:       UnsafeMemoryDMStore unsafeSummaryMemoryDMStore =
1:           taskSummaryDMStore.convertToUnsafeDMStore(getTaskSummarySchema());
1:   public void setSegmentPropertiesIndex(int segmentPropertiesIndex) {
1:     this.segmentPropertiesIndex = segmentPropertiesIndex;
1:   public int getSegmentPropertiesIndex() {
1:     return segmentPropertiesIndex;
commit:6118711
/////////////////////////////////////////////////////////////////////////
1: /*
1:  * Licensed to the Apache Software Foundation (ASF) under one or more
1:  * contributor license agreements.  See the NOTICE file distributed with
1:  * this work for additional information regarding copyright ownership.
1:  * The ASF licenses this file to You under the Apache License, Version 2.0
1:  * (the "License"); you may not use this file except in compliance with
1:  * the License.  You may obtain a copy of the License at
1:  *
1:  *    http://www.apache.org/licenses/LICENSE-2.0
1:  *
1:  * Unless required by applicable law or agreed to in writing, software
1:  * distributed under the License is distributed on an "AS IS" BASIS,
1:  * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
1:  * See the License for the specific language governing permissions and
1:  * limitations under the License.
1:  */
1: package org.apache.carbondata.core.indexstore.blockletindex;
1: 
0: import java.io.*;
0: import java.util.ArrayList;
0: import java.util.BitSet;
0: import java.util.List;
1: 
1: import org.apache.carbondata.common.logging.LogService;
1: import org.apache.carbondata.common.logging.LogServiceFactory;
1: import org.apache.carbondata.core.constants.CarbonCommonConstants;
1: import org.apache.carbondata.core.datamap.dev.DataMapModel;
1: import org.apache.carbondata.core.datamap.dev.cgdatamap.CoarseGrainDataMap;
1: import org.apache.carbondata.core.datastore.block.SegmentProperties;
1: import org.apache.carbondata.core.datastore.block.TableBlockInfo;
1: import org.apache.carbondata.core.indexstore.AbstractMemoryDMStore;
1: import org.apache.carbondata.core.indexstore.BlockMetaInfo;
1: import org.apache.carbondata.core.indexstore.Blocklet;
1: import org.apache.carbondata.core.indexstore.BlockletDetailInfo;
1: import org.apache.carbondata.core.indexstore.ExtendedBlocklet;
1: import org.apache.carbondata.core.indexstore.PartitionSpec;
1: import org.apache.carbondata.core.indexstore.SafeMemoryDMStore;
1: import org.apache.carbondata.core.indexstore.UnsafeMemoryDMStore;
1: import org.apache.carbondata.core.indexstore.row.DataMapRow;
1: import org.apache.carbondata.core.indexstore.row.DataMapRowImpl;
1: import org.apache.carbondata.core.indexstore.schema.CarbonRowSchema;
0: import org.apache.carbondata.core.indexstore.schema.SchemaGenerator;
1: import org.apache.carbondata.core.memory.MemoryException;
1: import org.apache.carbondata.core.metadata.blocklet.DataFileFooter;
1: import org.apache.carbondata.core.metadata.blocklet.index.BlockletIndex;
1: import org.apache.carbondata.core.metadata.blocklet.index.BlockletMinMaxIndex;
1: import org.apache.carbondata.core.metadata.schema.table.column.ColumnSchema;
1: import org.apache.carbondata.core.profiler.ExplainCollector;
1: import org.apache.carbondata.core.scan.filter.FilterExpressionProcessor;
1: import org.apache.carbondata.core.scan.filter.FilterUtil;
1: import org.apache.carbondata.core.scan.filter.executer.FilterExecuter;
1: import org.apache.carbondata.core.scan.filter.executer.ImplicitColumnFilterExecutor;
1: import org.apache.carbondata.core.scan.filter.resolver.FilterResolverIntf;
1: import org.apache.carbondata.core.util.BlockletDataMapUtil;
1: import org.apache.carbondata.core.util.ByteUtil;
1: import org.apache.carbondata.core.util.CarbonUtil;
1: import org.apache.carbondata.core.util.DataFileFooterConverter;
1: import org.apache.carbondata.core.util.path.CarbonTablePath;
1: 
0: import org.apache.commons.lang3.ArrayUtils;
1: import org.apache.commons.lang3.StringUtils;
1: import org.apache.hadoop.fs.Path;
0: import org.xerial.snappy.Snappy;
1: 
1: /**
1:  * Datamap implementation for block.
1:  */
1: public class BlockDataMap extends CoarseGrainDataMap
1:     implements BlockletDataMapRowIndexes, Serializable {
1: 
1:   private static final LogService LOGGER =
1:       LogServiceFactory.getLogService(BlockDataMap.class.getName());
1: 
1:   protected static final long serialVersionUID = -2170289352240810993L;
1:   /**
1:    * for CACHE_LEVEL=BLOCK and legacy store default blocklet id will be -1
1:    */
1:   private static final short BLOCK_DEFAULT_BLOCKLET_ID = -1;
1: 
1:   protected AbstractMemoryDMStore memoryDMStore;
1: 
1:   protected AbstractMemoryDMStore taskSummaryDMStore;
1: 
0:   // As it is a heavy object it is not recommended to serialize this object
0:   protected transient SegmentProperties segmentProperties;
1: 
0:   protected int[] columnCardinality;
1: 
0:   protected long blockletSchemaTime;
1:   /**
1:    * flag to check for store from 1.1 or any prior version
1:    */
1:   protected boolean isLegacyStore;
1: 
0:   @Override public void init(DataMapModel dataMapModel) throws IOException, MemoryException {
1:     long startTime = System.currentTimeMillis();
1:     assert (dataMapModel instanceof BlockletDataMapModel);
1:     BlockletDataMapModel blockletDataMapInfo = (BlockletDataMapModel) dataMapModel;
0:     DataFileFooterConverter fileFooterConverter = new DataFileFooterConverter();
1:     List<DataFileFooter> indexInfo = fileFooterConverter
0:         .getIndexInfo(blockletDataMapInfo.getFilePath(), blockletDataMapInfo.getFileData());
1:     Path path = new Path(blockletDataMapInfo.getFilePath());
0:     byte[] filePath = path.getParent().toString().getBytes(CarbonCommonConstants.DEFAULT_CHARSET);
1:     byte[] fileName = path.getName().toString().getBytes(CarbonCommonConstants.DEFAULT_CHARSET);
1:     byte[] segmentId =
1:         blockletDataMapInfo.getSegmentId().getBytes(CarbonCommonConstants.DEFAULT_CHARSET);
0:     byte[] schemaBinary = null;
1:     if (!indexInfo.isEmpty()) {
1:       DataFileFooter fileFooter = indexInfo.get(0);
1:       // store for 1.1 or any prior version will not have any blocklet information in file footer
1:       isLegacyStore = fileFooter.getBlockletList() == null;
1:       // init segment properties and create schema
0:       initSegmentProperties(fileFooter);
0:       schemaBinary = convertSchemaToBinary(fileFooter.getColumnInTable());
0:       createSchema(segmentProperties, blockletDataMapInfo.isAddToUnsafe());
0:       createSummarySchema(segmentProperties, schemaBinary, filePath, fileName, segmentId,
0:           blockletDataMapInfo.isAddToUnsafe());
1:     }
0:     // check for legacy store and load the metadata
0:     DataMapRowImpl summaryRow = loadMetadata(blockletDataMapInfo, indexInfo);
0:     finishWriting(filePath, fileName, segmentId, schemaBinary, summaryRow);
1:     if (LOGGER.isDebugEnabled()) {
1:       LOGGER.debug(
1:           "Time taken to load blocklet datamap from file : " + dataMapModel.getFilePath() + " is "
1:               + (System.currentTimeMillis() - startTime));
1:     }
1:   }
1: 
0:   private void finishWriting(byte[] filePath, byte[] fileName, byte[] segmentId,
0:       byte[] schemaBinary, DataMapRowImpl summaryRow) throws MemoryException {
1:     if (memoryDMStore != null) {
1:       memoryDMStore.finishWriting();
1:     }
0:     // TODO: schema binary not required. Instead maintain only the segmentProperties index and
0:     // get the cardinality and columnSchema from there
1:     if (null != taskSummaryDMStore) {
0:       addTaskSummaryRowToUnsafeMemoryStore(summaryRow, schemaBinary, filePath, fileName, segmentId);
1:       taskSummaryDMStore.finishWriting();
1:     }
1:   }
1: 
1:   /**
1:    * Method to check the cache level and load metadata based on that information
1:    *
1:    * @param blockletDataMapInfo
1:    * @param indexInfo
1:    * @throws IOException
1:    * @throws MemoryException
1:    */
0:   protected DataMapRowImpl loadMetadata(BlockletDataMapModel blockletDataMapInfo,
0:       List<DataFileFooter> indexInfo) throws IOException, MemoryException {
1:     if (isLegacyStore) {
0:       return loadBlockInfoForOldStore(blockletDataMapInfo, indexInfo);
1:     } else {
0:       return loadBlockMetaInfo(blockletDataMapInfo, indexInfo);
1:     }
1:   }
1: 
1:   /**
1:    * initialise segment properties
1:    *
1:    * @param fileFooter
1:    * @throws IOException
1:    */
0:   private void initSegmentProperties(DataFileFooter fileFooter) throws IOException {
1:     List<ColumnSchema> columnInTable = fileFooter.getColumnInTable();
0:     // TODO: remove blockletSchemaTime after maintaining the index of segmentProperties
0:     blockletSchemaTime = fileFooter.getSchemaUpdatedTimeStamp();
0:     columnCardinality = fileFooter.getSegmentInfo().getColumnCardinality();
0:     segmentProperties = new SegmentProperties(columnInTable, columnCardinality);
1:   }
1: 
1:   /**
1:    * This is old store scenario, here blocklet information is not available in index
1:    * file so load only block info. Old store refers to store in 1.1 or prior to 1.1 version
1:    *
1:    * @param blockletDataMapInfo
1:    * @param indexInfo
1:    * @throws IOException
1:    * @throws MemoryException
1:    */
0:   protected DataMapRowImpl loadBlockInfoForOldStore(BlockletDataMapModel blockletDataMapInfo,
0:       List<DataFileFooter> indexInfo) throws IOException, MemoryException {
1:     DataMapRowImpl summaryRow = null;
1:     for (DataFileFooter fileFooter : indexInfo) {
1:       TableBlockInfo blockInfo = fileFooter.getBlockInfo().getTableBlockInfo();
1:       BlockMetaInfo blockMetaInfo =
1:           blockletDataMapInfo.getBlockMetaInfoMap().get(blockInfo.getFilePath());
1:       // Here it loads info about all blocklets of index
1:       // Only add if the file exists physically. There are scenarios which index file exists inside
1:       // merge index but related carbondata files are deleted. In that case we first check whether
1:       // the file exists physically or not
1:       if (null != blockMetaInfo) {
1:         BlockletIndex blockletIndex = fileFooter.getBlockletIndex();
1:         BlockletMinMaxIndex minMaxIndex = blockletIndex.getMinMaxIndex();
1:         byte[][] minValues =
1:             BlockletDataMapUtil.updateMinValues(segmentProperties, minMaxIndex.getMinValues());
1:         byte[][] maxValues =
1:             BlockletDataMapUtil.updateMaxValues(segmentProperties, minMaxIndex.getMaxValues());
1:         // update min max values in case of old store for measures as measure min/max in
1:         // old stores in written opposite
1:         byte[][] updatedMinValues =
1:             CarbonUtil.updateMinMaxValues(fileFooter, maxValues, minValues, true);
1:         byte[][] updatedMaxValues =
1:             CarbonUtil.updateMinMaxValues(fileFooter, maxValues, minValues, false);
0:         summaryRow =
0:             loadToUnsafeBlock(fileFooter, segmentProperties, blockInfo.getFilePath(), summaryRow,
0:                 blockMetaInfo, updatedMinValues, updatedMaxValues);
1:       }
1:     }
1:     return summaryRow;
1:   }
1: 
1:   /**
1:    * Method to load block metadata information
1:    *
1:    * @param blockletDataMapInfo
1:    * @param indexInfo
1:    * @throws IOException
1:    * @throws MemoryException
1:    */
0:   private DataMapRowImpl loadBlockMetaInfo(BlockletDataMapModel blockletDataMapInfo,
0:       List<DataFileFooter> indexInfo) throws IOException, MemoryException {
1:     String tempFilePath = null;
1:     DataFileFooter previousDataFileFooter = null;
1:     int footerCounter = 0;
1:     byte[][] blockMinValues = null;
1:     byte[][] blockMaxValues = null;
1:     DataMapRowImpl summaryRow = null;
0:     List<Byte> blockletCountInEachBlock = new ArrayList<>(indexInfo.size());
0:     byte totalBlockletsInOneBlock = 0;
1:     boolean isLastFileFooterEntryNeedToBeAdded = false;
1:     for (DataFileFooter fileFooter : indexInfo) {
1:       TableBlockInfo blockInfo = fileFooter.getBlockInfo().getTableBlockInfo();
1:       BlockMetaInfo blockMetaInfo =
1:           blockletDataMapInfo.getBlockMetaInfoMap().get(blockInfo.getFilePath());
1:       footerCounter++;
1:       if (blockMetaInfo != null) {
1:         // this variable will be used for adding the DataMapRow entry every time a unique block
1:         // path is encountered
1:         if (null == tempFilePath) {
1:           tempFilePath = blockInfo.getFilePath();
1:           // 1st time assign the min and max values from the current file footer
1:           blockMinValues = fileFooter.getBlockletIndex().getMinMaxIndex().getMinValues();
1:           blockMaxValues = fileFooter.getBlockletIndex().getMinMaxIndex().getMaxValues();
1:           previousDataFileFooter = fileFooter;
1:           totalBlockletsInOneBlock++;
1:         } else if (blockInfo.getFilePath().equals(tempFilePath)) {
1:           // After iterating over all the blocklets that belong to one block we need to compute the
1:           // min and max at block level. So compare min and max values and update if required
1:           BlockletMinMaxIndex currentFooterMinMaxIndex =
1:               fileFooter.getBlockletIndex().getMinMaxIndex();
0:           blockMinValues = compareAndUpdateMinMax(segmentProperties.getColumnsValueSize(),
0:               currentFooterMinMaxIndex.getMinValues(), blockMinValues, true);
0:           blockMaxValues = compareAndUpdateMinMax(segmentProperties.getColumnsValueSize(),
0:               currentFooterMinMaxIndex.getMaxValues(), blockMaxValues, false);
1:           totalBlockletsInOneBlock++;
1:         }
1:         // as one task contains entries for all the blocklets we need iterate and load only the
1:         // with unique file path because each unique path will correspond to one
1:         // block in the task. OR condition is to handle the loading of last file footer
1:         if (!blockInfo.getFilePath().equals(tempFilePath) || footerCounter == indexInfo.size()) {
1:           TableBlockInfo previousBlockInfo =
1:               previousDataFileFooter.getBlockInfo().getTableBlockInfo();
0:           summaryRow = loadToUnsafeBlock(previousDataFileFooter, segmentProperties,
0:               previousBlockInfo.getFilePath(), summaryRow,
1:               blockletDataMapInfo.getBlockMetaInfoMap().get(previousBlockInfo.getFilePath()),
1:               blockMinValues, blockMaxValues);
1:           // flag to check whether last file footer entry is different from previous entry.
1:           // If yes then it need to be added at last
1:           isLastFileFooterEntryNeedToBeAdded =
1:               (footerCounter == indexInfo.size()) && (!blockInfo.getFilePath()
1:                   .equals(tempFilePath));
1:           // assign local variables values using the current file footer
1:           tempFilePath = blockInfo.getFilePath();
1:           blockMinValues = fileFooter.getBlockletIndex().getMinMaxIndex().getMinValues();
1:           blockMaxValues = fileFooter.getBlockletIndex().getMinMaxIndex().getMaxValues();
1:           previousDataFileFooter = fileFooter;
1:           blockletCountInEachBlock.add(totalBlockletsInOneBlock);
1:           // for next block count will start from 1 because a row is created whenever a new file
1:           // path comes. Here already a new file path has come so the count should start from 1
1:           totalBlockletsInOneBlock = 1;
1:         }
1:       }
1:     }
1:     // add the last file footer entry
1:     if (isLastFileFooterEntryNeedToBeAdded) {
0:       summaryRow = loadToUnsafeBlock(previousDataFileFooter, segmentProperties,
0:           previousDataFileFooter.getBlockInfo().getTableBlockInfo().getFilePath(), summaryRow,
0:           blockletDataMapInfo.getBlockMetaInfoMap()
0:               .get(previousDataFileFooter.getBlockInfo().getTableBlockInfo().getFilePath()),
1:           blockMinValues, blockMaxValues);
1:       blockletCountInEachBlock.add(totalBlockletsInOneBlock);
1:     }
0:     byte[] blockletCount = ArrayUtils
0:         .toPrimitive(blockletCountInEachBlock.toArray(new Byte[blockletCountInEachBlock.size()]));
0:     summaryRow.setByteArray(blockletCount, SUMMARY_BLOCKLET_COUNT);
1:     return summaryRow;
1:   }
1: 
1:   protected void setLocations(String[] locations, DataMapRow row, int ordinal)
1:       throws UnsupportedEncodingException {
1:     // Add location info
1:     String locationStr = StringUtils.join(locations, ',');
1:     row.setByteArray(locationStr.getBytes(CarbonCommonConstants.DEFAULT_CHARSET), ordinal);
1:   }
1: 
1:   /**
1:    * Load information for the block.It is the case can happen only for old stores
1:    * where blocklet information is not available in index file. So load only block information
1:    * and read blocklet information in executor.
1:    */
0:   protected DataMapRowImpl loadToUnsafeBlock(DataFileFooter fileFooter,
0:       SegmentProperties segmentProperties, String filePath, DataMapRowImpl summaryRow,
0:       BlockMetaInfo blockMetaInfo, byte[][] minValues, byte[][] maxValues) {
0:     int[] minMaxLen = segmentProperties.getColumnsValueSize();
0:     CarbonRowSchema[] schema = memoryDMStore.getSchema();
1:     // Add one row to maintain task level min max for segment pruning
1:     if (summaryRow == null) {
0:       summaryRow = new DataMapRowImpl(taskSummaryDMStore.getSchema());
1:     }
1:     DataMapRow row = new DataMapRowImpl(schema);
1:     int ordinal = 0;
1:     int taskMinMaxOrdinal = 0;
0:     row.setRow(addMinMax(minMaxLen, schema[ordinal], minValues), ordinal);
1:     // compute and set task level min values
0:     addTaskMinMaxValues(summaryRow, minMaxLen, taskSummaryDMStore.getSchema(), taskMinMaxOrdinal,
0:         minValues, TASK_MIN_VALUES_INDEX, true);
1:     ordinal++;
1:     taskMinMaxOrdinal++;
0:     row.setRow(addMinMax(minMaxLen, schema[ordinal], maxValues), ordinal);
1:     // compute and set task level max values
0:     addTaskMinMaxValues(summaryRow, minMaxLen, taskSummaryDMStore.getSchema(), taskMinMaxOrdinal,
0:         maxValues, TASK_MAX_VALUES_INDEX, false);
1:     ordinal++;
1:     // add total rows in one carbondata file
1:     row.setInt((int) fileFooter.getNumberOfRows(), ordinal++);
0:     // add file path
0:     // TODO: shorten file path
0:     byte[] filePathBytes = filePath.getBytes(CarbonCommonConstants.DEFAULT_CHARSET_CLASS);
1:     row.setByteArray(filePathBytes, ordinal++);
1:     // add version number
1:     row.setShort(fileFooter.getVersionId().number(), ordinal++);
1:     // add schema updated time
1:     row.setLong(fileFooter.getSchemaUpdatedTimeStamp(), ordinal++);
1:     // add block offset
1:     row.setLong(fileFooter.getBlockInfo().getTableBlockInfo().getBlockOffset(), ordinal++);
1:     try {
1:       setLocations(blockMetaInfo.getLocationInfo(), row, ordinal++);
1:       // store block size
1:       row.setLong(blockMetaInfo.getSize(), ordinal);
0:       memoryDMStore.addIndexRow(row);
1:     } catch (Exception e) {
1:       throw new RuntimeException(e);
1:     }
1:     return summaryRow;
1:   }
1: 
0:   private void addTaskSummaryRowToUnsafeMemoryStore(DataMapRow summaryRow, byte[] schemaBinary,
0:       byte[] filePath, byte[] fileName, byte[] segmentId) {
1:     // write the task summary info to unsafe memory store
1:     if (null != summaryRow) {
0:       // Add column schema , it is useful to generate segment properties in executor.
0:       // So we no need to read footer again there.
0:       if (schemaBinary != null) {
0:         summaryRow.setByteArray(schemaBinary, SUMMARY_SCHEMA);
1:       }
0:       summaryRow.setByteArray(filePath, SUMMARY_INDEX_PATH);
1:       summaryRow.setByteArray(fileName, SUMMARY_INDEX_FILE_NAME);
1:       summaryRow.setByteArray(segmentId, SUMMARY_SEGMENTID);
1:       try {
0:         taskSummaryDMStore.addIndexRow(summaryRow);
1:       } catch (Exception e) {
1:         throw new RuntimeException(e);
1:       }
1:     }
1:   }
1: 
0:   protected DataMapRow addMinMax(int[] minMaxLen, CarbonRowSchema carbonRowSchema,
1:       byte[][] minValues) {
1:     CarbonRowSchema[] minSchemas =
1:         ((CarbonRowSchema.StructCarbonRowSchema) carbonRowSchema).getChildSchemas();
1:     DataMapRow minRow = new DataMapRowImpl(minSchemas);
1:     int minOrdinal = 0;
1:     // min value adding
0:     for (int i = 0; i < minMaxLen.length; i++) {
1:       minRow.setByteArray(minValues[i], minOrdinal++);
1:     }
1:     return minRow;
1:   }
1: 
1:   /**
1:    * This method will compute min/max values at task level
1:    *
1:    * @param taskMinMaxRow
0:    * @param minMaxLen
1:    * @param carbonRowSchema
1:    * @param taskMinMaxOrdinal
1:    * @param minMaxValue
1:    * @param ordinal
1:    * @param isMinValueComparison
1:    */
0:   protected void addTaskMinMaxValues(DataMapRow taskMinMaxRow, int[] minMaxLen,
0:       CarbonRowSchema[] carbonRowSchema, int taskMinMaxOrdinal, byte[][] minMaxValue, int ordinal,
0:       boolean isMinValueComparison) {
1:     DataMapRow row = taskMinMaxRow.getRow(ordinal);
1:     byte[][] updatedMinMaxValues = null;
1:     if (null == row) {
1:       CarbonRowSchema[] minSchemas =
1:           ((CarbonRowSchema.StructCarbonRowSchema) carbonRowSchema[taskMinMaxOrdinal])
1:               .getChildSchemas();
1:       row = new DataMapRowImpl(minSchemas);
1:       updatedMinMaxValues = minMaxValue;
1:     } else {
1:       byte[][] existingMinMaxValues = getMinMaxValue(taskMinMaxRow, ordinal);
0:       updatedMinMaxValues = compareAndUpdateMinMax(minMaxLen, minMaxValue, existingMinMaxValues,
0:           isMinValueComparison);
1:     }
1:     int minMaxOrdinal = 0;
1:     // min/max value adding
0:     for (int i = 0; i < minMaxLen.length; i++) {
1:       row.setByteArray(updatedMinMaxValues[i], minMaxOrdinal++);
1:     }
1:     taskMinMaxRow.setRow(row, ordinal);
1:   }
1: 
1:   /**
1:    * This method will do min/max comparison of values and update if required
1:    *
0:    * @param minMaxLen
1:    * @param minMaxValueCompare1
1:    * @param minMaxValueCompare2
1:    * @param isMinValueComparison
1:    */
0:   private byte[][] compareAndUpdateMinMax(int[] minMaxLen, byte[][] minMaxValueCompare1,
1:       byte[][] minMaxValueCompare2, boolean isMinValueComparison) {
1:     // Compare and update min max values
0:     byte[][] updatedMinMaxValues = new byte[minMaxLen.length][];
1:     System.arraycopy(minMaxValueCompare1, 0, updatedMinMaxValues, 0, minMaxValueCompare1.length);
0:     for (int i = 0; i < minMaxLen.length; i++) {
1:       int compare = ByteUtil.UnsafeComparer.INSTANCE
1:           .compareTo(minMaxValueCompare2[i], minMaxValueCompare1[i]);
1:       if (isMinValueComparison) {
1:         if (compare < 0) {
1:           updatedMinMaxValues[i] = minMaxValueCompare2[i];
1:         }
1:       } else if (compare > 0) {
1:         updatedMinMaxValues[i] = minMaxValueCompare2[i];
1:       }
1:     }
1:     return updatedMinMaxValues;
1:   }
1: 
0:   protected void createSchema(SegmentProperties segmentProperties, boolean addToUnsafe)
1:       throws MemoryException {
0:     CarbonRowSchema[] schema = SchemaGenerator.createBlockSchema(segmentProperties);
0:     memoryDMStore = getMemoryDMStore(schema, addToUnsafe);
1:   }
1: 
1:   /**
1:    * Creates the schema to store summary information or the information which can be stored only
1:    * once per datamap. It stores datamap level max/min of each column and partition information of
1:    * datamap
1:    *
0:    * @param segmentProperties
1:    * @throws MemoryException
1:    */
0:   protected void createSummarySchema(SegmentProperties segmentProperties, byte[] schemaBinary,
0:       byte[] filePath, byte[] fileName, byte[] segmentId, boolean addToUnsafe)
1:       throws MemoryException {
0:     // flag to check whether it is required to store blocklet count of each carbondata file as
0:     // binary in summary schema. This will be true when it is not a legacy store (>1.1 version)
0:     // and CACHE_LEVEL=BLOCK
0:     boolean storeBlockletCount = !isLegacyStore;
0:     CarbonRowSchema[] taskSummarySchema = SchemaGenerator
0:         .createTaskSummarySchema(segmentProperties, schemaBinary, filePath, fileName, segmentId,
0:             storeBlockletCount);
0:     taskSummaryDMStore = getMemoryDMStore(taskSummarySchema, addToUnsafe);
1:   }
1: 
1:   @Override public boolean isScanRequired(FilterResolverIntf filterExp) {
0:     FilterExecuter filterExecuter =
0:         FilterUtil.getFilterExecuterTree(filterExp, segmentProperties, null);
0:     DataMapRow unsafeRow = taskSummaryDMStore.getDataMapRow(taskSummaryDMStore.getRowCount() - 1);
1:     boolean isScanRequired = FilterExpressionProcessor
1:         .isScanRequired(filterExecuter, getMinMaxValue(unsafeRow, TASK_MAX_VALUES_INDEX),
1:             getMinMaxValue(unsafeRow, TASK_MIN_VALUES_INDEX));
1:     if (isScanRequired) {
1:       return true;
1:     }
1:     return false;
1:   }
1: 
0:   private List<Blocklet> prune(FilterResolverIntf filterExp, SegmentProperties segmentProperties) {
1:     if (memoryDMStore.getRowCount() == 0) {
1:       return new ArrayList<>();
1:     }
1:     List<Blocklet> blocklets = new ArrayList<>();
0:     int numBlocklets = 0;
1:     if (filterExp == null) {
0:       numBlocklets = memoryDMStore.getRowCount();
0:       for (int i = 0; i < numBlocklets; i++) {
0:         DataMapRow safeRow = memoryDMStore.getDataMapRow(i).convertToSafeRow();
0:         blocklets.add(createBlocklet(safeRow, getBlockletId(safeRow)));
1:       }
1:     } else {
1:       // Remove B-tree jump logic as start and end key prepared is not
1:       // correct for old store scenarios
0:       int startIndex = 0;
0:       numBlocklets = memoryDMStore.getRowCount();
0:       FilterExecuter filterExecuter =
0:           FilterUtil.getFilterExecuterTree(filterExp, segmentProperties, null);
0:       while (startIndex < numBlocklets) {
0:         DataMapRow safeRow = memoryDMStore.getDataMapRow(startIndex).convertToSafeRow();
0:         String filePath = new String(safeRow.getByteArray(FILE_PATH_INDEX),
0:             CarbonCommonConstants.DEFAULT_CHARSET_CLASS);
1:         short blockletId = getBlockletId(safeRow);
1:         boolean isValid =
1:             addBlockBasedOnMinMaxValue(filterExecuter, getMinMaxValue(safeRow, MAX_VALUES_INDEX),
0:                 getMinMaxValue(safeRow, MIN_VALUES_INDEX), filePath, blockletId);
1:         if (isValid) {
0:           blocklets.add(createBlocklet(safeRow, blockletId));
1:         }
0:         startIndex++;
1:       }
1:     }
0:     ExplainCollector.addTotalBlocklets(numBlocklets);
1:     return blocklets;
1:   }
1: 
1:   @Override
1:   public List<Blocklet> prune(FilterResolverIntf filterExp, SegmentProperties segmentProperties,
1:       List<PartitionSpec> partitions) {
1:     if (memoryDMStore.getRowCount() == 0) {
1:       return new ArrayList<>();
1:     }
1:     // if it has partitioned datamap but there is no partitioned information stored, it means
1:     // partitions are dropped so return empty list.
1:     if (partitions != null) {
1:       if (!validatePartitionInfo(partitions)) {
1:         return new ArrayList<>();
1:       }
1:     }
1:     // Prune with filters if the partitions are existed in this datamap
1:     // changed segmentProperties to this.segmentProperties to make sure the pruning with its own
1:     // segmentProperties.
1:     // Its a temporary fix. The Interface DataMap.prune(FilterResolverIntf filterExp,
1:     // SegmentProperties segmentProperties, List<PartitionSpec> partitions) should be corrected
0:     return prune(filterExp, this.segmentProperties);
1:   }
1: 
1:   private boolean validatePartitionInfo(List<PartitionSpec> partitions) {
1:     // First get the partitions which are stored inside datamap.
1:     String[] fileDetails = getFileDetails();
1:     // Check the exact match of partition information inside the stored partitions.
1:     boolean found = false;
1:     Path folderPath = new Path(fileDetails[0]);
1:     for (PartitionSpec spec : partitions) {
1:       if (folderPath.equals(spec.getLocation()) && isCorrectUUID(fileDetails, spec)) {
1:         found = true;
1:         break;
1:       }
1:     }
1:     return found;
1:   }
1: 
1:   @Override public void finish() {
1: 
1:   }
1: 
1:   private boolean isCorrectUUID(String[] fileDetails, PartitionSpec spec) {
1:     boolean needToScan = false;
1:     if (spec.getUuid() != null) {
1:       String[] split = spec.getUuid().split("_");
1:       if (split[0].equals(fileDetails[2]) && CarbonTablePath.DataFileUtil
1:           .getTimeStampFromFileName(fileDetails[1]).equals(split[1])) {
1:         needToScan = true;
1:       }
1:     } else {
1:       needToScan = true;
1:     }
1:     return needToScan;
1:   }
1: 
1:   /**
1:    * select the blocks based on column min and max value
1:    *
1:    * @param filterExecuter
1:    * @param maxValue
1:    * @param minValue
1:    * @param filePath
1:    * @param blockletId
1:    * @return
1:    */
1:   private boolean addBlockBasedOnMinMaxValue(FilterExecuter filterExecuter, byte[][] maxValue,
1:       byte[][] minValue, String filePath, int blockletId) {
1:     BitSet bitSet = null;
1:     if (filterExecuter instanceof ImplicitColumnFilterExecutor) {
1:       String uniqueBlockPath = filePath.substring(filePath.lastIndexOf("/Part") + 1);
1:       // this case will come in case of old store where index file does not contain the
1:       // blocklet information
1:       if (blockletId != -1) {
1:         uniqueBlockPath = uniqueBlockPath + CarbonCommonConstants.FILE_SEPARATOR + blockletId;
1:       }
1:       bitSet = ((ImplicitColumnFilterExecutor) filterExecuter)
1:           .isFilterValuesPresentInBlockOrBlocklet(maxValue, minValue, uniqueBlockPath);
1:     } else {
1:       bitSet = filterExecuter.isScanRequired(maxValue, minValue);
1:     }
1:     if (!bitSet.isEmpty()) {
1:       return true;
1:     } else {
1:       return false;
1:     }
1:   }
1: 
1:   public ExtendedBlocklet getDetailedBlocklet(String blockletId) {
1:     if (isLegacyStore) {
1:       throw new UnsupportedOperationException("With legacy store only BlockletDataMap is allowed."
1:           + " In order to use other dataMaps upgrade to new store.");
1:     }
1:     int absoluteBlockletId = Integer.parseInt(blockletId);
1:     return createBlockletFromRelativeBlockletId(absoluteBlockletId);
1:   }
1: 
1:   /**
1:    * Method to get the relative blocklet ID. Absolute blocklet ID is the blocklet Id as per
1:    * task level but relative blocklet ID is id as per carbondata file/block level
1:    *
1:    * @param absoluteBlockletId
1:    * @return
1:    */
1:   private ExtendedBlocklet createBlockletFromRelativeBlockletId(int absoluteBlockletId) {
1:     short relativeBlockletId = -1;
1:     int rowIndex = 0;
1:     // return 0 if absoluteBlockletId is 0
1:     if (absoluteBlockletId == 0) {
1:       relativeBlockletId = (short) absoluteBlockletId;
1:     } else {
1:       int diff = absoluteBlockletId;
0:       byte[] blockletRowCountForEachBlock = getBlockletRowCountForEachBlock();
1:       // Example: absoluteBlockletID = 17, blockletRowCountForEachBlock = {4,3,2,5,7}
1:       // step1: diff = 17-4, diff = 13
1:       // step2: diff = 13-3, diff = 10
1:       // step3: diff = 10-2, diff = 8
1:       // step4: diff = 8-5, diff = 3
1:       // step5: diff = 3-7, diff = -4 (satisfies <= 0)
1:       // step6: relativeBlockletId = -4+7, relativeBlockletId = 3 (4th index starting from 0)
0:       for (byte blockletCount : blockletRowCountForEachBlock) {
1:         diff = diff - blockletCount;
1:         if (diff < 0) {
1:           relativeBlockletId = (short) (diff + blockletCount);
1:           break;
1:         }
1:         rowIndex++;
1:       }
1:     }
0:     DataMapRow safeRow = memoryDMStore.getDataMapRow(rowIndex).convertToSafeRow();
0:     return createBlocklet(safeRow, relativeBlockletId);
1:   }
1: 
1:   private byte[] getBlockletRowCountForEachBlock() {
1:     // taskSummary DM store will  have only one row
0:     return taskSummaryDMStore.getDataMapRow(taskSummaryDMStore.getRowCount() - 1)
0:         .getByteArray(SUMMARY_BLOCKLET_COUNT);
1:   }
1: 
1:   /**
1:    * Get the index file name of the blocklet data map
1:    *
1:    * @return
1:    */
0:   public String getIndexFileName() {
0:     DataMapRow unsafeRow = taskSummaryDMStore.getDataMapRow(0);
1:     try {
0:       return new String(unsafeRow.getByteArray(SUMMARY_INDEX_FILE_NAME),
1:           CarbonCommonConstants.DEFAULT_CHARSET);
1:     } catch (UnsupportedEncodingException e) {
1:       // should never happen!
1:       throw new IllegalArgumentException("UTF8 encoding is not supported", e);
1:     }
1:   }
1: 
1:   private byte[][] getMinMaxValue(DataMapRow row, int index) {
1:     DataMapRow minMaxRow = row.getRow(index);
1:     byte[][] minMax = new byte[minMaxRow.getColumnCount()][];
1:     for (int i = 0; i < minMax.length; i++) {
1:       minMax[i] = minMaxRow.getByteArray(i);
1:     }
1:     return minMax;
1:   }
1: 
1:   protected short getBlockletId(DataMapRow dataMapRow) {
1:     return BLOCK_DEFAULT_BLOCKLET_ID;
1:   }
1: 
0:   protected ExtendedBlocklet createBlocklet(DataMapRow row, short blockletId) {
0:     ExtendedBlocklet blocklet = new ExtendedBlocklet(
0:         new String(row.getByteArray(FILE_PATH_INDEX), CarbonCommonConstants.DEFAULT_CHARSET_CLASS),
0:         blockletId + "");
1:     BlockletDetailInfo detailInfo = getBlockletDetailInfo(row, blockletId, blocklet);
1:     detailInfo.setBlockletInfoBinary(new byte[0]);
1:     blocklet.setDetailInfo(detailInfo);
1:     return blocklet;
1:   }
1: 
1:   protected BlockletDetailInfo getBlockletDetailInfo(DataMapRow row, short blockletId,
1:       ExtendedBlocklet blocklet) {
1:     BlockletDetailInfo detailInfo = new BlockletDetailInfo();
1:     detailInfo.setRowCount(row.getInt(ROW_COUNT_INDEX));
1:     detailInfo.setVersionNumber(row.getShort(VERSION_INDEX));
0:     detailInfo.setDimLens(columnCardinality);
1:     detailInfo.setBlockletId(blockletId);
1:     detailInfo.setSchemaUpdatedTimeStamp(row.getLong(SCHEMA_UPADATED_TIME_INDEX));
1:     try {
1:       blocklet.setLocation(
1:           new String(row.getByteArray(LOCATIONS), CarbonCommonConstants.DEFAULT_CHARSET)
1:               .split(","));
1:     } catch (IOException e) {
1:       throw new RuntimeException(e);
1:     }
1:     detailInfo.setBlockFooterOffset(row.getLong(BLOCK_FOOTER_OFFSET));
0:     detailInfo.setColumnSchemaBinary(getColumnSchemaBinary());
1:     detailInfo.setBlockSize(row.getLong(BLOCK_LENGTH));
1:     detailInfo.setLegacyStore(isLegacyStore);
1:     return detailInfo;
1:   }
1: 
1:   private String[] getFileDetails() {
1:     try {
1:       String[] fileDetails = new String[3];
0:       DataMapRow unsafeRow = taskSummaryDMStore.getDataMapRow(0);
1:       fileDetails[0] = new String(unsafeRow.getByteArray(SUMMARY_INDEX_PATH),
1:           CarbonCommonConstants.DEFAULT_CHARSET);
1:       fileDetails[1] = new String(unsafeRow.getByteArray(SUMMARY_INDEX_FILE_NAME),
1:           CarbonCommonConstants.DEFAULT_CHARSET);
1:       fileDetails[2] = new String(unsafeRow.getByteArray(SUMMARY_SEGMENTID),
1:           CarbonCommonConstants.DEFAULT_CHARSET);
1:       return fileDetails;
1:     } catch (Exception e) {
1:       throw new RuntimeException(e);
1:     }
1:   }
1: 
0:   public byte[] getColumnSchemaBinary() {
0:     DataMapRow unsafeRow = taskSummaryDMStore.getDataMapRow(0);
0:     return unsafeRow.getByteArray(SUMMARY_SCHEMA);
1:   }
1: 
1:   /**
0:    * Convert schema to binary
1:    */
0:   private byte[] convertSchemaToBinary(List<ColumnSchema> columnSchemas) throws IOException {
0:     ByteArrayOutputStream stream = new ByteArrayOutputStream();
0:     DataOutput dataOutput = new DataOutputStream(stream);
0:     dataOutput.writeShort(columnSchemas.size());
0:     for (ColumnSchema columnSchema : columnSchemas) {
0:       if (columnSchema.getColumnReferenceId() == null) {
0:         columnSchema.setColumnReferenceId(columnSchema.getColumnUniqueId());
1:       }
0:       columnSchema.write(dataOutput);
1:     }
0:     byte[] byteArray = stream.toByteArray();
0:     // Compress with snappy to reduce the size of schema
0:     return Snappy.rawCompress(byteArray, byteArray.length);
1:   }
1: 
1:   @Override public void clear() {
1:     if (memoryDMStore != null) {
1:       memoryDMStore.freeMemory();
1:       memoryDMStore = null;
0:       segmentProperties = null;
1:     }
1:     // clear task min/max unsafe memory
1:     if (null != taskSummaryDMStore) {
1:       taskSummaryDMStore.freeMemory();
1:       taskSummaryDMStore = null;
1:     }
1:   }
1: 
1:   public long getMemorySize() {
1:     long memoryUsed = 0L;
1:     if (memoryDMStore != null) {
1:       memoryUsed += memoryDMStore.getMemoryUsed();
1:     }
1:     if (null != taskSummaryDMStore) {
1:       memoryUsed += taskSummaryDMStore.getMemoryUsed();
1:     }
1:     return memoryUsed;
1:   }
1: 
0:   public SegmentProperties getSegmentProperties() {
0:     return segmentProperties;
1:   }
1: 
0:   public void setSegmentProperties(SegmentProperties segmentProperties) {
0:     this.segmentProperties = segmentProperties;
1:   }
1: 
1:   public int[] getColumnCardinality() {
0:     return columnCardinality;
1:   }
1: 
0:   protected AbstractMemoryDMStore getMemoryDMStore(CarbonRowSchema[] schema, boolean addToUnsafe)
1:       throws MemoryException {
1:     AbstractMemoryDMStore memoryDMStore;
1:     if (addToUnsafe) {
0:       memoryDMStore = new UnsafeMemoryDMStore(schema);
1:     } else {
0:       memoryDMStore = new SafeMemoryDMStore(schema);
1:     }
1:     return memoryDMStore;
1:   }
1: 
1:   /**
1:    * This method will ocnvert safe to unsafe memory DM store
1:    *
1:    * @throws MemoryException
1:    */
1:   public void convertToUnsafeDMStore() throws MemoryException {
1:     if (memoryDMStore instanceof SafeMemoryDMStore) {
0:       UnsafeMemoryDMStore unsafeMemoryDMStore = memoryDMStore.convertToUnsafeDMStore();
1:       memoryDMStore.freeMemory();
1:       memoryDMStore = unsafeMemoryDMStore;
1:     }
1:     if (taskSummaryDMStore instanceof SafeMemoryDMStore) {
0:       UnsafeMemoryDMStore unsafeSummaryMemoryDMStore = taskSummaryDMStore.convertToUnsafeDMStore();
1:       taskSummaryDMStore.freeMemory();
1:       taskSummaryDMStore = unsafeSummaryMemoryDMStore;
1:     }
1:   }
1: 
1:   /**
0:    * Read column schema from binary
1:    *
0:    * @param schemaArray
1:    * @throws IOException
1:    */
0:   public List<ColumnSchema> readColumnSchema(byte[] schemaArray) throws IOException {
0:     // uncompress it.
0:     schemaArray = Snappy.uncompress(schemaArray);
0:     ByteArrayInputStream schemaStream = new ByteArrayInputStream(schemaArray);
0:     DataInput schemaInput = new DataInputStream(schemaStream);
0:     List<ColumnSchema> columnSchemas = new ArrayList<>();
0:     int size = schemaInput.readShort();
0:     for (int i = 0; i < size; i++) {
0:       ColumnSchema columnSchema = new ColumnSchema();
0:       columnSchema.readFields(schemaInput);
0:       columnSchemas.add(columnSchema);
1:     }
0:     return columnSchemas;
1:   }
1: 
0:   public long getBlockletSchemaTime() {
0:     return blockletSchemaTime;
1:   }
1: 
1: }
author:m00258959
-------------------------------------------------------------------------------
commit:8e78957
/////////////////////////////////////////////////////////////////////////
1:             getMinMaxCacheColumns(), blockInfo.getFilePath(), summaryRow,
/////////////////////////////////////////////////////////////////////////
1:               segmentProperties, getMinMaxCacheColumns(), previousBlockInfo.getFilePath(),
1:               summaryRow,
/////////////////////////////////////////////////////////////////////////
1:               getMinMaxCacheColumns(),
/////////////////////////////////////////////////////////////////////////
1:   protected List<CarbonColumn> getMinMaxCacheColumns() {
============================================================================