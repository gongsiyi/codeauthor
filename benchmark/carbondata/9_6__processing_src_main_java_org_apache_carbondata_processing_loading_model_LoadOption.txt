1:89cfd8e: /*
1:89cfd8e:  * Licensed to the Apache Software Foundation (ASF) under one or more
1:89cfd8e:  * contributor license agreements.  See the NOTICE file distributed with
1:89cfd8e:  * this work for additional information regarding copyright ownership.
1:89cfd8e:  * The ASF licenses this file to You under the Apache License, Version 2.0
1:89cfd8e:  * (the "License"); you may not use this file except in compliance with
1:89cfd8e:  * the License.  You may obtain a copy of the License at
1:89cfd8e:  *
1:89cfd8e:  *    http://www.apache.org/licenses/LICENSE-2.0
1:89cfd8e:  *
1:89cfd8e:  * Unless required by applicable law or agreed to in writing, software
1:89cfd8e:  * distributed under the License is distributed on an "AS IS" BASIS,
1:89cfd8e:  * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
1:89cfd8e:  * See the License for the specific language governing permissions and
1:89cfd8e:  * limitations under the License.
1:89cfd8e:  */
1:89cfd8e: 
1:89cfd8e: package org.apache.carbondata.processing.loading.model;
1:89cfd8e: 
1:89cfd8e: import java.io.IOException;
1:d23f7fa: import java.util.ArrayList;
1:89cfd8e: import java.util.HashMap;
1:859d71c: import java.util.LinkedList;
1:859d71c: import java.util.List;
1:89cfd8e: import java.util.Map;
1:89cfd8e: 
1:89cfd8e: import org.apache.carbondata.common.Maps;
1:89cfd8e: import org.apache.carbondata.common.annotations.InterfaceAudience;
1:89cfd8e: import org.apache.carbondata.common.exceptions.sql.InvalidLoadOptionException;
1:89cfd8e: import org.apache.carbondata.common.logging.LogService;
1:89cfd8e: import org.apache.carbondata.common.logging.LogServiceFactory;
1:89cfd8e: import org.apache.carbondata.core.constants.CarbonCommonConstants;
1:89cfd8e: import org.apache.carbondata.core.constants.CarbonLoadOptionConstants;
1:89cfd8e: import org.apache.carbondata.core.util.CarbonProperties;
1:89cfd8e: import org.apache.carbondata.core.util.CarbonUtil;
1:89cfd8e: import org.apache.carbondata.processing.loading.exception.CarbonDataLoadingException;
1:89cfd8e: import org.apache.carbondata.processing.util.CarbonDataProcessorUtil;
1:89cfd8e: import org.apache.carbondata.processing.util.CarbonLoaderUtil;
1:89cfd8e: 
1:89cfd8e: import org.apache.commons.lang.StringUtils;
1:89cfd8e: import org.apache.hadoop.conf.Configuration;
1:89cfd8e: 
1:89cfd8e: /**
1:89cfd8e:  * Provide utilities to populate loading options
1:89cfd8e:  */
1:e72bfd1: @InterfaceAudience.Internal
1:89cfd8e: public class LoadOption {
1:89cfd8e: 
1:89cfd8e:   private static LogService LOG = LogServiceFactory.getLogService(LoadOption.class.getName());
1:89cfd8e: 
1:89cfd8e:   /**
1:89cfd8e:    * Based on the input options, fill and return data loading options with default value
1:89cfd8e:    */
1:89cfd8e:   public static Map<String, String> fillOptionWithDefaultValue(
1:89cfd8e:       Map<String, String> options) throws InvalidLoadOptionException {
1:89cfd8e:     Map<String, String> optionsFinal = new HashMap<>();
1:89cfd8e:     optionsFinal.put("delimiter", Maps.getOrDefault(options, "delimiter", ","));
1:89cfd8e:     optionsFinal.put("quotechar", Maps.getOrDefault(options, "quotechar", "\""));
1:89cfd8e:     optionsFinal.put("fileheader", Maps.getOrDefault(options, "fileheader", ""));
1:89cfd8e:     optionsFinal.put("commentchar", Maps.getOrDefault(options, "commentchar", "#"));
1:89cfd8e:     optionsFinal.put("columndict", Maps.getOrDefault(options, "columndict", null));
1:89cfd8e: 
2:89cfd8e:     optionsFinal.put(
1:89cfd8e:         "escapechar",
1:89cfd8e:         CarbonLoaderUtil.getEscapeChar(Maps.getOrDefault(options,"escapechar", "\\")));
1:89cfd8e: 
1:89cfd8e:     optionsFinal.put(
1:89cfd8e:         "serialization_null_format",
1:89cfd8e:         Maps.getOrDefault(options, "serialization_null_format", "\\N"));
1:89cfd8e: 
1:89cfd8e:     optionsFinal.put(
1:89cfd8e:         "bad_records_logger_enable",
2:89cfd8e:         Maps.getOrDefault(
2:89cfd8e:             options,
1:89cfd8e:             "bad_records_logger_enable",
3:89cfd8e:             CarbonProperties.getInstance().getProperty(
1:89cfd8e:                 CarbonLoadOptionConstants.CARBON_OPTIONS_BAD_RECORDS_LOGGER_ENABLE,
1:89cfd8e:                 CarbonLoadOptionConstants.CARBON_OPTIONS_BAD_RECORDS_LOGGER_ENABLE_DEFAULT)));
1:89cfd8e: 
1:89cfd8e:     String badRecordActionValue = CarbonProperties.getInstance().getProperty(
1:89cfd8e:         CarbonCommonConstants.CARBON_BAD_RECORDS_ACTION,
1:89cfd8e:         CarbonCommonConstants.CARBON_BAD_RECORDS_ACTION_DEFAULT);
1:89cfd8e: 
1:89cfd8e:     optionsFinal.put(
1:89cfd8e:         "bad_records_action",
1:89cfd8e:         Maps.getOrDefault(
1:89cfd8e:             options,
1:89cfd8e:             "bad_records_action",
1:89cfd8e:             CarbonProperties.getInstance().getProperty(
1:89cfd8e:                 CarbonLoadOptionConstants.CARBON_OPTIONS_BAD_RECORDS_ACTION,
1:89cfd8e:                 badRecordActionValue)));
1:89cfd8e: 
1:89cfd8e:     optionsFinal.put(
1:89cfd8e:         "is_empty_data_bad_record",
1:89cfd8e:         Maps.getOrDefault(
1:89cfd8e:             options,
1:89cfd8e:             "is_empty_data_bad_record",
1:89cfd8e:             CarbonProperties.getInstance().getProperty(
1:89cfd8e:                 CarbonLoadOptionConstants.CARBON_OPTIONS_IS_EMPTY_DATA_BAD_RECORD,
1:89cfd8e:                 CarbonLoadOptionConstants.CARBON_OPTIONS_IS_EMPTY_DATA_BAD_RECORD_DEFAULT)));
1:89cfd8e: 
1:89cfd8e:     optionsFinal.put(
1:89cfd8e:         "skip_empty_line",
1:89cfd8e:         Maps.getOrDefault(
1:89cfd8e:             options,
1:89cfd8e:             "skip_empty_line",
1:89cfd8e:             CarbonProperties.getInstance().getProperty(
1:89cfd8e:                 CarbonLoadOptionConstants.CARBON_OPTIONS_SKIP_EMPTY_LINE)));
1:89cfd8e: 
1:89cfd8e:     optionsFinal.put(
1:89cfd8e:         "all_dictionary_path",
1:89cfd8e:         Maps.getOrDefault(options, "all_dictionary_path", ""));
1:89cfd8e: 
1:89cfd8e:     optionsFinal.put(
1:89cfd8e:         "complex_delimiter_level_1",
1:d23f7fa:         Maps.getOrDefault(options,"complex_delimiter_level_1", "$"));
1:89cfd8e: 
1:89cfd8e:     optionsFinal.put(
1:89cfd8e:         "complex_delimiter_level_2",
1:d23f7fa:         Maps.getOrDefault(options, "complex_delimiter_level_2", ":"));
1:89cfd8e: 
1:89cfd8e:     optionsFinal.put(
1:89cfd8e:         "dateformat",
1:89cfd8e:         Maps.getOrDefault(
1:89cfd8e:             options,
1:89cfd8e:             "dateformat",
1:89cfd8e:             CarbonProperties.getInstance().getProperty(
1:89cfd8e:                 CarbonLoadOptionConstants.CARBON_OPTIONS_DATEFORMAT,
1:89cfd8e:                 CarbonLoadOptionConstants.CARBON_OPTIONS_DATEFORMAT_DEFAULT)));
1:89cfd8e: 
1:89cfd8e:     optionsFinal.put(
1:89cfd8e:         "timestampformat",
1:89cfd8e:         Maps.getOrDefault(
1:89cfd8e:             options,
1:89cfd8e:             "timestampformat",
1:89cfd8e:             CarbonProperties.getInstance().getProperty(
1:89cfd8e:                 CarbonLoadOptionConstants.CARBON_OPTIONS_TIMESTAMPFORMAT,
1:89cfd8e:                 CarbonLoadOptionConstants.CARBON_OPTIONS_TIMESTAMPFORMAT_DEFAULT)));
1:89cfd8e: 
1:89cfd8e:     optionsFinal.put(
1:89cfd8e:         "global_sort_partitions",
1:89cfd8e:         Maps.getOrDefault(
1:89cfd8e:             options,
1:89cfd8e:             "global_sort_partitions",
1:89cfd8e:             CarbonProperties.getInstance().getProperty(
1:89cfd8e:                 CarbonLoadOptionConstants.CARBON_OPTIONS_GLOBAL_SORT_PARTITIONS,
1:89cfd8e:                 null)));
1:89cfd8e: 
1:89cfd8e:     optionsFinal.put("maxcolumns", Maps.getOrDefault(options, "maxcolumns", null));
1:89cfd8e: 
1:89cfd8e:     optionsFinal.put(
1:89cfd8e:         "batch_sort_size_inmb",
1:89cfd8e:         Maps.getOrDefault(
1:89cfd8e:             options,
1:89cfd8e:             "batch_sort_size_inmb",
1:89cfd8e:             CarbonProperties.getInstance().getProperty(
1:89cfd8e:                 CarbonLoadOptionConstants.CARBON_OPTIONS_BATCH_SORT_SIZE_INMB,
1:89cfd8e:                 CarbonProperties.getInstance().getProperty(
1:89cfd8e:                     CarbonCommonConstants.LOAD_BATCH_SORT_SIZE_INMB,
1:89cfd8e:                     CarbonCommonConstants.LOAD_BATCH_SORT_SIZE_INMB_DEFAULT))));
1:89cfd8e: 
1:89cfd8e:     String useOnePass = Maps.getOrDefault(
1:89cfd8e:         options,
1:89cfd8e:         "single_pass",
1:89cfd8e:         CarbonProperties.getInstance().getProperty(
1:89cfd8e:             CarbonLoadOptionConstants.CARBON_OPTIONS_SINGLE_PASS,
1:89cfd8e:             CarbonLoadOptionConstants.CARBON_OPTIONS_SINGLE_PASS_DEFAULT)).trim().toLowerCase();
1:89cfd8e: 
1:89cfd8e:     boolean singlePass;
1:89cfd8e: 
1:89cfd8e:     if (useOnePass.equalsIgnoreCase("true")) {
1:89cfd8e:       singlePass = true;
1:89cfd8e:     } else {
1:89cfd8e:       // when single_pass = false  and if either alldictionarypath
1:89cfd8e:       // or columnDict is configured the do not allow load
1:89cfd8e:       if (StringUtils.isNotEmpty(optionsFinal.get("all_dictionary_path")) ||
1:89cfd8e:           StringUtils.isNotEmpty(optionsFinal.get("columndict"))) {
1:89cfd8e:         throw new InvalidLoadOptionException(
1:89cfd8e:             "Can not use all_dictionary_path or columndict without single_pass.");
1:89cfd8e:       } else {
1:89cfd8e:         singlePass = false;
1:89cfd8e:       }
1:89cfd8e:     }
1:89cfd8e: 
1:89cfd8e:     optionsFinal.put("single_pass", String.valueOf(singlePass));
1:6cb6f83:     optionsFinal.put("sort_scope", "local_sort");
1:d5396b1:     optionsFinal.put("sort_column_bounds", Maps.getOrDefault(options, "sort_column_bounds", ""));
1:ff03645:     optionsFinal.put(CarbonCommonConstants.CARBON_LOAD_MIN_SIZE_INMB,
1:ff03645:         Maps.getOrDefault(options,CarbonCommonConstants.CARBON_LOAD_MIN_SIZE_INMB,
1:ff03645:             CarbonCommonConstants.CARBON_LOAD_MIN_NODE_SIZE_INMB_DEFAULT));
1:89cfd8e:     return optionsFinal;
1:89cfd8e:   }
1:89cfd8e: 
1:89cfd8e:   /**
1:89cfd8e:    * Return CSV header field names
1:89cfd8e:    */
1:89cfd8e:   public static String[] getCsvHeaderColumns(
1:89cfd8e:       CarbonLoadModel carbonLoadModel,
1:89cfd8e:       Configuration hadoopConf) throws IOException {
1:859d71c:     return getCsvHeaderColumns(carbonLoadModel, hadoopConf, new LinkedList<String>());
1:859d71c:   }
1:89cfd8e: 
1:859d71c:   /**
1:859d71c:    * Return CSV header field names, with partition column
1:859d71c:    */
1:859d71c:   public static String[] getCsvHeaderColumns(
1:859d71c:       CarbonLoadModel carbonLoadModel,
1:859d71c:       Configuration hadoopConf,
1:859d71c:       List<String> staticPartitionCols) throws IOException {
1:89cfd8e:     String delimiter;
1:89cfd8e:     if (StringUtils.isEmpty(carbonLoadModel.getCsvDelimiter())) {
1:89cfd8e:       delimiter = CarbonCommonConstants.COMMA;
1:89cfd8e:     } else {
1:89cfd8e:       delimiter = CarbonUtil.delimiterConverter(carbonLoadModel.getCsvDelimiter());
1:89cfd8e:     }
1:89cfd8e:     String csvFile = null;
1:89cfd8e:     String csvHeader = carbonLoadModel.getCsvHeader();
1:89cfd8e:     String[] csvColumns;
1:89cfd8e:     if (StringUtils.isBlank(csvHeader)) {
1:89cfd8e:       // read header from csv file
1:89cfd8e:       csvFile = carbonLoadModel.getFactFilePath().split(",")[0];
1:89cfd8e:       csvHeader = CarbonUtil.readHeader(csvFile, hadoopConf);
1:89cfd8e:       if (StringUtils.isBlank(csvHeader)) {
1:89cfd8e:         throw new CarbonDataLoadingException("First line of the csv is not valid.");
1:89cfd8e:       }
1:89cfd8e:       String[] headers = csvHeader.toLowerCase().split(delimiter);
1:89cfd8e:       csvColumns = new String[headers.length];
1:89cfd8e:       for (int i = 0; i < csvColumns.length; i++) {
1:89cfd8e:         csvColumns[i] = headers[i].replaceAll("\"", "").trim();
1:89cfd8e:       }
1:89cfd8e:     } else {
1:89cfd8e:       String[] headers = csvHeader.toLowerCase().split(CarbonCommonConstants.COMMA);
1:89cfd8e:       csvColumns = new String[headers.length];
1:89cfd8e:       for (int i = 0; i < csvColumns.length; i++) {
1:89cfd8e:         csvColumns[i] = headers[i].trim();
1:89cfd8e:       }
1:89cfd8e:     }
1:89cfd8e: 
1:cfbf7b6:     // In SDK flow, hadoopConf will always be null,
1:cfbf7b6:     // hence FileHeader check is not required for nontransactional table
1:cfbf7b6:     if (hadoopConf != null && !CarbonDataProcessorUtil
1:280a400:         .isHeaderValid(carbonLoadModel.getTableName(), csvColumns,
1:280a400:             carbonLoadModel.getCarbonDataLoadSchema(), staticPartitionCols)) {
1:89cfd8e:       if (csvFile == null) {
1:89cfd8e:         LOG.error("CSV header in DDL is not proper."
1:89cfd8e:             + " Column names in schema and CSV header are not the same.");
1:89cfd8e:         throw new CarbonDataLoadingException(
1:89cfd8e:             "CSV header in DDL is not proper. Column names in schema and CSV header are "
1:89cfd8e:                 + "not the same.");
1:89cfd8e:       } else {
1:89cfd8e:         LOG.error(
1:89cfd8e:             "CSV header in input file is not proper. Column names in schema and csv header are not "
1:89cfd8e:                 + "the same. Input file : " + CarbonUtil.removeAKSK(csvFile));
1:89cfd8e:         throw new CarbonDataLoadingException(
1:89cfd8e:             "CSV header in input file is not proper. Column names in schema and csv header are not "
1:89cfd8e:                 + "the same. Input file : " + CarbonUtil.removeAKSK(csvFile));
1:89cfd8e:       }
1:89cfd8e:     }
1:d23f7fa: 
1:d23f7fa:     // In case of static partition columns just change the name of header if already exists as
1:d23f7fa:     // we should not take the column from csv file and add them as new columns at the end.
1:d23f7fa:     if (staticPartitionCols.size() > 0) {
1:d23f7fa:       List<String> updatedColumns = new ArrayList<>();
1:d23f7fa:       for (int i = 0; i < csvColumns.length; i++) {
1:d23f7fa:         if (staticPartitionCols.contains(csvColumns[i])) {
1:d23f7fa:           updatedColumns.add(csvColumns[i] + "1");
1:d23f7fa:         } else {
1:d23f7fa:           updatedColumns.add(csvColumns[i]);
1:d23f7fa:         }
1:d23f7fa:       }
1:d23f7fa:       updatedColumns.addAll(staticPartitionCols);
1:d23f7fa:       return updatedColumns.toArray(new String[updatedColumns.size()]);
1:d23f7fa:     } else {
1:89cfd8e:       return csvColumns;
1:89cfd8e:     }
1:89cfd8e:   }
1:859d71c: 
1:d23f7fa: }
============================================================================
author:Indhumathi27
-------------------------------------------------------------------------------
commit:cfbf7b6
/////////////////////////////////////////////////////////////////////////
1:     // In SDK flow, hadoopConf will always be null,
1:     // hence FileHeader check is not required for nontransactional table
1:     if (hadoopConf != null && !CarbonDataProcessorUtil
author:kunal642
-------------------------------------------------------------------------------
commit:7a1d12a
/////////////////////////////////////////////////////////////////////////
author:Zhang Zhichao
-------------------------------------------------------------------------------
commit:ff03645
/////////////////////////////////////////////////////////////////////////
1:     optionsFinal.put(CarbonCommonConstants.CARBON_LOAD_MIN_SIZE_INMB,
1:         Maps.getOrDefault(options,CarbonCommonConstants.CARBON_LOAD_MIN_SIZE_INMB,
1:             CarbonCommonConstants.CARBON_LOAD_MIN_NODE_SIZE_INMB_DEFAULT));
author:ndwangsen
-------------------------------------------------------------------------------
commit:685087e
/////////////////////////////////////////////////////////////////////////
0:     optionsFinal.put(CarbonCommonConstants.CARBON_LOAD_MIN_SIZE_INMB, Maps.getOrDefault(options,CarbonCommonConstants.CARBON_LOAD_MIN_SIZE_INMB, CarbonCommonConstants
0:             .CARBON_LOAD_MIN_NODE_SIZE_INMB_DEFAULT));
author:sounakr
-------------------------------------------------------------------------------
commit:b7b8073
/////////////////////////////////////////////////////////////////////////
0:     if (carbonLoadModel.isCarbonTransactionalTable() && !CarbonDataProcessorUtil
author:ajantha-bhat
-------------------------------------------------------------------------------
commit:280a400
/////////////////////////////////////////////////////////////////////////
0:     if (!carbonLoadModel.isCarbonUnmanagedTable() && !CarbonDataProcessorUtil
1:         .isHeaderValid(carbonLoadModel.getTableName(), csvColumns,
1:             carbonLoadModel.getCarbonDataLoadSchema(), staticPartitionCols)) {
author:Jacky Li
-------------------------------------------------------------------------------
commit:6cb6f83
/////////////////////////////////////////////////////////////////////////
1:     optionsFinal.put("sort_scope", "local_sort");
commit:e72bfd1
/////////////////////////////////////////////////////////////////////////
1: @InterfaceAudience.Internal
commit:89cfd8e
/////////////////////////////////////////////////////////////////////////
1: /*
1:  * Licensed to the Apache Software Foundation (ASF) under one or more
1:  * contributor license agreements.  See the NOTICE file distributed with
1:  * this work for additional information regarding copyright ownership.
1:  * The ASF licenses this file to You under the Apache License, Version 2.0
1:  * (the "License"); you may not use this file except in compliance with
1:  * the License.  You may obtain a copy of the License at
1:  *
1:  *    http://www.apache.org/licenses/LICENSE-2.0
1:  *
1:  * Unless required by applicable law or agreed to in writing, software
1:  * distributed under the License is distributed on an "AS IS" BASIS,
1:  * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
1:  * See the License for the specific language governing permissions and
1:  * limitations under the License.
1:  */
1: 
1: package org.apache.carbondata.processing.loading.model;
1: 
1: import java.io.IOException;
1: import java.util.HashMap;
1: import java.util.Map;
1: 
1: import org.apache.carbondata.common.Maps;
1: import org.apache.carbondata.common.annotations.InterfaceAudience;
1: import org.apache.carbondata.common.exceptions.sql.InvalidLoadOptionException;
1: import org.apache.carbondata.common.logging.LogService;
1: import org.apache.carbondata.common.logging.LogServiceFactory;
1: import org.apache.carbondata.core.constants.CarbonCommonConstants;
1: import org.apache.carbondata.core.constants.CarbonLoadOptionConstants;
1: import org.apache.carbondata.core.util.CarbonProperties;
1: import org.apache.carbondata.core.util.CarbonUtil;
1: import org.apache.carbondata.processing.loading.exception.CarbonDataLoadingException;
1: import org.apache.carbondata.processing.util.CarbonDataProcessorUtil;
1: import org.apache.carbondata.processing.util.CarbonLoaderUtil;
1: 
1: import org.apache.commons.lang.StringUtils;
1: import org.apache.hadoop.conf.Configuration;
1: 
1: /**
1:  * Provide utilities to populate loading options
1:  */
0: @InterfaceAudience.Developer
1: public class LoadOption {
1: 
1:   private static LogService LOG = LogServiceFactory.getLogService(LoadOption.class.getName());
1: 
1:   /**
1:    * Based on the input options, fill and return data loading options with default value
1:    */
1:   public static Map<String, String> fillOptionWithDefaultValue(
1:       Map<String, String> options) throws InvalidLoadOptionException {
1:     Map<String, String> optionsFinal = new HashMap<>();
1:     optionsFinal.put("delimiter", Maps.getOrDefault(options, "delimiter", ","));
1:     optionsFinal.put("quotechar", Maps.getOrDefault(options, "quotechar", "\""));
1:     optionsFinal.put("fileheader", Maps.getOrDefault(options, "fileheader", ""));
1:     optionsFinal.put("commentchar", Maps.getOrDefault(options, "commentchar", "#"));
1:     optionsFinal.put("columndict", Maps.getOrDefault(options, "columndict", null));
1: 
1:     optionsFinal.put(
1:         "escapechar",
1:         CarbonLoaderUtil.getEscapeChar(Maps.getOrDefault(options,"escapechar", "\\")));
1: 
1:     optionsFinal.put(
1:         "serialization_null_format",
1:         Maps.getOrDefault(options, "serialization_null_format", "\\N"));
1: 
1:     optionsFinal.put(
1:         "bad_records_logger_enable",
1:         Maps.getOrDefault(
1:             options,
1:             "bad_records_logger_enable",
1:             CarbonProperties.getInstance().getProperty(
1:                 CarbonLoadOptionConstants.CARBON_OPTIONS_BAD_RECORDS_LOGGER_ENABLE,
1:                 CarbonLoadOptionConstants.CARBON_OPTIONS_BAD_RECORDS_LOGGER_ENABLE_DEFAULT)));
1: 
1:     String badRecordActionValue = CarbonProperties.getInstance().getProperty(
1:         CarbonCommonConstants.CARBON_BAD_RECORDS_ACTION,
1:         CarbonCommonConstants.CARBON_BAD_RECORDS_ACTION_DEFAULT);
1: 
1:     optionsFinal.put(
1:         "bad_records_action",
1:         Maps.getOrDefault(
1:             options,
1:             "bad_records_action",
1:             CarbonProperties.getInstance().getProperty(
1:                 CarbonLoadOptionConstants.CARBON_OPTIONS_BAD_RECORDS_ACTION,
1:                 badRecordActionValue)));
1: 
1:     optionsFinal.put(
1:         "is_empty_data_bad_record",
1:         Maps.getOrDefault(
1:             options,
1:             "is_empty_data_bad_record",
1:             CarbonProperties.getInstance().getProperty(
1:                 CarbonLoadOptionConstants.CARBON_OPTIONS_IS_EMPTY_DATA_BAD_RECORD,
1:                 CarbonLoadOptionConstants.CARBON_OPTIONS_IS_EMPTY_DATA_BAD_RECORD_DEFAULT)));
1: 
1:     optionsFinal.put(
1:         "skip_empty_line",
1:         Maps.getOrDefault(
1:             options,
1:             "skip_empty_line",
1:             CarbonProperties.getInstance().getProperty(
1:                 CarbonLoadOptionConstants.CARBON_OPTIONS_SKIP_EMPTY_LINE)));
1: 
1:     optionsFinal.put(
1:         "all_dictionary_path",
1:         Maps.getOrDefault(options, "all_dictionary_path", ""));
1: 
1:     optionsFinal.put(
1:         "complex_delimiter_level_1",
0:         Maps.getOrDefault(options,"complex_delimiter_level_1", "\\$"));
1: 
1:     optionsFinal.put(
1:         "complex_delimiter_level_2",
0:         Maps.getOrDefault(options, "complex_delimiter_level_2", "\\:"));
1: 
1:     optionsFinal.put(
1:         "dateformat",
1:         Maps.getOrDefault(
1:             options,
1:             "dateformat",
1:             CarbonProperties.getInstance().getProperty(
1:                 CarbonLoadOptionConstants.CARBON_OPTIONS_DATEFORMAT,
1:                 CarbonLoadOptionConstants.CARBON_OPTIONS_DATEFORMAT_DEFAULT)));
1: 
1:     optionsFinal.put(
1:         "timestampformat",
1:         Maps.getOrDefault(
1:             options,
1:             "timestampformat",
1:             CarbonProperties.getInstance().getProperty(
1:                 CarbonLoadOptionConstants.CARBON_OPTIONS_TIMESTAMPFORMAT,
1:                 CarbonLoadOptionConstants.CARBON_OPTIONS_TIMESTAMPFORMAT_DEFAULT)));
1: 
1:     optionsFinal.put(
1:         "global_sort_partitions",
1:         Maps.getOrDefault(
1:             options,
1:             "global_sort_partitions",
1:             CarbonProperties.getInstance().getProperty(
1:                 CarbonLoadOptionConstants.CARBON_OPTIONS_GLOBAL_SORT_PARTITIONS,
1:                 null)));
1: 
1:     optionsFinal.put("maxcolumns", Maps.getOrDefault(options, "maxcolumns", null));
1: 
1:     optionsFinal.put(
1:         "batch_sort_size_inmb",
1:         Maps.getOrDefault(
1:             options,
1:             "batch_sort_size_inmb",
1:             CarbonProperties.getInstance().getProperty(
1:                 CarbonLoadOptionConstants.CARBON_OPTIONS_BATCH_SORT_SIZE_INMB,
1:                 CarbonProperties.getInstance().getProperty(
1:                     CarbonCommonConstants.LOAD_BATCH_SORT_SIZE_INMB,
1:                     CarbonCommonConstants.LOAD_BATCH_SORT_SIZE_INMB_DEFAULT))));
1: 
1:     optionsFinal.put(
0:         "bad_record_path",
1:         Maps.getOrDefault(
1:             options,
0:             "bad_record_path",
1:             CarbonProperties.getInstance().getProperty(
0:                 CarbonLoadOptionConstants.CARBON_OPTIONS_BAD_RECORD_PATH,
1:                 CarbonProperties.getInstance().getProperty(
0:                     CarbonCommonConstants.CARBON_BADRECORDS_LOC,
0:                     CarbonCommonConstants.CARBON_BADRECORDS_LOC_DEFAULT_VAL))));
1: 
1:     String useOnePass = Maps.getOrDefault(
1:         options,
1:         "single_pass",
1:         CarbonProperties.getInstance().getProperty(
1:             CarbonLoadOptionConstants.CARBON_OPTIONS_SINGLE_PASS,
1:             CarbonLoadOptionConstants.CARBON_OPTIONS_SINGLE_PASS_DEFAULT)).trim().toLowerCase();
1: 
1:     boolean singlePass;
1: 
1:     if (useOnePass.equalsIgnoreCase("true")) {
1:       singlePass = true;
1:     } else {
1:       // when single_pass = false  and if either alldictionarypath
1:       // or columnDict is configured the do not allow load
1:       if (StringUtils.isNotEmpty(optionsFinal.get("all_dictionary_path")) ||
1:           StringUtils.isNotEmpty(optionsFinal.get("columndict"))) {
1:         throw new InvalidLoadOptionException(
1:             "Can not use all_dictionary_path or columndict without single_pass.");
1:       } else {
1:         singlePass = false;
1:       }
1:     }
1: 
1:     optionsFinal.put("single_pass", String.valueOf(singlePass));
1:     return optionsFinal;
1:   }
1: 
1:   /**
1:    * Return CSV header field names
1:    */
1:   public static String[] getCsvHeaderColumns(
1:       CarbonLoadModel carbonLoadModel,
1:       Configuration hadoopConf) throws IOException {
1:     String delimiter;
1:     if (StringUtils.isEmpty(carbonLoadModel.getCsvDelimiter())) {
1:       delimiter = CarbonCommonConstants.COMMA;
1:     } else {
1:       delimiter = CarbonUtil.delimiterConverter(carbonLoadModel.getCsvDelimiter());
1:     }
1:     String csvFile = null;
1:     String csvHeader = carbonLoadModel.getCsvHeader();
1:     String[] csvColumns;
1:     if (StringUtils.isBlank(csvHeader)) {
1:       // read header from csv file
1:       csvFile = carbonLoadModel.getFactFilePath().split(",")[0];
1:       csvHeader = CarbonUtil.readHeader(csvFile, hadoopConf);
1:       if (StringUtils.isBlank(csvHeader)) {
1:         throw new CarbonDataLoadingException("First line of the csv is not valid.");
1:       }
1:       String[] headers = csvHeader.toLowerCase().split(delimiter);
1:       csvColumns = new String[headers.length];
1:       for (int i = 0; i < csvColumns.length; i++) {
1:         csvColumns[i] = headers[i].replaceAll("\"", "").trim();
1:       }
1:     } else {
1:       String[] headers = csvHeader.toLowerCase().split(CarbonCommonConstants.COMMA);
1:       csvColumns = new String[headers.length];
1:       for (int i = 0; i < csvColumns.length; i++) {
1:         csvColumns[i] = headers[i].trim();
1:       }
1:     }
1: 
0:     if (!CarbonDataProcessorUtil.isHeaderValid(carbonLoadModel.getTableName(), csvColumns,
0:         carbonLoadModel.getCarbonDataLoadSchema())) {
1:       if (csvFile == null) {
1:         LOG.error("CSV header in DDL is not proper."
1:             + " Column names in schema and CSV header are not the same.");
1:         throw new CarbonDataLoadingException(
1:             "CSV header in DDL is not proper. Column names in schema and CSV header are "
1:                 + "not the same.");
1:       } else {
1:         LOG.error(
1:             "CSV header in input file is not proper. Column names in schema and csv header are not "
1:                 + "the same. Input file : " + CarbonUtil.removeAKSK(csvFile));
1:         throw new CarbonDataLoadingException(
1:             "CSV header in input file is not proper. Column names in schema and csv header are not "
1:                 + "the same. Input file : " + CarbonUtil.removeAKSK(csvFile));
1:       }
1:     }
1:     return csvColumns;
1:   }
1: }
author:QiangCai
-------------------------------------------------------------------------------
commit:d23f7fa
/////////////////////////////////////////////////////////////////////////
1: import java.util.ArrayList;
/////////////////////////////////////////////////////////////////////////
1:         Maps.getOrDefault(options,"complex_delimiter_level_1", "$"));
1:         Maps.getOrDefault(options, "complex_delimiter_level_2", ":"));
/////////////////////////////////////////////////////////////////////////
1: 
1:     // In case of static partition columns just change the name of header if already exists as
1:     // we should not take the column from csv file and add them as new columns at the end.
1:     if (staticPartitionCols.size() > 0) {
1:       List<String> updatedColumns = new ArrayList<>();
1:       for (int i = 0; i < csvColumns.length; i++) {
1:         if (staticPartitionCols.contains(csvColumns[i])) {
1:           updatedColumns.add(csvColumns[i] + "1");
1:         } else {
1:           updatedColumns.add(csvColumns[i]);
1:         }
1:       }
1:       updatedColumns.addAll(staticPartitionCols);
1:       return updatedColumns.toArray(new String[updatedColumns.size()]);
1:     } else {
0:       return csvColumns;
1:     }
author:xuchuanyin
-------------------------------------------------------------------------------
commit:859d71c
/////////////////////////////////////////////////////////////////////////
1: import java.util.LinkedList;
1: import java.util.List;
/////////////////////////////////////////////////////////////////////////
1:     return getCsvHeaderColumns(carbonLoadModel, hadoopConf, new LinkedList<String>());
1:   }
1: 
1:   /**
1:    * Return CSV header field names, with partition column
1:    */
1:   public static String[] getCsvHeaderColumns(
1:       CarbonLoadModel carbonLoadModel,
1:       Configuration hadoopConf,
1:       List<String> staticPartitionCols) throws IOException {
/////////////////////////////////////////////////////////////////////////
0:         carbonLoadModel.getCarbonDataLoadSchema(), staticPartitionCols)) {
/////////////////////////////////////////////////////////////////////////
0: 
commit:d5396b1
/////////////////////////////////////////////////////////////////////////
1:     optionsFinal.put("sort_column_bounds", Maps.getOrDefault(options, "sort_column_bounds", ""));
============================================================================