1:3ff574d: /*
1:3ff574d:  * Licensed to the Apache Software Foundation (ASF) under one or more
1:3ff574d:  * contributor license agreements.  See the NOTICE file distributed with
1:3ff574d:  * this work for additional information regarding copyright ownership.
1:3ff574d:  * The ASF licenses this file to You under the Apache License, Version 2.0
1:3ff574d:  * (the "License"); you may not use this file except in compliance with
1:3ff574d:  * the License.  You may obtain a copy of the License at
1:3ff574d:  *
1:3ff574d:  *    http://www.apache.org/licenses/LICENSE-2.0
1:3ff574d:  *
1:3ff574d:  * Unless required by applicable law or agreed to in writing, software
1:3ff574d:  * distributed under the License is distributed on an "AS IS" BASIS,
1:3ff574d:  * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
1:3ff574d:  * See the License for the specific language governing permissions and
1:3ff574d:  * limitations under the License.
1:3ff574d:  */
4:3ff574d: 
1:3ff574d: package org.apache.carbondata.store.worker;
1:3ff574d: 
1:3ff574d: import java.io.IOException;
1:b338459: import java.util.HashMap;
1:3ff574d: import java.util.Iterator;
1:3ff574d: import java.util.LinkedList;
1:3ff574d: import java.util.List;
1:747be9b: import java.util.Objects;
1:3ff574d: 
1:3ff574d: import org.apache.carbondata.common.annotations.InterfaceAudience;
1:3ff574d: import org.apache.carbondata.common.logging.LogService;
1:3ff574d: import org.apache.carbondata.common.logging.LogServiceFactory;
1:784b22d: import org.apache.carbondata.core.datamap.DataMapChooser;
1:b338459: import org.apache.carbondata.core.datamap.DataMapDistributable;
1:3ff574d: import org.apache.carbondata.core.datamap.Segment;
1:b338459: import org.apache.carbondata.core.datamap.dev.expr.DataMapDistributableWrapper;
1:3ff574d: import org.apache.carbondata.core.datamap.dev.expr.DataMapExprWrapper;
1:3ff574d: import org.apache.carbondata.core.datastore.block.TableBlockInfo;
1:8f1a029: import org.apache.carbondata.core.datastore.impl.FileFactory;
1:3ff574d: import org.apache.carbondata.core.datastore.row.CarbonRow;
1:3ff574d: import org.apache.carbondata.core.indexstore.ExtendedBlocklet;
1:3ff574d: import org.apache.carbondata.core.metadata.schema.table.CarbonTable;
1:3ff574d: import org.apache.carbondata.core.metadata.schema.table.TableInfo;
1:60dfdd3: import org.apache.carbondata.core.readcommitter.TableStatusReadCommittedScope;
1:2f85381: import org.apache.carbondata.core.scan.executor.impl.SearchModeDetailQueryExecutor;
1:2f85381: import org.apache.carbondata.core.scan.executor.impl.SearchModeVectorDetailQueryExecutor;
1:3ff574d: import org.apache.carbondata.core.scan.expression.Expression;
1:784b22d: import org.apache.carbondata.core.scan.filter.resolver.FilterResolverIntf;
1:3ff574d: import org.apache.carbondata.core.scan.model.QueryModel;
1:3ff574d: import org.apache.carbondata.core.scan.model.QueryModelBuilder;
1:60dfdd3: import org.apache.carbondata.core.statusmanager.LoadMetadataDetails;
1:60dfdd3: import org.apache.carbondata.core.statusmanager.SegmentStatusManager;
1:9b45c5b: import org.apache.carbondata.core.util.CarbonTaskInfo;
1:9b45c5b: import org.apache.carbondata.core.util.ThreadLocalTaskInfo;
1:60dfdd3: import org.apache.carbondata.core.util.path.CarbonTablePath;
1:3ff574d: import org.apache.carbondata.hadoop.CarbonInputSplit;
1:3ff574d: import org.apache.carbondata.hadoop.CarbonMultiBlockSplit;
1:3ff574d: import org.apache.carbondata.hadoop.CarbonRecordReader;
1:3ff574d: import org.apache.carbondata.hadoop.readsupport.impl.CarbonRowReadSupport;
1:3ff574d: 
1:2a9604c: import org.apache.hadoop.conf.Configuration;
1:3ff574d: import org.apache.spark.search.SearchRequest;
1:3ff574d: import org.apache.spark.search.SearchResult;
1:3ff574d: import org.apache.spark.search.ShutdownRequest;
1:3ff574d: import org.apache.spark.search.ShutdownResponse;
1:3ff574d: 
1:3ff574d: /**
1:3ff574d:  * Thread runnable for handling SearchRequest from master.
1:3ff574d:  */
1:3ff574d: @InterfaceAudience.Internal
1:3ff574d: public class SearchRequestHandler {
1:3ff574d: 
1:3ff574d:   private static final LogService LOG =
1:3ff574d:       LogServiceFactory.getLogService(SearchRequestHandler.class.getName());
1:3ff574d: 
1:3ff574d:   public SearchResult handleSearch(SearchRequest request) {
1:3ff574d:     try {
1:2f85381:       LOG.info(String.format("[SearchId:%d] receive search request", request.searchId()));
1:3ff574d:       List<CarbonRow> rows = handleRequest(request);
1:2f85381:       LOG.info(String.format("[SearchId:%d] sending success response", request.searchId()));
1:3ff574d:       return createSuccessResponse(request, rows);
1:3ff574d:     } catch (IOException | InterruptedException e) {
1:3ff574d:       LOG.error(e);
1:2f85381:       LOG.info(String.format("[SearchId:%d] sending failure response", request.searchId()));
1:3ff574d:       return createFailureResponse(request, e);
5:3ff574d:     }
1:3ff574d:   }
1:3ff574d: 
1:3ff574d:   public ShutdownResponse handleShutdown(ShutdownRequest request) {
1:2f85381:     LOG.info("Shutting down worker...");
1:2f85381:     SearchModeDetailQueryExecutor.shutdownThreadPool();
1:2f85381:     SearchModeVectorDetailQueryExecutor.shutdownThreadPool();
1:2f85381:     LOG.info("Worker shutted down");
1:3ff574d:     return new ShutdownResponse(Status.SUCCESS.ordinal(), "");
1:3ff574d:   }
1:3ff574d: 
1:784b22d:   private DataMapExprWrapper chooseFGDataMap(
1:784b22d:           CarbonTable table,
1:784b22d:           FilterResolverIntf filterInterface) {
1:784b22d:     DataMapChooser chooser = null;
1:784b22d:     try {
1:784b22d:       chooser = new DataMapChooser(table);
1:784b22d:       return chooser.chooseFGDataMap(filterInterface);
1:784b22d:     } catch (IOException e) {
1:784b22d:       LOG.audit(e.getMessage());
1:784b22d:       return null;
1:784b22d:     }
1:784b22d:   }
1:784b22d: 
1:3ff574d:   /**
1:3ff574d:    * Builds {@link QueryModel} and read data from files
1:3ff574d:    */
1:3ff574d:   private List<CarbonRow> handleRequest(SearchRequest request)
1:3ff574d:       throws IOException, InterruptedException {
1:9b45c5b:     CarbonTaskInfo carbonTaskInfo = new CarbonTaskInfo();
1:9b45c5b:     carbonTaskInfo.setTaskId(System.nanoTime());
1:9b45c5b:     ThreadLocalTaskInfo.setCarbonTaskInfo(carbonTaskInfo);
1:3ff574d:     TableInfo tableInfo = request.tableInfo();
1:3ff574d:     CarbonTable table = CarbonTable.buildFromTableInfo(tableInfo);
1:3ff574d:     QueryModel queryModel = createQueryModel(table, request);
1:3ff574d: 
1:3ff574d:     // in search mode, plain reader is better since it requires less memory
1:3ff574d:     queryModel.setVectorReader(false);
1:9b45c5b: 
1:3ff574d:     CarbonMultiBlockSplit mbSplit = request.split().value();
1:b338459:     List<TableBlockInfo> list = CarbonInputSplit.createBlocks(mbSplit.getAllSplits());
1:b338459:     queryModel.setTableBlockInfos(list);
1:3ff574d:     long limit = request.limit();
1:3ff574d:     long rowCount = 0;
1:3ff574d: 
1:2f85381:     LOG.info(String.format("[SearchId:%d] %s, number of block: %d",
1:2f85381:         request.searchId(), queryModel.toString(), mbSplit.getAllSplits().size()));
1:784b22d:     DataMapExprWrapper fgDataMap = chooseFGDataMap(table,
1:784b22d:             queryModel.getFilterExpressionResolverTree());
1:2f85381: 
1:747be9b:     // If there is DataMap selected in Master, prune the split by it
1:784b22d:     if (fgDataMap != null) {
1:784b22d:       queryModel = prune(request.searchId(), table, queryModel, mbSplit, fgDataMap);
1:747be9b:     }
1:3ff574d: 
1:3ff574d:     // In search mode, reader will read multiple blocks by using a thread pool
1:3ff574d:     CarbonRecordReader<CarbonRow> reader =
1:2a9604c:         new CarbonRecordReader<>(queryModel, new CarbonRowReadSupport(), new Configuration());
1:3ff574d: 
1:3ff574d:     // read all rows by the reader
1:3ff574d:     List<CarbonRow> rows = new LinkedList<>();
1:3ff574d:     try {
1:7ef9164:       reader.initialize(mbSplit, null);
1:7ef9164: 
1:3ff574d:       // loop to read required number of rows.
1:3ff574d:       // By default, if user does not specify the limit value, limit is Long.MaxValue
1:3ff574d:       while (reader.nextKeyValue() && rowCount < limit) {
1:3ff574d:         rows.add(reader.getCurrentValue());
1:3ff574d:         rowCount++;
1:3ff574d:       }
1:3ff574d:     } catch (InterruptedException e) {
1:3ff574d:       throw new IOException(e);
1:3ff574d:     } finally {
1:3ff574d:       reader.close();
1:3ff574d:     }
1:2f85381:     LOG.info(String.format("[SearchId:%d] scan completed, return %d rows",
1:2f85381:         request.searchId(), rows.size()));
1:3ff574d:     return rows;
1:3ff574d:   }
1:3ff574d: 
1:3ff574d:   /**
1:3ff574d:    * If there is FGDataMap defined for this table and filter condition in the query,
1:3ff574d:    * prune the splits by the DataMap and set the pruned split into the QueryModel and return
1:3ff574d:    */
1:747be9b:   private QueryModel prune(int queryId, CarbonTable table, QueryModel queryModel,
1:747be9b:       CarbonMultiBlockSplit mbSplit, DataMapExprWrapper datamap) throws IOException {
1:747be9b:     Objects.requireNonNull(datamap);
1:747be9b:     List<Segment> segments = new LinkedList<>();
1:b338459:     HashMap<String, Integer> uniqueSegments = new HashMap<>();
1:60dfdd3:     LoadMetadataDetails[] loadMetadataDetails =
1:60dfdd3:         SegmentStatusManager.readLoadMetadata(
1:60dfdd3:             CarbonTablePath.getMetadataPath(table.getTablePath()));
1:747be9b:     for (CarbonInputSplit split : mbSplit.getAllSplits()) {
1:60dfdd3:       String segmentId = Segment.getSegment(split.getSegmentId(), loadMetadataDetails).toString();
1:b338459:       if (uniqueSegments.get(segmentId) == null) {
1:60dfdd3:         segments.add(Segment.toSegment(segmentId,
1:60dfdd3:             new TableStatusReadCommittedScope(table.getAbsoluteTableIdentifier(),
1:8f1a029:                 loadMetadataDetails, FileFactory.getConfiguration())));
1:b338459:         uniqueSegments.put(segmentId, 1);
1:b338459:       } else {
1:b338459:         uniqueSegments.put(segmentId, uniqueSegments.get(segmentId) + 1);
1:b338459:       }
1:747be9b:     }
1:747be9b: 
1:b338459:     List<DataMapDistributableWrapper> distributables = datamap.toDistributable(segments);
1:b338459:     List<ExtendedBlocklet> prunnedBlocklets = new LinkedList<ExtendedBlocklet>();
1:b338459:     for (int i = 0; i < distributables.size(); i++) {
1:b338459:       DataMapDistributable dataMapDistributable = distributables.get(i).getDistributable();
1:b338459:       prunnedBlocklets.addAll(datamap.prune(dataMapDistributable, null));
1:b338459:     }
1:b338459: 
1:b338459:     HashMap<String, ExtendedBlocklet> pathToRead = new HashMap<>();
1:b338459:     for (ExtendedBlocklet prunedBlocklet : prunnedBlocklets) {
1:a5039ba:       pathToRead.put(prunedBlocklet.getFilePath().replace('\\', '/'), prunedBlocklet);
1:3ff574d:     }
1:747be9b: 
1:747be9b:     List<TableBlockInfo> blocks = queryModel.getTableBlockInfos();
1:747be9b:     List<TableBlockInfo> blockToRead = new LinkedList<>();
1:747be9b:     for (TableBlockInfo block : blocks) {
1:b338459:       if (pathToRead.keySet().contains(block.getFilePath())) {
1:b338459:         // If not set this, it will can't create FineGrainBlocklet object in
1:b338459:         // org.apache.carbondata.core.indexstore.blockletindex.BlockletDataRefNode.getIndexedData
1:b338459:         block.setDataMapWriterPath(pathToRead.get(block.getFilePath()).getDataMapWriterPath());
1:747be9b:         blockToRead.add(block);
1:747be9b:       }
1:747be9b:     }
1:747be9b:     LOG.info(String.format("[SearchId:%d] pruned using FG DataMap, pruned blocks: %d", queryId,
1:747be9b:         blockToRead.size()));
1:747be9b:     queryModel.setTableBlockInfos(blockToRead);
1:5593d16:     queryModel.setFG(true);
1:3ff574d:     return queryModel;
1:3ff574d:   }
1:3ff574d: 
1:3ff574d:   private QueryModel createQueryModel(CarbonTable table, SearchRequest request) {
1:3ff574d:     String[] projectColumns = request.projectColumns();
1:3ff574d:     Expression filter = null;
1:3ff574d:     if (request.filterExpression() != null) {
1:3ff574d:       filter = request.filterExpression();
1:3ff574d:     }
1:3ff574d:     return new QueryModelBuilder(table)
1:3ff574d:         .projectColumns(projectColumns)
1:3ff574d:         .filterExpression(filter)
1:3ff574d:         .build();
1:3ff574d:   }
1:3ff574d: 
1:3ff574d:   /**
1:3ff574d:    * create a failure response
1:3ff574d:    */
1:3ff574d:   private SearchResult createFailureResponse(SearchRequest request, Throwable throwable) {
1:2f85381:     return new SearchResult(request.searchId(), Status.FAILURE.ordinal(), throwable.getMessage(),
1:3ff574d:         new Object[0][]);
1:3ff574d:   }
1:3ff574d: 
1:3ff574d:   /**
1:3ff574d:    * create a success response with result rows
1:3ff574d:    */
1:3ff574d:   private SearchResult createSuccessResponse(SearchRequest request, List<CarbonRow> rows) {
1:3ff574d:     Iterator<CarbonRow> itor = rows.iterator();
1:3ff574d:     Object[][] output = new Object[rows.size()][];
1:3ff574d:     int i = 0;
1:3ff574d:     while (itor.hasNext()) {
1:3ff574d:       output[i++] = itor.next().getData();
1:3ff574d:     }
1:2f85381:     return new SearchResult(request.searchId(), Status.SUCCESS.ordinal(), "", output);
1:3ff574d:   }
1:3ff574d: 
1:3ff574d: }
============================================================================
author:kunal642
-------------------------------------------------------------------------------
commit:8f1a029
/////////////////////////////////////////////////////////////////////////
1: import org.apache.carbondata.core.datastore.impl.FileFactory;
/////////////////////////////////////////////////////////////////////////
1:                 loadMetadataDetails, FileFactory.getConfiguration())));
commit:2a9604c
/////////////////////////////////////////////////////////////////////////
1: import org.apache.hadoop.conf.Configuration;
/////////////////////////////////////////////////////////////////////////
1:         new CarbonRecordReader<>(queryModel, new CarbonRowReadSupport(), new Configuration());
author:xuchuanyin
-------------------------------------------------------------------------------
commit:a5039ba
/////////////////////////////////////////////////////////////////////////
1:       pathToRead.put(prunedBlocklet.getFilePath().replace('\\', '/'), prunedBlocklet);
author:xubo245
-------------------------------------------------------------------------------
commit:5593d16
/////////////////////////////////////////////////////////////////////////
1:     queryModel.setFG(true);
commit:b338459
/////////////////////////////////////////////////////////////////////////
1: import java.util.HashMap;
/////////////////////////////////////////////////////////////////////////
1: import org.apache.carbondata.core.datamap.DataMapDistributable;
1: import org.apache.carbondata.core.datamap.dev.expr.DataMapDistributableWrapper;
/////////////////////////////////////////////////////////////////////////
1:     List<TableBlockInfo> list = CarbonInputSplit.createBlocks(mbSplit.getAllSplits());
1:     queryModel.setTableBlockInfos(list);
/////////////////////////////////////////////////////////////////////////
1:     HashMap<String, Integer> uniqueSegments = new HashMap<>();
0:       String segmentId = split.getSegmentId();
1:       if (uniqueSegments.get(segmentId) == null) {
0:         segments.add(Segment.toSegment(
0:                 segmentId,
0:                 new LatestFilesReadCommittedScope(table.getTablePath(), segmentId)));
1:         uniqueSegments.put(segmentId, 1);
1:       } else {
1:         uniqueSegments.put(segmentId, uniqueSegments.get(segmentId) + 1);
1:       }
1:     List<DataMapDistributableWrapper> distributables = datamap.toDistributable(segments);
1:     List<ExtendedBlocklet> prunnedBlocklets = new LinkedList<ExtendedBlocklet>();
1:     for (int i = 0; i < distributables.size(); i++) {
1:       DataMapDistributable dataMapDistributable = distributables.get(i).getDistributable();
1:       prunnedBlocklets.addAll(datamap.prune(dataMapDistributable, null));
1:     }
1: 
1:     HashMap<String, ExtendedBlocklet> pathToRead = new HashMap<>();
1:     for (ExtendedBlocklet prunedBlocklet : prunnedBlocklets) {
0:       pathToRead.put(prunedBlocklet.getFilePath(), prunedBlocklet);
1:       if (pathToRead.keySet().contains(block.getFilePath())) {
1:         // If not set this, it will can't create FineGrainBlocklet object in
1:         // org.apache.carbondata.core.indexstore.blockletindex.BlockletDataRefNode.getIndexedData
1:         block.setDataMapWriterPath(pathToRead.get(block.getFilePath()).getDataMapWriterPath());
commit:784b22d
/////////////////////////////////////////////////////////////////////////
1: import org.apache.carbondata.core.datamap.DataMapChooser;
/////////////////////////////////////////////////////////////////////////
1: import org.apache.carbondata.core.scan.filter.resolver.FilterResolverIntf;
/////////////////////////////////////////////////////////////////////////
1:   private DataMapExprWrapper chooseFGDataMap(
1:           CarbonTable table,
1:           FilterResolverIntf filterInterface) {
1:     DataMapChooser chooser = null;
1:     try {
1:       chooser = new DataMapChooser(table);
1:       return chooser.chooseFGDataMap(filterInterface);
1:     } catch (IOException e) {
1:       LOG.audit(e.getMessage());
1:       return null;
1:     }
1:   }
1: 
/////////////////////////////////////////////////////////////////////////
1:     DataMapExprWrapper fgDataMap = chooseFGDataMap(table,
1:             queryModel.getFilterExpressionResolverTree());
1:     if (fgDataMap != null) {
1:       queryModel = prune(request.searchId(), table, queryModel, mbSplit, fgDataMap);
author:ravipesala
-------------------------------------------------------------------------------
commit:60dfdd3
/////////////////////////////////////////////////////////////////////////
1: import org.apache.carbondata.core.readcommitter.TableStatusReadCommittedScope;
1: import org.apache.carbondata.core.statusmanager.LoadMetadataDetails;
1: import org.apache.carbondata.core.statusmanager.SegmentStatusManager;
1: import org.apache.carbondata.core.util.path.CarbonTablePath;
/////////////////////////////////////////////////////////////////////////
1:     LoadMetadataDetails[] loadMetadataDetails =
1:         SegmentStatusManager.readLoadMetadata(
1:             CarbonTablePath.getMetadataPath(table.getTablePath()));
1:       String segmentId = Segment.getSegment(split.getSegmentId(), loadMetadataDetails).toString();
1:         segments.add(Segment.toSegment(segmentId,
1:             new TableStatusReadCommittedScope(table.getAbsoluteTableIdentifier(),
0:                 loadMetadataDetails)));
commit:9b45c5b
/////////////////////////////////////////////////////////////////////////
1: import org.apache.carbondata.core.util.CarbonTaskInfo;
1: import org.apache.carbondata.core.util.ThreadLocalTaskInfo;
/////////////////////////////////////////////////////////////////////////
1:     CarbonTaskInfo carbonTaskInfo = new CarbonTaskInfo();
1:     carbonTaskInfo.setTaskId(System.nanoTime());
1:     ThreadLocalTaskInfo.setCarbonTaskInfo(carbonTaskInfo);
1: 
author:Raghunandan S
-------------------------------------------------------------------------------
commit:7ef9164
/////////////////////////////////////////////////////////////////////////
1:       reader.initialize(mbSplit, null);
1: 
author:Jacky Li
-------------------------------------------------------------------------------
commit:747be9b
/////////////////////////////////////////////////////////////////////////
1: import java.util.Objects;
/////////////////////////////////////////////////////////////////////////
1:     // If there is DataMap selected in Master, prune the split by it
0:     if (request.dataMap() != null) {
0:       queryModel = prune(request.searchId(), table, queryModel, mbSplit, request.dataMap().get());
1:     }
/////////////////////////////////////////////////////////////////////////
1:   private QueryModel prune(int queryId, CarbonTable table, QueryModel queryModel,
1:       CarbonMultiBlockSplit mbSplit, DataMapExprWrapper datamap) throws IOException {
1:     Objects.requireNonNull(datamap);
1:     List<Segment> segments = new LinkedList<>();
1:     for (CarbonInputSplit split : mbSplit.getAllSplits()) {
0:       segments.add(
0:           Segment.toSegment(split.getSegmentId(),
0:               new LatestFilesReadCommittedScope(table.getTablePath())));
0:     List<ExtendedBlocklet> prunnedBlocklets = datamap.prune(segments, null);
1: 
0:     List<String> pathToRead = new LinkedList<>();
0:     for (ExtendedBlocklet prunnedBlocklet : prunnedBlocklets) {
0:       pathToRead.add(prunnedBlocklet.getPath());
1:     }
1: 
1:     List<TableBlockInfo> blocks = queryModel.getTableBlockInfos();
1:     List<TableBlockInfo> blockToRead = new LinkedList<>();
1:     for (TableBlockInfo block : blocks) {
0:       if (pathToRead.contains(block.getFilePath())) {
1:         blockToRead.add(block);
1:       }
1:     }
1:     LOG.info(String.format("[SearchId:%d] pruned using FG DataMap, pruned blocks: %d", queryId,
1:         blockToRead.size()));
1:     queryModel.setTableBlockInfos(blockToRead);
commit:03a735b
/////////////////////////////////////////////////////////////////////////
0:     if (wrapper.getDataMapLevel() == DataMapLevel.FG) {
commit:2f85381
/////////////////////////////////////////////////////////////////////////
1: import org.apache.carbondata.core.scan.executor.impl.SearchModeDetailQueryExecutor;
1: import org.apache.carbondata.core.scan.executor.impl.SearchModeVectorDetailQueryExecutor;
/////////////////////////////////////////////////////////////////////////
1:       LOG.info(String.format("[SearchId:%d] receive search request", request.searchId()));
1:       LOG.info(String.format("[SearchId:%d] sending success response", request.searchId()));
1:       LOG.info(String.format("[SearchId:%d] sending failure response", request.searchId()));
1:     LOG.info("Shutting down worker...");
1:     SearchModeDetailQueryExecutor.shutdownThreadPool();
1:     SearchModeVectorDetailQueryExecutor.shutdownThreadPool();
1:     LOG.info("Worker shutted down");
/////////////////////////////////////////////////////////////////////////
1:     LOG.info(String.format("[SearchId:%d] %s, number of block: %d",
1:         request.searchId(), queryModel.toString(), mbSplit.getAllSplits().size()));
1: 
0:     queryModel = tryPruneByFGDataMap(request.searchId(), table, queryModel, mbSplit);
/////////////////////////////////////////////////////////////////////////
1:     LOG.info(String.format("[SearchId:%d] scan completed, return %d rows",
1:         request.searchId(), rows.size()));
/////////////////////////////////////////////////////////////////////////
0:   private QueryModel tryPruneByFGDataMap(int queryId,
/////////////////////////////////////////////////////////////////////////
0:       LOG.info(String.format("[SearchId:%d] pruned using FG DataMap, pruned blocks: %d",
0:           queryId, blockToRead.size()));
/////////////////////////////////////////////////////////////////////////
1:     return new SearchResult(request.searchId(), Status.FAILURE.ordinal(), throwable.getMessage(),
/////////////////////////////////////////////////////////////////////////
1:     return new SearchResult(request.searchId(), Status.SUCCESS.ordinal(), "", output);
commit:3ff574d
/////////////////////////////////////////////////////////////////////////
1: /*
1:  * Licensed to the Apache Software Foundation (ASF) under one or more
1:  * contributor license agreements.  See the NOTICE file distributed with
1:  * this work for additional information regarding copyright ownership.
1:  * The ASF licenses this file to You under the Apache License, Version 2.0
1:  * (the "License"); you may not use this file except in compliance with
1:  * the License.  You may obtain a copy of the License at
1:  *
1:  *    http://www.apache.org/licenses/LICENSE-2.0
1:  *
1:  * Unless required by applicable law or agreed to in writing, software
1:  * distributed under the License is distributed on an "AS IS" BASIS,
1:  * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
1:  * See the License for the specific language governing permissions and
1:  * limitations under the License.
1:  */
1: 
1: package org.apache.carbondata.store.worker;
1: 
1: import java.io.IOException;
1: import java.util.Iterator;
1: import java.util.LinkedList;
1: import java.util.List;
1: 
1: import org.apache.carbondata.common.annotations.InterfaceAudience;
1: import org.apache.carbondata.common.logging.LogService;
1: import org.apache.carbondata.common.logging.LogServiceFactory;
0: import org.apache.carbondata.core.datamap.DataMapChooser;
0: import org.apache.carbondata.core.datamap.DataMapLevel;
1: import org.apache.carbondata.core.datamap.Segment;
1: import org.apache.carbondata.core.datamap.dev.expr.DataMapExprWrapper;
1: import org.apache.carbondata.core.datastore.block.TableBlockInfo;
1: import org.apache.carbondata.core.datastore.row.CarbonRow;
1: import org.apache.carbondata.core.indexstore.ExtendedBlocklet;
1: import org.apache.carbondata.core.metadata.schema.table.CarbonTable;
1: import org.apache.carbondata.core.metadata.schema.table.TableInfo;
0: import org.apache.carbondata.core.readcommitter.LatestFilesReadCommittedScope;
1: import org.apache.carbondata.core.scan.expression.Expression;
1: import org.apache.carbondata.core.scan.model.QueryModel;
1: import org.apache.carbondata.core.scan.model.QueryModelBuilder;
1: import org.apache.carbondata.hadoop.CarbonInputSplit;
1: import org.apache.carbondata.hadoop.CarbonMultiBlockSplit;
1: import org.apache.carbondata.hadoop.CarbonRecordReader;
1: import org.apache.carbondata.hadoop.readsupport.impl.CarbonRowReadSupport;
1: 
1: import org.apache.spark.search.SearchRequest;
1: import org.apache.spark.search.SearchResult;
1: import org.apache.spark.search.ShutdownRequest;
1: import org.apache.spark.search.ShutdownResponse;
1: 
1: /**
1:  * Thread runnable for handling SearchRequest from master.
1:  */
1: @InterfaceAudience.Internal
1: public class SearchRequestHandler {
1: 
1:   private static final LogService LOG =
1:       LogServiceFactory.getLogService(SearchRequestHandler.class.getName());
1: 
1:   public SearchResult handleSearch(SearchRequest request) {
1:     try {
1:       List<CarbonRow> rows = handleRequest(request);
1:       return createSuccessResponse(request, rows);
1:     } catch (IOException | InterruptedException e) {
1:       LOG.error(e);
1:       return createFailureResponse(request, e);
1:     }
1:   }
1: 
1:   public ShutdownResponse handleShutdown(ShutdownRequest request) {
1:     return new ShutdownResponse(Status.SUCCESS.ordinal(), "");
1:   }
1: 
1:   /**
1:    * Builds {@link QueryModel} and read data from files
1:    */
1:   private List<CarbonRow> handleRequest(SearchRequest request)
1:       throws IOException, InterruptedException {
1:     TableInfo tableInfo = request.tableInfo();
1:     CarbonTable table = CarbonTable.buildFromTableInfo(tableInfo);
1:     QueryModel queryModel = createQueryModel(table, request);
1: 
1:     // in search mode, plain reader is better since it requires less memory
1:     queryModel.setVectorReader(false);
1:     CarbonMultiBlockSplit mbSplit = request.split().value();
1:     long limit = request.limit();
1:     long rowCount = 0;
1: 
0:     // If there is FGDataMap, prune the split by applying FGDataMap
0:     queryModel = tryPruneByFGDataMap(table, queryModel, mbSplit);
1: 
1:     // In search mode, reader will read multiple blocks by using a thread pool
1:     CarbonRecordReader<CarbonRow> reader =
0:         new CarbonRecordReader<>(queryModel, new CarbonRowReadSupport());
0:     reader.initialize(mbSplit, null);
1: 
1:     // read all rows by the reader
1:     List<CarbonRow> rows = new LinkedList<>();
1:     try {
1:       // loop to read required number of rows.
1:       // By default, if user does not specify the limit value, limit is Long.MaxValue
1:       while (reader.nextKeyValue() && rowCount < limit) {
1:         rows.add(reader.getCurrentValue());
1:         rowCount++;
1:       }
1:     } catch (InterruptedException e) {
1:       throw new IOException(e);
1:     } finally {
1:       reader.close();
1:     }
1:     return rows;
1:   }
1: 
1:   /**
1:    * If there is FGDataMap defined for this table and filter condition in the query,
1:    * prune the splits by the DataMap and set the pruned split into the QueryModel and return
1:    */
0:   private QueryModel tryPruneByFGDataMap(
0:       CarbonTable table, QueryModel queryModel, CarbonMultiBlockSplit mbSplit) throws IOException {
0:     DataMapExprWrapper wrapper =
0:         DataMapChooser.get().choose(table, queryModel.getFilterExpressionResolverTree());
1: 
0:     if (wrapper.getDataMapType() == DataMapLevel.FG) {
0:       List<Segment> segments = new LinkedList<>();
0:       for (CarbonInputSplit split : mbSplit.getAllSplits()) {
0:         segments.add(Segment.toSegment(
0:             split.getSegmentId(), new LatestFilesReadCommittedScope(table.getTablePath())));
1:       }
0:       List<ExtendedBlocklet> prunnedBlocklets = wrapper.prune(segments, null);
1: 
0:       List<String> pathToRead = new LinkedList<>();
0:       for (ExtendedBlocklet prunnedBlocklet : prunnedBlocklets) {
0:         pathToRead.add(prunnedBlocklet.getPath());
1:       }
1: 
0:       List<TableBlockInfo> blocks = queryModel.getTableBlockInfos();
0:       List<TableBlockInfo> blockToRead = new LinkedList<>();
0:       for (TableBlockInfo block : blocks) {
0:         if (pathToRead.contains(block.getFilePath())) {
0:           blockToRead.add(block);
1:         }
1:       }
0:       queryModel.setTableBlockInfos(blockToRead);
1:     }
1:     return queryModel;
1:   }
1: 
1:   private QueryModel createQueryModel(CarbonTable table, SearchRequest request) {
1:     String[] projectColumns = request.projectColumns();
1:     Expression filter = null;
1:     if (request.filterExpression() != null) {
1:       filter = request.filterExpression();
1:     }
1:     return new QueryModelBuilder(table)
1:         .projectColumns(projectColumns)
1:         .filterExpression(filter)
1:         .build();
1:   }
1: 
1:   /**
1:    * create a failure response
1:    */
1:   private SearchResult createFailureResponse(SearchRequest request, Throwable throwable) {
0:     return new SearchResult(request.queryId(), Status.FAILURE.ordinal(), throwable.getMessage(),
1:         new Object[0][]);
1:   }
1: 
1:   /**
1:    * create a success response with result rows
1:    */
1:   private SearchResult createSuccessResponse(SearchRequest request, List<CarbonRow> rows) {
1:     Iterator<CarbonRow> itor = rows.iterator();
1:     Object[][] output = new Object[rows.size()][];
1:     int i = 0;
1:     while (itor.hasNext()) {
1:       output[i++] = itor.next().getData();
1:     }
0:     return new SearchResult(request.queryId(), Status.SUCCESS.ordinal(), "", output);
1:   }
1: 
1: }
============================================================================