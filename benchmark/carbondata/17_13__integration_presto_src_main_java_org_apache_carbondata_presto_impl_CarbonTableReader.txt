1:0e93a3f: /*
1:0e93a3f:  * Licensed to the Apache Software Foundation (ASF) under one or more
1:0e93a3f:  * contributor license agreements.  See the NOTICE file distributed with
1:0e93a3f:  * this work for additional information regarding copyright ownership.
1:0e93a3f:  * The ASF licenses this file to You under the Apache License, Version 2.0
1:0e93a3f:  * (the "License"); you may not use this file except in compliance with
1:0e93a3f:  * the License.  You may obtain a copy of the License at
3:0e93a3f:  *
1:0e93a3f:  *    http://www.apache.org/licenses/LICENSE-2.0
1:0e93a3f:  *
1:0e93a3f:  * Unless required by applicable law or agreed to in writing, software
1:0e93a3f:  * distributed under the License is distributed on an "AS IS" BASIS,
1:0e93a3f:  * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
1:0e93a3f:  * See the License for the specific language governing permissions and
1:0e93a3f:  * limitations under the License.
1:0e93a3f:  */
59:0e93a3f: 
1:0e93a3f: package org.apache.carbondata.presto.impl;
1:0e93a3f: 
1:0e93a3f: import java.io.IOException;
1:b681244: import java.util.ArrayList;
1:b681244: import java.util.Arrays;
1:e5e74fc: import java.util.Calendar;
1:e5e74fc: import java.util.Date;
1:e5e74fc: import java.util.HashMap;
1:3740535: import java.util.HashSet;
1:b681244: import java.util.List;
1:dc4f87b: import java.util.Objects;
1:b681244: import java.util.Set;
1:b681244: import java.util.UUID;
1:e5e74fc: import java.util.concurrent.atomic.AtomicReference;
1:0e93a3f: import java.util.stream.Collectors;
1:0e93a3f: import java.util.stream.Stream;
1:0e93a3f: 
1:d4a1577: import static java.util.Objects.requireNonNull;
1:d4a1577: 
1:3740535: import org.apache.carbondata.common.logging.LogService;
1:3740535: import org.apache.carbondata.common.logging.LogServiceFactory;
1:fca960e: import org.apache.carbondata.core.constants.CarbonCommonConstants;
1:c6ca640: import org.apache.carbondata.core.datamap.DataMapStoreManager;
1:0e93a3f: import org.apache.carbondata.core.datastore.filesystem.CarbonFile;
1:0e93a3f: import org.apache.carbondata.core.datastore.impl.FileFactory;
1:3740535: import org.apache.carbondata.core.indexstore.PartitionSpec;
1:0e93a3f: import org.apache.carbondata.core.metadata.AbsoluteTableIdentifier;
1:0e93a3f: import org.apache.carbondata.core.metadata.CarbonMetadata;
1:0e93a3f: import org.apache.carbondata.core.metadata.CarbonTableIdentifier;
1:3740535: import org.apache.carbondata.core.metadata.SegmentFileStore;
1:0e93a3f: import org.apache.carbondata.core.metadata.converter.SchemaConverter;
1:0e93a3f: import org.apache.carbondata.core.metadata.converter.ThriftWrapperSchemaConverterImpl;
1:3740535: import org.apache.carbondata.core.metadata.schema.PartitionInfo;
1:3740535: import org.apache.carbondata.core.metadata.schema.partition.PartitionType;
1:0e93a3f: import org.apache.carbondata.core.metadata.schema.table.CarbonTable;
1:0e93a3f: import org.apache.carbondata.core.metadata.schema.table.TableInfo;
1:0e93a3f: import org.apache.carbondata.core.reader.ThriftReader;
1:0e93a3f: import org.apache.carbondata.core.scan.expression.Expression;
1:3740535: import org.apache.carbondata.core.statusmanager.LoadMetadataDetails;
1:3740535: import org.apache.carbondata.core.statusmanager.SegmentStatusManager;
1:fca960e: import org.apache.carbondata.core.util.CarbonProperties;
1:0e93a3f: import org.apache.carbondata.core.util.path.CarbonTablePath;
1:9c83bd1: import org.apache.carbondata.hadoop.CarbonInputSplit;
1:9c83bd1: import org.apache.carbondata.hadoop.api.CarbonTableInputFormat;
1:3740535: import org.apache.carbondata.presto.PrestoFilterUtil;
1:0e93a3f: 
1:9c83bd1: import com.facebook.presto.hadoop.$internal.com.google.gson.Gson;
1:e5e74fc: import com.facebook.presto.hadoop.$internal.io.netty.util.internal.ConcurrentSet;
1:3740535: import com.facebook.presto.hadoop.$internal.org.apache.commons.collections.CollectionUtils;
1:3740535: import com.facebook.presto.spi.ColumnHandle;
1:0e93a3f: import com.facebook.presto.spi.SchemaTableName;
1:9c83bd1: import com.facebook.presto.spi.TableNotFoundException;
1:0e93a3f: import com.facebook.presto.spi.classloader.ThreadContextClassLoader;
1:3740535: import com.facebook.presto.spi.predicate.TupleDomain;
1:0e93a3f: import com.google.common.collect.ImmutableList;
1:0e93a3f: import com.google.common.collect.ImmutableSet;
1:0e93a3f: import com.google.inject.Inject;
1:e5e74fc: import org.apache.commons.lang.time.DateUtils;
1:0e93a3f: import org.apache.hadoop.conf.Configuration;
1:b681244: import org.apache.hadoop.fs.Path;
1:b681244: import org.apache.hadoop.fs.PathFilter;
1:9c83bd1: import org.apache.hadoop.mapred.JobConf;
1:0e93a3f: import org.apache.hadoop.mapreduce.InputSplit;
1:9c83bd1: import org.apache.hadoop.mapreduce.Job;
1:0e93a3f: import org.apache.thrift.TBase;
1:0e93a3f: 
1:dc4f87b: import static org.apache.hadoop.fs.s3a.Constants.ACCESS_KEY;
1:dc4f87b: import static org.apache.hadoop.fs.s3a.Constants.ENDPOINT;
1:dc4f87b: import static org.apache.hadoop.fs.s3a.Constants.SECRET_KEY;
1:0e93a3f: 
1:9c83bd1: /**
1:9c83bd1:  * CarbonTableReader will be a facade of these utils
1:0e93a3f:  * 1:CarbonMetadata,(logic table)
1:0e93a3f:  * 2:FileFactory, (physic table file)
1:0e93a3f:  * 3:CarbonCommonFactory, (offer some )
1:0e93a3f:  * 4:DictionaryFactory, (parse dictionary util)
1:b699ee6:  * Currently, it is mainly used to parse metadata of tables under
1:b699ee6:  * the configured carbondata-store path and filter the relevant
1:b699ee6:  * input splits with given query predicates.
1:0e93a3f:  */
1:0e93a3f: public class CarbonTableReader {
1:0e93a3f: 
1:9c83bd1:   // default PathFilter, accepts files in carbondata format (with .carbondata extension).
1:9c83bd1:   private static final PathFilter DefaultFilter = new PathFilter() {
1:9c83bd1:     @Override public boolean accept(Path path) {
1:9c83bd1:       return CarbonTablePath.isCarbonDataFile(path.getName());
1:fca960e:     }
1:9c83bd1:   };
1:dc4f87b:   public CarbonTableConfig config;
1:b699ee6:   /**
1:b699ee6:    * The names of the tables under the schema (this.carbonFileList).
1:b699ee6:    */
1:e5e74fc:   private ConcurrentSet<SchemaTableName> tableList;
1:b699ee6:   /**
1:b699ee6:    * carbonFileList represents the store path of the schema, which is configured as carbondata-store
1:b699ee6:    * in the CarbonData catalog file ($PRESTO_HOME$/etc/catalog/carbondata.properties).
1:b699ee6:    */
1:7c0e660:   private CarbonFile carbonFileList;
1:0e93a3f:   private FileFactory.FileType fileType;
1:b699ee6:   /**
1:b699ee6:    * A cache for Carbon reader, with this cache,
1:b699ee6:    * metadata of a table is only read from file system once.
1:b699ee6:    */
1:e5e74fc:   private AtomicReference<HashMap<SchemaTableName, CarbonTableCacheModel>> carbonCache;
1:9c83bd1: 
1:d4a1577:   private LoadMetadataDetails[] loadMetadataDetails;
1:3740535: 
1:01b48fc:   private String queryId;
1:01b48fc: 
1:3740535:   /**
1:3740535:    * Logger instance
1:3740535:    */
1:3740535:   private static final LogService LOGGER =
1:3740535:       LogServiceFactory.getLogService(CarbonTableReader.class.getName());
1:3740535: 
1:15ab6b0:   /**
1:15ab6b0:    * List Of Schemas
1:15ab6b0:    */
1:d4a1577:   private List<String> schemaNames = new ArrayList<>();
1:3740535: 
1:0e93a3f:   @Inject public CarbonTableReader(CarbonTableConfig config) {
1:0e93a3f:     this.config = requireNonNull(config, "CarbonTableConfig is null");
1:e5e74fc:     this.carbonCache = new AtomicReference(new HashMap());
1:e5e74fc:     tableList = new ConcurrentSet<>();
1:dc4f87b:     setS3Properties();
1:01b48fc:     populateCarbonProperties();
1:9c83bd1:   }
1:c6ca640: 
1:b699ee6:   /**
1:b699ee6:    * For presto worker node to initialize the metadata cache of a table.
1:9c83bd1:    *
1:b699ee6:    * @param table the name of the table and schema.
1:b699ee6:    * @return
1:b699ee6:    */
1:0e93a3f:   public CarbonTableCacheModel getCarbonCache(SchemaTableName table) {
1:c6ca640: 
1:e5e74fc:     if (!carbonCache.get().containsKey(table) || carbonCache.get().get(table) == null) {
1:e5e74fc:       // if this table is not cached, try to read the metadata of the table and cache it.
1:0e93a3f:       try (ThreadContextClassLoader ignored = new ThreadContextClassLoader(
1:e5e74fc:           FileFactory.class.getClassLoader())) {
1:7c0e660:         if (carbonFileList == null) {
1:0e93a3f:           fileType = FileFactory.getFileType(config.getStorePath());
8:0e93a3f:           try {
1:7c0e660:             carbonFileList = FileFactory.getCarbonFile(config.getStorePath(), fileType);
3:0e93a3f:           } catch (Exception ex) {
2:0e93a3f:             throw new RuntimeException(ex);
1:dc4f87b:           }
1:32405f4:         }
69:0e93a3f:       }
1:7036696:       updateSchemaTables(table);
1:0e93a3f:       parseCarbonMetadata(table);
1:0e93a3f:     }
1:e5e74fc:     if (carbonCache.get().containsKey(table)) {
1:e5e74fc:       return carbonCache.get().get(table);
1:9c83bd1:     } else {
1:9c83bd1:       return null;
1:9c83bd1:     }
1:0e93a3f:   }
1:0e93a3f: 
1:c6ca640:   private void removeTableFromCache(SchemaTableName table) {
1:e5e74fc:     DataMapStoreManager.getInstance()
1:e5e74fc:         .clearDataMaps(carbonCache.get().get(table).carbonTable.getAbsoluteTableIdentifier());
1:e5e74fc:     carbonCache.get().remove(table);
1:c6ca640:     tableList.remove(table);
1:e5e74fc: 
1:c6ca640:   }
1:c6ca640: 
1:2712330:   /**
1:b699ee6:    * Return the schema names under a schema store path (this.carbonFileList).
1:9c83bd1:    *
1:b699ee6:    * @return
1:b699ee6:    */
1:0e93a3f:   public List<String> getSchemaNames() {
1:0e93a3f:     return updateSchemaList();
1:0e93a3f:   }
1:0e93a3f: 
1:2712330:   /**
1:d4a1577:    * Get the CarbonFile instance which represents the store path in the configuration,
1:d4a1577:    * and assign it to this.carbonFileList.
1:9c83bd1:    *
1:b699ee6:    * @return
1:2712330:    */
1:9c83bd1:   private boolean updateCarbonFile() {
1:7c0e660:     if (carbonFileList == null) {
1:0e93a3f:       fileType = FileFactory.getFileType(config.getStorePath());
1:0e93a3f:       try {
1:7c0e660:         carbonFileList = FileFactory.getCarbonFile(config.getStorePath(), fileType);
1:0e93a3f:       } catch (Exception ex) {
1:0e93a3f:         throw new RuntimeException(ex);
1:0e93a3f:       }
1:0e93a3f:     }
6:0e93a3f:     return true;
1:0e93a3f:   }
1:0e93a3f: 
2:b699ee6:   /**
1:b699ee6:    * Return the schema names under a schema store path (this.carbonFileList).
1:9c83bd1:    *
1:b699ee6:    * @return
1:b699ee6:    */
1:9c83bd1:   private List<String> updateSchemaList() {
1:7c0e660:     updateCarbonFile();
1:7c0e660:     if (carbonFileList != null) {
1:15ab6b0:       Stream.of(carbonFileList.listFiles()).forEach(this::getName);
1:15ab6b0:       return schemaNames;
1:0e93a3f:     } else return ImmutableList.of();
1:0e93a3f:   }
1:0e93a3f: 
1:01b48fc:   private void getName(CarbonFile carbonFile) {
1:01b48fc:     if (!carbonFile.getName().equalsIgnoreCase("_system") && !carbonFile.getName()
1:01b48fc:         .equalsIgnoreCase(".ds_store")) {
1:01b48fc:       schemaNames.add(carbonFile.getName());
1:01b48fc:     }
1:15ab6b0:   }
1:15ab6b0: 
1:2712330:   /**
1:b699ee6:    * Get the names of the tables in the given schema.
1:9c83bd1:    *
1:b699ee6:    * @param schema name of the schema
1:b699ee6:    * @return
1:2712330:    */
1:0e93a3f:   public Set<String> getTableNames(String schema) {
1:0e93a3f:     requireNonNull(schema, "schema is null");
1:0e93a3f:     return updateTableList(schema);
1:15ab6b0:   }
1:0e93a3f: 
1:2712330:   /**
1:b699ee6:    * Get the names of the tables in the given schema.
1:9c83bd1:    *
1:b699ee6:    * @param schemaName name of the schema
1:b699ee6:    * @return
1:2712330:    */
1:9c83bd1:   private Set<String> updateTableList(String schemaName) {
1:1dc86d2:     updateCarbonFile();
1:9c83bd1:     List<CarbonFile> schema =
1:01b48fc:         Stream.of(carbonFileList.listFiles()).filter(a -> schemaName.equals(a.getName()))
1:01b48fc:             .collect(Collectors.toList());
1:0e93a3f:     if (schema.size() > 0) {
1:9c83bd1:       return Stream.of((schema.get(0)).listFiles()).map(CarbonFile::getName)
1:01b48fc:           .collect(Collectors.toSet());
1:0e93a3f:     } else return ImmutableSet.of();
1:0e93a3f:   }
1:0e93a3f: 
1:0e93a3f:   /**
1:b699ee6:    * Get the CarbonTable instance of the given table.
1:9c83bd1:    *
1:b699ee6:    * @param schemaTableName name of the given table.
1:b699ee6:    * @return
1:0e93a3f:    */
1:0e93a3f:   public CarbonTable getTable(SchemaTableName schemaTableName) {
1:0e93a3f:     try {
1:7036696:       updateSchemaTables(schemaTableName);
2:0e93a3f:     } catch (Exception e) {
1:0e93a3f:       throw new RuntimeException(e);
1:0e93a3f:     }
1:0e93a3f: 
1:0e93a3f:     requireNonNull(schemaTableName, "schemaTableName is null");
1:910d496:     return loadTableMetadata(schemaTableName);
1:0e93a3f:   }
1:0e93a3f: 
1:0e93a3f:   /**
1:b699ee6:    * Find all the tables under the schema store path (this.carbonFileList)
1:b699ee6:    * and cache all the table names in this.tableList. Notice that whenever this method
1:b699ee6:    * is called, it clears this.tableList and populate the list by reading the files.
1:0e93a3f:    */
1:7036696:   private void updateSchemaTables(SchemaTableName schemaTableName) {
1:e5e74fc:     // update logic determine later
1:e5e74fc:     boolean isKeyExists = carbonCache.get().containsKey(schemaTableName);
1:c6ca640: 
1:7c0e660:     if (carbonFileList == null) {
1:0e93a3f:       updateSchemaList();
1:0e93a3f:     }
1:c6ca640:     try {
1:bf6c471:       if (isKeyExists
1:bf6c471:           && !FileFactory.isFileExist(
1:3740535:           CarbonTablePath.getSchemaFilePath(
1:3740535:               carbonCache.get().get(schemaTableName).carbonTable.getTablePath()), fileType)) {
2:c6ca640:         removeTableFromCache(schemaTableName);
1:c6ca640:         throw new TableNotFoundException(schemaTableName);
1:c6ca640:       }
1:c6ca640:     } catch (IOException e) {
1:c6ca640:       throw new RuntimeException();
1:c6ca640:     }
1:e5e74fc: 
1:e5e74fc:     if (isKeyExists) {
1:15ab6b0:       CarbonTableCacheModel carbonTableCacheModel = carbonCache.get().get(schemaTableName);
1:01b48fc:       if (carbonTableCacheModel != null
1:01b48fc:           && carbonTableCacheModel.carbonTable.getTableInfo() != null) {
1:01b48fc:         Long latestTime = FileFactory.getCarbonFile(CarbonTablePath
1:01b48fc:             .getSchemaFilePath(carbonCache.get().get(schemaTableName).carbonTable.getTablePath()))
1:01b48fc:             .getLastModifiedTime();
1:15ab6b0:         Long oldTime = carbonTableCacheModel.carbonTable.getTableInfo().getLastUpdatedTime();
1:e5e74fc:         if (DateUtils.truncate(new Date(latestTime), Calendar.MINUTE)
1:e5e74fc:             .after(DateUtils.truncate(new Date(oldTime), Calendar.MINUTE))) {
1:e5e74fc:           removeTableFromCache(schemaTableName);
1:e5e74fc:         }
1:e5e74fc:       }
1:c6ca640:     }
1:e5e74fc:     if (!tableList.contains(schemaTableName)) {
1:7c0e660:       for (CarbonFile cf : carbonFileList.listFiles()) {
1:7c0e660:         if (!cf.getName().endsWith(".mdt")) {
1:7c0e660:           for (CarbonFile table : cf.listFiles()) {
1:7c0e660:             tableList.add(new SchemaTableName(cf.getName(), table.getName()));
1:0e93a3f:           }
1:0e93a3f:         }
1:0e93a3f:       }
1:0e93a3f:     }
1:0e93a3f:   }
1:0e93a3f: 
1:0e93a3f:   /**
1:b699ee6:    * Find the table with the given name and build a CarbonTable instance for it.
1:b699ee6:    * This method should be called after this.updateSchemaTables().
1:9c83bd1:    *
1:b699ee6:    * @param schemaTableName name of the given table.
1:b699ee6:    * @return
1:0e93a3f:    */
1:0e93a3f:   private CarbonTable loadTableMetadata(SchemaTableName schemaTableName) {
1:0e93a3f:     for (SchemaTableName table : tableList) {
1:0e93a3f:       if (!table.equals(schemaTableName)) continue;
1:0e93a3f: 
1:0e93a3f:       return parseCarbonMetadata(table);
1:0e93a3f:     }
1:92d1d97:     throw new TableNotFoundException(schemaTableName);
1:0e93a3f:   }
1:0e93a3f: 
1:0e93a3f:   /**
1:d4a1577:    * Read the metadata of the given table
1:d4a1577:    * and cache it in this.carbonCache (CarbonTableReader cache).
1:9c83bd1:    *
1:b699ee6:    * @param table name of the given table.
1:b699ee6:    * @return the CarbonTable instance which contains all the needed metadata for a table.
1:0e93a3f:    */
1:9c83bd1:   private CarbonTable parseCarbonMetadata(SchemaTableName table) {
1:0e93a3f:     CarbonTable result = null;
1:0e93a3f:     try {
1:e5e74fc:       CarbonTableCacheModel cache = carbonCache.get().get(table);
1:214d9eb:       if (cache == null) {
1:214d9eb:         cache = new CarbonTableCacheModel();
1:0e93a3f:       }
1:0e93a3f:       if (cache.isValid()) return cache.carbonTable;
1:0e93a3f: 
1:b699ee6:       // If table is not previously cached, then:
1:0e93a3f: 
1:b699ee6:       // Step 1: get store path of the table and cache it.
1:9c83bd1:       // create table identifier. the table id is randomly generated.
1:bf6c471:       CarbonTableIdentifier carbonTableIdentifier =
1:3740535:           new CarbonTableIdentifier(table.getSchemaName(), table.getTableName(),
1:3740535:               UUID.randomUUID().toString());
1:0e93a3f:       String storePath = config.getStorePath();
1:bf6c471:       String tablePath = storePath + "/" + carbonTableIdentifier.getDatabaseName() + "/"
1:bf6c471:           + carbonTableIdentifier.getTableName();
1:0e93a3f: 
1:b699ee6:       //Step 2: read the metadata (tableInfo) of the table.
1:0e93a3f:       ThriftReader.TBaseCreator createTBase = new ThriftReader.TBaseCreator() {
1:b699ee6:         // TBase is used to read and write thrift objects.
1:b699ee6:         // TableInfo is a kind of TBase used to read and write table information.
1:d4a1577:         // TableInfo is generated by thrift,
1:d4a1577:         // see schema.thrift under format/src/main/thrift for details.
1:0e93a3f:         public TBase create() {
1:0e93a3f:           return new org.apache.carbondata.format.TableInfo();
1:0e93a3f:         }
2:0e93a3f:       };
1:0e93a3f:       ThriftReader thriftReader =
1:3740535:           new ThriftReader(CarbonTablePath.getSchemaFilePath(tablePath), createTBase);
1:0e93a3f:       thriftReader.open();
1:0e93a3f:       org.apache.carbondata.format.TableInfo tableInfo =
1:3740535:           (org.apache.carbondata.format.TableInfo) thriftReader.read();
1:0e93a3f:       thriftReader.close();
1:c6ca640: 
1:b699ee6:       // Step 3: convert format level TableInfo to code level TableInfo
1:0e93a3f:       SchemaConverter schemaConverter = new ThriftWrapperSchemaConverterImpl();
1:d4a1577:       // wrapperTableInfo is the code level information of a table in carbondata core,
1:d4a1577:       // different from the Thrift TableInfo.
1:0e93a3f:       TableInfo wrapperTableInfo = schemaConverter
1:c6ca640:           .fromExternalToWrapperTableInfo(tableInfo, table.getSchemaName(), table.getTableName(),
1:1155d4d:               tablePath);
1:0e93a3f: 
1:b699ee6:       // Step 4: Load metadata info into CarbonMetadata
1:0e93a3f:       CarbonMetadata.getInstance().loadTableMetadata(wrapperTableInfo);
1:0e93a3f: 
1:bf6c471:       cache.carbonTable = CarbonMetadata.getInstance().getCarbonTable(
1:bf6c471:           table.getSchemaName(), table.getTableName());
1:0e93a3f: 
1:9c83bd1:       // cache the table
1:e5e74fc:       carbonCache.get().put(table, cache);
1:0e93a3f: 
1:0e93a3f:       result = cache.carbonTable;
1:0e93a3f:     } catch (Exception ex) {
1:0e93a3f:       throw new RuntimeException(ex);
1:0e93a3f:     }
1:0e93a3f: 
2:0e93a3f:     return result;
1:0e93a3f:   }
1:0e93a3f: 
1:01b48fc:   public List<CarbonLocalMultiBlockSplit> getInputSplits2(CarbonTableCacheModel tableCacheModel,
1:3740535:       Expression filters, TupleDomain<ColumnHandle> constraints) throws IOException {
1:0e93a3f:     List<CarbonLocalInputSplit> result = new ArrayList<>();
1:01b48fc:     List<CarbonLocalMultiBlockSplit> multiBlockSplitList = new ArrayList<>();
1:9c83bd1:     CarbonTable carbonTable = tableCacheModel.carbonTable;
1:bf6c471:     TableInfo tableInfo = tableCacheModel.carbonTable.getTableInfo();
1:2a9604c:     Configuration config = FileFactory.getConfiguration();
1:9c83bd1:     config.set(CarbonTableInputFormat.INPUT_SEGMENT_NUMBERS, "");
1:bf6c471:     String carbonTablePath = carbonTable.getAbsoluteTableIdentifier().getTablePath();
1:9c83bd1:     config.set(CarbonTableInputFormat.INPUT_DIR, carbonTablePath);
1:1155d4d:     config.set(CarbonTableInputFormat.DATABASE_NAME, carbonTable.getDatabaseName());
1:5fc7f06:     config.set(CarbonTableInputFormat.TABLE_NAME, carbonTable.getTableName());
1:01b48fc:     config.set("query.id", queryId);
1:0e93a3f: 
1:9c83bd1:     JobConf jobConf = new JobConf(config);
1:3740535:     List<PartitionSpec> filteredPartitions = new ArrayList();
1:3740535: 
1:3740535:     PartitionInfo partitionInfo = carbonTable.getPartitionInfo(carbonTable.getTableName());
1:3740535: 
1:01b48fc:     if (partitionInfo != null && partitionInfo.getPartitionType() == PartitionType.NATIVE_HIVE) {
1:9c83bd1:       try {
1:01b48fc:         loadMetadataDetails = SegmentStatusManager.readTableStatusFile(
1:01b48fc:             CarbonTablePath.getTableStatusFilePath(carbonTable.getTablePath()));
1:3740535:       } catch (IOException exception) {
1:3740535:         LOGGER.error(exception.getMessage());
1:3740535:         throw exception;
1:3740535:       }
1:01b48fc:       filteredPartitions = findRequiredPartitions(constraints, carbonTable, loadMetadataDetails);
1:3740535:     }
1:3740535:     try {
1:9c83bd1:       CarbonTableInputFormat.setTableInfo(config, tableInfo);
1:9c83bd1:       CarbonTableInputFormat carbonTableInputFormat =
1:01b48fc:           createInputFormat(jobConf, carbonTable.getAbsoluteTableIdentifier(), filters,
1:01b48fc:               filteredPartitions);
1:9c83bd1:       Job job = Job.getInstance(jobConf);
1:9c83bd1:       List<InputSplit> splits = carbonTableInputFormat.getSplits(job);
1:9c83bd1:       Gson gson = new Gson();
1:9c83bd1:       if (splits != null && splits.size() > 0) {
1:9c83bd1:         for (InputSplit inputSplit : splits) {
1:01b48fc:           CarbonInputSplit carbonInputSplit = (CarbonInputSplit) inputSplit;
1:9c83bd1:           result.add(new CarbonLocalInputSplit(carbonInputSplit.getSegmentId(),
1:9c83bd1:               carbonInputSplit.getPath().toString(), carbonInputSplit.getStart(),
1:9c83bd1:               carbonInputSplit.getLength(), Arrays.asList(carbonInputSplit.getLocations()),
1:9c83bd1:               carbonInputSplit.getNumberOfBlocklets(), carbonInputSplit.getVersion().number(),
1:01b48fc:               carbonInputSplit.getDeleteDeltaFiles(), carbonInputSplit.getBlockletId(),
1:9c83bd1:               gson.toJson(carbonInputSplit.getDetailInfo())));
1:0e93a3f:         }
1:01b48fc: 
1:01b48fc:         // Use block distribution
1:01b48fc:         List<List<CarbonLocalInputSplit>> inputSplits = new ArrayList(
1:01b48fc:             result.stream().map(x -> (CarbonLocalInputSplit) x).collect(Collectors.groupingBy(
1:01b48fc:                 carbonInput -> carbonInput.getSegmentId().concat(carbonInput.getPath()))).values());
1:01b48fc:         if (inputSplits != null) {
1:01b48fc:           for (int j = 0; j < inputSplits.size(); j++) {
1:01b48fc:             multiBlockSplitList.add(new CarbonLocalMultiBlockSplit(inputSplits.get(j),
1:01b48fc:                 inputSplits.get(j).stream().flatMap(f -> Arrays.stream(getLocations(f))).distinct()
1:01b48fc:                     .toArray(String[]::new)));
1:01b48fc:           }
1:01b48fc:         }
1:01b48fc:         LOGGER.error("Size fo MultiblockList   " + multiBlockSplitList.size());
1:01b48fc: 
1:0e93a3f:       }
1:9c83bd1: 
1:9c83bd1:     } catch (IOException e) {
1:9c83bd1:       throw new RuntimeException("Error creating Splits from CarbonTableInputFormat", e);
1:0e93a3f:     }
1:9c83bd1: 
1:01b48fc:     return multiBlockSplitList;
1:0e93a3f:   }
1:0e93a3f: 
1:01b48fc:   /**
1:01b48fc:    * Returns list of partition specs to query based on the domain constraints
1:01b48fc:    *
1:3740535:    * @param constraints
1:3740535:    * @param carbonTable
1:3740535:    * @throws IOException
1:3740535:    */
1:01b48fc:   private List<PartitionSpec> findRequiredPartitions(TupleDomain<ColumnHandle> constraints,
1:01b48fc:       CarbonTable carbonTable, LoadMetadataDetails[] loadMetadataDetails) throws IOException {
1:3740535:     Set<PartitionSpec> partitionSpecs = new HashSet<>();
1:3740535:     List<PartitionSpec> prunePartitions = new ArrayList();
1:3740535: 
1:3740535:     for (LoadMetadataDetails loadMetadataDetail : loadMetadataDetails) {
1:3740535:       SegmentFileStore segmentFileStore = null;
1:3740535:       try {
1:3740535:         segmentFileStore =
1:3740535:             new SegmentFileStore(carbonTable.getTablePath(), loadMetadataDetail.getSegmentFile());
1:3740535:         partitionSpecs.addAll(segmentFileStore.getPartitionSpecs());
1:3740535: 
1:3740535:       } catch (IOException exception) {
1:3740535:         LOGGER.error(exception.getMessage());
1:3740535:         throw exception;
1:3740535:       }
1:3740535:     }
1:3740535:     List<String> partitionValuesFromExpression =
1:3740535:         PrestoFilterUtil.getPartitionFilters(carbonTable, constraints);
1:3740535: 
1:01b48fc:     List<PartitionSpec> partitionSpecList = partitionSpecs.stream().filter(
1:01b48fc:         partitionSpec -> CollectionUtils
1:01b48fc:             .isSubCollection(partitionValuesFromExpression, partitionSpec.getPartitions()))
1:01b48fc:         .collect(Collectors.toList());
1:3740535: 
1:3740535:     prunePartitions.addAll(partitionSpecList);
1:3740535: 
1:3740535:     return prunePartitions;
1:3740535:   }
1:3740535: 
1:01b48fc:   private CarbonTableInputFormat<Object> createInputFormat(Configuration conf,
1:01b48fc:       AbsoluteTableIdentifier identifier, Expression filterExpression,
1:01b48fc:       List<PartitionSpec> filteredPartitions) throws IOException {
1:9c83bd1:     CarbonTableInputFormat format = new CarbonTableInputFormat<Object>();
1:01b48fc:     CarbonTableInputFormat
1:01b48fc:         .setTablePath(conf, identifier.appendWithLocalPrefix(identifier.getTablePath()));
1:9c83bd1:     CarbonTableInputFormat.setFilterPredicates(conf, filterExpression);
1:01b48fc:     if (filteredPartitions.size() != 0) {
1:3740535:       CarbonTableInputFormat.setPartitionsToPrune(conf, filteredPartitions);
1:3740535:     }
1:9c83bd1:     return format;
1:9c83bd1:   }
1:0e93a3f: 
1:01b48fc:   private void populateCarbonProperties() {
1:01b48fc:     addProperty(CarbonCommonConstants.UNSAFE_WORKING_MEMORY_IN_MB, config.getUnsafeMemoryInMb());
1:01b48fc:     addProperty(CarbonCommonConstants.ENABLE_UNSAFE_IN_QUERY_EXECUTION,
1:01b48fc:         config.getEnableUnsafeInQueryExecution());
1:01b48fc:     addProperty(CarbonCommonConstants.ENABLE_UNSAFE_COLUMN_PAGE,
1:01b48fc:         config.getEnableUnsafeColumnPage());
1:01b48fc:     addProperty(CarbonCommonConstants.ENABLE_UNSAFE_SORT, config.getEnableUnsafeSort());
1:01b48fc:     addProperty(CarbonCommonConstants.ENABLE_QUERY_STATISTICS, config.getEnableQueryStatistics());
1:01b48fc:   }
1:d4a1577: 
1:d4a1577:   private void setS3Properties() {
1:d4a1577:     FileFactory.getConfiguration().set(ACCESS_KEY, Objects.toString(config.getS3A_AcesssKey(), ""));
1:d4a1577:     FileFactory.getConfiguration().set(SECRET_KEY, Objects.toString(config.getS3A_SecretKey()));
1:01b48fc:     FileFactory.getConfiguration()
1:d4a1577:         .set(CarbonCommonConstants.S3_ACCESS_KEY, Objects.toString(config.getS3_AcesssKey(), ""));
2:dc4f87b:     FileFactory.getConfiguration()
1:d4a1577:         .set(CarbonCommonConstants.S3_SECRET_KEY, Objects.toString(config.getS3_SecretKey()));
1:d4a1577:     FileFactory.getConfiguration()
1:d4a1577:         .set(CarbonCommonConstants.S3N_ACCESS_KEY, Objects.toString(config.getS3N_AcesssKey(), ""));
1:d4a1577:     FileFactory.getConfiguration()
1:d4a1577:         .set(CarbonCommonConstants.S3N_SECRET_KEY, Objects.toString(config.getS3N_SecretKey(), ""));
1:d4a1577:     FileFactory.getConfiguration().set(ENDPOINT, Objects.toString(config.getS3EndPoint(), ""));
1:d4a1577:   }
1:0e93a3f: 
1:01b48fc:   private void addProperty(String propertyName, String propertyValue) {
1:01b48fc:     if (propertyValue != null) {
1:01b48fc:       CarbonProperties.getInstance().addProperty(propertyName, propertyValue);
1:01b48fc:     }
1:01b48fc:   }
1:01b48fc: 
1:01b48fc:   /**
1:01b48fc:    * @param cis
1:01b48fc:    * @return
1:01b48fc:    */
1:01b48fc:   private String[] getLocations(CarbonLocalInputSplit cis) {
1:01b48fc:     return cis.getLocations().toArray(new String[cis.getLocations().size()]);
1:01b48fc:   }
1:01b48fc: 
1:01b48fc:   public String getQueryId() {
1:01b48fc:     return queryId;
1:01b48fc:   }
1:01b48fc: 
1:01b48fc:   public void setQueryId(String queryId) {
1:01b48fc:     this.queryId = queryId;
1:1dc86d2:   }
1:01b48fc: }
============================================================================
author:kunal642
-------------------------------------------------------------------------------
commit:2a9604c
/////////////////////////////////////////////////////////////////////////
1:     Configuration config = FileFactory.getConfiguration();
author:ajantha-bhat
-------------------------------------------------------------------------------
commit:d4a1577
/////////////////////////////////////////////////////////////////////////
1: import static java.util.Objects.requireNonNull;
1: 
/////////////////////////////////////////////////////////////////////////
/////////////////////////////////////////////////////////////////////////
1:   private LoadMetadataDetails[] loadMetadataDetails;
/////////////////////////////////////////////////////////////////////////
1:   private List<String> schemaNames = new ArrayList<>();
/////////////////////////////////////////////////////////////////////////
1:    * Get the CarbonFile instance which represents the store path in the configuration,
1:    * and assign it to this.carbonFileList.
/////////////////////////////////////////////////////////////////////////
1:    * Read the metadata of the given table
1:    * and cache it in this.carbonCache (CarbonTableReader cache).
/////////////////////////////////////////////////////////////////////////
1:         // TableInfo is generated by thrift,
1:         // see schema.thrift under format/src/main/thrift for details.
/////////////////////////////////////////////////////////////////////////
1:       // wrapperTableInfo is the code level information of a table in carbondata core,
1:       // different from the Thrift TableInfo.
/////////////////////////////////////////////////////////////////////////
1: 
1:   private void setS3Properties() {
1:     FileFactory.getConfiguration().set(ACCESS_KEY, Objects.toString(config.getS3A_AcesssKey(), ""));
1:     FileFactory.getConfiguration().set(SECRET_KEY, Objects.toString(config.getS3A_SecretKey()));
1:         .set(CarbonCommonConstants.S3_ACCESS_KEY, Objects.toString(config.getS3_AcesssKey(), ""));
1:         .set(CarbonCommonConstants.S3_SECRET_KEY, Objects.toString(config.getS3_SecretKey()));
1:     FileFactory.getConfiguration()
1:         .set(CarbonCommonConstants.S3N_ACCESS_KEY, Objects.toString(config.getS3N_AcesssKey(), ""));
1:     FileFactory.getConfiguration()
1:         .set(CarbonCommonConstants.S3N_SECRET_KEY, Objects.toString(config.getS3N_SecretKey(), ""));
1:     FileFactory.getConfiguration().set(ENDPOINT, Objects.toString(config.getS3EndPoint(), ""));
1:   }
author:peter.wei
-------------------------------------------------------------------------------
commit:1dc86d2
/////////////////////////////////////////////////////////////////////////
1:     updateCarbonFile();
/////////////////////////////////////////////////////////////////////////
1: }
author:Bhavya
-------------------------------------------------------------------------------
commit:01b48fc
/////////////////////////////////////////////////////////////////////////
1:   private String queryId;
1: 
/////////////////////////////////////////////////////////////////////////
1:     populateCarbonProperties();
/////////////////////////////////////////////////////////////////////////
1:   private void getName(CarbonFile carbonFile) {
1:     if (!carbonFile.getName().equalsIgnoreCase("_system") && !carbonFile.getName()
1:         .equalsIgnoreCase(".ds_store")) {
1:       schemaNames.add(carbonFile.getName());
1:     }
/////////////////////////////////////////////////////////////////////////
1:         Stream.of(carbonFileList.listFiles()).filter(a -> schemaName.equals(a.getName()))
1:             .collect(Collectors.toList());
1:           .collect(Collectors.toSet());
/////////////////////////////////////////////////////////////////////////
1:       if (carbonTableCacheModel != null
1:           && carbonTableCacheModel.carbonTable.getTableInfo() != null) {
1:         Long latestTime = FileFactory.getCarbonFile(CarbonTablePath
1:             .getSchemaFilePath(carbonCache.get().get(schemaTableName).carbonTable.getTablePath()))
1:             .getLastModifiedTime();
/////////////////////////////////////////////////////////////////////////
/////////////////////////////////////////////////////////////////////////
1:   public List<CarbonLocalMultiBlockSplit> getInputSplits2(CarbonTableCacheModel tableCacheModel,
1:     List<CarbonLocalMultiBlockSplit> multiBlockSplitList = new ArrayList<>();
/////////////////////////////////////////////////////////////////////////
1:     config.set("query.id", queryId);
1:     if (partitionInfo != null && partitionInfo.getPartitionType() == PartitionType.NATIVE_HIVE) {
1:         loadMetadataDetails = SegmentStatusManager.readTableStatusFile(
1:             CarbonTablePath.getTableStatusFilePath(carbonTable.getTablePath()));
1:       filteredPartitions = findRequiredPartitions(constraints, carbonTable, loadMetadataDetails);
1:           createInputFormat(jobConf, carbonTable.getAbsoluteTableIdentifier(), filters,
1:               filteredPartitions);
1:           CarbonInputSplit carbonInputSplit = (CarbonInputSplit) inputSplit;
1:               carbonInputSplit.getDeleteDeltaFiles(), carbonInputSplit.getBlockletId(),
1: 
1:         // Use block distribution
1:         List<List<CarbonLocalInputSplit>> inputSplits = new ArrayList(
1:             result.stream().map(x -> (CarbonLocalInputSplit) x).collect(Collectors.groupingBy(
1:                 carbonInput -> carbonInput.getSegmentId().concat(carbonInput.getPath()))).values());
1:         if (inputSplits != null) {
1:           for (int j = 0; j < inputSplits.size(); j++) {
1:             multiBlockSplitList.add(new CarbonLocalMultiBlockSplit(inputSplits.get(j),
1:                 inputSplits.get(j).stream().flatMap(f -> Arrays.stream(getLocations(f))).distinct()
1:                     .toArray(String[]::new)));
1:           }
1:         }
1:         LOGGER.error("Size fo MultiblockList   " + multiBlockSplitList.size());
1: 
1:     return multiBlockSplitList;
1:   /**
1:    * Returns list of partition specs to query based on the domain constraints
1:    *
1:   private List<PartitionSpec> findRequiredPartitions(TupleDomain<ColumnHandle> constraints,
1:       CarbonTable carbonTable, LoadMetadataDetails[] loadMetadataDetails) throws IOException {
/////////////////////////////////////////////////////////////////////////
1:     List<PartitionSpec> partitionSpecList = partitionSpecs.stream().filter(
1:         partitionSpec -> CollectionUtils
1:             .isSubCollection(partitionValuesFromExpression, partitionSpec.getPartitions()))
1:         .collect(Collectors.toList());
1:   private CarbonTableInputFormat<Object> createInputFormat(Configuration conf,
1:       AbsoluteTableIdentifier identifier, Expression filterExpression,
1:       List<PartitionSpec> filteredPartitions) throws IOException {
1:     CarbonTableInputFormat
1:         .setTablePath(conf, identifier.appendWithLocalPrefix(identifier.getTablePath()));
1:     if (filteredPartitions.size() != 0) {
1:   private void populateCarbonProperties() {
1:     addProperty(CarbonCommonConstants.UNSAFE_WORKING_MEMORY_IN_MB, config.getUnsafeMemoryInMb());
1:     addProperty(CarbonCommonConstants.ENABLE_UNSAFE_IN_QUERY_EXECUTION,
1:         config.getEnableUnsafeInQueryExecution());
1:     addProperty(CarbonCommonConstants.ENABLE_UNSAFE_COLUMN_PAGE,
1:         config.getEnableUnsafeColumnPage());
1:     addProperty(CarbonCommonConstants.ENABLE_UNSAFE_SORT, config.getEnableUnsafeSort());
1:     addProperty(CarbonCommonConstants.ENABLE_QUERY_STATISTICS, config.getEnableQueryStatistics());
1:   }
1:     FileFactory.getConfiguration()
/////////////////////////////////////////////////////////////////////////
1:   private void addProperty(String propertyName, String propertyValue) {
1:     if (propertyValue != null) {
1:       CarbonProperties.getInstance().addProperty(propertyName, propertyValue);
1:     }
1:   }
1: 
1:   /**
1:    * @param cis
1:    * @return
1:    */
1:   private String[] getLocations(CarbonLocalInputSplit cis) {
1:     return cis.getLocations().toArray(new String[cis.getLocations().size()]);
1:   }
1: 
1:   public String getQueryId() {
1:     return queryId;
1:   }
1: 
1:   public void setQueryId(String queryId) {
1:     this.queryId = queryId;
1:   }
commit:fca960e
/////////////////////////////////////////////////////////////////////////
1: import org.apache.carbondata.core.constants.CarbonCommonConstants;
/////////////////////////////////////////////////////////////////////////
1: import org.apache.carbondata.core.util.CarbonProperties;
/////////////////////////////////////////////////////////////////////////
0:     if(config.getUnsafeMemoryInMb() != null) {
0:       CarbonProperties.getInstance().addProperty(CarbonCommonConstants.UNSAFE_WORKING_MEMORY_IN_MB,
0:           config.getUnsafeMemoryInMb());
1:     }
commit:e5e74fc
/////////////////////////////////////////////////////////////////////////
1: import java.util.Calendar;
1: import java.util.Date;
1: import java.util.HashMap;
1: import java.util.concurrent.atomic.AtomicReference;
/////////////////////////////////////////////////////////////////////////
1: import com.facebook.presto.hadoop.$internal.io.netty.util.internal.ConcurrentSet;
1: import org.apache.commons.lang.time.DateUtils;
/////////////////////////////////////////////////////////////////////////
1:   private ConcurrentSet<SchemaTableName> tableList;
/////////////////////////////////////////////////////////////////////////
1:   private AtomicReference<HashMap<SchemaTableName, CarbonTableCacheModel>> carbonCache;
1:     this.carbonCache = new AtomicReference(new HashMap());
1:     tableList = new ConcurrentSet<>();
/////////////////////////////////////////////////////////////////////////
1:     if (!carbonCache.get().containsKey(table) || carbonCache.get().get(table) == null) {
1:       // if this table is not cached, try to read the metadata of the table and cache it.
1:           FileFactory.class.getClassLoader())) {
/////////////////////////////////////////////////////////////////////////
1:     if (carbonCache.get().containsKey(table)) {
1:       return carbonCache.get().get(table);
1:     DataMapStoreManager.getInstance()
1:         .clearDataMaps(carbonCache.get().get(table).carbonTable.getAbsoluteTableIdentifier());
1:     carbonCache.get().remove(table);
1: 
/////////////////////////////////////////////////////////////////////////
1:     // update logic determine later
1:     boolean isKeyExists = carbonCache.get().containsKey(schemaTableName);
0:       if (isKeyExists && !FileFactory
0:           .isFileExist(carbonCache.get().get(schemaTableName).carbonTablePath.getSchemaFilePath(),
0:               fileType)) {
1: 
1:     if (isKeyExists) {
0:       CarbonTableCacheModel ctcm = carbonCache.get().get(schemaTableName);
0:       if(ctcm != null && ctcm.tableInfo != null) {
0:         Long latestTime = FileFactory.getCarbonFile(ctcm.carbonTablePath.getSchemaFilePath())
0:             .getLastModifiedTime();
0:         Long oldTime = ctcm.tableInfo.getLastUpdatedTime();
1:         if (DateUtils.truncate(new Date(latestTime), Calendar.MINUTE)
1:             .after(DateUtils.truncate(new Date(oldTime), Calendar.MINUTE))) {
1:           removeTableFromCache(schemaTableName);
1:         }
1:       }
1:     if (!tableList.contains(schemaTableName)) {
/////////////////////////////////////////////////////////////////////////
0:    * Read the metadata of the given table and cache it in this.carbonCache (CarbonTableReader cache).
/////////////////////////////////////////////////////////////////////////
1:       CarbonTableCacheModel cache = carbonCache.get().get(table);
/////////////////////////////////////////////////////////////////////////
1:       carbonCache.get().put(table, cache);
/////////////////////////////////////////////////////////////////////////
0:   private CarbonTableInputFormat<Object>  createInputFormat( Configuration conf,
0:        AbsoluteTableIdentifier identifier, Expression filterExpression)
commit:9c83bd1
/////////////////////////////////////////////////////////////////////////
1: import org.apache.carbondata.hadoop.CarbonInputSplit;
1: import org.apache.carbondata.hadoop.api.CarbonTableInputFormat;
1: import com.facebook.presto.hadoop.$internal.com.google.gson.Gson;
1: import com.facebook.presto.spi.TableNotFoundException;
1: import org.apache.hadoop.mapred.JobConf;
1: import org.apache.hadoop.mapreduce.Job;
1: /**
1:  * CarbonTableReader will be a facade of these utils
1:   // default PathFilter, accepts files in carbondata format (with .carbondata extension).
1:   private static final PathFilter DefaultFilter = new PathFilter() {
1:     @Override public boolean accept(Path path) {
1:       return CarbonTablePath.isCarbonDataFile(path.getName());
1:     }
1:   };
/////////////////////////////////////////////////////////////////////////
1:    *
0:     if (!cc.containsKey(table) || cc.get(table) == null) {
/////////////////////////////////////////////////////////////////////////
0:     if (cc.containsKey(table)) {
0:       return cc.get(table);
1:     } else {
1:       return null;
1:     }
1:    *
1:    *
1:   private boolean updateCarbonFile() {
/////////////////////////////////////////////////////////////////////////
1:    *
1:   private List<String> updateSchemaList() {
0:       return Stream.of(carbonFileList.listFiles()).map(CarbonFile::getName).collect(Collectors.toList());
1:    *
/////////////////////////////////////////////////////////////////////////
1:    *
1:   private Set<String> updateTableList(String schemaName) {
1:     List<CarbonFile> schema =
0:         Stream.of(carbonFileList.listFiles()).filter(a -> schemaName.equals(a.getName()))
0:             .collect(Collectors.toList());
1:       return Stream.of((schema.get(0)).listFiles()).map(CarbonFile::getName)
1:    *
/////////////////////////////////////////////////////////////////////////
0:   private void updateSchemaTables() {
/////////////////////////////////////////////////////////////////////////
1:    *
/////////////////////////////////////////////////////////////////////////
1:    *
1:   private CarbonTable parseCarbonMetadata(SchemaTableName table) {
/////////////////////////////////////////////////////////////////////////
1:       // create table identifier. the table id is randomly generated.
0:       // get the store path of the table.
0:       cache.carbonTablePath =
0:           PathFactory.getInstance().getCarbonTablePath(storePath, cache.carbonTableIdentifier, null);
1:       // cache the table
/////////////////////////////////////////////////////////////////////////
0:       // wrapperTableInfo is the code level information of a table in carbondata core, different from the Thrift TableInfo.
/////////////////////////////////////////////////////////////////////////
1: 
0:       Expression filters)  {
1:     CarbonTable carbonTable = tableCacheModel.carbonTable;
0:     TableInfo tableInfo = tableCacheModel.tableInfo;
0:     Configuration config = new Configuration();
1:     config.set(CarbonTableInputFormat.INPUT_SEGMENT_NUMBERS, "");
0:     String carbonTablePath = PathFactory.getInstance()
0:         .getCarbonTablePath(carbonTable.getAbsoluteTableIdentifier().getStorePath(),
0:             carbonTable.getCarbonTableIdentifier(), null).getPath();
1:     config.set(CarbonTableInputFormat.INPUT_DIR, carbonTablePath);
1:     try {
1:       CarbonTableInputFormat.setTableInfo(config, tableInfo);
1:       CarbonTableInputFormat carbonTableInputFormat =
0:           createInputFormat(config, carbonTable.getAbsoluteTableIdentifier(), filters);
1:       JobConf jobConf = new JobConf(config);
1:       Job job = Job.getInstance(jobConf);
1:       List<InputSplit> splits = carbonTableInputFormat.getSplits(job);
0:       CarbonInputSplit carbonInputSplit = null;
1:       Gson gson = new Gson();
1:       if (splits != null && splits.size() > 0) {
1:         for (InputSplit inputSplit : splits) {
0:           carbonInputSplit = (CarbonInputSplit) inputSplit;
1:           result.add(new CarbonLocalInputSplit(carbonInputSplit.getSegmentId(),
1:               carbonInputSplit.getPath().toString(), carbonInputSplit.getStart(),
1:               carbonInputSplit.getLength(), Arrays.asList(carbonInputSplit.getLocations()),
1:               carbonInputSplit.getNumberOfBlocklets(), carbonInputSplit.getVersion().number(),
0:               carbonInputSplit.getDeleteDeltaFiles(),
1:               gson.toJson(carbonInputSplit.getDetailInfo())));
1: 
1:     } catch (IOException e) {
1:       throw new RuntimeException("Error creating Splits from CarbonTableInputFormat", e);
1: 
0:   private CarbonTableInputFormat<Object>  createInputFormat( Configuration conf, AbsoluteTableIdentifier identifier, Expression filterExpression)
1:     CarbonTableInputFormat format = new CarbonTableInputFormat<Object>();
0:     CarbonTableInputFormat.setTablePath(conf,
0:         identifier.appendWithLocalPrefix(identifier.getTablePath()));
1:     CarbonTableInputFormat.setFilterPredicates(conf, filterExpression);
1:     return format;
1: }
author:anubhav100
-------------------------------------------------------------------------------
commit:dc4f87b
/////////////////////////////////////////////////////////////////////////
1: import java.util.Objects;
/////////////////////////////////////////////////////////////////////////
1: import static org.apache.hadoop.fs.s3a.Constants.ACCESS_KEY;
1: import static org.apache.hadoop.fs.s3a.Constants.ENDPOINT;
1: import static org.apache.hadoop.fs.s3a.Constants.SECRET_KEY;
/////////////////////////////////////////////////////////////////////////
1:   public CarbonTableConfig config;
/////////////////////////////////////////////////////////////////////////
1:     setS3Properties();
/////////////////////////////////////////////////////////////////////////
0:   private void setS3Properties(){
1:   FileFactory.getConfiguration()
0:       .set(ACCESS_KEY, Objects.toString(config.getS3A_AcesssKey(),""));
1:     FileFactory.getConfiguration()
0:         .set(SECRET_KEY, Objects.toString(config.getS3A_SecretKey()));
0:     FileFactory.getConfiguration().set(CarbonCommonConstants.S3_ACCESS_KEY,
0:       Objects.toString(config.getS3_AcesssKey(),""));
0:     FileFactory.getConfiguration().set(CarbonCommonConstants.S3_SECRET_KEY,
0:       Objects.toString(config.getS3_SecretKey()));
0:     FileFactory.getConfiguration().set(CarbonCommonConstants.S3N_ACCESS_KEY,
0:       Objects.toString(config.getS3N_AcesssKey(),""));
0:     FileFactory.getConfiguration().set(CarbonCommonConstants.S3N_SECRET_KEY,
0:       Objects.toString(config.getS3N_SecretKey(),""));
0:     FileFactory.getConfiguration().set(ENDPOINT,
0:       Objects.toString(config.getS3EndPoint(),""));
1: }
commit:15ab6b0
/////////////////////////////////////////////////////////////////////////
1:   /**
1:    * List Of Schemas
1:    */
0:  private  List<String> schemaNames = new ArrayList<>();
/////////////////////////////////////////////////////////////////////////
1:       Stream.of(carbonFileList.listFiles()).forEach(this::getName);
1:       return schemaNames;
0:   private void getName(CarbonFile carbonFile){
0:   if(!carbonFile.getName().equalsIgnoreCase("_system") && !carbonFile.getName().equalsIgnoreCase(".ds_store")){
0:     schemaNames.add(carbonFile.getName());
1:   }
1:   }
1: 
/////////////////////////////////////////////////////////////////////////
1:       CarbonTableCacheModel carbonTableCacheModel = carbonCache.get().get(schemaTableName);
0:       if(carbonTableCacheModel != null && carbonTableCacheModel.carbonTable.getTableInfo() != null) {
1:         Long oldTime = carbonTableCacheModel.carbonTable.getTableInfo().getLastUpdatedTime();
commit:3740535
/////////////////////////////////////////////////////////////////////////
1: import java.util.HashSet;
/////////////////////////////////////////////////////////////////////////
1: import org.apache.carbondata.common.logging.LogService;
1: import org.apache.carbondata.common.logging.LogServiceFactory;
1: import org.apache.carbondata.core.indexstore.PartitionSpec;
1: import org.apache.carbondata.core.metadata.SegmentFileStore;
1: import org.apache.carbondata.core.metadata.schema.PartitionInfo;
1: import org.apache.carbondata.core.metadata.schema.partition.PartitionType;
1: import org.apache.carbondata.core.statusmanager.LoadMetadataDetails;
1: import org.apache.carbondata.core.statusmanager.SegmentStatusManager;
1: import org.apache.carbondata.presto.PrestoFilterUtil;
1: import com.facebook.presto.hadoop.$internal.org.apache.commons.collections.CollectionUtils;
1: import com.facebook.presto.spi.ColumnHandle;
1: import com.facebook.presto.spi.predicate.TupleDomain;
/////////////////////////////////////////////////////////////////////////
0:   private LoadMetadataDetails loadMetadataDetails[];
1: 
1:   /**
1:    * Logger instance
1:    */
1:   private static final LogService LOGGER =
1:       LogServiceFactory.getLogService(CarbonTableReader.class.getName());
1: 
1: 
/////////////////////////////////////////////////////////////////////////
1:           CarbonTablePath.getSchemaFilePath(
1:               carbonCache.get().get(schemaTableName).carbonTable.getTablePath()), fileType)) {
/////////////////////////////////////////////////////////////////////////
1:           new CarbonTableIdentifier(table.getSchemaName(), table.getTableName(),
1:               UUID.randomUUID().toString());
/////////////////////////////////////////////////////////////////////////
1:           new ThriftReader(CarbonTablePath.getSchemaFilePath(tablePath), createTBase);
1:           (org.apache.carbondata.format.TableInfo) thriftReader.read();
/////////////////////////////////////////////////////////////////////////
1:       Expression filters, TupleDomain<ColumnHandle> constraints) throws IOException {
/////////////////////////////////////////////////////////////////////////
0:     JobConf jobConf = new JobConf(config);
1:     List<PartitionSpec> filteredPartitions = new ArrayList();
1: 
1:     PartitionInfo partitionInfo = carbonTable.getPartitionInfo(carbonTable.getTableName());
1: 
0:     if(partitionInfo!=null && partitionInfo.getPartitionType()== PartitionType.NATIVE_HIVE) {
1:       try {
0:         loadMetadataDetails= SegmentStatusManager
0:             .readTableStatusFile(CarbonTablePath.getTableStatusFilePath(carbonTable.getTablePath()));
1:       } catch (IOException exception) {
1:         LOGGER.error(exception.getMessage());
1:         throw exception;
1:       }
0:       filteredPartitions = findRequiredPartitions(constraints, carbonTable,loadMetadataDetails);
1:     }
0:           createInputFormat(jobConf, carbonTable.getAbsoluteTableIdentifier(), filters,filteredPartitions);
/////////////////////////////////////////////////////////////////////////
0:               carbonInputSplit.getPath().toString(), carbonInputSplit.getStart(),
0:               carbonInputSplit.getLength(), Arrays.asList(carbonInputSplit.getLocations()),
0:               carbonInputSplit.getNumberOfBlocklets(), carbonInputSplit.getVersion().number(),
0:               carbonInputSplit.getDeleteDeltaFiles(),
0:               gson.toJson(carbonInputSplit.getDetailInfo())));
/////////////////////////////////////////////////////////////////////////
0:   /** Returns list of partition specs to query based on the domain constraints
1:    * @param constraints
1:    * @param carbonTable
1:    * @throws IOException
1:    */
0:   private List<PartitionSpec> findRequiredPartitions(TupleDomain<ColumnHandle> constraints, CarbonTable carbonTable,
0:       LoadMetadataDetails[]loadMetadataDetails) throws IOException {
1:     Set<PartitionSpec> partitionSpecs = new HashSet<>();
1:     List<PartitionSpec> prunePartitions = new ArrayList();
1: 
1:     for (LoadMetadataDetails loadMetadataDetail : loadMetadataDetails) {
1:       SegmentFileStore segmentFileStore = null;
1:       try {
1:         segmentFileStore =
1:             new SegmentFileStore(carbonTable.getTablePath(), loadMetadataDetail.getSegmentFile());
1:         partitionSpecs.addAll(segmentFileStore.getPartitionSpecs());
1: 
1:       } catch (IOException exception) {
1:         LOGGER.error(exception.getMessage());
1:         throw exception;
1:       }
1:     }
1:     List<String> partitionValuesFromExpression =
1:         PrestoFilterUtil.getPartitionFilters(carbonTable, constraints);
1: 
0:     List<PartitionSpec> partitionSpecList = partitionSpecs.stream().filter( partitionSpec ->
0:         CollectionUtils.isSubCollection(partitionValuesFromExpression, partitionSpec.getPartitions())).collect(Collectors.toList());
1: 
1:     prunePartitions.addAll(partitionSpecList);
1: 
1:     return prunePartitions;
1:   }
1: 
0:       AbsoluteTableIdentifier identifier, Expression filterExpression, List<PartitionSpec> filteredPartitions)
0:       throws IOException {
0:         identifier.appendWithLocalPrefix(identifier.getTablePath()));
0:     if(filteredPartitions.size() != 0) {
1:       CarbonTableInputFormat.setPartitionsToPrune(conf, filteredPartitions);
1:     }
commit:c6ca640
/////////////////////////////////////////////////////////////////////////
1: import org.apache.carbondata.core.datamap.DataMapStoreManager;
/////////////////////////////////////////////////////////////////////////
0: import org.apache.carbondata.events.OperationEventListener;
0: import org.apache.carbondata.events.OperationListenerBus;
/////////////////////////////////////////////////////////////////////////
1: 
0: // if this table is not cached, try to read the metadata of the table and cache it.
0:               FileFactory.class.getClassLoader())) {
/////////////////////////////////////////////////////////////////////////
/////////////////////////////////////////////////////////////////////////
1:   private void removeTableFromCache(SchemaTableName table) {
0:     DataMapStoreManager.getInstance().clearDataMap(cc.get(table).carbonTable.getAbsoluteTableIdentifier());
0:     cc.remove(table);
1:     tableList.remove(table);
1:   }
1: 
/////////////////////////////////////////////////////////////////////////
0:             Stream.of(carbonFileList.listFiles()).filter(a -> schemaName.equals(a.getName()))
0:                     .collect(Collectors.toList());
0:               .collect(Collectors.toSet());
/////////////////////////////////////////////////////////////////////////
0: // update logic determine later
0:     boolean isKeyExists = cc.containsKey(schemaTableName);
1: 
1:     try {
0:       if(isKeyExists && !FileFactory.isFileExist(cc.get(schemaTableName).carbonTablePath.getSchemaFilePath(),fileType)){
1:         removeTableFromCache(schemaTableName);
1:         throw new TableNotFoundException(schemaTableName);
1:       }
1:     } catch (IOException e) {
0:       e.printStackTrace();
1:       throw new RuntimeException();
1:     }
0:     if(isKeyExists && FileFactory.getCarbonFile(cc.get(schemaTableName).carbonTablePath.getPath()).getLastModifiedTime() > cc.get(schemaTableName).tableInfo.getLastUpdatedTime()){
1:       removeTableFromCache(schemaTableName);
1:     }
/////////////////////////////////////////////////////////////////////////
1: 
/////////////////////////////////////////////////////////////////////////
0:               new CarbonTableIdentifier(table.getSchemaName(), table.getTableName(),
0:                       UUID.randomUUID().toString());
0:               PathFactory.getInstance().getCarbonTablePath(storePath, cache.carbonTableIdentifier, null);
/////////////////////////////////////////////////////////////////////////
0:               new ThriftReader(cache.carbonTablePath.getSchemaFilePath(), createTBase);
0:               (org.apache.carbondata.format.TableInfo) thriftReader.read();
1: 
1:               .fromExternalToWrapperTableInfo(tableInfo, table.getSchemaName(), table.getTableName(),
0:                       storePath);
0:               CarbonTablePath.getFolderContainingFile(cache.carbonTablePath.getSchemaFilePath()));
0:               .getCarbonTable(cache.carbonTableIdentifier.getTableUniqueName());
/////////////////////////////////////////////////////////////////////////
0:                                                      Expression filters)  {
/////////////////////////////////////////////////////////////////////////
0:             .getCarbonTablePath(carbonTable.getAbsoluteTableIdentifier().getStorePath(),
0:                     carbonTable.getCarbonTableIdentifier(), null).getPath();
0:               createInputFormat(config, carbonTable.getAbsoluteTableIdentifier(), filters);
/////////////////////////////////////////////////////////////////////////
0:                   carbonInputSplit.getPath().toString(), carbonInputSplit.getStart(),
0:                   carbonInputSplit.getLength(), Arrays.asList(carbonInputSplit.getLocations()),
0:                   carbonInputSplit.getNumberOfBlocklets(), carbonInputSplit.getVersion().number(),
0:                   carbonInputSplit.getDeleteDeltaFiles(),
0:                   gson.toJson(carbonInputSplit.getDetailInfo())));
/////////////////////////////////////////////////////////////////////////
0:           throws IOException {
0:             identifier.appendWithLocalPrefix(identifier.getTablePath()));
author:chenliang613
-------------------------------------------------------------------------------
commit:32405f4
/////////////////////////////////////////////////////////////////////////
0:       CarbonProperties.getInstance().addProperty(
0:           CarbonCommonConstants.UNSAFE_WORKING_MEMORY_IN_MB,
0:     if(config.getEnableUnsafeInQueryExecution() != null) {
0:       CarbonProperties.getInstance().addProperty(
0:           CarbonCommonConstants.ENABLE_UNSAFE_IN_QUERY_EXECUTION,
0:           config.getEnableUnsafeInQueryExecution());
1:     }
commit:53267c8
/////////////////////////////////////////////////////////////////////////
0:           new BTreeDataRefNodeFinder(segmentProperties.getEachDimColumnValueSize(),
0:               segmentProperties.getNumberOfSortColumns(),
0:               segmentProperties.getNumberOfNoDictSortColumns());
commit:7c0e660
/////////////////////////////////////////////////////////////////////////
/////////////////////////////////////////////////////////////////////////
1:   private CarbonFile carbonFileList;
/////////////////////////////////////////////////////////////////////////
1:         if (carbonFileList == null) {
1:             carbonFileList = FileFactory.getCarbonFile(config.getStorePath(), fileType);
/////////////////////////////////////////////////////////////////////////
0:   public boolean updateCarbonFile() {
1:     if (carbonFileList == null) {
1:         carbonFileList = FileFactory.getCarbonFile(config.getStorePath(), fileType);
/////////////////////////////////////////////////////////////////////////
1:     updateCarbonFile();
1:     if (carbonFileList != null) {
0:       List<String> schemaList =
0:           Stream.of(carbonFileList.listFiles()).map(a -> a.getName()).collect(Collectors.toList());
0:       return schemaList;
/////////////////////////////////////////////////////////////////////////
0:     List<CarbonFile> schema = Stream.of(carbonFileList.listFiles()).filter(a -> dbName.equals(a.getName()))
/////////////////////////////////////////////////////////////////////////
1:     if (carbonFileList == null) {
1:     for (CarbonFile cf : carbonFileList.listFiles()) {
1:       if (!cf.getName().endsWith(".mdt")) {
1:         for (CarbonFile table : cf.listFiles()) {
1:           tableList.add(new SchemaTableName(cf.getName(), table.getName()));
commit:0e93a3f
/////////////////////////////////////////////////////////////////////////
1: /*
1:  * Licensed to the Apache Software Foundation (ASF) under one or more
1:  * contributor license agreements.  See the NOTICE file distributed with
1:  * this work for additional information regarding copyright ownership.
1:  * The ASF licenses this file to You under the Apache License, Version 2.0
1:  * (the "License"); you may not use this file except in compliance with
1:  * the License.  You may obtain a copy of the License at
1:  *
1:  *    http://www.apache.org/licenses/LICENSE-2.0
1:  *
1:  * Unless required by applicable law or agreed to in writing, software
1:  * distributed under the License is distributed on an "AS IS" BASIS,
1:  * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
1:  * See the License for the specific language governing permissions and
1:  * limitations under the License.
1:  */
1: 
1: package org.apache.carbondata.presto.impl;
1: 
1: import com.facebook.presto.spi.SchemaTableName;
1: import com.facebook.presto.spi.classloader.ThreadContextClassLoader;
1: import com.google.common.collect.ImmutableList;
1: import com.google.common.collect.ImmutableSet;
1: import com.google.inject.Inject;
0: import org.apache.carbondata.core.constants.CarbonCommonConstants;
0: import org.apache.carbondata.core.datastore.*;
0: import org.apache.carbondata.core.datastore.block.*;
0: import org.apache.carbondata.core.datastore.exception.IndexBuilderException;
1: import org.apache.carbondata.core.datastore.filesystem.CarbonFile;
1: import org.apache.carbondata.core.datastore.impl.FileFactory;
0: import org.apache.carbondata.core.datastore.impl.btree.BTreeDataRefNodeFinder;
0: import org.apache.carbondata.core.datastore.impl.btree.BlockBTreeLeafNode;
0: import org.apache.carbondata.core.keygenerator.KeyGenException;
1: import org.apache.carbondata.core.metadata.AbsoluteTableIdentifier;
1: import org.apache.carbondata.core.metadata.CarbonMetadata;
1: import org.apache.carbondata.core.metadata.CarbonTableIdentifier;
0: import org.apache.carbondata.core.metadata.ColumnarFormatVersion;
1: import org.apache.carbondata.core.metadata.converter.SchemaConverter;
1: import org.apache.carbondata.core.metadata.converter.ThriftWrapperSchemaConverterImpl;
1: import org.apache.carbondata.core.metadata.schema.table.CarbonTable;
1: import org.apache.carbondata.core.metadata.schema.table.TableInfo;
0: import org.apache.carbondata.core.mutate.UpdateVO;
1: import org.apache.carbondata.core.reader.ThriftReader;
1: import org.apache.carbondata.core.scan.expression.Expression;
0: import org.apache.carbondata.core.scan.filter.FilterExpressionProcessor;
0: import org.apache.carbondata.core.scan.filter.FilterUtil;
0: import org.apache.carbondata.core.scan.filter.resolver.FilterResolverIntf;
0: import org.apache.carbondata.core.service.impl.PathFactory;
0: import org.apache.carbondata.core.statusmanager.SegmentStatusManager;
0: import org.apache.carbondata.core.statusmanager.SegmentUpdateStatusManager;
0: import org.apache.carbondata.core.util.CarbonUtil;
1: import org.apache.carbondata.core.util.path.CarbonTablePath;
0: import org.apache.carbondata.hadoop.CacheClient;
0: import org.apache.carbondata.hadoop.util.CarbonInputFormatUtil;
0: import org.apache.commons.lang3.StringUtils;
1: import org.apache.hadoop.conf.Configuration;
0: import org.apache.hadoop.fs.*;
1: import org.apache.hadoop.mapreduce.InputSplit;
0: import org.apache.hadoop.mapreduce.lib.input.FileSplit;
1: import org.apache.thrift.TBase;
1: 
0: import java.io.BufferedReader;
1: import java.io.IOException;
0: import java.io.InputStream;
0: import java.io.InputStreamReader;
0: import java.net.URI;
0: import java.util.*;
0: import java.util.concurrent.ConcurrentHashMap;
1: import java.util.stream.Collectors;
1: import java.util.stream.Stream;
1: 
0: import static java.util.Objects.requireNonNull;
1: 
1: public class CarbonTableReader {
1: 
0:   /** CarbonTableReader will be a facade of these utils
1:    *
1:    * 1:CarbonMetadata,(logic table)
1:    * 2:FileFactory, (physic table file)
1:    * 3:CarbonCommonFactory, (offer some )
1:    * 4:DictionaryFactory, (parse dictionary util)
1:    */
1: 
0:   private CarbonTableConfig config;
0:   private List<SchemaTableName> tableList;
0:   private CarbonFile dbStore;
1:   private FileFactory.FileType fileType;
1: 
0:   //as a cache for Carbon reader
0:   private ConcurrentHashMap<SchemaTableName, CarbonTableCacheModel> cc;
1: 
1:   @Inject public CarbonTableReader(CarbonTableConfig config) {
1:     this.config = requireNonNull(config, "CarbonTableConfig is null");
0:     this.cc = new ConcurrentHashMap<>();
1:   }
1: 
0:   //for worker node to initialize carbon metastore
1:   public CarbonTableCacheModel getCarbonCache(SchemaTableName table) {
0:     if (!cc.containsKey(table)) {
1:       try (ThreadContextClassLoader ignored = new ThreadContextClassLoader(
0:           FileFactory.class.getClassLoader())) {
0:         if (dbStore == null) {
1:           fileType = FileFactory.getFileType(config.getStorePath());
1:           try {
0:             dbStore = FileFactory.getCarbonFile(config.getStorePath(), fileType);
1:           } catch (Exception ex) {
1:             throw new RuntimeException(ex);
1:           }
1:         }
1:       }
0:       updateSchemaTables();
1:       parseCarbonMetadata(table);
1:     }
1: 
0:     if (cc.containsKey(table)) return cc.get(table);
0:     else return null;
1:   }
1: 
1:   public List<String> getSchemaNames() {
1:     return updateSchemaList();
1:   }
1: 
0:   //default PathFilter
0:   private static final PathFilter DefaultFilter = new PathFilter() {
0:     @Override public boolean accept(Path path) {
0:       return CarbonTablePath.isCarbonDataFile(path.getName());
1:     }
1:   };
1: 
0:   public boolean updateDbStore() {
0:     if (dbStore == null) {
1:       fileType = FileFactory.getFileType(config.getStorePath());
1:       try {
0:         dbStore = FileFactory.getCarbonFile(config.getStorePath(), fileType);
1:       } catch (Exception ex) {
1:         throw new RuntimeException(ex);
1:       }
1:     }
1:     return true;
1:   }
1: 
0:   public List<String> updateSchemaList() {
0:     updateDbStore();
1: 
0:     if (dbStore != null) {
0:       List<String> scs =
0:           Stream.of(dbStore.listFiles()).map(a -> a.getName()).collect(Collectors.toList());
0:       return scs;
1:     } else return ImmutableList.of();
1:   }
1: 
1:   public Set<String> getTableNames(String schema) {
1:     requireNonNull(schema, "schema is null");
1:     return updateTableList(schema);
1:   }
1: 
0:   public Set<String> updateTableList(String dbName) {
0:     List<CarbonFile> schema = Stream.of(dbStore.listFiles()).filter(a -> dbName.equals(a.getName()))
0:         .collect(Collectors.toList());
1:     if (schema.size() > 0) {
0:       return Stream.of((schema.get(0)).listFiles()).map(a -> a.getName())
0:           .collect(Collectors.toSet());
1:     } else return ImmutableSet.of();
1:   }
1: 
1:   public CarbonTable getTable(SchemaTableName schemaTableName) {
1:     try {
0:       updateSchemaTables();
1:     } catch (Exception e) {
1:       throw new RuntimeException(e);
1:     }
1: 
1:     requireNonNull(schemaTableName, "schemaTableName is null");
0:     CarbonTable table = loadTableMetadata(schemaTableName);
1: 
0:     return table;
1:   }
1: 
0:   public void updateSchemaTables() {
0:     //update logic determine later
0:     if (dbStore == null) {
1:       updateSchemaList();
1:     }
1: 
0:     tableList = new LinkedList<>();
0:     for (CarbonFile db : dbStore.listFiles()) {
0:       if (!db.getName().endsWith(".mdt")) {
0:         for (CarbonFile table : db.listFiles()) {
0:           tableList.add(new SchemaTableName(db.getName(), table.getName()));
1:         }
1:       }
1:     }
1:   }
1: 
1:   private CarbonTable loadTableMetadata(SchemaTableName schemaTableName) {
1:     for (SchemaTableName table : tableList) {
1:       if (!table.equals(schemaTableName)) continue;
1: 
1:       return parseCarbonMetadata(table);
1:     }
0:     return null;
1:   }
1: 
1:   /**
0:    * parse carbon metadata into cc(CarbonTableReader cache)
1:    */
0:   public CarbonTable parseCarbonMetadata(SchemaTableName table) {
1:     CarbonTable result = null;
1:     try {
0:       CarbonTableCacheModel cache = cc.getOrDefault(table, new CarbonTableCacheModel());
1:       if (cache.isValid()) return cache.carbonTable;
1: 
0:       //Step1: get table meta path, load carbon table param
1:       String storePath = config.getStorePath();
0:       cache.carbonTableIdentifier =
0:           new CarbonTableIdentifier(table.getSchemaName(), table.getTableName(),
0:               UUID.randomUUID().toString());
0:       cache.carbonTablePath =
0:           PathFactory.getInstance().getCarbonTablePath(storePath, cache.carbonTableIdentifier);
0:       cc.put(table, cache);
1: 
0:       //Step2: check file existed? read schema file
1:       ThriftReader.TBaseCreator createTBase = new ThriftReader.TBaseCreator() {
1:         public TBase create() {
1:           return new org.apache.carbondata.format.TableInfo();
1:         }
1:       };
1:       ThriftReader thriftReader =
0:           new ThriftReader(cache.carbonTablePath.getSchemaFilePath(), createTBase);
1:       thriftReader.open();
1:       org.apache.carbondata.format.TableInfo tableInfo =
0:           (org.apache.carbondata.format.TableInfo) thriftReader.read();
1:       thriftReader.close();
1: 
0:       //Format Level TableInfo? need transfer to Code Level TableInfo
1:       SchemaConverter schemaConverter = new ThriftWrapperSchemaConverterImpl();
1:       TableInfo wrapperTableInfo = schemaConverter
0:           .fromExternalToWrapperTableInfo(tableInfo, table.getSchemaName(), table.getTableName(),
0:               storePath);
0:       wrapperTableInfo.setMetaDataFilepath(
0:           CarbonTablePath.getFolderContainingFile(cache.carbonTablePath.getSchemaFilePath()));
0:       //load metadata info into CarbonMetadata
1:       CarbonMetadata.getInstance().loadTableMetadata(wrapperTableInfo);
1: 
0:       cache.tableInfo = wrapperTableInfo;
0:       cache.carbonTable = CarbonMetadata.getInstance()
0:           .getCarbonTable(cache.carbonTableIdentifier.getTableUniqueName());
1:       result = cache.carbonTable;
1:     } catch (Exception ex) {
1:       throw new RuntimeException(ex);
1:     }
1: 
1:     return result;
1:   }
1: 
0:   public List<CarbonLocalInputSplit> getInputSplits2(CarbonTableCacheModel tableCacheModel,
0:       Expression filters) throws Exception {
1: 
0:     //filter, filterSegment
0:     FilterExpressionProcessor filterExpressionProcessor = new FilterExpressionProcessor();
1: 
0:     AbsoluteTableIdentifier absoluteTableIdentifier =
0:         tableCacheModel.carbonTable.getAbsoluteTableIdentifier();
0:     CacheClient cacheClient = new CacheClient(absoluteTableIdentifier.getStorePath());
0:     List<String> invalidSegments = new ArrayList<>();
0:     List<UpdateVO> invalidTimestampsList = new ArrayList<>();
1: 
0:     // get all valid segments and set them into the configuration
0:     SegmentUpdateStatusManager updateStatusManager =
0:         new SegmentUpdateStatusManager(absoluteTableIdentifier);
0:     SegmentStatusManager segmentStatusManager = new SegmentStatusManager(absoluteTableIdentifier);
0:     SegmentStatusManager.ValidAndInvalidSegmentsInfo segments =
0:         segmentStatusManager.getValidAndInvalidSegments();
1: 
0:     tableCacheModel.segments = segments.getValidSegments().toArray(new String[0]);
0:     if (segments.getValidSegments().size() == 0) {
0:       return new ArrayList<>(0);
1:     }
1: 
0:     // remove entry in the segment index if there are invalid segments
0:     invalidSegments.addAll(segments.getInvalidSegments());
0:     for (String invalidSegmentId : invalidSegments) {
0:       invalidTimestampsList.add(updateStatusManager.getInvalidTimestampRange(invalidSegmentId));
1:     }
0:     if (invalidSegments.size() > 0) {
0:       List<TableSegmentUniqueIdentifier> invalidSegmentsIds =
0:           new ArrayList<>(invalidSegments.size());
0:       for (String segId : invalidSegments) {
0:         invalidSegmentsIds.add(new TableSegmentUniqueIdentifier(absoluteTableIdentifier, segId));
1:       }
0:       cacheClient.getSegmentAccessClient().invalidateAll(invalidSegmentsIds);
1:     }
1: 
0:     // get filter for segment
0:     CarbonInputFormatUtil.processFilterExpression(filters, tableCacheModel.carbonTable);
0:     FilterResolverIntf filterInterface = CarbonInputFormatUtil
0:         .resolveFilter(filters, tableCacheModel.carbonTable.getAbsoluteTableIdentifier());
1: 
1:     List<CarbonLocalInputSplit> result = new ArrayList<>();
0:     //for each segment fetch blocks matching filter in Driver BTree
0:     for (String segmentNo : tableCacheModel.segments) {
1:       try {
0:         List<DataRefNode> dataRefNodes =
0:             getDataBlocksOfSegment(filterExpressionProcessor, absoluteTableIdentifier,
0:                 tableCacheModel.carbonTablePath, filterInterface, segmentNo, cacheClient,
0:                 updateStatusManager);
0:         for (DataRefNode dataRefNode : dataRefNodes) {
0:           BlockBTreeLeafNode leafNode = (BlockBTreeLeafNode) dataRefNode;
0:           TableBlockInfo tableBlockInfo = leafNode.getTableBlockInfo();
1: 
0:           if (CarbonUtil.isInvalidTableBlock(tableBlockInfo,
0:               updateStatusManager.getInvalidTimestampRange(tableBlockInfo.getSegmentId()),
0:               updateStatusManager)) {
0:             continue;
1:           }
0:           result.add(new CarbonLocalInputSplit(segmentNo, tableBlockInfo.getFilePath(),
0:               tableBlockInfo.getBlockOffset(), tableBlockInfo.getBlockLength(),
0:               Arrays.asList(tableBlockInfo.getLocations()),
0:               tableBlockInfo.getBlockletInfos().getNoOfBlockLets(),
0:               tableBlockInfo.getVersion().number()));
1:         }
1:       } catch (Exception ex) {
1:         throw new RuntimeException(ex);
1:       }
1:     }
0:     cacheClient.close();
1:     return result;
1:   }
1: 
1:   /**
0:    * get data blocks of given segment
1:    */
0:   private List<DataRefNode> getDataBlocksOfSegment(
0:       FilterExpressionProcessor filterExpressionProcessor,
0:       AbsoluteTableIdentifier absoluteTableIdentifier, CarbonTablePath tablePath,
0:       FilterResolverIntf resolver, String segmentId, CacheClient cacheClient,
0:       SegmentUpdateStatusManager updateStatusManager) throws IOException {
0:     //DriverQueryStatisticsRecorder recorder = CarbonTimeStatisticsFactory.getQueryStatisticsRecorderInstance();
0:     //QueryStatistic statistic = new QueryStatistic();
1: 
0:     //Segment Index
0:     Map<SegmentTaskIndexStore.TaskBucketHolder, AbstractIndex> segmentIndexMap =
0:         getSegmentAbstractIndexs(absoluteTableIdentifier, tablePath, segmentId, cacheClient,
0:             updateStatusManager);
1: 
0:     List<DataRefNode> resultFilterredBlocks = new LinkedList<DataRefNode>();
1: 
0:     if (null != segmentIndexMap) {
0:       // build result
0:       for (AbstractIndex abstractIndex : segmentIndexMap.values()) {
0:         List<DataRefNode> filterredBlocks;
0:         // if no filter is given get all blocks from Btree Index
0:         if (null == resolver) {
0:           filterredBlocks = getDataBlocksOfIndex(abstractIndex);
0:         } else {
0:           //ignore filter
0:           //filterredBlocks = getDataBlocksOfIndex(abstractIndex);
1: 
0:           // apply filter and get matching blocks
0:           filterredBlocks = filterExpressionProcessor
0:               .getFilterredBlocks(abstractIndex.getDataRefNode(), resolver, abstractIndex,
0:                   absoluteTableIdentifier);
1:         }
0:         resultFilterredBlocks.addAll(filterredBlocks);
1:       }
1:     }
0:     //statistic.addStatistics(QueryStatisticsConstants.LOAD_BLOCKS_DRIVER, System.currentTimeMillis());
0:     //recorder.recordStatisticsForDriver(statistic, "123456"/*job.getConfiguration().get("query.id")*/);
0:     return resultFilterredBlocks;
1:   }
1: 
0:   private boolean isSegmentUpdate(SegmentTaskIndexWrapper segmentTaskIndexWrapper,
0:       UpdateVO updateDetails) {
0:     if (null != updateDetails.getLatestUpdateTimestamp()
0:         && updateDetails.getLatestUpdateTimestamp() > segmentTaskIndexWrapper
0:         .getRefreshedTimeStamp()) {
1:       return true;
1:     }
0:     return false;
1:   }
1: 
0:   private Map<SegmentTaskIndexStore.TaskBucketHolder, AbstractIndex> getSegmentAbstractIndexs(/*JobContext job,*/
0:       AbsoluteTableIdentifier absoluteTableIdentifier, CarbonTablePath tablePath, String segmentId,
0:       CacheClient cacheClient, SegmentUpdateStatusManager updateStatusManager) throws IOException {
0:     Map<SegmentTaskIndexStore.TaskBucketHolder, AbstractIndex> segmentIndexMap = null;
0:     SegmentTaskIndexWrapper segmentTaskIndexWrapper = null;
0:     boolean isSegmentUpdated = false;
0:     Set<SegmentTaskIndexStore.TaskBucketHolder> taskKeys = null;
0:     TableSegmentUniqueIdentifier tableSegmentUniqueIdentifier =
0:         new TableSegmentUniqueIdentifier(absoluteTableIdentifier, segmentId);
0:     segmentTaskIndexWrapper =
0:         cacheClient.getSegmentAccessClient().getIfPresent(tableSegmentUniqueIdentifier);
0:     UpdateVO updateDetails = updateStatusManager.getInvalidTimestampRange(segmentId);
0:     if (null != segmentTaskIndexWrapper) {
0:       segmentIndexMap = segmentTaskIndexWrapper.getTaskIdToTableSegmentMap();
0:       if (isSegmentUpdate(segmentTaskIndexWrapper, updateDetails)) {
0:         taskKeys = segmentIndexMap.keySet();
0:         isSegmentUpdated = true;
1:       }
1:     }
1: 
0:     // if segment tree is not loaded, load the segment tree
0:     if (segmentIndexMap == null || isSegmentUpdated) {
1: 
0:       List<FileStatus> fileStatusList = new LinkedList<FileStatus>();
0:       List<String> segs = new ArrayList<>();
0:       segs.add(segmentId);
1: 
0:       FileSystem fs =
0:           getFileStatusOfSegments(new String[] { segmentId }, tablePath, fileStatusList);
0:       List<InputSplit> splits = getSplit(fileStatusList, fs);
1: 
0:       List<FileSplit> carbonSplits = new ArrayList<>();
0:       for (InputSplit inputSplit : splits) {
0:         FileSplit fileSplit = (FileSplit) inputSplit;
0:         String segId = CarbonTablePath.DataPathUtil
0:             .getSegmentId(fileSplit.getPath().toString());//seperator?
0:         if (segId.equals(CarbonCommonConstants.INVALID_SEGMENT_ID)) {
0:           continue;
1:         }
0:         carbonSplits.add(fileSplit);
1:       }
1: 
0:       List<TableBlockInfo> tableBlockInfoList = new ArrayList<>();
0:       for (FileSplit inputSplit : carbonSplits) {
0:         if (isValidBlockBasedOnUpdateDetails(taskKeys, inputSplit, updateDetails,
0:             updateStatusManager, segmentId)) {
1: 
0:           BlockletInfos blockletInfos = new BlockletInfos(0, 0,
0:               0);//this level we do not need blocklet info!!!! Is this a trick?
0:           tableBlockInfoList.add(
0:               new TableBlockInfo(inputSplit.getPath().toString(), inputSplit.getStart(), segmentId,
0:                   inputSplit.getLocations(), inputSplit.getLength(), blockletInfos,
0:                   ColumnarFormatVersion
0:                       .valueOf(CarbonCommonConstants.CARBON_DATA_FILE_DEFAULT_VERSION), null/*new HashMap<>(CarbonCommonConstants.DEFAULT_COLLECTION_SIZE)*/));//null
1:         }
1:       }
1: 
0:       Map<String, List<TableBlockInfo>> segmentToTableBlocksInfos = new HashMap<>();
0:       segmentToTableBlocksInfos.put(segmentId, tableBlockInfoList);
0:       // get Btree blocks for given segment
0:       tableSegmentUniqueIdentifier.setSegmentToTableBlocksInfos(segmentToTableBlocksInfos);
0:       tableSegmentUniqueIdentifier.setIsSegmentUpdated(isSegmentUpdated);
0:       segmentTaskIndexWrapper =
0:           cacheClient.getSegmentAccessClient().get(tableSegmentUniqueIdentifier);
0:       segmentIndexMap = segmentTaskIndexWrapper.getTaskIdToTableSegmentMap();
1:     }
0:     return segmentIndexMap;
1:   }
1: 
0:   private boolean isValidBlockBasedOnUpdateDetails(
0:       Set<SegmentTaskIndexStore.TaskBucketHolder> taskKeys, FileSplit carbonInputSplit,
0:       UpdateVO updateDetails, SegmentUpdateStatusManager updateStatusManager, String segmentId) {
0:     String taskID = null;
0:     if (null != carbonInputSplit) {
0:       if (!updateStatusManager.isBlockValid(segmentId, carbonInputSplit.getPath().getName())) {
0:         return false;
1:       }
1: 
0:       if (null == taskKeys) {
1:         return true;
1:       }
1: 
0:       taskID = CarbonTablePath.DataFileUtil.getTaskNo(carbonInputSplit.getPath().getName());
0:       String bucketNo =
0:           CarbonTablePath.DataFileUtil.getBucketNo(carbonInputSplit.getPath().getName());
1: 
0:       SegmentTaskIndexStore.TaskBucketHolder taskBucketHolder =
0:           new SegmentTaskIndexStore.TaskBucketHolder(taskID, bucketNo);
1: 
0:       String blockTimestamp = carbonInputSplit.getPath().getName()
0:           .substring(carbonInputSplit.getPath().getName().lastIndexOf('-') + 1,
0:               carbonInputSplit.getPath().getName().lastIndexOf('.'));
0:       if (!(updateDetails.getUpdateDeltaStartTimestamp() != null
0:           && Long.parseLong(blockTimestamp) < updateDetails.getUpdateDeltaStartTimestamp())) {
0:         if (!taskKeys.contains(taskBucketHolder)) {
1:           return true;
1:         }
1:       }
1:     }
0:     return false;
1:   }
1: 
0:   private List<InputSplit> getSplit(List<FileStatus> fileStatusList, FileSystem targetSystem)
0:       throws IOException {
1: 
0:     Iterator split = fileStatusList.iterator();
1: 
0:     List<InputSplit> splits = new ArrayList<>();
1: 
0:     while (true) {
0:       while (true) {
0:         while (split.hasNext()) {
0:           FileStatus file = (FileStatus) split.next();
0:           Path path = file.getPath();
0:           long length = file.getLen();
0:           if (length != 0L) {
0:             BlockLocation[] blkLocations;
0:             if (file instanceof LocatedFileStatus) {
0:               blkLocations = ((LocatedFileStatus) file).getBlockLocations();
0:             } else {
0:               blkLocations = targetSystem.getFileBlockLocations(file, 0L, length);
1:             }
1: 
0:             if (this.isSplitable()) {
0:               long blockSize1 = file.getBlockSize();
0:               long splitSize = this.computeSplitSize(blockSize1, 1, Long.MAX_VALUE);
1: 
0:               long bytesRemaining;
0:               int blkIndex;
0:               for (
0:                   bytesRemaining = length;
0:                   (double) bytesRemaining / (double) splitSize > 1.1D;
0:                   bytesRemaining -= splitSize) {
0:                 blkIndex = this.getBlockIndex(blkLocations, length - bytesRemaining);
0:                 splits.add(this.makeSplit(path, length - bytesRemaining, splitSize,
0:                     blkLocations[blkIndex].getHosts()));
1:               }
1: 
0:               if (bytesRemaining != 0L) {
0:                 blkIndex = this.getBlockIndex(blkLocations, length - bytesRemaining);
0:                 splits.add(this.makeSplit(path, length - bytesRemaining, bytesRemaining,
0:                     blkLocations[blkIndex].getHosts()));
1:               }
0:             } else {
0:               splits.add(new org.apache.hadoop.mapreduce.lib.input.FileSplit(path, 0L, length,
0:                   blkLocations[0].getHosts()));
1:             }
0:           } else {
0:             splits.add(new org.apache.hadoop.mapreduce.lib.input.FileSplit(path, 0L, length,
0:                 new String[0]));
1:           }
1:         }
0:         return splits;
1:       }
1:     }
1: 
1:   }
1: 
0:   private String[] getValidPartitions() {
0:     //TODO: has to Identify partitions by partition pruning
0:     return new String[] { "0" };
1:   }
1: 
0:   private FileSystem getFileStatusOfSegments(String[] segmentsToConsider, CarbonTablePath tablePath,
0:       List<FileStatus> result) throws IOException {
0:     String[] partitionsToConsider = getValidPartitions();
0:     if (partitionsToConsider.length == 0) {
0:       throw new IOException("No partitions/data found");
1:     }
1: 
0:     FileSystem fs = null;
1: 
0:     //PathFilter inputFilter = getDataFileFilter(job);
1: 
0:     // get tokens for all the required FileSystem for table path
0:         /*TokenCache.obtainTokensForNamenodes(job.getCredentials(), new Path[] { tablePath },
0:                 job.getConfiguration());*/
1: 
0:     //get all data files of valid partitions and segments
0:     for (int i = 0; i < partitionsToConsider.length; ++i) {
0:       String partition = partitionsToConsider[i];
1: 
0:       for (int j = 0; j < segmentsToConsider.length; ++j) {
0:         String segmentId = segmentsToConsider[j];
0:         Path segmentPath = new Path(tablePath.getCarbonDataDirectoryPath(partition, segmentId));
1: 
1:         try {
0:           Configuration conf = new Configuration();
0:           fs = segmentPath.getFileSystem(conf);
0:           //fs.initialize(segmentPath.toUri(), conf);
1: 
0:           RemoteIterator<LocatedFileStatus> iter = fs.listLocatedStatus(segmentPath);
0:           while (iter.hasNext()) {
0:             LocatedFileStatus stat = iter.next();
0:             //if(stat.getPath().toString().contains("carbondata"))//carbondatacarbonInputFilter?
0:             if (DefaultFilter.accept(stat.getPath())) {
0:               if (stat.isDirectory()) {
0:                 addInputPathRecursively(result, fs, stat.getPath(), DefaultFilter);
0:               } else {
0:                 result.add(stat);
1:               }
1:             }
1:           }
1:         } catch (Exception ex) {
0:           System.out.println(ex.toString());
1:         }
1:       }
1:     }
0:     return fs;
1:   }
1: 
0:   protected void addInputPathRecursively(List<FileStatus> result, FileSystem fs, Path path,
0:       PathFilter inputFilter) throws IOException {
0:     RemoteIterator iter = fs.listLocatedStatus(path);
1: 
0:     while (iter.hasNext()) {
0:       LocatedFileStatus stat = (LocatedFileStatus) iter.next();
0:       if (inputFilter.accept(stat.getPath())) {
0:         if (stat.isDirectory()) {
0:           this.addInputPathRecursively(result, fs, stat.getPath(), inputFilter);
0:         } else {
0:           result.add(stat);
1:         }
1:       }
1:     }
1: 
1:   }
1: 
1:   /**
0:    * get data blocks of given btree
1:    */
0:   private List<DataRefNode> getDataBlocksOfIndex(AbstractIndex abstractIndex) {
0:     List<DataRefNode> blocks = new LinkedList<DataRefNode>();
0:     SegmentProperties segmentProperties = abstractIndex.getSegmentProperties();
1: 
1:     try {
0:       IndexKey startIndexKey = FilterUtil.prepareDefaultStartIndexKey(segmentProperties);
0:       IndexKey endIndexKey = FilterUtil.prepareDefaultEndIndexKey(segmentProperties);
1: 
0:       // Add all blocks of btree into result
0:       DataRefNodeFinder blockFinder =
0:           new BTreeDataRefNodeFinder(segmentProperties.getEachDimColumnValueSize());
0:       DataRefNode startBlock =
0:           blockFinder.findFirstDataBlock(abstractIndex.getDataRefNode(), startIndexKey);
0:       DataRefNode endBlock =
0:           blockFinder.findLastDataBlock(abstractIndex.getDataRefNode(), endIndexKey);
0:       while (startBlock != endBlock) {
0:         blocks.add(startBlock);
0:         startBlock = startBlock.getNextDataRefNode();
1:       }
0:       blocks.add(endBlock);
1: 
0:     } catch (KeyGenException e) {
0:       System.out.println("Could not generate start key" + e.getMessage());
1:     }
0:     return blocks;
1:   }
1: 
0:   private boolean isSplitable() {
1:     try {
0:       // Don't split the file if it is local file system
0:       if (this.fileType == FileFactory.FileType.LOCAL) {
0:         return false;
1:       }
1:     } catch (Exception e) {
1:       return true;
1:     }
1:     return true;
1:   }
1: 
0:   private long computeSplitSize(long blockSize, long minSize, long maxSize) {
0:     return Math.max(minSize, Math.min(maxSize, blockSize));
1:   }
1: 
0:   private FileSplit makeSplit(Path file, long start, long length, String[] hosts) {
0:     return new FileSplit(file, start, length, hosts);
1:   }
1: 
0:   private int getBlockIndex(BlockLocation[] blkLocations, long offset) {
0:     for (int i = 0; i < blkLocations.length; i++) {
0:       // is the offset inside this block?
0:       if ((blkLocations[i].getOffset() <= offset) && (offset
0:           < blkLocations[i].getOffset() + blkLocations[i].getLength())) {
0:         return i;
1:       }
1:     }
0:     BlockLocation last = blkLocations[blkLocations.length - 1];
0:     long fileLength = last.getOffset() + last.getLength() - 1;
0:     throw new IllegalArgumentException("Offset " + offset +
0:         " is outside of file (0.." +
0:         fileLength + ")");
1:   }
1: 
1:   /**
0:    * get total number of rows. for count(*)
1:    *
0:    * @throws IOException
0:    * @throws IndexBuilderException
1:    */
0:   public long getRowCount() throws IOException, IndexBuilderException {
0:     long rowCount = 0;
0:         /*AbsoluteTableIdentifier absoluteTableIdentifier = this.carbonTable.getAbsoluteTableIdentifier();
1: 
0:         // no of core to load the blocks in driver
0:         //addSegmentsIfEmpty(job, absoluteTableIdentifier);
0:         int numberOfCores = CarbonCommonConstants.NUMBER_OF_CORE_TO_LOAD_DRIVER_SEGMENT_DEFAULT_VALUE;
1:         try {
0:             numberOfCores = Integer.parseInt(CarbonProperties.getInstance()
0:                     .getProperty(CarbonCommonConstants.NUMBER_OF_CORE_TO_LOAD_DRIVER_SEGMENT));
0:         } catch (NumberFormatException e) {
0:             numberOfCores = CarbonCommonConstants.NUMBER_OF_CORE_TO_LOAD_DRIVER_SEGMENT_DEFAULT_VALUE;
1:         }
0:         // creating a thread pool
0:         ExecutorService threadPool = Executors.newFixedThreadPool(numberOfCores);
0:         List<Future<Map<String, AbstractIndex>>> loadedBlocks =
0:                 new ArrayList<Future<Map<String, AbstractIndex>>>();
0:         //for each segment fetch blocks matching filter in Driver BTree
0:         for (String segmentNo : this.segmentList) {
0:             // submitting the task
0:             loadedBlocks
0:                     .add(threadPool.submit(new BlocksLoaderThread(*//*job,*//* absoluteTableIdentifier, segmentNo)));
1:         }
0:         threadPool.shutdown();
1:         try {
0:             threadPool.awaitTermination(1, TimeUnit.HOURS);
0:         } catch (InterruptedException e) {
0:             throw new IndexBuilderException(e);
1:         }
1:         try {
0:             // adding all the rows of the blocks to get the total row
0:             // count
0:             for (Future<Map<String, AbstractIndex>> block : loadedBlocks) {
0:                 for (AbstractIndex abstractIndex : block.get().values()) {
0:                     rowCount += abstractIndex.getTotalNumberOfRows();
1:                 }
1:             }
0:         } catch (InterruptedException | ExecutionException e) {
0:             throw new IndexBuilderException(e);
0:         }*/
0:     return rowCount;
1:   }
1: }
commit:7ee8e27
/////////////////////////////////////////////////////////////////////////
commit:2712330
/////////////////////////////////////////////////////////////////////////
0:   //CarbonTableReader will be a facade of these utils
0:   //[
0:   // 1:CarbonMetadata,(logic table)
0:   // 2:FileFactory, (physic table file)
0:   // 3:CarbonCommonFactory, (offer some )
0:   // 4:DictionaryFactory, (parse dictionary util)
0:   //]
0:   private CarbonTableConfig config;
0:   private List<SchemaTableName> tableList;
0:   private CarbonFile dbStore;
0:   private FileFactory.FileType fileType;
0:   private ConcurrentHashMap<SchemaTableName, CarbonTableCacheModel> cc;
0:       //as a cache for Carbon reader
0:   @Inject public CarbonTableReader(CarbonTableConfig config) {
0:     this.config = requireNonNull(config, "CarbonTableConfig is null");
0:     this.cc = new ConcurrentHashMap<>();
0:   }
0:   public CarbonTableCacheModel getCarbonCache(SchemaTableName table) {
0:     if (!cc.containsKey(table))//for worker node to initalize carbon metastore
0:       try (ThreadContextClassLoader ignored = new ThreadContextClassLoader(
0:           FileFactory.class.getClassLoader())) {
0:         if (dbStore == null) {
0:           fileType = FileFactory.getFileType(config.getStorePath());
0:           try {
0:             dbStore = FileFactory.getCarbonFile(config.getStorePath(), fileType);
0:           } catch (Exception ex) {
0:           }
0:       }
0:       updateSchemaTables();
0:       parseCarbonMetadata(table);
0:     if (cc.containsKey(table)) return cc.get(table);
0:     else return null;//need to reload?*/
0:   }
0:   public List<String> getSchemaNames() {
0:     return updateSchemaList();
0:   }
0:   //default PathFilter
0:   private static final PathFilter DefaultFilter = new PathFilter() {
0:     @Override public boolean accept(Path path) {
0:       return CarbonTablePath.isCarbonDataFile(path.getName());
0:     }
0:   };
0:   public boolean updateDbStore() {
0:     if (dbStore == null) {
0:       fileType = FileFactory.getFileType(config.getStorePath());
0:       try {
0:         dbStore = FileFactory.getCarbonFile(config.getStorePath(), fileType);
0:       } catch (Exception ex) {
0:         throw new RuntimeException(ex);
0:       }
0:     }
0:     return true;
0:   }
0:   public List<String> updateSchemaList() {
0:     updateDbStore();
0:     if (dbStore != null) {
0:       List<String> scs =
0:           Stream.of(dbStore.listFiles()).map(a -> a.getName()).collect(Collectors.toList());
0:       return scs;
0:     } else return ImmutableList.of();
0:   }
0:   public Set<String> getTableNames(String schema) {
0:     requireNonNull(schema, "schema is null");
0:     return updateTableList(schema);
0:   }
0:   public Set<String> updateTableList(String dbName) {
0:     List<CarbonFile> schema = Stream.of(dbStore.listFiles()).filter(a -> dbName.equals(a.getName()))
0:         .collect(Collectors.toList());
0:     if (schema.size() > 0) {
0:       return Stream.of((schema.get(0)).listFiles()).map(a -> a.getName())
0:           .collect(Collectors.toSet());
0:     } else return ImmutableSet.of();
0:   }
0:   public CarbonTable getTable(SchemaTableName schemaTableName) {
0:     try {
0:       updateSchemaTables();
0:     } catch (Exception e) {
0:       throw new RuntimeException(e);
0:     requireNonNull(schemaTableName, "schemaTableName is null");
0:     CarbonTable table = loadTableMetadata(schemaTableName);
0:     return table;
0:   }
0:   public void updateSchemaTables() {
0:     //update logic determine later
0:     if (dbStore == null) {
0:       updateSchemaList();
0:     tableList = new LinkedList<>();
0:     for (CarbonFile db : dbStore.listFiles()) {
0:       if (!db.getName().endsWith(".mdt")) {
0:         for (CarbonFile table : db.listFiles()) {
0:           tableList.add(new SchemaTableName(db.getName(), table.getName()));
0:       }
0:     }
0:   }
0: 
0:   private CarbonTable loadTableMetadata(SchemaTableName schemaTableName) {
0:     for (SchemaTableName table : tableList) {
0:       if (!table.equals(schemaTableName)) continue;
0: 
0:       return parseCarbonMetadata(table);
0:     }
0:     return null;
0:   }
0: 
1:   /**
0:    * parse carbon metadata into cc(CarbonTableReader cache)
0:    **/
0:   public CarbonTable parseCarbonMetadata(SchemaTableName table) {
0:     CarbonTable result = null;
0:     try {
0:       //StoreFactory
0:       CarbonTableCacheModel cache = cc.getOrDefault(table, new CarbonTableCacheModel());
0:       if (cache.isValid()) return cache.carbonTable;
0: 
0:       //Step1: get table meta path, load carbon table param
0:       String storePath = config.getStorePath();
0:       cache.carbonTableIdentifier =
0:           new CarbonTableIdentifier(table.getSchemaName(), table.getTableName(),
0:               UUID.randomUUID().toString());
0:       cache.carbonTablePath =
0:           PathFactory.getInstance().getCarbonTablePath(storePath, cache.carbonTableIdentifier);
0:       cc.put(table, cache);
0: 
0:       //Step2: check file existed? read schema file
0:       ThriftReader.TBaseCreator createTBase = new ThriftReader.TBaseCreator() {
0:         public TBase create() {
0:           return new org.apache.carbondata.format.TableInfo();
0:         }
0:       };
0:       ThriftReader thriftReader =
0:           new ThriftReader(cache.carbonTablePath.getSchemaFilePath(), createTBase);
0:       thriftReader.open();
0:       org.apache.carbondata.format.TableInfo tableInfo =
0:           (org.apache.carbondata.format.TableInfo) thriftReader.read();
0:       thriftReader.close();
0: 
0:       //Format LevelTableInfo? ?Code LevelTableInfo
0:       SchemaConverter schemaConverter = new ThriftWrapperSchemaConverterImpl();
0:       TableInfo wrapperTableInfo = schemaConverter
0:           .fromExternalToWrapperTableInfo(tableInfo, table.getSchemaName(), table.getTableName(),
0:               storePath);
0:       wrapperTableInfo.setMetaDataFilepath(
0:           CarbonTablePath.getFolderContainingFile(cache.carbonTablePath.getSchemaFilePath()));
0:       //CarbonMetadata
0:       CarbonMetadata.getInstance().loadTableMetadata(wrapperTableInfo);
0: 
0:       cache.tableInfo = wrapperTableInfo;
0:       cache.carbonTable = CarbonMetadata.getInstance()
0:           .getCarbonTable(cache.carbonTableIdentifier.getTableUniqueName());
0:       result = cache.carbonTable;
0:     } catch (Exception ex) {
0:       throw new RuntimeException(ex);
0:     }
0: 
0:     return result;
0:   }
0: 
0:   public List<CarbonLocalInputSplit> getInputSplits2(CarbonTableCacheModel tableCacheModel,
0:       Expression filters) throws Exception {
0: 
0:     //filter, filterSegment
0:     FilterExpressionProcessor filterExpressionProcessor = new FilterExpressionProcessor();
0: 
0:     AbsoluteTableIdentifier absoluteTableIdentifier =
0:         tableCacheModel.carbonTable.getAbsoluteTableIdentifier();
0:     CacheClient cacheClient = new CacheClient(absoluteTableIdentifier.getStorePath());
0:     List<String> invalidSegments = new ArrayList<>();
0:     List<UpdateVO> invalidTimestampsList = new ArrayList<>();
0: 
0:     // get all valid segments and set them into the configuration
0:     SegmentUpdateStatusManager updateStatusManager =
0:         new SegmentUpdateStatusManager(absoluteTableIdentifier);
0:     SegmentStatusManager segmentStatusManager = new SegmentStatusManager(absoluteTableIdentifier);
0:     SegmentStatusManager.ValidAndInvalidSegmentsInfo segments =
0:         segmentStatusManager.getValidAndInvalidSegments();
0: 
0:     tableCacheModel.segments = segments.getValidSegments().toArray(new String[0]);
0:     if (segments.getValidSegments().size() == 0) {
0:       return new ArrayList<>(0);
0:     }
0: 
0:     // remove entry in the segment index if there are invalid segments
0:     invalidSegments.addAll(segments.getInvalidSegments());
0:     for (String invalidSegmentId : invalidSegments) {
0:       invalidTimestampsList.add(updateStatusManager.getInvalidTimestampRange(invalidSegmentId));
0:     }
0:     if (invalidSegments.size() > 0) {
0:       List<TableSegmentUniqueIdentifier> invalidSegmentsIds =
0:           new ArrayList<>(invalidSegments.size());
0:       for (String segId : invalidSegments) {
0:         invalidSegmentsIds.add(new TableSegmentUniqueIdentifier(absoluteTableIdentifier, segId));
0:       }
0:       cacheClient.getSegmentAccessClient().invalidateAll(invalidSegmentsIds);
0:     }
0: 
0:     // get filter for segment
0:     CarbonInputFormatUtil.processFilterExpression(filters, tableCacheModel.carbonTable);
0:     FilterResolverIntf filterInterface = CarbonInputFormatUtil
0:         .resolveFilter(filters, tableCacheModel.carbonTable.getAbsoluteTableIdentifier());
0: 
0:     List<CarbonLocalInputSplit> result = new ArrayList<>();
0:     //for each segment fetch blocks matching filter in Driver BTree
0:     for (String segmentNo : tableCacheModel.segments) {
0:       try {
0:         List<DataRefNode> dataRefNodes =
0:             getDataBlocksOfSegment(filterExpressionProcessor, absoluteTableIdentifier,
0:                 tableCacheModel.carbonTablePath, filterInterface, segmentNo, cacheClient,
0:                 updateStatusManager);
0:         for (DataRefNode dataRefNode : dataRefNodes) {
0:           BlockBTreeLeafNode leafNode = (BlockBTreeLeafNode) dataRefNode;
0:           TableBlockInfo tableBlockInfo = leafNode.getTableBlockInfo();
0: 
0:           if (CarbonUtil.isInvalidTableBlock(tableBlockInfo,
0:               updateStatusManager.getInvalidTimestampRange(tableBlockInfo.getSegmentId()),
0:               updateStatusManager)) {
0:             continue;
0:           }
0:           result.add(new CarbonLocalInputSplit(segmentNo, tableBlockInfo.getFilePath(),
0:               tableBlockInfo.getBlockOffset(), tableBlockInfo.getBlockLength(),
0:               Arrays.asList(tableBlockInfo.getLocations()),
0:               tableBlockInfo.getBlockletInfos().getNoOfBlockLets(),
0:               tableBlockInfo.getVersion().number()));
0:         }
0:       } catch (Exception ex) {
0:         throw new RuntimeException(ex);
0:       }
0:     }
0:     cacheClient.close();
0:     return result;
0:   }
0: 
1:   /**
0:    * get data blocks of given segment
1:    */
0:   private List<DataRefNode> getDataBlocksOfSegment(
0:       FilterExpressionProcessor filterExpressionProcessor,
0:       AbsoluteTableIdentifier absoluteTableIdentifier, CarbonTablePath tablePath,
0:       FilterResolverIntf resolver, String segmentId, CacheClient cacheClient,
0:       SegmentUpdateStatusManager updateStatusManager) throws IOException {
0:     //DriverQueryStatisticsRecorder recorder = CarbonTimeStatisticsFactory.getQueryStatisticsRecorderInstance();
0:     //QueryStatistic statistic = new QueryStatistic();
0: 
0:     //Segment Index
0:     Map<SegmentTaskIndexStore.TaskBucketHolder, AbstractIndex> segmentIndexMap =
0:         getSegmentAbstractIndexs(absoluteTableIdentifier, tablePath, segmentId, cacheClient,
0:             updateStatusManager);
0: 
0:     List<DataRefNode> resultFilterredBlocks = new LinkedList<DataRefNode>();
0: 
0:     if (null != segmentIndexMap) {
0:       // build result
0:       for (AbstractIndex abstractIndex : segmentIndexMap.values()) {
0:         List<DataRefNode> filterredBlocks;
0:         // if no filter is given get all blocks from Btree Index
0:         if (null == resolver) {
0:           filterredBlocks = getDataBlocksOfIndex(abstractIndex);
0:         } else {
0:           //ignore filter
0:           //filterredBlocks = getDataBlocksOfIndex(abstractIndex);
0: 
0:           // apply filter and get matching blocks
0:           filterredBlocks = filterExpressionProcessor
0:               .getFilterredBlocks(abstractIndex.getDataRefNode(), resolver, abstractIndex,
0:                   absoluteTableIdentifier);
0:         }
0:         resultFilterredBlocks.addAll(filterredBlocks);
0:       }
0:     }
0:     //statistic.addStatistics(QueryStatisticsConstants.LOAD_BLOCKS_DRIVER, System.currentTimeMillis());
0:     //recorder.recordStatisticsForDriver(statistic, "123456"/*job.getConfiguration().get("query.id")*/);
0:     return resultFilterredBlocks;
0:   }
0: 
0:   private boolean isSegmentUpdate(SegmentTaskIndexWrapper segmentTaskIndexWrapper,
0:       UpdateVO updateDetails) {
0:     if (null != updateDetails.getLatestUpdateTimestamp()
0:         && updateDetails.getLatestUpdateTimestamp() > segmentTaskIndexWrapper
0:         .getRefreshedTimeStamp()) {
0:       return true;
0:     }
0:     return false;
0:   }
0: 
0:   private Map<SegmentTaskIndexStore.TaskBucketHolder, AbstractIndex> getSegmentAbstractIndexs(/*JobContext job,*/
0:       AbsoluteTableIdentifier absoluteTableIdentifier, CarbonTablePath tablePath, String segmentId,
0:       CacheClient cacheClient, SegmentUpdateStatusManager updateStatusManager) throws IOException {
0:     Map<SegmentTaskIndexStore.TaskBucketHolder, AbstractIndex> segmentIndexMap = null;
0:     SegmentTaskIndexWrapper segmentTaskIndexWrapper = null;
0:     boolean isSegmentUpdated = false;
0:     Set<SegmentTaskIndexStore.TaskBucketHolder> taskKeys = null;
0:     TableSegmentUniqueIdentifier tableSegmentUniqueIdentifier =
0:         new TableSegmentUniqueIdentifier(absoluteTableIdentifier, segmentId);
0:     segmentTaskIndexWrapper =
0:         cacheClient.getSegmentAccessClient().getIfPresent(tableSegmentUniqueIdentifier);
0:     UpdateVO updateDetails = updateStatusManager.getInvalidTimestampRange(segmentId);
0:     if (null != segmentTaskIndexWrapper) {
0:       segmentIndexMap = segmentTaskIndexWrapper.getTaskIdToTableSegmentMap();
0:       if (isSegmentUpdate(segmentTaskIndexWrapper, updateDetails)) {
0:         taskKeys = segmentIndexMap.keySet();
0:         isSegmentUpdated = true;
0:       }
0:     }
0: 
0:     // if segment tree is not loaded, load the segment tree
0:     if (segmentIndexMap == null || isSegmentUpdated) {
0: 
0:       List<FileStatus> fileStatusList = new LinkedList<FileStatus>();
0:       List<String> segs = new ArrayList<>();
0:       segs.add(segmentId);
0: 
0:       FileSystem fs =
0:           getFileStatusOfSegments(new String[] { segmentId }, tablePath, fileStatusList);
0:       List<InputSplit> splits = getSplit(fileStatusList, fs);
0: 
0:       List<FileSplit> carbonSplits = new ArrayList<>();
0:       for (InputSplit inputSplit : splits) {
0:         FileSplit fileSplit = (FileSplit) inputSplit;
0:         String segId = CarbonTablePath.DataPathUtil
0:             .getSegmentId(fileSplit.getPath().toString());//seperator?
0:         if (segId.equals(CarbonCommonConstants.INVALID_SEGMENT_ID)) {
0:           continue;
0:         }
0:         carbonSplits.add(fileSplit);
0:       }
0: 
0:       List<TableBlockInfo> tableBlockInfoList = new ArrayList<>();
0:       for (FileSplit inputSplit : carbonSplits) {
0:         if (isValidBlockBasedOnUpdateDetails(taskKeys, inputSplit, updateDetails,
0:             updateStatusManager, segmentId)) {
0: 
0:           BlockletInfos blockletInfos = new BlockletInfos(0, 0,
0:               0);//this level we do not need blocklet info!!!! Is this a trick?
0:           tableBlockInfoList.add(
0:               new TableBlockInfo(inputSplit.getPath().toString(), inputSplit.getStart(), segmentId,
0:                   inputSplit.getLocations(), inputSplit.getLength(), blockletInfos,
0:                   ColumnarFormatVersion
0:                       .valueOf(CarbonCommonConstants.CARBON_DATA_FILE_DEFAULT_VERSION), null/*new HashMap<>(CarbonCommonConstants.DEFAULT_COLLECTION_SIZE)*/));//null
0:         }
0:       }
0: 
0:       Map<String, List<TableBlockInfo>> segmentToTableBlocksInfos = new HashMap<>();
0:       segmentToTableBlocksInfos.put(segmentId, tableBlockInfoList);
0:       // get Btree blocks for given segment
0:       tableSegmentUniqueIdentifier.setSegmentToTableBlocksInfos(segmentToTableBlocksInfos);
0:       tableSegmentUniqueIdentifier.setIsSegmentUpdated(isSegmentUpdated);
0:       segmentTaskIndexWrapper =
0:           cacheClient.getSegmentAccessClient().get(tableSegmentUniqueIdentifier);
0:       segmentIndexMap = segmentTaskIndexWrapper.getTaskIdToTableSegmentMap();
0:     }
0:     return segmentIndexMap;
0:   }
0: 
0:   private boolean isValidBlockBasedOnUpdateDetails(
0:       Set<SegmentTaskIndexStore.TaskBucketHolder> taskKeys, FileSplit carbonInputSplit,
0:       UpdateVO updateDetails, SegmentUpdateStatusManager updateStatusManager, String segmentId) {
0:     String taskID = null;
0:     if (null != carbonInputSplit) {
0:       if (!updateStatusManager.isBlockValid(segmentId, carbonInputSplit.getPath().getName())) {
0:       }
0: 
0:       if (null == taskKeys) {
0:         return true;
0:       }
0: 
0:       taskID = CarbonTablePath.DataFileUtil.getTaskNo(carbonInputSplit.getPath().getName());
0:       String bucketNo =
0:           CarbonTablePath.DataFileUtil.getBucketNo(carbonInputSplit.getPath().getName());
0: 
0:       SegmentTaskIndexStore.TaskBucketHolder taskBucketHolder =
0:           new SegmentTaskIndexStore.TaskBucketHolder(taskID, bucketNo);
0: 
0:       String blockTimestamp = carbonInputSplit.getPath().getName()
0:           .substring(carbonInputSplit.getPath().getName().lastIndexOf('-') + 1,
0:               carbonInputSplit.getPath().getName().lastIndexOf('.'));
0:       if (!(updateDetails.getUpdateDeltaStartTimestamp() != null
0:           && Long.parseLong(blockTimestamp) < updateDetails.getUpdateDeltaStartTimestamp())) {
0:         if (!taskKeys.contains(taskBucketHolder)) {
0:           return true;
0:         }
0:       }
0:     }
0:     return false;
0:   }
0: 
0:   private List<InputSplit> getSplit(List<FileStatus> fileStatusList, FileSystem targetSystem)
0:       throws IOException {
0: 
0:     Iterator split = fileStatusList.iterator();
0: 
0:     List<InputSplit> splits = new ArrayList<>();
0: 
0:     while (true) {
0:       while (true) {
0:         while (split.hasNext()) {
0:           FileStatus file = (FileStatus) split.next();
0:           Path path = file.getPath();
0:           long length = file.getLen();
0:           if (length != 0L) {
0:             BlockLocation[] blkLocations;
0:             if (file instanceof LocatedFileStatus) {
0:               blkLocations = ((LocatedFileStatus) file).getBlockLocations();
0:             } else {
0:               blkLocations = targetSystem.getFileBlockLocations(file, 0L, length);
0:             }
0: 
0:             if (this.isSplitable()) {
0:               long blockSize1 = file.getBlockSize();
0:               long splitSize = this.computeSplitSize(blockSize1, 1, Long.MAX_VALUE);
0: 
0:               long bytesRemaining;
0:               int blkIndex;
0:               for (
0:                   bytesRemaining = length;
0:                   (double) bytesRemaining / (double) splitSize > 1.1D;
0:                   bytesRemaining -= splitSize) {
0:                 blkIndex = this.getBlockIndex(blkLocations, length - bytesRemaining);
0:                 splits.add(this.makeSplit(path, length - bytesRemaining, splitSize,
0:                     blkLocations[blkIndex].getHosts()));
0:               }
0: 
0:               if (bytesRemaining != 0L) {
0:                 blkIndex = this.getBlockIndex(blkLocations, length - bytesRemaining);
0:                 splits.add(this.makeSplit(path, length - bytesRemaining, bytesRemaining,
0:                     blkLocations[blkIndex].getHosts()));
0:               }
0:             } else {
0:               splits.add(new org.apache.hadoop.mapreduce.lib.input.FileSplit(path, 0L, length,
0:                   blkLocations[0].getHosts()));
0:             }
0:           } else {
0:             splits.add(new org.apache.hadoop.mapreduce.lib.input.FileSplit(path, 0L, length,
0:                 new String[0]));
0:           }
0:         }
0:         return splits;
0:       }
0:   }
0:   private String[] getValidPartitions() {
0:     //TODO: has to Identify partitions by partition pruning
0:     return new String[] { "0" };
0:   }
0:   private FileSystem getFileStatusOfSegments(String[] segmentsToConsider, CarbonTablePath tablePath,
0:       List<FileStatus> result) throws IOException {
0:     String[] partitionsToConsider = getValidPartitions();
0:     if (partitionsToConsider.length == 0) {
0:       throw new IOException("No partitions/data found");
0:     FileSystem fs = null;
0:     //PathFilter inputFilter = getDataFileFilter(job);
0:     // get tokens for all the required FileSystem for table path
0:     //get all data files of valid partitions and segments
0:     for (int i = 0; i < partitionsToConsider.length; ++i) {
0:       String partition = partitionsToConsider[i];
0:       for (int j = 0; j < segmentsToConsider.length; ++j) {
0:         String segmentId = segmentsToConsider[j];
0:         Path segmentPath = new Path(tablePath.getCarbonDataDirectoryPath(partition, segmentId));
0:           Configuration conf = new Configuration();
0:           fs = segmentPath.getFileSystem(conf);
0:           //fs.initialize(segmentPath.toUri(), conf);
0:           RemoteIterator<LocatedFileStatus> iter = fs.listLocatedStatus(segmentPath);
0:           while (iter.hasNext()) {
0:             LocatedFileStatus stat = iter.next();
0:             //if(stat.getPath().toString().contains("carbondata"))//carbondatacarbonInputFilter?
0:             if (DefaultFilter.accept(stat.getPath())) {
0:               if (stat.isDirectory()) {
0:                 addInputPathRecursively(result, fs, stat.getPath(), DefaultFilter);
0:               } else {
0:                 result.add(stat);
0:               }
0:           }
0:         } catch (Exception ex) {
0:           System.out.println(ex.toString());
0:       }
0:     return fs;
0:   }
0:   protected void addInputPathRecursively(List<FileStatus> result, FileSystem fs, Path path,
0:       PathFilter inputFilter) throws IOException {
0:     RemoteIterator iter = fs.listLocatedStatus(path);
0: 
0:     while (iter.hasNext()) {
0:       LocatedFileStatus stat = (LocatedFileStatus) iter.next();
0:       if (inputFilter.accept(stat.getPath())) {
0:         if (stat.isDirectory()) {
0:           this.addInputPathRecursively(result, fs, stat.getPath(), inputFilter);
0:         } else {
0:           result.add(stat);
0:       }
0:   }
0: 
1:   /**
0:    * get data blocks of given btree
1:    */
0:   private List<DataRefNode> getDataBlocksOfIndex(AbstractIndex abstractIndex) {
0:     List<DataRefNode> blocks = new LinkedList<DataRefNode>();
0:     SegmentProperties segmentProperties = abstractIndex.getSegmentProperties();
0: 
0:     try {
0:       IndexKey startIndexKey = FilterUtil.prepareDefaultStartIndexKey(segmentProperties);
0:       IndexKey endIndexKey = FilterUtil.prepareDefaultEndIndexKey(segmentProperties);
0: 
0:       // Add all blocks of btree into result
0:       DataRefNodeFinder blockFinder =
0:           new BTreeDataRefNodeFinder(segmentProperties.getEachDimColumnValueSize());
0:       DataRefNode startBlock =
0:           blockFinder.findFirstDataBlock(abstractIndex.getDataRefNode(), startIndexKey);
0:       DataRefNode endBlock =
0:           blockFinder.findLastDataBlock(abstractIndex.getDataRefNode(), endIndexKey);
0:       while (startBlock != endBlock) {
0:         blocks.add(startBlock);
0:         startBlock = startBlock.getNextDataRefNode();
0:       }
0:       blocks.add(endBlock);
0: 
0:     } catch (KeyGenException e) {
0:       System.out.println("Could not generate start key" + e.getMessage());
0:     return blocks;
0:   }
0:   private boolean isSplitable() {
0:     try {
0:       // Don't split the file if it is local file system
0:       if (this.fileType == FileFactory.FileType.LOCAL) {
0:         return false;
0:       }
0:     } catch (Exception e) {
0:       return true;
0:     return true;
0:   }
0:   private long computeSplitSize(long blockSize, long minSize, long maxSize) {
0:     return Math.max(minSize, Math.min(maxSize, blockSize));
0:   }
0: 
0:   private FileSplit makeSplit(Path file, long start, long length, String[] hosts) {
0:     return new FileSplit(file, start, length, hosts);
0:   }
0: 
0:   private int getBlockIndex(BlockLocation[] blkLocations, long offset) {
0:     for (int i = 0; i < blkLocations.length; i++) {
0:       // is the offset inside this block?
0:       if ((blkLocations[i].getOffset() <= offset) && (offset
0:           < blkLocations[i].getOffset() + blkLocations[i].getLength())) {
0:         return i;
0:       }
0:     BlockLocation last = blkLocations[blkLocations.length - 1];
0:     long fileLength = last.getOffset() + last.getLength() - 1;
0:     throw new IllegalArgumentException("Offset " + offset +
0:         " is outside of file (0.." +
0:         fileLength + ")");
0:   }
1:   /**
0:    * get total number of rows. for count(*)
0:    *
0:    * @throws IOException
0:    * @throws IndexBuilderException
1:    */
0:   public long getRowCount() throws IOException, IndexBuilderException {
0:     long rowCount = 0;
/////////////////////////////////////////////////////////////////////////
0:     return rowCount;
0:   }
commit:32bf296
/////////////////////////////////////////////////////////////////////////
0:  * Licensed to the Apache Software Foundation (ASF) under one or more
0:  * contributor license agreements.  See the NOTICE file distributed with
0:  * this work for additional information regarding copyright ownership.
0:  * The ASF licenses this file to You under the Apache License, Version 2.0
0:  * (the "License"); you may not use this file except in compliance with
0:  * the License.  You may obtain a copy of the License at
0:  *    http://www.apache.org/licenses/LICENSE-2.0
/////////////////////////////////////////////////////////////////////////
0: package org.apache.carbondata.presto.impl;
commit:9d7dbea
author:Jacky Li
-------------------------------------------------------------------------------
commit:bf6c471
/////////////////////////////////////////////////////////////////////////
/////////////////////////////////////////////////////////////////////////
1:       if (isKeyExists
1:           && !FileFactory.isFileExist(
0:               CarbonTablePath.getSchemaFilePath(
0:                   carbonCache.get().get(schemaTableName).carbonTable.getTablePath()), fileType)) {
/////////////////////////////////////////////////////////////////////////
0:       if(ctcm != null && ctcm.carbonTable.getTableInfo() != null) {
0:         Long latestTime = FileFactory.getCarbonFile(
0:             CarbonTablePath.getSchemaFilePath(
0:                 carbonCache.get().get(schemaTableName).carbonTable.getTablePath())
0:         ).getLastModifiedTime();
0:         Long oldTime = ctcm.carbonTable.getTableInfo().getLastUpdatedTime();
/////////////////////////////////////////////////////////////////////////
1:       CarbonTableIdentifier carbonTableIdentifier =
1:       String tablePath = storePath + "/" + carbonTableIdentifier.getDatabaseName() + "/"
1:           + carbonTableIdentifier.getTableName();
/////////////////////////////////////////////////////////////////////////
0:               new ThriftReader(CarbonTablePath.getSchemaFilePath(tablePath), createTBase);
/////////////////////////////////////////////////////////////////////////
1:       cache.carbonTable = CarbonMetadata.getInstance().getCarbonTable(
1:           table.getSchemaName(), table.getTableName());
0: 
0:       // cache the table
0:       carbonCache.get().put(table, cache);
0: 
/////////////////////////////////////////////////////////////////////////
1:     TableInfo tableInfo = tableCacheModel.carbonTable.getTableInfo();
1:     String carbonTablePath = carbonTable.getAbsoluteTableIdentifier().getTablePath();
commit:2fe7758
/////////////////////////////////////////////////////////////////////////
0:           AbsoluteTableIdentifier.from(tablePath, cache.carbonTableIdentifier);
/////////////////////////////////////////////////////////////////////////
commit:214d9eb
/////////////////////////////////////////////////////////////////////////
0:       CarbonTableCacheModel cache = cc.get(table);
1:       if (cache == null) {
1:         cache = new CarbonTableCacheModel();
0:       }
commit:5fc7f06
/////////////////////////////////////////////////////////////////////////
1:     config.set(CarbonTableInputFormat.TABLE_NAME, carbonTable.getTableName());
author:xuchuanyin
-------------------------------------------------------------------------------
commit:910d496
/////////////////////////////////////////////////////////////////////////
1:     return loadTableMetadata(schemaTableName);
author:mohammadshahidkhan
-------------------------------------------------------------------------------
commit:1155d4d
/////////////////////////////////////////////////////////////////////////
0:       String storePath = config.getStorePath();
0:       String tablePath = storePath + "/" + cache.carbonTableIdentifier.getDatabaseName() + "/"
0:           + cache.carbonTableIdentifier.getTableName();
0: 
0: 
0:       AbsoluteTableIdentifier absoluteTableIdentifier =
0:           new AbsoluteTableIdentifier(tablePath, cache.carbonTableIdentifier);
0:           PathFactory.getInstance().getCarbonTablePath(absoluteTableIdentifier, null);
/////////////////////////////////////////////////////////////////////////
0:           .fromExternalToWrapperTableInfo(tableInfo, table.getSchemaName(), table.getTableName(),
1:               tablePath);
/////////////////////////////////////////////////////////////////////////
0:         .getCarbonTablePath(carbonTable.getAbsoluteTableIdentifier(), null).getPath();
1:     config.set(CarbonTableInputFormat.DATABASE_NAME, carbonTable.getDatabaseName());
0:     config.set(CarbonTableInputFormat.TABLE_NAME, carbonTable.getFactTableName());
author:ravipesala
-------------------------------------------------------------------------------
commit:1a62189
/////////////////////////////////////////////////////////////////////////
/////////////////////////////////////////////////////////////////////////
0:     DataMapStoreManager.getInstance().clearDataMaps(cc.get(table).carbonTable.getAbsoluteTableIdentifier());
commit:b681244
/////////////////////////////////////////////////////////////////////////
0: import java.io.IOException;
1: import java.util.ArrayList;
1: import java.util.Arrays;
0: import java.util.HashMap;
0: import java.util.Iterator;
0: import java.util.LinkedList;
1: import java.util.List;
0: import java.util.Map;
1: import java.util.Set;
1: import java.util.UUID;
0: import java.util.concurrent.ConcurrentHashMap;
0: import java.util.stream.Collectors;
0: import java.util.stream.Stream;
0: 
0: import org.apache.carbondata.core.datastore.DataRefNode;
0: import org.apache.carbondata.core.datastore.DataRefNodeFinder;
0: import org.apache.carbondata.core.datastore.IndexKey;
0: import org.apache.carbondata.core.datastore.SegmentTaskIndexStore;
0: import org.apache.carbondata.core.datastore.TableSegmentUniqueIdentifier;
0: import org.apache.carbondata.core.datastore.block.AbstractIndex;
0: import org.apache.carbondata.core.datastore.block.BlockletInfos;
0: import org.apache.carbondata.core.datastore.block.SegmentProperties;
0: import org.apache.carbondata.core.datastore.block.SegmentTaskIndexWrapper;
0: import org.apache.carbondata.core.datastore.block.TableBlockInfo;
/////////////////////////////////////////////////////////////////////////
0: 
0: import com.facebook.presto.spi.SchemaTableName;
0: import com.facebook.presto.spi.classloader.ThreadContextClassLoader;
0: import com.google.common.collect.ImmutableList;
0: import com.google.common.collect.ImmutableSet;
0: import com.google.inject.Inject;
0: import org.apache.hadoop.fs.BlockLocation;
0: import org.apache.hadoop.fs.FileStatus;
0: import org.apache.hadoop.fs.FileSystem;
0: import org.apache.hadoop.fs.LocatedFileStatus;
1: import org.apache.hadoop.fs.Path;
1: import org.apache.hadoop.fs.PathFilter;
0: import org.apache.hadoop.fs.RemoteIterator;
/////////////////////////////////////////////////////////////////////////
0:             if (CarbonUtil
0:                 .isInvalidTableBlock(tableBlockInfo.getSegmentId(), tableBlockInfo.getFilePath(),
0:                     invalidBlockVOForSegmentId, updateStatusManager)) {
author:Geetika Gupta
-------------------------------------------------------------------------------
commit:7036696
/////////////////////////////////////////////////////////////////////////
0:     tableList = new LinkedList<>();
/////////////////////////////////////////////////////////////////////////
1:       updateSchemaTables(table);
/////////////////////////////////////////////////////////////////////////
1:       updateSchemaTables(schemaTableName);
/////////////////////////////////////////////////////////////////////////
1:   private void updateSchemaTables(SchemaTableName schemaTableName) {
0:     if(!tableList.contains(schemaTableName)) {
0:       for (CarbonFile cf : carbonFileList.listFiles()) {
0:         if (!cf.getName().endsWith(".mdt")) {
0:           for (CarbonFile table : cf.listFiles()) {
0:             tableList.add(new SchemaTableName(cf.getName(), table.getName()));
0:           }
author:dhatchayani
-------------------------------------------------------------------------------
commit:d3a09e2
/////////////////////////////////////////////////////////////////////////
0: import org.apache.carbondata.core.scan.filter.SingleTableProvider;
0: import org.apache.carbondata.core.scan.filter.TableProvider;
/////////////////////////////////////////////////////////////////////////
0:       cache.carbonTablePath = PathFactory.getInstance()
0:           .getCarbonTablePath(storePath, cache.carbonTableIdentifier, null);
/////////////////////////////////////////////////////////////////////////
0:     TableProvider tableProvider = new SingleTableProvider(tableCacheModel.carbonTable);
0: 
0:         .resolveFilter(filters, tableCacheModel.carbonTable.getAbsoluteTableIdentifier(),
0:             tableProvider);
author:Raghunandan S
-------------------------------------------------------------------------------
commit:110f9b2
/////////////////////////////////////////////////////////////////////////
/////////////////////////////////////////////////////////////////////////
/////////////////////////////////////////////////////////////////////////
/////////////////////////////////////////////////////////////////////////
0:         if ((null == updateDetails) || (isValidBlockBasedOnUpdateDetails(
author:jatin
-------------------------------------------------------------------------------
commit:92d1d97
/////////////////////////////////////////////////////////////////////////
0: import com.facebook.presto.spi.TableNotFoundException;
/////////////////////////////////////////////////////////////////////////
1:     throw new TableNotFoundException(schemaTableName);
author:bianhq
-------------------------------------------------------------------------------
commit:b699ee6
/////////////////////////////////////////////////////////////////////////
0:  *
1:  * Currently, it is mainly used to parse metadata of tables under
1:  * the configured carbondata-store path and filter the relevant
1:  * input splits with given query predicates.
0: 
1:   /**
1:    * The names of the tables under the schema (this.carbonFileList).
1:    */
0: 
1:   /**
1:    * carbonFileList represents the store path of the schema, which is configured as carbondata-store
1:    * in the CarbonData catalog file ($PRESTO_HOME$/etc/catalog/carbondata.properties).
1:    */
1:   /**
1:    * A cache for Carbon reader, with this cache,
1:    * metadata of a table is only read from file system once.
1:    */
/////////////////////////////////////////////////////////////////////////
1:   /**
1:    * For presto worker node to initialize the metadata cache of a table.
1:    * @param table the name of the table and schema.
1:    * @return
1:    */
0:       // if this table is not cached, try to read the metadata of the table and cache it.
/////////////////////////////////////////////////////////////////////////
1:   /**
1:    * Return the schema names under a schema store path (this.carbonFileList).
1:    * @return
1:    */
0:   // default PathFilter, accepts files in carbondata format (with .carbondata extension).
1:   /**
0:    * Get the CarbonFile instance which represents the store path in the configuration, and assign it to
0:    * this.carbonFileList.
1:    * @return
1:    */
/////////////////////////////////////////////////////////////////////////
0:   /**
1:    * Return the schema names under a schema store path (this.carbonFileList).
1:    * @return
0:    */
/////////////////////////////////////////////////////////////////////////
0:   /**
1:    * Get the names of the tables in the given schema.
1:    * @param schema name of the schema
1:    * @return
0:    */
0:   /**
1:    * Get the names of the tables in the given schema.
1:    * @param schemaName name of the schema
1:    * @return
0:    */
0:   public Set<String> updateTableList(String schemaName) {
0:     List<CarbonFile> schema = Stream.of(carbonFileList.listFiles()).filter(a -> schemaName.equals(a.getName()))
/////////////////////////////////////////////////////////////////////////
0:   /**
1:    * Get the CarbonTable instance of the given table.
1:    * @param schemaTableName name of the given table.
1:    * @return
0:    */
/////////////////////////////////////////////////////////////////////////
0:   /**
1:    * Find all the tables under the schema store path (this.carbonFileList)
1:    * and cache all the table names in this.tableList. Notice that whenever this method
1:    * is called, it clears this.tableList and populate the list by reading the files.
0:    */
/////////////////////////////////////////////////////////////////////////
0:   /**
1:    * Find the table with the given name and build a CarbonTable instance for it.
1:    * This method should be called after this.updateSchemaTables().
1:    * @param schemaTableName name of the given table.
1:    * @return
0:    */
/////////////////////////////////////////////////////////////////////////
0:    * Read the metadata of the given table and cache it in this.cc (CarbonTableReader cache).
1:    * @param table name of the given table.
1:    * @return the CarbonTable instance which contains all the needed metadata for a table.
/////////////////////////////////////////////////////////////////////////
1:       // If table is not previously cached, then:
0: 
1:       // Step 1: get store path of the table and cache it.
0:         // create table identifier. the table id is randomly generated.
0:         // get the store path of the table.
0:         // cache the table
1:       //Step 2: read the metadata (tableInfo) of the table.
1:         // TBase is used to read and write thrift objects.
1:         // TableInfo is a kind of TBase used to read and write table information.
0:         // TableInfo is generated by thrift, see schema.thrift under format/src/main/thrift for details.
/////////////////////////////////////////////////////////////////////////
1:       // Step 3: convert format level TableInfo to code level TableInfo
0:         // wrapperTableInfo is the code level information of a table in carbondata core, different from the Thrift TableInfo.
0: 
1:       // Step 4: Load metadata info into CarbonMetadata
/////////////////////////////////////////////////////////////////////////
0:   /**
0:    * Apply filters to the table and get valid input splits of the table.
0:    * @param tableCacheModel the table
0:    * @param filters the filters
0:    * @return
0:    * @throws Exception
0:    */
/////////////////////////////////////////////////////////////////////////
0:    * Get all the data blocks of a given segment.
0:    * @param filterExpressionProcessor
0:    * @param absoluteTableIdentifier
0:    * @param tablePath
0:    * @param resolver
0:    * @param segmentId
0:    * @param cacheClient
0:    * @param updateStatusManager
0:    * @return
0:    * @throws IOException
/////////////////////////////////////////////////////////////////////////
0:   /**
0:    * Build and load the B-trees of the segment.
0:    * @param absoluteTableIdentifier
0:    * @param tablePath
0:    * @param segmentId
0:    * @param cacheClient
0:    * @param updateStatusManager
0:    * @return
0:    * @throws IOException
0:    */
/////////////////////////////////////////////////////////////////////////
0:   /**
0:    * Get the input splits of a set of carbondata files.
0:    * @param fileStatusList the file statuses of the set of carbondata files.
0:    * @param targetSystem hdfs FileSystem
0:    * @return
0:    * @throws IOException
0:    */
/////////////////////////////////////////////////////////////////////////
0:           // file is a carbondata file
/////////////////////////////////////////////////////////////////////////
0:                   (double) bytesRemaining / (double) splitSize > 1.1D;// when there are more than one splits left.
/////////////////////////////////////////////////////////////////////////
0:   /**
0:    * Get all file statuses of the carbondata files with a segmentId in segmentsToConsider
0:    * under the tablePath, and add them to the result.
0:    * @param segmentsToConsider
0:    * @param tablePath
0:    * @param result
0:    * @return the FileSystem instance been used in this function.
0:    * @throws IOException
0:    */
/////////////////////////////////////////////////////////////////////////
0:                 // DefaultFiler accepts carbondata files.
/////////////////////////////////////////////////////////////////////////
0:   /**
0:    * Get the FileStatus of all carbondata files under the path recursively,
0:    * and add the file statuses into the result
0:    * @param result
0:    * @param fs
0:    * @param path
0:    * @param inputFilter the filter used to determinate whether a path is a carbondata file
0:    * @throws IOException
0:    */
/////////////////////////////////////////////////////////////////////////
0:    * Get the data blocks of a b tree. the root node of the b tree is abstractIndex.dataRefNode.
0:    * BTreeNode is a sub class of DataRefNode.
0:    * @param abstractIndex
0:    * @return
author:sounakr
-------------------------------------------------------------------------------
commit:7656ad2
/////////////////////////////////////////////////////////////////////////
0:     UpdateVO invalidBlockVOForSegmentId = null;
0:     Boolean IUDTable = false;
/////////////////////////////////////////////////////////////////////////
0:     IUDTable = (updateStatusManager.getUpdateStatusDetails().length != 0);
0: 
0:       if (IUDTable) {
0:         // update not being performed on this table.
0:         invalidBlockVOForSegmentId =
0:             updateStatusManager.getInvalidTimestampRange(segmentNo);
0:       }
0: 
/////////////////////////////////////////////////////////////////////////
0:           if (IUDTable) {
0:             if (CarbonUtil.isInvalidTableBlock(tableBlockInfo, invalidBlockVOForSegmentId,
0:                 updateStatusManager)) {
0:               continue;
0:             }
/////////////////////////////////////////////////////////////////////////
0:     Long refreshedTime = segmentTaskIndexWrapper.getRefreshedTimeStamp();
0:     Long updateTime = updateDetails.getLatestUpdateTimestamp();
0:     if (null != refreshedTime && null != updateTime && updateTime > refreshedTime) {
/////////////////////////////////////////////////////////////////////////
0:     UpdateVO updateDetails = null;
0: 
0:     // Until Updates or Deletes being performed on the table Invalid Blocks will not
0:     // be formed. So it is unnecessary to get the InvalidTimeStampRange.
0:     if (updateStatusManager.getUpdateStatusDetails().length != 0) {
0:       updateDetails = updateStatusManager.getInvalidTimestampRange(segmentId);
0:     }
0: 
0:       // IUD operations should be performed on the table in order to mark the segment as Updated.
0:       // For Normal table no need to check for invalided blocks as there will be none of them.
0:       if ((null != updateDetails) && isSegmentUpdate(segmentTaskIndexWrapper, updateDetails)) {
/////////////////////////////////////////////////////////////////////////
0:         if ((null == updateDetails) || ((null != updateDetails) && isValidBlockBasedOnUpdateDetails(
0:             taskKeys, inputSplit, updateDetails, updateStatusManager, segmentId))) {
author:ffpeng90
-------------------------------------------------------------------------------
commit:4422c52
/////////////////////////////////////////////////////////////////////////
0: /** CarbonTableReader will be a facade of these utils
0:  *
0:  * 1:CarbonMetadata,(logic table)
0:  * 2:FileFactory, (physic table file)
0:  * 3:CarbonCommonFactory, (offer some )
0:  * 4:DictionaryFactory, (parse dictionary util)
0:  */
0:   // A cache for Carbon reader
/////////////////////////////////////////////////////////////////////////
0:   // for worker node to initialize carbon metastore
/////////////////////////////////////////////////////////////////////////
0:   // default PathFilter
/////////////////////////////////////////////////////////////////////////
0:     // update logic determine later
/////////////////////////////////////////////////////////////////////////
0:       // Step3: Transform Format Level TableInfo to Code Level TableInfo
0:       // Step4: Load metadata info into CarbonMetadata
/////////////////////////////////////////////////////////////////////////
0:     // need apply filters to segment
/////////////////////////////////////////////////////////////////////////
0:     // for each segment fetch blocks matching filter in Driver BTree
/////////////////////////////////////////////////////////////////////////
0:     // read segment index
/////////////////////////////////////////////////////////////////////////
0:         // if no filter is given, get all blocks from Btree Index
/////////////////////////////////////////////////////////////////////////
============================================================================