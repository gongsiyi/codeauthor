1:d7393da: /*
1:d7393da:  * Licensed to the Apache Software Foundation (ASF) under one or more
1:d7393da:  * contributor license agreements.  See the NOTICE file distributed with
1:d7393da:  * this work for additional information regarding copyright ownership.
1:d7393da:  * The ASF licenses this file to You under the Apache License, Version 2.0
1:d7393da:  * (the "License"); you may not use this file except in compliance with
1:d7393da:  * the License.  You may obtain a copy of the License at
1:d7393da:  *
1:d7393da:  *    http://www.apache.org/licenses/LICENSE-2.0
1:d7393da:  *
1:d7393da:  * Unless required by applicable law or agreed to in writing, software
1:d7393da:  * distributed under the License is distributed on an "AS IS" BASIS,
1:d7393da:  * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
1:d7393da:  * See the License for the specific language governing permissions and
1:d7393da:  * limitations under the License.
2:40c31e8:  */
4:d7393da: 
1:c723947: package org.apache.carbondata.streaming;
1:40c31e8: 
1:d7393da: import java.io.IOException;
1:d7393da: import java.nio.charset.Charset;
1:40c31e8: 
1:d7393da: import org.apache.carbondata.core.constants.CarbonCommonConstants;
1:2018048: import org.apache.carbondata.core.util.ObjectSerializationUtil;
1:d7393da: import org.apache.carbondata.processing.loading.model.CarbonLoadModel;
1:d7393da: 
1:d7393da: import org.apache.hadoop.conf.Configuration;
1:d7393da: import org.apache.hadoop.mapreduce.RecordWriter;
1:d7393da: import org.apache.hadoop.mapreduce.TaskAttemptContext;
1:d7393da: import org.apache.hadoop.mapreduce.lib.output.FileOutputFormat;
1:d7393da: 
3:40c31e8: /**
1:d7393da:  * Stream output format
1:40c31e8:  */
1:d7393da: public class CarbonStreamOutputFormat extends FileOutputFormat<Void, Object> {
1:d7393da: 
1:d7393da:   static final byte[] CARBON_SYNC_MARKER =
1:d7393da:       "@carbondata_sync".getBytes(Charset.forName(CarbonCommonConstants.DEFAULT_CHARSET));
1:d7393da: 
1:d7393da:   public static final String CARBON_ENCODER_ROW_BUFFER_SIZE = "carbon.stream.row.buffer.size";
1:d7393da: 
1:d7393da:   public static final int CARBON_ENCODER_ROW_BUFFER_SIZE_DEFAULT = 1024;
1:d7393da: 
1:d7393da:   public static final String CARBON_STREAM_BLOCKLET_ROW_NUMS = "carbon.stream.blocklet.row.nums";
1:d7393da: 
1:d7393da:   public static final int CARBON_STREAM_BLOCKLET_ROW_NUMS_DEFAULT = 32000;
1:d7393da: 
1:d7393da:   public static final String CARBON_STREAM_CACHE_SIZE = "carbon.stream.cache.size";
1:d7393da: 
1:d7393da:   public static final int CARBON_STREAM_CACHE_SIZE_DEFAULT = 32 * 1024 * 1024;
1:d7393da: 
1:d7393da:   private static final String LOAD_Model = "mapreduce.output.carbon.load.model";
1:40c31e8: 
1:40c31e8:   private static final String SEGMENT_ID = "carbon.segment.id";
1:40c31e8: 
1:d7393da:   @Override public RecordWriter<Void, Object> getRecordWriter(TaskAttemptContext job)
1:d7393da:       throws IOException, InterruptedException {
1:d7393da:     return new CarbonStreamRecordWriter(job);
1:d7393da:   }
1:d7393da: 
1:d7393da:   public static void setCarbonLoadModel(Configuration hadoopConf, CarbonLoadModel carbonLoadModel)
1:d7393da:       throws IOException {
1:d7393da:     if (carbonLoadModel != null) {
1:d7393da:       hadoopConf.set(LOAD_Model, ObjectSerializationUtil.convertObjectToString(carbonLoadModel));
1:d7393da:     }
1:d7393da:   }
1:d7393da: 
1:d7393da:   public static CarbonLoadModel getCarbonLoadModel(Configuration hadoopConf) throws IOException {
1:d7393da:     String value = hadoopConf.get(LOAD_Model);
1:d7393da:     if (value == null) {
1:d7393da:       return null;
1:d7393da:     } else {
1:d7393da:       return (CarbonLoadModel) ObjectSerializationUtil.convertStringToObject(value);
1:d7393da:     }
1:d7393da:   }
1:d7393da: 
1:40c31e8:   public static void setSegmentId(Configuration hadoopConf, String segmentId) throws IOException {
1:40c31e8:     if (segmentId != null) {
1:40c31e8:       hadoopConf.set(SEGMENT_ID, segmentId);
1:40c31e8:     }
1:40c31e8:   }
1:40c31e8: 
1:40c31e8:   public static String getSegmentId(Configuration hadoopConf) throws IOException {
1:40c31e8:     return hadoopConf.get(SEGMENT_ID);
1:40c31e8:   }
1:40c31e8: 
1:d7393da: }
============================================================================
author:akashrn5
-------------------------------------------------------------------------------
commit:2018048
/////////////////////////////////////////////////////////////////////////
1: import org.apache.carbondata.core.util.ObjectSerializationUtil;
author:Jacky Li
-------------------------------------------------------------------------------
commit:c723947
/////////////////////////////////////////////////////////////////////////
1: package org.apache.carbondata.streaming;
author:QiangCai
-------------------------------------------------------------------------------
commit:0f407de
/////////////////////////////////////////////////////////////////////////
commit:f8e0585
/////////////////////////////////////////////////////////////////////////
0:   public static final String HANDOFF_SIZE = "carbon.streaming.segment.max.size";
commit:40c31e8
/////////////////////////////////////////////////////////////////////////
1:   private static final String SEGMENT_ID = "carbon.segment.id";
1: 
1:   /**
0:    * if the byte size of streaming segment reach this value,
0:    * the system will create a new stream segment
1:    */
0:   public static final String HANDOFF_SIZE = "carbon.handoff.size";
1: 
1:   /**
0:    * the min handoff size of streaming segment, the unit is byte
1:    */
0:   public static final long HANDOFF_SIZE_MIN = 1024L * 1024 * 64;
1: 
1:   /**
0:    * the default handoff size of streaming segment, the unit is byte
1:    */
0:   public static final long HANDOFF_SIZE_DEFAULT = 1024L * 1024 * 1024;
1: 
/////////////////////////////////////////////////////////////////////////
1:   public static void setSegmentId(Configuration hadoopConf, String segmentId) throws IOException {
1:     if (segmentId != null) {
1:       hadoopConf.set(SEGMENT_ID, segmentId);
1:     }
1:   }
1: 
1:   public static String getSegmentId(Configuration hadoopConf) throws IOException {
1:     return hadoopConf.get(SEGMENT_ID);
1:   }
1: 
commit:d7393da
/////////////////////////////////////////////////////////////////////////
1: /*
1:  * Licensed to the Apache Software Foundation (ASF) under one or more
1:  * contributor license agreements.  See the NOTICE file distributed with
1:  * this work for additional information regarding copyright ownership.
1:  * The ASF licenses this file to You under the Apache License, Version 2.0
1:  * (the "License"); you may not use this file except in compliance with
1:  * the License.  You may obtain a copy of the License at
1:  *
1:  *    http://www.apache.org/licenses/LICENSE-2.0
1:  *
1:  * Unless required by applicable law or agreed to in writing, software
1:  * distributed under the License is distributed on an "AS IS" BASIS,
1:  * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
1:  * See the License for the specific language governing permissions and
1:  * limitations under the License.
0:  */
1: 
0: package org.apache.carbondata.hadoop.streaming;
1: 
1: import java.io.IOException;
1: import java.nio.charset.Charset;
1: 
1: import org.apache.carbondata.core.constants.CarbonCommonConstants;
0: import org.apache.carbondata.hadoop.util.ObjectSerializationUtil;
1: import org.apache.carbondata.processing.loading.model.CarbonLoadModel;
1: 
1: import org.apache.hadoop.conf.Configuration;
1: import org.apache.hadoop.mapreduce.RecordWriter;
1: import org.apache.hadoop.mapreduce.TaskAttemptContext;
1: import org.apache.hadoop.mapreduce.lib.output.FileOutputFormat;
1: 
0: /**
1:  * Stream output format
0:  */
1: public class CarbonStreamOutputFormat extends FileOutputFormat<Void, Object> {
1: 
1:   static final byte[] CARBON_SYNC_MARKER =
1:       "@carbondata_sync".getBytes(Charset.forName(CarbonCommonConstants.DEFAULT_CHARSET));
1: 
1:   public static final String CARBON_ENCODER_ROW_BUFFER_SIZE = "carbon.stream.row.buffer.size";
1: 
1:   public static final int CARBON_ENCODER_ROW_BUFFER_SIZE_DEFAULT = 1024;
1: 
1:   public static final String CARBON_STREAM_BLOCKLET_ROW_NUMS = "carbon.stream.blocklet.row.nums";
1: 
1:   public static final int CARBON_STREAM_BLOCKLET_ROW_NUMS_DEFAULT = 32000;
1: 
1:   public static final String CARBON_STREAM_CACHE_SIZE = "carbon.stream.cache.size";
1: 
1:   public static final int CARBON_STREAM_CACHE_SIZE_DEFAULT = 32 * 1024 * 1024;
1: 
1:   private static final String LOAD_Model = "mapreduce.output.carbon.load.model";
1: 
1:   @Override public RecordWriter<Void, Object> getRecordWriter(TaskAttemptContext job)
1:       throws IOException, InterruptedException {
1:     return new CarbonStreamRecordWriter(job);
1:   }
1: 
1:   public static void setCarbonLoadModel(Configuration hadoopConf, CarbonLoadModel carbonLoadModel)
1:       throws IOException {
1:     if (carbonLoadModel != null) {
1:       hadoopConf.set(LOAD_Model, ObjectSerializationUtil.convertObjectToString(carbonLoadModel));
1:     }
1:   }
1: 
1:   public static CarbonLoadModel getCarbonLoadModel(Configuration hadoopConf) throws IOException {
1:     String value = hadoopConf.get(LOAD_Model);
1:     if (value == null) {
1:       return null;
1:     } else {
1:       return (CarbonLoadModel) ObjectSerializationUtil.convertStringToObject(value);
1:     }
1:   }
1: 
1: }
============================================================================