1:f910cfa: /*
1:f910cfa:  * Licensed to the Apache Software Foundation (ASF) under one or more
1:f910cfa:  * contributor license agreements.  See the NOTICE file distributed with
1:f910cfa:  * this work for additional information regarding copyright ownership.
1:f910cfa:  * The ASF licenses this file to You under the Apache License, Version 2.0
1:f910cfa:  * (the "License"); you may not use this file except in compliance with
1:f910cfa:  * the License.  You may obtain a copy of the License at
1:f910cfa:  *
1:f910cfa:  *    http://www.apache.org/licenses/LICENSE-2.0
1:f910cfa:  *
1:f910cfa:  * Unless required by applicable law or agreed to in writing, software
1:f910cfa:  * distributed under the License is distributed on an "AS IS" BASIS,
1:f910cfa:  * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
1:f910cfa:  * See the License for the specific language governing permissions and
1:f910cfa:  * limitations under the License.
1:f910cfa:  */
1:f910cfa: 
1:f910cfa: package org.apache.carbondata.sdk.file;
1:f910cfa: 
1:4d3ecfb: import java.io.ByteArrayInputStream;
1:4d3ecfb: import java.io.ByteArrayOutputStream;
1:4d3ecfb: import java.io.DataInputStream;
1:f910cfa: import java.io.File;
1:f910cfa: import java.io.FileFilter;
1:f910cfa: import java.io.IOException;
1:4d3ecfb: import java.io.InputStream;
1:f910cfa: 
1:f910cfa: import org.apache.carbondata.common.exceptions.sql.InvalidLoadOptionException;
1:f910cfa: import org.apache.carbondata.core.constants.CarbonCommonConstants;
1:cf55028: import org.apache.carbondata.core.datastore.impl.FileFactory;
1:cf55028: import org.apache.carbondata.core.util.CarbonProperties;
1:f910cfa: import org.apache.carbondata.core.util.path.CarbonTablePath;
1:f910cfa: 
1:4d3ecfb: import org.apache.avro.file.DataFileWriter;
1:4d3ecfb: import org.apache.avro.generic.GenericData;
1:4d3ecfb: import org.apache.avro.generic.GenericDatumReader;
1:4d3ecfb: import org.apache.avro.generic.GenericDatumWriter;
1:4d3ecfb: import org.apache.avro.io.DecoderFactory;
1:4d3ecfb: import org.apache.avro.io.Encoder;
1:4d3ecfb: import org.apache.avro.io.JsonDecoder;
1:8f1a029: import org.apache.hadoop.conf.Configuration;
1:f910cfa: import org.junit.Assert;
1:f910cfa: 
1:cfb8ed9: public class TestUtil {
1:f910cfa: 
1:8f1a029:   public static Configuration configuration = new Configuration();
1:8f1a029: 
1:4d3ecfb:   public static GenericData.Record jsonToAvro(String json, String avroSchema) throws IOException {
1:4d3ecfb:     InputStream input = null;
1:4d3ecfb:     DataFileWriter writer = null;
1:4d3ecfb:     Encoder encoder = null;
1:4d3ecfb:     ByteArrayOutputStream output = null;
1:4d3ecfb:     try {
1:4d3ecfb:       org.apache.avro.Schema schema = new org.apache.avro.Schema.Parser().parse(avroSchema);
1:4d3ecfb:       GenericDatumReader reader = new GenericDatumReader (schema);
1:4d3ecfb:       input = new ByteArrayInputStream(json.getBytes());
1:4d3ecfb:       output = new ByteArrayOutputStream();
1:4d3ecfb:       DataInputStream din = new DataInputStream(input);
1:4d3ecfb:       writer = new DataFileWriter (new GenericDatumWriter ());
1:4d3ecfb:       writer.create(schema, output);
1:4d3ecfb:       JsonDecoder decoder = DecoderFactory.get().jsonDecoder(schema, din);
1:4d3ecfb:       GenericData.Record datum = null;
1:4d3ecfb:       datum = (GenericData.Record) reader.read(null, decoder);
1:4d3ecfb:       return datum;
1:4d3ecfb:     } finally {
1:4d3ecfb:       try {
1:4d3ecfb:         input.close();
1:4d3ecfb:         writer.close();
1:4d3ecfb:       } catch (Exception e) {
1:4d3ecfb:         e.printStackTrace();
1:4d3ecfb:       }
1:4d3ecfb:     }
1:4d3ecfb:   }
1:4d3ecfb: 
1:f910cfa:   static void writeFilesAndVerify(Schema schema, String path) {
1:f910cfa:     writeFilesAndVerify(schema, path, null);
1:f910cfa:   }
1:f910cfa: 
1:f910cfa:   static void writeFilesAndVerify(Schema schema, String path, String[] sortColumns) {
1:4b8dc0a:     writeFilesAndVerify(100, schema, path, sortColumns, false, -1, -1, true);
1:f910cfa:   }
1:f910cfa: 
1:92d9b92:   public static void writeFilesAndVerify(int rows, Schema schema, String path, boolean persistSchema) {
1:92d9b92:     writeFilesAndVerify(rows, schema, path, null, persistSchema, -1, -1, true);
1:f910cfa:   }
1:f910cfa: 
1:4b8dc0a:   public static void writeFilesAndVerify(Schema schema, String path, boolean persistSchema,
1:4b8dc0a:       boolean isTransactionalTable) {
1:4b8dc0a:     writeFilesAndVerify(100, schema, path, null, persistSchema, -1, -1, isTransactionalTable);
1:4b8dc0a:   }
1:4b8dc0a: 
1:f910cfa:   /**
1:290ef5a:    * write file and verify
1:290ef5a:    *
1:290ef5a:    * @param rows                 number of rows
1:290ef5a:    * @param schema               schema
1:290ef5a:    * @param path                 table store path
1:290ef5a:    * @param persistSchema        whether persist schema
1:290ef5a:    * @param isTransactionalTable whether is transactional table
1:290ef5a:    */
1:290ef5a:   public static void writeFilesAndVerify(int rows, Schema schema, String path, boolean persistSchema,
1:290ef5a:     boolean isTransactionalTable) {
1:290ef5a:     writeFilesAndVerify(rows, schema, path, null, persistSchema, -1, -1, isTransactionalTable);
1:290ef5a:   }
1:290ef5a: 
1:290ef5a:   /**
1:f910cfa:    * Invoke CarbonWriter API to write carbon files and assert the file is rewritten
1:f910cfa:    * @param rows number of rows to write
1:f910cfa:    * @param schema schema of the file
1:f910cfa:    * @param path local write path
1:f910cfa:    * @param sortColumns sort columns
1:f910cfa:    * @param persistSchema true if want to persist schema file
1:f910cfa:    * @param blockletSize blockletSize in the file, -1 for default size
1:f910cfa:    * @param blockSize blockSize in the file, -1 for default size
1:4b8dc0a:    * @param isTransactionalTable set to true if this is written for Transactional Table.
1:f910cfa:    */
1:526e3bf:   public static void writeFilesAndVerify(int rows, Schema schema, String path, String[] sortColumns,
1:4b8dc0a:       boolean persistSchema, int blockletSize, int blockSize, boolean isTransactionalTable) {
1:f910cfa:     try {
1:f910cfa:       CarbonWriterBuilder builder = CarbonWriter.builder()
1:4b8dc0a:           .isTransactionalTable(isTransactionalTable)
1:f910cfa:           .outputPath(path);
1:f910cfa:       if (sortColumns != null) {
1:f910cfa:         builder = builder.sortBy(sortColumns);
1:f910cfa:       }
1:f910cfa:       if (persistSchema) {
1:f910cfa:         builder = builder.persistSchemaFile(true);
1:f910cfa:       }
1:f910cfa:       if (blockletSize != -1) {
1:f910cfa:         builder = builder.withBlockletSize(blockletSize);
1:f910cfa:       }
1:f910cfa:       if (blockSize != -1) {
1:f910cfa:         builder = builder.withBlockSize(blockSize);
1:f910cfa:       }
1:f910cfa: 
1:8f1a029:       CarbonWriter writer = builder.buildWriterForCSVInput(schema, configuration);
1:f910cfa: 
1:f910cfa:       for (int i = 0; i < rows; i++) {
1:f910cfa:         writer.write(new String[]{"robot" + (i % 10), String.valueOf(i), String.valueOf((double) i / 2)});
1:f910cfa:       }
1:f910cfa:       writer.close();
1:f910cfa:     } catch (IOException e) {
1:f910cfa:       e.printStackTrace();
1:f910cfa:       Assert.fail(e.getMessage());
1:f910cfa:     } catch (InvalidLoadOptionException l) {
1:f910cfa:       l.printStackTrace();
1:f910cfa:       Assert.fail(l.getMessage());
1:f910cfa:     }
1:f910cfa: 
1:4b8dc0a:     File segmentFolder = null;
1:4b8dc0a:     if (isTransactionalTable) {
1:4b8dc0a:       segmentFolder = new File(CarbonTablePath.getSegmentPath(path, "null"));
1:f910cfa:       Assert.assertTrue(segmentFolder.exists());
1:4b8dc0a:     } else {
1:4b8dc0a:       segmentFolder = new File(path);
2:4b8dc0a:       Assert.assertTrue(segmentFolder.exists());
1:cf55028:     }
1:f910cfa: 
1:f910cfa:     File[] dataFiles = segmentFolder.listFiles(new FileFilter() {
1:f910cfa:       @Override public boolean accept(File pathname) {
1:f910cfa:         return pathname.getName().endsWith(CarbonCommonConstants.FACT_FILE_EXT);
1:f910cfa:       }
1:f910cfa:     });
1:f910cfa:     Assert.assertNotNull(dataFiles);
1:f910cfa:     Assert.assertTrue(dataFiles.length > 0);
1:f910cfa:   }
1:cf55028: 
1:cf55028:   /**
1:cf55028:    * verify whether the file exists
1:cf55028:    * if delete the file success or file not exists, then return true; otherwise return false
1:cf55028:    *
1:cf55028:    * @return boolean
1:cf55028:    */
1:cf55028:   public static boolean cleanMdtFile() {
1:cf55028:     String fileName = CarbonProperties.getInstance().getSystemFolderLocation()
1:cf55028:             + CarbonCommonConstants.FILE_SEPARATOR + "datamap.mdtfile";
1:cf55028:     try {
1:cf55028:       if (FileFactory.isFileExist(fileName)) {
1:cf55028:         File file = new File(fileName);
1:cf55028:         file.delete();
1:cf55028:         return true;
1:cf55028:       } else {
1:cf55028:         return true;
1:f910cfa:       }
1:cf55028:     } catch (IOException e) {
1:cf55028:       e.printStackTrace();
1:cf55028:       return false;
1:cf55028:     }
1:cf55028:   }
1:cf55028: 
1:cf55028:   /**
1:cf55028:    * verify whether the mdt file exists
1:cf55028:    * if the file exists, then return true; otherwise return false
1:cf55028:    *
1:cf55028:    * @return boolean
1:cf55028:    */
1:cf55028:   public static boolean verifyMdtFile() {
1:cf55028:     String fileName = CarbonProperties.getInstance().getSystemFolderLocation()
1:cf55028:             + CarbonCommonConstants.FILE_SEPARATOR + "datamap.mdtfile";
1:cf55028:     try {
1:cf55028:       if (FileFactory.isFileExist(fileName)) {
1:cf55028:         return true;
1:cf55028:       }
1:cf55028:       return false;
1:cf55028:     } catch (IOException e) {
1:cf55028:       throw new RuntimeException("IO exception:", e);
1:cf55028:     }
1:cf55028:   }
1:4b8dc0a: }
============================================================================
author:kunal642
-------------------------------------------------------------------------------
commit:8f1a029
/////////////////////////////////////////////////////////////////////////
1: import org.apache.hadoop.conf.Configuration;
1:   public static Configuration configuration = new Configuration();
1: 
/////////////////////////////////////////////////////////////////////////
1:       CarbonWriter writer = builder.buildWriterForCSVInput(schema, configuration);
commit:26eb2d0
/////////////////////////////////////////////////////////////////////////
/////////////////////////////////////////////////////////////////////////
0:       CarbonWriter writer = builder.buildWriterForCSVInput(schema);
author:Jacky Li
-------------------------------------------------------------------------------
commit:526e3bf
/////////////////////////////////////////////////////////////////////////
1:   public static void writeFilesAndVerify(int rows, Schema schema, String path, String[] sortColumns,
commit:cfb8ed9
/////////////////////////////////////////////////////////////////////////
1: public class TestUtil {
/////////////////////////////////////////////////////////////////////////
0:   public static void writeFilesAndVerify(Schema schema, String path, boolean persistSchema) {
commit:f910cfa
/////////////////////////////////////////////////////////////////////////
1: /*
1:  * Licensed to the Apache Software Foundation (ASF) under one or more
1:  * contributor license agreements.  See the NOTICE file distributed with
1:  * this work for additional information regarding copyright ownership.
1:  * The ASF licenses this file to You under the Apache License, Version 2.0
1:  * (the "License"); you may not use this file except in compliance with
1:  * the License.  You may obtain a copy of the License at
1:  *
1:  *    http://www.apache.org/licenses/LICENSE-2.0
1:  *
1:  * Unless required by applicable law or agreed to in writing, software
1:  * distributed under the License is distributed on an "AS IS" BASIS,
1:  * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
1:  * See the License for the specific language governing permissions and
1:  * limitations under the License.
1:  */
1: 
1: package org.apache.carbondata.sdk.file;
1: 
1: import java.io.File;
1: import java.io.FileFilter;
1: import java.io.IOException;
1: 
1: import org.apache.carbondata.common.exceptions.sql.InvalidLoadOptionException;
1: import org.apache.carbondata.core.constants.CarbonCommonConstants;
1: import org.apache.carbondata.core.util.path.CarbonTablePath;
1: 
1: import org.junit.Assert;
1: 
0: class TestUtil {
1: 
1:   static void writeFilesAndVerify(Schema schema, String path) {
1:     writeFilesAndVerify(schema, path, null);
1:   }
1: 
1:   static void writeFilesAndVerify(Schema schema, String path, String[] sortColumns) {
0:     writeFilesAndVerify(100, schema, path, sortColumns, false, -1, -1);
1:   }
1: 
0:   static void writeFilesAndVerify(Schema schema, String path, boolean persistSchema) {
0:     writeFilesAndVerify(100, schema, path, null, persistSchema, -1, -1);
1:   }
1: 
1:   /**
1:    * Invoke CarbonWriter API to write carbon files and assert the file is rewritten
1:    * @param rows number of rows to write
1:    * @param schema schema of the file
1:    * @param path local write path
1:    * @param sortColumns sort columns
1:    * @param persistSchema true if want to persist schema file
1:    * @param blockletSize blockletSize in the file, -1 for default size
1:    * @param blockSize blockSize in the file, -1 for default size
1:    */
0:   static void writeFilesAndVerify(int rows, Schema schema, String path, String[] sortColumns,
0:       boolean persistSchema, int blockletSize, int blockSize) {
1:     try {
1:       CarbonWriterBuilder builder = CarbonWriter.builder()
0:           .withSchema(schema)
1:           .outputPath(path);
1:       if (sortColumns != null) {
1:         builder = builder.sortBy(sortColumns);
1:       }
1:       if (persistSchema) {
1:         builder = builder.persistSchemaFile(true);
1:       }
1:       if (blockletSize != -1) {
1:         builder = builder.withBlockletSize(blockletSize);
1:       }
1:       if (blockSize != -1) {
1:         builder = builder.withBlockSize(blockSize);
1:       }
1: 
0:       CarbonWriter writer = builder.buildWriterForCSVInput();
1: 
1:       for (int i = 0; i < rows; i++) {
1:         writer.write(new String[]{"robot" + (i % 10), String.valueOf(i), String.valueOf((double) i / 2)});
1:       }
1:       writer.close();
1:     } catch (IOException e) {
1:       e.printStackTrace();
1:       Assert.fail(e.getMessage());
1:     } catch (InvalidLoadOptionException l) {
1:       l.printStackTrace();
1:       Assert.fail(l.getMessage());
1:     }
1: 
0:     File segmentFolder = new File(CarbonTablePath.getSegmentPath(path, "null"));
1:     Assert.assertTrue(segmentFolder.exists());
1: 
1:     File[] dataFiles = segmentFolder.listFiles(new FileFilter() {
1:       @Override public boolean accept(File pathname) {
1:         return pathname.getName().endsWith(CarbonCommonConstants.FACT_FILE_EXT);
1:       }
1:     });
1:     Assert.assertNotNull(dataFiles);
1:     Assert.assertTrue(dataFiles.length > 0);
1:   }
1: }
author:rahul
-------------------------------------------------------------------------------
commit:4d3ecfb
/////////////////////////////////////////////////////////////////////////
1: import java.io.ByteArrayInputStream;
1: import java.io.ByteArrayOutputStream;
1: import java.io.DataInputStream;
1: import java.io.InputStream;
/////////////////////////////////////////////////////////////////////////
1: import org.apache.avro.file.DataFileWriter;
1: import org.apache.avro.generic.GenericData;
1: import org.apache.avro.generic.GenericDatumReader;
1: import org.apache.avro.generic.GenericDatumWriter;
1: import org.apache.avro.io.DecoderFactory;
1: import org.apache.avro.io.Encoder;
1: import org.apache.avro.io.JsonDecoder;
1:   public static GenericData.Record jsonToAvro(String json, String avroSchema) throws IOException {
1:     InputStream input = null;
1:     DataFileWriter writer = null;
1:     Encoder encoder = null;
1:     ByteArrayOutputStream output = null;
1:     try {
1:       org.apache.avro.Schema schema = new org.apache.avro.Schema.Parser().parse(avroSchema);
1:       GenericDatumReader reader = new GenericDatumReader (schema);
1:       input = new ByteArrayInputStream(json.getBytes());
1:       output = new ByteArrayOutputStream();
1:       DataInputStream din = new DataInputStream(input);
1:       writer = new DataFileWriter (new GenericDatumWriter ());
1:       writer.create(schema, output);
1:       JsonDecoder decoder = DecoderFactory.get().jsonDecoder(schema, din);
1:       GenericData.Record datum = null;
1:       datum = (GenericData.Record) reader.read(null, decoder);
1:       return datum;
1:     } finally {
1:       try {
1:         input.close();
1:         writer.close();
1:       } catch (Exception e) {
1:         e.printStackTrace();
1:       }
1:     }
1:   }
1: 
author:xubo245
-------------------------------------------------------------------------------
commit:290ef5a
/////////////////////////////////////////////////////////////////////////
1:    * write file and verify
1:    *
1:    * @param rows                 number of rows
1:    * @param schema               schema
1:    * @param path                 table store path
1:    * @param persistSchema        whether persist schema
1:    * @param isTransactionalTable whether is transactional table
1:    */
1:   public static void writeFilesAndVerify(int rows, Schema schema, String path, boolean persistSchema,
1:     boolean isTransactionalTable) {
1:     writeFilesAndVerify(rows, schema, path, null, persistSchema, -1, -1, isTransactionalTable);
1:   }
1: 
1:   /**
commit:cf55028
/////////////////////////////////////////////////////////////////////////
1: import org.apache.carbondata.core.datastore.impl.FileFactory;
1: import org.apache.carbondata.core.util.CarbonProperties;
/////////////////////////////////////////////////////////////////////////
1: 
1:   /**
1:    * verify whether the file exists
1:    * if delete the file success or file not exists, then return true; otherwise return false
1:    *
1:    * @return boolean
1:    */
1:   public static boolean cleanMdtFile() {
1:     String fileName = CarbonProperties.getInstance().getSystemFolderLocation()
1:             + CarbonCommonConstants.FILE_SEPARATOR + "datamap.mdtfile";
1:     try {
1:       if (FileFactory.isFileExist(fileName)) {
1:         File file = new File(fileName);
1:         file.delete();
1:         return true;
1:       } else {
1:         return true;
1:       }
1:     } catch (IOException e) {
1:       e.printStackTrace();
1:       return false;
1:     }
1:   }
1: 
1:   /**
1:    * verify whether the mdt file exists
1:    * if the file exists, then return true; otherwise return false
1:    *
1:    * @return boolean
1:    */
1:   public static boolean verifyMdtFile() {
1:     String fileName = CarbonProperties.getInstance().getSystemFolderLocation()
1:             + CarbonCommonConstants.FILE_SEPARATOR + "datamap.mdtfile";
1:     try {
1:       if (FileFactory.isFileExist(fileName)) {
1:         return true;
1:       }
1:       return false;
1:     } catch (IOException e) {
1:       throw new RuntimeException("IO exception:", e);
1:     }
1:   }
author:rahulforallp
-------------------------------------------------------------------------------
commit:92d9b92
/////////////////////////////////////////////////////////////////////////
1:   public static void writeFilesAndVerify(int rows, Schema schema, String path, boolean persistSchema) {
1:     writeFilesAndVerify(rows, schema, path, null, persistSchema, -1, -1, true);
author:sounakr
-------------------------------------------------------------------------------
commit:4b8dc0a
/////////////////////////////////////////////////////////////////////////
1:     writeFilesAndVerify(100, schema, path, sortColumns, false, -1, -1, true);
0:     writeFilesAndVerify(100, schema, path, null, persistSchema, -1, -1, true);
1:   }
1: 
1:   public static void writeFilesAndVerify(Schema schema, String path, boolean persistSchema,
1:       boolean isTransactionalTable) {
1:     writeFilesAndVerify(100, schema, path, null, persistSchema, -1, -1, isTransactionalTable);
/////////////////////////////////////////////////////////////////////////
1:    * @param isTransactionalTable set to true if this is written for Transactional Table.
1:       boolean persistSchema, int blockletSize, int blockSize, boolean isTransactionalTable) {
1:           .isTransactionalTable(isTransactionalTable)
/////////////////////////////////////////////////////////////////////////
1:     File segmentFolder = null;
1:     if (isTransactionalTable) {
1:       segmentFolder = new File(CarbonTablePath.getSegmentPath(path, "null"));
1:       Assert.assertTrue(segmentFolder.exists());
1:     } else {
1:       segmentFolder = new File(path);
1:       Assert.assertTrue(segmentFolder.exists());
1:     }
commit:b7b8073
/////////////////////////////////////////////////////////////////////////
0:           .isTransactionalTable(true)
============================================================================