1:e6a4f64: /*
1:e6a4f64:  * Licensed to the Apache Software Foundation (ASF) under one or more
1:e6a4f64:  * contributor license agreements.  See the NOTICE file distributed with
1:e6a4f64:  * this work for additional information regarding copyright ownership.
1:e6a4f64:  * The ASF licenses this file to You under the Apache License, Version 2.0
1:e6a4f64:  * (the "License"); you may not use this file except in compliance with
1:e6a4f64:  * the License.  You may obtain a copy of the License at
1:e6a4f64:  *
1:e6a4f64:  *    http://www.apache.org/licenses/LICENSE-2.0
1:e6a4f64:  *
1:e6a4f64:  * Unless required by applicable law or agreed to in writing, software
1:e6a4f64:  * distributed under the License is distributed on an "AS IS" BASIS,
1:e6a4f64:  * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
1:e6a4f64:  * See the License for the specific language governing permissions and
1:e6a4f64:  * limitations under the License.
1:e6a4f64:  */
1:e6a4f64: 
1:e6a4f64: package org.apache.carbondata.core.datastore.page.encoding;
1:e6a4f64: 
1:e6a4f64: import java.io.ByteArrayOutputStream;
1:e6a4f64: import java.io.DataOutputStream;
1:e6a4f64: import java.io.IOException;
1:e6a4f64: import java.nio.ByteBuffer;
1:e6a4f64: import java.util.ArrayList;
1:e6a4f64: import java.util.List;
1:e6a4f64: 
1:438b442: import org.apache.carbondata.common.logging.LogService;
1:438b442: import org.apache.carbondata.common.logging.LogServiceFactory;
1:438b442: import org.apache.carbondata.core.datastore.ColumnType;
1:438b442: import org.apache.carbondata.core.datastore.TableSpec;
1:e6a4f64: import org.apache.carbondata.core.datastore.compression.Compressor;
1:e6a4f64: import org.apache.carbondata.core.datastore.compression.CompressorFactory;
1:e6a4f64: import org.apache.carbondata.core.datastore.page.ColumnPage;
1:e6a4f64: import org.apache.carbondata.core.datastore.page.ComplexColumnPage;
1:8c1ddbf: import org.apache.carbondata.core.datastore.page.encoding.compress.DirectCompressCodec;
1:e6a4f64: import org.apache.carbondata.core.memory.MemoryException;
1:956833e: import org.apache.carbondata.core.metadata.datatype.DataTypes;
1:e6a4f64: import org.apache.carbondata.core.util.CarbonMetadataUtil;
1:e6a4f64: import org.apache.carbondata.core.util.CarbonUtil;
1:e6a4f64: import org.apache.carbondata.format.BlockletMinMaxIndex;
1:e6a4f64: import org.apache.carbondata.format.DataChunk2;
1:e6a4f64: import org.apache.carbondata.format.Encoding;
1:e710339: import org.apache.carbondata.format.LocalDictionaryChunk;
1:e710339: import org.apache.carbondata.format.LocalDictionaryChunkMeta;
1:e6a4f64: import org.apache.carbondata.format.PresenceMeta;
1:e6a4f64: 
1:438b442: import static org.apache.carbondata.core.datastore.page.encoding.DefaultEncodingFactory.selectCodecByAlgorithmForFloating;
1:438b442: import static org.apache.carbondata.core.datastore.page.encoding.DefaultEncodingFactory.selectCodecByAlgorithmForIntegral;
1:438b442: 
1:e6a4f64: public abstract class ColumnPageEncoder {
1:e6a4f64: 
1:438b442:   /**
1:438b442:    * logger
1:438b442:    */
1:438b442:   private static final LogService LOGGER =
1:438b442:       LogServiceFactory.getLogService(ColumnPageEncoder.class.getName());
1:438b442: 
1:e6a4f64:   protected abstract byte[] encodeData(ColumnPage input) throws MemoryException, IOException;
1:e6a4f64: 
1:e6a4f64:   protected abstract List<Encoding> getEncodingList();
1:e6a4f64: 
1:e6a4f64:   protected abstract ColumnPageEncoderMeta getEncoderMeta(ColumnPage inputPage);
1:e6a4f64: 
1:e6a4f64:   /**
1:e6a4f64:    * Return a encoded column page by encoding the input page
1:e6a4f64:    * The encoded binary data and metadata are wrapped in encoding column page
1:e6a4f64:    */
1:e6a4f64:   public EncodedColumnPage encode(ColumnPage inputPage) throws IOException, MemoryException {
1:e6a4f64:     byte[] encodedBytes = encodeData(inputPage);
1:e6a4f64:     DataChunk2 pageMetadata = buildPageMetadata(inputPage, encodedBytes);
1:e710339:     return new EncodedColumnPage(pageMetadata, encodedBytes, inputPage);
1:e6a4f64:   }
1:e6a4f64: 
1:e6a4f64:   private DataChunk2 buildPageMetadata(ColumnPage inputPage, byte[] encodedBytes)
1:e6a4f64:       throws IOException {
1:e6a4f64:     DataChunk2 dataChunk = new DataChunk2();
1:e6a4f64:     dataChunk.setData_page_length(encodedBytes.length);
1:e6a4f64:     fillBasicFields(inputPage, dataChunk);
1:e6a4f64:     fillNullBitSet(inputPage, dataChunk);
1:e6a4f64:     fillEncoding(inputPage, dataChunk);
1:e6a4f64:     fillMinMaxIndex(inputPage, dataChunk);
1:e6a4f64:     fillLegacyFields(dataChunk);
1:e6a4f64:     return dataChunk;
1:e6a4f64:   }
1:e6a4f64: 
1:e6a4f64:   private void fillBasicFields(ColumnPage inputPage, DataChunk2 dataChunk) {
1:8f08c4a:     dataChunk.setChunk_meta(
1:8f08c4a:         CarbonMetadataUtil.getChunkCompressorMeta(inputPage.getColumnCompressorName()));
1:e6a4f64:     dataChunk.setNumberOfRowsInpage(inputPage.getPageSize());
1:e6a4f64:     dataChunk.setRowMajor(false);
1:e6a4f64:   }
1:e6a4f64: 
1:e6a4f64:   private void fillNullBitSet(ColumnPage inputPage, DataChunk2 dataChunk) {
1:e6a4f64:     PresenceMeta presenceMeta = new PresenceMeta();
1:e6a4f64:     presenceMeta.setPresent_bit_streamIsSet(true);
1:8f08c4a:     Compressor compressor = CompressorFactory.getInstance().getCompressor(
1:8f08c4a:         inputPage.getColumnCompressorName());
1:e6a4f64:     presenceMeta.setPresent_bit_stream(
1:e6a4f64:         compressor.compressByte(inputPage.getNullBits().toByteArray()));
1:e6a4f64:     dataChunk.setPresence(presenceMeta);
1:e6a4f64:   }
1:e6a4f64: 
1:e6a4f64:   private void fillEncoding(ColumnPage inputPage, DataChunk2 dataChunk) throws IOException {
1:e6a4f64:     dataChunk.setEncoders(getEncodingList());
1:e6a4f64:     dataChunk.setEncoder_meta(buildEncoderMeta(inputPage));
1:e6a4f64:   }
1:e6a4f64: 
1:e6a4f64:   private List<ByteBuffer> buildEncoderMeta(ColumnPage inputPage) throws IOException {
1:e6a4f64:     ColumnPageEncoderMeta meta = getEncoderMeta(inputPage);
1:e6a4f64:     List<ByteBuffer> metaDatas = new ArrayList<>();
1:e6a4f64:     if (meta != null) {
1:e6a4f64:       ByteArrayOutputStream stream = new ByteArrayOutputStream();
1:e6a4f64:       DataOutputStream out = new DataOutputStream(stream);
1:e6a4f64:       meta.write(out);
1:e6a4f64:       metaDatas.add(ByteBuffer.wrap(stream.toByteArray()));
1:e6a4f64:     }
1:e6a4f64:     return metaDatas;
1:e6a4f64:   }
1:e6a4f64: 
1:e6a4f64:   private void fillMinMaxIndex(ColumnPage inputPage, DataChunk2 dataChunk) {
1:e6a4f64:     dataChunk.setMin_max(buildMinMaxIndex(inputPage));
1:e6a4f64:   }
1:e6a4f64: 
1:e6a4f64:   private BlockletMinMaxIndex buildMinMaxIndex(ColumnPage inputPage) {
1:e6a4f64:     BlockletMinMaxIndex index = new BlockletMinMaxIndex();
1:e6a4f64:     byte[] bytes = CarbonUtil.getValueAsBytes(
1:e6a4f64:         inputPage.getDataType(), inputPage.getStatistics().getMax());
1:e6a4f64:     ByteBuffer max = ByteBuffer.wrap(
1:e6a4f64:         bytes);
1:e6a4f64:     ByteBuffer min = ByteBuffer.wrap(
1:e6a4f64:         CarbonUtil.getValueAsBytes(inputPage.getDataType(), inputPage.getStatistics().getMin()));
1:e6a4f64:     index.addToMax_values(max);
1:e6a4f64:     index.addToMin_values(min);
1:e6a4f64:     return index;
1:e6a4f64:   }
1:e6a4f64: 
1:e6a4f64:   /**
1:e6a4f64:    * `buildPageMetadata` will call this for backward compatibility
1:e6a4f64:    */
1:e6a4f64:   protected void fillLegacyFields(DataChunk2 dataChunk)
1:e6a4f64:       throws IOException {
1:e6a4f64:     // Subclass should override this to update datachunk2 if any backward compatibility if required,
1:e6a4f64:     // For example, when using IndexStorageCodec, rle_page_length and rowid_page_length need to be
1:e6a4f64:     // updated
1:e6a4f64:   }
1:e6a4f64: 
1:e6a4f64:   /**
1:e6a4f64:    * Apply encoding algorithm for complex column page and return the coded data
1:e6a4f64:    * TODO: remove this interface after complex column page is unified with column page
1:e6a4f64:    */
1:e6a4f64:   public static EncodedColumnPage[] encodeComplexColumn(ComplexColumnPage input)
1:e6a4f64:       throws IOException, MemoryException {
1:438b442:     EncodedColumnPage[] encodedPages = new EncodedColumnPage[input.getComplexColumnIndex()];
1:e6a4f64:     int index = 0;
1:438b442:     while (index < input.getComplexColumnIndex()) {
1:e710339:       ColumnPage subColumnPage = input.getColumnPage(index);
1:e710339:       encodedPages[index] = encodedColumn(subColumnPage);
1:6297ea0:       index++;
1:e6a4f64:     }
1:e6a4f64:     return encodedPages;
1:e6a4f64:   }
1:e6a4f64: 
1:e710339:   public static EncodedColumnPage encodedColumn(ColumnPage page)
1:e6a4f64:       throws IOException, MemoryException {
1:438b442:     ColumnPageEncoder pageEncoder = createCodecForDimension(page);
1:438b442:     if (pageEncoder == null) {
1:438b442:       ColumnPageEncoder encoder = new DirectCompressCodec(DataTypes.BYTE_ARRAY).createEncoder(null);
1:438b442:       return encoder.encode(page);
1:438b442:     } else {
1:438b442:       LOGGER.debug("Encoder result ---> Source data type: " + pageEncoder.getEncoderMeta(page)
1:438b442:           .getColumnSpec().getSchemaDataType() + " Destination data type: " + pageEncoder
1:438b442:           .getEncoderMeta(page).getStoreDataType() + " for the column: " + pageEncoder
1:438b442:           .getEncoderMeta(page).getColumnSpec().getFieldName());
1:438b442: 
1:438b442:       return pageEncoder.encode(page);
1:438b442:     }
1:e6a4f64:   }
1:e6a4f64: 
1:438b442:   private static ColumnPageEncoder createCodecForDimension(ColumnPage inputPage) {
1:438b442:     TableSpec.ColumnSpec columnSpec = inputPage.getColumnSpec();
1:438b442:     if (columnSpec.getColumnType() == ColumnType.COMPLEX_PRIMITIVE) {
1:438b442:       if (inputPage.getDataType() == DataTypes.BYTE_ARRAY
1:438b442:           || inputPage.getDataType() == DataTypes.STRING) {
1:438b442:         // use legacy encoder
1:438b442:         return null;
1:438b442:       } else if ((inputPage.getDataType() == DataTypes.BYTE) || (inputPage.getDataType()
1:438b442:           == DataTypes.SHORT) || (inputPage.getDataType() == DataTypes.INT) || (
1:438b442:           inputPage.getDataType() == DataTypes.LONG)) {
1:438b442:         return selectCodecByAlgorithmForIntegral(inputPage.getStatistics(), true)
1:438b442:             .createEncoder(null);
1:438b442:       } else if ((inputPage.getDataType() == DataTypes.FLOAT) || (inputPage.getDataType()
1:438b442:           == DataTypes.DOUBLE)) {
1:438b442:         return selectCodecByAlgorithmForFloating(inputPage.getStatistics(), true)
1:438b442:             .createEncoder(null);
1:438b442:       }
1:438b442:     }
1:438b442:     // use legacy encoder
1:438b442:     return null;
1:438b442:   }
1:438b442: 
1:438b442: 
1:e710339:   /**
1:e710339:    * Below method to encode the dictionary page
1:e710339:    * @param dictionaryPage
1:e710339:    * dictionary column page
1:e710339:    * @return local dictionary chunk
1:e710339:    * @throws IOException
1:e710339:    * Problem in encoding
1:e710339:    * @throws MemoryException
1:e710339:    * problem in encoding
1:e710339:    */
1:e710339:   public LocalDictionaryChunk encodeDictionary(ColumnPage dictionaryPage)
1:e710339:       throws IOException, MemoryException {
1:e710339:     LocalDictionaryChunk localDictionaryChunk = new LocalDictionaryChunk();
1:e710339:     localDictionaryChunk.setDictionary_data(encodeData(dictionaryPage));
1:e710339:     LocalDictionaryChunkMeta localDictionaryChunkMeta = new LocalDictionaryChunkMeta();
1:e710339:     localDictionaryChunkMeta.setEncoders(getEncodingList());
1:e710339:     localDictionaryChunkMeta.setEncoder_meta(buildEncoderMeta(dictionaryPage));
1:e710339:     localDictionaryChunk.setDictionary_meta(localDictionaryChunkMeta);
1:e710339:     return localDictionaryChunk;
1:e710339:   }
1:e710339: 
1:e6a4f64: }
============================================================================
author:xuchuanyin
-------------------------------------------------------------------------------
commit:8f08c4a
/////////////////////////////////////////////////////////////////////////
1:     dataChunk.setChunk_meta(
1:         CarbonMetadataUtil.getChunkCompressorMeta(inputPage.getColumnCompressorName()));
/////////////////////////////////////////////////////////////////////////
1:     Compressor compressor = CompressorFactory.getInstance().getCompressor(
1:         inputPage.getColumnCompressorName());
author:ajantha-bhat
-------------------------------------------------------------------------------
commit:438b442
/////////////////////////////////////////////////////////////////////////
1: import org.apache.carbondata.common.logging.LogService;
1: import org.apache.carbondata.common.logging.LogServiceFactory;
1: import org.apache.carbondata.core.datastore.ColumnType;
1: import org.apache.carbondata.core.datastore.TableSpec;
/////////////////////////////////////////////////////////////////////////
1: import static org.apache.carbondata.core.datastore.page.encoding.DefaultEncodingFactory.selectCodecByAlgorithmForFloating;
1: import static org.apache.carbondata.core.datastore.page.encoding.DefaultEncodingFactory.selectCodecByAlgorithmForIntegral;
1: 
1:   /**
1:    * logger
1:    */
1:   private static final LogService LOGGER =
1:       LogServiceFactory.getLogService(ColumnPageEncoder.class.getName());
1: 
/////////////////////////////////////////////////////////////////////////
1:     EncodedColumnPage[] encodedPages = new EncodedColumnPage[input.getComplexColumnIndex()];
1:     while (index < input.getComplexColumnIndex()) {
/////////////////////////////////////////////////////////////////////////
1:     ColumnPageEncoder pageEncoder = createCodecForDimension(page);
1:     if (pageEncoder == null) {
1:       ColumnPageEncoder encoder = new DirectCompressCodec(DataTypes.BYTE_ARRAY).createEncoder(null);
1:       return encoder.encode(page);
1:     } else {
1:       LOGGER.debug("Encoder result ---> Source data type: " + pageEncoder.getEncoderMeta(page)
1:           .getColumnSpec().getSchemaDataType() + " Destination data type: " + pageEncoder
1:           .getEncoderMeta(page).getStoreDataType() + " for the column: " + pageEncoder
1:           .getEncoderMeta(page).getColumnSpec().getFieldName());
1: 
1:       return pageEncoder.encode(page);
1:     }
1:   private static ColumnPageEncoder createCodecForDimension(ColumnPage inputPage) {
1:     TableSpec.ColumnSpec columnSpec = inputPage.getColumnSpec();
1:     if (columnSpec.getColumnType() == ColumnType.COMPLEX_PRIMITIVE) {
1:       if (inputPage.getDataType() == DataTypes.BYTE_ARRAY
1:           || inputPage.getDataType() == DataTypes.STRING) {
1:         // use legacy encoder
1:         return null;
1:       } else if ((inputPage.getDataType() == DataTypes.BYTE) || (inputPage.getDataType()
1:           == DataTypes.SHORT) || (inputPage.getDataType() == DataTypes.INT) || (
1:           inputPage.getDataType() == DataTypes.LONG)) {
1:         return selectCodecByAlgorithmForIntegral(inputPage.getStatistics(), true)
1:             .createEncoder(null);
1:       } else if ((inputPage.getDataType() == DataTypes.FLOAT) || (inputPage.getDataType()
1:           == DataTypes.DOUBLE)) {
1:         return selectCodecByAlgorithmForFloating(inputPage.getStatistics(), true)
1:             .createEncoder(null);
1:       }
1:     }
1:     // use legacy encoder
1:     return null;
1:   }
1: 
1: 
author:kumarvishal09
-------------------------------------------------------------------------------
commit:e710339
/////////////////////////////////////////////////////////////////////////
/////////////////////////////////////////////////////////////////////////
1: import org.apache.carbondata.format.LocalDictionaryChunk;
1: import org.apache.carbondata.format.LocalDictionaryChunkMeta;
/////////////////////////////////////////////////////////////////////////
1:     return new EncodedColumnPage(pageMetadata, encodedBytes, inputPage);
/////////////////////////////////////////////////////////////////////////
0:     while (index < input.getDepth()) {
1:       ColumnPage subColumnPage = input.getColumnPage(index);
1:       encodedPages[index] = encodedColumn(subColumnPage);
1:   public static EncodedColumnPage encodedColumn(ColumnPage page)
1:   /**
1:    * Below method to encode the dictionary page
1:    * @param dictionaryPage
1:    * dictionary column page
1:    * @return local dictionary chunk
1:    * @throws IOException
1:    * Problem in encoding
1:    * @throws MemoryException
1:    * problem in encoding
1:    */
1:   public LocalDictionaryChunk encodeDictionary(ColumnPage dictionaryPage)
1:       throws IOException, MemoryException {
1:     LocalDictionaryChunk localDictionaryChunk = new LocalDictionaryChunk();
1:     localDictionaryChunk.setDictionary_data(encodeData(dictionaryPage));
1:     LocalDictionaryChunkMeta localDictionaryChunkMeta = new LocalDictionaryChunkMeta();
1:     localDictionaryChunkMeta.setEncoders(getEncodingList());
1:     localDictionaryChunkMeta.setEncoder_meta(buildEncoderMeta(dictionaryPage));
1:     localDictionaryChunk.setDictionary_meta(localDictionaryChunkMeta);
1:     return localDictionaryChunk;
1:   }
1: 
commit:6297ea0
/////////////////////////////////////////////////////////////////////////
0:       encodedPages[index] = encodeChildColumn(subColumnPage, input.getComplexColumnType(index));
1:       index++;
0:   private static EncodedColumnPage encodeChildColumn(byte[][] data, ColumnType complexDataType)
0:     TableSpec.ColumnSpec spec = TableSpec.ColumnSpec
0:         .newInstance("complex_inner_column", DataTypes.BYTE_ARRAY, complexDataType);
author:Jacky Li
-------------------------------------------------------------------------------
commit:f209e8e
/////////////////////////////////////////////////////////////////////////
0:     TableSpec.ColumnSpec spec = TableSpec.ColumnSpec.newInstance("complex_inner_column",
0:         DataTypes.BYTE_ARRAY, ColumnType.COMPLEX);
commit:956833e
/////////////////////////////////////////////////////////////////////////
1: import org.apache.carbondata.core.metadata.datatype.DataTypes;
/////////////////////////////////////////////////////////////////////////
0:         new TableSpec.ColumnSpec("complex_inner_column", DataTypes.BYTE_ARRAY, ColumnType.COMPLEX);
0:     ColumnPageEncoder encoder = new DirectCompressCodec(DataTypes.BYTE_ARRAY).createEncoder(null);
commit:8c1ddbf
/////////////////////////////////////////////////////////////////////////
0: import org.apache.carbondata.core.datastore.ColumnType;
0: import org.apache.carbondata.core.datastore.TableSpec;
1: import org.apache.carbondata.core.datastore.page.encoding.compress.DirectCompressCodec;
0: import org.apache.carbondata.core.metadata.datatype.DataType;
/////////////////////////////////////////////////////////////////////////
0:     TableSpec.ColumnSpec spec =
0:         new TableSpec.ColumnSpec("complex_inner_column", DataType.BYTE_ARRAY, ColumnType.COMPLEX);
0:     ColumnPage page = ColumnPage.wrapByteArrayPage(spec, data);
0:     ColumnPageEncoder encoder = new DirectCompressCodec(DataType.BYTE_ARRAY).createEncoder(null);
0:     return encoder.encode(page);
commit:e6a4f64
/////////////////////////////////////////////////////////////////////////
1: /*
1:  * Licensed to the Apache Software Foundation (ASF) under one or more
1:  * contributor license agreements.  See the NOTICE file distributed with
1:  * this work for additional information regarding copyright ownership.
1:  * The ASF licenses this file to You under the Apache License, Version 2.0
1:  * (the "License"); you may not use this file except in compliance with
1:  * the License.  You may obtain a copy of the License at
1:  *
1:  *    http://www.apache.org/licenses/LICENSE-2.0
1:  *
1:  * Unless required by applicable law or agreed to in writing, software
1:  * distributed under the License is distributed on an "AS IS" BASIS,
1:  * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
1:  * See the License for the specific language governing permissions and
1:  * limitations under the License.
1:  */
1: 
1: package org.apache.carbondata.core.datastore.page.encoding;
1: 
1: import java.io.ByteArrayOutputStream;
1: import java.io.DataOutputStream;
1: import java.io.IOException;
1: import java.nio.ByteBuffer;
1: import java.util.ArrayList;
0: import java.util.Iterator;
1: import java.util.List;
1: 
1: import org.apache.carbondata.core.datastore.compression.Compressor;
1: import org.apache.carbondata.core.datastore.compression.CompressorFactory;
1: import org.apache.carbondata.core.datastore.page.ColumnPage;
1: import org.apache.carbondata.core.datastore.page.ComplexColumnPage;
0: import org.apache.carbondata.core.datastore.page.encoding.dimension.legacy.ComplexDimensionIndexCodec;
1: import org.apache.carbondata.core.memory.MemoryException;
1: import org.apache.carbondata.core.util.CarbonMetadataUtil;
1: import org.apache.carbondata.core.util.CarbonUtil;
1: import org.apache.carbondata.format.BlockletMinMaxIndex;
1: import org.apache.carbondata.format.DataChunk2;
1: import org.apache.carbondata.format.Encoding;
1: import org.apache.carbondata.format.PresenceMeta;
1: 
1: public abstract class ColumnPageEncoder {
1: 
1:   protected abstract byte[] encodeData(ColumnPage input) throws MemoryException, IOException;
1: 
1:   protected abstract List<Encoding> getEncodingList();
1: 
1:   protected abstract ColumnPageEncoderMeta getEncoderMeta(ColumnPage inputPage);
1: 
1:   /**
1:    * Return a encoded column page by encoding the input page
1:    * The encoded binary data and metadata are wrapped in encoding column page
1:    */
1:   public EncodedColumnPage encode(ColumnPage inputPage) throws IOException, MemoryException {
1:     byte[] encodedBytes = encodeData(inputPage);
1:     DataChunk2 pageMetadata = buildPageMetadata(inputPage, encodedBytes);
0:     return new EncodedColumnPage(pageMetadata, encodedBytes, inputPage.getStatistics());
1:   }
1: 
1:   private DataChunk2 buildPageMetadata(ColumnPage inputPage, byte[] encodedBytes)
1:       throws IOException {
1:     DataChunk2 dataChunk = new DataChunk2();
1:     dataChunk.setData_page_length(encodedBytes.length);
1:     fillBasicFields(inputPage, dataChunk);
1:     fillNullBitSet(inputPage, dataChunk);
1:     fillEncoding(inputPage, dataChunk);
1:     fillMinMaxIndex(inputPage, dataChunk);
1:     fillLegacyFields(dataChunk);
1:     return dataChunk;
1:   }
1: 
1:   private void fillBasicFields(ColumnPage inputPage, DataChunk2 dataChunk) {
0:     dataChunk.setChunk_meta(CarbonMetadataUtil.getSnappyChunkCompressionMeta());
1:     dataChunk.setNumberOfRowsInpage(inputPage.getPageSize());
1:     dataChunk.setRowMajor(false);
1:   }
1: 
1:   private void fillNullBitSet(ColumnPage inputPage, DataChunk2 dataChunk) {
1:     PresenceMeta presenceMeta = new PresenceMeta();
1:     presenceMeta.setPresent_bit_streamIsSet(true);
0:     Compressor compressor = CompressorFactory.getInstance().getCompressor();
1:     presenceMeta.setPresent_bit_stream(
1:         compressor.compressByte(inputPage.getNullBits().toByteArray()));
1:     dataChunk.setPresence(presenceMeta);
1:   }
1: 
1:   private void fillEncoding(ColumnPage inputPage, DataChunk2 dataChunk) throws IOException {
1:     dataChunk.setEncoders(getEncodingList());
1:     dataChunk.setEncoder_meta(buildEncoderMeta(inputPage));
1:   }
1: 
1:   private List<ByteBuffer> buildEncoderMeta(ColumnPage inputPage) throws IOException {
1:     ColumnPageEncoderMeta meta = getEncoderMeta(inputPage);
1:     List<ByteBuffer> metaDatas = new ArrayList<>();
1:     if (meta != null) {
1:       ByteArrayOutputStream stream = new ByteArrayOutputStream();
1:       DataOutputStream out = new DataOutputStream(stream);
1:       meta.write(out);
1:       metaDatas.add(ByteBuffer.wrap(stream.toByteArray()));
1:     }
1:     return metaDatas;
1:   }
1: 
1:   private void fillMinMaxIndex(ColumnPage inputPage, DataChunk2 dataChunk) {
1:     dataChunk.setMin_max(buildMinMaxIndex(inputPage));
1:   }
1: 
1:   private BlockletMinMaxIndex buildMinMaxIndex(ColumnPage inputPage) {
1:     BlockletMinMaxIndex index = new BlockletMinMaxIndex();
1:     byte[] bytes = CarbonUtil.getValueAsBytes(
1:         inputPage.getDataType(), inputPage.getStatistics().getMax());
1:     ByteBuffer max = ByteBuffer.wrap(
1:         bytes);
1:     ByteBuffer min = ByteBuffer.wrap(
1:         CarbonUtil.getValueAsBytes(inputPage.getDataType(), inputPage.getStatistics().getMin()));
1:     index.addToMax_values(max);
1:     index.addToMin_values(min);
1:     return index;
1:   }
1: 
1:   /**
1:    * `buildPageMetadata` will call this for backward compatibility
1:    */
1:   protected void fillLegacyFields(DataChunk2 dataChunk)
1:       throws IOException {
1:     // Subclass should override this to update datachunk2 if any backward compatibility if required,
1:     // For example, when using IndexStorageCodec, rle_page_length and rowid_page_length need to be
1:     // updated
1:   }
1: 
1:   /**
1:    * Apply encoding algorithm for complex column page and return the coded data
1:    * TODO: remove this interface after complex column page is unified with column page
1:    */
1:   public static EncodedColumnPage[] encodeComplexColumn(ComplexColumnPage input)
1:       throws IOException, MemoryException {
0:     EncodedColumnPage[] encodedPages = new EncodedColumnPage[input.getDepth()];
1:     int index = 0;
0:     Iterator<byte[][]> iterator = input.iterator();
0:     while (iterator.hasNext()) {
0:       byte[][] subColumnPage = iterator.next();
0:       encodedPages[index++] = encodeChildColumn(subColumnPage);
1:     }
1:     return encodedPages;
1:   }
1: 
0:   private static EncodedColumnPage encodeChildColumn(byte[][] data)
1:       throws IOException, MemoryException {
0:     Compressor compressor = CompressorFactory.getInstance().getCompressor();
0:     ComplexDimensionIndexCodec codec = new ComplexDimensionIndexCodec(false, false, compressor);
0:     ColumnPageEncoder encoder = codec.createEncoder(null);
0:     return encoder.encode(ColumnPage.wrapByteArrayPage(data));
1:   }
1: 
1: }
============================================================================