1:531dcd2: /*
1:531dcd2:  * Licensed to the Apache Software Foundation (ASF) under one or more
1:531dcd2:  * contributor license agreements.  See the NOTICE file distributed with
1:531dcd2:  * this work for additional information regarding copyright ownership.
1:531dcd2:  * The ASF licenses this file to You under the Apache License, Version 2.0
1:531dcd2:  * (the "License"); you may not use this file except in compliance with
1:531dcd2:  * the License.  You may obtain a copy of the License at
1:531dcd2:  *
1:531dcd2:  *    http://www.apache.org/licenses/LICENSE-2.0
1:531dcd2:  *
1:531dcd2:  * Unless required by applicable law or agreed to in writing, software
1:531dcd2:  * distributed under the License is distributed on an "AS IS" BASIS,
1:531dcd2:  * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
1:531dcd2:  * See the License for the specific language governing permissions and
1:531dcd2:  * limitations under the License.
2:531dcd2:  */
3:531dcd2: 
1:531dcd2: package org.apache.carbondata.presto;
1:531dcd2: 
1:531dcd2: import java.io.IOException;
1:531dcd2: import java.util.ArrayList;
1:531dcd2: import java.util.List;
1:531dcd2: import java.util.Map;
1:531dcd2: 
1:531dcd2: import org.apache.carbondata.core.cache.dictionary.Dictionary;
1:531dcd2: import org.apache.carbondata.core.datastore.block.TableBlockInfo;
1:531dcd2: import org.apache.carbondata.core.keygenerator.directdictionary.DirectDictionaryGenerator;
1:531dcd2: import org.apache.carbondata.core.keygenerator.directdictionary.DirectDictionaryKeyGeneratorFactory;
1:531dcd2: import org.apache.carbondata.core.metadata.datatype.DataType;
1:956833e: import org.apache.carbondata.core.metadata.datatype.DataTypes;
1:e5e74fc: import org.apache.carbondata.core.metadata.datatype.StructField;
1:531dcd2: import org.apache.carbondata.core.metadata.encoder.Encoding;
1:531dcd2: import org.apache.carbondata.core.scan.executor.QueryExecutor;
1:531dcd2: import org.apache.carbondata.core.scan.executor.QueryExecutorFactory;
1:531dcd2: import org.apache.carbondata.core.scan.executor.exception.QueryExecutionException;
1:daa6465: import org.apache.carbondata.core.scan.model.ProjectionDimension;
1:daa6465: import org.apache.carbondata.core.scan.model.ProjectionMeasure;
1:531dcd2: import org.apache.carbondata.core.scan.model.QueryModel;
1:531dcd2: import org.apache.carbondata.core.scan.result.iterator.AbstractDetailQueryResultIterator;
1:531dcd2: import org.apache.carbondata.core.scan.result.vector.CarbonColumnVector;
1:531dcd2: import org.apache.carbondata.core.scan.result.vector.CarbonColumnarBatch;
1:01b48fc: import org.apache.carbondata.core.stats.QueryStatistic;
1:01b48fc: import org.apache.carbondata.core.stats.QueryStatisticsConstants;
1:01b48fc: import org.apache.carbondata.core.stats.QueryStatisticsRecorder;
1:01b48fc: import org.apache.carbondata.core.stats.TaskStatistics;
1:531dcd2: import org.apache.carbondata.core.util.CarbonUtil;
1:531dcd2: import org.apache.carbondata.hadoop.AbstractRecordReader;
1:531dcd2: import org.apache.carbondata.hadoop.CarbonInputSplit;
1:531dcd2: import org.apache.carbondata.hadoop.CarbonMultiBlockSplit;
1:531dcd2: 
1:531dcd2: import org.apache.hadoop.mapreduce.InputSplit;
1:531dcd2: import org.apache.hadoop.mapreduce.TaskAttemptContext;
1:531dcd2: 
2:531dcd2: /**
1:531dcd2:  * A specialized RecordReader that reads into InternalRows or ColumnarBatches directly using the
1:531dcd2:  * carbondata column APIs and fills the data directly into columns.
1:531dcd2:  */
1:daa6465: class PrestoCarbonVectorizedRecordReader extends AbstractRecordReader<Object> {
1:531dcd2: 
1:531dcd2:   private int batchIdx = 0;
1:531dcd2: 
1:531dcd2:   private int numBatched = 0;
1:531dcd2: 
1:e5e74fc:   private CarbonVectorBatch columnarBatch;
1:531dcd2: 
1:531dcd2:   private CarbonColumnarBatch carbonColumnarBatch;
1:531dcd2: 
1:531dcd2:   /**
1:531dcd2:    * If true, this class returns batches instead of rows.
1:531dcd2:    */
1:531dcd2:   private boolean returnColumnarBatch;
1:531dcd2: 
1:531dcd2:   private QueryModel queryModel;
1:531dcd2: 
1:531dcd2:   private AbstractDetailQueryResultIterator iterator;
1:531dcd2: 
1:531dcd2:   private QueryExecutor queryExecutor;
1:531dcd2: 
1:01b48fc:   private long taskId;
1:01b48fc: 
1:01b48fc:   private long queryStartTime;
1:01b48fc: 
1:a4c2ef5:   private CarbonDictionaryDecodeReadSupport readSupport;
1:a4c2ef5: 
1:01b48fc:   public PrestoCarbonVectorizedRecordReader(QueryExecutor queryExecutor, QueryModel queryModel,
1:a4c2ef5:       AbstractDetailQueryResultIterator iterator, CarbonDictionaryDecodeReadSupport readSupport) {
1:531dcd2:     this.queryModel = queryModel;
1:531dcd2:     this.iterator = iterator;
1:531dcd2:     this.queryExecutor = queryExecutor;
1:a4c2ef5:     this.readSupport = readSupport;
1:531dcd2:     enableReturningBatches();
1:01b48fc:     this.queryStartTime = System.currentTimeMillis();
2:531dcd2:   }
1:531dcd2: 
1:531dcd2:   /**
1:531dcd2:    * Implementation of RecordReader API.
1:531dcd2:    */
1:531dcd2:   @Override public void initialize(InputSplit inputSplit, TaskAttemptContext taskAttemptContext)
1:531dcd2:       throws IOException, InterruptedException, UnsupportedOperationException {
1:531dcd2:     // The input split can contain single HDFS block or multiple blocks, so firstly get all the
1:531dcd2:     // blocks and then set them in the query model.
1:531dcd2:     List<CarbonInputSplit> splitList;
1:531dcd2:     if (inputSplit instanceof CarbonInputSplit) {
1:531dcd2:       splitList = new ArrayList<>(1);
1:531dcd2:       splitList.add((CarbonInputSplit) inputSplit);
1:531dcd2:     } else if (inputSplit instanceof CarbonMultiBlockSplit) {
1:531dcd2:       // contains multiple blocks, this is an optimization for concurrent query.
1:531dcd2:       CarbonMultiBlockSplit multiBlockSplit = (CarbonMultiBlockSplit) inputSplit;
1:531dcd2:       splitList = multiBlockSplit.getAllSplits();
1:531dcd2:     } else {
1:531dcd2:       throw new RuntimeException("unsupported input split type: " + inputSplit);
1:531dcd2:     }
1:531dcd2:     List<TableBlockInfo> tableBlockInfoList = CarbonInputSplit.createBlocks(splitList);
1:531dcd2:     queryModel.setTableBlockInfos(tableBlockInfoList);
1:531dcd2:     queryModel.setVectorReader(true);
1:531dcd2:     try {
1:2a9604c:       queryExecutor =
1:2a9604c:           QueryExecutorFactory.getQueryExecutor(queryModel, taskAttemptContext.getConfiguration());
1:531dcd2:       iterator = (AbstractDetailQueryResultIterator) queryExecutor.execute(queryModel);
1:531dcd2:     } catch (QueryExecutionException e) {
1:531dcd2:       throw new InterruptedException(e.getMessage());
1:531dcd2:     }
1:531dcd2:   }
1:531dcd2: 
1:531dcd2:   @Override public void close() throws IOException {
1:531dcd2:     logStatistics(rowCount, queryModel.getStatisticsRecorder());
1:531dcd2:     if (columnarBatch != null) {
1:531dcd2:       columnarBatch = null;
1:531dcd2:     }
1:531dcd2:     // clear dictionary cache
1:531dcd2:     Map<String, Dictionary> columnToDictionaryMapping = queryModel.getColumnToDictionaryMapping();
1:531dcd2:     if (null != columnToDictionaryMapping) {
1:531dcd2:       for (Map.Entry<String, Dictionary> entry : columnToDictionaryMapping.entrySet()) {
1:531dcd2:         CarbonUtil.clearDictionaryCache(entry.getValue());
1:531dcd2:       }
1:531dcd2:     }
1:531dcd2:     try {
1:531dcd2:       queryExecutor.finish();
1:531dcd2:     } catch (QueryExecutionException e) {
1:531dcd2:       throw new IOException(e);
1:531dcd2:     }
1:01b48fc: 
1:01b48fc:     logStatistics(taskId, queryStartTime, queryModel.getStatisticsRecorder());
1:531dcd2:   }
1:531dcd2: 
1:531dcd2:   @Override public boolean nextKeyValue() throws IOException, InterruptedException {
1:531dcd2:     resultBatch();
1:531dcd2: 
1:531dcd2:     if (returnColumnarBatch) return nextBatch();
1:531dcd2: 
1:531dcd2:     if (batchIdx >= numBatched) {
1:531dcd2:       if (!nextBatch()) return false;
1:531dcd2:     }
1:531dcd2:     ++batchIdx;
1:531dcd2:     return true;
1:531dcd2:   }
1:531dcd2: 
1:531dcd2:   @Override public Object getCurrentValue() throws IOException, InterruptedException {
1:531dcd2:     if (returnColumnarBatch) {
1:531dcd2:       rowCount += columnarBatch.numValidRows();
1:531dcd2:       return columnarBatch;
1:e5e74fc:     } else {
1:e5e74fc:       return null;
1:531dcd2:     }
1:531dcd2:   }
1:531dcd2: 
1:531dcd2:   @Override public Void getCurrentKey() throws IOException, InterruptedException {
1:531dcd2:     return null;
1:531dcd2:   }
1:531dcd2: 
1:531dcd2:   @Override public float getProgress() throws IOException, InterruptedException {
1:531dcd2:     // TODO : Implement it based on total number of rows it is going to retrive.
1:531dcd2:     return 0;
1:531dcd2:   }
1:531dcd2: 
1:531dcd2:   /**
1:531dcd2:    * Returns the ColumnarBatch object that will be used for all rows returned by this reader.
1:531dcd2:    * This object is reused. Calling this enables the vectorized reader. This should be called
1:531dcd2:    * before any calls to nextKeyValue/nextBatch.
1:531dcd2:    */
1:531dcd2: 
1:e5e74fc:   private void initBatch() {
1:daa6465:     List<ProjectionDimension> queryDimension = queryModel.getProjectionDimensions();
1:daa6465:     List<ProjectionMeasure> queryMeasures = queryModel.getProjectionMeasures();
1:531dcd2:     StructField[] fields = new StructField[queryDimension.size() + queryMeasures.size()];
1:531dcd2:     for (int i = 0; i < queryDimension.size(); i++) {
1:daa6465:       ProjectionDimension dim = queryDimension.get(i);
1:531dcd2:       if (dim.getDimension().hasEncoding(Encoding.DIRECT_DICTIONARY)) {
1:531dcd2:         DirectDictionaryGenerator generator = DirectDictionaryKeyGeneratorFactory
1:531dcd2:             .getDirectDictionaryGenerator(dim.getDimension().getDataType());
1:a4c2ef5:         fields[dim.getOrdinal()] = new StructField(dim.getColumnName(), generator.getReturnType());
1:531dcd2:       } else if (!dim.getDimension().hasEncoding(Encoding.DICTIONARY)) {
1:a4c2ef5:         fields[dim.getOrdinal()] =
1:a4c2ef5:             new StructField(dim.getColumnName(), dim.getDimension().getDataType());
1:531dcd2:       } else if (dim.getDimension().isComplex()) {
1:a4c2ef5:         fields[dim.getOrdinal()] =
1:a4c2ef5:             new StructField(dim.getColumnName(), dim.getDimension().getDataType());
1:531dcd2:       } else {
1:a4c2ef5:         fields[dim.getOrdinal()] = new StructField(dim.getColumnName(), DataTypes.INT);
1:531dcd2:       }
1:531dcd2:     }
1:531dcd2: 
1:daa6465:     for (ProjectionMeasure msr : queryMeasures) {
1:956833e:       DataType dataType = msr.getMeasure().getDataType();
1:e2f1997:       if (dataType == DataTypes.BOOLEAN || dataType == DataTypes.SHORT || dataType == DataTypes.INT
1:e2f1997:           || dataType == DataTypes.LONG) {
1:daa6465:         fields[msr.getOrdinal()] =
1:e2f1997:             new StructField(msr.getColumnName(), msr.getMeasure().getDataType());
1:f209e8e:       } else if (DataTypes.isDecimal(dataType)) {
1:daa6465:         fields[msr.getOrdinal()] =
1:e2f1997:             new StructField(msr.getColumnName(), msr.getMeasure().getDataType());
1:956833e:       } else {
1:daa6465:         fields[msr.getOrdinal()] = new StructField(msr.getColumnName(), DataTypes.DOUBLE);
1:531dcd2:       }
1:531dcd2:     }
1:531dcd2: 
1:a4c2ef5:     columnarBatch = CarbonVectorBatch.allocate(fields, readSupport);
1:531dcd2:     CarbonColumnVector[] vectors = new CarbonColumnVector[fields.length];
1:531dcd2:     boolean[] filteredRows = new boolean[columnarBatch.capacity()];
1:531dcd2:     for (int i = 0; i < fields.length; i++) {
1:e5e74fc:       vectors[i] = new CarbonColumnVectorWrapper(columnarBatch.column(i), filteredRows);
1:531dcd2:     }
1:531dcd2:     carbonColumnarBatch = new CarbonColumnarBatch(vectors, columnarBatch.capacity(), filteredRows);
1:531dcd2:   }
1:531dcd2: 
1:e5e74fc:   private CarbonVectorBatch resultBatch() {
1:531dcd2:     if (columnarBatch == null) initBatch();
1:531dcd2:     return columnarBatch;
1:531dcd2:   }
1:531dcd2: 
1:531dcd2:   /*
1:531dcd2:    * Can be called before any rows are returned to enable returning columnar batches directly.
1:531dcd2:    */
1:531dcd2:   private void enableReturningBatches() {
1:531dcd2:     returnColumnarBatch = true;
1:531dcd2:   }
1:531dcd2: 
1:531dcd2:   /**
1:531dcd2:    * Advances to the next batch of rows. Returns false if there are no more.
1:531dcd2:    */
1:531dcd2:   private boolean nextBatch() {
1:531dcd2:     columnarBatch.reset();
1:531dcd2:     carbonColumnarBatch.reset();
1:531dcd2:     if (iterator.hasNext()) {
1:531dcd2:       iterator.processNextBatch(carbonColumnarBatch);
1:531dcd2:       int actualSize = carbonColumnarBatch.getActualSize();
1:531dcd2:       columnarBatch.setNumRows(actualSize);
1:531dcd2:       numBatched = actualSize;
1:531dcd2:       batchIdx = 0;
1:531dcd2:       return true;
1:531dcd2:     }
1:531dcd2:     return false;
1:531dcd2:   }
1:531dcd2: 
1:a4c2ef5:   public CarbonVectorBatch getColumnarBatch() {
1:a4c2ef5:     return columnarBatch;
1:a4c2ef5:   }
1:01b48fc:   public void setTaskId(long taskId) {
1:01b48fc:     this.taskId = taskId;
1:01b48fc:   }
1:01b48fc: 
1:01b48fc:   /**
1:01b48fc:    * For Logging the Statistics
1:01b48fc:    * @param taskId
1:01b48fc:    * @param queryStartTime
1:01b48fc:    * @param recorder
1:01b48fc:    */
1:01b48fc:   private void  logStatistics(
1:01b48fc:       Long taskId,
1:01b48fc:       Long queryStartTime,
1:01b48fc:       QueryStatisticsRecorder recorder
1:01b48fc:   ) {
1:01b48fc:     if (null != recorder) {
1:01b48fc:       QueryStatistic queryStatistic = new QueryStatistic();
1:01b48fc:       queryStatistic.addFixedTimeStatistic(QueryStatisticsConstants.EXECUTOR_PART,
1:01b48fc:           System.currentTimeMillis() - queryStartTime);
1:01b48fc:       recorder.recordStatistics(queryStatistic);
1:01b48fc:       // print executor query statistics for each task_id
1:01b48fc:       TaskStatistics statistics = recorder.statisticsForTask(taskId, queryStartTime);
1:01b48fc:       recorder.logStatisticsForTask(statistics);
1:01b48fc:     }
1:01b48fc:   }
1:531dcd2: 
1:531dcd2: }
============================================================================
author:kunal642
-------------------------------------------------------------------------------
commit:2a9604c
/////////////////////////////////////////////////////////////////////////
1:       queryExecutor =
1:           QueryExecutorFactory.getQueryExecutor(queryModel, taskAttemptContext.getConfiguration());
author:sv71294
-------------------------------------------------------------------------------
commit:a4c2ef5
/////////////////////////////////////////////////////////////////////////
1:   private CarbonDictionaryDecodeReadSupport readSupport;
1: 
1:       AbstractDetailQueryResultIterator iterator, CarbonDictionaryDecodeReadSupport readSupport) {
1:     this.readSupport = readSupport;
/////////////////////////////////////////////////////////////////////////
1:         fields[dim.getOrdinal()] = new StructField(dim.getColumnName(), generator.getReturnType());
1:         fields[dim.getOrdinal()] =
1:             new StructField(dim.getColumnName(), dim.getDimension().getDataType());
1:         fields[dim.getOrdinal()] =
1:             new StructField(dim.getColumnName(), dim.getDimension().getDataType());
1:         fields[dim.getOrdinal()] = new StructField(dim.getColumnName(), DataTypes.INT);
/////////////////////////////////////////////////////////////////////////
1:     columnarBatch = CarbonVectorBatch.allocate(fields, readSupport);
/////////////////////////////////////////////////////////////////////////
/////////////////////////////////////////////////////////////////////////
1:   public CarbonVectorBatch getColumnarBatch() {
1:     return columnarBatch;
1:   }
author:Bhavya
-------------------------------------------------------------------------------
commit:01b48fc
/////////////////////////////////////////////////////////////////////////
1: import org.apache.carbondata.core.stats.QueryStatistic;
1: import org.apache.carbondata.core.stats.QueryStatisticsConstants;
1: import org.apache.carbondata.core.stats.QueryStatisticsRecorder;
1: import org.apache.carbondata.core.stats.TaskStatistics;
/////////////////////////////////////////////////////////////////////////
1:   private long taskId;
1: 
1:   private long queryStartTime;
1: 
1:   public PrestoCarbonVectorizedRecordReader(QueryExecutor queryExecutor, QueryModel queryModel,
0:       AbstractDetailQueryResultIterator iterator) {
1:     this.queryStartTime = System.currentTimeMillis();
/////////////////////////////////////////////////////////////////////////
1: 
1:     logStatistics(taskId, queryStartTime, queryModel.getStatisticsRecorder());
/////////////////////////////////////////////////////////////////////////
1:   public void setTaskId(long taskId) {
1:     this.taskId = taskId;
1:   }
1: 
1:   /**
1:    * For Logging the Statistics
1:    * @param taskId
1:    * @param queryStartTime
1:    * @param recorder
1:    */
1:   private void  logStatistics(
1:       Long taskId,
1:       Long queryStartTime,
1:       QueryStatisticsRecorder recorder
1:   ) {
1:     if (null != recorder) {
1:       QueryStatistic queryStatistic = new QueryStatistic();
1:       queryStatistic.addFixedTimeStatistic(QueryStatisticsConstants.EXECUTOR_PART,
1:           System.currentTimeMillis() - queryStartTime);
1:       recorder.recordStatistics(queryStatistic);
1:       // print executor query statistics for each task_id
1:       TaskStatistics statistics = recorder.statisticsForTask(taskId, queryStartTime);
1:       recorder.logStatisticsForTask(statistics);
1:     }
1:   }
commit:e5e74fc
/////////////////////////////////////////////////////////////////////////
1: import org.apache.carbondata.core.metadata.datatype.StructField;
/////////////////////////////////////////////////////////////////////////
/////////////////////////////////////////////////////////////////////////
1:   private CarbonVectorBatch columnarBatch;
/////////////////////////////////////////////////////////////////////////
/////////////////////////////////////////////////////////////////////////
/////////////////////////////////////////////////////////////////////////
1:     } else {
1:       return null;
/////////////////////////////////////////////////////////////////////////
1:   private void initBatch() {
/////////////////////////////////////////////////////////////////////////
0:            generator.getReturnType());
0:             dim.getDimension().getDataType());
0:            dim.getDimension().getDataType());
0:             DataTypes.INT);
/////////////////////////////////////////////////////////////////////////
0:             msr.getMeasure().getDataType());
0:            msr.getMeasure().getDataType());
0:             DataTypes.DOUBLE);
0:     columnarBatch = CarbonVectorBatch.allocate(fields);
1:       vectors[i] = new CarbonColumnVectorWrapper(columnarBatch.column(i), filteredRows);
1:   private CarbonVectorBatch resultBatch() {
commit:531dcd2
/////////////////////////////////////////////////////////////////////////
1: /*
1:  * Licensed to the Apache Software Foundation (ASF) under one or more
1:  * contributor license agreements.  See the NOTICE file distributed with
1:  * this work for additional information regarding copyright ownership.
1:  * The ASF licenses this file to You under the Apache License, Version 2.0
1:  * (the "License"); you may not use this file except in compliance with
1:  * the License.  You may obtain a copy of the License at
1:  *
1:  *    http://www.apache.org/licenses/LICENSE-2.0
1:  *
1:  * Unless required by applicable law or agreed to in writing, software
1:  * distributed under the License is distributed on an "AS IS" BASIS,
1:  * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
1:  * See the License for the specific language governing permissions and
1:  * limitations under the License.
1:  */
1: 
1: package org.apache.carbondata.presto;
1: 
1: import java.io.IOException;
1: import java.util.ArrayList;
1: import java.util.List;
1: import java.util.Map;
1: 
1: import org.apache.carbondata.core.cache.dictionary.Dictionary;
1: import org.apache.carbondata.core.datastore.block.TableBlockInfo;
1: import org.apache.carbondata.core.keygenerator.directdictionary.DirectDictionaryGenerator;
1: import org.apache.carbondata.core.keygenerator.directdictionary.DirectDictionaryKeyGeneratorFactory;
1: import org.apache.carbondata.core.metadata.datatype.DataType;
1: import org.apache.carbondata.core.metadata.encoder.Encoding;
1: import org.apache.carbondata.core.scan.executor.QueryExecutor;
1: import org.apache.carbondata.core.scan.executor.QueryExecutorFactory;
1: import org.apache.carbondata.core.scan.executor.exception.QueryExecutionException;
0: import org.apache.carbondata.core.scan.model.QueryDimension;
0: import org.apache.carbondata.core.scan.model.QueryMeasure;
1: import org.apache.carbondata.core.scan.model.QueryModel;
1: import org.apache.carbondata.core.scan.result.iterator.AbstractDetailQueryResultIterator;
1: import org.apache.carbondata.core.scan.result.vector.CarbonColumnVector;
1: import org.apache.carbondata.core.scan.result.vector.CarbonColumnarBatch;
1: import org.apache.carbondata.core.util.CarbonUtil;
1: import org.apache.carbondata.hadoop.AbstractRecordReader;
1: import org.apache.carbondata.hadoop.CarbonInputSplit;
1: import org.apache.carbondata.hadoop.CarbonMultiBlockSplit;
1: 
1: import org.apache.hadoop.mapreduce.InputSplit;
1: import org.apache.hadoop.mapreduce.TaskAttemptContext;
0: import org.apache.spark.memory.MemoryMode;
0: import org.apache.spark.sql.execution.vectorized.ColumnarBatch;
0: import org.apache.spark.sql.types.DecimalType;
0: import org.apache.spark.sql.types.StructField;
0: import org.apache.spark.sql.types.StructType;
1: 
1: /**
1:  * A specialized RecordReader that reads into InternalRows or ColumnarBatches directly using the
1:  * carbondata column APIs and fills the data directly into columns.
1:  */
0: class CarbonVectorizedRecordReader extends AbstractRecordReader<Object> {
1: 
1:   private int batchIdx = 0;
1: 
1:   private int numBatched = 0;
1: 
0:   private ColumnarBatch columnarBatch;
1: 
1:   private CarbonColumnarBatch carbonColumnarBatch;
1: 
1:   /**
1:    * If true, this class returns batches instead of rows.
1:    */
1:   private boolean returnColumnarBatch;
1: 
1:   /**
0:    * The default config on whether columnarBatch should be offheap.
1:    */
0:   private static final MemoryMode DEFAULT_MEMORY_MODE = MemoryMode.OFF_HEAP;
1: 
1:   private QueryModel queryModel;
1: 
1:   private AbstractDetailQueryResultIterator iterator;
1: 
1:   private QueryExecutor queryExecutor;
1: 
0:   public CarbonVectorizedRecordReader(QueryExecutor queryExecutor, QueryModel queryModel, AbstractDetailQueryResultIterator iterator) {
1:     this.queryModel = queryModel;
1:     this.iterator = iterator;
1:     this.queryExecutor = queryExecutor;
1:     enableReturningBatches();
1:   }
1: 
1:   /**
1:    * Implementation of RecordReader API.
1:    */
1:   @Override public void initialize(InputSplit inputSplit, TaskAttemptContext taskAttemptContext)
1:       throws IOException, InterruptedException, UnsupportedOperationException {
1:     // The input split can contain single HDFS block or multiple blocks, so firstly get all the
1:     // blocks and then set them in the query model.
1:     List<CarbonInputSplit> splitList;
1:     if (inputSplit instanceof CarbonInputSplit) {
1:       splitList = new ArrayList<>(1);
1:       splitList.add((CarbonInputSplit) inputSplit);
1:     } else if (inputSplit instanceof CarbonMultiBlockSplit) {
1:       // contains multiple blocks, this is an optimization for concurrent query.
1:       CarbonMultiBlockSplit multiBlockSplit = (CarbonMultiBlockSplit) inputSplit;
1:       splitList = multiBlockSplit.getAllSplits();
1:     } else {
1:       throw new RuntimeException("unsupported input split type: " + inputSplit);
1:     }
1:     List<TableBlockInfo> tableBlockInfoList = CarbonInputSplit.createBlocks(splitList);
1:     queryModel.setTableBlockInfos(tableBlockInfoList);
1:     queryModel.setVectorReader(true);
1:     try {
0:       queryExecutor = QueryExecutorFactory.getQueryExecutor(queryModel);
1:       iterator = (AbstractDetailQueryResultIterator) queryExecutor.execute(queryModel);
1:     } catch (QueryExecutionException e) {
1:       throw new InterruptedException(e.getMessage());
1:     }
1:   }
1: 
1:   @Override public void close() throws IOException {
1:     logStatistics(rowCount, queryModel.getStatisticsRecorder());
1:     if (columnarBatch != null) {
0:       columnarBatch.close();
1:       columnarBatch = null;
1:     }
1:     // clear dictionary cache
1:     Map<String, Dictionary> columnToDictionaryMapping = queryModel.getColumnToDictionaryMapping();
1:     if (null != columnToDictionaryMapping) {
1:       for (Map.Entry<String, Dictionary> entry : columnToDictionaryMapping.entrySet()) {
1:         CarbonUtil.clearDictionaryCache(entry.getValue());
1:       }
1:     }
1:     try {
1:       queryExecutor.finish();
1:     } catch (QueryExecutionException e) {
1:       throw new IOException(e);
1:     }
1:   }
1: 
1:   @Override public boolean nextKeyValue() throws IOException, InterruptedException {
1:     resultBatch();
1: 
1:     if (returnColumnarBatch) return nextBatch();
1: 
1:     if (batchIdx >= numBatched) {
1:       if (!nextBatch()) return false;
1:     }
1:     ++batchIdx;
1:     return true;
1:   }
1: 
1:   @Override public Object getCurrentValue() throws IOException, InterruptedException {
1:     if (returnColumnarBatch) {
1:       rowCount += columnarBatch.numValidRows();
1:       return columnarBatch;
1:     }
0:     rowCount += 1;
0:     return columnarBatch.getRow(batchIdx - 1);
1:   }
1: 
1:   @Override public Void getCurrentKey() throws IOException, InterruptedException {
1:     return null;
1:   }
1: 
1:   @Override public float getProgress() throws IOException, InterruptedException {
1:     // TODO : Implement it based on total number of rows it is going to retrive.
1:     return 0;
1:   }
1: 
1:   /**
1:    * Returns the ColumnarBatch object that will be used for all rows returned by this reader.
1:    * This object is reused. Calling this enables the vectorized reader. This should be called
1:    * before any calls to nextKeyValue/nextBatch.
1:    */
1: 
0:   private void initBatch(MemoryMode memMode) {
0:     List<QueryDimension> queryDimension = queryModel.getQueryDimension();
0:     List<QueryMeasure> queryMeasures = queryModel.getQueryMeasures();
1:     StructField[] fields = new StructField[queryDimension.size() + queryMeasures.size()];
1:     for (int i = 0; i < queryDimension.size(); i++) {
0:       QueryDimension dim = queryDimension.get(i);
1:       if (dim.getDimension().hasEncoding(Encoding.DIRECT_DICTIONARY)) {
1:         DirectDictionaryGenerator generator = DirectDictionaryKeyGeneratorFactory
1:             .getDirectDictionaryGenerator(dim.getDimension().getDataType());
0:         fields[dim.getQueryOrder()] = new StructField(dim.getColumnName(),
0:             CarbonTypeUtil.convertCarbonToSparkDataType(generator.getReturnType()), true, null);
1:       } else if (!dim.getDimension().hasEncoding(Encoding.DICTIONARY)) {
0:         fields[dim.getQueryOrder()] = new StructField(dim.getColumnName(),
0:             CarbonTypeUtil.convertCarbonToSparkDataType(dim.getDimension().getDataType()), true,
0:             null);
1:       } else if (dim.getDimension().isComplex()) {
0:         fields[dim.getQueryOrder()] = new StructField(dim.getColumnName(),
0:             CarbonTypeUtil.convertCarbonToSparkDataType(dim.getDimension().getDataType()), true,
0:             null);
1:       } else {
0:         fields[dim.getQueryOrder()] = new StructField(dim.getColumnName(),
0:             CarbonTypeUtil.convertCarbonToSparkDataType(DataType.INT), true, null);
1:       }
1:     }
1: 
0:     for (int i = 0; i < queryMeasures.size(); i++) {
0:       QueryMeasure msr = queryMeasures.get(i);
0:       switch (msr.getMeasure().getDataType()) {
0:         case SHORT:
0:         case INT:
0:         case LONG:
0:           fields[msr.getQueryOrder()] = new StructField(msr.getColumnName(),
0:               CarbonTypeUtil.convertCarbonToSparkDataType(msr.getMeasure().getDataType()), true,
0:               null);
0:           break;
0:         case DECIMAL:
0:           fields[msr.getQueryOrder()] = new StructField(msr.getColumnName(),
0:               new DecimalType(msr.getMeasure().getPrecision(),
0:                   msr.getMeasure().getScale()), true, null);
0:           break;
0:         default:
0:           fields[msr.getQueryOrder()] = new StructField(msr.getColumnName(),
0:               CarbonTypeUtil.convertCarbonToSparkDataType(DataType.DOUBLE), true, null);
1:       }
1:     }
1: 
0:     columnarBatch = ColumnarBatch.allocate(new StructType(fields), memMode);
1:     CarbonColumnVector[] vectors = new CarbonColumnVector[fields.length];
1:     boolean[] filteredRows = new boolean[columnarBatch.capacity()];
1:     for (int i = 0; i < fields.length; i++) {
0:       vectors[i] = new ColumnarVectorWrapper(columnarBatch.column(i), filteredRows);
1:     }
1:     carbonColumnarBatch = new CarbonColumnarBatch(vectors, columnarBatch.capacity(), filteredRows);
1:   }
1: 
0:   private void initBatch() {
0:     initBatch(DEFAULT_MEMORY_MODE);
1:   }
1: 
0:   private ColumnarBatch resultBatch() {
1:     if (columnarBatch == null) initBatch();
1:     return columnarBatch;
1:   }
1: 
1:   /*
1:    * Can be called before any rows are returned to enable returning columnar batches directly.
1:    */
1:   private void enableReturningBatches() {
1:     returnColumnarBatch = true;
1:   }
1: 
1:   /**
1:    * Advances to the next batch of rows. Returns false if there are no more.
1:    */
1:   private boolean nextBatch() {
1:     columnarBatch.reset();
1:     carbonColumnarBatch.reset();
1:     if (iterator.hasNext()) {
1:       iterator.processNextBatch(carbonColumnarBatch);
1:       int actualSize = carbonColumnarBatch.getActualSize();
1:       columnarBatch.setNumRows(actualSize);
1:       numBatched = actualSize;
1:       batchIdx = 0;
1:       return true;
1:     }
1:     return false;
1:   }
1: 
1: 
1: }
author:Jacky Li
-------------------------------------------------------------------------------
commit:daa6465
/////////////////////////////////////////////////////////////////////////
1: import org.apache.carbondata.core.scan.model.ProjectionDimension;
1: import org.apache.carbondata.core.scan.model.ProjectionMeasure;
/////////////////////////////////////////////////////////////////////////
1: class PrestoCarbonVectorizedRecordReader extends AbstractRecordReader<Object> {
/////////////////////////////////////////////////////////////////////////
0:   public PrestoCarbonVectorizedRecordReader(QueryExecutor queryExecutor, QueryModel queryModel, AbstractDetailQueryResultIterator iterator) {
/////////////////////////////////////////////////////////////////////////
1:     List<ProjectionDimension> queryDimension = queryModel.getProjectionDimensions();
1:     List<ProjectionMeasure> queryMeasures = queryModel.getProjectionMeasures();
1:       ProjectionDimension dim = queryDimension.get(i);
0:         fields[dim.getOrdinal()] = new StructField(dim.getColumnName(),
0:         fields[dim.getOrdinal()] = new StructField(dim.getColumnName(),
0:         fields[dim.getOrdinal()] = new StructField(dim.getColumnName(),
0:         fields[dim.getOrdinal()] = new StructField(dim.getColumnName(),
1:     for (ProjectionMeasure msr : queryMeasures) {
1:         fields[msr.getOrdinal()] =
1:         fields[msr.getOrdinal()] =
1:         fields[msr.getOrdinal()] = new StructField(msr.getColumnName(), DataTypes.DOUBLE);
commit:f209e8e
/////////////////////////////////////////////////////////////////////////
1:       } else if (DataTypes.isDecimal(dataType)) {
commit:956833e
/////////////////////////////////////////////////////////////////////////
1: import org.apache.carbondata.core.metadata.datatype.DataTypes;
/////////////////////////////////////////////////////////////////////////
0:             CarbonTypeUtil.convertCarbonToSparkDataType(DataTypes.INT), true, null);
1:       DataType dataType = msr.getMeasure().getDataType();
0:       if (dataType == DataTypes.SHORT || dataType == DataTypes.INT || dataType == DataTypes.LONG) {
0:         fields[msr.getQueryOrder()] = new StructField(msr.getColumnName(),
0:             CarbonTypeUtil.convertCarbonToSparkDataType(msr.getMeasure().getDataType()), true,
0:             null);
0:       } else if (dataType == DataTypes.DECIMAL) {
0:         fields[msr.getQueryOrder()] = new StructField(msr.getColumnName(),
0:             new DecimalType(msr.getMeasure().getPrecision(), msr.getMeasure().getScale()), true,
0:             null);
1:       } else {
0:         fields[msr.getQueryOrder()] = new StructField(msr.getColumnName(),
0:             CarbonTypeUtil.convertCarbonToSparkDataType(DataTypes.DOUBLE), true, null);
author:anubhav100
-------------------------------------------------------------------------------
commit:e2f1997
/////////////////////////////////////////////////////////////////////////
0:     for (QueryMeasure msr : queryMeasures) {
1:       if (dataType == DataTypes.BOOLEAN || dataType == DataTypes.SHORT || dataType == DataTypes.INT
1:           || dataType == DataTypes.LONG) {
0:         fields[msr.getQueryOrder()] =
1:             new StructField(msr.getColumnName(), msr.getMeasure().getDataType());
0:         fields[msr.getQueryOrder()] =
1:             new StructField(msr.getColumnName(), msr.getMeasure().getDataType());
0:         fields[msr.getQueryOrder()] = new StructField(msr.getColumnName(), DataTypes.DOUBLE);
author:QiangCai
-------------------------------------------------------------------------------
commit:d7393da
/////////////////////////////////////////////////////////////////////////
0: import org.apache.carbondata.hadoop.util.CarbonTypeUtil;
============================================================================