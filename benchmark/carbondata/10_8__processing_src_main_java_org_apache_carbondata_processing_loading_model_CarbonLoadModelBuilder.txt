1:89cfd8e: /*
1:89cfd8e:  * Licensed to the Apache Software Foundation (ASF) under one or more
1:89cfd8e:  * contributor license agreements.  See the NOTICE file distributed with
1:89cfd8e:  * this work for additional information regarding copyright ownership.
1:89cfd8e:  * The ASF licenses this file to You under the Apache License, Version 2.0
1:89cfd8e:  * (the "License"); you may not use this file except in compliance with
1:89cfd8e:  * the License.  You may obtain a copy of the License at
1:89cfd8e:  *
1:89cfd8e:  *    http://www.apache.org/licenses/LICENSE-2.0
1:89cfd8e:  *
1:89cfd8e:  * Unless required by applicable law or agreed to in writing, software
1:89cfd8e:  * distributed under the License is distributed on an "AS IS" BASIS,
1:89cfd8e:  * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
1:89cfd8e:  * See the License for the specific language governing permissions and
1:89cfd8e:  * limitations under the License.
1:89cfd8e:  */
3:89cfd8e: 
1:89cfd8e: package org.apache.carbondata.processing.loading.model;
1:89cfd8e: 
1:89cfd8e: import java.io.IOException;
1:89cfd8e: import java.text.SimpleDateFormat;
1:859d71c: import java.util.ArrayList;
1:859d71c: import java.util.HashMap;
1:89cfd8e: import java.util.List;
1:89cfd8e: import java.util.Map;
1:89cfd8e: 
1:89cfd8e: import org.apache.carbondata.common.Maps;
1:89cfd8e: import org.apache.carbondata.common.Strings;
1:89cfd8e: import org.apache.carbondata.common.annotations.InterfaceAudience;
1:89cfd8e: import org.apache.carbondata.common.constants.LoggerAction;
1:89cfd8e: import org.apache.carbondata.common.exceptions.sql.InvalidLoadOptionException;
1:8f08c4a: import org.apache.carbondata.common.logging.LogService;
1:8f08c4a: import org.apache.carbondata.common.logging.LogServiceFactory;
1:89cfd8e: import org.apache.carbondata.core.constants.CarbonCommonConstants;
1:8f08c4a: import org.apache.carbondata.core.datastore.compression.CompressorFactory;
1:89cfd8e: import org.apache.carbondata.core.metadata.schema.table.CarbonTable;
1:89cfd8e: import org.apache.carbondata.core.metadata.schema.table.column.CarbonColumn;
1:89cfd8e: import org.apache.carbondata.core.util.CarbonProperties;
1:89cfd8e: import org.apache.carbondata.core.util.CarbonUtil;
1:89cfd8e: import org.apache.carbondata.processing.loading.constants.DataLoadProcessorConstants;
1:89cfd8e: import org.apache.carbondata.processing.loading.csvinput.CSVInputFormat;
1:89cfd8e: import org.apache.carbondata.processing.loading.sort.SortScopeOptions;
1:7a1d12a: import org.apache.carbondata.processing.util.CarbonBadRecordUtil;
1:89cfd8e: import org.apache.carbondata.processing.util.TableOptionConstant;
1:89cfd8e: 
1:89cfd8e: import org.apache.commons.lang.StringUtils;
1:89cfd8e: import org.apache.hadoop.conf.Configuration;
1:89cfd8e: 
1:89cfd8e: /**
1:89cfd8e:  * Builder for {@link CarbonLoadModel}
1:89cfd8e:  */
1:e72bfd1: @InterfaceAudience.Internal
1:89cfd8e: public class CarbonLoadModelBuilder {
1:8f08c4a:   private static final LogService LOGGER = LogServiceFactory.getLogService(
1:8f08c4a:       CarbonLoadModelBuilder.class.getName());
1:89cfd8e:   private CarbonTable table;
1:89cfd8e: 
1:89cfd8e:   public CarbonLoadModelBuilder(CarbonTable table) {
1:89cfd8e:     this.table = table;
1:89cfd8e:   }
1:89cfd8e: 
1:89cfd8e:   /**
1:89cfd8e:    * build CarbonLoadModel for data loading
1:89cfd8e:    * @param options Load options from user input
1:b7b8073:    * @param taskNo
1:89cfd8e:    * @return a new CarbonLoadModel instance
1:89cfd8e:    */
1:7a1d12a:   public CarbonLoadModel build(Map<String, String>  options, long UUID, String taskNo)
1:b7b8073:       throws InvalidLoadOptionException, IOException {
1:89cfd8e:     Map<String, String> optionsFinal = LoadOption.fillOptionWithDefaultValue(options);
1:6cb6f83: 
1:89cfd8e:     if (!options.containsKey("fileheader")) {
1:89cfd8e:       List<CarbonColumn> csvHeader = table.getCreateOrderColumn(table.getTableName());
1:89cfd8e:       String[] columns = new String[csvHeader.size()];
1:89cfd8e:       for (int i = 0; i < columns.length; i++) {
1:89cfd8e:         columns[i] = csvHeader.get(i).getColName();
1:89cfd8e:       }
1:89cfd8e:       optionsFinal.put("fileheader", Strings.mkString(columns, ","));
1:89cfd8e:     }
1:7a1d12a:     optionsFinal.put("bad_record_path", CarbonBadRecordUtil.getBadRecordsPath(options, table));
1:f5c7a19:     optionsFinal.put("sort_scope",
1:f5c7a19:         Maps.getOrDefault(options, "sort_scope", CarbonCommonConstants.LOAD_SORT_SCOPE_DEFAULT));
1:89cfd8e:     CarbonLoadModel model = new CarbonLoadModel();
1:b7b8073:     model.setCarbonTransactionalTable(table.isTransactionalTable());
1:280a400:     model.setFactTimeStamp(UUID);
1:b7b8073:     model.setTaskNo(taskNo);
1:89cfd8e: 
1:89cfd8e:     // we have provided 'fileheader', so it hadoopConf can be null
1:89cfd8e:     build(options, optionsFinal, model, null);
1:6b70b7e:     String timestampFormat = options.get("timestampformat");
1:6b70b7e:     if (timestampFormat == null) {
1:6b70b7e:       timestampFormat = CarbonProperties.getInstance()
1:6b70b7e:           .getProperty(CarbonCommonConstants.CARBON_TIMESTAMP_FORMAT,
1:6b70b7e:               CarbonCommonConstants.CARBON_TIMESTAMP_DEFAULT_FORMAT);
1:6b70b7e:     }
1:6b70b7e:     String dateFormat = options.get("dateFormat");
1:6b70b7e:     if (dateFormat == null) {
1:6b70b7e:       dateFormat = CarbonProperties.getInstance()
1:6b70b7e:           .getProperty(CarbonCommonConstants.CARBON_DATE_FORMAT,
1:6b70b7e:               CarbonCommonConstants.CARBON_DATE_DEFAULT_FORMAT);
1:6b70b7e:     }
1:6b70b7e:     model.setDateFormat(dateFormat);
1:6b70b7e:     model.setTimestampformat(timestampFormat);
1:89cfd8e:     model.setUseOnePass(Boolean.parseBoolean(Maps.getOrDefault(options, "onepass", "false")));
1:89cfd8e:     model.setDictionaryServerHost(Maps.getOrDefault(options, "dicthost", null));
1:89cfd8e:     try {
1:89cfd8e:       model.setDictionaryServerPort(Integer.parseInt(Maps.getOrDefault(options, "dictport", "-1")));
1:89cfd8e:     } catch (NumberFormatException e) {
1:89cfd8e:       throw new InvalidLoadOptionException(e.getMessage());
1:89cfd8e:     }
1:8f08c4a:     validateAndSetColumnCompressor(model);
1:89cfd8e:     return model;
1:89cfd8e:   }
1:280a400: 
1:89cfd8e:   /**
1:89cfd8e:    * build CarbonLoadModel for data loading
1:89cfd8e:    * @param options Load options from user input
1:89cfd8e:    * @param optionsFinal Load options that populated with default values for optional options
1:89cfd8e:    * @param carbonLoadModel The output load model
1:89cfd8e:    * @param hadoopConf hadoopConf is needed to read CSV header if there 'fileheader' is not set in
1:89cfd8e:    *                   user provided load options
1:89cfd8e:    */
1:89cfd8e:   public void build(
1:89cfd8e:       Map<String, String> options,
1:89cfd8e:       Map<String, String> optionsFinal,
1:89cfd8e:       CarbonLoadModel carbonLoadModel,
1:89cfd8e:       Configuration hadoopConf) throws InvalidLoadOptionException, IOException {
1:859d71c:     build(options, optionsFinal, carbonLoadModel, hadoopConf, new HashMap<String, String>(), false);
1:859d71c:   }
1:859d71c: 
1:859d71c:   /**
1:859d71c:    * build CarbonLoadModel for data loading
1:859d71c:    * @param options Load options from user input
1:859d71c:    * @param optionsFinal Load options that populated with default values for optional options
1:859d71c:    * @param carbonLoadModel The output load model
1:859d71c:    * @param hadoopConf hadoopConf is needed to read CSV header if there 'fileheader' is not set in
1:859d71c:    *                   user provided load options
1:859d71c:    * @param partitions partition name map to path
1:859d71c:    * @param isDataFrame true if build for load for dataframe
1:859d71c:    */
1:859d71c:   public void build(
1:859d71c:       Map<String, String> options,
1:859d71c:       Map<String, String> optionsFinal,
1:859d71c:       CarbonLoadModel carbonLoadModel,
1:859d71c:       Configuration hadoopConf,
1:859d71c:       Map<String, String> partitions,
1:859d71c:       boolean isDataFrame) throws InvalidLoadOptionException, IOException {
1:89cfd8e:     carbonLoadModel.setTableName(table.getTableName());
1:89cfd8e:     carbonLoadModel.setDatabaseName(table.getDatabaseName());
1:89cfd8e:     carbonLoadModel.setTablePath(table.getTablePath());
1:89cfd8e:     carbonLoadModel.setTableName(table.getTableName());
1:b7b8073:     carbonLoadModel.setCarbonTransactionalTable(table.isTransactionalTable());
1:89cfd8e:     CarbonDataLoadSchema dataLoadSchema = new CarbonDataLoadSchema(table);
1:89cfd8e:     // Need to fill dimension relation
1:89cfd8e:     carbonLoadModel.setCarbonDataLoadSchema(dataLoadSchema);
1:89cfd8e:     String sort_scope = optionsFinal.get("sort_scope");
1:89cfd8e:     String single_pass = optionsFinal.get("single_pass");
1:89cfd8e:     String bad_records_logger_enable = optionsFinal.get("bad_records_logger_enable");
1:89cfd8e:     String bad_records_action = optionsFinal.get("bad_records_action");
1:89cfd8e:     String bad_record_path = optionsFinal.get("bad_record_path");
1:89cfd8e:     String global_sort_partitions = optionsFinal.get("global_sort_partitions");
1:89cfd8e:     String timestampformat = optionsFinal.get("timestampformat");
1:89cfd8e:     String dateFormat = optionsFinal.get("dateformat");
1:89cfd8e:     String delimeter = optionsFinal.get("delimiter");
1:89cfd8e:     String complex_delimeter_level1 = optionsFinal.get("complex_delimiter_level_1");
1:89cfd8e:     String complex_delimeter_level2 = optionsFinal.get("complex_delimiter_level_2");
1:89cfd8e:     String all_dictionary_path = optionsFinal.get("all_dictionary_path");
1:89cfd8e:     String column_dict = optionsFinal.get("columndict");
1:89cfd8e:     validateDateTimeFormat(timestampformat, "TimestampFormat");
1:89cfd8e:     validateDateTimeFormat(dateFormat, "DateFormat");
1:89cfd8e:     validateSortScope(sort_scope);
1:89cfd8e: 
1:89cfd8e:     if (Boolean.parseBoolean(bad_records_logger_enable) ||
1:89cfd8e:         LoggerAction.REDIRECT.name().equalsIgnoreCase(bad_records_action)) {
1:7a1d12a:       if (!StringUtils.isEmpty(bad_record_path)) {
1:7a1d12a:         bad_record_path = CarbonUtil.checkAndAppendHDFSUrl(bad_record_path);
1:7a1d12a:       } else {
1:7a1d12a:         throw new InvalidLoadOptionException(
1:7a1d12a:             "Cannot redirect bad records as bad record location is not provided.");
1:89cfd8e:       }
1:89cfd8e:     }
1:89cfd8e: 
1:89cfd8e:     carbonLoadModel.setBadRecordsLocation(bad_record_path);
1:89cfd8e: 
1:89cfd8e:     validateGlobalSortPartitions(global_sort_partitions);
1:89cfd8e:     carbonLoadModel.setEscapeChar(checkDefaultValue(optionsFinal.get("escapechar"), "\\"));
1:89cfd8e:     carbonLoadModel.setQuoteChar(checkDefaultValue(optionsFinal.get("quotechar"), "\""));
1:89cfd8e:     carbonLoadModel.setCommentChar(checkDefaultValue(optionsFinal.get("commentchar"), "#"));
1:89cfd8e: 
1:89cfd8e:     // if there isn't file header in csv file and load sql doesn't provide FILEHEADER option,
1:89cfd8e:     // we should use table schema to generate file header.
1:89cfd8e:     String fileHeader = optionsFinal.get("fileheader");
1:89cfd8e:     String headerOption = options.get("header");
1:89cfd8e:     if (headerOption != null) {
1:89cfd8e:       if (!headerOption.equalsIgnoreCase("true") &&
1:89cfd8e:           !headerOption.equalsIgnoreCase("false")) {
1:89cfd8e:         throw new InvalidLoadOptionException(
1:89cfd8e:             "'header' option should be either 'true' or 'false'.");
1:89cfd8e:       }
1:89cfd8e:       // whether the csv file has file header, the default value is true
1:89cfd8e:       if (Boolean.valueOf(headerOption)) {
1:89cfd8e:         if (!StringUtils.isEmpty(fileHeader)) {
1:89cfd8e:           throw new InvalidLoadOptionException(
1:89cfd8e:               "When 'header' option is true, 'fileheader' option is not required.");
1:89cfd8e:         }
1:89cfd8e:       } else {
1:89cfd8e:         if (StringUtils.isEmpty(fileHeader)) {
1:89cfd8e:           List<CarbonColumn> columns = table.getCreateOrderColumn(table.getTableName());
1:89cfd8e:           String[] columnNames = new String[columns.size()];
1:89cfd8e:           for (int i = 0; i < columnNames.length; i++) {
1:89cfd8e:             columnNames[i] = columns.get(i).getColName();
1:89cfd8e:           }
1:89cfd8e:           fileHeader = Strings.mkString(columnNames, ",");
1:89cfd8e:         }
1:89cfd8e:       }
1:89cfd8e:     }
1:89cfd8e: 
1:89cfd8e:     carbonLoadModel.setTimestampformat(timestampformat);
1:89cfd8e:     carbonLoadModel.setDateFormat(dateFormat);
1:89cfd8e:     carbonLoadModel.setDefaultTimestampFormat(
1:89cfd8e:         CarbonProperties.getInstance().getProperty(
1:89cfd8e:             CarbonCommonConstants.CARBON_TIMESTAMP_FORMAT,
1:89cfd8e:             CarbonCommonConstants.CARBON_TIMESTAMP_DEFAULT_FORMAT));
1:89cfd8e: 
1:89cfd8e:     carbonLoadModel.setDefaultDateFormat(
1:89cfd8e:         CarbonProperties.getInstance().getProperty(
1:89cfd8e:             CarbonCommonConstants.CARBON_DATE_FORMAT,
1:89cfd8e:             CarbonCommonConstants.CARBON_DATE_DEFAULT_FORMAT));
1:89cfd8e: 
1:89cfd8e:     carbonLoadModel.setSerializationNullFormat(
1:89cfd8e:         TableOptionConstant.SERIALIZATION_NULL_FORMAT.getName() + "," +
1:89cfd8e:             optionsFinal.get("serialization_null_format"));
1:89cfd8e: 
1:89cfd8e:     carbonLoadModel.setBadRecordsLoggerEnable(
1:89cfd8e:         TableOptionConstant.BAD_RECORDS_LOGGER_ENABLE.getName() + "," + bad_records_logger_enable);
1:89cfd8e: 
1:89cfd8e:     carbonLoadModel.setBadRecordsAction(
1:89cfd8e:         TableOptionConstant.BAD_RECORDS_ACTION.getName() + "," + bad_records_action.toUpperCase());
1:89cfd8e: 
1:89cfd8e:     carbonLoadModel.setIsEmptyDataBadRecord(
1:89cfd8e:         DataLoadProcessorConstants.IS_EMPTY_DATA_BAD_RECORD + "," +
1:89cfd8e:             optionsFinal.get("is_empty_data_bad_record"));
1:89cfd8e: 
1:89cfd8e:     carbonLoadModel.setSkipEmptyLine(optionsFinal.get("skip_empty_line"));
1:89cfd8e: 
1:89cfd8e:     carbonLoadModel.setSortScope(sort_scope);
1:89cfd8e:     carbonLoadModel.setBatchSortSizeInMb(optionsFinal.get("batch_sort_size_inmb"));
1:89cfd8e:     carbonLoadModel.setGlobalSortPartitions(global_sort_partitions);
1:89cfd8e:     carbonLoadModel.setUseOnePass(Boolean.parseBoolean(single_pass));
1:89cfd8e: 
1:89cfd8e:     if (delimeter.equalsIgnoreCase(complex_delimeter_level1) ||
1:89cfd8e:         complex_delimeter_level1.equalsIgnoreCase(complex_delimeter_level2) ||
1:89cfd8e:         delimeter.equalsIgnoreCase(complex_delimeter_level2)) {
1:89cfd8e:       throw new InvalidLoadOptionException("Field Delimiter and Complex types delimiter are same");
1:89cfd8e:     } else {
1:d23f7fa:       carbonLoadModel.setComplexDelimiterLevel1(complex_delimeter_level1);
1:d23f7fa:       carbonLoadModel.setComplexDelimiterLevel2(complex_delimeter_level2);
1:89cfd8e:     }
1:89cfd8e:     // set local dictionary path, and dictionary file extension
1:89cfd8e:     carbonLoadModel.setAllDictPath(all_dictionary_path);
1:89cfd8e:     carbonLoadModel.setCsvDelimiter(CarbonUtil.unescapeChar(delimeter));
1:89cfd8e:     carbonLoadModel.setCsvHeader(fileHeader);
1:89cfd8e:     carbonLoadModel.setColDictFilePath(column_dict);
1:859d71c: 
1:859d71c:     List<String> ignoreColumns = new ArrayList<>();
1:859d71c:     if (!isDataFrame) {
1:859d71c:       for (Map.Entry<String, String> partition : partitions.entrySet()) {
1:859d71c:         if (partition.getValue() != null) {
1:859d71c:           ignoreColumns.add(partition.getKey());
1:859d71c:         }
1:859d71c:       }
1:859d71c:     }
1:859d71c: 
1:89cfd8e:     carbonLoadModel.setCsvHeaderColumns(
1:859d71c:         LoadOption.getCsvHeaderColumns(carbonLoadModel, hadoopConf, ignoreColumns));
1:89cfd8e: 
1:89cfd8e:     int validatedMaxColumns = validateMaxColumns(
1:89cfd8e:         carbonLoadModel.getCsvHeaderColumns(),
1:89cfd8e:         optionsFinal.get("maxcolumns"));
1:89cfd8e: 
1:89cfd8e:     carbonLoadModel.setMaxColumns(String.valueOf(validatedMaxColumns));
1:de0f545:     if (carbonLoadModel.isCarbonTransactionalTable()) {
1:de0f545:       carbonLoadModel.readAndSetLoadMetadataDetails();
1:de0f545:     }
1:d5396b1:     carbonLoadModel.setSortColumnsBoundsStr(optionsFinal.get("sort_column_bounds"));
1:ff03645:     carbonLoadModel.setLoadMinSize(
1:ff03645:         optionsFinal.get(CarbonCommonConstants.CARBON_LOAD_MIN_SIZE_INMB));
1:8f08c4a: 
1:8f08c4a:     validateAndSetColumnCompressor(carbonLoadModel);
1:89cfd8e:   }
1:89cfd8e: 
1:89cfd8e:   private int validateMaxColumns(String[] csvHeaders, String maxColumns)
1:89cfd8e:       throws InvalidLoadOptionException {
1:89cfd8e:     /*
1:89cfd8e:     User configures both csvheadercolumns, maxcolumns,
1:89cfd8e:       if csvheadercolumns >= maxcolumns, give error
1:89cfd8e:       if maxcolumns > threashold, give error
1:89cfd8e:     User configures csvheadercolumns
1:89cfd8e:       if csvheadercolumns >= maxcolumns(default) then maxcolumns = csvheadercolumns+1
1:89cfd8e:       if csvheadercolumns >= threashold, give error
1:89cfd8e:     User configures nothing
1:89cfd8e:       if csvheadercolumns >= maxcolumns(default) then maxcolumns = csvheadercolumns+1
1:89cfd8e:       if csvheadercolumns >= threashold, give error
1:89cfd8e:      */
1:89cfd8e:     int columnCountInSchema = csvHeaders.length;
1:89cfd8e:     int maxNumberOfColumnsForParsing = 0;
1:89cfd8e:     Integer maxColumnsInt = getMaxColumnValue(maxColumns);
1:89cfd8e:     if (maxColumnsInt != null) {
1:89cfd8e:       if (columnCountInSchema >= maxColumnsInt) {
1:89cfd8e:         throw new InvalidLoadOptionException(
1:89cfd8e:             "csv headers should be less than the max columns " + maxColumnsInt);
1:89cfd8e:       } else if (maxColumnsInt > CSVInputFormat.THRESHOLD_MAX_NUMBER_OF_COLUMNS_FOR_PARSING) {
1:89cfd8e:         throw new InvalidLoadOptionException(
1:89cfd8e:             "max columns cannot be greater than the threshold value: " +
1:89cfd8e:                 CSVInputFormat.THRESHOLD_MAX_NUMBER_OF_COLUMNS_FOR_PARSING);
1:89cfd8e:       } else {
1:89cfd8e:         maxNumberOfColumnsForParsing = maxColumnsInt;
1:89cfd8e:       }
1:89cfd8e:     } else if (columnCountInSchema >= CSVInputFormat.THRESHOLD_MAX_NUMBER_OF_COLUMNS_FOR_PARSING) {
1:89cfd8e:       throw new InvalidLoadOptionException(
1:89cfd8e:           "csv header columns should be less than max threashold: " +
1:89cfd8e:               CSVInputFormat.THRESHOLD_MAX_NUMBER_OF_COLUMNS_FOR_PARSING);
1:89cfd8e:     } else if (columnCountInSchema >= CSVInputFormat.DEFAULT_MAX_NUMBER_OF_COLUMNS_FOR_PARSING) {
1:89cfd8e:       maxNumberOfColumnsForParsing = columnCountInSchema + 1;
1:89cfd8e:     } else {
1:89cfd8e:       maxNumberOfColumnsForParsing = CSVInputFormat.DEFAULT_MAX_NUMBER_OF_COLUMNS_FOR_PARSING;
1:89cfd8e:     }
1:89cfd8e:     return maxNumberOfColumnsForParsing;
1:89cfd8e:   }
1:89cfd8e: 
1:89cfd8e:   private Integer getMaxColumnValue(String maxColumn) {
1:89cfd8e:     return (maxColumn == null) ? null : Integer.parseInt(maxColumn);
1:89cfd8e:   }
1:89cfd8e: 
1:89cfd8e:   /**
1:89cfd8e:    * validates both timestamp and date for illegal values
1:89cfd8e:    */
1:89cfd8e:   private void validateDateTimeFormat(String dateTimeLoadFormat, String dateTimeLoadOption)
1:89cfd8e:       throws InvalidLoadOptionException {
1:89cfd8e:     // allowing empty value to be configured for dateformat option.
1:89cfd8e:     if (dateTimeLoadFormat != null && !dateTimeLoadFormat.trim().equalsIgnoreCase("")) {
1:89cfd8e:       try {
1:89cfd8e:         new SimpleDateFormat(dateTimeLoadFormat);
1:89cfd8e:       } catch (IllegalArgumentException e) {
1:89cfd8e:         throw new InvalidLoadOptionException(
1:89cfd8e:             "Error: Wrong option: " + dateTimeLoadFormat + " is provided for option "
1:89cfd8e:                 + dateTimeLoadOption);
1:89cfd8e:       }
1:89cfd8e:     }
1:89cfd8e:   }
1:89cfd8e: 
1:89cfd8e:   private void validateSortScope(String sortScope) throws InvalidLoadOptionException {
1:89cfd8e:     if (sortScope != null) {
1:d23f7fa:       // We support global sort for Hive standard partition, but don't support
1:d23f7fa:       // global sort for other partition type.
1:89cfd8e:       if (table.getPartitionInfo(table.getTableName()) != null &&
1:d23f7fa:           !table.isHivePartitionTable() &&
1:89cfd8e:           sortScope.equalsIgnoreCase(SortScopeOptions.SortScope.GLOBAL_SORT.toString())) {
1:d23f7fa:         throw new InvalidLoadOptionException("Don't support use global sort on "
1:d23f7fa:             + table.getPartitionInfo().getPartitionType() +  " partition table.");
1:89cfd8e:       }
1:89cfd8e:     }
1:89cfd8e:   }
1:89cfd8e: 
1:89cfd8e:   private void validateGlobalSortPartitions(String globalSortPartitions)
1:89cfd8e:       throws InvalidLoadOptionException {
1:89cfd8e:     if (globalSortPartitions != null) {
1:89cfd8e:       try {
1:89cfd8e:         int num = Integer.parseInt(globalSortPartitions);
1:89cfd8e:         if (num <= 0) {
1:89cfd8e:           throw new InvalidLoadOptionException("'GLOBAL_SORT_PARTITIONS' should be greater than 0");
1:89cfd8e:         }
1:89cfd8e:       } catch (NumberFormatException e) {
1:89cfd8e:         throw new InvalidLoadOptionException(e.getMessage());
1:89cfd8e:       }
1:89cfd8e:     }
1:89cfd8e:   }
1:89cfd8e: 
1:8f08c4a:   private void validateAndSetColumnCompressor(CarbonLoadModel carbonLoadModel)
1:8f08c4a:       throws InvalidLoadOptionException {
1:8f08c4a:     try {
1:8f08c4a:       String columnCompressor = carbonLoadModel.getColumnCompressor();
1:8f08c4a:       if (StringUtils.isBlank(columnCompressor)) {
1:8f08c4a:         columnCompressor = CarbonProperties.getInstance().getProperty(
1:8f08c4a:             CarbonCommonConstants.COMPRESSOR, CarbonCommonConstants.DEFAULT_COMPRESSOR);
1:8f08c4a:       }
1:8f08c4a:       // check and load compressor
1:8f08c4a:       CompressorFactory.getInstance().getCompressor(columnCompressor);
1:8f08c4a:       carbonLoadModel.setColumnCompressor(columnCompressor);
1:8f08c4a:     } catch (Exception e) {
1:8f08c4a:       LOGGER.error(e);
1:8f08c4a:       throw new InvalidLoadOptionException("Failed to load the compressor");
1:8f08c4a:     }
1:8f08c4a:   }
1:8f08c4a: 
1:89cfd8e:   /**
1:89cfd8e:    * check whether using default value or not
1:89cfd8e:    */
1:89cfd8e:   private String checkDefaultValue(String value, String defaultValue) {
1:89cfd8e:     if (StringUtils.isEmpty(value)) {
1:89cfd8e:       return defaultValue;
1:89cfd8e:     } else {
1:89cfd8e:       return value;
1:89cfd8e:     }
1:89cfd8e:   }
1:89cfd8e: }
============================================================================
author:xuchuanyin
-------------------------------------------------------------------------------
commit:8f08c4a
/////////////////////////////////////////////////////////////////////////
1: import org.apache.carbondata.common.logging.LogService;
1: import org.apache.carbondata.common.logging.LogServiceFactory;
1: import org.apache.carbondata.core.datastore.compression.CompressorFactory;
/////////////////////////////////////////////////////////////////////////
1:   private static final LogService LOGGER = LogServiceFactory.getLogService(
1:       CarbonLoadModelBuilder.class.getName());
/////////////////////////////////////////////////////////////////////////
1:     validateAndSetColumnCompressor(model);
/////////////////////////////////////////////////////////////////////////
1: 
1:     validateAndSetColumnCompressor(carbonLoadModel);
/////////////////////////////////////////////////////////////////////////
1:   private void validateAndSetColumnCompressor(CarbonLoadModel carbonLoadModel)
1:       throws InvalidLoadOptionException {
1:     try {
1:       String columnCompressor = carbonLoadModel.getColumnCompressor();
1:       if (StringUtils.isBlank(columnCompressor)) {
1:         columnCompressor = CarbonProperties.getInstance().getProperty(
1:             CarbonCommonConstants.COMPRESSOR, CarbonCommonConstants.DEFAULT_COMPRESSOR);
1:       }
1:       // check and load compressor
1:       CompressorFactory.getInstance().getCompressor(columnCompressor);
1:       carbonLoadModel.setColumnCompressor(columnCompressor);
1:     } catch (Exception e) {
1:       LOGGER.error(e);
1:       throw new InvalidLoadOptionException("Failed to load the compressor");
1:     }
1:   }
1: 
commit:859d71c
/////////////////////////////////////////////////////////////////////////
1: import java.util.ArrayList;
1: import java.util.HashMap;
/////////////////////////////////////////////////////////////////////////
1:     build(options, optionsFinal, carbonLoadModel, hadoopConf, new HashMap<String, String>(), false);
1:   }
1: 
1:   /**
1:    * build CarbonLoadModel for data loading
1:    * @param options Load options from user input
1:    * @param optionsFinal Load options that populated with default values for optional options
1:    * @param carbonLoadModel The output load model
1:    * @param hadoopConf hadoopConf is needed to read CSV header if there 'fileheader' is not set in
1:    *                   user provided load options
1:    * @param partitions partition name map to path
1:    * @param isDataFrame true if build for load for dataframe
1:    */
1:   public void build(
1:       Map<String, String> options,
1:       Map<String, String> optionsFinal,
1:       CarbonLoadModel carbonLoadModel,
1:       Configuration hadoopConf,
1:       Map<String, String> partitions,
1:       boolean isDataFrame) throws InvalidLoadOptionException, IOException {
/////////////////////////////////////////////////////////////////////////
1: 
1:     List<String> ignoreColumns = new ArrayList<>();
1:     if (!isDataFrame) {
1:       for (Map.Entry<String, String> partition : partitions.entrySet()) {
1:         if (partition.getValue() != null) {
1:           ignoreColumns.add(partition.getKey());
1:         }
1:       }
1:     }
1: 
1:         LoadOption.getCsvHeaderColumns(carbonLoadModel, hadoopConf, ignoreColumns));
commit:d5396b1
/////////////////////////////////////////////////////////////////////////
1:     carbonLoadModel.setSortColumnsBoundsStr(optionsFinal.get("sort_column_bounds"));
author:ajantha-bhat
-------------------------------------------------------------------------------
commit:f5c7a19
/////////////////////////////////////////////////////////////////////////
1:     optionsFinal.put("sort_scope",
1:         Maps.getOrDefault(options, "sort_scope", CarbonCommonConstants.LOAD_SORT_SCOPE_DEFAULT));
commit:280a400
/////////////////////////////////////////////////////////////////////////
0:       Map<String, String> options, long UUID) throws InvalidLoadOptionException, IOException {
/////////////////////////////////////////////////////////////////////////
0:     model.setCarbonUnmanagedTable(table.isUnManagedTable());
1:     model.setFactTimeStamp(UUID);
1: 
author:ravipesala
-------------------------------------------------------------------------------
commit:de0f545
/////////////////////////////////////////////////////////////////////////
1:     if (carbonLoadModel.isCarbonTransactionalTable()) {
1:       carbonLoadModel.readAndSetLoadMetadataDetails();
1:     }
author:kunal642
-------------------------------------------------------------------------------
commit:7a1d12a
/////////////////////////////////////////////////////////////////////////
1: import org.apache.carbondata.processing.util.CarbonBadRecordUtil;
/////////////////////////////////////////////////////////////////////////
1:   public CarbonLoadModel build(Map<String, String>  options, long UUID, String taskNo)
/////////////////////////////////////////////////////////////////////////
1:     optionsFinal.put("bad_record_path", CarbonBadRecordUtil.getBadRecordsPath(options, table));
/////////////////////////////////////////////////////////////////////////
1:       if (!StringUtils.isEmpty(bad_record_path)) {
1:         bad_record_path = CarbonUtil.checkAndAppendHDFSUrl(bad_record_path);
1:       } else {
1:         throw new InvalidLoadOptionException(
1:             "Cannot redirect bad records as bad record location is not provided.");
commit:6b70b7e
/////////////////////////////////////////////////////////////////////////
1:     String timestampFormat = options.get("timestampformat");
1:     if (timestampFormat == null) {
1:       timestampFormat = CarbonProperties.getInstance()
1:           .getProperty(CarbonCommonConstants.CARBON_TIMESTAMP_FORMAT,
1:               CarbonCommonConstants.CARBON_TIMESTAMP_DEFAULT_FORMAT);
1:     }
1:     String dateFormat = options.get("dateFormat");
1:     if (dateFormat == null) {
1:       dateFormat = CarbonProperties.getInstance()
1:           .getProperty(CarbonCommonConstants.CARBON_DATE_FORMAT,
1:               CarbonCommonConstants.CARBON_DATE_DEFAULT_FORMAT);
1:     }
1:     model.setDateFormat(dateFormat);
1:     model.setTimestampformat(timestampFormat);
author:Zhang Zhichao
-------------------------------------------------------------------------------
commit:ff03645
/////////////////////////////////////////////////////////////////////////
1:     carbonLoadModel.setLoadMinSize(
1:         optionsFinal.get(CarbonCommonConstants.CARBON_LOAD_MIN_SIZE_INMB));
author:ndwangsen
-------------------------------------------------------------------------------
commit:685087e
/////////////////////////////////////////////////////////////////////////
0:     carbonLoadModel.setLoadMinSize(optionsFinal.get(CarbonCommonConstants.CARBON_LOAD_MIN_SIZE_INMB));
author:BJangir
-------------------------------------------------------------------------------
commit:b2060c6
/////////////////////////////////////////////////////////////////////////
author:sounakr
-------------------------------------------------------------------------------
commit:b7b8073
/////////////////////////////////////////////////////////////////////////
1:    * @param taskNo
0:   public CarbonLoadModel build(Map<String, String> options, long UUID, String taskNo)
1:       throws InvalidLoadOptionException, IOException {
/////////////////////////////////////////////////////////////////////////
1:     model.setCarbonTransactionalTable(table.isTransactionalTable());
1:     model.setTaskNo(taskNo);
/////////////////////////////////////////////////////////////////////////
1:     carbonLoadModel.setCarbonTransactionalTable(table.isTransactionalTable());
author:Jacky Li
-------------------------------------------------------------------------------
commit:6cb6f83
/////////////////////////////////////////////////////////////////////////
1: 
commit:e72bfd1
/////////////////////////////////////////////////////////////////////////
1: @InterfaceAudience.Internal
commit:89cfd8e
/////////////////////////////////////////////////////////////////////////
1: /*
1:  * Licensed to the Apache Software Foundation (ASF) under one or more
1:  * contributor license agreements.  See the NOTICE file distributed with
1:  * this work for additional information regarding copyright ownership.
1:  * The ASF licenses this file to You under the Apache License, Version 2.0
1:  * (the "License"); you may not use this file except in compliance with
1:  * the License.  You may obtain a copy of the License at
1:  *
1:  *    http://www.apache.org/licenses/LICENSE-2.0
1:  *
1:  * Unless required by applicable law or agreed to in writing, software
1:  * distributed under the License is distributed on an "AS IS" BASIS,
1:  * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
1:  * See the License for the specific language governing permissions and
1:  * limitations under the License.
1:  */
1: 
1: package org.apache.carbondata.processing.loading.model;
1: 
1: import java.io.IOException;
1: import java.text.SimpleDateFormat;
1: import java.util.List;
1: import java.util.Map;
1: 
1: import org.apache.carbondata.common.Maps;
1: import org.apache.carbondata.common.Strings;
1: import org.apache.carbondata.common.annotations.InterfaceAudience;
1: import org.apache.carbondata.common.constants.LoggerAction;
1: import org.apache.carbondata.common.exceptions.sql.InvalidLoadOptionException;
1: import org.apache.carbondata.core.constants.CarbonCommonConstants;
1: import org.apache.carbondata.core.metadata.schema.table.CarbonTable;
1: import org.apache.carbondata.core.metadata.schema.table.column.CarbonColumn;
1: import org.apache.carbondata.core.util.CarbonProperties;
1: import org.apache.carbondata.core.util.CarbonUtil;
1: import org.apache.carbondata.processing.loading.constants.DataLoadProcessorConstants;
1: import org.apache.carbondata.processing.loading.csvinput.CSVInputFormat;
1: import org.apache.carbondata.processing.loading.sort.SortScopeOptions;
1: import org.apache.carbondata.processing.util.TableOptionConstant;
1: 
1: import org.apache.commons.lang.StringUtils;
1: import org.apache.hadoop.conf.Configuration;
1: 
1: /**
1:  * Builder for {@link CarbonLoadModel}
1:  */
0: @InterfaceAudience.Developer
1: public class CarbonLoadModelBuilder {
1: 
1:   private CarbonTable table;
1: 
1:   public CarbonLoadModelBuilder(CarbonTable table) {
1:     this.table = table;
1:   }
1: 
1:   /**
1:    * build CarbonLoadModel for data loading
1:    * @param options Load options from user input
1:    * @return a new CarbonLoadModel instance
1:    */
0:   public CarbonLoadModel build(
0:       Map<String, String> options) throws InvalidLoadOptionException, IOException {
1:     Map<String, String> optionsFinal = LoadOption.fillOptionWithDefaultValue(options);
0:     optionsFinal.put("sort_scope", "no_sort");
1:     if (!options.containsKey("fileheader")) {
1:       List<CarbonColumn> csvHeader = table.getCreateOrderColumn(table.getTableName());
1:       String[] columns = new String[csvHeader.size()];
1:       for (int i = 0; i < columns.length; i++) {
1:         columns[i] = csvHeader.get(i).getColName();
1:       }
1:       optionsFinal.put("fileheader", Strings.mkString(columns, ","));
1:     }
1:     CarbonLoadModel model = new CarbonLoadModel();
1: 
1:     // we have provided 'fileheader', so it hadoopConf can be null
1:     build(options, optionsFinal, model, null);
1: 
0:     // set default values
0:     model.setTimestampformat(CarbonCommonConstants.CARBON_TIMESTAMP_DEFAULT_FORMAT);
0:     model.setDateFormat(CarbonCommonConstants.CARBON_DATE_DEFAULT_FORMAT);
1:     model.setUseOnePass(Boolean.parseBoolean(Maps.getOrDefault(options, "onepass", "false")));
1:     model.setDictionaryServerHost(Maps.getOrDefault(options, "dicthost", null));
1:     try {
1:       model.setDictionaryServerPort(Integer.parseInt(Maps.getOrDefault(options, "dictport", "-1")));
1:     } catch (NumberFormatException e) {
1:       throw new InvalidLoadOptionException(e.getMessage());
1:     }
1:     return model;
1:   }
1: 
1:   /**
1:    * build CarbonLoadModel for data loading
1:    * @param options Load options from user input
1:    * @param optionsFinal Load options that populated with default values for optional options
1:    * @param carbonLoadModel The output load model
1:    * @param hadoopConf hadoopConf is needed to read CSV header if there 'fileheader' is not set in
1:    *                   user provided load options
1:    */
1:   public void build(
1:       Map<String, String> options,
1:       Map<String, String> optionsFinal,
1:       CarbonLoadModel carbonLoadModel,
1:       Configuration hadoopConf) throws InvalidLoadOptionException, IOException {
1:     carbonLoadModel.setTableName(table.getTableName());
1:     carbonLoadModel.setDatabaseName(table.getDatabaseName());
1:     carbonLoadModel.setTablePath(table.getTablePath());
1:     carbonLoadModel.setTableName(table.getTableName());
1:     CarbonDataLoadSchema dataLoadSchema = new CarbonDataLoadSchema(table);
1:     // Need to fill dimension relation
1:     carbonLoadModel.setCarbonDataLoadSchema(dataLoadSchema);
1:     String sort_scope = optionsFinal.get("sort_scope");
1:     String single_pass = optionsFinal.get("single_pass");
1:     String bad_records_logger_enable = optionsFinal.get("bad_records_logger_enable");
1:     String bad_records_action = optionsFinal.get("bad_records_action");
1:     String bad_record_path = optionsFinal.get("bad_record_path");
1:     String global_sort_partitions = optionsFinal.get("global_sort_partitions");
1:     String timestampformat = optionsFinal.get("timestampformat");
1:     String dateFormat = optionsFinal.get("dateformat");
1:     String delimeter = optionsFinal.get("delimiter");
1:     String complex_delimeter_level1 = optionsFinal.get("complex_delimiter_level_1");
1:     String complex_delimeter_level2 = optionsFinal.get("complex_delimiter_level_2");
1:     String all_dictionary_path = optionsFinal.get("all_dictionary_path");
1:     String column_dict = optionsFinal.get("columndict");
1:     validateDateTimeFormat(timestampformat, "TimestampFormat");
1:     validateDateTimeFormat(dateFormat, "DateFormat");
1:     validateSortScope(sort_scope);
1: 
1:     if (Boolean.parseBoolean(bad_records_logger_enable) ||
1:         LoggerAction.REDIRECT.name().equalsIgnoreCase(bad_records_action)) {
0:       bad_record_path = CarbonUtil.checkAndAppendHDFSUrl(bad_record_path);
0:       if (!CarbonUtil.isValidBadStorePath(bad_record_path)) {
0:         throw new InvalidLoadOptionException("Invalid bad records location.");
1:       }
1:     }
1:     carbonLoadModel.setBadRecordsLocation(bad_record_path);
1: 
1:     validateGlobalSortPartitions(global_sort_partitions);
1:     carbonLoadModel.setEscapeChar(checkDefaultValue(optionsFinal.get("escapechar"), "\\"));
1:     carbonLoadModel.setQuoteChar(checkDefaultValue(optionsFinal.get("quotechar"), "\""));
1:     carbonLoadModel.setCommentChar(checkDefaultValue(optionsFinal.get("commentchar"), "#"));
1: 
1:     // if there isn't file header in csv file and load sql doesn't provide FILEHEADER option,
1:     // we should use table schema to generate file header.
1:     String fileHeader = optionsFinal.get("fileheader");
1:     String headerOption = options.get("header");
1:     if (headerOption != null) {
1:       if (!headerOption.equalsIgnoreCase("true") &&
1:           !headerOption.equalsIgnoreCase("false")) {
1:         throw new InvalidLoadOptionException(
1:             "'header' option should be either 'true' or 'false'.");
1:       }
1:       // whether the csv file has file header, the default value is true
1:       if (Boolean.valueOf(headerOption)) {
1:         if (!StringUtils.isEmpty(fileHeader)) {
1:           throw new InvalidLoadOptionException(
1:               "When 'header' option is true, 'fileheader' option is not required.");
1:         }
1:       } else {
1:         if (StringUtils.isEmpty(fileHeader)) {
1:           List<CarbonColumn> columns = table.getCreateOrderColumn(table.getTableName());
1:           String[] columnNames = new String[columns.size()];
1:           for (int i = 0; i < columnNames.length; i++) {
1:             columnNames[i] = columns.get(i).getColName();
1:           }
1:           fileHeader = Strings.mkString(columnNames, ",");
1:         }
1:       }
1:     }
1: 
1:     carbonLoadModel.setTimestampformat(timestampformat);
1:     carbonLoadModel.setDateFormat(dateFormat);
1:     carbonLoadModel.setDefaultTimestampFormat(
1:         CarbonProperties.getInstance().getProperty(
1:             CarbonCommonConstants.CARBON_TIMESTAMP_FORMAT,
1:             CarbonCommonConstants.CARBON_TIMESTAMP_DEFAULT_FORMAT));
1: 
1:     carbonLoadModel.setDefaultDateFormat(
1:         CarbonProperties.getInstance().getProperty(
1:             CarbonCommonConstants.CARBON_DATE_FORMAT,
1:             CarbonCommonConstants.CARBON_DATE_DEFAULT_FORMAT));
1: 
1:     carbonLoadModel.setSerializationNullFormat(
1:         TableOptionConstant.SERIALIZATION_NULL_FORMAT.getName() + "," +
1:             optionsFinal.get("serialization_null_format"));
1: 
1:     carbonLoadModel.setBadRecordsLoggerEnable(
1:         TableOptionConstant.BAD_RECORDS_LOGGER_ENABLE.getName() + "," + bad_records_logger_enable);
1: 
1:     carbonLoadModel.setBadRecordsAction(
1:         TableOptionConstant.BAD_RECORDS_ACTION.getName() + "," + bad_records_action.toUpperCase());
1: 
1:     carbonLoadModel.setIsEmptyDataBadRecord(
1:         DataLoadProcessorConstants.IS_EMPTY_DATA_BAD_RECORD + "," +
1:             optionsFinal.get("is_empty_data_bad_record"));
1: 
1:     carbonLoadModel.setSkipEmptyLine(optionsFinal.get("skip_empty_line"));
1: 
1:     carbonLoadModel.setSortScope(sort_scope);
1:     carbonLoadModel.setBatchSortSizeInMb(optionsFinal.get("batch_sort_size_inmb"));
1:     carbonLoadModel.setGlobalSortPartitions(global_sort_partitions);
1:     carbonLoadModel.setUseOnePass(Boolean.parseBoolean(single_pass));
1: 
1:     if (delimeter.equalsIgnoreCase(complex_delimeter_level1) ||
1:         complex_delimeter_level1.equalsIgnoreCase(complex_delimeter_level2) ||
1:         delimeter.equalsIgnoreCase(complex_delimeter_level2)) {
1:       throw new InvalidLoadOptionException("Field Delimiter and Complex types delimiter are same");
1:     } else {
0:       carbonLoadModel.setComplexDelimiterLevel1(
0:           CarbonUtil.delimiterConverter(complex_delimeter_level1));
0:       carbonLoadModel.setComplexDelimiterLevel2(
0:           CarbonUtil.delimiterConverter(complex_delimeter_level2));
1:     }
1:     // set local dictionary path, and dictionary file extension
1:     carbonLoadModel.setAllDictPath(all_dictionary_path);
1:     carbonLoadModel.setCsvDelimiter(CarbonUtil.unescapeChar(delimeter));
1:     carbonLoadModel.setCsvHeader(fileHeader);
1:     carbonLoadModel.setColDictFilePath(column_dict);
1:     carbonLoadModel.setCsvHeaderColumns(
0:         LoadOption.getCsvHeaderColumns(carbonLoadModel, hadoopConf));
1: 
1:     int validatedMaxColumns = validateMaxColumns(
1:         carbonLoadModel.getCsvHeaderColumns(),
1:         optionsFinal.get("maxcolumns"));
1: 
1:     carbonLoadModel.setMaxColumns(String.valueOf(validatedMaxColumns));
0:     carbonLoadModel.readAndSetLoadMetadataDetails();
1:   }
1: 
1:   private int validateMaxColumns(String[] csvHeaders, String maxColumns)
1:       throws InvalidLoadOptionException {
1:     /*
1:     User configures both csvheadercolumns, maxcolumns,
1:       if csvheadercolumns >= maxcolumns, give error
1:       if maxcolumns > threashold, give error
1:     User configures csvheadercolumns
1:       if csvheadercolumns >= maxcolumns(default) then maxcolumns = csvheadercolumns+1
1:       if csvheadercolumns >= threashold, give error
1:     User configures nothing
1:       if csvheadercolumns >= maxcolumns(default) then maxcolumns = csvheadercolumns+1
1:       if csvheadercolumns >= threashold, give error
1:      */
1:     int columnCountInSchema = csvHeaders.length;
1:     int maxNumberOfColumnsForParsing = 0;
1:     Integer maxColumnsInt = getMaxColumnValue(maxColumns);
1:     if (maxColumnsInt != null) {
1:       if (columnCountInSchema >= maxColumnsInt) {
1:         throw new InvalidLoadOptionException(
1:             "csv headers should be less than the max columns " + maxColumnsInt);
1:       } else if (maxColumnsInt > CSVInputFormat.THRESHOLD_MAX_NUMBER_OF_COLUMNS_FOR_PARSING) {
1:         throw new InvalidLoadOptionException(
1:             "max columns cannot be greater than the threshold value: " +
1:                 CSVInputFormat.THRESHOLD_MAX_NUMBER_OF_COLUMNS_FOR_PARSING);
1:       } else {
1:         maxNumberOfColumnsForParsing = maxColumnsInt;
1:       }
1:     } else if (columnCountInSchema >= CSVInputFormat.THRESHOLD_MAX_NUMBER_OF_COLUMNS_FOR_PARSING) {
1:       throw new InvalidLoadOptionException(
1:           "csv header columns should be less than max threashold: " +
1:               CSVInputFormat.THRESHOLD_MAX_NUMBER_OF_COLUMNS_FOR_PARSING);
1:     } else if (columnCountInSchema >= CSVInputFormat.DEFAULT_MAX_NUMBER_OF_COLUMNS_FOR_PARSING) {
1:       maxNumberOfColumnsForParsing = columnCountInSchema + 1;
1:     } else {
1:       maxNumberOfColumnsForParsing = CSVInputFormat.DEFAULT_MAX_NUMBER_OF_COLUMNS_FOR_PARSING;
1:     }
1:     return maxNumberOfColumnsForParsing;
1:   }
1: 
1:   private Integer getMaxColumnValue(String maxColumn) {
1:     return (maxColumn == null) ? null : Integer.parseInt(maxColumn);
1:   }
1: 
1:   /**
1:    * validates both timestamp and date for illegal values
1:    */
1:   private void validateDateTimeFormat(String dateTimeLoadFormat, String dateTimeLoadOption)
1:       throws InvalidLoadOptionException {
1:     // allowing empty value to be configured for dateformat option.
1:     if (dateTimeLoadFormat != null && !dateTimeLoadFormat.trim().equalsIgnoreCase("")) {
1:       try {
1:         new SimpleDateFormat(dateTimeLoadFormat);
1:       } catch (IllegalArgumentException e) {
1:         throw new InvalidLoadOptionException(
1:             "Error: Wrong option: " + dateTimeLoadFormat + " is provided for option "
1:                 + dateTimeLoadOption);
1:       }
1:     }
1:   }
1: 
1:   private void validateSortScope(String sortScope) throws InvalidLoadOptionException {
1:     if (sortScope != null) {
0:       // Don't support use global sort on partitioned table.
1:       if (table.getPartitionInfo(table.getTableName()) != null &&
1:           sortScope.equalsIgnoreCase(SortScopeOptions.SortScope.GLOBAL_SORT.toString())) {
0:         throw new InvalidLoadOptionException("Don't support use global sort on partitioned table.");
1:       }
1:     }
1:   }
1: 
1:   private void validateGlobalSortPartitions(String globalSortPartitions)
1:       throws InvalidLoadOptionException {
1:     if (globalSortPartitions != null) {
1:       try {
1:         int num = Integer.parseInt(globalSortPartitions);
1:         if (num <= 0) {
1:           throw new InvalidLoadOptionException("'GLOBAL_SORT_PARTITIONS' should be greater than 0");
1:         }
1:       } catch (NumberFormatException e) {
1:         throw new InvalidLoadOptionException(e.getMessage());
1:       }
1:     }
1:   }
1: 
1:   /**
1:    * check whether using default value or not
1:    */
1:   private String checkDefaultValue(String value, String defaultValue) {
1:     if (StringUtils.isEmpty(value)) {
1:       return defaultValue;
1:     } else {
1:       return value;
1:     }
1:   }
1: }
author:QiangCai
-------------------------------------------------------------------------------
commit:d23f7fa
/////////////////////////////////////////////////////////////////////////
0:       bad_record_path = CarbonUtil.checkAndAppendHDFSUrl(bad_record_path);
0: 
/////////////////////////////////////////////////////////////////////////
1:       carbonLoadModel.setComplexDelimiterLevel1(complex_delimeter_level1);
1:       carbonLoadModel.setComplexDelimiterLevel2(complex_delimeter_level2);
/////////////////////////////////////////////////////////////////////////
1:       // We support global sort for Hive standard partition, but don't support
1:       // global sort for other partition type.
1:           !table.isHivePartitionTable() &&
1:         throw new InvalidLoadOptionException("Don't support use global sort on "
1:             + table.getPartitionInfo().getPartitionType() +  " partition table.");
============================================================================