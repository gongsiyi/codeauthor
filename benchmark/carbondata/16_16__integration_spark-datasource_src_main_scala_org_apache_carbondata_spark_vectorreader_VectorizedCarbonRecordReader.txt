1:376d69f: /*
1:376d69f:  * Licensed to the Apache Software Foundation (ASF) under one or more
1:376d69f:  * contributor license agreements.  See the NOTICE file distributed with
1:376d69f:  * this work for additional information regarding copyright ownership.
1:376d69f:  * The ASF licenses this file to You under the Apache License, Version 2.0
1:376d69f:  * (the "License"); you may not use this file except in compliance with
1:376d69f:  * the License.  You may obtain a copy of the License at
1:376d69f:  *
1:376d69f:  *    http://www.apache.org/licenses/LICENSE-2.0
1:376d69f:  *
1:376d69f:  * Unless required by applicable law or agreed to in writing, software
1:376d69f:  * distributed under the License is distributed on an "AS IS" BASIS,
1:376d69f:  * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
1:376d69f:  * See the License for the specific language governing permissions and
1:376d69f:  * limitations under the License.
1:376d69f:  */
1:376d69f: 
1:376d69f: package org.apache.carbondata.spark.vectorreader;
1:376d69f: 
1:a31598e: import java.io.FileNotFoundException;
1:376d69f: import java.io.IOException;
1:376d69f: import java.util.ArrayList;
1:376d69f: import java.util.List;
1:376d69f: import java.util.Map;
1:376d69f: 
1:c5e72a4: import org.apache.carbondata.common.logging.LogService;
1:c5e72a4: import org.apache.carbondata.common.logging.LogServiceFactory;
1:376d69f: import org.apache.carbondata.core.cache.dictionary.Dictionary;
1:ce09aaa: import org.apache.carbondata.core.datastore.block.TableBlockInfo;
1:376d69f: import org.apache.carbondata.core.keygenerator.directdictionary.DirectDictionaryGenerator;
1:376d69f: import org.apache.carbondata.core.keygenerator.directdictionary.DirectDictionaryKeyGeneratorFactory;
1:ce09aaa: import org.apache.carbondata.core.metadata.datatype.DataType;
1:956833e: import org.apache.carbondata.core.metadata.datatype.DataTypes;
1:ce09aaa: import org.apache.carbondata.core.metadata.encoder.Encoding;
1:ce09aaa: import org.apache.carbondata.core.scan.executor.QueryExecutor;
1:ce09aaa: import org.apache.carbondata.core.scan.executor.QueryExecutorFactory;
1:ce09aaa: import org.apache.carbondata.core.scan.executor.exception.QueryExecutionException;
1:daa6465: import org.apache.carbondata.core.scan.model.ProjectionDimension;
1:daa6465: import org.apache.carbondata.core.scan.model.ProjectionMeasure;
1:ce09aaa: import org.apache.carbondata.core.scan.model.QueryModel;
1:ce09aaa: import org.apache.carbondata.core.scan.result.iterator.AbstractDetailQueryResultIterator;
1:ce09aaa: import org.apache.carbondata.core.scan.result.vector.CarbonColumnVector;
1:ce09aaa: import org.apache.carbondata.core.scan.result.vector.CarbonColumnarBatch;
1:376d69f: import org.apache.carbondata.core.util.CarbonUtil;
1:ec2d742: import org.apache.carbondata.hadoop.AbstractRecordReader;
1:376d69f: import org.apache.carbondata.hadoop.CarbonInputSplit;
1:376d69f: import org.apache.carbondata.hadoop.CarbonMultiBlockSplit;
1:e3f98fa: import org.apache.carbondata.hadoop.InputMetricsStats;
1:376d69f: 
1:7ef9164: import org.apache.commons.lang3.exception.ExceptionUtils;
1:376d69f: import org.apache.hadoop.mapreduce.InputSplit;
1:376d69f: import org.apache.hadoop.mapreduce.TaskAttemptContext;
1:376d69f: import org.apache.spark.memory.MemoryMode;
1:347b8e1: import org.apache.spark.sql.carbondata.execution.datasources.CarbonSparkDataSourceUtil;
1:347b8e1: import org.apache.spark.sql.catalyst.InternalRow;
1:347b8e1: import org.apache.spark.sql.execution.vectorized.ColumnVectorUtils;
1:bcef656: import org.apache.spark.sql.CarbonVectorProxy;
1:376d69f: import org.apache.spark.sql.types.DecimalType;
1:376d69f: import org.apache.spark.sql.types.StructField;
1:74c3eb1: import org.apache.spark.sql.types.StructType;
1:376d69f: 
1:376d69f: /**
1:376d69f:  * A specialized RecordReader that reads into InternalRows or ColumnarBatches directly using the
1:376d69f:  * carbondata column APIs and fills the data directly into columns.
1:376d69f:  */
1:347b8e1: public class VectorizedCarbonRecordReader extends AbstractRecordReader<Object> {
1:376d69f: 
1:c5e72a4:   private static final LogService LOGGER =
1:c5e72a4:       LogServiceFactory.getLogService(VectorizedCarbonRecordReader.class.getName());
1:9a25dc6: 
1:376d69f:   private int batchIdx = 0;
1:376d69f: 
1:bcef656:   private static final int DEFAULT_BATCH_SIZE = 4 * 1024;
1:bcef656: 
1:376d69f:   private int numBatched = 0;
1:376d69f: 
1:bcef656:   private CarbonVectorProxy vectorProxy;
1:376d69f: 
1:376d69f:   private CarbonColumnarBatch carbonColumnarBatch;
1:376d69f: 
1:376d69f:   /**
1:376d69f:    * If true, this class returns batches instead of rows.
1:376d69f:    */
1:376d69f:   private boolean returnColumnarBatch;
1:376d69f: 
1:3a4b881:   private boolean[] isNoDictStringField;
1:3a4b881: 
1:376d69f:   /**
1:e9c24c5:    * The default config on whether columnarBatch should be onheap.
1:376d69f:    */
1:376d69f:   private static final MemoryMode DEFAULT_MEMORY_MODE = MemoryMode.ON_HEAP;
1:376d69f: 
1:376d69f:   private QueryModel queryModel;
1:376d69f: 
1:376d69f:   private AbstractDetailQueryResultIterator iterator;
1:376d69f: 
1:376d69f:   private QueryExecutor queryExecutor;
1:376d69f: 
1:e3f98fa:   private InputMetricsStats inputMetricsStats;
1:c5e72a4: 
1:9a25dc6:   public VectorizedCarbonRecordReader(QueryModel queryModel, InputMetricsStats inputMetricsStats,
1:9a25dc6:       String enableBatch) {
1:376d69f:     this.queryModel = queryModel;
1:e3f98fa:     this.inputMetricsStats = inputMetricsStats;
1:9a25dc6:     if (enableBatch.equals("true")) {
1:376d69f:       enableReturningBatches();
1:376d69f:     }
1:3a4b881:   }
1:376d69f: 
1:9a25dc6: 
1:376d69f:   /*
1:376d69f:  * Can be called before any rows are returned to enable returning columnar batches directly.
1:376d69f:  */
1:376d69f:   public void enableReturningBatches() {
1:376d69f:     returnColumnarBatch = true;
1:3a4b881:   }
1:9a25dc6: 
1:376d69f:   /**
1:376d69f:    * Implementation of RecordReader API.
1:376d69f:    */
1:daa6465:   @Override
1:daa6465:   public void initialize(InputSplit inputSplit, TaskAttemptContext taskAttemptContext)
1:376d69f:       throws IOException, InterruptedException, UnsupportedOperationException {
1:376d69f:     // The input split can contain single HDFS block or multiple blocks, so firstly get all the
1:376d69f:     // blocks and then set them in the query model.
1:376d69f:     List<CarbonInputSplit> splitList;
1:376d69f:     if (inputSplit instanceof CarbonInputSplit) {
1:376d69f:       splitList = new ArrayList<>(1);
1:376d69f:       splitList.add((CarbonInputSplit) inputSplit);
1:376d69f:     } else if (inputSplit instanceof CarbonMultiBlockSplit) {
1:376d69f:       // contains multiple blocks, this is an optimization for concurrent query.
1:376d69f:       CarbonMultiBlockSplit multiBlockSplit = (CarbonMultiBlockSplit) inputSplit;
1:376d69f:       splitList = multiBlockSplit.getAllSplits();
1:376d69f:     } else {
1:376d69f:       throw new RuntimeException("unsupported input split type: " + inputSplit);
1:376d69f:     }
1:376d69f:     List<TableBlockInfo> tableBlockInfoList = CarbonInputSplit.createBlocks(splitList);
1:376d69f:     queryModel.setTableBlockInfos(tableBlockInfoList);
1:376d69f:     queryModel.setVectorReader(true);
1:376d69f:     try {
1:2a9604c:       queryExecutor =
1:2a9604c:           QueryExecutorFactory.getQueryExecutor(queryModel, taskAttemptContext.getConfiguration());
1:376d69f:       iterator = (AbstractDetailQueryResultIterator) queryExecutor.execute(queryModel);
1:376d69f:     } catch (QueryExecutionException e) {
1:7ef9164:       if (ExceptionUtils.indexOfThrowable(e, FileNotFoundException.class) > 0) {
1:7ef9164:         LOGGER.error(e);
1:7ef9164:         throw new InterruptedException(
1:7ef9164:             "Insert overwrite may be in progress.Please check " + e.getMessage());
1:638ed1f:       }
1:376d69f:       throw new InterruptedException(e.getMessage());
1:c5e72a4:     } catch (Exception e) {
1:7ef9164:       if (ExceptionUtils.indexOfThrowable(e, FileNotFoundException.class) > 0) {
1:7ef9164:         LOGGER.error(e);
1:7ef9164:         throw new InterruptedException(
1:7ef9164:             "Insert overwrite may be in progress.Please check " + e.getMessage());
1:376d69f:       }
1:c5e72a4:       throw e;
1:376d69f:     }
1:376d69f:   }
1:376d69f: 
1:daa6465:   @Override
1:daa6465:   public void close() throws IOException {
1:ec2d742:     logStatistics(rowCount, queryModel.getStatisticsRecorder());
1:bcef656:     if (vectorProxy != null) {
1:bcef656:       vectorProxy.close();
1:bcef656:       vectorProxy = null;
1:9a25dc6:     }
1:376d69f:     // clear dictionary cache
1:376d69f:     Map<String, Dictionary> columnToDictionaryMapping = queryModel.getColumnToDictionaryMapping();
1:376d69f:     if (null != columnToDictionaryMapping) {
1:376d69f:       for (Map.Entry<String, Dictionary> entry : columnToDictionaryMapping.entrySet()) {
1:376d69f:         CarbonUtil.clearDictionaryCache(entry.getValue());
1:376d69f:       }
1:376d69f:     }
1:376d69f:     try {
1:376d69f:       queryExecutor.finish();
1:376d69f:     } catch (QueryExecutionException e) {
1:376d69f:       throw new IOException(e);
1:376d69f:     }
1:376d69f:   }
1:376d69f: 
1:daa6465:   @Override
1:daa6465:   public boolean nextKeyValue() throws IOException, InterruptedException {
1:376d69f:     resultBatch();
1:376d69f: 
1:daa6465:     if (returnColumnarBatch) {
1:daa6465:       return nextBatch();
1:9a25dc6:     }
1:376d69f: 
1:376d69f:     if (batchIdx >= numBatched) {
1:376d69f:       if (!nextBatch()) return false;
1:376d69f:     }
1:376d69f:     ++batchIdx;
1:376d69f:     return true;
1:376d69f:   }
1:376d69f: 
1:daa6465:   @Override
1:daa6465:   public Object getCurrentValue() throws IOException, InterruptedException {
1:ec2d742:     if (returnColumnarBatch) {
1:bcef656:       int value = vectorProxy.numRows();
1:e3f98fa:       rowCount += value;
1:77217b3:       if (inputMetricsStats != null) {
1:77217b3:         inputMetricsStats.incrementRecordRead((long) value);
1:376d69f:       }
1:bcef656:       return vectorProxy.getColumnarBatch();
1:daa6465:     }
1:ec2d742:     rowCount += 1;
1:bcef656:     return vectorProxy.getRow(batchIdx - 1);
1:77217b3:   }
1:376d69f: 
1:daa6465:   @Override
1:daa6465:   public Void getCurrentKey() throws IOException, InterruptedException {
1:376d69f:     return null;
1:376d69f:   }
1:376d69f: 
1:daa6465:   @Override
1:daa6465:   public float getProgress() throws IOException, InterruptedException {
1:376d69f:     // TODO : Implement it based on total number of rows it is going to retrive.
1:376d69f:     return 0;
1:376d69f:   }
1:376d69f: 
1:376d69f:   /**
1:376d69f:    * Returns the ColumnarBatch object that will be used for all rows returned by this reader.
1:376d69f:    * This object is reused. Calling this enables the vectorized reader. This should be called
1:376d69f:    * before any calls to nextKeyValue/nextBatch.
1:376d69f:    */
1:376d69f: 
1:347b8e1:   public void initBatch(MemoryMode memMode, StructType partitionColumns,
1:347b8e1:       InternalRow partitionValues) {
1:daa6465:     List<ProjectionDimension> queryDimension = queryModel.getProjectionDimensions();
1:daa6465:     List<ProjectionMeasure> queryMeasures = queryModel.getProjectionMeasures();
1:376d69f:     StructField[] fields = new StructField[queryDimension.size() + queryMeasures.size()];
1:3a4b881:     this.isNoDictStringField = new boolean[queryDimension.size() + queryMeasures.size()];
1:376d69f:     for (int i = 0; i < queryDimension.size(); i++) {
1:daa6465:       ProjectionDimension dim = queryDimension.get(i);
1:376d69f:       if (dim.getDimension().hasEncoding(Encoding.DIRECT_DICTIONARY)) {
1:376d69f:         DirectDictionaryGenerator generator = DirectDictionaryKeyGeneratorFactory
1:376d69f:             .getDirectDictionaryGenerator(dim.getDimension().getDataType());
1:daa6465:         fields[dim.getOrdinal()] = new StructField(dim.getColumnName(),
1:347b8e1:             CarbonSparkDataSourceUtil.convertCarbonToSparkDataType(generator.getReturnType()), true, null);
1:376d69f:       } else if (!dim.getDimension().hasEncoding(Encoding.DICTIONARY)) {
1:3a4b881:         if (dim.getDimension().getDataType() == DataTypes.STRING
1:2ccdbb7:             || dim.getDimension().getDataType() == DataTypes.VARCHAR || dim.getDimension()
1:2ccdbb7:             .getColumnSchema().isLocalDictColumn()) {
1:3a4b881:           this.isNoDictStringField[dim.getOrdinal()] = true;
1:376d69f:         }
1:daa6465:         fields[dim.getOrdinal()] = new StructField(dim.getColumnName(),
1:347b8e1:             CarbonSparkDataSourceUtil.convertCarbonToSparkDataType(dim.getDimension().getDataType()), true,
1:376d69f:             null);
1:376d69f:       } else if (dim.getDimension().isComplex()) {
1:daa6465:         fields[dim.getOrdinal()] = new StructField(dim.getColumnName(),
1:347b8e1:             CarbonSparkDataSourceUtil.convertCarbonToSparkDataType(dim.getDimension().getDataType()), true,
1:376d69f:             null);
1:376d69f:       } else {
1:daa6465:         fields[dim.getOrdinal()] = new StructField(dim.getColumnName(),
1:347b8e1:             CarbonSparkDataSourceUtil.convertCarbonToSparkDataType(DataTypes.INT), true, null);
1:376d69f:       }
1:376d69f:     }
1:376d69f: 
1:376d69f:     for (int i = 0; i < queryMeasures.size(); i++) {
1:daa6465:       ProjectionMeasure msr = queryMeasures.get(i);
1:956833e:       DataType dataType = msr.getMeasure().getDataType();
1:6abdd97:       if (dataType == DataTypes.BOOLEAN || dataType == DataTypes.SHORT ||
1:6abdd97:           dataType == DataTypes.INT || dataType == DataTypes.LONG) {
1:daa6465:         fields[msr.getOrdinal()] = new StructField(msr.getColumnName(),
1:347b8e1:             CarbonSparkDataSourceUtil.convertCarbonToSparkDataType(msr.getMeasure().getDataType()), true,
1:376d69f:             null);
1:f209e8e:       } else if (DataTypes.isDecimal(dataType)) {
1:daa6465:         fields[msr.getOrdinal()] = new StructField(msr.getColumnName(),
1:956833e:             new DecimalType(msr.getMeasure().getPrecision(), msr.getMeasure().getScale()), true,
2:956833e:             null);
1:638ed1f:       } else {
1:daa6465:         fields[msr.getOrdinal()] = new StructField(msr.getColumnName(),
1:347b8e1:             CarbonSparkDataSourceUtil.convertCarbonToSparkDataType(DataTypes.DOUBLE), true, null);
1:376d69f:       }
1:376d69f:     }
1:347b8e1:     StructType schema = new StructType(fields);
1:347b8e1:     if (partitionColumns != null) {
1:347b8e1:       for (StructField field : partitionColumns.fields()) {
1:347b8e1:         schema = schema.add(field);
1:347b8e1:       }
1:347b8e1:     }
1:74c3eb1:     vectorProxy = new CarbonVectorProxy(DEFAULT_MEMORY_MODE,schema,DEFAULT_BATCH_SIZE);
1:347b8e1:     if (partitionColumns != null) {
1:347b8e1:       int partitionIdx = fields.length;
1:347b8e1:       for (int i = 0; i < partitionColumns.fields().length; i++) {
1:74c3eb1:         ColumnVectorUtils.populate(vectorProxy.column(i + partitionIdx), partitionValues, i);
1:74c3eb1:         vectorProxy.column(i + partitionIdx).setIsConstant();
1:347b8e1:       }
1:347b8e1:     }
1:376d69f:     CarbonColumnVector[] vectors = new CarbonColumnVector[fields.length];
1:bcef656:     boolean[] filteredRows = new boolean[vectorProxy.numRows()];
1:376d69f:     for (int i = 0; i < fields.length; i++) {
1:bcef656:       vectors[i] = new ColumnarVectorWrapper(vectorProxy, filteredRows, i);
1:74c3eb1:       if (isNoDictStringField[i]) {
1:74c3eb1:         if (vectors[i] instanceof ColumnarVectorWrapper) {
1:74c3eb1:           ((ColumnarVectorWrapper) vectors[i]).reserveDictionaryIds();
1:74c3eb1:         }
1:74c3eb1:       }
1:376d69f:     }
1:bcef656:     carbonColumnarBatch = new CarbonColumnarBatch(vectors, vectorProxy.numRows(), filteredRows);
1:376d69f:   }
1:376d69f: 
1:376d69f:   private void initBatch() {
1:347b8e1:     initBatch(DEFAULT_MEMORY_MODE, new StructType(), InternalRow.empty());
1:376d69f:   }
1:376d69f: 
1:daa6465:   private void resultBatch() {
1:bcef656:     if (vectorProxy == null) initBatch();
1:376d69f:   }
1:376d69f: 
1:376d69f: 
1:376d69f: 
1:376d69f:   /**
1:376d69f:    * Advances to the next batch of rows. Returns false if there are no more.
1:376d69f:    */
1:6fee993:   private boolean nextBatch() {
1:3a4b881:     if (null != isNoDictStringField) {
1:3a4b881:       for (int i = 0; i < isNoDictStringField.length; i++) {
1:bcef656:         if (isNoDictStringField[i]) {
1:bcef656:           vectorProxy.resetDictionaryIds(i);
1:3a4b881:         }
1:3a4b881:       }
1:3a4b881:     }
1:bcef656:     vectorProxy.reset();
1:376d69f:     carbonColumnarBatch.reset();
1:376d69f:     if (iterator.hasNext()) {
1:376d69f:       iterator.processNextBatch(carbonColumnarBatch);
1:376d69f:       int actualSize = carbonColumnarBatch.getActualSize();
1:bcef656:       vectorProxy.setNumRows(actualSize);
1:376d69f:       numBatched = actualSize;
1:376d69f:       batchIdx = 0;
1:376d69f:       return true;
1:376d69f:     }
1:376d69f:     return false;
1:376d69f:   }
1:376d69f: 
1:376d69f: }
============================================================================
author:akashrn5
-------------------------------------------------------------------------------
commit:2ccdbb7
/////////////////////////////////////////////////////////////////////////
1:             || dim.getDimension().getDataType() == DataTypes.VARCHAR || dim.getDimension()
1:             .getColumnSchema().isLocalDictColumn()) {
author:sandeep-katta
-------------------------------------------------------------------------------
commit:74c3eb1
/////////////////////////////////////////////////////////////////////////
1: import org.apache.spark.sql.types.StructType;
/////////////////////////////////////////////////////////////////////////
1:     vectorProxy = new CarbonVectorProxy(DEFAULT_MEMORY_MODE,schema,DEFAULT_BATCH_SIZE);
1:         ColumnVectorUtils.populate(vectorProxy.column(i + partitionIdx), partitionValues, i);
1:         vectorProxy.column(i + partitionIdx).setIsConstant();
1:       if (isNoDictStringField[i]) {
1:         if (vectors[i] instanceof ColumnarVectorWrapper) {
1:           ((ColumnarVectorWrapper) vectors[i]).reserveDictionaryIds();
1:         }
1:       }
author:sujith71955
-------------------------------------------------------------------------------
commit:bcef656
/////////////////////////////////////////////////////////////////////////
0: <<<<<<< 2f537b724f6f03ab40c95f7ecc8ebd38f6500099:integration/spark-datasource/src/main/scala/org/apache/carbondata/spark/vectorreader/VectorizedCarbonRecordReader.java
0: =======
1: import org.apache.spark.sql.CarbonVectorProxy;
0: >>>>>>> [CARBONDATA-2532][Integration] Carbon to support spark 2.3 version, ColumnVector Interface:integration/spark2/src/main/java/org/apache/carbondata/spark/vectorreader/VectorizedCarbonRecordReader.java
/////////////////////////////////////////////////////////////////////////
1:   private static final int DEFAULT_BATCH_SIZE = 4 * 1024;
1: 
1:   private CarbonVectorProxy vectorProxy;
/////////////////////////////////////////////////////////////////////////
1:     if (vectorProxy != null) {
1:       vectorProxy.close();
1:       vectorProxy = null;
/////////////////////////////////////////////////////////////////////////
1:       int value = vectorProxy.numRows();
1:       return vectorProxy.getColumnarBatch();
1:     return vectorProxy.getRow(batchIdx - 1);
/////////////////////////////////////////////////////////////////////////
/////////////////////////////////////////////////////////////////////////
0:     vectorProxy = new CarbonVectorProxy(MemoryMode.OFF_HEAP,DEFAULT_BATCH_SIZE,fields);
1:     boolean[] filteredRows = new boolean[vectorProxy.numRows()];
1:     if (isNoDictStringField[i]) {
0:       vectorProxy.reserveDictionaryIds(vectorProxy.numRows(), i);
1:       vectors[i] = new ColumnarVectorWrapper(vectorProxy, filteredRows, i);
1:     carbonColumnarBatch = new CarbonColumnarBatch(vectors, vectorProxy.numRows(), filteredRows);
/////////////////////////////////////////////////////////////////////////
1:     if (vectorProxy == null) initBatch();
/////////////////////////////////////////////////////////////////////////
1:           vectorProxy.resetDictionaryIds(i);
1:     vectorProxy.reset();
1:       vectorProxy.setNumRows(actualSize);
author:kunal642
-------------------------------------------------------------------------------
commit:2a9604c
/////////////////////////////////////////////////////////////////////////
1:       queryExecutor =
1:           QueryExecutorFactory.getQueryExecutor(queryModel, taskAttemptContext.getConfiguration());
author:ravipesala
-------------------------------------------------------------------------------
commit:347b8e1
/////////////////////////////////////////////////////////////////////////
1: import org.apache.spark.sql.carbondata.execution.datasources.CarbonSparkDataSourceUtil;
1: import org.apache.spark.sql.catalyst.InternalRow;
1: import org.apache.spark.sql.execution.vectorized.ColumnVectorUtils;
/////////////////////////////////////////////////////////////////////////
1: public class VectorizedCarbonRecordReader extends AbstractRecordReader<Object> {
/////////////////////////////////////////////////////////////////////////
1:   public void initBatch(MemoryMode memMode, StructType partitionColumns,
1:       InternalRow partitionValues) {
/////////////////////////////////////////////////////////////////////////
1:             CarbonSparkDataSourceUtil.convertCarbonToSparkDataType(generator.getReturnType()), true, null);
1:             CarbonSparkDataSourceUtil.convertCarbonToSparkDataType(dim.getDimension().getDataType()), true,
1:             CarbonSparkDataSourceUtil.convertCarbonToSparkDataType(dim.getDimension().getDataType()), true,
1:             CarbonSparkDataSourceUtil.convertCarbonToSparkDataType(DataTypes.INT), true, null);
/////////////////////////////////////////////////////////////////////////
1:             CarbonSparkDataSourceUtil.convertCarbonToSparkDataType(msr.getMeasure().getDataType()), true,
/////////////////////////////////////////////////////////////////////////
1:             CarbonSparkDataSourceUtil.convertCarbonToSparkDataType(DataTypes.DOUBLE), true, null);
1:     StructType schema = new StructType(fields);
1:     if (partitionColumns != null) {
1:       for (StructField field : partitionColumns.fields()) {
1:         schema = schema.add(field);
1:       }
1:     }
0:     columnarBatch = ColumnarBatch.allocate(schema, memMode);
1:     if (partitionColumns != null) {
1:       int partitionIdx = fields.length;
1:       for (int i = 0; i < partitionColumns.fields().length; i++) {
0:         ColumnVectorUtils.populate(columnarBatch.column(i + partitionIdx), partitionValues, i);
0:         columnarBatch.column(i + partitionIdx).setIsConstant();
1:       }
1:     }
/////////////////////////////////////////////////////////////////////////
1:     initBatch(DEFAULT_MEMORY_MODE, new StructType(), InternalRow.empty());
commit:1c5b4a5
/////////////////////////////////////////////////////////////////////////
0:     boolean[] filteredRows = new boolean[columnarBatch.capacity()];
0:       vectors[i] = new ColumnarVectorWrapper(columnarBatch.column(i), filteredRows);
0:     carbonColumnarBatch = new CarbonColumnarBatch(vectors, columnarBatch.capacity(), filteredRows);
commit:8f59a32
/////////////////////////////////////////////////////////////////////////
0:   private static final MemoryMode DEFAULT_MEMORY_MODE = MemoryMode.OFF_HEAP;
commit:376d69f
/////////////////////////////////////////////////////////////////////////
1: /*
1:  * Licensed to the Apache Software Foundation (ASF) under one or more
1:  * contributor license agreements.  See the NOTICE file distributed with
1:  * this work for additional information regarding copyright ownership.
1:  * The ASF licenses this file to You under the Apache License, Version 2.0
1:  * (the "License"); you may not use this file except in compliance with
1:  * the License.  You may obtain a copy of the License at
1:  *
1:  *    http://www.apache.org/licenses/LICENSE-2.0
1:  *
1:  * Unless required by applicable law or agreed to in writing, software
1:  * distributed under the License is distributed on an "AS IS" BASIS,
1:  * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
1:  * See the License for the specific language governing permissions and
1:  * limitations under the License.
1:  */
1: 
1: package org.apache.carbondata.spark.vectorreader;
1: 
1: import java.io.IOException;
1: import java.util.ArrayList;
1: import java.util.List;
1: import java.util.Map;
1: 
1: import org.apache.carbondata.core.cache.dictionary.Dictionary;
0: import org.apache.carbondata.core.carbon.datastore.block.TableBlockInfo;
0: import org.apache.carbondata.core.carbon.metadata.datatype.DataType;
0: import org.apache.carbondata.core.carbon.metadata.encoder.Encoding;
1: import org.apache.carbondata.core.keygenerator.directdictionary.DirectDictionaryGenerator;
1: import org.apache.carbondata.core.keygenerator.directdictionary.DirectDictionaryKeyGeneratorFactory;
1: import org.apache.carbondata.core.util.CarbonUtil;
1: import org.apache.carbondata.hadoop.CarbonInputSplit;
1: import org.apache.carbondata.hadoop.CarbonMultiBlockSplit;
0: import org.apache.carbondata.scan.executor.QueryExecutor;
0: import org.apache.carbondata.scan.executor.QueryExecutorFactory;
0: import org.apache.carbondata.scan.executor.exception.QueryExecutionException;
0: import org.apache.carbondata.scan.model.QueryDimension;
0: import org.apache.carbondata.scan.model.QueryMeasure;
0: import org.apache.carbondata.scan.model.QueryModel;
0: import org.apache.carbondata.scan.result.iterator.AbstractDetailQueryResultIterator;
0: import org.apache.carbondata.scan.result.vector.CarbonColumnVector;
0: import org.apache.carbondata.scan.result.vector.CarbonColumnarBatch;
0: import org.apache.carbondata.spark.util.CarbonScalaUtil;
1: 
1: import org.apache.hadoop.mapreduce.InputSplit;
0: import org.apache.hadoop.mapreduce.RecordReader;
1: import org.apache.hadoop.mapreduce.TaskAttemptContext;
1: import org.apache.spark.memory.MemoryMode;
0: import org.apache.spark.sql.execution.vectorized.ColumnarBatch;
1: import org.apache.spark.sql.types.DecimalType;
1: import org.apache.spark.sql.types.StructField;
0: import org.apache.spark.sql.types.StructType;
1: 
1: /**
1:  * A specialized RecordReader that reads into InternalRows or ColumnarBatches directly using the
1:  * carbondata column APIs and fills the data directly into columns.
1:  */
0: public class VectorizedCarbonRecordReader extends RecordReader<Void, Object> {
1: 
1:   private int batchIdx = 0;
1: 
1:   private int numBatched = 0;
1: 
0:   private ColumnarBatch columnarBatch;
1: 
1:   private CarbonColumnarBatch carbonColumnarBatch;
1: 
1:   /**
1:    * If true, this class returns batches instead of rows.
1:    */
1:   private boolean returnColumnarBatch;
1: 
1:   /**
0:    * The default config on whether columnarBatch should be offheap.
1:    */
1:   private static final MemoryMode DEFAULT_MEMORY_MODE = MemoryMode.ON_HEAP;
1: 
1:   private QueryModel queryModel;
1: 
1:   private AbstractDetailQueryResultIterator iterator;
1: 
1:   private QueryExecutor queryExecutor;
1: 
0:   public VectorizedCarbonRecordReader(QueryModel queryModel) {
1:     this.queryModel = queryModel;
1:     enableReturningBatches();
1:   }
1: 
1:   /**
1:    * Implementation of RecordReader API.
1:    */
0:   @Override public void initialize(InputSplit inputSplit, TaskAttemptContext taskAttemptContext)
1:       throws IOException, InterruptedException, UnsupportedOperationException {
1:     // The input split can contain single HDFS block or multiple blocks, so firstly get all the
1:     // blocks and then set them in the query model.
1:     List<CarbonInputSplit> splitList;
1:     if (inputSplit instanceof CarbonInputSplit) {
1:       splitList = new ArrayList<>(1);
1:       splitList.add((CarbonInputSplit) inputSplit);
1:     } else if (inputSplit instanceof CarbonMultiBlockSplit) {
1:       // contains multiple blocks, this is an optimization for concurrent query.
1:       CarbonMultiBlockSplit multiBlockSplit = (CarbonMultiBlockSplit) inputSplit;
1:       splitList = multiBlockSplit.getAllSplits();
1:     } else {
1:       throw new RuntimeException("unsupported input split type: " + inputSplit);
1:     }
1:     List<TableBlockInfo> tableBlockInfoList = CarbonInputSplit.createBlocks(splitList);
1:     queryModel.setTableBlockInfos(tableBlockInfoList);
1:     queryModel.setVectorReader(true);
1:     try {
0:       queryExecutor = QueryExecutorFactory.getQueryExecutor(queryModel);
1:       iterator = (AbstractDetailQueryResultIterator) queryExecutor.execute(queryModel);
1:     } catch (QueryExecutionException e) {
1:       throw new InterruptedException(e.getMessage());
1:     }
1:   }
1: 
0:   @Override public void close() throws IOException {
0:     if (columnarBatch != null) {
0:       columnarBatch.close();
0:       columnarBatch = null;
1:     }
1:     // clear dictionary cache
1:     Map<String, Dictionary> columnToDictionaryMapping = queryModel.getColumnToDictionaryMapping();
1:     if (null != columnToDictionaryMapping) {
1:       for (Map.Entry<String, Dictionary> entry : columnToDictionaryMapping.entrySet()) {
1:         CarbonUtil.clearDictionaryCache(entry.getValue());
1:       }
1:     }
1:     try {
1:       queryExecutor.finish();
1:     } catch (QueryExecutionException e) {
1:       throw new IOException(e);
1:     }
1:   }
1: 
0:   @Override public boolean nextKeyValue() throws IOException, InterruptedException {
1:     resultBatch();
1: 
0:     if (returnColumnarBatch) return nextBatch();
1: 
1:     if (batchIdx >= numBatched) {
1:       if (!nextBatch()) return false;
1:     }
1:     ++batchIdx;
1:     return true;
1:   }
1: 
0:   @Override public Object getCurrentValue() throws IOException, InterruptedException {
0:     if (returnColumnarBatch) return columnarBatch;
0:     return columnarBatch.getRow(batchIdx - 1);
1:   }
1: 
0:   @Override public Void getCurrentKey() throws IOException, InterruptedException {
1:     return null;
1:   }
1: 
0:   @Override public float getProgress() throws IOException, InterruptedException {
1:     // TODO : Implement it based on total number of rows it is going to retrive.
1:     return 0;
1:   }
1: 
1:   /**
1:    * Returns the ColumnarBatch object that will be used for all rows returned by this reader.
1:    * This object is reused. Calling this enables the vectorized reader. This should be called
1:    * before any calls to nextKeyValue/nextBatch.
1:    */
1: 
0:   public void initBatch(MemoryMode memMode) {
0:     List<QueryDimension> queryDimension = queryModel.getQueryDimension();
0:     List<QueryMeasure> queryMeasures = queryModel.getQueryMeasures();
1:     StructField[] fields = new StructField[queryDimension.size() + queryMeasures.size()];
1:     for (int i = 0; i < queryDimension.size(); i++) {
0:       QueryDimension dim = queryDimension.get(i);
1:       if (dim.getDimension().hasEncoding(Encoding.DIRECT_DICTIONARY)) {
1:         DirectDictionaryGenerator generator = DirectDictionaryKeyGeneratorFactory
1:             .getDirectDictionaryGenerator(dim.getDimension().getDataType());
0:         fields[dim.getQueryOrder()] = new StructField(dim.getColumnName(),
0:             CarbonScalaUtil.convertCarbonToSparkDataType(generator.getReturnType()), true, null);
1:       } else if (!dim.getDimension().hasEncoding(Encoding.DICTIONARY)) {
0:         fields[dim.getQueryOrder()] = new StructField(dim.getColumnName(),
0:             CarbonScalaUtil.convertCarbonToSparkDataType(dim.getDimension().getDataType()), true,
1:             null);
1:       } else if (dim.getDimension().isComplex()) {
0:         fields[dim.getQueryOrder()] = new StructField(dim.getColumnName(),
0:             CarbonScalaUtil.convertCarbonToSparkDataType(dim.getDimension().getDataType()), true,
1:             null);
1:       } else {
0:         fields[dim.getQueryOrder()] = new StructField(dim.getColumnName(),
0:             CarbonScalaUtil.convertCarbonToSparkDataType(DataType.INT), true, null);
1:       }
1:     }
1: 
1:     for (int i = 0; i < queryMeasures.size(); i++) {
0:       QueryMeasure msr = queryMeasures.get(i);
0:       switch (msr.getMeasure().getDataType()) {
0:         case SHORT:
0:         case INT:
0:         case LONG:
0:           fields[msr.getQueryOrder()] = new StructField(msr.getColumnName(),
0:               CarbonScalaUtil.convertCarbonToSparkDataType(msr.getMeasure().getDataType()), true,
1:               null);
0:           break;
0:         case DECIMAL:
0:           fields[msr.getQueryOrder()] = new StructField(msr.getColumnName(),
0:               new DecimalType(msr.getMeasure().getPrecision(),
0:                   msr.getMeasure().getScale()), true, null);
0:           break;
0:         default:
0:           fields[msr.getQueryOrder()] = new StructField(msr.getColumnName(),
0:               CarbonScalaUtil.convertCarbonToSparkDataType(DataType.DOUBLE), true, null);
1:       }
1:     }
1: 
0:     columnarBatch = ColumnarBatch.allocate(new StructType(fields), memMode);
1:     CarbonColumnVector[] vectors = new CarbonColumnVector[fields.length];
1:     for (int i = 0; i < fields.length; i++) {
0:       vectors[i] = new ColumnarVectorWrapper(columnarBatch.column(i));
1:     }
0:     carbonColumnarBatch = new CarbonColumnarBatch(vectors, columnarBatch.capacity());
1:   }
1: 
1:   private void initBatch() {
0:     initBatch(DEFAULT_MEMORY_MODE);
1:   }
1: 
0:   private ColumnarBatch resultBatch() {
0:     if (columnarBatch == null) initBatch();
0:     return columnarBatch;
1:   }
1: 
1:   /*
1:    * Can be called before any rows are returned to enable returning columnar batches directly.
1:    */
1:   public void enableReturningBatches() {
1:     returnColumnarBatch = true;
1:   }
1: 
1:   /**
1:    * Advances to the next batch of rows. Returns false if there are no more.
1:    */
0:   public boolean nextBatch() throws IOException {
0:     columnarBatch.reset();
1:     carbonColumnarBatch.reset();
1:     if (iterator.hasNext()) {
1:       iterator.processNextBatch(carbonColumnarBatch);
1:       int actualSize = carbonColumnarBatch.getActualSize();
0:       columnarBatch.setNumRows(actualSize);
1:       numBatched = actualSize;
1:       batchIdx = 0;
1:       return true;
1:     }
1:     return false;
1:   }
1: 
1: }
author:kumarvishal09
-------------------------------------------------------------------------------
commit:3a4b881
/////////////////////////////////////////////////////////////////////////
1:   private boolean[] isNoDictStringField;
1: 
/////////////////////////////////////////////////////////////////////////
1:     this.isNoDictStringField = new boolean[queryDimension.size() + queryMeasures.size()];
/////////////////////////////////////////////////////////////////////////
1:         if (dim.getDimension().getDataType() == DataTypes.STRING
0:             || dim.getDimension().getDataType() == DataTypes.VARCHAR) {
1:           this.isNoDictStringField[dim.getOrdinal()] = true;
1:         }
/////////////////////////////////////////////////////////////////////////
0:       if (isNoDictStringField[i]) {
0:         columnarBatch.column(i).reserveDictionaryIds(columnarBatch.capacity());
1:       }
/////////////////////////////////////////////////////////////////////////
1:     if (null != isNoDictStringField) {
1:       for (int i = 0; i < isNoDictStringField.length; i++) {
0:         if (isNoDictStringField[i]) {
0:           columnarBatch.column(i).getDictionaryIds().reset();
1:         }
1:       }
1:     }
author:Raghunandan S
-------------------------------------------------------------------------------
commit:7ef9164
/////////////////////////////////////////////////////////////////////////
1: import org.apache.commons.lang3.exception.ExceptionUtils;
/////////////////////////////////////////////////////////////////////////
1:       if (ExceptionUtils.indexOfThrowable(e, FileNotFoundException.class) > 0) {
1:         LOGGER.error(e);
1:         throw new InterruptedException(
1:             "Insert overwrite may be in progress.Please check " + e.getMessage());
1:       if (ExceptionUtils.indexOfThrowable(e, FileNotFoundException.class) > 0) {
1:         LOGGER.error(e);
1:         throw new InterruptedException(
1:             "Insert overwrite may be in progress.Please check " + e.getMessage());
commit:bb0b347
/////////////////////////////////////////////////////////////////////////
0:       inputMetricsStats.incrementRecordRead((long)value);
author:Manhua
-------------------------------------------------------------------------------
commit:2fc0ad3
/////////////////////////////////////////////////////////////////////////
/////////////////////////////////////////////////////////////////////////
/////////////////////////////////////////////////////////////////////////
0:       queryExecutor = QueryExecutorFactory.getQueryExecutor(queryModel);
commit:638ed1f
/////////////////////////////////////////////////////////////////////////
0: import org.apache.carbondata.core.constants.CarbonCommonConstants;
/////////////////////////////////////////////////////////////////////////
0: import org.apache.carbondata.core.scan.executor.impl.SearchModeVectorDetailQueryExecutor;
0: import org.apache.carbondata.core.util.CarbonProperties;
/////////////////////////////////////////////////////////////////////////
0:       if (CarbonProperties.getInstance().getProperty(
0:               CarbonCommonConstants.CARBON_SEARCH_MODE_ENABLE,
0:               CarbonCommonConstants.CARBON_SEARCH_MODE_ENABLE_DEFAULT).equals("true")) {
0:         queryExecutor = new SearchModeVectorDetailQueryExecutor();
1:       } else {
0:         queryExecutor = QueryExecutorFactory.getQueryExecutor(queryModel);
1:       }
author:sounakr
-------------------------------------------------------------------------------
commit:9a25dc6
/////////////////////////////////////////////////////////////////////////
1:   public VectorizedCarbonRecordReader(QueryModel queryModel, InputMetricsStats inputMetricsStats,
1:       String enableBatch) {
1:     if (enableBatch.equals("true")) {
0:       enableReturningBatches();
1:     }
1:   }
1: 
1: 
0:   /*
0:  * Can be called before any rows are returned to enable returning columnar batches directly.
0:  */
0:   public void enableReturningBatches() {
0:     returnColumnarBatch = true;
/////////////////////////////////////////////////////////////////////////
1: 
author:Jacky Li
-------------------------------------------------------------------------------
commit:daa6465
/////////////////////////////////////////////////////////////////////////
1: import org.apache.carbondata.core.scan.model.ProjectionDimension;
1: import org.apache.carbondata.core.scan.model.ProjectionMeasure;
/////////////////////////////////////////////////////////////////////////
1:   @Override
1:   public void initialize(InputSplit inputSplit, TaskAttemptContext taskAttemptContext)
/////////////////////////////////////////////////////////////////////////
1:   @Override
1:   public void close() throws IOException {
/////////////////////////////////////////////////////////////////////////
1:   @Override
1:   public boolean nextKeyValue() throws IOException, InterruptedException {
1:     if (returnColumnarBatch) {
1:       return nextBatch();
1:     }
/////////////////////////////////////////////////////////////////////////
1:   @Override
1:   public Object getCurrentValue() throws IOException, InterruptedException {
/////////////////////////////////////////////////////////////////////////
1:   @Override
1:   public Void getCurrentKey() throws IOException, InterruptedException {
1:   @Override
1:   public float getProgress() throws IOException, InterruptedException {
/////////////////////////////////////////////////////////////////////////
1:     List<ProjectionDimension> queryDimension = queryModel.getProjectionDimensions();
1:     List<ProjectionMeasure> queryMeasures = queryModel.getProjectionMeasures();
1:       ProjectionDimension dim = queryDimension.get(i);
1:         fields[dim.getOrdinal()] = new StructField(dim.getColumnName(),
1:         fields[dim.getOrdinal()] = new StructField(dim.getColumnName(),
1:         fields[dim.getOrdinal()] = new StructField(dim.getColumnName(),
1:         fields[dim.getOrdinal()] = new StructField(dim.getColumnName(),
1:       ProjectionMeasure msr = queryMeasures.get(i);
1:         fields[msr.getOrdinal()] = new StructField(msr.getColumnName(),
1:         fields[msr.getOrdinal()] = new StructField(msr.getColumnName(),
1:         fields[msr.getOrdinal()] = new StructField(msr.getColumnName(),
/////////////////////////////////////////////////////////////////////////
1:   private void resultBatch() {
commit:77217b3
/////////////////////////////////////////////////////////////////////////
1:       if (inputMetricsStats != null) {
1:         inputMetricsStats.incrementRecordRead((long) value);
1:       }
commit:f209e8e
/////////////////////////////////////////////////////////////////////////
1:       } else if (DataTypes.isDecimal(dataType)) {
commit:956833e
/////////////////////////////////////////////////////////////////////////
1: import org.apache.carbondata.core.metadata.datatype.DataTypes;
/////////////////////////////////////////////////////////////////////////
0:             CarbonScalaUtil.convertCarbonToSparkDataType(DataTypes.INT), true, null);
1:       DataType dataType = msr.getMeasure().getDataType();
0:       if (dataType == DataTypes.SHORT || dataType == DataTypes.INT || dataType == DataTypes.LONG) {
0:         fields[msr.getQueryOrder()] = new StructField(msr.getColumnName(),
0:             CarbonScalaUtil.convertCarbonToSparkDataType(msr.getMeasure().getDataType()), true,
1:             null);
0:       } else if (dataType == DataTypes.DECIMAL) {
0:         fields[msr.getQueryOrder()] = new StructField(msr.getColumnName(),
1:             new DecimalType(msr.getMeasure().getPrecision(), msr.getMeasure().getScale()), true,
1:             null);
0:       } else {
0:         fields[msr.getQueryOrder()] = new StructField(msr.getColumnName(),
0:             CarbonScalaUtil.convertCarbonToSparkDataType(DataTypes.DOUBLE), true, null);
author:kushalsaha
-------------------------------------------------------------------------------
commit:c5e72a4
/////////////////////////////////////////////////////////////////////////
1: import org.apache.carbondata.common.logging.LogService;
1: import org.apache.carbondata.common.logging.LogServiceFactory;
/////////////////////////////////////////////////////////////////////////
1:   private static final LogService LOGGER =
1:       LogServiceFactory.getLogService(VectorizedCarbonRecordReader.class.getName());
1: 
/////////////////////////////////////////////////////////////////////////
0:               "Insert overwrite may be in progress.Please check " + e.getMessage());
1:     } catch (Exception e) {
0:       Throwable ext = e;
0:       while (ext != null) {
0:         if (ext instanceof FileNotFoundException) {
0:           LOGGER.error(e);
0:           throw new InterruptedException(
0:               "Insert overwrite may be in progress.Please check " + e.getMessage());
0:         }
0:         ext = ext.getCause();
0:       }
1:       throw e;
commit:a31598e
/////////////////////////////////////////////////////////////////////////
1: import java.io.FileNotFoundException;
/////////////////////////////////////////////////////////////////////////
0:       Throwable ext = e;
0:       while (ext != null) {
0:         if (ext instanceof FileNotFoundException) {
0:           throw new InterruptedException(
0:               e.getMessage() + ". insert overwrite may be in progress.Please check");
0:         }
0:         ext = ext.getCause();
0:       }
author:xubo245
-------------------------------------------------------------------------------
commit:6abdd97
/////////////////////////////////////////////////////////////////////////
1:       if (dataType == DataTypes.BOOLEAN || dataType == DataTypes.SHORT ||
1:           dataType == DataTypes.INT || dataType == DataTypes.LONG) {
author:Ravindra Pesala
-------------------------------------------------------------------------------
commit:e9c24c5
/////////////////////////////////////////////////////////////////////////
1:    * The default config on whether columnarBatch should be onheap.
0:   private static final MemoryMode DEFAULT_MEMORY_MODE = MemoryMode.ON_HEAP;
author:Manohar
-------------------------------------------------------------------------------
commit:e3f98fa
/////////////////////////////////////////////////////////////////////////
1: import org.apache.carbondata.hadoop.InputMetricsStats;
/////////////////////////////////////////////////////////////////////////
1:   private InputMetricsStats inputMetricsStats;
0: 
0:   public VectorizedCarbonRecordReader(QueryModel queryModel, InputMetricsStats inputMetricsStats) {
1:     this.inputMetricsStats = inputMetricsStats;
/////////////////////////////////////////////////////////////////////////
0:       int value = columnarBatch.numValidRows();
1:       rowCount += value;
0:       inputMetricsStats.incrementRecordRead(new Long(value));
author:nareshpr
-------------------------------------------------------------------------------
commit:ec2d742
/////////////////////////////////////////////////////////////////////////
1: import org.apache.carbondata.hadoop.AbstractRecordReader;
/////////////////////////////////////////////////////////////////////////
0: class VectorizedCarbonRecordReader extends AbstractRecordReader<Object> {
/////////////////////////////////////////////////////////////////////////
1:     logStatistics(rowCount, queryModel.getStatisticsRecorder());
/////////////////////////////////////////////////////////////////////////
1:     if (returnColumnarBatch) {
0:       rowCount += columnarBatch.numValidRows();
0:       return columnarBatch;
0:     }
1:     rowCount += 1;
author:jackylk
-------------------------------------------------------------------------------
commit:ce09aaa
/////////////////////////////////////////////////////////////////////////
1: import org.apache.carbondata.core.datastore.block.TableBlockInfo;
1: import org.apache.carbondata.core.metadata.datatype.DataType;
1: import org.apache.carbondata.core.metadata.encoder.Encoding;
1: import org.apache.carbondata.core.scan.executor.QueryExecutor;
1: import org.apache.carbondata.core.scan.executor.QueryExecutorFactory;
1: import org.apache.carbondata.core.scan.executor.exception.QueryExecutionException;
0: import org.apache.carbondata.core.scan.model.QueryDimension;
0: import org.apache.carbondata.core.scan.model.QueryMeasure;
1: import org.apache.carbondata.core.scan.model.QueryModel;
1: import org.apache.carbondata.core.scan.result.iterator.AbstractDetailQueryResultIterator;
1: import org.apache.carbondata.core.scan.result.vector.CarbonColumnVector;
1: import org.apache.carbondata.core.scan.result.vector.CarbonColumnarBatch;
commit:6fee993
/////////////////////////////////////////////////////////////////////////
0: class VectorizedCarbonRecordReader extends RecordReader<Void, Object> {
/////////////////////////////////////////////////////////////////////////
0:   private void initBatch(MemoryMode memMode) {
/////////////////////////////////////////////////////////////////////////
0:   private void enableReturningBatches() {
1:   private boolean nextBatch() {
============================================================================