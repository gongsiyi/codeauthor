1:c09ef99: /*
1:c09ef99:  * Licensed to the Apache Software Foundation (ASF) under one or more
1:c09ef99:  * contributor license agreements.  See the NOTICE file distributed with
1:c09ef99:  * this work for additional information regarding copyright ownership.
1:c09ef99:  * The ASF licenses this file to You under the Apache License, Version 2.0
1:c09ef99:  * (the "License"); you may not use this file except in compliance with
1:c09ef99:  * the License.  You may obtain a copy of the License at
1:c09ef99:  *
1:c09ef99:  *    http://www.apache.org/licenses/LICENSE-2.0
1:c09ef99:  *
1:c09ef99:  * Unless required by applicable law or agreed to in writing, software
1:c09ef99:  * distributed under the License is distributed on an "AS IS" BASIS,
1:c09ef99:  * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
1:c09ef99:  * See the License for the specific language governing permissions and
1:c09ef99:  * limitations under the License.
1:c09ef99:  */
10:c09ef99: 
1:c09ef99: package org.apache.carbondata.hadoop.api;
1:c09ef99: 
1:c09ef99: import java.io.ByteArrayInputStream;
1:c09ef99: import java.io.DataInputStream;
1:c09ef99: import java.io.IOException;
1:c09ef99: import java.lang.reflect.Constructor;
1:22958d9: import java.util.*;
1:c09ef99: 
1:c09ef99: import org.apache.carbondata.core.constants.CarbonCommonConstants;
1:fb1516c: import org.apache.carbondata.core.constants.CarbonCommonConstantsInternal;
1:c09ef99: import org.apache.carbondata.core.datamap.DataMapChooser;
1:2018048: import org.apache.carbondata.core.datamap.DataMapJob;
1:3894e1d: import org.apache.carbondata.core.datamap.DataMapStoreManager;
1:2018048: import org.apache.carbondata.core.datamap.DataMapUtil;
1:c09ef99: import org.apache.carbondata.core.datamap.Segment;
1:3894e1d: import org.apache.carbondata.core.datamap.TableDataMap;
1:c09ef99: import org.apache.carbondata.core.datamap.dev.expr.DataMapExprWrapper;
1:c0de9f1: import org.apache.carbondata.core.datamap.dev.expr.DataMapWrapperSimpleInfo;
1:c09ef99: import org.apache.carbondata.core.exception.InvalidConfigurationException;
1:c09ef99: import org.apache.carbondata.core.indexstore.ExtendedBlocklet;
1:c09ef99: import org.apache.carbondata.core.indexstore.PartitionSpec;
1:c09ef99: import org.apache.carbondata.core.metadata.AbsoluteTableIdentifier;
1:c09ef99: import org.apache.carbondata.core.metadata.ColumnarFormatVersion;
1:c09ef99: import org.apache.carbondata.core.metadata.schema.PartitionInfo;
1:c09ef99: import org.apache.carbondata.core.metadata.schema.partition.PartitionType;
1:c09ef99: import org.apache.carbondata.core.metadata.schema.table.CarbonTable;
1:c09ef99: import org.apache.carbondata.core.metadata.schema.table.TableInfo;
1:5f68a79: import org.apache.carbondata.core.metadata.schema.table.column.ColumnSchema;
1:c09ef99: import org.apache.carbondata.core.mutate.UpdateVO;
1:03a735b: import org.apache.carbondata.core.profiler.ExplainCollector;
1:347b8e1: import org.apache.carbondata.core.readcommitter.ReadCommittedScope;
1:c09ef99: import org.apache.carbondata.core.scan.expression.Expression;
1:c09ef99: import org.apache.carbondata.core.scan.filter.resolver.FilterResolverIntf;
1:c09ef99: import org.apache.carbondata.core.scan.model.QueryModel;
1:3ff574d: import org.apache.carbondata.core.scan.model.QueryModelBuilder;
1:c09ef99: import org.apache.carbondata.core.stats.QueryStatistic;
1:c09ef99: import org.apache.carbondata.core.stats.QueryStatisticsConstants;
1:c09ef99: import org.apache.carbondata.core.stats.QueryStatisticsRecorder;
1:1cea4d3: import org.apache.carbondata.core.util.BlockletDataMapUtil;
1:c09ef99: import org.apache.carbondata.core.util.CarbonProperties;
1:c09ef99: import org.apache.carbondata.core.util.CarbonTimeStatisticsFactory;
1:c09ef99: import org.apache.carbondata.core.util.CarbonUtil;
1:c09ef99: import org.apache.carbondata.core.util.DataTypeConverter;
1:c09ef99: import org.apache.carbondata.core.util.DataTypeConverterImpl;
1:2018048: import org.apache.carbondata.core.util.ObjectSerializationUtil;
1:c09ef99: import org.apache.carbondata.core.util.path.CarbonTablePath;
1:c09ef99: import org.apache.carbondata.hadoop.CarbonInputSplit;
1:c09ef99: import org.apache.carbondata.hadoop.CarbonMultiBlockSplit;
1:c09ef99: import org.apache.carbondata.hadoop.CarbonProjection;
1:c09ef99: import org.apache.carbondata.hadoop.CarbonRecordReader;
1:c09ef99: import org.apache.carbondata.hadoop.readsupport.CarbonReadSupport;
1:c09ef99: import org.apache.carbondata.hadoop.readsupport.impl.DictionaryDecodeReadSupport;
1:c09ef99: 
1:aeb2ec4: import org.apache.commons.collections.CollectionUtils;
1:c09ef99: import org.apache.commons.logging.Log;
1:c09ef99: import org.apache.commons.logging.LogFactory;
1:c09ef99: import org.apache.hadoop.conf.Configuration;
1:c09ef99: import org.apache.hadoop.fs.FileSystem;
1:c09ef99: import org.apache.hadoop.fs.LocalFileSystem;
1:c09ef99: import org.apache.hadoop.fs.Path;
1:c09ef99: import org.apache.hadoop.mapreduce.InputSplit;
1:c09ef99: import org.apache.hadoop.mapreduce.JobContext;
1:c09ef99: import org.apache.hadoop.mapreduce.RecordReader;
1:c09ef99: import org.apache.hadoop.mapreduce.TaskAttemptContext;
1:c09ef99: import org.apache.hadoop.mapreduce.lib.input.FileInputFormat;
1:c09ef99: import org.apache.hadoop.mapreduce.lib.input.FileSplit;
1:c09ef99: import org.apache.hadoop.mapreduce.security.TokenCache;
1:c09ef99: 
1:c09ef99: /**
1:c09ef99:  * Base class for carbondata input format, there are two input format implementations:
1:c09ef99:  * 1. CarbonFileInputFormat: for reading carbondata files without table level metadata support.
1:c09ef99:  *
1:c09ef99:  * 2. CarbonTableInputFormat: for reading carbondata files with table level metadata support,
1:c09ef99:  * such as segment and explicit schema metadata.
1:c09ef99:  *
1:c09ef99:  * @param <T>
1:c09ef99:  */
1:c09ef99: public abstract class CarbonInputFormat<T> extends FileInputFormat<Void, T> {
1:c09ef99:   // comma separated list of input segment numbers
1:c09ef99:   public static final String INPUT_SEGMENT_NUMBERS =
1:c09ef99:       "mapreduce.input.carboninputformat.segmentnumbers";
1:c09ef99:   private static final String VALIDATE_INPUT_SEGMENT_IDs =
1:c09ef99:       "mapreduce.input.carboninputformat.validsegments";
1:c09ef99:   // comma separated list of input files
1:c09ef99:   private static final String ALTER_PARTITION_ID = "mapreduce.input.carboninputformat.partitionid";
1:c09ef99:   private static final Log LOG = LogFactory.getLog(CarbonInputFormat.class);
1:c09ef99:   private static final String FILTER_PREDICATE =
1:c09ef99:       "mapreduce.input.carboninputformat.filter.predicate";
1:c09ef99:   private static final String COLUMN_PROJECTION = "mapreduce.input.carboninputformat.projection";
1:c09ef99:   private static final String TABLE_INFO = "mapreduce.input.carboninputformat.tableinfo";
1:b7b8073:   private static final String CARBON_TRANSACTIONAL_TABLE =
1:b7b8073:       "mapreduce.input.carboninputformat.transactional";
1:c09ef99:   private static final String CARBON_READ_SUPPORT = "mapreduce.input.carboninputformat.readsupport";
1:c09ef99:   private static final String CARBON_CONVERTER = "mapreduce.input.carboninputformat.converter";
1:c09ef99:   public static final String DATABASE_NAME = "mapreduce.input.carboninputformat.databaseName";
1:c09ef99:   public static final String TABLE_NAME = "mapreduce.input.carboninputformat.tableName";
1:c09ef99:   private static final String PARTITIONS_TO_PRUNE =
1:c09ef99:       "mapreduce.input.carboninputformat.partitions.to.prune";
1:3ff574d:   private static final String FGDATAMAP_PRUNING = "mapreduce.input.carboninputformat.fgdatamap";
1:347b8e1:   private static final String READ_COMMITTED_SCOPE =
1:347b8e1:       "mapreduce.input.carboninputformat.read.committed.scope";
1:c09ef99: 
1:d5bec4d:   // record segment number and hit blocks
1:d5bec4d:   protected int numSegments = 0;
1:d5bec4d:   protected int numStreamSegments = 0;
1:0528a79:   protected int numStreamFiles = 0;
1:0528a79:   protected int hitedStreamFiles = 0;
1:d5bec4d:   protected int numBlocks = 0;
1:d5bec4d: 
1:d5bec4d:   public int getNumSegments() {
1:d5bec4d:     return numSegments;
1:d5bec4d:   }
1:d5bec4d: 
1:d5bec4d:   public int getNumStreamSegments() {
1:d5bec4d:     return numStreamSegments;
1:d5bec4d:   }
1:d5bec4d: 
1:0528a79:   public int getNumStreamFiles() {
1:0528a79:     return numStreamFiles;
1:0528a79:   }
1:0528a79: 
1:0528a79:   public int getHitedStreamFiles() {
1:0528a79:     return hitedStreamFiles;
1:0528a79:   }
1:0528a79: 
1:d5bec4d:   public int getNumBlocks() {
1:d5bec4d:     return numBlocks;
1:d5bec4d:   }
1:d5bec4d: 
1:4a47630:   /**
1:c09ef99:    * Set the `tableInfo` in `configuration`
1:4a47630:    */
1:c09ef99:   public static void setTableInfo(Configuration configuration, TableInfo tableInfo)
3:c09ef99:       throws IOException {
1:c09ef99:     if (null != tableInfo) {
1:c09ef99:       configuration.set(TABLE_INFO, CarbonUtil.encodeToString(tableInfo.serialize()));
10:c09ef99:     }
1:c09ef99:   }
1:c09ef99: 
1:a7faef8:   /**
1:c09ef99:    * Get TableInfo object from `configuration`
1:c09ef99:    */
1:c09ef99:   protected static TableInfo getTableInfo(Configuration configuration) throws IOException {
1:c09ef99:     String tableInfoStr = configuration.get(TABLE_INFO);
1:c09ef99:     if (tableInfoStr == null) {
2:c09ef99:       return null;
2:c09ef99:     } else {
1:c09ef99:       TableInfo output = new TableInfo();
1:c09ef99:       output.readFields(new DataInputStream(
1:c09ef99:           new ByteArrayInputStream(CarbonUtil.decodeStringToBytes(tableInfoStr))));
1:c09ef99:       return output;
1:c09ef99:     }
1:c09ef99:   }
1:03a735b: 
1:c09ef99:   /**
1:c09ef99:    * Get the cached CarbonTable or create it by TableInfo in `configuration`
1:c09ef99:    */
1:531ecdf:   public abstract CarbonTable getOrCreateCarbonTable(Configuration configuration)
1:c09ef99:       throws IOException;
1:c09ef99: 
1:c09ef99:   public static void setTablePath(Configuration configuration, String tablePath) {
1:c09ef99:     configuration.set(FileInputFormat.INPUT_DIR, tablePath);
1:280a400:   }
1:280a400: 
1:b7b8073:   public static void setTransactionalTable(Configuration configuration,
1:b7b8073:       boolean isTransactionalTable) {
1:b7b8073:     configuration.set(CARBON_TRANSACTIONAL_TABLE, String.valueOf(isTransactionalTable));
1:c09ef99:   }
1:c09ef99: 
1:c09ef99:   public static void setPartitionIdList(Configuration configuration, List<String> partitionIds) {
1:c09ef99:     configuration.set(ALTER_PARTITION_ID, partitionIds.toString());
1:c09ef99:   }
1:c09ef99: 
1:c09ef99:   /**
1:c09ef99:    * It sets unresolved filter expression.
1:c09ef99:    *
1:c09ef99:    * @param configuration
1:03a735b:    * @para    DataMapJob dataMapJob = getDataMapJob(job.getConfiguration());
1:03a735b: m filterExpression
1:c09ef99:    */
1:c09ef99:   public static void setFilterPredicates(Configuration configuration, Expression filterExpression) {
1:c09ef99:     if (filterExpression == null) {
1:c09ef99:       return;
1:c09ef99:     }
1:531ecdf:     try {
1:c09ef99:       String filterString = ObjectSerializationUtil.convertObjectToString(filterExpression);
1:c09ef99:       configuration.set(FILTER_PREDICATE, filterString);
1:531ecdf:     } catch (Exception e) {
1:c09ef99:       throw new RuntimeException("Error while setting filter expression to Job", e);
1:c09ef99:     }
1:c09ef99:   }
1:c09ef99: 
1:c09ef99:   /**
1:a7faef8:    * Set the column projection column names
1:a7faef8:    *
1:a7faef8:    * @param configuration     Configuration info
1:a7faef8:    * @param projectionColumns projection columns name
1:a7faef8:    */
1:a7faef8:   public static void setColumnProjection(Configuration configuration, String[] projectionColumns) {
1:a7faef8:     Objects.requireNonNull(projectionColumns);
1:a7faef8:     if (projectionColumns.length < 1) {
1:a7faef8:       throw new RuntimeException("Projection can't be empty");
1:a7faef8:     }
1:a7faef8:     StringBuilder builder = new StringBuilder();
1:a7faef8:     for (String column : projectionColumns) {
1:a7faef8:       builder.append(column).append(",");
1:a7faef8:     }
1:a7faef8:     String columnString = builder.toString();
1:a7faef8:     columnString = columnString.substring(0, columnString.length() - 1);
1:a7faef8:     configuration.set(COLUMN_PROJECTION, columnString);
1:a7faef8:   }
1:a7faef8: 
1:a7faef8:   /**
1:a7faef8:    * Set the column projection column names from CarbonProjection
1:a7faef8:    *
1:a7faef8:    * @param configuration Configuration info
1:a7faef8:    * @param projection    CarbonProjection object that includes unique projection column name
1:a7faef8:    */
1:c09ef99:   public static void setColumnProjection(Configuration configuration, CarbonProjection projection) {
1:c09ef99:     if (projection == null || projection.isEmpty()) {
1:c09ef99:       return;
1:c09ef99:     }
1:c09ef99:     String[] allColumns = projection.getAllColumns();
1:c09ef99:     StringBuilder builder = new StringBuilder();
1:c09ef99:     for (String column : allColumns) {
1:c09ef99:       builder.append(column).append(",");
1:c09ef99:     }
1:c09ef99:     String columnString = builder.toString();
1:c09ef99:     columnString = columnString.substring(0, columnString.length() - 1);
1:c09ef99:     configuration.set(COLUMN_PROJECTION, columnString);
1:c09ef99:   }
1:c09ef99: 
1:c09ef99:   public static String getColumnProjection(Configuration configuration) {
1:c09ef99:     return configuration.get(COLUMN_PROJECTION);
1:c09ef99:   }
1:c09ef99: 
1:3ff574d:   public static void setFgDataMapPruning(Configuration configuration, boolean enable) {
1:3ff574d:     configuration.set(FGDATAMAP_PRUNING, String.valueOf(enable));
1:3ff574d:   }
1:3ff574d: 
1:3ff574d:   public static boolean isFgDataMapPruningEnable(Configuration configuration) {
1:3ff574d:     String enable = configuration.get(FGDATAMAP_PRUNING);
1:3ff574d: 
1:3ff574d:     // if FDDATAMAP_PRUNING is not set, by default we will use FGDataMap
1:3ff574d:     return (enable == null) || enable.equalsIgnoreCase("true");
1:3ff574d:   }
1:3ff574d: 
1:c09ef99:   /**
1:c09ef99:    * Set list of segments to access
1:c09ef99:    */
1:c09ef99:   public static void setSegmentsToAccess(Configuration configuration, List<Segment> validSegments) {
1:c09ef99:     configuration.set(INPUT_SEGMENT_NUMBERS, CarbonUtil.convertToString(validSegments));
1:c09ef99:   }
1:c09ef99: 
1:c09ef99:   /**
1:fb1516c:    * Set `CARBON_INPUT_SEGMENTS` from property to configuration
1:fb1516c:    */
1:c09ef99:   public static void setQuerySegment(Configuration conf, AbsoluteTableIdentifier identifier) {
1:c09ef99:     String dbName = identifier.getCarbonTableIdentifier().getDatabaseName().toLowerCase();
1:c09ef99:     String tbName = identifier.getCarbonTableIdentifier().getTableName().toLowerCase();
1:c09ef99:     String segmentNumbersFromProperty = CarbonProperties.getInstance()
1:c09ef99:         .getProperty(CarbonCommonConstants.CARBON_INPUT_SEGMENTS + dbName + "." + tbName, "*");
1:c09ef99:     if (!segmentNumbersFromProperty.trim().equals("*")) {
1:c58eb43:       CarbonInputFormat.setSegmentsToAccess(conf,
1:c58eb43:           Segment.toSegmentList(segmentNumbersFromProperty.split(","), null));
1:c09ef99:     }
1:c09ef99:   }
1:c09ef99: 
1:c09ef99:   /**
1:c09ef99:    * Set `CARBON_INPUT_SEGMENTS` from property to configuration
1:c09ef99:    */
1:fb1516c:   public static void setQuerySegment(Configuration conf, String segmentList) {
1:fb1516c:     if (!segmentList.trim().equals("*")) {
1:fb1516c:       CarbonInputFormat
1:c58eb43:           .setSegmentsToAccess(conf, Segment.toSegmentList(segmentList.split(","), null));
1:fb1516c:     }
1:fb1516c:   }
1:fb1516c: 
1:fb1516c:   /**
1:c09ef99:    * set list of segment to access
1:c09ef99:    */
1:c09ef99:   public static void setValidateSegmentsToAccess(Configuration configuration, Boolean validate) {
1:c09ef99:     configuration.set(CarbonInputFormat.VALIDATE_INPUT_SEGMENT_IDs, validate.toString());
1:c09ef99:   }
1:c09ef99: 
1:c09ef99:   /**
1:c09ef99:    * get list of segment to access
1:c09ef99:    */
1:c09ef99:   public static boolean getValidateSegmentsToAccess(Configuration configuration) {
1:c09ef99:     return configuration.get(CarbonInputFormat.VALIDATE_INPUT_SEGMENT_IDs, "true")
1:c09ef99:         .equalsIgnoreCase("true");
1:c09ef99:   }
1:c09ef99: 
1:c09ef99:   /**
1:c09ef99:    * set list of partitions to prune
1:c09ef99:    */
1:c09ef99:   public static void setPartitionsToPrune(Configuration configuration,
1:c09ef99:       List<PartitionSpec> partitions) {
1:c09ef99:     if (partitions == null) {
1:c09ef99:       return;
1:c09ef99:     }
2:c09ef99:     try {
1:c09ef99:       String partitionString =
1:c09ef99:           ObjectSerializationUtil.convertObjectToString(new ArrayList<>(partitions));
1:c09ef99:       configuration.set(PARTITIONS_TO_PRUNE, partitionString);
2:c09ef99:     } catch (Exception e) {
1:c09ef99:       throw new RuntimeException("Error while setting patition information to Job" + partitions, e);
1:c09ef99:     }
1:c09ef99:   }
1:c09ef99: 
1:c09ef99:   /**
1:c09ef99:    * get list of partitions to prune
1:c09ef99:    */
1:c09ef99:   public static List<PartitionSpec> getPartitionsToPrune(Configuration configuration)
1:c09ef99:       throws IOException {
1:c09ef99:     String partitionString = configuration.get(PARTITIONS_TO_PRUNE);
1:c09ef99:     if (partitionString != null) {
1:c09ef99:       return (List<PartitionSpec>) ObjectSerializationUtil.convertStringToObject(partitionString);
1:c09ef99:     }
1:c09ef99:     return null;
1:c09ef99:   }
1:c09ef99: 
1:c09ef99:   public AbsoluteTableIdentifier getAbsoluteTableIdentifier(Configuration configuration)
1:c09ef99:       throws IOException {
1:c09ef99:     String tablePath = configuration.get(INPUT_DIR, "");
1:c09ef99:     try {
1:c09ef99:       return AbsoluteTableIdentifier
1:c09ef99:           .from(tablePath, getDatabaseName(configuration), getTableName(configuration));
1:c09ef99:     } catch (InvalidConfigurationException e) {
1:c09ef99:       throw new IOException(e);
1:c09ef99:     }
1:c09ef99:   }
1:c09ef99: 
1:347b8e1:   public static void setReadCommittedScope(Configuration configuration,
1:347b8e1:       ReadCommittedScope committedScope) {
1:347b8e1:     if (committedScope == null) {
1:347b8e1:       return;
1:347b8e1:     }
1:347b8e1:     try {
1:347b8e1:       String subFoldersString = ObjectSerializationUtil.convertObjectToString(committedScope);
1:347b8e1:       configuration.set(READ_COMMITTED_SCOPE, subFoldersString);
1:347b8e1:     } catch (Exception e) {
1:347b8e1:       throw new RuntimeException(
1:347b8e1:           "Error while setting committedScope information to Job" + committedScope, e);
1:347b8e1:     }
1:347b8e1:   }
1:347b8e1: 
1:347b8e1:   public static ReadCommittedScope getReadCommittedScope(Configuration configuration)
1:347b8e1:       throws IOException {
1:347b8e1:     String subFoldersString = configuration.get(READ_COMMITTED_SCOPE);
1:347b8e1:     if (subFoldersString != null) {
1:347b8e1:       return (ReadCommittedScope) ObjectSerializationUtil.convertStringToObject(subFoldersString);
1:347b8e1:     }
1:347b8e1:     return null;
1:347b8e1:   }
1:347b8e1: 
1:c09ef99:   /**
1:c09ef99:    * {@inheritDoc}
1:c09ef99:    * Configurations FileInputFormat.INPUT_DIR
1:c09ef99:    * are used to get table path to read.
1:c09ef99:    *
1:c09ef99:    * @param job
1:c09ef99:    * @return List<InputSplit> list of CarbonInputSplit
2:c09ef99:    * @throws IOException
1:c09ef99:    */
1:c09ef99:   @Override public abstract List<InputSplit> getSplits(JobContext job) throws IOException;
1:c09ef99: 
1:c09ef99:   protected Expression getFilterPredicates(Configuration configuration) {
1:c09ef99:     try {
1:c09ef99:       String filterExprString = configuration.get(FILTER_PREDICATE);
1:c09ef99:       if (filterExprString == null) {
1:c09ef99:         return null;
1:c09ef99:       }
1:c09ef99:       Object filter = ObjectSerializationUtil.convertStringToObject(filterExprString);
1:c09ef99:       return (Expression) filter;
1:c09ef99:     } catch (IOException e) {
1:c09ef99:       throw new RuntimeException("Error while reading filter expression", e);
1:c09ef99:     }
1:c09ef99:   }
1:c09ef99: 
1:c09ef99:   /**
1:c09ef99:    * get data blocks of given segment
1:c09ef99:    */
1:c58eb43:   protected List<CarbonInputSplit> getDataBlocksOfSegment(JobContext job, CarbonTable carbonTable,
1:3894e1d:       Expression expression, BitSet matchedPartitions, List<Segment> segmentIds,
1:c58eb43:       PartitionInfo partitionInfo, List<Integer> oldPartitionIdList) throws IOException {
1:c09ef99: 
1:c09ef99:     QueryStatisticsRecorder recorder = CarbonTimeStatisticsFactory.createDriverRecorder();
1:c09ef99:     QueryStatistic statistic = new QueryStatistic();
1:c09ef99: 
1:c09ef99:     // get tokens for all the required FileSystem for table path
1:c09ef99:     TokenCache.obtainTokensForNamenodes(job.getCredentials(),
1:7e0803f:         new Path[] { new Path(carbonTable.getTablePath()) }, job.getConfiguration());
1:4a47630:     List<ExtendedBlocklet> prunedBlocklets =
1:3894e1d:         getPrunedBlocklets(job, carbonTable, expression, segmentIds);
1:03a735b: 
1:b338459:     List<CarbonInputSplit> resultFilteredBlocks = new ArrayList<>();
1:c09ef99:     int partitionIndex = 0;
1:c09ef99:     List<Integer> partitionIdList = new ArrayList<>();
1:c09ef99:     if (partitionInfo != null && partitionInfo.getPartitionType() != PartitionType.NATIVE_HIVE) {
1:c09ef99:       partitionIdList = partitionInfo.getPartitionIds();
1:531ecdf:     }
1:c09ef99:     for (ExtendedBlocklet blocklet : prunedBlocklets) {
1:c09ef99:       long partitionId = CarbonTablePath.DataFileUtil
1:c09ef99:           .getTaskIdFromTaskNo(CarbonTablePath.DataFileUtil.getTaskNo(blocklet.getPath()));
1:03a735b: 
1:c09ef99:       // OldPartitionIdList is only used in alter table partition command because it change
1:c09ef99:       // partition info first and then read data.
1:c09ef99:       // For other normal query should use newest partitionIdList
1:c09ef99:       if (partitionInfo != null && partitionInfo.getPartitionType() != PartitionType.NATIVE_HIVE) {
1:c09ef99:         if (oldPartitionIdList != null) {
1:c09ef99:           partitionIndex = oldPartitionIdList.indexOf((int) partitionId);
1:c09ef99:         } else {
1:c09ef99:           partitionIndex = partitionIdList.indexOf((int) partitionId);
1:531ecdf:         }
1:c09ef99:       }
1:c09ef99:       if (partitionIndex != -1) {
1:c09ef99:         // matchedPartitions variable will be null in two cases as follows
1:c09ef99:         // 1. the table is not a partition table
1:c09ef99:         // 2. the table is a partition table, and all partitions are matched by query
1:c09ef99:         // for partition table, the task id of carbaondata file name is the partition id.
1:c09ef99:         // if this partition is not required, here will skip it.
1:c09ef99:         if (matchedPartitions == null || matchedPartitions.get(partitionIndex)) {
1:c09ef99:           CarbonInputSplit inputSplit = convertToCarbonInputSplit(blocklet);
1:c09ef99:           if (inputSplit != null) {
1:b338459:             resultFilteredBlocks.add(inputSplit);
1:c09ef99:           }
1:c09ef99:         }
1:c09ef99:       }
1:c09ef99:     }
1:c09ef99:     statistic
1:c09ef99:         .addStatistics(QueryStatisticsConstants.LOAD_BLOCKS_DRIVER, System.currentTimeMillis());
1:c09ef99:     recorder.recordStatisticsForDriver(statistic, job.getConfiguration().get("query.id"));
1:b338459:     return resultFilteredBlocks;
1:c09ef99:   }
1:531ecdf: 
1:c09ef99:   /**
1:22958d9:    * for explain command
1:22958d9:    * get number of block by counting distinct file path of blocklets
1:22958d9:    */
1:22958d9:   private int getBlockCount(List<ExtendedBlocklet> blocklets) {
1:22958d9:     Set<String> filepaths = new HashSet<>();
1:22958d9:     for (ExtendedBlocklet blocklet: blocklets) {
1:22958d9:       filepaths.add(blocklet.getPath());
1:22958d9:     }
1:22958d9:     return filepaths.size();
1:22958d9:   }
1:22958d9: 
1:22958d9:   /**
1:4a47630:    * Prune the blocklets using the filter expression with available datamaps.
1:aeb2ec4:    * First pruned with default blocklet datamap, then pruned with CG and FG datamaps
1:c09ef99:    */
1:4a47630:   private List<ExtendedBlocklet> getPrunedBlocklets(JobContext job, CarbonTable carbonTable,
1:3894e1d:       Expression expression, List<Segment> segmentIds) throws IOException {
1:03a735b:     ExplainCollector.addPruningInfo(carbonTable.getTableName());
1:3894e1d:     FilterResolverIntf resolver = null;
1:3894e1d:     if (expression != null) {
1:3894e1d:       carbonTable.processFilterExpression(expression, null, null);
1:3894e1d:       resolver = CarbonTable.resolveFilter(expression, carbonTable.getAbsoluteTableIdentifier());
1:3894e1d:       ExplainCollector.setFilterStatement(expression.getStatement());
1:03a735b:     } else {
1:03a735b:       ExplainCollector.setFilterStatement("none");
1:03a735b:     }
1:03a735b: 
1:4a47630:     boolean distributedCG = Boolean.parseBoolean(CarbonProperties.getInstance()
1:4a47630:         .getProperty(CarbonCommonConstants.USE_DISTRIBUTED_DATAMAP,
1:4a47630:             CarbonCommonConstants.USE_DISTRIBUTED_DATAMAP_DEFAULT));
1:2018048:     DataMapJob dataMapJob = DataMapUtil.getDataMapJob(job.getConfiguration());
1:4a47630:     List<PartitionSpec> partitionsToPrune = getPartitionsToPrune(job.getConfiguration());
1:4a47630:     // First prune using default datamap on driver side.
1:3894e1d:     TableDataMap defaultDataMap = DataMapStoreManager.getInstance().getDefaultDataMap(carbonTable);
1:3894e1d:     List<ExtendedBlocklet> prunedBlocklets = null;
1:3894e1d:     if (carbonTable.isTransactionalTable()) {
1:3894e1d:       prunedBlocklets = defaultDataMap.prune(segmentIds, resolver, partitionsToPrune);
1:3894e1d:     } else {
1:3894e1d:       prunedBlocklets = defaultDataMap.prune(segmentIds, expression, partitionsToPrune);
1:3894e1d:     }
1:c0de9f1: 
1:22958d9:     ExplainCollector.setDefaultDataMapPruningBlockHit(getBlockCount(prunedBlocklets));
1:22958d9: 
1:aeb2ec4:     if (prunedBlocklets.size() == 0) {
1:aeb2ec4:       return prunedBlocklets;
1:aeb2ec4:     }
1:531ecdf: 
1:747be9b:     DataMapChooser chooser = new DataMapChooser(getOrCreateCarbonTable(job.getConfiguration()));
1:747be9b: 
1:4a47630:     // Get the available CG datamaps and prune further.
1:747be9b:     DataMapExprWrapper cgDataMapExprWrapper = chooser.chooseCGDataMap(resolver);
1:4a47630:     if (cgDataMapExprWrapper != null) {
1:4a47630:       // Prune segments from already pruned blocklets
1:4a47630:       pruneSegments(segmentIds, prunedBlocklets);
1:aeb2ec4:       List<ExtendedBlocklet> cgPrunedBlocklets;
1:4a47630:       // Again prune with CG datamap.
1:4a47630:       if (distributedCG && dataMapJob != null) {
1:aeb2ec4:         cgPrunedBlocklets = DataMapUtil.executeDataMapJob(carbonTable,
1:aeb2ec4:             resolver, segmentIds, cgDataMapExprWrapper, dataMapJob, partitionsToPrune);
1:4a47630:       } else {
1:aeb2ec4:         cgPrunedBlocklets = cgDataMapExprWrapper.prune(segmentIds, partitionsToPrune);
1:4a47630:       }
1:aeb2ec4:       // since index datamap prune in segment scope,
1:aeb2ec4:       // the result need to intersect with previous pruned result
1:1cea4d3:       prunedBlocklets = intersectFilteredBlocklets(carbonTable, prunedBlocklets, cgPrunedBlocklets);
1:03a735b:       ExplainCollector.recordCGDataMapPruning(
1:c0de9f1:           DataMapWrapperSimpleInfo.fromDataMapWrapper(cgDataMapExprWrapper),
1:22958d9:           prunedBlocklets.size(), getBlockCount(prunedBlocklets));
1:4a47630:     }
1:aeb2ec4: 
1:aeb2ec4:     if (prunedBlocklets.size() == 0) {
1:4a47630:       return prunedBlocklets;
1:aeb2ec4:     }
1:4a47630:     // Now try to prune with FG DataMap.
1:747be9b:     if (isFgDataMapPruningEnable(job.getConfiguration()) && dataMapJob != null) {
1:747be9b:       DataMapExprWrapper fgDataMapExprWrapper = chooser.chooseFGDataMap(resolver);
1:747be9b:       if (fgDataMapExprWrapper != null) {
1:4a47630:         // Prune segments from already pruned blocklets
1:4a47630:         pruneSegments(segmentIds, prunedBlocklets);
1:aeb2ec4:         List<ExtendedBlocklet> fgPrunedBlocklets = DataMapUtil.executeDataMapJob(carbonTable,
1:aeb2ec4:             resolver, segmentIds, fgDataMapExprWrapper, dataMapJob, partitionsToPrune);
1:aeb2ec4:         // note that the 'fgPrunedBlocklets' has extra datamap related info compared with
1:aeb2ec4:         // 'prunedBlocklets', so the intersection should keep the elements in 'fgPrunedBlocklets'
1:1cea4d3:         prunedBlocklets = intersectFilteredBlocklets(carbonTable, prunedBlocklets,
1:1cea4d3:             fgPrunedBlocklets);
1:c0de9f1:         ExplainCollector.recordFGDataMapPruning(
1:c0de9f1:             DataMapWrapperSimpleInfo.fromDataMapWrapper(fgDataMapExprWrapper),
1:22958d9:             prunedBlocklets.size(), getBlockCount(prunedBlocklets));
1:4a47630:       }
1:4a47630:     }
1:4a47630:     return prunedBlocklets;
1:4a47630:   }
1:4a47630: 
1:1cea4d3:   private List<ExtendedBlocklet> intersectFilteredBlocklets(CarbonTable carbonTable,
1:1cea4d3:       List<ExtendedBlocklet> previousDataMapPrunedBlocklets,
1:1cea4d3:       List<ExtendedBlocklet> otherDataMapPrunedBlocklets) {
1:1cea4d3:     List<ExtendedBlocklet> prunedBlocklets = null;
1:3cbabcd:     if (BlockletDataMapUtil.isCacheLevelBlock(carbonTable)) {
1:1cea4d3:       prunedBlocklets = new ArrayList<>();
1:1cea4d3:       for (ExtendedBlocklet otherBlocklet : otherDataMapPrunedBlocklets) {
1:1cea4d3:         if (previousDataMapPrunedBlocklets.contains(otherBlocklet)) {
1:1cea4d3:           prunedBlocklets.add(otherBlocklet);
1:1cea4d3:         }
1:1cea4d3:       }
1:1cea4d3:     } else {
1:1cea4d3:       prunedBlocklets = (List) CollectionUtils
1:1cea4d3:           .intersection(otherDataMapPrunedBlocklets, previousDataMapPrunedBlocklets);
1:1cea4d3:     }
1:1cea4d3:     return prunedBlocklets;
1:1cea4d3:   }
1:4a47630: 
1:4a47630:   /**
1:4a47630:    * Prune the segments from the already pruned blocklets.
1:4a47630:    * @param segments
1:4a47630:    * @param prunedBlocklets
1:4a47630:    */
1:4a47630:   private void pruneSegments(List<Segment> segments, List<ExtendedBlocklet> prunedBlocklets) {
1:4a47630:     List<Segment> toBeRemovedSegments = new ArrayList<>();
1:4a47630:     for (Segment segment : segments) {
1:4a47630:       boolean found = false;
1:4a47630:       // Clear the old pruned index files if any present
1:8b33ab2:       segment.getFilteredIndexShardNames().clear();
1:4a47630:       // Check the segment exist in any of the pruned blocklets.
1:4a47630:       for (ExtendedBlocklet blocklet : prunedBlocklets) {
1:60dfdd3:         if (blocklet.getSegmentId().equals(segment.toString())) {
1:4a47630:           found = true;
1:4a47630:           // Set the pruned index file to the segment for further pruning.
1:9db662a:           String shardName = CarbonTablePath.getShardName(blocklet.getFilePath());
1:9db662a:           segment.setFilteredIndexShardName(shardName);
1:4a47630:         }
1:4a47630:       }
1:4a47630:       // Add to remove segments list if not present in pruned blocklets.
1:4a47630:       if (!found) {
1:4a47630:         toBeRemovedSegments.add(segment);
1:4a47630:       }
1:4a47630:     }
1:4a47630:     // Remove all segments which are already pruned from pruned blocklets
1:4a47630:     segments.removeAll(toBeRemovedSegments);
1:4a47630:   }
1:4a47630: 
1:c09ef99:   private CarbonInputSplit convertToCarbonInputSplit(ExtendedBlocklet blocklet) throws IOException {
1:c09ef99:     CarbonInputSplit split = CarbonInputSplit
1:c09ef99:         .from(blocklet.getSegmentId(), blocklet.getBlockletId(),
1:c09ef99:             new FileSplit(new Path(blocklet.getPath()), 0, blocklet.getLength(),
1:c09ef99:                 blocklet.getLocations()),
1:c09ef99:             ColumnarFormatVersion.valueOf((short) blocklet.getDetailInfo().getVersionNumber()),
1:c09ef99:             blocklet.getDataMapWriterPath());
1:c09ef99:     split.setDetailInfo(blocklet.getDetailInfo());
1:c09ef99:     return split;
1:aeb2ec4:   }
1:1cea4d3: 
1:c09ef99:   @Override public RecordReader<Void, T> createRecordReader(InputSplit inputSplit,
1:c09ef99:       TaskAttemptContext taskAttemptContext) throws IOException, InterruptedException {
1:c09ef99:     Configuration configuration = taskAttemptContext.getConfiguration();
1:c09ef99:     QueryModel queryModel = createQueryModel(inputSplit, taskAttemptContext);
1:c09ef99:     CarbonReadSupport<T> readSupport = getReadSupportClass(configuration);
1:2a9604c:     return new CarbonRecordReader<T>(queryModel, readSupport,
1:2a9604c:         taskAttemptContext.getConfiguration());
1:747be9b:   }
1:03a735b: 
1:c09ef99:   public QueryModel createQueryModel(InputSplit inputSplit, TaskAttemptContext taskAttemptContext)
1:c09ef99:       throws IOException {
1:c09ef99:     Configuration configuration = taskAttemptContext.getConfiguration();
1:c09ef99:     CarbonTable carbonTable = getOrCreateCarbonTable(configuration);
1:c09ef99: 
1:3ff574d:     // set projection column in the query model
1:c09ef99:     String projectionString = getColumnProjection(configuration);
1:3ff574d:     String[] projectColumns;
1:c09ef99:     if (projectionString != null) {
1:3ff574d:       projectColumns = projectionString.split(",");
1:3ff574d:     } else {
1:3ff574d:       projectColumns = new String[]{};
1:c09ef99:     }
1:3ff574d:     QueryModel queryModel = new QueryModelBuilder(carbonTable)
1:3ff574d:         .projectColumns(projectColumns)
1:3ff574d:         .filterExpression(getFilterPredicates(configuration))
1:3ff574d:         .dataConverter(getDataTypeConverter(configuration))
1:3ff574d:         .build();
1:c09ef99: 
1:c09ef99:     // update the file level index store if there are invalid segment
1:c09ef99:     if (inputSplit instanceof CarbonMultiBlockSplit) {
1:c09ef99:       CarbonMultiBlockSplit split = (CarbonMultiBlockSplit) inputSplit;
1:c09ef99:       List<String> invalidSegments = split.getAllSplits().get(0).getInvalidSegments();
1:c09ef99:       if (invalidSegments.size() > 0) {
1:c09ef99:         queryModel.setInvalidSegmentIds(invalidSegments);
1:c09ef99:       }
1:c09ef99:       List<UpdateVO> invalidTimestampRangeList =
1:c09ef99:           split.getAllSplits().get(0).getInvalidTimestampRange();
1:c09ef99:       if ((null != invalidTimestampRangeList) && (invalidTimestampRangeList.size() > 0)) {
1:c09ef99:         queryModel.setInvalidBlockForSegmentId(invalidTimestampRangeList);
1:c09ef99:       }
1:c09ef99:     }
1:c09ef99:     return queryModel;
1:c09ef99:   }
1:c09ef99: 
1:c09ef99:   public CarbonReadSupport<T> getReadSupportClass(Configuration configuration) {
1:c09ef99:     String readSupportClass = configuration.get(CARBON_READ_SUPPORT);
1:c09ef99:     //By default it uses dictionary decoder read class
1:c09ef99:     CarbonReadSupport<T> readSupport = null;
1:c09ef99:     if (readSupportClass != null) {
1:c09ef99:       try {
1:c09ef99:         Class<?> myClass = Class.forName(readSupportClass);
1:c09ef99:         Constructor<?> constructor = myClass.getConstructors()[0];
1:c09ef99:         Object object = constructor.newInstance();
1:c09ef99:         if (object instanceof CarbonReadSupport) {
1:c09ef99:           readSupport = (CarbonReadSupport) object;
1:c09ef99:         }
1:c09ef99:       } catch (ClassNotFoundException ex) {
1:c09ef99:         LOG.error("Class " + readSupportClass + "not found", ex);
1:c09ef99:       } catch (Exception ex) {
1:c09ef99:         LOG.error("Error while creating " + readSupportClass, ex);
1:c09ef99:       }
1:c09ef99:     } else {
1:c09ef99:       readSupport = new DictionaryDecodeReadSupport<>();
1:c09ef99:     }
1:c09ef99:     return readSupport;
1:c09ef99:   }
1:c09ef99: 
1:c09ef99:   @Override protected boolean isSplitable(JobContext context, Path filename) {
1:c09ef99:     try {
1:c09ef99:       // Don't split the file if it is local file system
1:c09ef99:       FileSystem fileSystem = filename.getFileSystem(context.getConfiguration());
1:c09ef99:       if (fileSystem instanceof LocalFileSystem) {
1:c09ef99:         return false;
1:c09ef99:       }
1:c09ef99:     } catch (Exception e) {
1:c09ef99:       return true;
1:c09ef99:     }
1:c09ef99:     return true;
1:c09ef99:   }
1:5f68a79: 
1:c09ef99:   public static void setCarbonReadSupport(Configuration configuration,
1:c09ef99:       Class<? extends CarbonReadSupport> readSupportClass) {
1:c09ef99:     if (readSupportClass != null) {
1:c09ef99:       configuration.set(CARBON_READ_SUPPORT, readSupportClass.getName());
1:c09ef99:     }
1:c09ef99:   }
1:c09ef99: 
1:5f68a79:   /**
1:c09ef99:    * It is optional, if user does not set then it reads from store
1:c09ef99:    *
1:c09ef99:    * @param configuration
1:cfb8ed9:    * @param converterClass is the Data type converter for different computing engine
1:c09ef99:    */
1:cfb8ed9:   public static void setDataTypeConverter(
1:cfb8ed9:       Configuration configuration, Class<? extends DataTypeConverter> converterClass) {
1:cfb8ed9:     if (null != converterClass) {
1:cfb8ed9:       configuration.set(CARBON_CONVERTER, converterClass.getCanonicalName());
1:c09ef99:     }
1:c09ef99:   }
1:c09ef99: 
1:c09ef99:   public static DataTypeConverter getDataTypeConverter(Configuration configuration)
1:c09ef99:       throws IOException {
1:cfb8ed9:     String converterClass = configuration.get(CARBON_CONVERTER);
1:cfb8ed9:     if (converterClass == null) {
1:c09ef99:       return new DataTypeConverterImpl();
1:c09ef99:     }
1:cfb8ed9: 
1:cfb8ed9:     try {
1:cfb8ed9:       return (DataTypeConverter) Class.forName(converterClass).newInstance();
1:cfb8ed9:     } catch (Exception e) {
1:cfb8ed9:       throw new IOException(e);
1:cfb8ed9:     }
1:c09ef99:   }
1:c09ef99: 
1:c09ef99:   public static void setDatabaseName(Configuration configuration, String databaseName) {
1:c09ef99:     if (null != databaseName) {
1:c09ef99:       configuration.set(DATABASE_NAME, databaseName);
1:c09ef99:     }
1:c09ef99:   }
1:c09ef99: 
1:c09ef99:   public static String getDatabaseName(Configuration configuration)
1:c09ef99:       throws InvalidConfigurationException {
1:c09ef99:     String databseName = configuration.get(DATABASE_NAME);
1:c09ef99:     if (null == databseName) {
1:c09ef99:       throw new InvalidConfigurationException("Database name is not set.");
1:c09ef99:     }
1:c09ef99:     return databseName;
1:c09ef99:   }
1:c09ef99: 
1:c09ef99:   public static void setTableName(Configuration configuration, String tableName) {
1:c09ef99:     if (null != tableName) {
1:c09ef99:       configuration.set(TABLE_NAME, tableName);
1:c09ef99:     }
1:c09ef99:   }
1:c09ef99: 
1:c09ef99:   public static String getTableName(Configuration configuration)
1:c09ef99:       throws InvalidConfigurationException {
1:c09ef99:     String tableName = configuration.get(TABLE_NAME);
1:c09ef99:     if (tableName == null) {
1:c09ef99:       throw new InvalidConfigurationException("Table name is not set");
1:c09ef99:     }
1:c09ef99:     return tableName;
1:c09ef99:   }
1:fb1516c: 
1:fb1516c:   public static void setAccessStreamingSegments(Configuration configuration, Boolean validate)
1:fb1516c:       throws InvalidConfigurationException {
1:fb1516c:     configuration.set(
1:fb1516c:         CarbonCommonConstantsInternal.QUERY_ON_PRE_AGG_STREAMING + "." + getDatabaseName(
1:fb1516c:             configuration) + "." + getTableName(configuration), validate.toString());
1:fb1516c:   }
1:fb1516c: 
1:fb1516c:   public static boolean getAccessStreamingSegments(Configuration configuration) {
1:fb1516c:     try {
1:fb1516c:       return configuration.get(
1:fb1516c:           CarbonCommonConstantsInternal.QUERY_ON_PRE_AGG_STREAMING + "." + getDatabaseName(
1:fb1516c:               configuration) + "." + getTableName(
1:fb1516c:                   configuration), "false").equalsIgnoreCase("true");
1:fb1516c: 
1:fb1516c:     } catch (InvalidConfigurationException e) {
1:fb1516c:       return false;
1:fb1516c:     }
1:fb1516c:   }
1:c09ef99: 
1:c09ef99:   /**
1:5f68a79:    * Project all Columns for carbon reader
1:5f68a79:    *
1:5f68a79:    * @return String araay of columnNames
1:5f68a79:    * @param carbonTable
1:5f68a79:    */
1:5f68a79:   public String[] projectAllColumns(CarbonTable carbonTable) {
1:5f68a79:     List<ColumnSchema> colList = carbonTable.getTableInfo().getFactTable().getListOfColumns();
1:5f68a79:     List<String> projectColumn = new ArrayList<>();
1:5f68a79:     for (ColumnSchema cols : colList) {
1:5f68a79:       if (cols.getSchemaOrdinal() != -1) {
1:fb6dffe:         projectColumn.add(cols.getColumnName());
1:5f68a79:       }
1:5f68a79:     }
1:5f68a79:     String[] projectionColumns = new String[projectColumn.size()];
1:5f68a79:     int i = 0;
1:5f68a79:     for (String columnName : projectColumn) {
1:5f68a79:       projectionColumns[i] = columnName;
1:5f68a79:       i++;
1:5f68a79:     }
1:5f68a79:     return projectionColumns;
1:5f68a79:   }
1:c09ef99: }
============================================================================
author:Manhua
-------------------------------------------------------------------------------
commit:22958d9
/////////////////////////////////////////////////////////////////////////
1: import java.util.*;
/////////////////////////////////////////////////////////////////////////
1:    * for explain command
1:    * get number of block by counting distinct file path of blocklets
1:    */
1:   private int getBlockCount(List<ExtendedBlocklet> blocklets) {
1:     Set<String> filepaths = new HashSet<>();
1:     for (ExtendedBlocklet blocklet: blocklets) {
1:       filepaths.add(blocklet.getPath());
1:     }
1:     return filepaths.size();
1:   }
1: 
1:   /**
/////////////////////////////////////////////////////////////////////////
1:     ExplainCollector.setDefaultDataMapPruningBlockHit(getBlockCount(prunedBlocklets));
1: 
/////////////////////////////////////////////////////////////////////////
1:           prunedBlocklets.size(), getBlockCount(prunedBlocklets));
/////////////////////////////////////////////////////////////////////////
1:             prunedBlocklets.size(), getBlockCount(prunedBlocklets));
commit:3cbabcd
/////////////////////////////////////////////////////////////////////////
/////////////////////////////////////////////////////////////////////////
/////////////////////////////////////////////////////////////////////////
1:     if (BlockletDataMapUtil.isCacheLevelBlock(carbonTable)) {
author:QiangCai
-------------------------------------------------------------------------------
commit:0528a79
/////////////////////////////////////////////////////////////////////////
1:   protected int numStreamFiles = 0;
1:   protected int hitedStreamFiles = 0;
/////////////////////////////////////////////////////////////////////////
1:   public int getNumStreamFiles() {
1:     return numStreamFiles;
1:   }
1: 
1:   public int getHitedStreamFiles() {
1:     return hitedStreamFiles;
1:   }
1: 
commit:d5bec4d
/////////////////////////////////////////////////////////////////////////
1:   // record segment number and hit blocks
1:   protected int numSegments = 0;
1:   protected int numStreamSegments = 0;
1:   protected int numBlocks = 0;
1: 
1:   public int getNumSegments() {
1:     return numSegments;
1:   }
1: 
1:   public int getNumStreamSegments() {
1:     return numStreamSegments;
1:   }
1: 
1:   public int getNumBlocks() {
1:     return numBlocks;
1:   }
1: 
author:ravipesala
-------------------------------------------------------------------------------
commit:3894e1d
/////////////////////////////////////////////////////////////////////////
1: import org.apache.carbondata.core.datamap.DataMapStoreManager;
1: import org.apache.carbondata.core.datamap.TableDataMap;
/////////////////////////////////////////////////////////////////////////
1:       Expression expression, BitSet matchedPartitions, List<Segment> segmentIds,
/////////////////////////////////////////////////////////////////////////
1:         getPrunedBlocklets(job, carbonTable, expression, segmentIds);
/////////////////////////////////////////////////////////////////////////
1:       Expression expression, List<Segment> segmentIds) throws IOException {
1:     FilterResolverIntf resolver = null;
1:     if (expression != null) {
1:       carbonTable.processFilterExpression(expression, null, null);
1:       resolver = CarbonTable.resolveFilter(expression, carbonTable.getAbsoluteTableIdentifier());
1:       ExplainCollector.setFilterStatement(expression.getStatement());
/////////////////////////////////////////////////////////////////////////
1:     TableDataMap defaultDataMap = DataMapStoreManager.getInstance().getDefaultDataMap(carbonTable);
1:     List<ExtendedBlocklet> prunedBlocklets = null;
1:     if (carbonTable.isTransactionalTable()) {
1:       prunedBlocklets = defaultDataMap.prune(segmentIds, resolver, partitionsToPrune);
1:     } else {
1:       prunedBlocklets = defaultDataMap.prune(segmentIds, expression, partitionsToPrune);
1:     }
commit:347b8e1
/////////////////////////////////////////////////////////////////////////
1: import org.apache.carbondata.core.readcommitter.ReadCommittedScope;
/////////////////////////////////////////////////////////////////////////
1:   private static final String READ_COMMITTED_SCOPE =
1:       "mapreduce.input.carboninputformat.read.committed.scope";
/////////////////////////////////////////////////////////////////////////
1:   public static void setReadCommittedScope(Configuration configuration,
1:       ReadCommittedScope committedScope) {
1:     if (committedScope == null) {
1:       return;
1:     }
1:     try {
1:       String subFoldersString = ObjectSerializationUtil.convertObjectToString(committedScope);
1:       configuration.set(READ_COMMITTED_SCOPE, subFoldersString);
1:     } catch (Exception e) {
1:       throw new RuntimeException(
1:           "Error while setting committedScope information to Job" + committedScope, e);
1:     }
1:   }
1: 
1:   public static ReadCommittedScope getReadCommittedScope(Configuration configuration)
1:       throws IOException {
1:     String subFoldersString = configuration.get(READ_COMMITTED_SCOPE);
1:     if (subFoldersString != null) {
1:       return (ReadCommittedScope) ObjectSerializationUtil.convertStringToObject(subFoldersString);
1:     }
1:     return null;
1:   }
1: 
commit:60dfdd3
/////////////////////////////////////////////////////////////////////////
1:         if (blocklet.getSegmentId().equals(segment.toString())) {
commit:8b33ab2
/////////////////////////////////////////////////////////////////////////
1:       segment.getFilteredIndexShardNames().clear();
0:           String uniqueTaskName = CarbonTablePath.getUniqueTaskName(blocklet.getTaskName());
0:           segment.setFilteredIndexShardName(uniqueTaskName);
commit:4a47630
/////////////////////////////////////////////////////////////////////////
1:     List<ExtendedBlocklet> prunedBlocklets =
0:         getPrunedBlocklets(job, carbonTable, resolver, segmentIds);
/////////////////////////////////////////////////////////////////////////
1:   /**
1:    * Prune the blocklets using the filter expression with available datamaps.
1:    */
1:   private List<ExtendedBlocklet> getPrunedBlocklets(JobContext job, CarbonTable carbonTable,
0:       FilterResolverIntf resolver, List<Segment> segmentIds) throws IOException {
1:     boolean distributedCG = Boolean.parseBoolean(CarbonProperties.getInstance()
1:         .getProperty(CarbonCommonConstants.USE_DISTRIBUTED_DATAMAP,
1:             CarbonCommonConstants.USE_DISTRIBUTED_DATAMAP_DEFAULT));
0:     DataMapJob dataMapJob = getDataMapJob(job.getConfiguration());
1:     List<PartitionSpec> partitionsToPrune = getPartitionsToPrune(job.getConfiguration());
1:     // First prune using default datamap on driver side.
0:     DataMapExprWrapper dataMapExprWrapper = DataMapChooser.get()
0:         .getDefaultDataMap(getOrCreateCarbonTable(job.getConfiguration()), resolver);
0:     List<ExtendedBlocklet> prunedBlocklets =
0:         dataMapExprWrapper.prune(segmentIds, partitionsToPrune);
1:     // Get the available CG datamaps and prune further.
0:     DataMapExprWrapper cgDataMapExprWrapper = DataMapChooser.get()
0:         .chooseCGDataMap(getOrCreateCarbonTable(job.getConfiguration()), resolver);
1:     if (cgDataMapExprWrapper != null) {
1:       // Prune segments from already pruned blocklets
1:       pruneSegments(segmentIds, prunedBlocklets);
1:       // Again prune with CG datamap.
1:       if (distributedCG && dataMapJob != null) {
0:         prunedBlocklets =
0:             executeDataMapJob(carbonTable, resolver, segmentIds, dataMapExprWrapper, dataMapJob,
0:                 partitionsToPrune);
1:       } else {
0:         prunedBlocklets = dataMapExprWrapper.prune(segmentIds, partitionsToPrune);
1:       }
1:     }
1:     // Now try to prune with FG DataMap.
0:     dataMapExprWrapper = DataMapChooser.get()
0:         .chooseFGDataMap(getOrCreateCarbonTable(job.getConfiguration()), resolver);
0:     if (dataMapExprWrapper != null && dataMapExprWrapper.getDataMapType() == DataMapLevel.FG
0:         && isFgDataMapPruningEnable(job.getConfiguration()) && dataMapJob != null) {
1:       // Prune segments from already pruned blocklets
1:       pruneSegments(segmentIds, prunedBlocklets);
0:       prunedBlocklets =
0:           executeDataMapJob(carbonTable, resolver, segmentIds, dataMapExprWrapper, dataMapJob,
0:               partitionsToPrune);
1:     }
1:     return prunedBlocklets;
1:   }
1: 
0:   private List<ExtendedBlocklet> executeDataMapJob(CarbonTable carbonTable,
0:       FilterResolverIntf resolver, List<Segment> segmentIds, DataMapExprWrapper dataMapExprWrapper,
0:       DataMapJob dataMapJob, List<PartitionSpec> partitionsToPrune) throws IOException {
0:     DistributableDataMapFormat datamapDstr =
0:         new DistributableDataMapFormat(carbonTable, dataMapExprWrapper, segmentIds,
0:             partitionsToPrune, BlockletDataMapFactory.class.getName());
0:     List<ExtendedBlocklet> prunedBlocklets = dataMapJob.execute(datamapDstr, resolver);
0:     // Apply expression on the blocklets.
0:     prunedBlocklets = dataMapExprWrapper.pruneBlocklets(prunedBlocklets);
1:     return prunedBlocklets;
1:   }
1: 
1:   /**
1:    * Prune the segments from the already pruned blocklets.
1:    * @param segments
1:    * @param prunedBlocklets
1:    */
1:   private void pruneSegments(List<Segment> segments, List<ExtendedBlocklet> prunedBlocklets) {
1:     List<Segment> toBeRemovedSegments = new ArrayList<>();
1:     for (Segment segment : segments) {
1:       boolean found = false;
1:       // Clear the old pruned index files if any present
0:       segment.getFilteredIndexFiles().clear();
1:       // Check the segment exist in any of the pruned blocklets.
1:       for (ExtendedBlocklet blocklet : prunedBlocklets) {
0:         if (blocklet.getSegmentId().equals(segment.getSegmentNo())) {
1:           found = true;
1:           // Set the pruned index file to the segment for further pruning.
0:           String carbonIndexFileName =
0:               CarbonTablePath.getCarbonIndexFileName(blocklet.getBlockId());
0:           segment.setFilteredIndexFile(carbonIndexFileName);
1:         }
1:       }
1:       // Add to remove segments list if not present in pruned blocklets.
1:       if (!found) {
1:         toBeRemovedSegments.add(segment);
1:       }
1:     }
1:     // Remove all segments which are already pruned from pruned blocklets
1:     segments.removeAll(toBeRemovedSegments);
1:   }
1: 
author:manishgupta88
-------------------------------------------------------------------------------
commit:fb6dffe
/////////////////////////////////////////////////////////////////////////
1:         projectColumn.add(cols.getColumnName());
author:kunal642
-------------------------------------------------------------------------------
commit:2a9604c
/////////////////////////////////////////////////////////////////////////
1:     return new CarbonRecordReader<T>(queryModel, readSupport,
1:         taskAttemptContext.getConfiguration());
author:xuchuanyin
-------------------------------------------------------------------------------
commit:1cea4d3
/////////////////////////////////////////////////////////////////////////
0: import org.apache.carbondata.core.indexstore.blockletindex.BlockletDataMapFactory;
/////////////////////////////////////////////////////////////////////////
1: import org.apache.carbondata.core.util.BlockletDataMapUtil;
/////////////////////////////////////////////////////////////////////////
1:       prunedBlocklets = intersectFilteredBlocklets(carbonTable, prunedBlocklets, cgPrunedBlocklets);
/////////////////////////////////////////////////////////////////////////
1:         prunedBlocklets = intersectFilteredBlocklets(carbonTable, prunedBlocklets,
1:             fgPrunedBlocklets);
/////////////////////////////////////////////////////////////////////////
1:   private List<ExtendedBlocklet> intersectFilteredBlocklets(CarbonTable carbonTable,
1:       List<ExtendedBlocklet> previousDataMapPrunedBlocklets,
1:       List<ExtendedBlocklet> otherDataMapPrunedBlocklets) {
1:     List<ExtendedBlocklet> prunedBlocklets = null;
0:     if (BlockletDataMapUtil.isCacheLevelBlock(
0:         carbonTable, BlockletDataMapFactory.CACHE_LEVEL_BLOCKLET)) {
1:       prunedBlocklets = new ArrayList<>();
1:       for (ExtendedBlocklet otherBlocklet : otherDataMapPrunedBlocklets) {
1:         if (previousDataMapPrunedBlocklets.contains(otherBlocklet)) {
1:           prunedBlocklets.add(otherBlocklet);
1:         }
1:       }
1:     } else {
1:       prunedBlocklets = (List) CollectionUtils
1:           .intersection(otherDataMapPrunedBlocklets, previousDataMapPrunedBlocklets);
1:     }
1:     return prunedBlocklets;
1:   }
1: 
commit:c0de9f1
/////////////////////////////////////////////////////////////////////////
1: import org.apache.carbondata.core.datamap.dev.expr.DataMapWrapperSimpleInfo;
/////////////////////////////////////////////////////////////////////////
0:     DataMapExprWrapper dataMapExprWrapper = DataMapChooser
0:         .getDefaultDataMap(getOrCreateCarbonTable(job.getConfiguration()), resolver);
0:     List<ExtendedBlocklet> prunedBlocklets =
0:         dataMapExprWrapper.prune(segmentIds, partitionsToPrune);
1: 
0:     ExplainCollector.recordDefaultDataMapPruning(
0:         DataMapWrapperSimpleInfo.fromDataMapWrapper(dataMapExprWrapper), prunedBlocklets.size());
/////////////////////////////////////////////////////////////////////////
1:           DataMapWrapperSimpleInfo.fromDataMapWrapper(cgDataMapExprWrapper),
0:           prunedBlocklets.size());
/////////////////////////////////////////////////////////////////////////
1:         ExplainCollector.recordFGDataMapPruning(
1:             DataMapWrapperSimpleInfo.fromDataMapWrapper(fgDataMapExprWrapper),
commit:aeb2ec4
/////////////////////////////////////////////////////////////////////////
1: import org.apache.commons.collections.CollectionUtils;
/////////////////////////////////////////////////////////////////////////
1:    * First pruned with default blocklet datamap, then pruned with CG and FG datamaps
/////////////////////////////////////////////////////////////////////////
0:     DataMapExprWrapper dataMapExprWrapper = DataMapChooser.getDefaultDataMap(
0:         getOrCreateCarbonTable(job.getConfiguration()), resolver);
0:     List<ExtendedBlocklet> prunedBlocklets = dataMapExprWrapper.prune(segmentIds,
0:         partitionsToPrune);
0:     ExplainCollector.recordDefaultDataMapPruning(dataMapExprWrapper.getDataMapSchema(),
0:         prunedBlocklets.size());
1:     if (prunedBlocklets.size() == 0) {
1:       return prunedBlocklets;
1:     }
/////////////////////////////////////////////////////////////////////////
1:       List<ExtendedBlocklet> cgPrunedBlocklets;
1:         cgPrunedBlocklets = DataMapUtil.executeDataMapJob(carbonTable,
1:             resolver, segmentIds, cgDataMapExprWrapper, dataMapJob, partitionsToPrune);
1:         cgPrunedBlocklets = cgDataMapExprWrapper.prune(segmentIds, partitionsToPrune);
1:       // since index datamap prune in segment scope,
1:       // the result need to intersect with previous pruned result
0:       prunedBlocklets = (List) CollectionUtils.intersection(
0:           cgPrunedBlocklets, prunedBlocklets);
1: 
1:     if (prunedBlocklets.size() == 0) {
0:       return prunedBlocklets;
1:     }
1:         List<ExtendedBlocklet> fgPrunedBlocklets = DataMapUtil.executeDataMapJob(carbonTable,
1:             resolver, segmentIds, fgDataMapExprWrapper, dataMapJob, partitionsToPrune);
1:         // note that the 'fgPrunedBlocklets' has extra datamap related info compared with
1:         // 'prunedBlocklets', so the intersection should keep the elements in 'fgPrunedBlocklets'
0:         prunedBlocklets = (List) CollectionUtils.intersection(fgPrunedBlocklets,
0:             prunedBlocklets);
0:         ExplainCollector.recordFGDataMapPruning(fgDataMapExprWrapper.getDataMapSchema(),
0:             prunedBlocklets.size());
1:     }
author:ajantha-bhat
-------------------------------------------------------------------------------
commit:5f68a79
/////////////////////////////////////////////////////////////////////////
1: import org.apache.carbondata.core.metadata.schema.table.column.ColumnSchema;
/////////////////////////////////////////////////////////////////////////
1: 
1:   /**
1:    * Project all Columns for carbon reader
1:    *
1:    * @return String araay of columnNames
1:    * @param carbonTable
1:    */
1:   public String[] projectAllColumns(CarbonTable carbonTable) {
1:     List<ColumnSchema> colList = carbonTable.getTableInfo().getFactTable().getListOfColumns();
1:     List<String> projectColumn = new ArrayList<>();
1:     for (ColumnSchema cols : colList) {
1:       if (cols.getSchemaOrdinal() != -1) {
0:         projectColumn.add(cols.getColumnUniqueId());
1:       }
1:     }
1:     String[] projectionColumns = new String[projectColumn.size()];
1:     int i = 0;
1:     for (String columnName : projectColumn) {
1:       projectionColumns[i] = columnName;
1:       i++;
1:     }
1:     return projectionColumns;
1:   }
commit:280a400
/////////////////////////////////////////////////////////////////////////
0: import org.apache.carbondata.core.readcommitter.ReadCommittedScope;
/////////////////////////////////////////////////////////////////////////
0:   private static final String UNMANAGED_TABLE = "mapreduce.input.carboninputformat.unmanaged";
/////////////////////////////////////////////////////////////////////////
0:   public static void setUnmanagedTable(Configuration configuration, boolean isUnmanagedTable) {
0:     configuration.set(UNMANAGED_TABLE, String.valueOf(isUnmanagedTable));
1:   }
1: 
/////////////////////////////////////////////////////////////////////////
0:       List<Integer> oldPartitionIdList, ReadCommittedScope readCommittedScope) throws IOException {
/////////////////////////////////////////////////////////////////////////
0:       prunedBlocklets = dataMapExprWrapper.prune(segmentIds, partitionsToPrune, readCommittedScope);
author:xubo245
-------------------------------------------------------------------------------
commit:b338459
/////////////////////////////////////////////////////////////////////////
1:     List<CarbonInputSplit> resultFilteredBlocks = new ArrayList<>();
/////////////////////////////////////////////////////////////////////////
1:             resultFilteredBlocks.add(inputSplit);
/////////////////////////////////////////////////////////////////////////
1:     return resultFilteredBlocks;
commit:a7faef8
/////////////////////////////////////////////////////////////////////////
0: import java.util.Objects;
/////////////////////////////////////////////////////////////////////////
1:   /**
1:    * Set the column projection column names
1:    *
1:    * @param configuration     Configuration info
1:    * @param projectionColumns projection columns name
1:    */
1:   public static void setColumnProjection(Configuration configuration, String[] projectionColumns) {
1:     Objects.requireNonNull(projectionColumns);
1:     if (projectionColumns.length < 1) {
1:       throw new RuntimeException("Projection can't be empty");
1:     }
1:     StringBuilder builder = new StringBuilder();
1:     for (String column : projectionColumns) {
1:       builder.append(column).append(",");
1:     }
1:     String columnString = builder.toString();
1:     columnString = columnString.substring(0, columnString.length() - 1);
1:     configuration.set(COLUMN_PROJECTION, columnString);
1:   }
1: 
1:   /**
1:    * Set the column projection column names from CarbonProjection
1:    *
1:    * @param configuration Configuration info
1:    * @param projection    CarbonProjection object that includes unique projection column name
1:    */
author:akashrn5
-------------------------------------------------------------------------------
commit:2018048
/////////////////////////////////////////////////////////////////////////
1: import org.apache.carbondata.core.datamap.DataMapJob;
1: import org.apache.carbondata.core.datamap.DataMapUtil;
/////////////////////////////////////////////////////////////////////////
1: import org.apache.carbondata.core.util.ObjectSerializationUtil;
/////////////////////////////////////////////////////////////////////////
/////////////////////////////////////////////////////////////////////////
/////////////////////////////////////////////////////////////////////////
/////////////////////////////////////////////////////////////////////////
1:     DataMapJob dataMapJob = DataMapUtil.getDataMapJob(job.getConfiguration());
/////////////////////////////////////////////////////////////////////////
0:         prunedBlocklets = DataMapUtil
0:             .executeDataMapJob(carbonTable, resolver, segmentIds, cgDataMapExprWrapper, dataMapJob,
/////////////////////////////////////////////////////////////////////////
0:         prunedBlocklets = DataMapUtil
0:             .executeDataMapJob(carbonTable, resolver, segmentIds, fgDataMapExprWrapper, dataMapJob,
/////////////////////////////////////////////////////////////////////////
author:Jacky Li
-------------------------------------------------------------------------------
commit:747be9b
/////////////////////////////////////////////////////////////////////////
/////////////////////////////////////////////////////////////////////////
0:     DataMapExprWrapper dataMapExprWrapper = DataMapChooser
/////////////////////////////////////////////////////////////////////////
1:     DataMapChooser chooser = new DataMapChooser(getOrCreateCarbonTable(job.getConfiguration()));
1: 
1:     DataMapExprWrapper cgDataMapExprWrapper = chooser.chooseCGDataMap(resolver);
/////////////////////////////////////////////////////////////////////////
1:     if (isFgDataMapPruningEnable(job.getConfiguration()) && dataMapJob != null) {
1:       DataMapExprWrapper fgDataMapExprWrapper = chooser.chooseFGDataMap(resolver);
1:       if (fgDataMapExprWrapper != null) {
0:         // Prune segments from already pruned blocklets
0:         pruneSegments(segmentIds, prunedBlocklets);
0:         prunedBlocklets =
0:             executeDataMapJob(carbonTable, resolver, segmentIds, fgDataMapExprWrapper, dataMapJob,
0:                 partitionsToPrune);
0:         ExplainCollector.recordFGDataMapPruning(
0:             fgDataMapExprWrapper.getDataMapSchema(), prunedBlocklets.size());
1:       }
0:     } // TODO: add a else branch to push FGDataMap pruning to reader side
commit:9db662a
/////////////////////////////////////////////////////////////////////////
1:           String shardName = CarbonTablePath.getShardName(blocklet.getFilePath());
1:           segment.setFilteredIndexShardName(shardName);
commit:03a735b
/////////////////////////////////////////////////////////////////////////
1: import org.apache.carbondata.core.profiler.ExplainCollector;
/////////////////////////////////////////////////////////////////////////
1:    * @para    DataMapJob dataMapJob = getDataMapJob(job.getConfiguration());
1: m filterExpression
/////////////////////////////////////////////////////////////////////////
1:     ExplainCollector.addPruningInfo(carbonTable.getTableName());
0:     if (resolver != null) {
0:       ExplainCollector.setFilterStatement(resolver.getFilterExpression().getStatement());
1:     } else {
1:       ExplainCollector.setFilterStatement("none");
1:     }
1: 
/////////////////////////////////////////////////////////////////////////
1: 
0:     ExplainCollector.recordDefaultDataMapPruning(
0:         dataMapExprWrapper.getDataMapSchema(), prunedBlocklets.size());
1: 
/////////////////////////////////////////////////////////////////////////
1: 
1:       ExplainCollector.recordCGDataMapPruning(
0:           cgDataMapExprWrapper.getDataMapSchema(), prunedBlocklets.size());
0:     if (dataMapExprWrapper != null && dataMapExprWrapper.getDataMapLevel() == DataMapLevel.FG
1: 
0:       ExplainCollector.recordFGDataMapPruning(
0:           dataMapExprWrapper.getDataMapSchema(), prunedBlocklets.size());
commit:84267dc
/////////////////////////////////////////////////////////////////////////
0:             executeDataMapJob(carbonTable, resolver, segmentIds, cgDataMapExprWrapper, dataMapJob,
0:         prunedBlocklets = cgDataMapExprWrapper.prune(segmentIds, partitionsToPrune);
commit:3ff574d
/////////////////////////////////////////////////////////////////////////
1: import org.apache.carbondata.core.scan.model.QueryModelBuilder;
/////////////////////////////////////////////////////////////////////////
1:   private static final String FGDATAMAP_PRUNING = "mapreduce.input.carboninputformat.fgdatamap";
/////////////////////////////////////////////////////////////////////////
1:   public static void setFgDataMapPruning(Configuration configuration, boolean enable) {
1:     configuration.set(FGDATAMAP_PRUNING, String.valueOf(enable));
1:   }
1: 
1:   public static boolean isFgDataMapPruningEnable(Configuration configuration) {
1:     String enable = configuration.get(FGDATAMAP_PRUNING);
1: 
1:     // if FDDATAMAP_PRUNING is not set, by default we will use FGDataMap
1:     return (enable == null) || enable.equalsIgnoreCase("true");
1:   }
1: 
/////////////////////////////////////////////////////////////////////////
0:     DataMapLevel dataMapLevel = dataMapExprWrapper.getDataMapType();
0:     if (dataMapJob != null &&
0:         (distributedCG ||
0:         (dataMapLevel == DataMapLevel.FG && isFgDataMapPruningEnable(job.getConfiguration())))) {
/////////////////////////////////////////////////////////////////////////
1:     // set projection column in the query model
1:     String[] projectColumns;
1:       projectColumns = projectionString.split(",");
1:     } else {
1:       projectColumns = new String[]{};
1:     QueryModel queryModel = new QueryModelBuilder(carbonTable)
1:         .projectColumns(projectColumns)
1:         .filterExpression(getFilterPredicates(configuration))
1:         .dataConverter(getDataTypeConverter(configuration))
1:         .build();
commit:cfb8ed9
/////////////////////////////////////////////////////////////////////////
1:    * @param converterClass is the Data type converter for different computing engine
1:   public static void setDataTypeConverter(
1:       Configuration configuration, Class<? extends DataTypeConverter> converterClass) {
1:     if (null != converterClass) {
1:       configuration.set(CARBON_CONVERTER, converterClass.getCanonicalName());
1:     String converterClass = configuration.get(CARBON_CONVERTER);
1:     if (converterClass == null) {
1: 
1:     try {
1:       return (DataTypeConverter) Class.forName(converterClass).newInstance();
1:     } catch (Exception e) {
1:       throw new IOException(e);
1:     }
commit:7e0803f
/////////////////////////////////////////////////////////////////////////
0:       CarbonTable carbonTable, FilterResolverIntf resolver,
/////////////////////////////////////////////////////////////////////////
1:         new Path[] { new Path(carbonTable.getTablePath()) }, job.getConfiguration());
/////////////////////////////////////////////////////////////////////////
0:           new DistributableDataMapFormat(carbonTable, dataMapExprWrapper, segmentIds,
commit:df5d7a9
/////////////////////////////////////////////////////////////////////////
/////////////////////////////////////////////////////////////////////////
0:     carbonTable.processFilterExpression(filter, isFilterDimensions, isFilterMeasures);
0:     FilterResolverIntf filterIntf = carbonTable.resolveFilter(filter, tableProvider);
author:dhatchayani
-------------------------------------------------------------------------------
commit:531ecdf
/////////////////////////////////////////////////////////////////////////
1:   public abstract CarbonTable getOrCreateCarbonTable(Configuration configuration)
/////////////////////////////////////////////////////////////////////////
0:   public static void setDataMapJob(Configuration configuration, Object dataMapJob)
/////////////////////////////////////////////////////////////////////////
0:     String className = "org.apache.carbondata.hadoop.api.DistributableDataMapFormat";
0:     FileInputFormat dataMapFormat =
0:         createDataMapJob(carbonTable, dataMapExprWrapper, segmentIds, partitionsToPrune, className);
0:     List<ExtendedBlocklet> prunedBlocklets =
0:         dataMapJob.execute((DistributableDataMapFormat) dataMapFormat, resolver);
1: 
0:   public static FileInputFormat createDataMapJob(CarbonTable carbonTable,
0:       DataMapExprWrapper dataMapExprWrapper, List<Segment> segments,
0:       List<PartitionSpec> partitionsToPrune, String clsName) {
1:     try {
0:       Constructor<?> cons = Class.forName(clsName).getDeclaredConstructors()[0];
0:       return (FileInputFormat) cons
0:           .newInstance(carbonTable, dataMapExprWrapper, segments, partitionsToPrune,
0:               BlockletDataMapFactory.class.getName());
1:     } catch (Exception e) {
0:       throw new RuntimeException(e);
1:     }
1:   }
1: 
author:sounakr
-------------------------------------------------------------------------------
commit:b7b8073
/////////////////////////////////////////////////////////////////////////
1:   private static final String CARBON_TRANSACTIONAL_TABLE =
1:       "mapreduce.input.carboninputformat.transactional";
/////////////////////////////////////////////////////////////////////////
1:   public static void setTransactionalTable(Configuration configuration,
1:       boolean isTransactionalTable) {
1:     configuration.set(CARBON_TRANSACTIONAL_TABLE, String.valueOf(isTransactionalTable));
commit:c58eb43
/////////////////////////////////////////////////////////////////////////
/////////////////////////////////////////////////////////////////////////
1:       CarbonInputFormat.setSegmentsToAccess(conf,
1:           Segment.toSegmentList(segmentNumbersFromProperty.split(","), null));
/////////////////////////////////////////////////////////////////////////
1:           .setSegmentsToAccess(conf, Segment.toSegmentList(segmentList.split(","), null));
/////////////////////////////////////////////////////////////////////////
1:   protected List<CarbonInputSplit> getDataBlocksOfSegment(JobContext job, CarbonTable carbonTable,
0:       FilterResolverIntf resolver, BitSet matchedPartitions, List<Segment> segmentIds,
1:       PartitionInfo partitionInfo, List<Integer> oldPartitionIdList) throws IOException {
/////////////////////////////////////////////////////////////////////////
0:       prunedBlocklets = dataMapExprWrapper.prune(segmentIds, partitionsToPrune);
author:kumarvishal
-------------------------------------------------------------------------------
commit:fb1516c
/////////////////////////////////////////////////////////////////////////
1: import org.apache.carbondata.core.constants.CarbonCommonConstantsInternal;
/////////////////////////////////////////////////////////////////////////
1:    * Set `CARBON_INPUT_SEGMENTS` from property to configuration
1:    */
1:   public static void setQuerySegment(Configuration conf, String segmentList) {
1:     if (!segmentList.trim().equals("*")) {
1:       CarbonInputFormat
0:           .setSegmentsToAccess(conf, Segment.toSegmentList(segmentList.split(",")));
1:     }
1:   }
1: 
1:   /**
/////////////////////////////////////////////////////////////////////////
1: 
1:   public static void setAccessStreamingSegments(Configuration configuration, Boolean validate)
1:       throws InvalidConfigurationException {
1:     configuration.set(
1:         CarbonCommonConstantsInternal.QUERY_ON_PRE_AGG_STREAMING + "." + getDatabaseName(
1:             configuration) + "." + getTableName(configuration), validate.toString());
1:   }
1: 
1:   public static boolean getAccessStreamingSegments(Configuration configuration) {
1:     try {
1:       return configuration.get(
1:           CarbonCommonConstantsInternal.QUERY_ON_PRE_AGG_STREAMING + "." + getDatabaseName(
1:               configuration) + "." + getTableName(
1:                   configuration), "false").equalsIgnoreCase("true");
1: 
1:     } catch (InvalidConfigurationException e) {
1:       return false;
1:     }
1:   }
author:Ajantha-Bhat
-------------------------------------------------------------------------------
commit:c09ef99
/////////////////////////////////////////////////////////////////////////
1: /*
1:  * Licensed to the Apache Software Foundation (ASF) under one or more
1:  * contributor license agreements.  See the NOTICE file distributed with
1:  * this work for additional information regarding copyright ownership.
1:  * The ASF licenses this file to You under the Apache License, Version 2.0
1:  * (the "License"); you may not use this file except in compliance with
1:  * the License.  You may obtain a copy of the License at
1:  *
1:  *    http://www.apache.org/licenses/LICENSE-2.0
1:  *
1:  * Unless required by applicable law or agreed to in writing, software
1:  * distributed under the License is distributed on an "AS IS" BASIS,
1:  * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
1:  * See the License for the specific language governing permissions and
1:  * limitations under the License.
1:  */
1: 
1: package org.apache.carbondata.hadoop.api;
1: 
1: import java.io.ByteArrayInputStream;
1: import java.io.DataInputStream;
1: import java.io.IOException;
1: import java.lang.reflect.Constructor;
0: import java.util.ArrayList;
0: import java.util.BitSet;
0: import java.util.List;
1: 
1: import org.apache.carbondata.core.constants.CarbonCommonConstants;
1: import org.apache.carbondata.core.datamap.DataMapChooser;
0: import org.apache.carbondata.core.datamap.DataMapLevel;
1: import org.apache.carbondata.core.datamap.Segment;
1: import org.apache.carbondata.core.datamap.dev.expr.DataMapExprWrapper;
1: import org.apache.carbondata.core.exception.InvalidConfigurationException;
1: import org.apache.carbondata.core.indexstore.ExtendedBlocklet;
1: import org.apache.carbondata.core.indexstore.PartitionSpec;
0: import org.apache.carbondata.core.indexstore.blockletindex.BlockletDataMapFactory;
1: import org.apache.carbondata.core.metadata.AbsoluteTableIdentifier;
1: import org.apache.carbondata.core.metadata.ColumnarFormatVersion;
1: import org.apache.carbondata.core.metadata.schema.PartitionInfo;
1: import org.apache.carbondata.core.metadata.schema.partition.PartitionType;
1: import org.apache.carbondata.core.metadata.schema.table.CarbonTable;
1: import org.apache.carbondata.core.metadata.schema.table.TableInfo;
1: import org.apache.carbondata.core.mutate.UpdateVO;
1: import org.apache.carbondata.core.scan.expression.Expression;
0: import org.apache.carbondata.core.scan.filter.SingleTableProvider;
0: import org.apache.carbondata.core.scan.filter.TableProvider;
1: import org.apache.carbondata.core.scan.filter.resolver.FilterResolverIntf;
1: import org.apache.carbondata.core.scan.model.QueryModel;
1: import org.apache.carbondata.core.stats.QueryStatistic;
1: import org.apache.carbondata.core.stats.QueryStatisticsConstants;
1: import org.apache.carbondata.core.stats.QueryStatisticsRecorder;
1: import org.apache.carbondata.core.util.CarbonProperties;
1: import org.apache.carbondata.core.util.CarbonTimeStatisticsFactory;
1: import org.apache.carbondata.core.util.CarbonUtil;
1: import org.apache.carbondata.core.util.DataTypeConverter;
1: import org.apache.carbondata.core.util.DataTypeConverterImpl;
1: import org.apache.carbondata.core.util.path.CarbonTablePath;
1: import org.apache.carbondata.hadoop.CarbonInputSplit;
1: import org.apache.carbondata.hadoop.CarbonMultiBlockSplit;
1: import org.apache.carbondata.hadoop.CarbonProjection;
1: import org.apache.carbondata.hadoop.CarbonRecordReader;
1: import org.apache.carbondata.hadoop.readsupport.CarbonReadSupport;
1: import org.apache.carbondata.hadoop.readsupport.impl.DictionaryDecodeReadSupport;
0: import org.apache.carbondata.hadoop.util.CarbonInputFormatUtil;
0: import org.apache.carbondata.hadoop.util.ObjectSerializationUtil;
1: 
1: import org.apache.commons.logging.Log;
1: import org.apache.commons.logging.LogFactory;
1: import org.apache.hadoop.conf.Configuration;
1: import org.apache.hadoop.fs.FileSystem;
1: import org.apache.hadoop.fs.LocalFileSystem;
1: import org.apache.hadoop.fs.Path;
1: import org.apache.hadoop.mapreduce.InputSplit;
1: import org.apache.hadoop.mapreduce.JobContext;
1: import org.apache.hadoop.mapreduce.RecordReader;
1: import org.apache.hadoop.mapreduce.TaskAttemptContext;
1: import org.apache.hadoop.mapreduce.lib.input.FileInputFormat;
1: import org.apache.hadoop.mapreduce.lib.input.FileSplit;
1: import org.apache.hadoop.mapreduce.security.TokenCache;
1: 
1: /**
1:  * Base class for carbondata input format, there are two input format implementations:
1:  * 1. CarbonFileInputFormat: for reading carbondata files without table level metadata support.
1:  *
1:  * 2. CarbonTableInputFormat: for reading carbondata files with table level metadata support,
1:  * such as segment and explicit schema metadata.
1:  *
1:  * @param <T>
1:  */
1: public abstract class CarbonInputFormat<T> extends FileInputFormat<Void, T> {
1:   // comma separated list of input segment numbers
1:   public static final String INPUT_SEGMENT_NUMBERS =
1:       "mapreduce.input.carboninputformat.segmentnumbers";
1:   private static final String VALIDATE_INPUT_SEGMENT_IDs =
1:       "mapreduce.input.carboninputformat.validsegments";
1:   // comma separated list of input files
1:   private static final String ALTER_PARTITION_ID = "mapreduce.input.carboninputformat.partitionid";
1:   private static final Log LOG = LogFactory.getLog(CarbonInputFormat.class);
1:   private static final String FILTER_PREDICATE =
1:       "mapreduce.input.carboninputformat.filter.predicate";
1:   private static final String COLUMN_PROJECTION = "mapreduce.input.carboninputformat.projection";
1:   private static final String TABLE_INFO = "mapreduce.input.carboninputformat.tableinfo";
1:   private static final String CARBON_READ_SUPPORT = "mapreduce.input.carboninputformat.readsupport";
1:   private static final String CARBON_CONVERTER = "mapreduce.input.carboninputformat.converter";
0:   private static final String DATA_MAP_DSTR = "mapreduce.input.carboninputformat.datamapdstr";
1:   public static final String DATABASE_NAME = "mapreduce.input.carboninputformat.databaseName";
1:   public static final String TABLE_NAME = "mapreduce.input.carboninputformat.tableName";
1:   private static final String PARTITIONS_TO_PRUNE =
1:       "mapreduce.input.carboninputformat.partitions.to.prune";
1: 
1:   /**
1:    * Set the `tableInfo` in `configuration`
1:    */
1:   public static void setTableInfo(Configuration configuration, TableInfo tableInfo)
1:       throws IOException {
1:     if (null != tableInfo) {
1:       configuration.set(TABLE_INFO, CarbonUtil.encodeToString(tableInfo.serialize()));
1:     }
1:   }
1: 
1:   /**
1:    * Get TableInfo object from `configuration`
1:    */
1:   protected static TableInfo getTableInfo(Configuration configuration) throws IOException {
1:     String tableInfoStr = configuration.get(TABLE_INFO);
1:     if (tableInfoStr == null) {
1:       return null;
1:     } else {
1:       TableInfo output = new TableInfo();
1:       output.readFields(new DataInputStream(
1:           new ByteArrayInputStream(CarbonUtil.decodeStringToBytes(tableInfoStr))));
1:       return output;
1:     }
1:   }
1: 
1:   /**
1:    * Get the cached CarbonTable or create it by TableInfo in `configuration`
1:    */
0:   protected abstract CarbonTable getOrCreateCarbonTable(Configuration configuration)
1:       throws IOException;
1: 
1:   public static void setTablePath(Configuration configuration, String tablePath) {
1:     configuration.set(FileInputFormat.INPUT_DIR, tablePath);
1:   }
1: 
1:   public static void setPartitionIdList(Configuration configuration, List<String> partitionIds) {
1:     configuration.set(ALTER_PARTITION_ID, partitionIds.toString());
1:   }
1: 
0:   public static void setDataMapJob(Configuration configuration, DataMapJob dataMapJob)
1:       throws IOException {
0:     if (dataMapJob != null) {
0:       String toString = ObjectSerializationUtil.convertObjectToString(dataMapJob);
0:       configuration.set(DATA_MAP_DSTR, toString);
1:     }
1:   }
1: 
0:   public static DataMapJob getDataMapJob(Configuration configuration) throws IOException {
0:     String jobString = configuration.get(DATA_MAP_DSTR);
0:     if (jobString != null) {
0:       return (DataMapJob) ObjectSerializationUtil.convertStringToObject(jobString);
1:     }
1:     return null;
1:   }
1: 
1:   /**
1:    * It sets unresolved filter expression.
1:    *
1:    * @param configuration
0:    * @param filterExpression
1:    */
1:   public static void setFilterPredicates(Configuration configuration, Expression filterExpression) {
1:     if (filterExpression == null) {
1:       return;
1:     }
1:     try {
1:       String filterString = ObjectSerializationUtil.convertObjectToString(filterExpression);
1:       configuration.set(FILTER_PREDICATE, filterString);
1:     } catch (Exception e) {
1:       throw new RuntimeException("Error while setting filter expression to Job", e);
1:     }
1:   }
1: 
1:   public static void setColumnProjection(Configuration configuration, CarbonProjection projection) {
1:     if (projection == null || projection.isEmpty()) {
1:       return;
1:     }
1:     String[] allColumns = projection.getAllColumns();
1:     StringBuilder builder = new StringBuilder();
1:     for (String column : allColumns) {
1:       builder.append(column).append(",");
1:     }
1:     String columnString = builder.toString();
1:     columnString = columnString.substring(0, columnString.length() - 1);
1:     configuration.set(COLUMN_PROJECTION, columnString);
1:   }
1: 
1:   public static String getColumnProjection(Configuration configuration) {
1:     return configuration.get(COLUMN_PROJECTION);
1:   }
1: 
1:   /**
1:    * Set list of segments to access
1:    */
1:   public static void setSegmentsToAccess(Configuration configuration, List<Segment> validSegments) {
1:     configuration.set(INPUT_SEGMENT_NUMBERS, CarbonUtil.convertToString(validSegments));
1:   }
1: 
1:   /**
1:    * Set `CARBON_INPUT_SEGMENTS` from property to configuration
1:    */
1:   public static void setQuerySegment(Configuration conf, AbsoluteTableIdentifier identifier) {
1:     String dbName = identifier.getCarbonTableIdentifier().getDatabaseName().toLowerCase();
1:     String tbName = identifier.getCarbonTableIdentifier().getTableName().toLowerCase();
1:     String segmentNumbersFromProperty = CarbonProperties.getInstance()
1:         .getProperty(CarbonCommonConstants.CARBON_INPUT_SEGMENTS + dbName + "." + tbName, "*");
1:     if (!segmentNumbersFromProperty.trim().equals("*")) {
0:       CarbonInputFormat
0:           .setSegmentsToAccess(conf, Segment.toSegmentList(segmentNumbersFromProperty.split(",")));
1:     }
1:   }
1: 
1:   /**
1:    * set list of segment to access
1:    */
1:   public static void setValidateSegmentsToAccess(Configuration configuration, Boolean validate) {
1:     configuration.set(CarbonInputFormat.VALIDATE_INPUT_SEGMENT_IDs, validate.toString());
1:   }
1: 
1:   /**
1:    * get list of segment to access
1:    */
1:   public static boolean getValidateSegmentsToAccess(Configuration configuration) {
1:     return configuration.get(CarbonInputFormat.VALIDATE_INPUT_SEGMENT_IDs, "true")
1:         .equalsIgnoreCase("true");
1:   }
1: 
1:   /**
1:    * set list of partitions to prune
1:    */
1:   public static void setPartitionsToPrune(Configuration configuration,
1:       List<PartitionSpec> partitions) {
1:     if (partitions == null) {
1:       return;
1:     }
1:     try {
1:       String partitionString =
1:           ObjectSerializationUtil.convertObjectToString(new ArrayList<>(partitions));
1:       configuration.set(PARTITIONS_TO_PRUNE, partitionString);
1:     } catch (Exception e) {
1:       throw new RuntimeException("Error while setting patition information to Job" + partitions, e);
1:     }
1:   }
1: 
1:   /**
1:    * get list of partitions to prune
1:    */
1:   public static List<PartitionSpec> getPartitionsToPrune(Configuration configuration)
1:       throws IOException {
1:     String partitionString = configuration.get(PARTITIONS_TO_PRUNE);
1:     if (partitionString != null) {
1:       return (List<PartitionSpec>) ObjectSerializationUtil.convertStringToObject(partitionString);
1:     }
1:     return null;
1:   }
1: 
1:   public AbsoluteTableIdentifier getAbsoluteTableIdentifier(Configuration configuration)
1:       throws IOException {
1:     String tablePath = configuration.get(INPUT_DIR, "");
1:     try {
1:       return AbsoluteTableIdentifier
1:           .from(tablePath, getDatabaseName(configuration), getTableName(configuration));
1:     } catch (InvalidConfigurationException e) {
1:       throw new IOException(e);
1:     }
1:   }
1: 
1:   /**
1:    * {@inheritDoc}
1:    * Configurations FileInputFormat.INPUT_DIR
1:    * are used to get table path to read.
1:    *
1:    * @param job
1:    * @return List<InputSplit> list of CarbonInputSplit
1:    * @throws IOException
1:    */
1:   @Override public abstract List<InputSplit> getSplits(JobContext job) throws IOException;
1: 
1:   protected Expression getFilterPredicates(Configuration configuration) {
1:     try {
1:       String filterExprString = configuration.get(FILTER_PREDICATE);
1:       if (filterExprString == null) {
1:         return null;
1:       }
1:       Object filter = ObjectSerializationUtil.convertStringToObject(filterExprString);
1:       return (Expression) filter;
1:     } catch (IOException e) {
1:       throw new RuntimeException("Error while reading filter expression", e);
1:     }
1:   }
1: 
1:   /**
1:    * get data blocks of given segment
1:    */
0:   protected List<CarbonInputSplit> getDataBlocksOfSegment(JobContext job,
0:       AbsoluteTableIdentifier absoluteTableIdentifier, FilterResolverIntf resolver,
0:       BitSet matchedPartitions, List<Segment> segmentIds, PartitionInfo partitionInfo,
0:       List<Integer> oldPartitionIdList) throws IOException {
1: 
1:     QueryStatisticsRecorder recorder = CarbonTimeStatisticsFactory.createDriverRecorder();
1:     QueryStatistic statistic = new QueryStatistic();
1: 
1:     // get tokens for all the required FileSystem for table path
1:     TokenCache.obtainTokensForNamenodes(job.getCredentials(),
0:         new Path[] { new Path(absoluteTableIdentifier.getTablePath()) }, job.getConfiguration());
0:     boolean distributedCG = Boolean.parseBoolean(CarbonProperties.getInstance()
0:         .getProperty(CarbonCommonConstants.USE_DISTRIBUTED_DATAMAP,
0:             CarbonCommonConstants.USE_DISTRIBUTED_DATAMAP_DEFAULT));
0:     DataMapExprWrapper dataMapExprWrapper =
0:         DataMapChooser.get().choose(getOrCreateCarbonTable(job.getConfiguration()), resolver);
0:     DataMapJob dataMapJob = getDataMapJob(job.getConfiguration());
0:     List<PartitionSpec> partitionsToPrune = getPartitionsToPrune(job.getConfiguration());
0:     List<ExtendedBlocklet> prunedBlocklets;
0:     if (distributedCG || dataMapExprWrapper.getDataMapType() == DataMapLevel.FG) {
0:       DistributableDataMapFormat datamapDstr =
0:           new DistributableDataMapFormat(absoluteTableIdentifier, dataMapExprWrapper, segmentIds,
0:               partitionsToPrune, BlockletDataMapFactory.class.getName());
0:       prunedBlocklets = dataMapJob.execute(datamapDstr, resolver);
0:       // Apply expression on the blocklets.
0:       prunedBlocklets = dataMapExprWrapper.pruneBlocklets(prunedBlocklets);
1:     } else {
0:       prunedBlocklets = dataMapExprWrapper.prune(segmentIds, partitionsToPrune);
1:     }
1: 
0:     List<CarbonInputSplit> resultFilterredBlocks = new ArrayList<>();
1:     int partitionIndex = 0;
1:     List<Integer> partitionIdList = new ArrayList<>();
1:     if (partitionInfo != null && partitionInfo.getPartitionType() != PartitionType.NATIVE_HIVE) {
1:       partitionIdList = partitionInfo.getPartitionIds();
1:     }
1:     for (ExtendedBlocklet blocklet : prunedBlocklets) {
1:       long partitionId = CarbonTablePath.DataFileUtil
1:           .getTaskIdFromTaskNo(CarbonTablePath.DataFileUtil.getTaskNo(blocklet.getPath()));
1: 
1:       // OldPartitionIdList is only used in alter table partition command because it change
1:       // partition info first and then read data.
1:       // For other normal query should use newest partitionIdList
1:       if (partitionInfo != null && partitionInfo.getPartitionType() != PartitionType.NATIVE_HIVE) {
1:         if (oldPartitionIdList != null) {
1:           partitionIndex = oldPartitionIdList.indexOf((int) partitionId);
1:         } else {
1:           partitionIndex = partitionIdList.indexOf((int) partitionId);
1:         }
1:       }
1:       if (partitionIndex != -1) {
1:         // matchedPartitions variable will be null in two cases as follows
1:         // 1. the table is not a partition table
1:         // 2. the table is a partition table, and all partitions are matched by query
1:         // for partition table, the task id of carbaondata file name is the partition id.
1:         // if this partition is not required, here will skip it.
1:         if (matchedPartitions == null || matchedPartitions.get(partitionIndex)) {
1:           CarbonInputSplit inputSplit = convertToCarbonInputSplit(blocklet);
1:           if (inputSplit != null) {
0:             resultFilterredBlocks.add(inputSplit);
1:           }
1:         }
1:       }
1:     }
1:     statistic
1:         .addStatistics(QueryStatisticsConstants.LOAD_BLOCKS_DRIVER, System.currentTimeMillis());
1:     recorder.recordStatisticsForDriver(statistic, job.getConfiguration().get("query.id"));
0:     return resultFilterredBlocks;
1:   }
1: 
1:   private CarbonInputSplit convertToCarbonInputSplit(ExtendedBlocklet blocklet) throws IOException {
1:     CarbonInputSplit split = CarbonInputSplit
1:         .from(blocklet.getSegmentId(), blocklet.getBlockletId(),
1:             new FileSplit(new Path(blocklet.getPath()), 0, blocklet.getLength(),
1:                 blocklet.getLocations()),
1:             ColumnarFormatVersion.valueOf((short) blocklet.getDetailInfo().getVersionNumber()),
1:             blocklet.getDataMapWriterPath());
1:     split.setDetailInfo(blocklet.getDetailInfo());
1:     return split;
1:   }
1: 
1:   @Override public RecordReader<Void, T> createRecordReader(InputSplit inputSplit,
1:       TaskAttemptContext taskAttemptContext) throws IOException, InterruptedException {
1:     Configuration configuration = taskAttemptContext.getConfiguration();
1:     QueryModel queryModel = createQueryModel(inputSplit, taskAttemptContext);
1:     CarbonReadSupport<T> readSupport = getReadSupportClass(configuration);
0:     return new CarbonRecordReader<T>(queryModel, readSupport);
1:   }
1: 
1:   public QueryModel createQueryModel(InputSplit inputSplit, TaskAttemptContext taskAttemptContext)
1:       throws IOException {
1:     Configuration configuration = taskAttemptContext.getConfiguration();
1:     CarbonTable carbonTable = getOrCreateCarbonTable(configuration);
0:     TableProvider tableProvider = new SingleTableProvider(carbonTable);
1: 
0:     // query plan includes projection column
1:     String projectionString = getColumnProjection(configuration);
0:     String[] projectionColumnNames = null;
1:     if (projectionString != null) {
0:       projectionColumnNames = projectionString.split(",");
1:     }
0:     QueryModel queryModel = carbonTable
0:         .createQueryWithProjection(projectionColumnNames, getDataTypeConverter(configuration));
1: 
0:     // set the filter to the query model in order to filter blocklet before scan
0:     Expression filter = getFilterPredicates(configuration);
0:     boolean[] isFilterDimensions = new boolean[carbonTable.getDimensionOrdinalMax()];
0:     // getAllMeasures returns list of visible and invisible columns
0:     boolean[] isFilterMeasures = new boolean[carbonTable.getAllMeasures().size()];
0:     CarbonInputFormatUtil
0:         .processFilterExpression(filter, carbonTable, isFilterDimensions, isFilterMeasures);
0:     queryModel.setIsFilterDimensions(isFilterDimensions);
0:     queryModel.setIsFilterMeasures(isFilterMeasures);
0:     FilterResolverIntf filterIntf = CarbonInputFormatUtil
0:         .resolveFilter(filter, carbonTable.getAbsoluteTableIdentifier(), tableProvider);
0:     queryModel.setFilterExpressionResolverTree(filterIntf);
1: 
1:     // update the file level index store if there are invalid segment
1:     if (inputSplit instanceof CarbonMultiBlockSplit) {
1:       CarbonMultiBlockSplit split = (CarbonMultiBlockSplit) inputSplit;
1:       List<String> invalidSegments = split.getAllSplits().get(0).getInvalidSegments();
1:       if (invalidSegments.size() > 0) {
1:         queryModel.setInvalidSegmentIds(invalidSegments);
1:       }
1:       List<UpdateVO> invalidTimestampRangeList =
1:           split.getAllSplits().get(0).getInvalidTimestampRange();
1:       if ((null != invalidTimestampRangeList) && (invalidTimestampRangeList.size() > 0)) {
1:         queryModel.setInvalidBlockForSegmentId(invalidTimestampRangeList);
1:       }
1:     }
1:     return queryModel;
1:   }
1: 
1:   public CarbonReadSupport<T> getReadSupportClass(Configuration configuration) {
1:     String readSupportClass = configuration.get(CARBON_READ_SUPPORT);
1:     //By default it uses dictionary decoder read class
1:     CarbonReadSupport<T> readSupport = null;
1:     if (readSupportClass != null) {
1:       try {
1:         Class<?> myClass = Class.forName(readSupportClass);
1:         Constructor<?> constructor = myClass.getConstructors()[0];
1:         Object object = constructor.newInstance();
1:         if (object instanceof CarbonReadSupport) {
1:           readSupport = (CarbonReadSupport) object;
1:         }
1:       } catch (ClassNotFoundException ex) {
1:         LOG.error("Class " + readSupportClass + "not found", ex);
1:       } catch (Exception ex) {
1:         LOG.error("Error while creating " + readSupportClass, ex);
1:       }
1:     } else {
1:       readSupport = new DictionaryDecodeReadSupport<>();
1:     }
1:     return readSupport;
1:   }
1: 
1:   @Override protected boolean isSplitable(JobContext context, Path filename) {
1:     try {
1:       // Don't split the file if it is local file system
1:       FileSystem fileSystem = filename.getFileSystem(context.getConfiguration());
1:       if (fileSystem instanceof LocalFileSystem) {
1:         return false;
1:       }
1:     } catch (Exception e) {
1:       return true;
1:     }
1:     return true;
1:   }
1: 
1:   public static void setCarbonReadSupport(Configuration configuration,
1:       Class<? extends CarbonReadSupport> readSupportClass) {
1:     if (readSupportClass != null) {
1:       configuration.set(CARBON_READ_SUPPORT, readSupportClass.getName());
1:     }
1:   }
1: 
1:   /**
1:    * It is optional, if user does not set then it reads from store
1:    *
1:    * @param configuration
0:    * @param converter is the Data type converter for different computing engine
1:    * @throws IOException
1:    */
0:   public static void setDataTypeConverter(Configuration configuration, DataTypeConverter converter)
1:       throws IOException {
0:     if (null != converter) {
0:       configuration.set(CARBON_CONVERTER,
0:           ObjectSerializationUtil.convertObjectToString(converter));
1:     }
1:   }
1: 
1:   public static DataTypeConverter getDataTypeConverter(Configuration configuration)
1:       throws IOException {
0:     String converter = configuration.get(CARBON_CONVERTER);
0:     if (converter == null) {
1:       return new DataTypeConverterImpl();
1:     }
0:     return (DataTypeConverter) ObjectSerializationUtil.convertStringToObject(converter);
1:   }
1: 
1:   public static void setDatabaseName(Configuration configuration, String databaseName) {
1:     if (null != databaseName) {
1:       configuration.set(DATABASE_NAME, databaseName);
1:     }
1:   }
1: 
1:   public static String getDatabaseName(Configuration configuration)
1:       throws InvalidConfigurationException {
1:     String databseName = configuration.get(DATABASE_NAME);
1:     if (null == databseName) {
1:       throw new InvalidConfigurationException("Database name is not set.");
1:     }
1:     return databseName;
1:   }
1: 
1:   public static void setTableName(Configuration configuration, String tableName) {
1:     if (null != tableName) {
1:       configuration.set(TABLE_NAME, tableName);
1:     }
1:   }
1: 
1:   public static String getTableName(Configuration configuration)
1:       throws InvalidConfigurationException {
1:     String tableName = configuration.get(TABLE_NAME);
1:     if (tableName == null) {
1:       throw new InvalidConfigurationException("Table name is not set");
1:     }
1:     return tableName;
1:   }
1: }
============================================================================