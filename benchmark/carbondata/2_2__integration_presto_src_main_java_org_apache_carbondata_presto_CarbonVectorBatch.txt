1:e5e74fc: /*
1:e5e74fc:  * Licensed to the Apache Software Foundation (ASF) under one or more
1:e5e74fc:  * contributor license agreements.  See the NOTICE file distributed with
1:e5e74fc:  * this work for additional information regarding copyright ownership.
1:e5e74fc:  * The ASF licenses this file to You under the Apache License, Version 2.0
1:e5e74fc:  * (the "License"); you may not use this file except in compliance with
1:e5e74fc:  * the License.  You may obtain a copy of the License at
1:e5e74fc:  *
1:e5e74fc:  *    http://www.apache.org/licenses/LICENSE-2.0
1:e5e74fc:  *
1:e5e74fc:  * Unless required by applicable law or agreed to in writing, software
1:e5e74fc:  * distributed under the License is distributed on an "AS IS" BASIS,
1:e5e74fc:  * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
1:e5e74fc:  * See the License for the specific language governing permissions and
1:e5e74fc:  * limitations under the License.
1:e5e74fc:  */
1:e5e74fc: package org.apache.carbondata.presto;
11:e5e74fc: 
1:e5e74fc: import java.util.Arrays;
1:e5e74fc: import java.util.HashSet;
1:e5e74fc: import java.util.Set;
1:e5e74fc: 
1:a4c2ef5: import org.apache.carbondata.core.cache.dictionary.Dictionary;
1:a4c2ef5: import org.apache.carbondata.core.metadata.datatype.DataType;
1:a4c2ef5: import org.apache.carbondata.core.metadata.datatype.DataTypes;
1:a4c2ef5: import org.apache.carbondata.core.metadata.datatype.DecimalType;
1:e5e74fc: import org.apache.carbondata.core.metadata.datatype.StructField;
1:e5e74fc: import org.apache.carbondata.core.scan.result.vector.impl.CarbonColumnVectorImpl;
1:a4c2ef5: import org.apache.carbondata.presto.readers.BooleanStreamReader;
1:a4c2ef5: import org.apache.carbondata.presto.readers.DecimalSliceStreamReader;
1:a4c2ef5: import org.apache.carbondata.presto.readers.DoubleStreamReader;
1:a4c2ef5: import org.apache.carbondata.presto.readers.IntegerStreamReader;
1:a4c2ef5: import org.apache.carbondata.presto.readers.LongStreamReader;
1:a4c2ef5: import org.apache.carbondata.presto.readers.ObjectStreamReader;
1:a4c2ef5: import org.apache.carbondata.presto.readers.ShortStreamReader;
1:a4c2ef5: import org.apache.carbondata.presto.readers.SliceStreamReader;
1:a4c2ef5: import org.apache.carbondata.presto.readers.TimestampStreamReader;
1:a4c2ef5: 
1:a4c2ef5: import com.facebook.presto.spi.block.SliceArrayBlock;
1:e5e74fc: 
1:e5e74fc: public class CarbonVectorBatch {
1:e5e74fc: 
1:a4c2ef5:   private static final int DEFAULT_BATCH_SIZE = 4 * 1024;
1:e5e74fc: 
1:e5e74fc:   private final int capacity;
1:e5e74fc:   private final CarbonColumnVectorImpl[] columns;
1:e5e74fc:   // True if the row is filtered.
1:e5e74fc:   private final boolean[] filteredRows;
1:e5e74fc:   // Column indices that cannot have null values.
1:e5e74fc:   private final Set<Integer> nullFilteredColumns;
1:a4c2ef5:   private int numRows;
1:e5e74fc:   // Total number of rows that have been filtered.
1:e5e74fc:   private int numRowsFiltered = 0;
1:e5e74fc: 
1:a4c2ef5:   private CarbonVectorBatch(StructField[] schema, CarbonDictionaryDecodeReadSupport readSupport,
1:a4c2ef5:       int maxRows) {
1:e5e74fc:     this.capacity = maxRows;
1:e5e74fc:     this.columns = new CarbonColumnVectorImpl[schema.length];
1:e5e74fc:     this.nullFilteredColumns = new HashSet<>();
1:e5e74fc:     this.filteredRows = new boolean[maxRows];
1:a4c2ef5:     Dictionary[] dictionaries = readSupport.getDictionaries();
1:a4c2ef5:     DataType[] dataTypes = readSupport.getDataTypes();
1:e5e74fc: 
1:e5e74fc:     for (int i = 0; i < schema.length; ++i) {
1:a4c2ef5:       columns[i] = createDirectStreamReader(maxRows, dataTypes[i], schema[i], dictionaries[i],
1:a4c2ef5:           readSupport.getSliceArrayBlock(i));
1:e5e74fc:     }
1:e5e74fc:   }
1:e5e74fc: 
1:a4c2ef5:   public static CarbonVectorBatch allocate(StructField[] schema,
1:a4c2ef5:       CarbonDictionaryDecodeReadSupport readSupport) {
1:a4c2ef5:     return new CarbonVectorBatch(schema, readSupport, DEFAULT_BATCH_SIZE);
1:e5e74fc:   }
1:e5e74fc: 
1:a4c2ef5:   private CarbonColumnVectorImpl createDirectStreamReader(int batchSize, DataType dataType,
1:a4c2ef5:       StructField field, Dictionary dictionary, SliceArrayBlock dictionarySliceArrayBlock) {
1:a4c2ef5:     if (dataType == DataTypes.BOOLEAN) {
1:a4c2ef5:       return new BooleanStreamReader(batchSize, field.getDataType(), dictionary);
1:a4c2ef5:     } else if (dataType == DataTypes.SHORT) {
1:a4c2ef5:       return new ShortStreamReader(batchSize, field.getDataType(), dictionary);
1:a4c2ef5:     } else if (dataType == DataTypes.INT || dataType == DataTypes.DATE) {
1:a4c2ef5:       return new IntegerStreamReader(batchSize, field.getDataType(), dictionary);
1:a4c2ef5:     } else if (dataType == DataTypes.TIMESTAMP) {
1:a4c2ef5:       return new TimestampStreamReader(batchSize, field.getDataType(), dictionary);
1:a4c2ef5:     } else if (dataType == DataTypes.LONG) {
1:a4c2ef5:       return new LongStreamReader(batchSize, field.getDataType(), dictionary);
1:a4c2ef5:     } else if (dataType == DataTypes.DOUBLE) {
1:a4c2ef5:       return new DoubleStreamReader(batchSize, field.getDataType(), dictionary);
1:a4c2ef5:     } else if (dataType == DataTypes.STRING) {
1:a4c2ef5:       return new SliceStreamReader(batchSize, field.getDataType(), dictionarySliceArrayBlock);
1:a4c2ef5:     } else if (DataTypes.isDecimal(dataType)) {
1:a4c2ef5:       return new DecimalSliceStreamReader(batchSize, (DecimalType) field.getDataType(), dictionary);
1:a4c2ef5:     } else {
1:a4c2ef5:       return new ObjectStreamReader(batchSize, field.getDataType());
1:a4c2ef5:     }
1:e5e74fc:   }
1:a4c2ef5: 
1:e5e74fc:   /**
1:e5e74fc:    * Resets the batch for writing.
1:e5e74fc:    */
1:e5e74fc:   public void reset() {
1:e5e74fc:     for (int i = 0; i < numCols(); ++i) {
1:e5e74fc:       columns[i].reset();
1:e5e74fc:     }
1:e5e74fc:     if (this.numRowsFiltered > 0) {
1:e5e74fc:       Arrays.fill(filteredRows, false);
1:e5e74fc:     }
1:e5e74fc:     this.numRows = 0;
1:e5e74fc:     this.numRowsFiltered = 0;
1:e5e74fc:   }
1:e5e74fc: 
1:e5e74fc:   /**
1:e5e74fc:    * Returns the number of columns that make up this batch.
1:e5e74fc:    */
1:a4c2ef5:   public int numCols() {
1:a4c2ef5:     return columns.length;
1:a4c2ef5:   }
1:e5e74fc: 
1:e5e74fc:   /**
1:e5e74fc:    * Sets the number of rows that are valid. Additionally, marks all rows as "filtered" if one or
1:e5e74fc:    * more of their attributes are part of a non-nullable column.
1:e5e74fc:    */
1:e5e74fc:   public void setNumRows(int numRows) {
1:a4c2ef5:     assert (numRows <= this.capacity);
1:e5e74fc:     this.numRows = numRows;
1:e5e74fc: 
1:e5e74fc:     for (int ordinal : nullFilteredColumns) {
1:e5e74fc:       for (int rowId = 0; rowId < numRows; rowId++) {
1:e5e74fc:         if (!filteredRows[rowId] && columns[ordinal].isNull(rowId)) {
1:e5e74fc:           filteredRows[rowId] = true;
1:e5e74fc:           ++numRowsFiltered;
1:e5e74fc:         }
1:e5e74fc:       }
1:e5e74fc:     }
1:e5e74fc:   }
1:e5e74fc: 
1:e5e74fc:   /**
1:e5e74fc:    * Returns the number of rows for read, including filtered rows.
1:e5e74fc:    */
1:a4c2ef5:   public int numRows() {
1:a4c2ef5:     return numRows;
1:a4c2ef5:   }
1:e5e74fc: 
1:e5e74fc:   /**
1:e5e74fc:    * Returns the number of valid rows.
1:e5e74fc:    */
1:e5e74fc:   public int numValidRows() {
1:a4c2ef5:     assert (numRowsFiltered <= numRows);
1:e5e74fc:     return numRows - numRowsFiltered;
1:e5e74fc:   }
1:e5e74fc: 
1:e5e74fc:   /**
1:e5e74fc:    * Returns the column at `ordinal`.
1:e5e74fc:    */
1:a4c2ef5:   public CarbonColumnVectorImpl column(int ordinal) {
1:a4c2ef5:     return columns[ordinal];
1:a4c2ef5:   }
1:e5e74fc: 
1:e5e74fc:   /**
1:e5e74fc:    * Returns the max capacity (in number of rows) for this batch.
1:e5e74fc:    */
1:a4c2ef5:   public int capacity() {
1:a4c2ef5:     return capacity;
1:a4c2ef5:   }
1:e5e74fc: 
1:e5e74fc: }
============================================================================
author:sv71294
-------------------------------------------------------------------------------
commit:a4c2ef5
/////////////////////////////////////////////////////////////////////////
1: import org.apache.carbondata.core.cache.dictionary.Dictionary;
1: import org.apache.carbondata.core.metadata.datatype.DataType;
1: import org.apache.carbondata.core.metadata.datatype.DataTypes;
1: import org.apache.carbondata.core.metadata.datatype.DecimalType;
1: import org.apache.carbondata.presto.readers.BooleanStreamReader;
1: import org.apache.carbondata.presto.readers.DecimalSliceStreamReader;
1: import org.apache.carbondata.presto.readers.DoubleStreamReader;
1: import org.apache.carbondata.presto.readers.IntegerStreamReader;
1: import org.apache.carbondata.presto.readers.LongStreamReader;
1: import org.apache.carbondata.presto.readers.ObjectStreamReader;
1: import org.apache.carbondata.presto.readers.ShortStreamReader;
1: import org.apache.carbondata.presto.readers.SliceStreamReader;
1: import org.apache.carbondata.presto.readers.TimestampStreamReader;
1: 
1: import com.facebook.presto.spi.block.SliceArrayBlock;
1:   private static final int DEFAULT_BATCH_SIZE = 4 * 1024;
1:   private int numRows;
1:   private CarbonVectorBatch(StructField[] schema, CarbonDictionaryDecodeReadSupport readSupport,
1:       int maxRows) {
1:     Dictionary[] dictionaries = readSupport.getDictionaries();
1:     DataType[] dataTypes = readSupport.getDataTypes();
1:       columns[i] = createDirectStreamReader(maxRows, dataTypes[i], schema[i], dictionaries[i],
1:           readSupport.getSliceArrayBlock(i));
1:   public static CarbonVectorBatch allocate(StructField[] schema,
1:       CarbonDictionaryDecodeReadSupport readSupport) {
1:     return new CarbonVectorBatch(schema, readSupport, DEFAULT_BATCH_SIZE);
1:   private CarbonColumnVectorImpl createDirectStreamReader(int batchSize, DataType dataType,
1:       StructField field, Dictionary dictionary, SliceArrayBlock dictionarySliceArrayBlock) {
1:     if (dataType == DataTypes.BOOLEAN) {
1:       return new BooleanStreamReader(batchSize, field.getDataType(), dictionary);
1:     } else if (dataType == DataTypes.SHORT) {
1:       return new ShortStreamReader(batchSize, field.getDataType(), dictionary);
1:     } else if (dataType == DataTypes.INT || dataType == DataTypes.DATE) {
1:       return new IntegerStreamReader(batchSize, field.getDataType(), dictionary);
1:     } else if (dataType == DataTypes.TIMESTAMP) {
1:       return new TimestampStreamReader(batchSize, field.getDataType(), dictionary);
1:     } else if (dataType == DataTypes.LONG) {
1:       return new LongStreamReader(batchSize, field.getDataType(), dictionary);
1:     } else if (dataType == DataTypes.DOUBLE) {
1:       return new DoubleStreamReader(batchSize, field.getDataType(), dictionary);
1:     } else if (dataType == DataTypes.STRING) {
1:       return new SliceStreamReader(batchSize, field.getDataType(), dictionarySliceArrayBlock);
1:     } else if (DataTypes.isDecimal(dataType)) {
1:       return new DecimalSliceStreamReader(batchSize, (DecimalType) field.getDataType(), dictionary);
1:     } else {
1:       return new ObjectStreamReader(batchSize, field.getDataType());
1:     }
1: 
/////////////////////////////////////////////////////////////////////////
1:   public int numCols() {
1:     return columns.length;
1:   }
1:     assert (numRows <= this.capacity);
/////////////////////////////////////////////////////////////////////////
1:   public int numRows() {
1:     return numRows;
1:   }
1:     assert (numRowsFiltered <= numRows);
1:   public CarbonColumnVectorImpl column(int ordinal) {
1:     return columns[ordinal];
1:   }
1:   public int capacity() {
1:     return capacity;
1:   }
author:Bhavya
-------------------------------------------------------------------------------
commit:01b48fc
/////////////////////////////////////////////////////////////////////////
0:   private static final int DEFAULT_BATCH_SIZE =  4 * 1024;
commit:e5e74fc
/////////////////////////////////////////////////////////////////////////
1: /*
1:  * Licensed to the Apache Software Foundation (ASF) under one or more
1:  * contributor license agreements.  See the NOTICE file distributed with
1:  * this work for additional information regarding copyright ownership.
1:  * The ASF licenses this file to You under the Apache License, Version 2.0
1:  * (the "License"); you may not use this file except in compliance with
1:  * the License.  You may obtain a copy of the License at
1:  *
1:  *    http://www.apache.org/licenses/LICENSE-2.0
1:  *
1:  * Unless required by applicable law or agreed to in writing, software
1:  * distributed under the License is distributed on an "AS IS" BASIS,
1:  * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
1:  * See the License for the specific language governing permissions and
1:  * limitations under the License.
1:  */
1: package org.apache.carbondata.presto;
1: 
1: import java.util.Arrays;
1: import java.util.HashSet;
1: import java.util.Set;
1: 
1: import org.apache.carbondata.core.metadata.datatype.StructField;
1: import org.apache.carbondata.core.scan.result.vector.impl.CarbonColumnVectorImpl;
1: 
1: public class CarbonVectorBatch {
1: 
0:   private static final int DEFAULT_BATCH_SIZE = 1024;
1: 
0:   private final StructField[] schema;
1:   private final int capacity;
0:   private int numRows;
1:   private final CarbonColumnVectorImpl[] columns;
1: 
1:   // True if the row is filtered.
1:   private final boolean[] filteredRows;
1: 
1:   // Column indices that cannot have null values.
1:   private final Set<Integer> nullFilteredColumns;
1: 
1:   // Total number of rows that have been filtered.
1:   private int numRowsFiltered = 0;
1: 
1: 
0:   private CarbonVectorBatch(StructField[] schema, int maxRows) {
0:     this.schema = schema;
1:     this.capacity = maxRows;
1:     this.columns = new CarbonColumnVectorImpl[schema.length];
1:     this.nullFilteredColumns = new HashSet<>();
1:     this.filteredRows = new boolean[maxRows];
1: 
1:     for (int i = 0; i < schema.length; ++i) {
0:       StructField field = schema[i];
0:       columns[i] = new CarbonColumnVectorImpl(maxRows, field.getDataType());
1:     }
1: 
1:   }
1: 
1: 
0:   public static CarbonVectorBatch allocate(StructField[] schema) {
0:     return new CarbonVectorBatch(schema, DEFAULT_BATCH_SIZE);
1:   }
1: 
0:   public static CarbonVectorBatch allocate(StructField[] schema,  int maxRows) {
0:     return new CarbonVectorBatch(schema, maxRows);
1:   }
1:   /**
1:    * Resets the batch for writing.
1:    */
1:   public void reset() {
1:     for (int i = 0; i < numCols(); ++i) {
1:       columns[i].reset();
1:     }
1:     if (this.numRowsFiltered > 0) {
1:       Arrays.fill(filteredRows, false);
1:     }
1:     this.numRows = 0;
1:     this.numRowsFiltered = 0;
1:   }
1: 
1: 
1:   /**
1:    * Returns the number of columns that make up this batch.
1:    */
0:   public int numCols() { return columns.length; }
1: 
1:   /**
1:    * Sets the number of rows that are valid. Additionally, marks all rows as "filtered" if one or
1:    * more of their attributes are part of a non-nullable column.
1:    */
1:   public void setNumRows(int numRows) {
0:     assert(numRows <= this.capacity);
1:     this.numRows = numRows;
1: 
1:     for (int ordinal : nullFilteredColumns) {
1:       for (int rowId = 0; rowId < numRows; rowId++) {
1:         if (!filteredRows[rowId] && columns[ordinal].isNull(rowId)) {
1:           filteredRows[rowId] = true;
1:           ++numRowsFiltered;
1:         }
1:       }
1:     }
1:   }
1: 
1: 
1:   /**
1:    * Returns the number of rows for read, including filtered rows.
1:    */
0:   public int numRows() { return numRows; }
1: 
1:   /**
1:    * Returns the number of valid rows.
1:    */
1:   public int numValidRows() {
0:     assert(numRowsFiltered <= numRows);
1:     return numRows - numRowsFiltered;
1:   }
1: 
1:   /**
1:    * Returns the column at `ordinal`.
1:    */
0:   public CarbonColumnVectorImpl column(int ordinal) { return columns[ordinal]; }
1: 
1:   /**
1:    * Returns the max capacity (in number of rows) for this batch.
1:    */
0:   public int capacity() { return capacity; }
1: 
1: 
1: 
1: }
============================================================================