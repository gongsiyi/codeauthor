1:bbb1092: /*
1:bbb1092:  * Licensed to the Apache Software Foundation (ASF) under one or more
1:bbb1092:  * contributor license agreements.  See the NOTICE file distributed with
1:bbb1092:  * this work for additional information regarding copyright ownership.
1:bbb1092:  * The ASF licenses this file to You under the Apache License, Version 2.0
1:bbb1092:  * (the "License"); you may not use this file except in compliance with
1:bbb1092:  * the License.  You may obtain a copy of the License at
1:8b33ab2:  *
1:bbb1092:  *    http://www.apache.org/licenses/LICENSE-2.0
2:bbb1092:  *
1:bbb1092:  * Unless required by applicable law or agreed to in writing, software
1:bbb1092:  * distributed under the License is distributed on an "AS IS" BASIS,
1:bbb1092:  * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
1:bbb1092:  * See the License for the specific language governing permissions and
1:bbb1092:  * limitations under the License.
6:bbb1092:  */
44:bbb1092: 
1:bbb1092: package org.apache.carbondata.datamap.lucene;
1:5229443: 
1:f184de8: import java.io.File;
1:bbb1092: import java.io.IOException;
1:bbb1092: import java.io.UnsupportedEncodingException;
1:f184de8: import java.nio.ByteBuffer;
1:f184de8: import java.util.Arrays;
1:f184de8: import java.util.HashMap;
1:860e144: import java.util.List;
1:f184de8: import java.util.Map;
1:860e144: 
1:bbb1092: import org.apache.carbondata.common.annotations.InterfaceAudience;
1:bbb1092: import org.apache.carbondata.common.logging.LogService;
1:bbb1092: import org.apache.carbondata.common.logging.LogServiceFactory;
1:860e144: import org.apache.carbondata.core.constants.CarbonCommonConstants;
1:5397c05: import org.apache.carbondata.core.datamap.Segment;
1:bbb1092: import org.apache.carbondata.core.datamap.dev.DataMapWriter;
1:bbb1092: import org.apache.carbondata.core.datastore.impl.FileFactory;
1:bbb1092: import org.apache.carbondata.core.datastore.page.ColumnPage;
1:bbb1092: import org.apache.carbondata.core.metadata.datatype.DataType;
1:bbb1092: import org.apache.carbondata.core.metadata.datatype.DataTypes;
1:9db662a: import org.apache.carbondata.core.metadata.schema.table.column.CarbonColumn;
1:860e144: import org.apache.carbondata.core.util.CarbonProperties;
1:860e144: 
1:5229443: import org.apache.hadoop.conf.Configuration;
1:bbb1092: import org.apache.hadoop.fs.FileSystem;
1:bbb1092: import org.apache.hadoop.fs.Path;
1:bbb1092: import org.apache.lucene.analysis.Analyzer;
1:07a77fa: import org.apache.lucene.analysis.CharArraySet;
1:bbb1092: import org.apache.lucene.analysis.standard.StandardAnalyzer;
1:07a77fa: import org.apache.lucene.codecs.Codec;
1:860e144: import org.apache.lucene.codecs.lucene50.Lucene50StoredFieldsFormat;
1:860e144: import org.apache.lucene.codecs.lucene62.Lucene62Codec;
1:bbb1092: import org.apache.lucene.document.Document;
1:bbb1092: import org.apache.lucene.document.DoublePoint;
1:bbb1092: import org.apache.lucene.document.Field;
1:bbb1092: import org.apache.lucene.document.FloatPoint;
1:bbb1092: import org.apache.lucene.document.IntPoint;
1:bbb1092: import org.apache.lucene.document.IntRangeField;
1:bbb1092: import org.apache.lucene.document.LongPoint;
1:bbb1092: import org.apache.lucene.document.StoredField;
1:bbb1092: import org.apache.lucene.document.TextField;
1:bbb1092: import org.apache.lucene.index.IndexWriter;
1:bbb1092: import org.apache.lucene.index.IndexWriterConfig;
1:bbb1092: import org.apache.lucene.store.Directory;
1:bbb1092: import org.apache.lucene.store.RAMDirectory;
1:bbb1092: import org.apache.solr.store.hdfs.HdfsDirectory;
1:f184de8: import org.roaringbitmap.IntIterator;
1:f184de8: import org.roaringbitmap.RoaringBitmap;
1:9db662a: 
6:bbb1092: /**
1:bbb1092:  * Implementation to write lucene index while loading
1:bbb1092:  */
1:bbb1092: @InterfaceAudience.Internal
1:bbb1092: public class LuceneDataMapWriter extends DataMapWriter {
1:bbb1092:   /**
1:bbb1092:    * logger
1:bbb1092:    */
1:bbb1092:   private static final LogService LOGGER =
1:bbb1092:       LogServiceFactory.getLogService(LuceneDataMapWriter.class.getName());
1:bbb1092: 
1:bbb1092:   /**
1:bbb1092:    * index writer
1:bbb1092:    */
1:bbb1092:   private IndexWriter indexWriter = null;
1:bbb1092: 
1:bbb1092:   private Analyzer analyzer = null;
1:bbb1092: 
1:21c5fb1:   public static final String PAGEID_NAME = "pageId";
1:bbb1092: 
1:21c5fb1:   public static final String ROWID_NAME = "rowId";
1:9db662a: 
1:07a77fa:   private Codec speedCodec = new Lucene62Codec(Lucene50StoredFieldsFormat.Mode.BEST_SPEED);
1:07a77fa: 
1:07a77fa:   private Codec compressionCodec =
1:07a77fa:       new Lucene62Codec(Lucene50StoredFieldsFormat.Mode.BEST_COMPRESSION);
1:07a77fa: 
1:f184de8:   private Map<LuceneColumnKeys, Map<Integer, RoaringBitmap>> cache = new HashMap<>();
1:f184de8: 
1:f184de8:   private int cacheSize;
1:f184de8: 
1:f184de8:   private ByteBuffer intBuffer = ByteBuffer.allocate(4);
1:f184de8: 
1:f184de8:   private boolean storeBlockletWise;
1:f184de8: 
1:9db662a:   LuceneDataMapWriter(String tablePath, String dataMapName, List<CarbonColumn> indexColumns,
1:f184de8:       Segment segment, String shardName, int flushSize,
1:f184de8:       boolean storeBlockletWise) {
1:9db662a:     super(tablePath, dataMapName, indexColumns, segment, shardName);
1:f184de8:     this.cacheSize = flushSize;
1:f184de8:     this.storeBlockletWise = storeBlockletWise;
1:860e144:   }
1:860e144: 
1:860e144:   /**
1:bbb1092:    * Start of new block notification.
1:860e144:    */
1:9db662a:   public void onBlockStart(String blockId) throws IOException {
1:5229443: 
1:5229443:   }
1:f184de8: 
1:860e144:   /**
1:f184de8:    * End of block notification
1:860e144:    */
1:f184de8:   public void onBlockEnd(String blockId) throws IOException {
1:f184de8: 
1:860e144:   }
1:f184de8: 
1:f184de8:   private RAMDirectory ramDir;
1:f184de8:   private IndexWriter ramIndexWriter;
1:f184de8: 
1:f184de8:   /**
1:f184de8:    * Start of new blocklet notification.
1:f184de8:    */
1:f184de8:   public void onBlockletStart(int blockletId) throws IOException {
1:5229443:     if (null == analyzer) {
1:07a77fa:       if (CarbonProperties.getInstance()
1:07a77fa:           .getProperty(CarbonCommonConstants.CARBON_LUCENE_INDEX_STOP_WORDS,
1:07a77fa:               CarbonCommonConstants.CARBON_LUCENE_INDEX_STOP_WORDS_DEFAULT)
1:07a77fa:           .equalsIgnoreCase("true")) {
1:07a77fa:         analyzer = new StandardAnalyzer(CharArraySet.EMPTY_SET);
1:07a77fa:       } else {
1:07a77fa:         analyzer = new StandardAnalyzer();
1:07a77fa:       }
1:860e144:     }
1:f184de8:     // save index data into ram, write into disk after one page finished
1:f184de8:     ramDir = new RAMDirectory();
1:f184de8:     ramIndexWriter = new IndexWriter(ramDir, new IndexWriterConfig(analyzer));
1:f184de8: 
1:5229443:     if (indexWriter != null) {
1:5229443:       return;
1:860e144:     }
1:5229443:     // get index path, put index data into segment's path
1:f184de8:     String dataMapPath;
1:f184de8:     if (storeBlockletWise) {
1:f184de8:       dataMapPath = this.dataMapPath + File.separator + blockletId;
1:860e144:     } else {
1:f184de8:       dataMapPath = this.dataMapPath;
1:860e144:     }
1:9db662a:     Path indexPath = FileFactory.getPath(dataMapPath);
1:5229443:     FileSystem fs = FileFactory.getFileSystem(indexPath);
1:5229443: 
1:5229443:     // if index path not exists, create it
1:5229443:     if (!fs.exists(indexPath)) {
1:9db662a:       if (!fs.mkdirs(indexPath)) {
1:9db662a:         throw new IOException("Failed to create directory " + dataMapPath);
1:5229443:       }
1:5229443:     }
1:5229443: 
1:5229443:     // the indexWriter closes the FileSystem on closing the writer, so for a new configuration
1:5229443:     // and disable the cache for the index writer, it will be closed on closing the writer
1:2a9604c:     Configuration conf = FileFactory.getConfiguration();
1:5229443:     conf.set("fs.hdfs.impl.disable.cache", "true");
1:5229443: 
1:5229443:     // create a index writer
1:5229443:     Directory indexDir = new HdfsDirectory(indexPath, conf);
1:5229443: 
1:5229443:     IndexWriterConfig indexWriterConfig = new IndexWriterConfig(analyzer);
1:5229443:     if (CarbonProperties.getInstance()
1:5229443:         .getProperty(CarbonCommonConstants.CARBON_LUCENE_COMPRESSION_MODE,
1:5229443:             CarbonCommonConstants.CARBON_LUCENE_COMPRESSION_MODE_DEFAULT)
1:5229443:         .equalsIgnoreCase(CarbonCommonConstants.CARBON_LUCENE_COMPRESSION_MODE_DEFAULT)) {
1:07a77fa:       indexWriterConfig.setCodec(speedCodec);
1:5229443:     } else {
1:5229443:       indexWriterConfig
1:07a77fa:           .setCodec(compressionCodec);
1:5229443:     }
1:5229443: 
1:f184de8:     indexWriter = new IndexWriter(indexDir, indexWriterConfig);
1:860e144:   }
1:860e144: 
1:f184de8:   /**
1:bbb1092:    * End of blocklet notification
1:f184de8:    */
1:9db662a:   public void onBlockletEnd(int blockletId) throws IOException {
1:9db662a:     // close ram writer
1:9db662a:     ramIndexWriter.close();
1:f184de8: 
1:9db662a:     // add ram index data into disk
1:9db662a:     indexWriter.addIndexes(ramDir);
1:9db662a: 
1:9db662a:     // delete this ram data
1:9db662a:     ramDir.close();
1:f184de8: 
1:f184de8:     if (storeBlockletWise) {
1:f184de8:       flushCache(cache, getIndexColumns(), indexWriter, storeBlockletWise);
1:f184de8:       indexWriter.close();
1:f184de8:       indexWriter = null;
1:f184de8:     }
1:f184de8:   }
1:9db662a: 
1:bbb1092:   /**
1:bbb1092:    * Add the column pages row to the datamap, order of pages is same as `indexColumns` in
1:bbb1092:    * DataMapMeta returned in DataMapFactory.
1:bbb1092:    * Implementation should copy the content of `pages` as needed, because `pages` memory
1:bbb1092:    * may be freed after this method returns, if using unsafe column page.
1:bbb1092:    */
1:9db662a:   public void onPageAdded(int blockletId, int pageId, int pageSize, ColumnPage[] pages)
1:9db662a:       throws IOException {
1:f184de8:     // save index data into ram, write into disk after one page finished
1:f184de8:     int columnsCount = pages.length;
1:f184de8:     if (columnsCount <= 0) {
1:f184de8:       LOGGER.warn("No data in the page " + pageId + "with blockletid " + blockletId
1:f184de8:           + " to write lucene datamap");
1:f184de8:       return;
1:f184de8:     }
1:bbb1092:     for (int rowId = 0; rowId < pageSize; rowId++) {
1:6bef57b:       // add indexed columns value into the document
1:f184de8:       LuceneColumnKeys columns = new LuceneColumnKeys(getIndexColumns().size());
1:f184de8:       int i = 0;
1:f184de8:       for (ColumnPage page : pages) {
1:f184de8:         if (!page.getNullBits().get(rowId)) {
1:f184de8:           columns.colValues[i++] = getValue(page, rowId);
1:f184de8:         }
1:f184de8:       }
1:f184de8:       if (cacheSize > 0) {
1:f184de8:         addToCache(columns, rowId, pageId, blockletId, cache, intBuffer, storeBlockletWise);
1:860e144:       } else {
1:f184de8:         addData(columns, rowId, pageId, blockletId, intBuffer, ramIndexWriter, getIndexColumns(),
1:f184de8:             storeBlockletWise);
1:f184de8:       }
1:f184de8:     }
1:f184de8:     if (cacheSize > 0) {
1:f184de8:       flushCacheIfPossible();
1:f184de8:     }
1:9db662a:   }
1:9db662a: 
1:f184de8:   private static void addField(Document doc, Object key, String fieldName, Field.Store store) {
1:bbb1092:     //get field name
1:f184de8:     if (key instanceof Byte) {
1:bbb1092:       // byte type , use int range to deal with byte, lucene has no byte type
1:f184de8:       byte value = (Byte) key;
1:bbb1092:       IntRangeField field =
1:bbb1092:           new IntRangeField(fieldName, new int[] { Byte.MIN_VALUE }, new int[] { Byte.MAX_VALUE });
1:bbb1092:       field.setIntValue(value);
1:bbb1092:       doc.add(field);
1:9db662a: 
1:bbb1092:       // if need store it , add StoredField
1:bbb1092:       if (store == Field.Store.YES) {
1:bbb1092:         doc.add(new StoredField(fieldName, (int) value));
1:9db662a:       }
1:f184de8:     } else if (key instanceof Short) {
1:bbb1092:       // short type , use int range to deal with short type, lucene has no short type
1:f184de8:       short value = (Short) key;
1:bbb1092:       IntRangeField field = new IntRangeField(fieldName, new int[] { Short.MIN_VALUE },
1:bbb1092:           new int[] { Short.MAX_VALUE });
1:bbb1092:       field.setShortValue(value);
1:bbb1092:       doc.add(field);
1:9db662a: 
1:bbb1092:       // if need store it , add StoredField
1:bbb1092:       if (store == Field.Store.YES) {
1:bbb1092:         doc.add(new StoredField(fieldName, (int) value));
1:9db662a:       }
1:f184de8:     } else if (key instanceof Integer) {
1:bbb1092:       // int type , use int point to deal with int type
1:f184de8:       int value = (Integer) key;
1:f184de8:       doc.add(new IntPoint(fieldName, new int[] { value }));
1:6bef57b: 
1:bbb1092:       // if need store it , add StoredField
1:bbb1092:       if (store == Field.Store.YES) {
1:bbb1092:         doc.add(new StoredField(fieldName, value));
21:bbb1092:       }
1:f184de8:     } else if (key instanceof Long) {
1:bbb1092:       // long type , use long point to deal with long type
1:f184de8:       long value = (Long) key;
1:f184de8:       doc.add(new LongPoint(fieldName, new long[] { value }));
1:bbb1092: 
1:bbb1092:       // if need store it , add StoredField
1:bbb1092:       if (store == Field.Store.YES) {
1:bbb1092:         doc.add(new StoredField(fieldName, value));
1:bbb1092:       }
1:f184de8:     } else if (key instanceof Float) {
1:f184de8:       float value = (Float) key;
1:f184de8:       doc.add(new FloatPoint(fieldName, new float[] { value }));
1:bbb1092:       if (store == Field.Store.YES) {
1:9db662a:         doc.add(new FloatPoint(fieldName, value));
1:bbb1092:       }
1:f184de8:     } else if (key instanceof Double) {
1:f184de8:       double value = (Double) key;
1:f184de8:       doc.add(new DoublePoint(fieldName, new double[] { value }));
1:bbb1092:       if (store == Field.Store.YES) {
1:9db662a:         doc.add(new DoublePoint(fieldName, value));
1:bbb1092:       }
1:f184de8:     } else if (key instanceof String) {
1:f184de8:       String strValue = (String) key;
1:bbb1092:       doc.add(new TextField(fieldName, strValue, store));
1:f184de8:     } else if (key instanceof Boolean) {
1:f184de8:       boolean value = (Boolean) key;
1:bbb1092:       IntRangeField field = new IntRangeField(fieldName, new int[] { 0 }, new int[] { 1 });
1:bbb1092:       field.setIntValue(value ? 1 : 0);
1:bbb1092:       doc.add(field);
1:bbb1092:       if (store == Field.Store.YES) {
1:bbb1092:         doc.add(new StoredField(fieldName, value ? 1 : 0));
1:f184de8:       }
1:f184de8:     }
1:bbb1092:   }
1:f184de8: 
1:f184de8:   private Object getValue(ColumnPage page, int rowId) {
1:f184de8: 
1:f184de8:     //get field type
1:860e144:     DataType type = page.getColumnSpec().getSchemaDataType();
1:f184de8:     Object value = null;
1:f184de8:     if (type == DataTypes.BYTE) {
1:f184de8:       // byte type , use int range to deal with byte, lucene has no byte type
1:f184de8:       value = page.getByte(rowId);
1:f184de8:     } else if (type == DataTypes.SHORT) {
1:f184de8:       // short type , use int range to deal with short type, lucene has no short type
1:f184de8:       value = page.getShort(rowId);
1:f184de8:     } else if (type == DataTypes.INT) {
1:f184de8:       // int type , use int point to deal with int type
1:f184de8:       value = page.getInt(rowId);
1:f184de8:     } else if (type == DataTypes.LONG) {
1:f184de8:       // long type , use long point to deal with long type
1:f184de8:       value = page.getLong(rowId);
1:f184de8:     } else if (type == DataTypes.FLOAT) {
1:f184de8:       value = page.getFloat(rowId);
1:f184de8:     } else if (type == DataTypes.DOUBLE) {
1:f184de8:       value = page.getDouble(rowId);
1:f184de8:     } else if (type == DataTypes.STRING) {
1:f184de8:       byte[] bytes = page.getBytes(rowId);
1:f184de8:       try {
1:f184de8:         value = new String(bytes, 2, bytes.length - 2, "UTF-8");
1:f184de8:       } catch (UnsupportedEncodingException e) {
1:f184de8:         throw new RuntimeException(e);
1:f184de8:       }
1:f184de8:     } else if (type == DataTypes.DATE) {
1:f184de8:       throw new RuntimeException("unsupported data type " + type);
1:f184de8:     } else if (type == DataTypes.TIMESTAMP) {
1:f184de8:       throw new RuntimeException("unsupported data type " + type);
1:f184de8:     } else if (type == DataTypes.BOOLEAN) {
1:f184de8:       value = page.getBoolean(rowId);
1:f184de8:     } else {
1:bbb1092:       LOGGER.error("unsupport data type " + type);
2:9db662a:       throw new RuntimeException("unsupported data type " + type);
1:bbb1092:     }
1:f184de8:     return value;
1:f184de8:   }
1:f184de8: 
1:f184de8:   public static void addToCache(LuceneColumnKeys key, int rowId, int pageId, int blockletId,
1:f184de8:       Map<LuceneColumnKeys, Map<Integer, RoaringBitmap>> cache, ByteBuffer intBuffer,
1:f184de8:       boolean storeBlockletWise) {
1:f184de8:     Map<Integer, RoaringBitmap> setMap = cache.get(key);
1:f184de8:     if (setMap == null) {
1:f184de8:       setMap = new HashMap<>();
1:f184de8:       cache.put(key, setMap);
1:f184de8:     }
1:f184de8:     int combinKey;
1:f184de8:     if (!storeBlockletWise) {
1:f184de8:       intBuffer.clear();
1:f184de8:       intBuffer.putShort((short) blockletId);
1:f184de8:       intBuffer.putShort((short) pageId);
1:f184de8:       intBuffer.rewind();
1:f184de8:       combinKey = intBuffer.getInt();
1:f184de8:     } else {
1:f184de8:       combinKey = pageId;
1:f184de8:     }
1:f184de8:     RoaringBitmap bitSet = setMap.get(combinKey);
1:f184de8:     if (bitSet == null) {
1:f184de8:       bitSet = new RoaringBitmap();
1:f184de8:       setMap.put(combinKey, bitSet);
1:f184de8:     }
1:f184de8:     bitSet.add(rowId);
1:f184de8:   }
1:f184de8: 
1:f184de8:   public static void addData(LuceneColumnKeys key, int rowId, int pageId, int blockletId,
1:f184de8:       ByteBuffer intBuffer, IndexWriter indexWriter, List<CarbonColumn> indexCols,
1:f184de8:       boolean storeBlockletWise) throws IOException {
1:f184de8: 
1:f184de8:     Document document = new Document();
1:f184de8:     for (int i = 0; i < key.getColValues().length; i++) {
1:f184de8:       addField(document, key.getColValues()[i], indexCols.get(i).getColName(), Field.Store.NO);
1:f184de8:     }
1:f184de8:     intBuffer.clear();
1:f184de8:     if (storeBlockletWise) {
1:f184de8:       // No need to store blocklet id to it.
1:f184de8:       intBuffer.putShort((short) pageId);
1:f184de8:       intBuffer.putShort((short) rowId);
1:f184de8:       intBuffer.rewind();
1:f184de8:       document.add(new StoredField(ROWID_NAME, intBuffer.getInt()));
2:f184de8:     } else {
1:f184de8:       intBuffer.putShort((short) blockletId);
1:f184de8:       intBuffer.putShort((short) pageId);
1:f184de8:       intBuffer.rewind();
1:f184de8:       document.add(new StoredField(PAGEID_NAME, intBuffer.getInt()));
1:f184de8:       document.add(new StoredField(ROWID_NAME, (short) rowId));
1:f184de8:     }
1:f184de8:     indexWriter.addDocument(document);
1:f184de8:   }
1:f184de8: 
1:f184de8:   private void flushCacheIfPossible() throws IOException {
1:f184de8:     if (cache.size() > cacheSize) {
1:f184de8:       flushCache(cache, getIndexColumns(), indexWriter, storeBlockletWise);
1:f184de8:     }
1:f184de8:   }
1:f184de8: 
1:f184de8:   public static void flushCache(Map<LuceneColumnKeys, Map<Integer, RoaringBitmap>> cache,
1:f184de8:       List<CarbonColumn> indexCols, IndexWriter indexWriter, boolean storeBlockletWise)
1:f184de8:       throws IOException {
1:f184de8:     for (Map.Entry<LuceneColumnKeys, Map<Integer, RoaringBitmap>> entry : cache.entrySet()) {
1:f184de8:       Document document = new Document();
1:f184de8:       LuceneColumnKeys key = entry.getKey();
1:f184de8:       for (int i = 0; i < key.getColValues().length; i++) {
1:f184de8:         addField(document, key.getColValues()[i], indexCols.get(i).getColName(), Field.Store.NO);
1:f184de8:       }
1:f184de8:       Map<Integer, RoaringBitmap> value = entry.getValue();
1:f184de8:       int count = 0;
1:f184de8:       for (Map.Entry<Integer, RoaringBitmap> pageData : value.entrySet()) {
1:f184de8:         RoaringBitmap bitMap = pageData.getValue();
1:f184de8:         int cardinality = bitMap.getCardinality();
1:f184de8:         // Each row is short and pageid is stored in int
1:f184de8:         ByteBuffer byteBuffer = ByteBuffer.allocate(cardinality * 2 + 4);
1:f184de8:         if (!storeBlockletWise) {
1:f184de8:           byteBuffer.putInt(pageData.getKey());
1:f184de8:         } else {
1:f184de8:           byteBuffer.putShort(pageData.getKey().shortValue());
1:f184de8:         }
1:f184de8:         IntIterator intIterator = bitMap.getIntIterator();
1:f184de8:         while (intIterator.hasNext()) {
1:f184de8:           byteBuffer.putShort((short) intIterator.next());
1:f184de8:         }
1:f184de8:         document.add(new StoredField(PAGEID_NAME + count, byteBuffer.array()));
1:f184de8:         count++;
1:f184de8:       }
1:f184de8:       indexWriter.addDocument(document);
1:f184de8:     }
1:f184de8:     cache.clear();
1:bbb1092:   }
1:bbb1092: 
1:bbb1092:   /**
1:bbb1092:    * This is called during closing of writer.So after this call no more data will be sent to this
1:bbb1092:    * class.
1:bbb1092:    */
1:bbb1092:   public void finish() throws IOException {
1:7f4bd3d:     if (!isWritingFinished()) {
1:7f4bd3d:       flushCache(cache, getIndexColumns(), indexWriter, storeBlockletWise);
1:7f4bd3d:       // finished a file , close this index writer
1:7f4bd3d:       if (indexWriter != null) {
1:7f4bd3d:         indexWriter.close();
1:7f4bd3d:         indexWriter = null;
1:7f4bd3d:       }
1:7f4bd3d:       setWritingFinished(true);
1:8b33ab2:     }
1:bbb1092:   }
1:bbb1092: 
1:f184de8:   /**
1:f184de8:    * Keeps column values of a single row.
1:f184de8:    */
1:f184de8:   public static class LuceneColumnKeys {
1:f184de8: 
1:f184de8:     private Object[] colValues;
1:f184de8: 
1:f184de8:     public LuceneColumnKeys(int size) {
1:f184de8:       colValues = new Object[size];
1:f184de8:     }
1:f184de8: 
1:f184de8:     public Object[] getColValues() {
1:f184de8:       return colValues;
1:f184de8:     }
1:f184de8: 
1:f184de8:     @Override
1:f184de8:     public boolean equals(Object o) {
1:f184de8:       if (this == o) return true;
1:f184de8:       if (o == null || getClass() != o.getClass()) return false;
1:f184de8:       LuceneColumnKeys that = (LuceneColumnKeys) o;
1:f184de8:       return Arrays.equals(colValues, that.colValues);
1:f184de8:     }
1:f184de8: 
1:f184de8:     @Override
1:f184de8:     public int hashCode() {
1:f184de8:       return Arrays.hashCode(colValues);
1:f184de8:     }
1:f184de8:   }
1:bbb1092: }
============================================================================
author:kunal642
-------------------------------------------------------------------------------
commit:2a9604c
/////////////////////////////////////////////////////////////////////////
1:     Configuration conf = FileFactory.getConfiguration();
author:akashrn5
-------------------------------------------------------------------------------
commit:7f4bd3d
/////////////////////////////////////////////////////////////////////////
1:     if (!isWritingFinished()) {
1:       flushCache(cache, getIndexColumns(), indexWriter, storeBlockletWise);
1:       // finished a file , close this index writer
1:       if (indexWriter != null) {
1:         indexWriter.close();
1:         indexWriter = null;
1:       }
1:       setWritingFinished(true);
commit:07a77fa
/////////////////////////////////////////////////////////////////////////
1: import org.apache.lucene.analysis.CharArraySet;
1: import org.apache.lucene.codecs.Codec;
/////////////////////////////////////////////////////////////////////////
1:   private Codec speedCodec = new Lucene62Codec(Lucene50StoredFieldsFormat.Mode.BEST_SPEED);
1: 
1:   private Codec compressionCodec =
1:       new Lucene62Codec(Lucene50StoredFieldsFormat.Mode.BEST_COMPRESSION);
1: 
/////////////////////////////////////////////////////////////////////////
1:       if (CarbonProperties.getInstance()
1:           .getProperty(CarbonCommonConstants.CARBON_LUCENE_INDEX_STOP_WORDS,
1:               CarbonCommonConstants.CARBON_LUCENE_INDEX_STOP_WORDS_DEFAULT)
1:           .equalsIgnoreCase("true")) {
1:         analyzer = new StandardAnalyzer(CharArraySet.EMPTY_SET);
1:       } else {
1:         analyzer = new StandardAnalyzer();
1:       }
/////////////////////////////////////////////////////////////////////////
1:       indexWriterConfig.setCodec(speedCodec);
1:           .setCodec(compressionCodec);
commit:5229443
/////////////////////////////////////////////////////////////////////////
/////////////////////////////////////////////////////////////////////////
1: import org.apache.hadoop.conf.Configuration;
/////////////////////////////////////////////////////////////////////////
0:   private String indexShardName = null;
1: 
/////////////////////////////////////////////////////////////////////////
0:     if (this.indexShardName == null || !this.indexShardName.equals(indexShardName)) {
1:       if (indexWriter != null) {
1:         return;
1:       }
1:       // get index path, put index data into segment's path
0:       String strIndexPath = getIndexPath(indexShardName);
0:       Path indexPath = FileFactory.getPath(strIndexPath);
1:       FileSystem fs = FileFactory.getFileSystem(indexPath);
1: 
1:       // if index path not exists, create it
1:       if (!fs.exists(indexPath)) {
0:         fs.mkdirs(indexPath);
1:       }
1: 
1:       if (null == analyzer) {
0:         analyzer = new StandardAnalyzer();
1:       }
1: 
1:       // the indexWriter closes the FileSystem on closing the writer, so for a new configuration
1:       // and disable the cache for the index writer, it will be closed on closing the writer
0:       Configuration conf = new Configuration();
1:       conf.set("fs.hdfs.impl.disable.cache", "true");
1: 
1:       // create a index writer
1:       Directory indexDir = new HdfsDirectory(indexPath, conf);
1: 
1:       IndexWriterConfig indexWriterConfig = new IndexWriterConfig(analyzer);
1:       if (CarbonProperties.getInstance()
1:           .getProperty(CarbonCommonConstants.CARBON_LUCENE_COMPRESSION_MODE,
1:               CarbonCommonConstants.CARBON_LUCENE_COMPRESSION_MODE_DEFAULT)
1:           .equalsIgnoreCase(CarbonCommonConstants.CARBON_LUCENE_COMPRESSION_MODE_DEFAULT)) {
0:         indexWriterConfig.setCodec(new Lucene62Codec(Lucene50StoredFieldsFormat.Mode.BEST_SPEED));
1:       } else {
1:         indexWriterConfig
0:             .setCodec(new Lucene62Codec(Lucene50StoredFieldsFormat.Mode.BEST_COMPRESSION));
1:       }
1: 
0:       indexWriter = new IndexWriter(indexDir, new IndexWriterConfig(analyzer));
/////////////////////////////////////////////////////////////////////////
commit:860e144
/////////////////////////////////////////////////////////////////////////
1: import java.util.List;
1: import org.apache.carbondata.core.constants.CarbonCommonConstants;
0: import org.apache.carbondata.core.datastore.filesystem.CarbonFile;
0: import org.apache.carbondata.core.datastore.filesystem.CarbonFileFilter;
1: import org.apache.carbondata.core.util.CarbonProperties;
1: import org.apache.lucene.codecs.lucene50.Lucene50StoredFieldsFormat;
1: import org.apache.lucene.codecs.lucene62.Lucene62Codec;
/////////////////////////////////////////////////////////////////////////
0:   private List<String> indexedCarbonColumns = null;
1: 
/////////////////////////////////////////////////////////////////////////
0:       String writeDirectoryPath, boolean isFineGrain, List<String> indexedCarbonColumns) {
0:     this.indexedCarbonColumns = indexedCarbonColumns;
0:   private String getIndexPath(long taskId) {
0:       return genDataMapStorePathOnTaskId(identifier.getTablePath(), segmentId, dataMapName, taskId);
0:       return genDataMapStorePathOnTaskId(identifier.getTablePath(), segmentId, dataMapName, taskId);
0:   public void onBlockStart(String blockId, long taskId) throws IOException {
0:     String strIndexPath = getIndexPath(taskId);
/////////////////////////////////////////////////////////////////////////
1: 
0:     IndexWriterConfig indexWriterConfig = new IndexWriterConfig(analyzer);
0:     if (CarbonProperties.getInstance()
0:         .getProperty(CarbonCommonConstants.CARBON_LUCENE_COMPRESSION_MODE,
0:             CarbonCommonConstants.CARBON_LUCENE_COMPRESSION_MODE_DEFAULT)
0:         .equalsIgnoreCase(CarbonCommonConstants.CARBON_LUCENE_COMPRESSION_MODE_DEFAULT)) {
0:       indexWriterConfig.setCodec(new Lucene62Codec(Lucene50StoredFieldsFormat.Mode.BEST_SPEED));
1:     } else {
0:       indexWriterConfig
0:           .setCodec(new Lucene62Codec(Lucene50StoredFieldsFormat.Mode.BEST_COMPRESSION));
1:     }
1: 
/////////////////////////////////////////////////////////////////////////
0:         if (indexedCarbonColumns.contains(pages[colIdx].getColumnSpec().getFieldName())) {
0:           if (!pages[colIdx].getNullBits().get(rowId)) {
0:             addField(doc, pages[colIdx], rowId, Field.Store.NO);
1:           }
/////////////////////////////////////////////////////////////////////////
1:     DataType type = page.getColumnSpec().getSchemaDataType();
/////////////////////////////////////////////////////////////////////////
0:   public static String genDataMapStorePath(String tablePath, String segmentId, String dataMapName) {
1:   /**
0:    * Return store path for datamap based on the taskId, if three tasks get launched during loading,
0:    * then three folders will be created based on the three task Ids and lucene index file will be
0:    * written into those folders
0:    * @return store path based on taskID
1:    */
0:   private static String genDataMapStorePathOnTaskId(String tablePath, String segmentId,
0:       String dataMapName, long taskId) {
0:     return CarbonTablePath.getSegmentPath(tablePath, segmentId) + File.separator + dataMapName
0:         + File.separator + dataMapName + CarbonCommonConstants.UNDERSCORE + taskId
0:         + CarbonCommonConstants.UNDERSCORE + System.currentTimeMillis();
1:   }
1: 
1:   /**
0:    * returns all the directories of lucene index files for query
0:    * @param tablePath
0:    * @param segmentId
0:    * @param dataMapName
0:    * @return
1:    */
0:   public static CarbonFile[] getAllIndexDirs(String tablePath, String segmentId,
0:       final String dataMapName) {
0:     String dmPath =
0:         CarbonTablePath.getSegmentPath(tablePath, segmentId) + File.separator + dataMapName;
0:     FileFactory.FileType fileType = FileFactory.getFileType(dmPath);
0:     final CarbonFile dirPath = FileFactory.getCarbonFile(dmPath, fileType);
0:     return dirPath.listFiles(new CarbonFileFilter() {
0:       @Override public boolean accept(CarbonFile file) {
0:         if (file.isDirectory() && file.getName().startsWith(dataMapName)) {
0:           return true;
1:         } else {
0:           return false;
1:         }
1:       }
0:     });
1:   }
author:ravipesala
-------------------------------------------------------------------------------
commit:f184de8
/////////////////////////////////////////////////////////////////////////
1: import java.io.File;
1: import java.nio.ByteBuffer;
1: import java.util.Arrays;
1: import java.util.HashMap;
1: import java.util.Map;
/////////////////////////////////////////////////////////////////////////
1: import org.roaringbitmap.IntIterator;
1: import org.roaringbitmap.RoaringBitmap;
/////////////////////////////////////////////////////////////////////////
1:   private Map<LuceneColumnKeys, Map<Integer, RoaringBitmap>> cache = new HashMap<>();
1: 
1:   private int cacheSize;
1: 
1:   private ByteBuffer intBuffer = ByteBuffer.allocate(4);
1: 
1:   private boolean storeBlockletWise;
1: 
1:       Segment segment, String shardName, int flushSize,
1:       boolean storeBlockletWise) {
1:     this.cacheSize = flushSize;
1:     this.storeBlockletWise = storeBlockletWise;
1: 
1:   }
1: 
1:   /**
1:    * End of block notification
1:    */
1:   public void onBlockEnd(String blockId) throws IOException {
1: 
1:   }
1: 
1:   private RAMDirectory ramDir;
1:   private IndexWriter ramIndexWriter;
1: 
1:   /**
1:    * Start of new blocklet notification.
1:    */
1:   public void onBlockletStart(int blockletId) throws IOException {
0:     if (null == analyzer) {
0:       analyzer = new StandardAnalyzer();
1:     }
1:     // save index data into ram, write into disk after one page finished
1:     ramDir = new RAMDirectory();
1:     ramIndexWriter = new IndexWriter(ramDir, new IndexWriterConfig(analyzer));
1: 
1:     String dataMapPath;
1:     if (storeBlockletWise) {
1:       dataMapPath = this.dataMapPath + File.separator + blockletId;
1:     } else {
1:       dataMapPath = this.dataMapPath;
1:     }
/////////////////////////////////////////////////////////////////////////
/////////////////////////////////////////////////////////////////////////
1:     indexWriter = new IndexWriter(indexDir, indexWriterConfig);
/////////////////////////////////////////////////////////////////////////
1: 
1:     if (storeBlockletWise) {
1:       flushCache(cache, getIndexColumns(), indexWriter, storeBlockletWise);
1:       indexWriter.close();
1:       indexWriter = null;
1:     }
/////////////////////////////////////////////////////////////////////////
1:     // save index data into ram, write into disk after one page finished
1:     int columnsCount = pages.length;
1:     if (columnsCount <= 0) {
1:       LOGGER.warn("No data in the page " + pageId + "with blockletid " + blockletId
1:           + " to write lucene datamap");
1:       return;
1:     }
1:       LuceneColumnKeys columns = new LuceneColumnKeys(getIndexColumns().size());
1:       int i = 0;
1:       for (ColumnPage page : pages) {
1:         if (!page.getNullBits().get(rowId)) {
1:           columns.colValues[i++] = getValue(page, rowId);
1:       if (cacheSize > 0) {
1:         addToCache(columns, rowId, pageId, blockletId, cache, intBuffer, storeBlockletWise);
1:       } else {
1:         addData(columns, rowId, pageId, blockletId, intBuffer, ramIndexWriter, getIndexColumns(),
1:             storeBlockletWise);
1:       }
1:     if (cacheSize > 0) {
1:       flushCacheIfPossible();
1:     }
1:   private static void addField(Document doc, Object key, String fieldName, Field.Store store) {
1:     if (key instanceof Byte) {
1:       byte value = (Byte) key;
/////////////////////////////////////////////////////////////////////////
1:     } else if (key instanceof Short) {
1:       short value = (Short) key;
/////////////////////////////////////////////////////////////////////////
1:     } else if (key instanceof Integer) {
1:       int value = (Integer) key;
1:       doc.add(new IntPoint(fieldName, new int[] { value }));
1:     } else if (key instanceof Long) {
1:       long value = (Long) key;
1:       doc.add(new LongPoint(fieldName, new long[] { value }));
1:     } else if (key instanceof Float) {
1:       float value = (Float) key;
1:       doc.add(new FloatPoint(fieldName, new float[] { value }));
1:     } else if (key instanceof Double) {
1:       double value = (Double) key;
1:       doc.add(new DoublePoint(fieldName, new double[] { value }));
1:     } else if (key instanceof String) {
1:       String strValue = (String) key;
1:     } else if (key instanceof Boolean) {
1:       boolean value = (Boolean) key;
1:     }
1:   }
1: 
1:   private Object getValue(ColumnPage page, int rowId) {
1: 
1:     //get field type
0:     DataType type = page.getColumnSpec().getSchemaDataType();
1:     Object value = null;
1:     if (type == DataTypes.BYTE) {
1:       // byte type , use int range to deal with byte, lucene has no byte type
1:       value = page.getByte(rowId);
1:     } else if (type == DataTypes.SHORT) {
1:       // short type , use int range to deal with short type, lucene has no short type
1:       value = page.getShort(rowId);
1:     } else if (type == DataTypes.INT) {
1:       // int type , use int point to deal with int type
1:       value = page.getInt(rowId);
1:     } else if (type == DataTypes.LONG) {
1:       // long type , use long point to deal with long type
1:       value = page.getLong(rowId);
1:     } else if (type == DataTypes.FLOAT) {
1:       value = page.getFloat(rowId);
1:     } else if (type == DataTypes.DOUBLE) {
1:       value = page.getDouble(rowId);
1:     } else if (type == DataTypes.STRING) {
1:       byte[] bytes = page.getBytes(rowId);
1:       try {
1:         value = new String(bytes, 2, bytes.length - 2, "UTF-8");
1:       } catch (UnsupportedEncodingException e) {
1:         throw new RuntimeException(e);
1:       }
1:     } else if (type == DataTypes.DATE) {
1:       throw new RuntimeException("unsupported data type " + type);
1:     } else if (type == DataTypes.TIMESTAMP) {
1:       throw new RuntimeException("unsupported data type " + type);
1:     } else if (type == DataTypes.BOOLEAN) {
1:       value = page.getBoolean(rowId);
1:     return value;
1:   }
1: 
1:   public static void addToCache(LuceneColumnKeys key, int rowId, int pageId, int blockletId,
1:       Map<LuceneColumnKeys, Map<Integer, RoaringBitmap>> cache, ByteBuffer intBuffer,
1:       boolean storeBlockletWise) {
1:     Map<Integer, RoaringBitmap> setMap = cache.get(key);
1:     if (setMap == null) {
1:       setMap = new HashMap<>();
1:       cache.put(key, setMap);
1:     }
1:     int combinKey;
1:     if (!storeBlockletWise) {
1:       intBuffer.clear();
1:       intBuffer.putShort((short) blockletId);
1:       intBuffer.putShort((short) pageId);
1:       intBuffer.rewind();
1:       combinKey = intBuffer.getInt();
1:     } else {
1:       combinKey = pageId;
1:     }
1:     RoaringBitmap bitSet = setMap.get(combinKey);
1:     if (bitSet == null) {
1:       bitSet = new RoaringBitmap();
1:       setMap.put(combinKey, bitSet);
1:     }
1:     bitSet.add(rowId);
1:   }
1: 
1:   public static void addData(LuceneColumnKeys key, int rowId, int pageId, int blockletId,
1:       ByteBuffer intBuffer, IndexWriter indexWriter, List<CarbonColumn> indexCols,
1:       boolean storeBlockletWise) throws IOException {
1: 
1:     Document document = new Document();
1:     for (int i = 0; i < key.getColValues().length; i++) {
1:       addField(document, key.getColValues()[i], indexCols.get(i).getColName(), Field.Store.NO);
1:     }
1:     intBuffer.clear();
1:     if (storeBlockletWise) {
1:       // No need to store blocklet id to it.
1:       intBuffer.putShort((short) pageId);
1:       intBuffer.putShort((short) rowId);
1:       intBuffer.rewind();
1:       document.add(new StoredField(ROWID_NAME, intBuffer.getInt()));
1:     } else {
1:       intBuffer.putShort((short) blockletId);
1:       intBuffer.putShort((short) pageId);
1:       intBuffer.rewind();
1:       document.add(new StoredField(PAGEID_NAME, intBuffer.getInt()));
1:       document.add(new StoredField(ROWID_NAME, (short) rowId));
1:     }
1:     indexWriter.addDocument(document);
1:   }
1: 
1:   private void flushCacheIfPossible() throws IOException {
1:     if (cache.size() > cacheSize) {
1:       flushCache(cache, getIndexColumns(), indexWriter, storeBlockletWise);
1:     }
1:   }
1: 
1:   public static void flushCache(Map<LuceneColumnKeys, Map<Integer, RoaringBitmap>> cache,
1:       List<CarbonColumn> indexCols, IndexWriter indexWriter, boolean storeBlockletWise)
1:       throws IOException {
1:     for (Map.Entry<LuceneColumnKeys, Map<Integer, RoaringBitmap>> entry : cache.entrySet()) {
1:       Document document = new Document();
1:       LuceneColumnKeys key = entry.getKey();
1:       for (int i = 0; i < key.getColValues().length; i++) {
1:         addField(document, key.getColValues()[i], indexCols.get(i).getColName(), Field.Store.NO);
1:       }
1:       Map<Integer, RoaringBitmap> value = entry.getValue();
1:       int count = 0;
1:       for (Map.Entry<Integer, RoaringBitmap> pageData : value.entrySet()) {
1:         RoaringBitmap bitMap = pageData.getValue();
1:         int cardinality = bitMap.getCardinality();
1:         // Each row is short and pageid is stored in int
1:         ByteBuffer byteBuffer = ByteBuffer.allocate(cardinality * 2 + 4);
1:         if (!storeBlockletWise) {
1:           byteBuffer.putInt(pageData.getKey());
1:         } else {
1:           byteBuffer.putShort(pageData.getKey().shortValue());
1:         }
1:         IntIterator intIterator = bitMap.getIntIterator();
1:         while (intIterator.hasNext()) {
1:           byteBuffer.putShort((short) intIterator.next());
1:         }
1:         document.add(new StoredField(PAGEID_NAME + count, byteBuffer.array()));
1:         count++;
1:       }
1:       indexWriter.addDocument(document);
1:     }
1:     cache.clear();
/////////////////////////////////////////////////////////////////////////
0:     flushCache(cache, getIndexColumns(), indexWriter, storeBlockletWise);
1:   /**
1:    * Keeps column values of a single row.
1:    */
1:   public static class LuceneColumnKeys {
1: 
1:     private Object[] colValues;
1: 
1:     public LuceneColumnKeys(int size) {
1:       colValues = new Object[size];
1:     }
1: 
1:     public Object[] getColValues() {
1:       return colValues;
1:     }
1: 
1:     @Override
1:     public boolean equals(Object o) {
1:       if (this == o) return true;
1:       if (o == null || getClass() != o.getClass()) return false;
1:       LuceneColumnKeys that = (LuceneColumnKeys) o;
1:       return Arrays.equals(colValues, that.colValues);
1:     }
1: 
1:     @Override
1:     public int hashCode() {
1:       return Arrays.hashCode(colValues);
1:     }
1:   }
commit:8b33ab2
/////////////////////////////////////////////////////////////////////////
/////////////////////////////////////////////////////////////////////////
/////////////////////////////////////////////////////////////////////////
/////////////////////////////////////////////////////////////////////////
0:   private String getIndexPath(String taskName) {
0:       return genDataMapStorePathOnTaskId(identifier.getTablePath(), segmentId, dataMapName,
0:           taskName);
0:       return genDataMapStorePathOnTaskId(identifier.getTablePath(), segmentId, dataMapName,
0:           taskName);
0:   public void onBlockStart(String blockId, String indexShardName) throws IOException {
0:     String strIndexPath = getIndexPath(indexShardName);
/////////////////////////////////////////////////////////////////////////
/////////////////////////////////////////////////////////////////////////
/////////////////////////////////////////////////////////////////////////
0:     // finished a file , close this index writer
0:     if (indexWriter != null) {
0:       indexWriter.close();
1:     }
/////////////////////////////////////////////////////////////////////////
0:    * Return store path for datamap based on the taskName,if three tasks get launched during loading,
1:    *
0:       String dataMapName, String taskName) {
0:         + File.separator + taskName;
/////////////////////////////////////////////////////////////////////////
0:     return dirPath.listFiles();
author:Indhumathi27
-------------------------------------------------------------------------------
commit:61afa42
/////////////////////////////////////////////////////////////////////////
author:Jacky Li
-------------------------------------------------------------------------------
commit:9db662a
/////////////////////////////////////////////////////////////////////////
/////////////////////////////////////////////////////////////////////////
1: import org.apache.carbondata.core.metadata.schema.table.column.CarbonColumn;
/////////////////////////////////////////////////////////////////////////
/////////////////////////////////////////////////////////////////////////
1:   LuceneDataMapWriter(String tablePath, String dataMapName, List<CarbonColumn> indexColumns,
0:       Segment segment, String shardName, boolean isFineGrain) {
1:     super(tablePath, dataMapName, indexColumns, segment, shardName);
1:   public void onBlockStart(String blockId) throws IOException {
0:     if (indexWriter != null) {
0:       return;
1:     }
0:     // get index path, put index data into segment's path
1:     Path indexPath = FileFactory.getPath(dataMapPath);
0:     FileSystem fs = FileFactory.getFileSystem(indexPath);
1: 
0:     // if index path not exists, create it
0:     if (!fs.exists(indexPath)) {
1:       if (!fs.mkdirs(indexPath)) {
1:         throw new IOException("Failed to create directory " + dataMapPath);
0:     if (null == analyzer) {
0:       analyzer = new StandardAnalyzer();
1:     }
1: 
0:     // the indexWriter closes the FileSystem on closing the writer, so for a new configuration
0:     // and disable the cache for the index writer, it will be closed on closing the writer
0:     Configuration conf = new Configuration();
0:     conf.set("fs.hdfs.impl.disable.cache", "true");
1: 
0:     // create a index writer
0:     Directory indexDir = new HdfsDirectory(indexPath, conf);
1: 
0:     IndexWriterConfig indexWriterConfig = new IndexWriterConfig(analyzer);
0:     if (CarbonProperties.getInstance()
0:         .getProperty(CarbonCommonConstants.CARBON_LUCENE_COMPRESSION_MODE,
0:             CarbonCommonConstants.CARBON_LUCENE_COMPRESSION_MODE_DEFAULT)
0:         .equalsIgnoreCase(CarbonCommonConstants.CARBON_LUCENE_COMPRESSION_MODE_DEFAULT)) {
0:       indexWriterConfig.setCodec(new Lucene62Codec(Lucene50StoredFieldsFormat.Mode.BEST_SPEED));
0:     } else {
0:       indexWriterConfig
0:           .setCodec(new Lucene62Codec(Lucene50StoredFieldsFormat.Mode.BEST_COMPRESSION));
1:     }
1: 
0:     indexWriter = new IndexWriter(indexDir, new IndexWriterConfig(analyzer));
/////////////////////////////////////////////////////////////////////////
0:   private RAMDirectory ramDir;
0:   private IndexWriter ramIndexWriter;
1: 
0:   public void onBlockletStart(int blockletId) throws IOException {
0:     // save index data into ram, write into disk after one page finished
0:     ramDir = new RAMDirectory();
0:     ramIndexWriter = new IndexWriter(ramDir, new IndexWriterConfig(analyzer));
1:   public void onBlockletEnd(int blockletId) throws IOException {
1:     // close ram writer
1:     ramIndexWriter.close();
1:     // add ram index data into disk
1:     indexWriter.addIndexes(ramDir);
1: 
1:     // delete this ram data
1:     ramDir.close();
/////////////////////////////////////////////////////////////////////////
1:   public void onPageAdded(int blockletId, int pageId, int pageSize, ColumnPage[] pages)
1:       throws IOException {
0:       doc.add(new IntPoint(BLOCKLETID_NAME, blockletId));
0:         doc.add(new IntPoint(PAGEID_NAME, pageId));
0:         doc.add(new IntPoint(ROWID_NAME, rowId));
0:       List<CarbonColumn> indexColumns = getIndexColumns();
0:       for (int i = 0; i < pages.length; i++) {
0:         // add to lucene only if value is not null
0:         if (!pages[i].getNullBits().get(rowId)) {
0:           addField(doc, pages[i].getData(rowId), indexColumns.get(i), Field.Store.NO);
0:   private boolean addField(Document doc, Object data, CarbonColumn column, Field.Store store) {
0:     String fieldName = column.getColName();
0:     DataType type = column.getDataType();
0:       byte value = (byte) data;
/////////////////////////////////////////////////////////////////////////
0:       short value = (short) data;
/////////////////////////////////////////////////////////////////////////
0:       int value = (int) data;
0:       doc.add(new IntPoint(fieldName, value));
/////////////////////////////////////////////////////////////////////////
0:       long value = (long) data;
0:       doc.add(new LongPoint(fieldName, value));
0:       float value = (float) data;
1:       doc.add(new FloatPoint(fieldName, value));
0:       double value = (double) data;
1:       doc.add(new DoublePoint(fieldName, value));
0:       byte[] value = (byte[]) data;
/////////////////////////////////////////////////////////////////////////
1:       throw new RuntimeException("unsupported data type " + type);
1:       throw new RuntimeException("unsupported data type " + type);
0:       boolean value = (boolean) data;
/////////////////////////////////////////////////////////////////////////
commit:6bef57b
/////////////////////////////////////////////////////////////////////////
1: 
/////////////////////////////////////////////////////////////////////////
1:       // add indexed columns value into the document
0:       for (ColumnPage page : pages) {
0:         if (!page.getNullBits().get(rowId)) {
0:           addField(doc, page, rowId, Field.Store.NO);
commit:bbb1092
/////////////////////////////////////////////////////////////////////////
1: /*
1:  * Licensed to the Apache Software Foundation (ASF) under one or more
1:  * contributor license agreements.  See the NOTICE file distributed with
1:  * this work for additional information regarding copyright ownership.
1:  * The ASF licenses this file to You under the Apache License, Version 2.0
1:  * (the "License"); you may not use this file except in compliance with
1:  * the License.  You may obtain a copy of the License at
1:  *
1:  *    http://www.apache.org/licenses/LICENSE-2.0
1:  *
1:  * Unless required by applicable law or agreed to in writing, software
1:  * distributed under the License is distributed on an "AS IS" BASIS,
1:  * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
1:  * See the License for the specific language governing permissions and
1:  * limitations under the License.
1:  */
1: 
1: package org.apache.carbondata.datamap.lucene;
1: 
0: import java.io.File;
1: import java.io.IOException;
1: import java.io.UnsupportedEncodingException;
1: 
1: import org.apache.carbondata.common.annotations.InterfaceAudience;
1: import org.apache.carbondata.common.logging.LogService;
1: import org.apache.carbondata.common.logging.LogServiceFactory;
1: import org.apache.carbondata.core.datamap.dev.DataMapWriter;
1: import org.apache.carbondata.core.datastore.impl.FileFactory;
1: import org.apache.carbondata.core.datastore.page.ColumnPage;
0: import org.apache.carbondata.core.metadata.AbsoluteTableIdentifier;
1: import org.apache.carbondata.core.metadata.datatype.DataType;
1: import org.apache.carbondata.core.metadata.datatype.DataTypes;
0: import org.apache.carbondata.core.util.path.CarbonTablePath;
1: 
1: import org.apache.hadoop.fs.FileSystem;
1: import org.apache.hadoop.fs.Path;
1: import org.apache.lucene.analysis.Analyzer;
1: import org.apache.lucene.analysis.standard.StandardAnalyzer;
1: import org.apache.lucene.document.Document;
1: import org.apache.lucene.document.DoublePoint;
1: import org.apache.lucene.document.Field;
1: import org.apache.lucene.document.FloatPoint;
1: import org.apache.lucene.document.IntPoint;
1: import org.apache.lucene.document.IntRangeField;
1: import org.apache.lucene.document.LongPoint;
1: import org.apache.lucene.document.StoredField;
0: import org.apache.lucene.document.StringField;
1: import org.apache.lucene.document.TextField;
1: import org.apache.lucene.index.IndexWriter;
1: import org.apache.lucene.index.IndexWriterConfig;
1: import org.apache.lucene.store.Directory;
1: import org.apache.lucene.store.RAMDirectory;
1: import org.apache.solr.store.hdfs.HdfsDirectory;
1: 
1: /**
1:  * Implementation to write lucene index while loading
1:  */
1: @InterfaceAudience.Internal
1: public class LuceneDataMapWriter extends DataMapWriter {
1:   /**
1:    * logger
1:    */
1:   private static final LogService LOGGER =
1:       LogServiceFactory.getLogService(LuceneDataMapWriter.class.getName());
1: 
1:   /**
1:    * index writer
1:    */
1:   private IndexWriter indexWriter = null;
1: 
1:   private Analyzer analyzer = null;
1: 
0:   private String blockId = null;
1: 
0:   private String dataMapName = null;
1: 
0:   private boolean isFineGrain = true;
1: 
0:   private static final String BLOCKID_NAME = "blockId";
1: 
0:   private static final String BLOCKLETID_NAME = "blockletId";
1: 
0:   private static final String PAGEID_NAME = "pageId";
1: 
0:   private static final String ROWID_NAME = "rowId";
1: 
0:   LuceneDataMapWriter(AbsoluteTableIdentifier identifier, String dataMapName, String segmentId,
0:       String writeDirectoryPath, boolean isFineGrain) {
0:     super(identifier, segmentId, writeDirectoryPath);
0:     this.dataMapName = dataMapName;
0:     this.isFineGrain = isFineGrain;
1:   }
1: 
0:   private String getIndexPath() {
0:     if (isFineGrain) {
0:       return genDataMapStorePath(identifier.getTablePath(), segmentId, dataMapName);
0:     } else {
0:       // TODO: where write data in coarse grain data map
0:       return genDataMapStorePath(identifier.getTablePath(), segmentId, dataMapName);
1:     }
1:   }
1: 
1:   /**
1:    * Start of new block notification.
1:    */
0:   public void onBlockStart(String blockId) throws IOException {
0:     // save this block id for lucene index , used in onPageAdd function
0:     this.blockId = blockId;
1: 
0:     // get index path, put index data into segment's path
0:     String strIndexPath = getIndexPath();
0:     Path indexPath = FileFactory.getPath(strIndexPath);
0:     FileSystem fs = FileFactory.getFileSystem(indexPath);
1: 
0:     // if index path not exists, create it
0:     if (fs.exists(indexPath)) {
0:       fs.mkdirs(indexPath);
1:     }
1: 
0:     if (null == analyzer) {
0:       analyzer = new StandardAnalyzer();
1:     }
1: 
0:     // create a index writer
0:     Directory indexDir = new HdfsDirectory(indexPath, FileFactory.getConfiguration());
0:     indexWriter = new IndexWriter(indexDir, new IndexWriterConfig(analyzer));
1: 
1:   }
1: 
1:   /**
0:    * End of block notification
1:    */
0:   public void onBlockEnd(String blockId) throws IOException {
0:     // clean this block id
0:     this.blockId = null;
1: 
0:     // finished a file , close this index writer
0:     if (indexWriter != null) {
0:       indexWriter.close();
1:     }
1: 
1:   }
1: 
1:   /**
0:    * Start of new blocklet notification.
1:    */
0:   public void onBlockletStart(int blockletId) {
1: 
1:   }
1: 
1:   /**
1:    * End of blocklet notification
1:    */
0:   public void onBlockletEnd(int blockletId) {
1: 
1:   }
1: 
1:   /**
1:    * Add the column pages row to the datamap, order of pages is same as `indexColumns` in
1:    * DataMapMeta returned in DataMapFactory.
1:    * Implementation should copy the content of `pages` as needed, because `pages` memory
1:    * may be freed after this method returns, if using unsafe column page.
1:    */
0:   public void onPageAdded(int blockletId, int pageId, ColumnPage[] pages) throws IOException {
0:     // save index data into ram, write into disk after one page finished
0:     RAMDirectory ramDir = new RAMDirectory();
0:     IndexWriter ramIndexWriter = new IndexWriter(ramDir, new IndexWriterConfig(analyzer));
1: 
0:     int columnsCount = pages.length;
0:     if (columnsCount <= 0) {
0:       LOGGER.warn("empty data");
0:       ramIndexWriter.close();
0:       ramDir.close();
0:       return;
1:     }
0:     int pageSize = pages[0].getPageSize();
1:     for (int rowId = 0; rowId < pageSize; rowId++) {
0:       // create a new document
0:       Document doc = new Document();
1: 
0:       // add block id, save this id
0:       doc.add(new StringField(BLOCKID_NAME, blockId, Field.Store.YES));
1: 
0:       // add blocklet Id
0:       doc.add(new IntPoint(BLOCKLETID_NAME, new int[] { blockletId }));
0:       doc.add(new StoredField(BLOCKLETID_NAME, blockletId));
0:       //doc.add(new NumericDocValuesField(BLOCKLETID_NAME,blockletId));
1: 
0:       // add page id and row id in Fine Grain data map
0:       if (isFineGrain) {
0:         // add page Id
0:         doc.add(new IntPoint(PAGEID_NAME, new int[] { pageId }));
0:         doc.add(new StoredField(PAGEID_NAME, pageId));
0:         //doc.add(new NumericDocValuesField(PAGEID_NAME,pageId));
1: 
0:         // add row id
0:         doc.add(new IntPoint(ROWID_NAME, new int[] { rowId }));
0:         doc.add(new StoredField(ROWID_NAME, rowId));
0:         //doc.add(new NumericDocValuesField(ROWID_NAME,rowId));
1:       }
1: 
0:       // add other fields
0:       for (int colIdx = 0; colIdx < columnsCount; colIdx++) {
0:         if (!pages[colIdx].getNullBits().get(rowId)) {
0:           addField(doc, pages[colIdx], rowId, Field.Store.NO);
1:         }
1:       }
1: 
0:       // add this document
0:       ramIndexWriter.addDocument(doc);
1: 
1:     }
0:     // close ram writer
0:     ramIndexWriter.close();
1: 
0:     // add ram index data into disk
0:     indexWriter.addIndexes(new Directory[] { ramDir });
1: 
0:     // delete this ram data
0:     ramDir.close();
1:   }
1: 
0:   private boolean addField(Document doc, ColumnPage page, int rowId, Field.Store store) {
1:     //get field name
0:     String fieldName = page.getColumnSpec().getFieldName();
1: 
0:     //get field type
0:     DataType type = page.getDataType();
1: 
0:     if (type == DataTypes.BYTE) {
1:       // byte type , use int range to deal with byte, lucene has no byte type
0:       byte value = page.getByte(rowId);
1:       IntRangeField field =
1:           new IntRangeField(fieldName, new int[] { Byte.MIN_VALUE }, new int[] { Byte.MAX_VALUE });
1:       field.setIntValue(value);
1:       doc.add(field);
1: 
1:       // if need store it , add StoredField
1:       if (store == Field.Store.YES) {
1:         doc.add(new StoredField(fieldName, (int) value));
1:       }
0:     } else if (type == DataTypes.SHORT) {
1:       // short type , use int range to deal with short type, lucene has no short type
0:       short value = page.getShort(rowId);
1:       IntRangeField field = new IntRangeField(fieldName, new int[] { Short.MIN_VALUE },
1:           new int[] { Short.MAX_VALUE });
1:       field.setShortValue(value);
1:       doc.add(field);
1: 
1:       // if need store it , add StoredField
1:       if (store == Field.Store.YES) {
1:         doc.add(new StoredField(fieldName, (int) value));
1:       }
0:     } else if (type == DataTypes.INT) {
1:       // int type , use int point to deal with int type
0:       int value = page.getInt(rowId);
0:       doc.add(new IntPoint(fieldName, new int[] { value }));
1: 
1:       // if need store it , add StoredField
1:       if (store == Field.Store.YES) {
1:         doc.add(new StoredField(fieldName, value));
1:       }
0:     } else if (type == DataTypes.LONG) {
1:       // long type , use long point to deal with long type
0:       long value = page.getLong(rowId);
0:       doc.add(new LongPoint(fieldName, new long[] { value }));
1: 
1:       // if need store it , add StoredField
1:       if (store == Field.Store.YES) {
1:         doc.add(new StoredField(fieldName, value));
1:       }
0:     } else if (type == DataTypes.FLOAT) {
0:       float value = page.getFloat(rowId);
0:       doc.add(new FloatPoint(fieldName, new float[] { value }));
1:       if (store == Field.Store.YES) {
0:         doc.add(new FloatPoint(fieldName, value));
1:       }
0:     } else if (type == DataTypes.DOUBLE) {
0:       double value = page.getDouble(rowId);
0:       doc.add(new DoublePoint(fieldName, new double[] { value }));
1:       if (store == Field.Store.YES) {
0:         doc.add(new DoublePoint(fieldName, value));
1:       }
0:     } else if (type == DataTypes.STRING) {
0:       byte[] value = page.getBytes(rowId);
0:       // TODO: how to get string value
0:       String strValue = null;
0:       try {
0:         strValue = new String(value, 2, value.length - 2, "UTF-8");
0:       } catch (UnsupportedEncodingException e) {
0:         throw new RuntimeException(e);
1:       }
1:       doc.add(new TextField(fieldName, strValue, store));
0:     } else if (type == DataTypes.DATE) {
0:       // TODO: how to get data value
0:     } else if (type == DataTypes.TIMESTAMP) {
0:       // TODO: how to get
0:     } else if (type == DataTypes.BOOLEAN) {
0:       boolean value = page.getBoolean(rowId);
1:       IntRangeField field = new IntRangeField(fieldName, new int[] { 0 }, new int[] { 1 });
1:       field.setIntValue(value ? 1 : 0);
1:       doc.add(field);
1:       if (store == Field.Store.YES) {
1:         doc.add(new StoredField(fieldName, value ? 1 : 0));
1:       }
0:     } else {
1:       LOGGER.error("unsupport data type " + type);
0:       throw new RuntimeException("unsupported data type " + type);
1:     }
0:     return true;
1:   }
1: 
1:   /**
1:    * This is called during closing of writer.So after this call no more data will be sent to this
1:    * class.
1:    */
1:   public void finish() throws IOException {
1: 
1:   }
1: 
1:   /**
0:    * Return store path for datamap
1:    */
0:   static String genDataMapStorePath(String tablePath, String segmentId, String dataMapName) {
0:     return CarbonTablePath.getSegmentPath(tablePath, segmentId) + File.separator + dataMapName;
1:   }
1: 
1: }
author:QiangCai
-------------------------------------------------------------------------------
commit:21c5fb1
/////////////////////////////////////////////////////////////////////////
0:   public static final String BLOCKLETID_NAME = "blockletId";
1:   public static final String PAGEID_NAME = "pageId";
1:   public static final String ROWID_NAME = "rowId";
/////////////////////////////////////////////////////////////////////////
0:     if (indexWriter != null) {
0:       return;
0:     }
/////////////////////////////////////////////////////////////////////////
0:   public static String genDataMapStorePathOnTaskId(String tablePath, String segmentId,
author:xuchuanyin
-------------------------------------------------------------------------------
commit:ecd6c0c
/////////////////////////////////////////////////////////////////////////
0:     if (!fs.exists(indexPath)) {
commit:5397c05
/////////////////////////////////////////////////////////////////////////
1: import org.apache.carbondata.core.datamap.Segment;
/////////////////////////////////////////////////////////////////////////
0:   LuceneDataMapWriter(AbsoluteTableIdentifier identifier, String dataMapName, Segment segment,
0:     super(identifier, segment, writeDirectoryPath);
============================================================================