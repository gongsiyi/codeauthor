1:d7393da: /*
1:d7393da:  * Licensed to the Apache Software Foundation (ASF) under one or more
1:d7393da:  * contributor license agreements.  See the NOTICE file distributed with
1:d7393da:  * this work for additional information regarding copyright ownership.
1:d7393da:  * The ASF licenses this file to You under the Apache License, Version 2.0
1:d7393da:  * (the "License"); you may not use this file except in compliance with
1:d7393da:  * the License.  You may obtain a copy of the License at
1:d7393da:  *
1:d7393da:  *    http://www.apache.org/licenses/LICENSE-2.0
1:d7393da:  *
1:d7393da:  * Unless required by applicable law or agreed to in writing, software
1:d7393da:  * distributed under the License is distributed on an "AS IS" BASIS,
1:d7393da:  * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
1:d7393da:  * See the License for the specific language governing permissions and
1:d7393da:  * limitations under the License.
1:d7393da:  */
7:d7393da: 
1:c723947: package org.apache.carbondata.streaming;
1:d7393da: 
1:d7393da: import java.io.DataOutputStream;
1:d7393da: import java.io.File;
1:d7393da: import java.io.IOException;
1:d7393da: import java.math.BigDecimal;
1:d7393da: import java.util.ArrayList;
1:d7393da: import java.util.BitSet;
1:d7393da: import java.util.List;
1:d7393da: 
1:d7393da: import org.apache.carbondata.common.logging.LogService;
1:d7393da: import org.apache.carbondata.common.logging.LogServiceFactory;
1:8f08c4a: import org.apache.carbondata.core.constants.CarbonCommonConstants;
1:8f08c4a: import org.apache.carbondata.core.datastore.compression.CompressorFactory;
1:d7393da: import org.apache.carbondata.core.datastore.filesystem.CarbonFile;
1:d7393da: import org.apache.carbondata.core.datastore.impl.FileFactory;
1:d7393da: import org.apache.carbondata.core.datastore.row.CarbonRow;
1:21a72bf: import org.apache.carbondata.core.metadata.blocklet.index.BlockletMinMaxIndex;
1:933e30c: import org.apache.carbondata.core.metadata.datatype.DataType;
1:d7393da: import org.apache.carbondata.core.metadata.datatype.DataTypes;
1:d7393da: import org.apache.carbondata.core.metadata.schema.table.CarbonTable;
1:d7393da: import org.apache.carbondata.core.metadata.schema.table.column.ColumnSchema;
1:8f08c4a: import org.apache.carbondata.core.reader.CarbonHeaderReader;
1:21a72bf: import org.apache.carbondata.core.util.ByteUtil;
1:d7393da: import org.apache.carbondata.core.util.CarbonMetadataUtil;
1:d7393da: import org.apache.carbondata.core.util.CarbonUtil;
1:d7393da: import org.apache.carbondata.core.util.DataTypeUtil;
1:d7393da: import org.apache.carbondata.core.util.path.CarbonTablePath;
1:d7393da: import org.apache.carbondata.format.FileHeader;
1:d7393da: import org.apache.carbondata.processing.loading.BadRecordsLogger;
1:837fdd2: import org.apache.carbondata.processing.loading.BadRecordsLoggerProvider;
1:d7393da: import org.apache.carbondata.processing.loading.CarbonDataLoadConfiguration;
1:d7393da: import org.apache.carbondata.processing.loading.DataField;
1:d7393da: import org.apache.carbondata.processing.loading.DataLoadProcessBuilder;
1:d7393da: import org.apache.carbondata.processing.loading.converter.RowConverter;
1:d7393da: import org.apache.carbondata.processing.loading.converter.impl.RowConverterImpl;
1:d7393da: import org.apache.carbondata.processing.loading.model.CarbonLoadModel;
1:d7393da: import org.apache.carbondata.processing.loading.parser.RowParser;
1:d7393da: import org.apache.carbondata.processing.loading.parser.impl.RowParserImpl;
1:d7393da: import org.apache.carbondata.processing.store.writer.AbstractFactDataWriter;
1:d7393da: import org.apache.carbondata.processing.util.CarbonDataProcessorUtil;
1:21a72bf: import org.apache.carbondata.streaming.segment.StreamSegment;
1:d7393da: 
1:d7393da: import org.apache.hadoop.conf.Configuration;
1:d7393da: import org.apache.hadoop.mapreduce.RecordWriter;
1:d7393da: import org.apache.hadoop.mapreduce.TaskAttemptContext;
1:d7393da: import org.apache.hadoop.mapreduce.TaskID;
1:d7393da: /**
1:d7393da:  * Stream record writer
1:d7393da:  */
1:d7393da: public class CarbonStreamRecordWriter extends RecordWriter<Void, Object> {
1:d7393da: 
1:d7393da:   private static final LogService LOGGER =
1:d7393da:       LogServiceFactory.getLogService(CarbonStreamRecordWriter.class.getName());
1:d7393da: 
1:d7393da:   // basic info
1:d7393da:   private Configuration hadoopConf;
1:e7f9422:   private CarbonLoadModel carbonLoadModel;
1:d7393da:   private CarbonDataLoadConfiguration configuration;
1:d7393da:   private CarbonTable carbonTable;
1:d7393da:   private int maxRowNums;
1:d7393da:   private int maxCacheSize;
1:d7393da: 
1:d7393da:   // parser and converter
1:d7393da:   private RowParser rowParser;
1:40c31e8:   private BadRecordsLogger badRecordLogger;
1:d7393da:   private RowConverter converter;
1:d7393da:   private CarbonRow currentRow = new CarbonRow(null);
1:d7393da: 
1:d7393da:   // encoder
1:d7393da:   private DataField[] dataFields;
1:d7393da:   private BitSet nullBitSet;
1:d7393da:   private boolean[] isNoDictionaryDimensionColumn;
1:d7393da:   private int dimensionWithComplexCount;
1:d7393da:   private int measureCount;
1:933e30c:   private DataType[] measureDataTypes;
1:d7393da:   private StreamBlockletWriter output = null;
1:8f08c4a:   private String compressorName;
1:d7393da: 
1:d7393da:   // data write
1:d7393da:   private String segmentDir;
1:d7393da:   private String fileName;
1:d7393da:   private DataOutputStream outputStream;
1:d7393da:   private boolean isFirstRow = true;
1:d7393da:   private boolean hasException = false;
1:d7393da: 
1:21a72bf:   // batch level stats collector
1:21a72bf:   private BlockletMinMaxIndex batchMinMaxIndex;
1:21a72bf:   private boolean isClosed = false;
1:21a72bf: 
1:d7393da:   CarbonStreamRecordWriter(TaskAttemptContext job) throws IOException {
1:d7393da:     initialize(job);
1:d7393da:   }
1:d7393da: 
1:e7f9422:   public CarbonStreamRecordWriter(TaskAttemptContext job, CarbonLoadModel carbonLoadModel)
1:e7f9422:       throws IOException {
1:e7f9422:     this.carbonLoadModel = carbonLoadModel;
1:e7f9422:     initialize(job);
1:e7f9422:   }
1:e7f9422: 
1:d7393da:   private void initialize(TaskAttemptContext job) throws IOException {
1:d7393da:     // set basic information
1:d7393da:     hadoopConf = job.getConfiguration();
1:d7393da:     if (carbonLoadModel == null) {
1:e7f9422:       carbonLoadModel = CarbonStreamOutputFormat.getCarbonLoadModel(hadoopConf);
1:e7f9422:       if (carbonLoadModel == null) {
1:e7f9422:         throw new IOException(
1:e7f9422:             "CarbonStreamRecordWriter require configuration: mapreduce.output.carbon.load.model");
1:e7f9422:       }
1:d7393da:     }
1:40c31e8:     String segmentId = CarbonStreamOutputFormat.getSegmentId(hadoopConf);
1:40c31e8:     carbonLoadModel.setSegmentId(segmentId);
1:d7393da:     carbonTable = carbonLoadModel.getCarbonDataLoadSchema().getCarbonTable();
1:4430178:     long taskNo = TaskID.forName(hadoopConf.get("mapred.tip.id")).getId();
1:d7393da:     carbonLoadModel.setTaskNo("" + taskNo);
1:d7393da:     configuration = DataLoadProcessBuilder.createConfiguration(carbonLoadModel);
1:d7393da:     maxRowNums = hadoopConf.getInt(CarbonStreamOutputFormat.CARBON_STREAM_BLOCKLET_ROW_NUMS,
1:d7393da:         CarbonStreamOutputFormat.CARBON_STREAM_BLOCKLET_ROW_NUMS_DEFAULT) - 1;
1:d7393da:     maxCacheSize = hadoopConf.getInt(CarbonStreamOutputFormat.CARBON_STREAM_CACHE_SIZE,
1:d7393da:         CarbonStreamOutputFormat.CARBON_STREAM_CACHE_SIZE_DEFAULT);
1:d7393da: 
1:bf6c471:     segmentDir = CarbonTablePath.getSegmentPath(
1:bf6c471:         carbonTable.getAbsoluteTableIdentifier().getTablePath(), segmentId);
1:60dfdd3:     fileName = CarbonTablePath.getCarbonDataFileName(0, taskNo, 0, 0, "0", segmentId);
1:d7393da:   }
1:d7393da: 
1:d7393da:   private void initializeAtFirstRow() throws IOException, InterruptedException {
1:d7393da:     // initialize metadata
1:d7393da:     isNoDictionaryDimensionColumn =
1:d7393da:         CarbonDataProcessorUtil.getNoDictionaryMapping(configuration.getDataFields());
1:d7393da:     dimensionWithComplexCount = configuration.getDimensionCount();
1:d7393da:     measureCount = configuration.getMeasureCount();
1:d7393da:     dataFields = configuration.getDataFields();
1:933e30c:     measureDataTypes = new DataType[measureCount];
1:d7393da:     for (int i = 0; i < measureCount; i++) {
1:d7393da:       measureDataTypes[i] =
1:933e30c:           dataFields[dimensionWithComplexCount + i].getColumn().getDataType();
1:d7393da:     }
1:d7393da:     // initialize parser and converter
1:d7393da:     rowParser = new RowParserImpl(dataFields, configuration);
1:837fdd2:     badRecordLogger = BadRecordsLoggerProvider.createBadRecordLogger(configuration);
1:d7393da:     converter = new RowConverterImpl(configuration.getDataFields(), configuration, badRecordLogger);
1:d7393da:     configuration.setCardinalityFinder(converter);
1:d7393da:     converter.initialize();
1:d7393da: 
1:8f08c4a:     // initialize data writer and compressor
1:d7393da:     String filePath = segmentDir + File.separator + fileName;
1:d7393da:     FileFactory.FileType fileType = FileFactory.getFileType(filePath);
1:d7393da:     CarbonFile carbonFile = FileFactory.getCarbonFile(filePath, fileType);
1:d7393da:     if (carbonFile.exists()) {
1:d7393da:       // if the file is existed, use the append api
1:d7393da:       outputStream = FileFactory.getDataOutputStreamUsingAppend(filePath, fileType);
1:8f08c4a:       // get the compressor from the fileheader. In legacy store,
1:8f08c4a:       // the compressor name is not set and it use snappy compressor
1:8f08c4a:       FileHeader header = new CarbonHeaderReader(filePath).readHeader();
1:8f08c4a:       if (header.isSetCompressor_name()) {
1:8f08c4a:         compressorName = header.getCompressor_name();
1:d7393da:       } else {
1:8f08c4a:         compressorName = CompressorFactory.SupportedCompressor.SNAPPY.getName();
1:8f08c4a:       }
1:8f08c4a:     } else {
1:d7393da:       // IF the file is not existed, use the create api
1:d7393da:       outputStream = FileFactory.getDataOutputStream(filePath, fileType);
1:8f08c4a:       compressorName = carbonTable.getTableInfo().getFactTable().getTableProperties().get(
1:8f08c4a:           CarbonCommonConstants.COMPRESSOR);
1:8f08c4a:       if (null == compressorName) {
1:8f08c4a:         compressorName = CompressorFactory.getInstance().getCompressor().getName();
1:8f08c4a:       }
1:d7393da:       writeFileHeader();
1:d7393da:     }
1:d9bb647: 
1:d7393da:     // initialize encoder
1:d7393da:     nullBitSet = new BitSet(dataFields.length);
1:d7393da:     int rowBufferSize = hadoopConf.getInt(CarbonStreamOutputFormat.CARBON_ENCODER_ROW_BUFFER_SIZE,
1:d7393da:         CarbonStreamOutputFormat.CARBON_ENCODER_ROW_BUFFER_SIZE_DEFAULT);
1:21a72bf:     output = new StreamBlockletWriter(maxCacheSize, maxRowNums, rowBufferSize,
1:21a72bf:         isNoDictionaryDimensionColumn.length, measureCount,
1:8f08c4a:         measureDataTypes, compressorName);
1:d7393da: 
1:d9bb647:     isFirstRow = false;
1:d7393da:   }
1:d7393da: 
1:d7393da:   @Override public void write(Void key, Object value) throws IOException, InterruptedException {
1:d7393da:     if (isFirstRow) {
1:d7393da:       initializeAtFirstRow();
1:d7393da:     }
1:d7393da:     // null bit set
1:d7393da:     nullBitSet.clear();
1:5969312:     Object[] rowData = (Object[]) value;
1:5969312:     currentRow.setRawData(rowData);
1:d7393da:     // parse and convert row
1:5969312:     currentRow.setData(rowParser.parseRow(rowData));
1:5969312:     CarbonRow updatedCarbonRow = converter.convert(currentRow);
1:5969312:     if (updatedCarbonRow == null) {
1:5969312:       output.skipRow();
1:5969312:       currentRow.clearData();
3:5969312:     } else {
1:d7393da:       for (int i = 0; i < dataFields.length; i++) {
1:d7393da:         if (null == currentRow.getObject(i)) {
1:d7393da:           nullBitSet.set(i);
1:d7393da:         }
1:d7393da:       }
1:d7393da:       output.nextRow();
1:d7393da:       byte[] b = nullBitSet.toByteArray();
1:d7393da:       output.writeShort(b.length);
1:d7393da:       if (b.length > 0) {
1:d7393da:         output.writeBytes(b);
1:d7393da:       }
1:d7393da:       int dimCount = 0;
1:d7393da:       Object columnValue;
1:d7393da:       // primitive type dimension
1:d7393da:       for (; dimCount < isNoDictionaryDimensionColumn.length; dimCount++) {
1:d7393da:         columnValue = currentRow.getObject(dimCount);
1:d7393da:         if (null != columnValue) {
1:d7393da:           if (isNoDictionaryDimensionColumn[dimCount]) {
1:d7393da:             byte[] col = (byte[]) columnValue;
1:d7393da:             output.writeShort(col.length);
1:d7393da:             output.writeBytes(col);
1:21a72bf:             output.dimStatsCollectors[dimCount].update(col);
1:d7393da:           } else {
1:d7393da:             output.writeInt((int) columnValue);
1:21a72bf:             output.dimStatsCollectors[dimCount].update(ByteUtil.toBytes((int) columnValue));
1:d7393da:           }
1:21a72bf:         } else {
1:21a72bf:           output.dimStatsCollectors[dimCount].updateNull(0);
1:d7393da:         }
1:d7393da:       }
1:d7393da:       // complex type dimension
1:d7393da:       for (; dimCount < dimensionWithComplexCount; dimCount++) {
1:d7393da:         columnValue = currentRow.getObject(dimCount);
1:d7393da:         if (null != columnValue) {
1:d7393da:           byte[] col = (byte[]) columnValue;
1:d7393da:           output.writeShort(col.length);
1:d7393da:           output.writeBytes(col);
1:d7393da:         }
1:d7393da:       }
1:d7393da:       // measure
1:5969312:       DataType dataType;
1:d7393da:       for (int msrCount = 0; msrCount < measureCount; msrCount++) {
1:d7393da:         columnValue = currentRow.getObject(dimCount + msrCount);
1:d7393da:         if (null != columnValue) {
1:d7393da:           dataType = measureDataTypes[msrCount];
1:5969312:           if (dataType == DataTypes.BOOLEAN) {
1:d7393da:             output.writeBoolean((boolean) columnValue);
1:21a72bf:             output.msrStatsCollectors[msrCount].update((byte) ((boolean) columnValue ? 1 : 0));
1:5969312:           } else if (dataType == DataTypes.SHORT) {
1:d7393da:             output.writeShort((short) columnValue);
1:21a72bf:             output.msrStatsCollectors[msrCount].update((short) columnValue);
1:5969312:           } else if (dataType == DataTypes.INT) {
1:d7393da:             output.writeInt((int) columnValue);
1:21a72bf:             output.msrStatsCollectors[msrCount].update((int) columnValue);
1:5969312:           } else if (dataType == DataTypes.LONG) {
1:d7393da:             output.writeLong((long) columnValue);
1:21a72bf:             output.msrStatsCollectors[msrCount].update((long) columnValue);
1:5969312:           } else if (dataType == DataTypes.DOUBLE) {
1:d7393da:             output.writeDouble((double) columnValue);
1:21a72bf:             output.msrStatsCollectors[msrCount].update((double) columnValue);
1:5969312:           } else if (DataTypes.isDecimal(dataType)) {
1:d7393da:             BigDecimal val = (BigDecimal) columnValue;
1:d7393da:             byte[] bigDecimalInBytes = DataTypeUtil.bigDecimalToByte(val);
1:d7393da:             output.writeShort(bigDecimalInBytes.length);
1:d7393da:             output.writeBytes(bigDecimalInBytes);
1:21a72bf:             output.msrStatsCollectors[msrCount].update((BigDecimal) columnValue);
1:d7393da:           } else {
1:d7393da:             String msg =
1:d7393da:                 "unsupported data type:" + dataFields[dimCount + msrCount].getColumn().getDataType()
1:d7393da:                 .getName();
1:d7393da:             LOGGER.error(msg);
1:d7393da:             throw new IOException(msg);
1:d7393da:           }
1:21a72bf:         } else {
1:21a72bf:           output.msrStatsCollectors[msrCount].updateNull(0);
1:d7393da:         }
1:d7393da:       }
6:5969312:     }
1:d7393da: 
1:d7393da:     if (output.isFull()) {
1:d7393da:       appendBlockletToDataFile();
1:d7393da:     }
1:d7393da:   }
1:d7393da: 
1:d7393da:   private void writeFileHeader() throws IOException {
1:d7393da:     List<ColumnSchema> wrapperColumnSchemaList = CarbonUtil
1:5fc7f06:         .getColumnSchemaList(carbonTable.getDimensionByTableName(carbonTable.getTableName()),
1:5fc7f06:             carbonTable.getMeasureByTableName(carbonTable.getTableName()));
1:d7393da:     int[] dimLensWithComplex = new int[wrapperColumnSchemaList.size()];
1:d7393da:     for (int i = 0; i < dimLensWithComplex.length; i++) {
1:d7393da:       dimLensWithComplex[i] = Integer.MAX_VALUE;
1:d7393da:     }
1:d7393da:     int[] dictionaryColumnCardinality =
1:d7393da:         CarbonUtil.getFormattedCardinality(dimLensWithComplex, wrapperColumnSchemaList);
1:d7393da:     List<Integer> cardinality = new ArrayList<>();
1:d7393da:     List<org.apache.carbondata.format.ColumnSchema> columnSchemaList = AbstractFactDataWriter
1:d7393da:         .getColumnSchemaListAndCardinality(cardinality, dictionaryColumnCardinality,
1:d7393da:             wrapperColumnSchemaList);
1:d7393da:     FileHeader fileHeader =
1:d7393da:         CarbonMetadataUtil.getFileHeader(true, columnSchemaList, System.currentTimeMillis());
1:d7393da:     fileHeader.setIs_footer_present(false);
1:d7393da:     fileHeader.setIs_splitable(true);
1:d7393da:     fileHeader.setSync_marker(CarbonStreamOutputFormat.CARBON_SYNC_MARKER);
1:8f08c4a:     fileHeader.setCompressor_name(compressorName);
1:d7393da:     outputStream.write(CarbonUtil.getByteArray(fileHeader));
1:d7393da:   }
1:d7393da: 
1:d7393da:   /**
1:d7393da:    * write a blocklet to file
1:d7393da:    */
1:d7393da:   private void appendBlockletToDataFile() throws IOException {
1:d7393da:     if (output.getRowIndex() == -1) {
1:d7393da:       return;
1:d7393da:     }
1:d7393da:     output.apppendBlocklet(outputStream);
1:d7393da:     outputStream.flush();
1:21a72bf:     if (!isClosed) {
1:21a72bf:       batchMinMaxIndex = StreamSegment.mergeBlockletMinMax(
1:21a72bf:           batchMinMaxIndex, output.generateBlockletMinMax(), measureDataTypes);
1:21a72bf:     }
1:d7393da:     // reset data
1:d7393da:     output.reset();
1:d7393da:   }
1:d7393da: 
1:21a72bf:   public BlockletMinMaxIndex getBatchMinMaxIndex() {
1:daa91c8:     if (output == null) {
1:daa91c8:       return StreamSegment.mergeBlockletMinMax(
1:daa91c8:           batchMinMaxIndex, null, measureDataTypes);
1:daa91c8:     }
1:21a72bf:     return StreamSegment.mergeBlockletMinMax(
1:21a72bf:         batchMinMaxIndex, output.generateBlockletMinMax(), measureDataTypes);
1:21a72bf:   }
1:21a72bf: 
1:21a72bf:   public DataType[] getMeasureDataTypes() {
1:21a72bf:     return measureDataTypes;
1:21a72bf:   }
1:21a72bf: 
1:21a72bf:   @Override
1:21a72bf:   public void close(TaskAttemptContext context) throws IOException, InterruptedException {
1:d7393da:     try {
1:21a72bf:       isClosed = true;
1:d7393da:       // append remain buffer data
1:40c31e8:       if (!hasException && !isFirstRow) {
1:d7393da:         appendBlockletToDataFile();
1:d7393da:         converter.finish();
1:d7393da:       }
1:d7393da:     } finally {
1:d7393da:       // close resource
1:d7393da:       CarbonUtil.closeStreams(outputStream);
1:40c31e8:       if (output != null) {
1:40c31e8:         output.close();
1:40c31e8:       }
1:40c31e8:       if (badRecordLogger != null) {
1:40c31e8:         badRecordLogger.closeStreams();
1:40c31e8:       }
1:d7393da:     }
1:d7393da:   }
1:d7393da: 
1:d7393da:   public String getSegmentDir() {
1:d7393da:     return segmentDir;
1:d7393da:   }
1:d7393da: 
1:d7393da:   public String getFileName() {
1:d7393da:     return fileName;
1:d7393da:   }
1:bcc9cf0: 
1:bcc9cf0:   public void setHasException(boolean hasException) {
1:bcc9cf0:     this.hasException = hasException;
1:bcc9cf0:   }
1:d7393da: }
============================================================================
author:QiangCai
-------------------------------------------------------------------------------
commit:daa91c8
/////////////////////////////////////////////////////////////////////////
1:     if (output == null) {
1:       return StreamSegment.mergeBlockletMinMax(
1:           batchMinMaxIndex, null, measureDataTypes);
1:     }
commit:21a72bf
/////////////////////////////////////////////////////////////////////////
1: import org.apache.carbondata.core.metadata.blocklet.index.BlockletMinMaxIndex;
1: import org.apache.carbondata.core.util.ByteUtil;
/////////////////////////////////////////////////////////////////////////
1: import org.apache.carbondata.streaming.segment.StreamSegment;
/////////////////////////////////////////////////////////////////////////
1:   // batch level stats collector
1:   private BlockletMinMaxIndex batchMinMaxIndex;
1:   private boolean isClosed = false;
1: 
/////////////////////////////////////////////////////////////////////////
/////////////////////////////////////////////////////////////////////////
1:     output = new StreamBlockletWriter(maxCacheSize, maxRowNums, rowBufferSize,
1:         isNoDictionaryDimensionColumn.length, measureCount,
0:         measureDataTypes);
/////////////////////////////////////////////////////////////////////////
/////////////////////////////////////////////////////////////////////////
/////////////////////////////////////////////////////////////////////////
/////////////////////////////////////////////////////////////////////////
1:             output.dimStatsCollectors[dimCount].update(col);
1:             output.dimStatsCollectors[dimCount].update(ByteUtil.toBytes((int) columnValue));
1:         } else {
1:           output.dimStatsCollectors[dimCount].updateNull(0);
/////////////////////////////////////////////////////////////////////////
1:             output.msrStatsCollectors[msrCount].update((byte) ((boolean) columnValue ? 1 : 0));
1:             output.msrStatsCollectors[msrCount].update((short) columnValue);
1:             output.msrStatsCollectors[msrCount].update((int) columnValue);
1:             output.msrStatsCollectors[msrCount].update((long) columnValue);
1:             output.msrStatsCollectors[msrCount].update((double) columnValue);
1:             output.msrStatsCollectors[msrCount].update((BigDecimal) columnValue);
/////////////////////////////////////////////////////////////////////////
1:         } else {
1:           output.msrStatsCollectors[msrCount].updateNull(0);
/////////////////////////////////////////////////////////////////////////
1:     if (!isClosed) {
1:       batchMinMaxIndex = StreamSegment.mergeBlockletMinMax(
1:           batchMinMaxIndex, output.generateBlockletMinMax(), measureDataTypes);
1:     }
1:   public BlockletMinMaxIndex getBatchMinMaxIndex() {
1:     return StreamSegment.mergeBlockletMinMax(
1:         batchMinMaxIndex, output.generateBlockletMinMax(), measureDataTypes);
1:   }
1: 
1:   public DataType[] getMeasureDataTypes() {
1:     return measureDataTypes;
1:   }
1: 
1:   @Override
1:   public void close(TaskAttemptContext context) throws IOException, InterruptedException {
1:       isClosed = true;
commit:d9bb647
/////////////////////////////////////////////////////////////////////////
/////////////////////////////////////////////////////////////////////////
1: 
1:     isFirstRow = false;
commit:bcc9cf0
/////////////////////////////////////////////////////////////////////////
1: 
1:   public void setHasException(boolean hasException) {
1:     this.hasException = hasException;
1:   }
commit:e7f9422
/////////////////////////////////////////////////////////////////////////
1:   private CarbonLoadModel carbonLoadModel;
/////////////////////////////////////////////////////////////////////////
1:   public CarbonStreamRecordWriter(TaskAttemptContext job, CarbonLoadModel carbonLoadModel)
1:       throws IOException {
1:     this.carbonLoadModel = carbonLoadModel;
1:     initialize(job);
1:   }
1: 
1:       carbonLoadModel = CarbonStreamOutputFormat.getCarbonLoadModel(hadoopConf);
1:       if (carbonLoadModel == null) {
1:         throw new IOException(
1:             "CarbonStreamRecordWriter require configuration: mapreduce.output.carbon.load.model");
1:       }
commit:40c31e8
/////////////////////////////////////////////////////////////////////////
1:   private BadRecordsLogger badRecordLogger;
/////////////////////////////////////////////////////////////////////////
1:     String segmentId = CarbonStreamOutputFormat.getSegmentId(hadoopConf);
1:     carbonLoadModel.setSegmentId(segmentId);
/////////////////////////////////////////////////////////////////////////
0:     segmentDir = tablePath.getSegmentDir("0", segmentId);
/////////////////////////////////////////////////////////////////////////
0:     badRecordLogger = DataConverterProcessorStepImpl.createBadRecordLogger(configuration);
/////////////////////////////////////////////////////////////////////////
1:       if (!hasException && !isFirstRow) {
1:       if (output != null) {
1:         output.close();
1:       }
1:       if (badRecordLogger != null) {
1:         badRecordLogger.closeStreams();
1:       }
commit:d7393da
/////////////////////////////////////////////////////////////////////////
1: /*
1:  * Licensed to the Apache Software Foundation (ASF) under one or more
1:  * contributor license agreements.  See the NOTICE file distributed with
1:  * this work for additional information regarding copyright ownership.
1:  * The ASF licenses this file to You under the Apache License, Version 2.0
1:  * (the "License"); you may not use this file except in compliance with
1:  * the License.  You may obtain a copy of the License at
1:  *
1:  *    http://www.apache.org/licenses/LICENSE-2.0
1:  *
1:  * Unless required by applicable law or agreed to in writing, software
1:  * distributed under the License is distributed on an "AS IS" BASIS,
1:  * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
1:  * See the License for the specific language governing permissions and
1:  * limitations under the License.
1:  */
1: 
0: package org.apache.carbondata.hadoop.streaming;
1: 
1: import java.io.DataOutputStream;
1: import java.io.File;
1: import java.io.IOException;
1: import java.math.BigDecimal;
1: import java.util.ArrayList;
1: import java.util.BitSet;
1: import java.util.List;
1: 
1: import org.apache.carbondata.common.logging.LogService;
1: import org.apache.carbondata.common.logging.LogServiceFactory;
1: import org.apache.carbondata.core.datastore.filesystem.CarbonFile;
1: import org.apache.carbondata.core.datastore.impl.FileFactory;
1: import org.apache.carbondata.core.datastore.row.CarbonRow;
1: import org.apache.carbondata.core.metadata.datatype.DataTypes;
1: import org.apache.carbondata.core.metadata.schema.table.CarbonTable;
1: import org.apache.carbondata.core.metadata.schema.table.column.ColumnSchema;
1: import org.apache.carbondata.core.util.CarbonMetadataUtil;
1: import org.apache.carbondata.core.util.CarbonUtil;
1: import org.apache.carbondata.core.util.DataTypeUtil;
0: import org.apache.carbondata.core.util.path.CarbonStorePath;
1: import org.apache.carbondata.core.util.path.CarbonTablePath;
1: import org.apache.carbondata.format.FileHeader;
1: import org.apache.carbondata.processing.loading.BadRecordsLogger;
1: import org.apache.carbondata.processing.loading.CarbonDataLoadConfiguration;
1: import org.apache.carbondata.processing.loading.DataField;
1: import org.apache.carbondata.processing.loading.DataLoadProcessBuilder;
1: import org.apache.carbondata.processing.loading.converter.RowConverter;
1: import org.apache.carbondata.processing.loading.converter.impl.RowConverterImpl;
1: import org.apache.carbondata.processing.loading.model.CarbonLoadModel;
1: import org.apache.carbondata.processing.loading.parser.RowParser;
1: import org.apache.carbondata.processing.loading.parser.impl.RowParserImpl;
0: import org.apache.carbondata.processing.loading.steps.DataConverterProcessorStepImpl;
1: import org.apache.carbondata.processing.store.writer.AbstractFactDataWriter;
1: import org.apache.carbondata.processing.util.CarbonDataProcessorUtil;
1: 
1: import org.apache.hadoop.conf.Configuration;
1: import org.apache.hadoop.mapreduce.RecordWriter;
1: import org.apache.hadoop.mapreduce.TaskAttemptContext;
1: import org.apache.hadoop.mapreduce.TaskID;
1: 
1: /**
1:  * Stream record writer
1:  */
1: public class CarbonStreamRecordWriter extends RecordWriter<Void, Object> {
1: 
1:   private static final LogService LOGGER =
1:       LogServiceFactory.getLogService(CarbonStreamRecordWriter.class.getName());
1: 
1:   // basic info
1:   private Configuration hadoopConf;
1:   private CarbonDataLoadConfiguration configuration;
1:   private CarbonTable carbonTable;
1:   private int maxRowNums;
1:   private int maxCacheSize;
1: 
1:   // parser and converter
1:   private RowParser rowParser;
1:   private RowConverter converter;
1:   private CarbonRow currentRow = new CarbonRow(null);
1: 
1:   // encoder
1:   private DataField[] dataFields;
1:   private BitSet nullBitSet;
1:   private boolean[] isNoDictionaryDimensionColumn;
1:   private int dimensionWithComplexCount;
1:   private int measureCount;
0:   private int[] measureDataTypes;
1:   private StreamBlockletWriter output = null;
1: 
1:   // data write
1:   private String segmentDir;
1:   private String fileName;
1:   private DataOutputStream outputStream;
1:   private boolean isFirstRow = true;
1:   private boolean hasException = false;
1: 
1:   CarbonStreamRecordWriter(TaskAttemptContext job) throws IOException {
1:     initialize(job);
1:   }
1: 
1:   private void initialize(TaskAttemptContext job) throws IOException {
1:     // set basic information
1:     hadoopConf = job.getConfiguration();
0:     CarbonLoadModel carbonLoadModel = CarbonStreamOutputFormat.getCarbonLoadModel(hadoopConf);
1:     if (carbonLoadModel == null) {
0:       throw new IOException(
0:           "CarbonStreamRecordWriter require configuration: mapreduce.output.carbon.load.model");
1:     }
1:     carbonTable = carbonLoadModel.getCarbonDataLoadSchema().getCarbonTable();
0:     int taskNo = TaskID.forName(hadoopConf.get("mapred.tip.id")).getId();
1:     carbonLoadModel.setTaskNo("" + taskNo);
1:     configuration = DataLoadProcessBuilder.createConfiguration(carbonLoadModel);
1:     maxRowNums = hadoopConf.getInt(CarbonStreamOutputFormat.CARBON_STREAM_BLOCKLET_ROW_NUMS,
1:         CarbonStreamOutputFormat.CARBON_STREAM_BLOCKLET_ROW_NUMS_DEFAULT) - 1;
1:     maxCacheSize = hadoopConf.getInt(CarbonStreamOutputFormat.CARBON_STREAM_CACHE_SIZE,
1:         CarbonStreamOutputFormat.CARBON_STREAM_CACHE_SIZE_DEFAULT);
1: 
0:     CarbonTablePath tablePath =
0:         CarbonStorePath.getCarbonTablePath(carbonTable.getAbsoluteTableIdentifier());
0:     segmentDir = tablePath.getSegmentDir("0", carbonLoadModel.getSegmentId());
0:     fileName = CarbonTablePath.getCarbonDataFileName(0, taskNo, 0, 0, "0");
1:   }
1: 
1:   private void initializeAtFirstRow() throws IOException, InterruptedException {
0:     isFirstRow = false;
1: 
1:     // initialize metadata
1:     isNoDictionaryDimensionColumn =
1:         CarbonDataProcessorUtil.getNoDictionaryMapping(configuration.getDataFields());
1:     dimensionWithComplexCount = configuration.getDimensionCount();
1:     measureCount = configuration.getMeasureCount();
1:     dataFields = configuration.getDataFields();
0:     measureDataTypes = new int[measureCount];
1:     for (int i = 0; i < measureCount; i++) {
1:       measureDataTypes[i] =
0:           dataFields[dimensionWithComplexCount + i].getColumn().getDataType().getId();
1:     }
1: 
1:     // initialize parser and converter
1:     rowParser = new RowParserImpl(dataFields, configuration);
0:     BadRecordsLogger badRecordLogger =
0:         DataConverterProcessorStepImpl.createBadRecordLogger(configuration);
1:     converter = new RowConverterImpl(configuration.getDataFields(), configuration, badRecordLogger);
1:     configuration.setCardinalityFinder(converter);
1:     converter.initialize();
1: 
1:     // initialize encoder
1:     nullBitSet = new BitSet(dataFields.length);
1:     int rowBufferSize = hadoopConf.getInt(CarbonStreamOutputFormat.CARBON_ENCODER_ROW_BUFFER_SIZE,
1:         CarbonStreamOutputFormat.CARBON_ENCODER_ROW_BUFFER_SIZE_DEFAULT);
0:     output = new StreamBlockletWriter(maxCacheSize, maxRowNums, rowBufferSize);
1: 
0:     // initialize data writer
1:     String filePath = segmentDir + File.separator + fileName;
1:     FileFactory.FileType fileType = FileFactory.getFileType(filePath);
1:     CarbonFile carbonFile = FileFactory.getCarbonFile(filePath, fileType);
1:     if (carbonFile.exists()) {
1:       // if the file is existed, use the append api
1:       outputStream = FileFactory.getDataOutputStreamUsingAppend(filePath, fileType);
1:     } else {
1:       // IF the file is not existed, use the create api
1:       outputStream = FileFactory.getDataOutputStream(filePath, fileType);
1:       writeFileHeader();
1:     }
1:   }
1: 
1:   @Override public void write(Void key, Object value) throws IOException, InterruptedException {
1:     if (isFirstRow) {
1:       initializeAtFirstRow();
1:     }
1: 
1:     // parse and convert row
0:     currentRow.setData(rowParser.parseRow((Object[]) value));
0:     converter.convert(currentRow);
1: 
1:     // null bit set
1:     nullBitSet.clear();
1:     for (int i = 0; i < dataFields.length; i++) {
1:       if (null == currentRow.getObject(i)) {
1:         nullBitSet.set(i);
1:       }
1:     }
1:     output.nextRow();
1:     byte[] b = nullBitSet.toByteArray();
1:     output.writeShort(b.length);
1:     if (b.length > 0) {
1:       output.writeBytes(b);
1:     }
1:     int dimCount = 0;
1:     Object columnValue;
1: 
1:     // primitive type dimension
1:     for (; dimCount < isNoDictionaryDimensionColumn.length; dimCount++) {
1:       columnValue = currentRow.getObject(dimCount);
1:       if (null != columnValue) {
1:         if (isNoDictionaryDimensionColumn[dimCount]) {
1:           byte[] col = (byte[]) columnValue;
1:           output.writeShort(col.length);
1:           output.writeBytes(col);
1:         } else {
1:           output.writeInt((int) columnValue);
1:         }
1:       }
1:     }
1:     // complex type dimension
1:     for (; dimCount < dimensionWithComplexCount; dimCount++) {
1:       columnValue = currentRow.getObject(dimCount);
1:       if (null != columnValue) {
1:         byte[] col = (byte[]) columnValue;
1:         output.writeShort(col.length);
1:         output.writeBytes(col);
1:       }
1:     }
1:     // measure
0:     int dataType;
1:     for (int msrCount = 0; msrCount < measureCount; msrCount++) {
1:       columnValue = currentRow.getObject(dimCount + msrCount);
1:       if (null != columnValue) {
1:         dataType = measureDataTypes[msrCount];
0:         if (dataType == DataTypes.BOOLEAN_TYPE_ID) {
1:           output.writeBoolean((boolean) columnValue);
0:         } else if (dataType == DataTypes.SHORT_TYPE_ID) {
1:           output.writeShort((short) columnValue);
0:         } else if (dataType == DataTypes.INT_TYPE_ID) {
1:           output.writeInt((int) columnValue);
0:         } else if (dataType == DataTypes.LONG_TYPE_ID) {
1:           output.writeLong((long) columnValue);
0:         } else if (dataType == DataTypes.DOUBLE_TYPE_ID) {
1:           output.writeDouble((double) columnValue);
0:         } else if (dataType == DataTypes.DECIMAL_TYPE_ID) {
1:           BigDecimal val = (BigDecimal) columnValue;
1:           byte[] bigDecimalInBytes = DataTypeUtil.bigDecimalToByte(val);
1:           output.writeShort(bigDecimalInBytes.length);
1:           output.writeBytes(bigDecimalInBytes);
1:         } else {
1:           String msg =
1:               "unsupported data type:" + dataFields[dimCount + msrCount].getColumn().getDataType()
1:                   .getName();
1:           LOGGER.error(msg);
1:           throw new IOException(msg);
1:         }
1:       }
1:     }
1: 
1:     if (output.isFull()) {
1:       appendBlockletToDataFile();
1:     }
1:   }
1: 
1:   private void writeFileHeader() throws IOException {
1:     List<ColumnSchema> wrapperColumnSchemaList = CarbonUtil
0:         .getColumnSchemaList(carbonTable.getDimensionByTableName(carbonTable.getFactTableName()),
0:             carbonTable.getMeasureByTableName(carbonTable.getFactTableName()));
1:     int[] dimLensWithComplex = new int[wrapperColumnSchemaList.size()];
1:     for (int i = 0; i < dimLensWithComplex.length; i++) {
1:       dimLensWithComplex[i] = Integer.MAX_VALUE;
1:     }
1:     int[] dictionaryColumnCardinality =
1:         CarbonUtil.getFormattedCardinality(dimLensWithComplex, wrapperColumnSchemaList);
1:     List<Integer> cardinality = new ArrayList<>();
1:     List<org.apache.carbondata.format.ColumnSchema> columnSchemaList = AbstractFactDataWriter
1:         .getColumnSchemaListAndCardinality(cardinality, dictionaryColumnCardinality,
1:             wrapperColumnSchemaList);
1:     FileHeader fileHeader =
1:         CarbonMetadataUtil.getFileHeader(true, columnSchemaList, System.currentTimeMillis());
1:     fileHeader.setIs_footer_present(false);
1:     fileHeader.setIs_splitable(true);
1:     fileHeader.setSync_marker(CarbonStreamOutputFormat.CARBON_SYNC_MARKER);
1:     outputStream.write(CarbonUtil.getByteArray(fileHeader));
1:   }
1: 
1:   /**
1:    * write a blocklet to file
1:    */
1:   private void appendBlockletToDataFile() throws IOException {
1:     if (output.getRowIndex() == -1) {
1:       return;
1:     }
1:     output.apppendBlocklet(outputStream);
1:     outputStream.flush();
1:     // reset data
1:     output.reset();
1:   }
1: 
0:   @Override public void close(TaskAttemptContext context) throws IOException, InterruptedException {
1:     try {
1:       // append remain buffer data
0:       if (!hasException) {
1:         appendBlockletToDataFile();
1:         converter.finish();
1:       }
1:     } finally {
1:       // close resource
1:       CarbonUtil.closeStreams(outputStream);
0:       output.close();
1:     }
1:   }
1: 
1:   public String getSegmentDir() {
1:     return segmentDir;
1:   }
1: 
1:   public String getFileName() {
1:     return fileName;
1:   }
1: }
author:xuchuanyin
-------------------------------------------------------------------------------
commit:8f08c4a
/////////////////////////////////////////////////////////////////////////
1: import org.apache.carbondata.core.constants.CarbonCommonConstants;
1: import org.apache.carbondata.core.datastore.compression.CompressorFactory;
/////////////////////////////////////////////////////////////////////////
1: import org.apache.carbondata.core.reader.CarbonHeaderReader;
/////////////////////////////////////////////////////////////////////////
1:   private String compressorName;
/////////////////////////////////////////////////////////////////////////
0: 
1:     // initialize data writer and compressor
1:       // get the compressor from the fileheader. In legacy store,
1:       // the compressor name is not set and it use snappy compressor
1:       FileHeader header = new CarbonHeaderReader(filePath).readHeader();
1:       if (header.isSetCompressor_name()) {
1:         compressorName = header.getCompressor_name();
1:       } else {
1:         compressorName = CompressorFactory.SupportedCompressor.SNAPPY.getName();
1:       }
1:       compressorName = carbonTable.getTableInfo().getFactTable().getTableProperties().get(
1:           CarbonCommonConstants.COMPRESSOR);
1:       if (null == compressorName) {
1:         compressorName = CompressorFactory.getInstance().getCompressor().getName();
1:       }
0: 
0:     // initialize encoder
0:     nullBitSet = new BitSet(dataFields.length);
0:     int rowBufferSize = hadoopConf.getInt(CarbonStreamOutputFormat.CARBON_ENCODER_ROW_BUFFER_SIZE,
0:         CarbonStreamOutputFormat.CARBON_ENCODER_ROW_BUFFER_SIZE_DEFAULT);
0:     output = new StreamBlockletWriter(maxCacheSize, maxRowNums, rowBufferSize,
0:         isNoDictionaryDimensionColumn.length, measureCount,
1:         measureDataTypes, compressorName);
0: 
/////////////////////////////////////////////////////////////////////////
1:     fileHeader.setCompressor_name(compressorName);
author:ravipesala
-------------------------------------------------------------------------------
commit:60dfdd3
/////////////////////////////////////////////////////////////////////////
1:     fileName = CarbonTablePath.getCarbonDataFileName(0, taskNo, 0, 0, "0", segmentId);
commit:4430178
/////////////////////////////////////////////////////////////////////////
1:     long taskNo = TaskID.forName(hadoopConf.get("mapred.tip.id")).getId();
author:Geetika Gupta
-------------------------------------------------------------------------------
commit:5969312
/////////////////////////////////////////////////////////////////////////
1:     Object[] rowData = (Object[]) value;
1:     currentRow.setRawData(rowData);
0:     // parse and convert row
1:     currentRow.setData(rowParser.parseRow(rowData));
1:     CarbonRow updatedCarbonRow = converter.convert(currentRow);
1:     if (updatedCarbonRow == null) {
1:       output.skipRow();
1:       currentRow.clearData();
1:     } else {
0:       for (int i = 0; i < dataFields.length; i++) {
0:         if (null == currentRow.getObject(i)) {
0:           nullBitSet.set(i);
1:         }
0:       output.nextRow();
0:       byte[] b = nullBitSet.toByteArray();
0:       output.writeShort(b.length);
0:       if (b.length > 0) {
0:         output.writeBytes(b);
1:       }
0:       int dimCount = 0;
0:       Object columnValue;
0:       // primitive type dimension
0:       for (; dimCount < isNoDictionaryDimensionColumn.length; dimCount++) {
0:         columnValue = currentRow.getObject(dimCount);
0:         if (null != columnValue) {
0:           if (isNoDictionaryDimensionColumn[dimCount]) {
0:             byte[] col = (byte[]) columnValue;
0:             output.writeShort(col.length);
0:             output.writeBytes(col);
1:           } else {
0:             output.writeInt((int) columnValue);
1:           }
1:         }
1:       }
0:       // complex type dimension
0:       for (; dimCount < dimensionWithComplexCount; dimCount++) {
0:         columnValue = currentRow.getObject(dimCount);
0:         if (null != columnValue) {
0:       // measure
1:       DataType dataType;
0:       for (int msrCount = 0; msrCount < measureCount; msrCount++) {
0:         columnValue = currentRow.getObject(dimCount + msrCount);
0:         if (null != columnValue) {
0:           dataType = measureDataTypes[msrCount];
1:           if (dataType == DataTypes.BOOLEAN) {
0:             output.writeBoolean((boolean) columnValue);
1:           } else if (dataType == DataTypes.SHORT) {
0:             output.writeShort((short) columnValue);
1:           } else if (dataType == DataTypes.INT) {
0:             output.writeInt((int) columnValue);
1:           } else if (dataType == DataTypes.LONG) {
0:             output.writeLong((long) columnValue);
1:           } else if (dataType == DataTypes.DOUBLE) {
0:             output.writeDouble((double) columnValue);
1:           } else if (DataTypes.isDecimal(dataType)) {
0:             BigDecimal val = (BigDecimal) columnValue;
0:             byte[] bigDecimalInBytes = DataTypeUtil.bigDecimalToByte(val);
0:             output.writeShort(bigDecimalInBytes.length);
0:             output.writeBytes(bigDecimalInBytes);
1:           } else {
0:             String msg =
0:                 "unsupported data type:" + dataFields[dimCount + msrCount].getColumn().getDataType()
0:                 .getName();
0:             LOGGER.error(msg);
0:             throw new IOException(msg);
1:           }
author:Jacky Li
-------------------------------------------------------------------------------
commit:c723947
/////////////////////////////////////////////////////////////////////////
1: package org.apache.carbondata.streaming;
commit:bf6c471
/////////////////////////////////////////////////////////////////////////
/////////////////////////////////////////////////////////////////////////
1:     segmentDir = CarbonTablePath.getSegmentPath(
1:         carbonTable.getAbsoluteTableIdentifier().getTablePath(), segmentId);
commit:5bedd77
/////////////////////////////////////////////////////////////////////////
0:     segmentDir = tablePath.getSegmentDir(segmentId);
commit:5fc7f06
/////////////////////////////////////////////////////////////////////////
1:         .getColumnSchemaList(carbonTable.getDimensionByTableName(carbonTable.getTableName()),
1:             carbonTable.getMeasureByTableName(carbonTable.getTableName()));
commit:933e30c
/////////////////////////////////////////////////////////////////////////
1: import org.apache.carbondata.core.metadata.datatype.DataType;
/////////////////////////////////////////////////////////////////////////
1:   private DataType[] measureDataTypes;
/////////////////////////////////////////////////////////////////////////
1:     measureDataTypes = new DataType[measureCount];
1:           dataFields[dimensionWithComplexCount + i].getColumn().getDataType();
/////////////////////////////////////////////////////////////////////////
0:     DataType dataType;
0:         if (dataType == DataTypes.BOOLEAN) {
0:         } else if (dataType == DataTypes.SHORT) {
0:         } else if (dataType == DataTypes.INT) {
0:         } else if (dataType == DataTypes.LONG) {
0:         } else if (dataType == DataTypes.DOUBLE) {
0:         } else if (DataTypes.isDecimal(dataType)) {
author:mohammadshahidkhan
-------------------------------------------------------------------------------
commit:837fdd2
/////////////////////////////////////////////////////////////////////////
1: import org.apache.carbondata.processing.loading.BadRecordsLoggerProvider;
/////////////////////////////////////////////////////////////////////////
/////////////////////////////////////////////////////////////////////////
1:     badRecordLogger = BadRecordsLoggerProvider.createBadRecordLogger(configuration);
============================================================================