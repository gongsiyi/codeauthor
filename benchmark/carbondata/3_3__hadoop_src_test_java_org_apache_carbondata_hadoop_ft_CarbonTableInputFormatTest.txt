1:a324e5d: /*
1:a324e5d:  * Licensed to the Apache Software Foundation (ASF) under one or more
1:a324e5d:  * contributor license agreements.  See the NOTICE file distributed with
1:a324e5d:  * this work for additional information regarding copyright ownership.
1:a324e5d:  * The ASF licenses this file to You under the Apache License, Version 2.0
1:a324e5d:  * (the "License"); you may not use this file except in compliance with
1:a324e5d:  * the License.  You may obtain a copy of the License at
1:a324e5d:  *
1:a324e5d:  *    http://www.apache.org/licenses/LICENSE-2.0
1:a324e5d:  *
1:a324e5d:  * Unless required by applicable law or agreed to in writing, software
1:a324e5d:  * distributed under the License is distributed on an "AS IS" BASIS,
1:a324e5d:  * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
1:a324e5d:  * See the License for the specific language governing permissions and
1:a324e5d:  * limitations under the License.
1:a324e5d:  */
2:a324e5d: 
1:a324e5d: package org.apache.carbondata.hadoop.ft;
1:a324e5d: 
1:6e77f2b: import java.io.BufferedReader;
1:6e77f2b: import java.io.BufferedWriter;
1:a324e5d: import java.io.File;
1:a324e5d: import java.io.FileFilter;
1:6e77f2b: import java.io.FileReader;
1:6e77f2b: import java.io.FileWriter;
1:6e77f2b: import java.io.IOException;
1:a324e5d: import java.util.List;
1:a324e5d: import java.util.UUID;
1:a324e5d: 
1:a324e5d: import org.apache.carbondata.core.constants.CarbonCommonConstants;
1:6e77f2b: import org.apache.carbondata.core.metadata.AbsoluteTableIdentifier;
1:a324e5d: import org.apache.carbondata.core.metadata.datatype.DataTypes;
1:a324e5d: import org.apache.carbondata.core.scan.expression.ColumnExpression;
1:a324e5d: import org.apache.carbondata.core.scan.expression.Expression;
1:a324e5d: import org.apache.carbondata.core.scan.expression.LiteralExpression;
1:a324e5d: import org.apache.carbondata.core.scan.expression.conditional.EqualToExpression;
1:a324e5d: import org.apache.carbondata.core.util.CarbonProperties;
1:6e77f2b: import org.apache.carbondata.core.util.CarbonUtil;
1:a324e5d: import org.apache.carbondata.core.util.path.CarbonTablePath;
1:6e77f2b: import org.apache.carbondata.hadoop.CarbonProjection;
1:a324e5d: import org.apache.carbondata.hadoop.api.CarbonTableInputFormat;
1:c723947: import org.apache.carbondata.hadoop.testutil.StoreCreator;
1:bea277f: 
1:a324e5d: import org.apache.hadoop.conf.Configuration;
1:a324e5d: import org.apache.hadoop.fs.Path;
1:6e77f2b: import org.apache.hadoop.io.IntWritable;
1:6e77f2b: import org.apache.hadoop.io.Text;
1:a324e5d: import org.apache.hadoop.mapred.JobConf;
1:a324e5d: import org.apache.hadoop.mapreduce.Job;
1:6e77f2b: import org.apache.hadoop.mapreduce.Mapper;
1:a324e5d: import org.apache.hadoop.mapreduce.lib.input.FileInputFormat;
1:6e77f2b: import org.apache.hadoop.mapreduce.lib.output.FileOutputFormat;
1:6e77f2b: import org.apache.hadoop.mapreduce.lib.output.TextOutputFormat;
1:a324e5d: import org.junit.Assert;
1:a324e5d: import org.junit.Test;
1:a324e5d: 
1:a324e5d: public class CarbonTableInputFormatTest {
1:a324e5d:   // changed setUp to static init block to avoid un wanted multiple time store creation
1:3894e1d:   private static StoreCreator creator;
1:a324e5d:   static {
1:a324e5d:     CarbonProperties.getInstance().
1:a324e5d:         addProperty(CarbonCommonConstants.CARBON_BADRECORDS_LOC, "/tmp/carbon/badrecords");
1:bea277f:     CarbonProperties.getInstance()
1:bea277f:         .addProperty(CarbonCommonConstants.CARBON_SYSTEM_FOLDER_LOCATION, "/tmp/carbon/");
1:6e77f2b:     try {
1:3894e1d:       creator = new StoreCreator(new File("target/store").getAbsolutePath(),
1:3894e1d:           new File("../hadoop/src/test/resources/data.csv").getCanonicalPath());
1:3894e1d:       creator.createCarbonStore();
1:6e77f2b:     } catch (Exception e) {
1:6e77f2b:       Assert.fail("create table failed: " + e.getMessage());
1:6e77f2b:     }
1:a324e5d:   }
1:a324e5d: 
1:a324e5d:   @Test public void testGetFilteredSplits() throws Exception {
1:a324e5d:     CarbonTableInputFormat carbonInputFormat = new CarbonTableInputFormat();
1:a324e5d:     JobConf jobConf = new JobConf(new Configuration());
1:a324e5d:     Job job = Job.getInstance(jobConf);
1:a324e5d:     job.getConfiguration().set("query.id", UUID.randomUUID().toString());
1:3894e1d:     String tblPath = creator.getAbsoluteTableIdentifier().getTablePath();
1:a324e5d:     FileInputFormat.addInputPath(job, new Path(tblPath));
1:3894e1d:     CarbonTableInputFormat.setDatabaseName(job.getConfiguration(), creator.getAbsoluteTableIdentifier().getDatabaseName());
1:3894e1d:     CarbonTableInputFormat.setTableName(job.getConfiguration(), creator.getAbsoluteTableIdentifier().getTableName());
1:a324e5d:     Expression expression = new EqualToExpression(new ColumnExpression("country", DataTypes.STRING),
1:a324e5d:         new LiteralExpression("china", DataTypes.STRING));
1:a324e5d:     CarbonTableInputFormat.setFilterPredicates(job.getConfiguration(), expression);
1:a324e5d:     List splits = carbonInputFormat.getSplits(job);
1:a324e5d: 
1:a324e5d:     Assert.assertTrue(splits != null);
1:a324e5d:     Assert.assertTrue(!splits.isEmpty());
1:a324e5d:   }
1:a324e5d: 
1:a324e5d:   @Test
1:a324e5d:   public void testGetSplits() throws Exception {
1:a324e5d:     CarbonTableInputFormat carbonInputFormat = new CarbonTableInputFormat();
1:a324e5d:     JobConf jobConf = new JobConf(new Configuration());
1:a324e5d:     Job job = Job.getInstance(jobConf);
1:a324e5d:     job.getConfiguration().set("query.id", UUID.randomUUID().toString());
1:3894e1d:     String tblPath = creator.getAbsoluteTableIdentifier().getTablePath();
1:a324e5d:     FileInputFormat.addInputPath(job, new Path(tblPath));
1:3894e1d:     CarbonTableInputFormat.setDatabaseName(job.getConfiguration(), creator.getAbsoluteTableIdentifier().getDatabaseName());
1:3894e1d:     CarbonTableInputFormat.setTableName(job.getConfiguration(), creator.getAbsoluteTableIdentifier().getTableName());
1:a324e5d:     // list files to get the carbondata file
1:3894e1d:     String segmentPath = CarbonTablePath.getSegmentPath(creator.getAbsoluteTableIdentifier().getTablePath(), "0");
1:a324e5d:     File segmentDir = new File(segmentPath);
1:a324e5d:     if (segmentDir.exists() && segmentDir.isDirectory()) {
1:a324e5d:       File[] files = segmentDir.listFiles(new FileFilter() {
1:a324e5d:         @Override
1:a324e5d:         public boolean accept(File pathname) {
1:a324e5d:           return pathname.getName().endsWith("carbondata");
1:a324e5d:         }
1:a324e5d:       });
1:a324e5d:       if (files != null && files.length > 0) {
1:a324e5d:         job.getConfiguration().set(CarbonTableInputFormat.INPUT_FILES, files[0].getName());
1:a324e5d:       }
1:a324e5d:     }
1:a324e5d:     List splits = carbonInputFormat.getSplits(job);
1:a324e5d: 
1:a324e5d:     Assert.assertTrue(splits != null && splits.size() == 1);
1:a324e5d:   }
1:a324e5d: 
1:6e77f2b:   @Test public void testInputFormatMapperReadAllRowsAndColumns() throws Exception {
1:6e77f2b:     try {
1:6e77f2b:       String outPath = "target/output";
1:6e77f2b:       CarbonProjection carbonProjection = new CarbonProjection();
1:6e77f2b:       carbonProjection.addColumn("ID");
1:6e77f2b:       carbonProjection.addColumn("date");
1:6e77f2b:       carbonProjection.addColumn("country");
1:6e77f2b:       carbonProjection.addColumn("name");
1:6e77f2b:       carbonProjection.addColumn("phonetype");
1:6e77f2b:       carbonProjection.addColumn("serialname");
1:6e77f2b:       carbonProjection.addColumn("salary");
1:6e77f2b:       runJob(outPath, carbonProjection, null);
1:6e77f2b:       Assert.assertEquals("Count lines are not matching", 1000, countTheLines(outPath));
1:6e77f2b:       Assert.assertEquals("Column count are not matching", 7, countTheColumns(outPath));
1:6e77f2b:     } catch (Exception e) {
1:6e77f2b:       e.printStackTrace();
1:6e77f2b:       Assert.assertTrue("failed", false);
1:6e77f2b:       throw e;
1:6e77f2b:     } finally {
1:3894e1d:       creator.clearDataMaps();
1:6e77f2b:     }
1:6e77f2b:   }
1:6e77f2b: 
1:6e77f2b:   @Test public void testInputFormatMapperReadAllRowsAndFewColumns() throws Exception {
1:6e77f2b:     try {
1:6e77f2b:       String outPath = "target/output2";
1:6e77f2b:       CarbonProjection carbonProjection = new CarbonProjection();
1:6e77f2b:       carbonProjection.addColumn("ID");
1:6e77f2b:       carbonProjection.addColumn("country");
1:6e77f2b:       carbonProjection.addColumn("salary");
1:6e77f2b:       runJob(outPath, carbonProjection, null);
1:6e77f2b: 
1:6e77f2b:       Assert.assertEquals("Count lines are not matching", 1000, countTheLines(outPath));
1:6e77f2b:       Assert.assertEquals("Column count are not matching", 3, countTheColumns(outPath));
1:6e77f2b:     } catch (Exception e) {
1:6e77f2b:       e.printStackTrace();
1:6e77f2b:       Assert.assertTrue("failed", false);
1:6e77f2b:     } finally {
1:3894e1d:       creator.clearDataMaps();
1:6e77f2b:     }
1:6e77f2b:   }
1:6e77f2b: 
1:6e77f2b:   @Test public void testInputFormatMapperReadAllRowsAndFewColumnsWithFilter() throws Exception {
1:6e77f2b:     try {
1:6e77f2b:       String outPath = "target/output3";
1:6e77f2b:       CarbonProjection carbonProjection = new CarbonProjection();
1:6e77f2b:       carbonProjection.addColumn("ID");
1:6e77f2b:       carbonProjection.addColumn("country");
1:6e77f2b:       carbonProjection.addColumn("salary");
1:6e77f2b:       Expression expression =
1:6e77f2b:           new EqualToExpression(new ColumnExpression("country", DataTypes.STRING),
1:6e77f2b:               new LiteralExpression("france", DataTypes.STRING));
1:6e77f2b:       runJob(outPath, carbonProjection, expression);
1:6e77f2b:       Assert.assertEquals("Count lines are not matching", 101, countTheLines(outPath));
1:6e77f2b:       Assert.assertEquals("Column count are not matching", 3, countTheColumns(outPath));
1:6e77f2b:     } catch (Exception e) {
1:6e77f2b:       Assert.assertTrue("failed", false);
1:6e77f2b:     } finally {
1:3894e1d:       creator.clearDataMaps();
1:6e77f2b:     }
1:6e77f2b:   }
1:6e77f2b: 
1:6e77f2b: 
1:6e77f2b:   private int countTheLines(String outPath) throws Exception {
1:6e77f2b:     File file = new File(outPath);
1:6e77f2b:     if (file.exists()) {
1:6e77f2b:       BufferedReader reader = new BufferedReader(new FileReader(file));
1:6e77f2b:       int i = 0;
1:6e77f2b:       while (reader.readLine() != null) {
1:6e77f2b:         i++;
1:6e77f2b:       }
1:6e77f2b:       reader.close();
1:6e77f2b:       return i;
1:6e77f2b:     }
1:6e77f2b:     return 0;
1:6e77f2b:   }
1:6e77f2b: 
1:6e77f2b:   private int countTheColumns(String outPath) throws Exception {
1:6e77f2b:     File file = new File(outPath);
1:6e77f2b:     if (file.exists()) {
1:6e77f2b:       BufferedReader reader = new BufferedReader(new FileReader(file));
1:6e77f2b:       String[] split = reader.readLine().split(",");
1:6e77f2b:       reader.close();
1:6e77f2b:       return split.length;
1:6e77f2b:     }
1:6e77f2b:     return 0;
1:6e77f2b:   }
1:6e77f2b: 
1:6e77f2b:   public static class Map extends Mapper<Void, Object[], Text, Text> {
1:6e77f2b: 
1:6e77f2b:     private BufferedWriter fileWriter;
1:6e77f2b: 
1:6e77f2b:     public void setup(Context context) throws IOException, InterruptedException {
1:6e77f2b:       String outPath = context.getConfiguration().get("outpath");
1:6e77f2b:       File outFile = new File(outPath);
1:6e77f2b:       try {
1:6e77f2b:         fileWriter = new BufferedWriter(new FileWriter(outFile));
1:6e77f2b:       } catch (Exception e) {
1:6e77f2b:         throw new RuntimeException(e);
1:6e77f2b:       }
1:6e77f2b:     }
1:6e77f2b: 
1:6e77f2b:     public void map(Void key, Object[] value, Context context) throws IOException {
1:6e77f2b:       StringBuilder builder = new StringBuilder();
1:6e77f2b:       for (int i = 0; i < value.length; i++) {
1:6e77f2b:         builder.append(value[i]).append(",");
1:6e77f2b:       }
1:6e77f2b:       fileWriter.write(builder.toString().substring(0, builder.toString().length() - 1));
1:6e77f2b:       fileWriter.newLine();
1:6e77f2b:     }
1:6e77f2b: 
1:6e77f2b:     @Override public void cleanup(Context context) throws IOException, InterruptedException {
1:6e77f2b:       super.cleanup(context);
1:6e77f2b:       fileWriter.close();
1:6e77f2b:       context.write(new Text(), new Text());
1:6e77f2b:     }
1:6e77f2b:   }
1:6e77f2b: 
1:6e77f2b:   private void runJob(String outPath, CarbonProjection projection, Expression filter)
1:6e77f2b:       throws Exception {
1:6e77f2b: 
1:6e77f2b:     Configuration configuration = new Configuration();
1:6e77f2b:     configuration.set("mapreduce.cluster.local.dir", new File(outPath + "1").getCanonicalPath());
1:6e77f2b:     Job job = Job.getInstance(configuration);
1:6e77f2b:     job.setJarByClass(CarbonTableInputFormatTest.class);
1:6e77f2b:     job.setOutputKeyClass(Text.class);
1:6e77f2b:     job.setOutputValueClass(IntWritable.class);
1:6e77f2b:     job.setMapperClass(Map.class);
1:6e77f2b:     job.setInputFormatClass(CarbonTableInputFormat.class);
1:6e77f2b:     job.setOutputFormatClass(TextOutputFormat.class);
1:3894e1d:     AbsoluteTableIdentifier abs = creator.getAbsoluteTableIdentifier();
1:6e77f2b:     if (projection != null) {
1:6e77f2b:       CarbonTableInputFormat.setColumnProjection(job.getConfiguration(), projection);
1:6e77f2b:     }
1:6e77f2b:     if (filter != null) {
1:6e77f2b:       CarbonTableInputFormat.setFilterPredicates(job.getConfiguration(), filter);
1:6e77f2b:     }
1:6e77f2b:     CarbonTableInputFormat.setDatabaseName(job.getConfiguration(),
1:6e77f2b:         abs.getCarbonTableIdentifier().getDatabaseName());
1:6e77f2b:     CarbonTableInputFormat.setTableName(job.getConfiguration(),
1:6e77f2b:         abs.getCarbonTableIdentifier().getTableName());
1:6e77f2b:     FileInputFormat.addInputPath(job, new Path(abs.getTablePath()));
1:6e77f2b:     CarbonUtil.deleteFoldersAndFiles(new File(outPath + "1"));
1:6e77f2b:     FileOutputFormat.setOutputPath(job, new Path(outPath + "1"));
1:6e77f2b:     job.getConfiguration().set("outpath", outPath);
1:6e77f2b:     job.getConfiguration().set("query.id", String.valueOf(System.nanoTime()));
1:6e77f2b:     boolean status = job.waitForCompletion(true);
1:6e77f2b:   }
1:a324e5d: }
============================================================================
author:ravipesala
-------------------------------------------------------------------------------
commit:3894e1d
/////////////////////////////////////////////////////////////////////////
1:   private static StoreCreator creator;
1:       creator = new StoreCreator(new File("target/store").getAbsolutePath(),
1:           new File("../hadoop/src/test/resources/data.csv").getCanonicalPath());
1:       creator.createCarbonStore();
/////////////////////////////////////////////////////////////////////////
1:     String tblPath = creator.getAbsoluteTableIdentifier().getTablePath();
1:     CarbonTableInputFormat.setDatabaseName(job.getConfiguration(), creator.getAbsoluteTableIdentifier().getDatabaseName());
1:     CarbonTableInputFormat.setTableName(job.getConfiguration(), creator.getAbsoluteTableIdentifier().getTableName());
/////////////////////////////////////////////////////////////////////////
1:     String tblPath = creator.getAbsoluteTableIdentifier().getTablePath();
1:     CarbonTableInputFormat.setDatabaseName(job.getConfiguration(), creator.getAbsoluteTableIdentifier().getDatabaseName());
1:     CarbonTableInputFormat.setTableName(job.getConfiguration(), creator.getAbsoluteTableIdentifier().getTableName());
1:     String segmentPath = CarbonTablePath.getSegmentPath(creator.getAbsoluteTableIdentifier().getTablePath(), "0");
/////////////////////////////////////////////////////////////////////////
1:       creator.clearDataMaps();
/////////////////////////////////////////////////////////////////////////
1:       creator.clearDataMaps();
/////////////////////////////////////////////////////////////////////////
1:       creator.clearDataMaps();
/////////////////////////////////////////////////////////////////////////
1:     AbsoluteTableIdentifier abs = creator.getAbsoluteTableIdentifier();
author:mohammadshahidkhan
-------------------------------------------------------------------------------
commit:bea277f
/////////////////////////////////////////////////////////////////////////
1: 
/////////////////////////////////////////////////////////////////////////
1:     CarbonProperties.getInstance()
1:         .addProperty(CarbonCommonConstants.CARBON_SYSTEM_FOLDER_LOCATION, "/tmp/carbon/");
author:Jacky Li
-------------------------------------------------------------------------------
commit:c723947
/////////////////////////////////////////////////////////////////////////
/////////////////////////////////////////////////////////////////////////
1: import org.apache.carbondata.hadoop.testutil.StoreCreator;
/////////////////////////////////////////////////////////////////////////
commit:6e77f2b
/////////////////////////////////////////////////////////////////////////
1: import java.io.BufferedReader;
1: import java.io.BufferedWriter;
1: import java.io.FileReader;
1: import java.io.FileWriter;
1: import java.io.IOException;
1: import org.apache.carbondata.core.metadata.AbsoluteTableIdentifier;
1: import org.apache.carbondata.core.util.CarbonUtil;
1: import org.apache.carbondata.hadoop.CarbonProjection;
1: import org.apache.hadoop.io.IntWritable;
1: import org.apache.hadoop.io.Text;
1: import org.apache.hadoop.mapreduce.Mapper;
1: import org.apache.hadoop.mapreduce.lib.output.FileOutputFormat;
1: import org.apache.hadoop.mapreduce.lib.output.TextOutputFormat;
/////////////////////////////////////////////////////////////////////////
1:     try {
0:       StoreCreator.createCarbonStore();
1:     } catch (Exception e) {
1:       Assert.fail("create table failed: " + e.getMessage());
1:     }
/////////////////////////////////////////////////////////////////////////
1:   @Test public void testInputFormatMapperReadAllRowsAndColumns() throws Exception {
1:     try {
1:       String outPath = "target/output";
1:       CarbonProjection carbonProjection = new CarbonProjection();
1:       carbonProjection.addColumn("ID");
1:       carbonProjection.addColumn("date");
1:       carbonProjection.addColumn("country");
1:       carbonProjection.addColumn("name");
1:       carbonProjection.addColumn("phonetype");
1:       carbonProjection.addColumn("serialname");
1:       carbonProjection.addColumn("salary");
1:       runJob(outPath, carbonProjection, null);
1:       Assert.assertEquals("Count lines are not matching", 1000, countTheLines(outPath));
1:       Assert.assertEquals("Column count are not matching", 7, countTheColumns(outPath));
1:     } catch (Exception e) {
1:       e.printStackTrace();
1:       Assert.assertTrue("failed", false);
1:       throw e;
1:     } finally {
0:       StoreCreator.clearDataMaps();
1:     }
1:   }
1: 
1:   @Test public void testInputFormatMapperReadAllRowsAndFewColumns() throws Exception {
1:     try {
1:       String outPath = "target/output2";
1:       CarbonProjection carbonProjection = new CarbonProjection();
1:       carbonProjection.addColumn("ID");
1:       carbonProjection.addColumn("country");
1:       carbonProjection.addColumn("salary");
1:       runJob(outPath, carbonProjection, null);
1: 
1:       Assert.assertEquals("Count lines are not matching", 1000, countTheLines(outPath));
1:       Assert.assertEquals("Column count are not matching", 3, countTheColumns(outPath));
1:     } catch (Exception e) {
1:       e.printStackTrace();
1:       Assert.assertTrue("failed", false);
1:     } finally {
0:       StoreCreator.clearDataMaps();
1:     }
1:   }
1: 
1:   @Test public void testInputFormatMapperReadAllRowsAndFewColumnsWithFilter() throws Exception {
1:     try {
1:       String outPath = "target/output3";
1:       CarbonProjection carbonProjection = new CarbonProjection();
1:       carbonProjection.addColumn("ID");
1:       carbonProjection.addColumn("country");
1:       carbonProjection.addColumn("salary");
1:       Expression expression =
1:           new EqualToExpression(new ColumnExpression("country", DataTypes.STRING),
1:               new LiteralExpression("france", DataTypes.STRING));
1:       runJob(outPath, carbonProjection, expression);
1:       Assert.assertEquals("Count lines are not matching", 101, countTheLines(outPath));
1:       Assert.assertEquals("Column count are not matching", 3, countTheColumns(outPath));
1:     } catch (Exception e) {
1:       Assert.assertTrue("failed", false);
1:     } finally {
0:       StoreCreator.clearDataMaps();
1:     }
1:   }
1: 
1: 
1:   private int countTheLines(String outPath) throws Exception {
1:     File file = new File(outPath);
1:     if (file.exists()) {
1:       BufferedReader reader = new BufferedReader(new FileReader(file));
1:       int i = 0;
1:       while (reader.readLine() != null) {
1:         i++;
1:       }
1:       reader.close();
1:       return i;
1:     }
1:     return 0;
1:   }
1: 
1:   private int countTheColumns(String outPath) throws Exception {
1:     File file = new File(outPath);
1:     if (file.exists()) {
1:       BufferedReader reader = new BufferedReader(new FileReader(file));
1:       String[] split = reader.readLine().split(",");
1:       reader.close();
1:       return split.length;
1:     }
1:     return 0;
1:   }
1: 
1:   public static class Map extends Mapper<Void, Object[], Text, Text> {
1: 
1:     private BufferedWriter fileWriter;
1: 
1:     public void setup(Context context) throws IOException, InterruptedException {
1:       String outPath = context.getConfiguration().get("outpath");
1:       File outFile = new File(outPath);
1:       try {
1:         fileWriter = new BufferedWriter(new FileWriter(outFile));
1:       } catch (Exception e) {
1:         throw new RuntimeException(e);
1:       }
1:     }
1: 
1:     public void map(Void key, Object[] value, Context context) throws IOException {
1:       StringBuilder builder = new StringBuilder();
1:       for (int i = 0; i < value.length; i++) {
1:         builder.append(value[i]).append(",");
1:       }
1:       fileWriter.write(builder.toString().substring(0, builder.toString().length() - 1));
1:       fileWriter.newLine();
1:     }
1: 
1:     @Override public void cleanup(Context context) throws IOException, InterruptedException {
1:       super.cleanup(context);
1:       fileWriter.close();
1:       context.write(new Text(), new Text());
1:     }
1:   }
1: 
1:   private void runJob(String outPath, CarbonProjection projection, Expression filter)
1:       throws Exception {
1: 
1:     Configuration configuration = new Configuration();
1:     configuration.set("mapreduce.cluster.local.dir", new File(outPath + "1").getCanonicalPath());
1:     Job job = Job.getInstance(configuration);
1:     job.setJarByClass(CarbonTableInputFormatTest.class);
1:     job.setOutputKeyClass(Text.class);
1:     job.setOutputValueClass(IntWritable.class);
1:     job.setMapperClass(Map.class);
1:     job.setInputFormatClass(CarbonTableInputFormat.class);
1:     job.setOutputFormatClass(TextOutputFormat.class);
0:     AbsoluteTableIdentifier abs = StoreCreator.getAbsoluteTableIdentifier();
1:     if (projection != null) {
1:       CarbonTableInputFormat.setColumnProjection(job.getConfiguration(), projection);
1:     }
1:     if (filter != null) {
1:       CarbonTableInputFormat.setFilterPredicates(job.getConfiguration(), filter);
1:     }
1:     CarbonTableInputFormat.setDatabaseName(job.getConfiguration(),
1:         abs.getCarbonTableIdentifier().getDatabaseName());
1:     CarbonTableInputFormat.setTableName(job.getConfiguration(),
1:         abs.getCarbonTableIdentifier().getTableName());
1:     FileInputFormat.addInputPath(job, new Path(abs.getTablePath()));
1:     CarbonUtil.deleteFoldersAndFiles(new File(outPath + "1"));
1:     FileOutputFormat.setOutputPath(job, new Path(outPath + "1"));
1:     job.getConfiguration().set("outpath", outPath);
1:     job.getConfiguration().set("query.id", String.valueOf(System.nanoTime()));
1:     boolean status = job.waitForCompletion(true);
1:   }
commit:a324e5d
/////////////////////////////////////////////////////////////////////////
1: /*
1:  * Licensed to the Apache Software Foundation (ASF) under one or more
1:  * contributor license agreements.  See the NOTICE file distributed with
1:  * this work for additional information regarding copyright ownership.
1:  * The ASF licenses this file to You under the Apache License, Version 2.0
1:  * (the "License"); you may not use this file except in compliance with
1:  * the License.  You may obtain a copy of the License at
1:  *
1:  *    http://www.apache.org/licenses/LICENSE-2.0
1:  *
1:  * Unless required by applicable law or agreed to in writing, software
1:  * distributed under the License is distributed on an "AS IS" BASIS,
1:  * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
1:  * See the License for the specific language governing permissions and
1:  * limitations under the License.
1:  */
1: 
1: package org.apache.carbondata.hadoop.ft;
1: 
1: import java.io.File;
1: import java.io.FileFilter;
1: import java.util.List;
1: import java.util.UUID;
1: 
0: import junit.framework.TestCase;
1: 
1: import org.apache.carbondata.core.constants.CarbonCommonConstants;
1: import org.apache.carbondata.core.metadata.datatype.DataTypes;
1: import org.apache.carbondata.core.scan.expression.ColumnExpression;
1: import org.apache.carbondata.core.scan.expression.Expression;
1: import org.apache.carbondata.core.scan.expression.LiteralExpression;
1: import org.apache.carbondata.core.scan.expression.conditional.EqualToExpression;
1: import org.apache.carbondata.core.util.CarbonProperties;
1: import org.apache.carbondata.core.util.path.CarbonTablePath;
1: import org.apache.carbondata.hadoop.api.CarbonTableInputFormat;
0: import org.apache.carbondata.hadoop.test.util.StoreCreator;
1: import org.apache.hadoop.conf.Configuration;
1: import org.apache.hadoop.fs.Path;
1: import org.apache.hadoop.mapred.JobConf;
1: import org.apache.hadoop.mapreduce.Job;
1: import org.apache.hadoop.mapreduce.lib.input.FileInputFormat;
0: import org.junit.After;
1: import org.junit.Assert;
0: import org.junit.Before;
1: import org.junit.Test;
1: 
1: public class CarbonTableInputFormatTest {
1:   // changed setUp to static init block to avoid un wanted multiple time store creation
1:   static {
1:     CarbonProperties.getInstance().
1:         addProperty(CarbonCommonConstants.CARBON_BADRECORDS_LOC, "/tmp/carbon/badrecords");
0:     StoreCreator.createCarbonStore();
1:   }
1: 
1:   @Test public void testGetFilteredSplits() throws Exception {
1:     CarbonTableInputFormat carbonInputFormat = new CarbonTableInputFormat();
1:     JobConf jobConf = new JobConf(new Configuration());
1:     Job job = Job.getInstance(jobConf);
1:     job.getConfiguration().set("query.id", UUID.randomUUID().toString());
0:     String tblPath = StoreCreator.getAbsoluteTableIdentifier().getTablePath();
1:     FileInputFormat.addInputPath(job, new Path(tblPath));
0:     CarbonTableInputFormat.setDatabaseName(job.getConfiguration(), StoreCreator.getAbsoluteTableIdentifier().getDatabaseName());
0:     CarbonTableInputFormat.setTableName(job.getConfiguration(), StoreCreator.getAbsoluteTableIdentifier().getTableName());
1:     Expression expression = new EqualToExpression(new ColumnExpression("country", DataTypes.STRING),
1:         new LiteralExpression("china", DataTypes.STRING));
1:     CarbonTableInputFormat.setFilterPredicates(job.getConfiguration(), expression);
1:     List splits = carbonInputFormat.getSplits(job);
1: 
1:     Assert.assertTrue(splits != null);
1:     Assert.assertTrue(!splits.isEmpty());
1:   }
1: 
1:   @Test
1:   public void testGetSplits() throws Exception {
1:     CarbonTableInputFormat carbonInputFormat = new CarbonTableInputFormat();
1:     JobConf jobConf = new JobConf(new Configuration());
1:     Job job = Job.getInstance(jobConf);
1:     job.getConfiguration().set("query.id", UUID.randomUUID().toString());
0:     String tblPath = StoreCreator.getAbsoluteTableIdentifier().getTablePath();
1:     FileInputFormat.addInputPath(job, new Path(tblPath));
0:     CarbonTableInputFormat.setDatabaseName(job.getConfiguration(), StoreCreator.getAbsoluteTableIdentifier().getDatabaseName());
0:     CarbonTableInputFormat.setTableName(job.getConfiguration(), StoreCreator.getAbsoluteTableIdentifier().getTableName());
1:     // list files to get the carbondata file
0:     String segmentPath = CarbonTablePath.getSegmentPath(StoreCreator.getAbsoluteTableIdentifier().getTablePath(), "0");
1:     File segmentDir = new File(segmentPath);
1:     if (segmentDir.exists() && segmentDir.isDirectory()) {
1:       File[] files = segmentDir.listFiles(new FileFilter() {
1:         @Override
1:         public boolean accept(File pathname) {
1:           return pathname.getName().endsWith("carbondata");
1:         }
1:       });
1:       if (files != null && files.length > 0) {
1:         job.getConfiguration().set(CarbonTableInputFormat.INPUT_FILES, files[0].getName());
1:       }
1:     }
1:     List splits = carbonInputFormat.getSplits(job);
1: 
1:     Assert.assertTrue(splits != null && splits.size() == 1);
1:   }
1: 
1: }
============================================================================