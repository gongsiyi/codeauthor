1:2cf1104: /*
1:2cf1104:  * Licensed to the Apache Software Foundation (ASF) under one or more
1:2cf1104:  * contributor license agreements.  See the NOTICE file distributed with
1:2cf1104:  * this work for additional information regarding copyright ownership.
1:2cf1104:  * The ASF licenses this file to You under the Apache License, Version 2.0
1:2cf1104:  * (the "License"); you may not use this file except in compliance with
1:2cf1104:  * the License.  You may obtain a copy of the License at
2:2cf1104:  *
1:2cf1104:  *    http://www.apache.org/licenses/LICENSE-2.0
1:2cf1104:  *
1:2cf1104:  * Unless required by applicable law or agreed to in writing, software
1:2cf1104:  * distributed under the License is distributed on an "AS IS" BASIS,
1:2cf1104:  * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
1:2cf1104:  * See the License for the specific language governing permissions and
1:2cf1104:  * limitations under the License.
2:2cf1104:  */
1:2cf1104: package org.apache.carbondata.core.datastore.chunk.reader.measure;
1:dc83b2a: 
1:2cf1104: import java.io.IOException;
1:2cf1104: import java.util.List;
1:dc83b2a: 
1:daa6465: import org.apache.carbondata.core.datastore.FileReader;
1:2cf1104: import org.apache.carbondata.core.datastore.chunk.impl.MeasureRawColumnChunk;
1:2cf1104: import org.apache.carbondata.core.metadata.blocklet.BlockletInfo;
3:2cf1104: 
2:2cf1104: /**
1:2cf1104:  * Abstract class for V2, V3 format measure column reader
1:2cf1104:  */
1:2cf1104: public abstract class AbstractMeasureChunkReaderV2V3Format extends AbstractMeasureChunkReader {
1:2cf1104: 
1:2cf1104:   /**
1:2cf1104:    * measure column chunks offset
1:2cf1104:    */
1:2cf1104:   protected List<Long> measureColumnChunkOffsets;
1:2cf1104: 
1:2cf1104:   /**
1:2cf1104:    * measure column chunks length
1:2cf1104:    */
1:2cf1104:   protected List<Integer> measureColumnChunkLength;
1:2cf1104: 
1:2cf1104:   public AbstractMeasureChunkReaderV2V3Format(final BlockletInfo blockletInfo,
1:2cf1104:       final String filePath) {
1:2cf1104:     super(filePath, blockletInfo.getNumberOfRows());
1:2cf1104:     this.measureColumnChunkOffsets = blockletInfo.getMeasureChunkOffsets();
1:2cf1104:     this.measureColumnChunkLength = blockletInfo.getMeasureChunksLength();
1:edda248:   }
1:2cf1104: 
1:2cf1104:   /**
1:2cf1104:    * Below method will be used to read the chunk based on block indexes
1:2cf1104:    * Reading logic of below method is: Except last column all the column chunk
1:2cf1104:    * can be read in group if not last column then read data of all the column
1:2cf1104:    * present in block index together then process it. For last column read is
1:2cf1104:    * separately and process
1:2cf1104:    *
1:2cf1104:    * @param fileReader   file reader to read the blocks from file
1:daa6465:    * @param columnIndexRange blocks range to be read, columnIndexGroup[i] is one group, inside the
1:daa6465:    *                         group, columnIndexGroup[i][0] is start column index,
1:daa6465:    *                         and columnIndexGroup[i][1] is end column index
1:2cf1104:    * @return measure column chunks
1:2cf1104:    * @throws IOException
1:2cf1104:    */
1:daa6465:   public MeasureRawColumnChunk[] readRawMeasureChunks(FileReader fileReader,
1:daa6465:       int[][] columnIndexRange) throws IOException {
1:2cf1104:     // read the column chunk based on block index and add
1:2cf1104:     MeasureRawColumnChunk[] dataChunks =
1:2cf1104:         new MeasureRawColumnChunk[measureColumnChunkOffsets.size()];
1:daa6465:     if (columnIndexRange.length == 0) {
1:2cf1104:       return dataChunks;
1:9e064ee:     }
1:2cf1104:     MeasureRawColumnChunk[] groupChunk = null;
1:2cf1104:     int index = 0;
1:daa6465:     for (int i = 0; i < columnIndexRange.length - 1; i++) {
1:2cf1104:       index = 0;
1:daa6465:       groupChunk = readRawMeasureChunksInGroup(
1:daa6465:           fileReader, columnIndexRange[i][0], columnIndexRange[i][1]);
1:daa6465:       for (int j = columnIndexRange[i][0]; j <= columnIndexRange[i][1]; j++) {
1:2cf1104:         dataChunks[j] = groupChunk[index++];
5:2cf1104:       }
1:2cf1104:     }
1:daa6465:     if (columnIndexRange[columnIndexRange.length - 1][0] == measureColumnChunkOffsets.size() - 1) {
1:daa6465:       dataChunks[columnIndexRange[columnIndexRange.length - 1][0]] =
1:daa6465:           readRawMeasureChunk(fileReader, columnIndexRange[columnIndexRange.length - 1][0]);
1:7ef9164:     } else {
1:daa6465:       groupChunk = readRawMeasureChunksInGroup(
1:daa6465:           fileReader, columnIndexRange[columnIndexRange.length - 1][0],
1:daa6465:           columnIndexRange[columnIndexRange.length - 1][1]);
1:2cf1104:       index = 0;
1:daa6465:       for (int j = columnIndexRange[columnIndexRange.length - 1][0];
1:daa6465:            j <= columnIndexRange[columnIndexRange.length - 1][1]; j++) {
1:2cf1104:         dataChunks[j] = groupChunk[index++];
1:7ef9164:       }
1:2cf1104:     }
1:2cf1104:     return dataChunks;
1:2cf1104:   }
1:2cf1104: 
1:2cf1104:   /**
1:2cf1104:    * Below method will be used to read measure chunk data in group.
1:2cf1104:    * This method will be useful to avoid multiple IO while reading the
1:2cf1104:    * data from
1:2cf1104:    *
1:2cf1104:    * @param fileReader               file reader to read the data
1:daa6465:    * @param startColumnIndex first column index to be read
1:daa6465:    * @param endColumnIndex   end column index to be read
1:2cf1104:    * @return measure raw chunkArray
1:2cf1104:    * @throws IOException
1:2cf1104:    */
1:daa6465:   protected abstract MeasureRawColumnChunk[] readRawMeasureChunksInGroup(FileReader fileReader,
1:daa6465:       int startColumnIndex, int endColumnIndex) throws IOException;
1:2cf1104: 
1:2cf1104: }
============================================================================
author:ajantha-bhat
-------------------------------------------------------------------------------
commit:438b442
/////////////////////////////////////////////////////////////////////////
/////////////////////////////////////////////////////////////////////////
author:Raghunandan S
-------------------------------------------------------------------------------
commit:7ef9164
/////////////////////////////////////////////////////////////////////////
0:   protected BitSet getNullBitSet(org.apache.carbondata.format.PresenceMeta presentMetadataThrift) {
0:     final byte[] present_bit_stream = presentMetadataThrift.getPresent_bit_stream();
0:     if (null != present_bit_stream) {
0:       return BitSet
0:           .valueOf(compressor.unCompressByte(present_bit_stream));
1:     } else {
0:       return new BitSet(1);
1:     }
commit:7422690
/////////////////////////////////////////////////////////////////////////
author:Jacky Li
-------------------------------------------------------------------------------
commit:daa6465
/////////////////////////////////////////////////////////////////////////
1: import org.apache.carbondata.core.datastore.FileReader;
/////////////////////////////////////////////////////////////////////////
1:    * @param columnIndexRange blocks range to be read, columnIndexGroup[i] is one group, inside the
1:    *                         group, columnIndexGroup[i][0] is start column index,
1:    *                         and columnIndexGroup[i][1] is end column index
1:   public MeasureRawColumnChunk[] readRawMeasureChunks(FileReader fileReader,
1:       int[][] columnIndexRange) throws IOException {
1:     if (columnIndexRange.length == 0) {
1:     for (int i = 0; i < columnIndexRange.length - 1; i++) {
1:       groupChunk = readRawMeasureChunksInGroup(
1:           fileReader, columnIndexRange[i][0], columnIndexRange[i][1]);
1:       for (int j = columnIndexRange[i][0]; j <= columnIndexRange[i][1]; j++) {
1:     if (columnIndexRange[columnIndexRange.length - 1][0] == measureColumnChunkOffsets.size() - 1) {
1:       dataChunks[columnIndexRange[columnIndexRange.length - 1][0]] =
1:           readRawMeasureChunk(fileReader, columnIndexRange[columnIndexRange.length - 1][0]);
1:       groupChunk = readRawMeasureChunksInGroup(
1:           fileReader, columnIndexRange[columnIndexRange.length - 1][0],
1:           columnIndexRange[columnIndexRange.length - 1][1]);
1:       for (int j = columnIndexRange[columnIndexRange.length - 1][0];
1:            j <= columnIndexRange[columnIndexRange.length - 1][1]; j++) {
/////////////////////////////////////////////////////////////////////////
1:    * @param startColumnIndex first column index to be read
1:    * @param endColumnIndex   end column index to be read
1:   protected abstract MeasureRawColumnChunk[] readRawMeasureChunksInGroup(FileReader fileReader,
1:       int startColumnIndex, int endColumnIndex) throws IOException;
commit:e6a4f64
/////////////////////////////////////////////////////////////////////////
0: import org.apache.carbondata.core.datastore.compression.Compressor;
/////////////////////////////////////////////////////////////////////////
0:   protected BitSet getNullBitSet(
0:     Compressor compressor = CompressorFactory.getInstance().getCompressor();
0:     return BitSet.valueOf(
0:         compressor.unCompressByte(presentMetadataThrift.getPresent_bit_stream()));
author:jackylk
-------------------------------------------------------------------------------
commit:bc3e684
/////////////////////////////////////////////////////////////////////////
/////////////////////////////////////////////////////////////////////////
commit:7359601
/////////////////////////////////////////////////////////////////////////
0: import org.apache.carbondata.core.memory.MemoryException;
/////////////////////////////////////////////////////////////////////////
0:       DataChunk2 measureColumnChunk, int copyPoint) throws MemoryException {
commit:edda248
/////////////////////////////////////////////////////////////////////////
0: import org.apache.carbondata.core.datastore.page.ColumnPage;
0: import org.apache.carbondata.core.datastore.page.encoding.ColumnPageCodec;
0: import org.apache.carbondata.core.metadata.ValueEncoderMeta;
0: import org.apache.carbondata.core.util.CarbonUtil;
0: import org.apache.carbondata.format.DataChunk2;
/////////////////////////////////////////////////////////////////////////
0:   protected ColumnPage decodeMeasure(MeasureRawColumnChunk measureRawColumnChunk,
0:       DataChunk2 measureColumnChunk, int copyPoint) {
0:     // for measure, it should have only one ValueEncoderMeta
0:     assert (measureColumnChunk.getEncoder_meta().size() == 1);
0:     byte[] encodedMeta = measureColumnChunk.getEncoder_meta().get(0).array();
0:     ValueEncoderMeta meta = CarbonUtil.deserializeEncoderMetaV3(encodedMeta);
0:     ColumnPageCodec codec = strategy.createCodec(meta);
0:     byte[] rawData = measureRawColumnChunk.getRawData().array();
0:     return codec.decode(rawData, copyPoint, measureColumnChunk.data_page_length);
1:   }
commit:dc83b2a
/////////////////////////////////////////////////////////////////////////
1: 
1: 
author:ravipesala
-------------------------------------------------------------------------------
commit:9e064ee
/////////////////////////////////////////////////////////////////////////
0: import java.nio.ByteBuffer;
/////////////////////////////////////////////////////////////////////////
0:     List<ByteBuffer> encoder_meta = measureColumnChunk.getEncoder_meta();
0:     assert (encoder_meta.size() > 0);
0:     byte[] encodedMeta = encoder_meta.get(0).array();
0:     int scale = -1;
0:     int precision = -1;
0:     if (encoder_meta.size() > 1) {
0:       ByteBuffer decimalInfo = encoder_meta.get(1);
0:       scale = decimalInfo.getInt();
0:       precision = decimalInfo.getInt();
1:     }
0:     ColumnPageCodec codec = strategy.createCodec(meta, scale, precision);
author:kumarvishal
-------------------------------------------------------------------------------
commit:2cf1104
/////////////////////////////////////////////////////////////////////////
1: /*
1:  * Licensed to the Apache Software Foundation (ASF) under one or more
1:  * contributor license agreements.  See the NOTICE file distributed with
1:  * this work for additional information regarding copyright ownership.
1:  * The ASF licenses this file to You under the Apache License, Version 2.0
1:  * (the "License"); you may not use this file except in compliance with
1:  * the License.  You may obtain a copy of the License at
1:  *
1:  *    http://www.apache.org/licenses/LICENSE-2.0
1:  *
1:  * Unless required by applicable law or agreed to in writing, software
1:  * distributed under the License is distributed on an "AS IS" BASIS,
1:  * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
1:  * See the License for the specific language governing permissions and
1:  * limitations under the License.
1:  */
1: package org.apache.carbondata.core.datastore.chunk.reader.measure;
1: 
1: import java.io.IOException;
0: import java.util.BitSet;
1: import java.util.List;
1: 
0: import org.apache.carbondata.core.datastore.FileHolder;
1: import org.apache.carbondata.core.datastore.chunk.impl.MeasureRawColumnChunk;
0: import org.apache.carbondata.core.datastore.compression.CompressorFactory;
1: import org.apache.carbondata.core.metadata.blocklet.BlockletInfo;
0: import org.apache.carbondata.core.metadata.blocklet.datachunk.PresenceMeta;
1: 
1: /**
1:  * Abstract class for V2, V3 format measure column reader
1:  */
1: public abstract class AbstractMeasureChunkReaderV2V3Format extends AbstractMeasureChunkReader {
1: 
1:   /**
1:    * measure column chunks offset
1:    */
1:   protected List<Long> measureColumnChunkOffsets;
1: 
1:   /**
1:    * measure column chunks length
1:    */
1:   protected List<Integer> measureColumnChunkLength;
1: 
1:   public AbstractMeasureChunkReaderV2V3Format(final BlockletInfo blockletInfo,
1:       final String filePath) {
1:     super(filePath, blockletInfo.getNumberOfRows());
1:     this.measureColumnChunkOffsets = blockletInfo.getMeasureChunkOffsets();
1:     this.measureColumnChunkLength = blockletInfo.getMeasureChunksLength();
1:   }
1: 
1:   /**
1:    * Below method will be used to read the chunk based on block indexes
1:    * Reading logic of below method is: Except last column all the column chunk
1:    * can be read in group if not last column then read data of all the column
1:    * present in block index together then process it. For last column read is
1:    * separately and process
1:    *
1:    * @param fileReader   file reader to read the blocks from file
0:    * @param blockIndexes blocks range to be read
1:    * @return measure column chunks
1:    * @throws IOException
1:    */
0:   public MeasureRawColumnChunk[] readRawMeasureChunks(FileHolder fileReader, int[][] blockIndexes)
0:       throws IOException {
1:     // read the column chunk based on block index and add
1:     MeasureRawColumnChunk[] dataChunks =
1:         new MeasureRawColumnChunk[measureColumnChunkOffsets.size()];
0:     if (blockIndexes.length == 0) {
1:       return dataChunks;
1:     }
1:     MeasureRawColumnChunk[] groupChunk = null;
1:     int index = 0;
0:     for (int i = 0; i < blockIndexes.length - 1; i++) {
1:       index = 0;
0:       groupChunk = readRawMeasureChunksInGroup(fileReader, blockIndexes[i][0], blockIndexes[i][1]);
0:       for (int j = blockIndexes[i][0]; j <= blockIndexes[i][1]; j++) {
1:         dataChunks[j] = groupChunk[index++];
1:       }
1:     }
0:     if (blockIndexes[blockIndexes.length - 1][0] == measureColumnChunkOffsets.size() - 1) {
0:       dataChunks[blockIndexes[blockIndexes.length - 1][0]] =
0:           readRawMeasureChunk(fileReader, blockIndexes[blockIndexes.length - 1][0]);
0:     } else {
0:       groupChunk = readRawMeasureChunksInGroup(fileReader, blockIndexes[blockIndexes.length - 1][0],
0:           blockIndexes[blockIndexes.length - 1][1]);
1:       index = 0;
0:       for (int j = blockIndexes[blockIndexes.length - 1][0];
0:            j <= blockIndexes[blockIndexes.length - 1][1]; j++) {
1:         dataChunks[j] = groupChunk[index++];
1:       }
1:     }
1:     return dataChunks;
1:   }
1: 
1:   /**
0:    * Below method will be used to convert the thrift presence meta to wrapper
0:    * presence meta
1:    *
0:    * @param presentMetadataThrift
0:    * @return wrapper presence meta
1:    */
0:   protected PresenceMeta getPresenceMeta(
0:       org.apache.carbondata.format.PresenceMeta presentMetadataThrift) {
0:     PresenceMeta presenceMeta = new PresenceMeta();
0:     presenceMeta.setRepresentNullValues(presentMetadataThrift.isRepresents_presence());
0:     presenceMeta.setBitSet(BitSet.valueOf(CompressorFactory.getInstance().getCompressor()
0:         .unCompressByte(presentMetadataThrift.getPresent_bit_stream())));
0:     return presenceMeta;
1:   }
1: 
1:   /**
1:    * Below method will be used to read measure chunk data in group.
1:    * This method will be useful to avoid multiple IO while reading the
1:    * data from
1:    *
1:    * @param fileReader               file reader to read the data
0:    * @param startColumnBlockletIndex first column blocklet index to be read
0:    * @param endColumnBlockletIndex   end column blocklet index to be read
1:    * @return measure raw chunkArray
1:    * @throws IOException
1:    */
0:   protected abstract MeasureRawColumnChunk[] readRawMeasureChunksInGroup(FileHolder fileReader,
0:       int startColumnBlockletIndex, int endColumnBlockletIndex) throws IOException;
1: }
============================================================================