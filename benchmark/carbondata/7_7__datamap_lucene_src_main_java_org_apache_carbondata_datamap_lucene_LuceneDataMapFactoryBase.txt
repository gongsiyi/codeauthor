1:bbb1092: /*
1:bbb1092:  * Licensed to the Apache Software Foundation (ASF) under one or more
1:bbb1092:  * contributor license agreements.  See the NOTICE file distributed with
1:bbb1092:  * this work for additional information regarding copyright ownership.
1:bbb1092:  * The ASF licenses this file to You under the Apache License, Version 2.0
1:bbb1092:  * (the "License"); you may not use this file except in compliance with
1:bbb1092:  * the License.  You may obtain a copy of the License at
1:bbb1092:  *
1:bbb1092:  *    http://www.apache.org/licenses/LICENSE-2.0
1:bbb1092:  *
1:bbb1092:  * Unless required by applicable law or agreed to in writing, software
1:bbb1092:  * distributed under the License is distributed on an "AS IS" BASIS,
1:bbb1092:  * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
1:bbb1092:  * See the License for the specific language governing permissions and
1:bbb1092:  * limitations under the License.
2:bbb1092:  */
10:bbb1092: 
1:bbb1092: package org.apache.carbondata.datamap.lucene;
1:5229443: 
1:bbb1092: import java.io.IOException;
1:bbb1092: import java.util.ArrayList;
1:5229443: import java.util.Arrays;
1:bbb1092: import java.util.List;
1:bbb1092: import java.util.Objects;
1:bbb1092: 
1:bbb1092: import org.apache.carbondata.common.annotations.InterfaceAudience;
1:d23f7fa: import org.apache.carbondata.common.exceptions.sql.MalformedDataMapCommandException;
1:bbb1092: import org.apache.carbondata.common.logging.LogService;
1:bbb1092: import org.apache.carbondata.common.logging.LogServiceFactory;
1:bbb1092: import org.apache.carbondata.core.datamap.DataMapDistributable;
1:b338459: import org.apache.carbondata.core.datamap.DataMapLevel;
1:bbb1092: import org.apache.carbondata.core.datamap.DataMapMeta;
1:5229443: import org.apache.carbondata.core.datamap.DataMapStoreManager;
1:5397c05: import org.apache.carbondata.core.datamap.Segment;
1:5229443: import org.apache.carbondata.core.datamap.TableDataMap;
1:bbb1092: import org.apache.carbondata.core.datamap.dev.DataMap;
1:747be9b: import org.apache.carbondata.core.datamap.dev.DataMapBuilder;
1:bbb1092: import org.apache.carbondata.core.datamap.dev.DataMapFactory;
1:bbb1092: import org.apache.carbondata.core.datamap.dev.DataMapWriter;
1:cd7c210: import org.apache.carbondata.core.datastore.block.SegmentProperties;
1:860e144: import org.apache.carbondata.core.datastore.filesystem.CarbonFile;
1:5229443: import org.apache.carbondata.core.datastore.filesystem.CarbonFileFilter;
1:860e144: import org.apache.carbondata.core.datastore.impl.FileFactory;
1:bbb1092: import org.apache.carbondata.core.metadata.AbsoluteTableIdentifier;
1:d23f7fa: import org.apache.carbondata.core.metadata.datatype.DataTypes;
1:860e144: import org.apache.carbondata.core.metadata.encoder.Encoding;
1:bbb1092: import org.apache.carbondata.core.metadata.schema.table.CarbonTable;
1:bbb1092: import org.apache.carbondata.core.metadata.schema.table.DataMapSchema;
1:d23f7fa: import org.apache.carbondata.core.metadata.schema.table.column.CarbonColumn;
1:bbb1092: import org.apache.carbondata.core.scan.filter.intf.ExpressionType;
1:860e144: import org.apache.carbondata.core.statusmanager.SegmentStatusManager;
1:860e144: import org.apache.carbondata.core.util.CarbonUtil;
1:bbb1092: import org.apache.carbondata.core.util.path.CarbonTablePath;
1:bbb1092: import org.apache.carbondata.events.Event;
1:bbb1092: 
1:bbb1092: import org.apache.lucene.analysis.Analyzer;
1:bbb1092: import org.apache.lucene.analysis.standard.StandardAnalyzer;
1:bbb1092: 
1:860e144: /**
1:bbb1092:  * Base implementation for CG and FG lucene DataMapFactory.
1:860e144:  */
1:bbb1092: @InterfaceAudience.Internal
1:9db662a: abstract class LuceneDataMapFactoryBase<T extends DataMap> extends DataMapFactory<T> {
1:860e144: 
2:bbb1092:   /**
1:f184de8:    * Size of the cache to maintain in Lucene writer, if specified then it tries to aggregate the
1:f184de8:    * unique data till the cache limit and flush to Lucene.
1:f184de8:    * It is best suitable for low cardinality dimensions.
1:f184de8:    */
1:f184de8:   static final String FLUSH_CACHE = "flush_cache";
1:f184de8: 
1:f184de8:   /**
1:f184de8:    * By default it does not use any cache.
1:f184de8:    */
1:f184de8:   static final String FLUSH_CACHE_DEFAULT_SIZE = "-1";
1:f184de8: 
1:f184de8:   /**
1:f184de8:    * when made as true then store the data in blocklet wise in lucene , it means new folder will be
1:f184de8:    * created for each blocklet thus it eliminates storing on blockletid in lucene.
1:f184de8:    * And also it makes lucene small chuns of data
1:f184de8:    */
1:f184de8:   static final String SPLIT_BLOCKLET = "split_blocklet";
1:f184de8: 
1:f184de8:   /**
1:f184de8:    * By default it is false
1:f184de8:    */
1:f184de8:   static final String SPLIT_BLOCKLET_DEFAULT = "true";
1:f184de8:   /**
1:bbb1092:    * Logger
1:bbb1092:    */
1:bbb1092:   final LogService LOGGER = LogServiceFactory.getLogService(this.getClass().getName());
1:bbb1092: 
1:bbb1092:   /**
1:bbb1092:    * table's index columns
1:bbb1092:    */
1:bbb1092:   DataMapMeta dataMapMeta = null;
1:bbb1092: 
1:bbb1092:   /**
1:bbb1092:    * analyzer for lucene
1:bbb1092:    */
1:bbb1092:   Analyzer analyzer = null;
1:bbb1092: 
1:bbb1092:   /**
1:bbb1092:    * index name
1:bbb1092:    */
1:bbb1092:   String dataMapName = null;
1:bbb1092: 
1:bbb1092:   /**
1:bbb1092:    * table identifier
1:bbb1092:    */
1:bbb1092:   AbsoluteTableIdentifier tableIdentifier = null;
1:860e144: 
1:f184de8:   List<CarbonColumn> indexedCarbonColumns = null;
1:f184de8: 
1:f184de8:   int flushCacheSize;
1:f184de8: 
1:f184de8:   boolean storeBlockletWise;
1:f184de8: 
1:9db662a:   public LuceneDataMapFactoryBase(CarbonTable carbonTable, DataMapSchema dataMapSchema)
1:9db662a:       throws MalformedDataMapCommandException {
1:9db662a:     super(carbonTable, dataMapSchema);
1:860e144:     Objects.requireNonNull(carbonTable.getAbsoluteTableIdentifier());
1:bbb1092:     Objects.requireNonNull(dataMapSchema);
1:bbb1092: 
1:860e144:     this.tableIdentifier = carbonTable.getAbsoluteTableIdentifier();
1:bbb1092:     this.dataMapName = dataMapSchema.getDataMapName();
1:bbb1092: 
1:d23f7fa:     // validate DataMapSchema and get index columns
1:f184de8:     indexedCarbonColumns =  carbonTable.getIndexedColumns(dataMapSchema);;
1:f184de8:     flushCacheSize = validateAndGetWriteCacheSize(dataMapSchema);
1:f184de8:     storeBlockletWise = validateAndGetStoreBlockletWise(dataMapSchema);
1:bbb1092: 
1:bbb1092:     // add optimizedOperations
1:bbb1092:     List<ExpressionType> optimizedOperations = new ArrayList<ExpressionType>();
1:d23f7fa:     // optimizedOperations.add(ExpressionType.EQUALS);
1:d23f7fa:     // optimizedOperations.add(ExpressionType.GREATERTHAN);
1:d23f7fa:     // optimizedOperations.add(ExpressionType.GREATERTHAN_EQUALTO);
1:d23f7fa:     // optimizedOperations.add(ExpressionType.LESSTHAN);
1:d23f7fa:     // optimizedOperations.add(ExpressionType.LESSTHAN_EQUALTO);
1:d23f7fa:     // optimizedOperations.add(ExpressionType.NOT);
1:bbb1092:     optimizedOperations.add(ExpressionType.TEXT_MATCH);
1:f184de8:     this.dataMapMeta = new DataMapMeta(indexedCarbonColumns, optimizedOperations);
1:bbb1092:     // get analyzer
1:bbb1092:     // TODO: how to get analyzer ?
1:bbb1092:     analyzer = new StandardAnalyzer();
1:860e144:   }
1:860e144: 
1:f184de8:   public static int validateAndGetWriteCacheSize(DataMapSchema schema) {
1:f184de8:     String cacheStr = schema.getProperties().get(FLUSH_CACHE);
1:f184de8:     if (cacheStr == null) {
1:f184de8:       cacheStr = FLUSH_CACHE_DEFAULT_SIZE;
1:f184de8:     }
1:f184de8:     int cacheSize;
1:d8562e5:     try {
1:f184de8:       cacheSize = Integer.parseInt(cacheStr);
1:f184de8:     } catch (NumberFormatException e) {
1:f184de8:       cacheSize = -1;
1:f184de8:     }
1:f184de8:     return cacheSize;
1:f184de8:   }
1:f184de8: 
1:f184de8:   public static boolean validateAndGetStoreBlockletWise(DataMapSchema schema) {
1:f184de8:     String splitBlockletStr = schema.getProperties().get(SPLIT_BLOCKLET);
1:f184de8:     if (splitBlockletStr == null) {
1:f184de8:       splitBlockletStr = SPLIT_BLOCKLET_DEFAULT;
1:f184de8:     }
1:f184de8:     boolean splitBlockletWise;
1:f184de8:     try {
1:f184de8:       splitBlockletWise = Boolean.parseBoolean(splitBlockletStr);
1:f184de8:     } catch (NumberFormatException e) {
1:f184de8:       splitBlockletWise = true;
1:f184de8:     }
1:f184de8:     return splitBlockletWise;
1:f184de8:   }
1:860e144:   /**
1:860e144:    * this method will delete the datamap folders during drop datamap
1:860e144:    * @throws MalformedDataMapCommandException
1:860e144:    */
1:860e144:   private void deleteDatamap() throws MalformedDataMapCommandException {
1:860e144:     SegmentStatusManager ssm = new SegmentStatusManager(tableIdentifier);
1:f184de8:     try {
1:860e144:       List<Segment> validSegments = ssm.getValidAndInvalidSegments().getValidSegments();
1:860e144:       for (Segment segment : validSegments) {
1:d8562e5:         deleteDatamapData(segment);
1:860e144:       }
1:d8562e5:     } catch (IOException | RuntimeException ex) {
3:860e144:       throw new MalformedDataMapCommandException(
1:d8562e5:           "drop datamap failed, failed to delete datamap directory");
1:860e144:     }
6:bbb1092:   }
1:bbb1092: 
1:bbb1092:   /**
1:bbb1092:    * Return a new write for this datamap
1:bbb1092:    */
1:1fd3703:   @Override
1:cd7c210:   public DataMapWriter createWriter(Segment segment, String shardName,
1:cd7c210:       SegmentProperties segmentProperties) {
1:9db662a:     LOGGER.info("lucene data write to " + shardName);
1:9db662a:     return new LuceneDataMapWriter(getCarbonTable().getTablePath(), dataMapName,
1:f184de8:         dataMapMeta.getIndexedColumns(), segment, shardName, flushCacheSize,
1:f184de8:         storeBlockletWise);
1:9db662a:   }
1:9db662a: 
1:5397c05:   @Override
1:cd7c210:   public DataMapBuilder createBuilder(Segment segment, String shardName,
1:cd7c210:       SegmentProperties segmentProperties) {
1:747be9b:     return new LuceneDataMapBuilder(getCarbonTable().getTablePath(), dataMapName,
1:f184de8:         segment, shardName, dataMapMeta.getIndexedColumns(), flushCacheSize, storeBlockletWise);
1:bbb1092:   }
1:bbb1092: 
1:bbb1092:   /**
1:bbb1092:    * Get all distributable objects of a segmentid
1:bbb1092:    */
1:5397c05:   @Override
1:5397c05:   public List<DataMapDistributable> toDistributable(Segment segment) {
1:8b33ab2:     List<DataMapDistributable> lstDataMapDistribute = new ArrayList<>();
1:5229443:     CarbonFile[] indexDirs =
1:5229443:         getAllIndexDirs(tableIdentifier.getTablePath(), segment.getSegmentNo());
1:2018048:     if (segment.getFilteredIndexShardNames().size() == 0) {
1:2018048:       for (CarbonFile indexDir : indexDirs) {
1:60dfdd3:         DataMapDistributable luceneDataMapDistributable =
1:60dfdd3:             new LuceneDataMapDistributable(tableIdentifier.getTablePath(),
1:60dfdd3:                 indexDir.getAbsolutePath());
1:2018048:         lstDataMapDistribute.add(luceneDataMapDistributable);
1:2018048:       }
1:2018048:       return lstDataMapDistribute;
1:2018048:     }
1:860e144:     for (CarbonFile indexDir : indexDirs) {
1:8b33ab2:       // Filter out the tasks which are filtered through CG datamap.
1:b338459:       if (getDataMapLevel() != DataMapLevel.FG &&
1:b338459:           !segment.getFilteredIndexShardNames().contains(indexDir.getName())) {
1:8b33ab2:         continue;
1:8b33ab2:       }
1:2018048:       DataMapDistributable luceneDataMapDistributable = new LuceneDataMapDistributable(
1:2018048:           CarbonTablePath.getSegmentPath(tableIdentifier.getTablePath(), segment.getSegmentNo()),
1:2018048:           indexDir.getAbsolutePath());
1:860e144:       lstDataMapDistribute.add(luceneDataMapDistributable);
1:860e144:     }
1:bbb1092:     return lstDataMapDistribute;
1:bbb1092:   }
1:bbb1092: 
1:5397c05:   @Override
1:bbb1092:   public void fireEvent(Event event) {
1:bbb1092: 
1:bbb1092:   }
1:bbb1092: 
1:bbb1092:   /**
1:bbb1092:    * Clears datamap of the segment
1:bbb1092:    */
1:5397c05:   @Override
1:5397c05:   public void clear(Segment segment) {
1:bbb1092: 
1:bbb1092:   }
1:bbb1092: 
1:bbb1092:   /**
1:bbb1092:    * Clear all datamaps from memory
1:bbb1092:    */
1:5397c05:   @Override
1:bbb1092:   public void clear() {
1:bbb1092: 
1:bbb1092:   }
1:bbb1092: 
1:747be9b:   @Override
1:1fd3703:   public void deleteDatamapData(Segment segment) throws IOException {
1:860e144:     try {
1:d8562e5:       String segmentId = segment.getSegmentNo();
1:d8562e5:       String datamapPath = CarbonTablePath
1:d8562e5:           .getDataMapStorePath(tableIdentifier.getTablePath(), segmentId, dataMapName);
1:d8562e5:       if (FileFactory.isFileExist(datamapPath)) {
1:d8562e5:         CarbonFile file = FileFactory.getCarbonFile(datamapPath,
1:d8562e5:             FileFactory.getFileType(datamapPath));
1:d8562e5:         CarbonUtil.deleteFoldersAndFilesSilent(file);
1:d8562e5:       }
1:1fd3703:     } catch (InterruptedException ex) {
1:1fd3703:       throw new IOException("drop datamap failed, failed to delete datamap directory");
1:d8562e5:     }
1:d8562e5:   }
1:d8562e5: 
1:1fd3703:   @Override
1:1fd3703:   public void deleteDatamapData() {
1:860e144:     try {
1:860e144:       deleteDatamap();
1:860e144:     } catch (MalformedDataMapCommandException ex) {
1:860e144:       LOGGER.error(ex, "failed to delete datamap directory ");
1:860e144:     }
1:860e144:   }
1:860e144: 
1:bbb1092:   /**
1:bbb1092:    * Return metadata of this datamap
1:bbb1092:    */
1:bbb1092:   public DataMapMeta getMeta() {
1:bbb1092:     return dataMapMeta;
1:860e144:   }
1:5229443: 
1:5229443:   /**
1:5229443:    * returns all the directories of lucene index files for query
1:5229443:    * @param tablePath
1:5229443:    * @param segmentId
1:5229443:    * @return
1:5229443:    */
1:5229443:   private CarbonFile[] getAllIndexDirs(String tablePath, String segmentId) {
1:5229443:     List<CarbonFile> indexDirs = new ArrayList<>();
1:5229443:     List<TableDataMap> dataMaps = new ArrayList<>();
1:5229443:     try {
1:5229443:       // there can be multiple lucene datamaps present on a table, so get all datamaps and form
1:5229443:       // the path till the index file directories in all datamaps folders present in each segment
1:9db662a:       dataMaps = DataMapStoreManager.getInstance().getAllDataMap(getCarbonTable());
1:5229443:     } catch (IOException ex) {
1:5229443:       LOGGER.error("failed to get datamaps");
1:5229443:     }
1:5229443:     if (dataMaps.size() > 0) {
1:5229443:       for (TableDataMap dataMap : dataMaps) {
1:46f0c85:         if (dataMap.getDataMapSchema().getDataMapName().equals(this.dataMapName)) {
1:46f0c85:           List<CarbonFile> indexFiles;
1:46f0c85:           String dmPath = CarbonTablePath.getDataMapStorePath(tablePath, segmentId,
1:46f0c85:               dataMap.getDataMapSchema().getDataMapName());
1:46f0c85:           FileFactory.FileType fileType = FileFactory.getFileType(dmPath);
1:46f0c85:           final CarbonFile dirPath = FileFactory.getCarbonFile(dmPath, fileType);
1:46f0c85:           indexFiles = Arrays.asList(dirPath.listFiles(new CarbonFileFilter() {
1:46f0c85:             @Override
1:46f0c85:             public boolean accept(CarbonFile file) {
1:46f0c85:               return file.isDirectory();
1:46f0c85:             }
1:46f0c85:           }));
1:46f0c85:           indexDirs.addAll(indexFiles);
1:46f0c85:         }
1:5229443:       }
1:5229443:     }
1:5229443:     return indexDirs.toArray(new CarbonFile[0]);
1:5229443:   }
1:9db662a: 
1:9db662a:   /**
1:9db662a:    * Further validate whether it is string column and dictionary column.
1:9db662a:    * Currently only string and non-dictionary column is supported for Lucene DataMap
1:9db662a:    */
1:9db662a:   @Override
1:9db662a:   public void validate() throws MalformedDataMapCommandException {
1:9db662a:     super.validate();
1:9db662a:     List<CarbonColumn> indexColumns = getCarbonTable().getIndexedColumns(getDataMapSchema());
1:9db662a: 
1:9db662a:     for (CarbonColumn column : indexColumns) {
1:9db662a:       if (column.getDataType() != DataTypes.STRING) {
1:9db662a:         throw new MalformedDataMapCommandException(String.format(
1:9db662a:             "Only String column is supported, column '%s' is %s type. ",
1:9db662a:             column.getColName(), column.getDataType()));
1:860e144:       } else if (column.getEncoder().contains(Encoding.DICTIONARY)) {
1:9db662a:         throw new MalformedDataMapCommandException(String.format(
1:9db662a:             "Dictionary column is not supported, column '%s' is dictionary column",
1:9db662a:             column.getColName()));
1:5229443:       }
1:9db662a:     }
1:9db662a:   }
1:9db662a: }
============================================================================
author:Manhua
-------------------------------------------------------------------------------
commit:46f0c85
/////////////////////////////////////////////////////////////////////////
1:         if (dataMap.getDataMapSchema().getDataMapName().equals(this.dataMapName)) {
1:           List<CarbonFile> indexFiles;
1:           String dmPath = CarbonTablePath.getDataMapStorePath(tablePath, segmentId,
1:               dataMap.getDataMapSchema().getDataMapName());
1:           FileFactory.FileType fileType = FileFactory.getFileType(dmPath);
1:           final CarbonFile dirPath = FileFactory.getCarbonFile(dmPath, fileType);
1:           indexFiles = Arrays.asList(dirPath.listFiles(new CarbonFileFilter() {
1:             @Override
1:             public boolean accept(CarbonFile file) {
1:               return file.isDirectory();
1:             }
1:           }));
1:           indexDirs.addAll(indexFiles);
1:         }
author:xuchuanyin
-------------------------------------------------------------------------------
commit:1fd3703
/////////////////////////////////////////////////////////////////////////
1:   @Override
1:   public void deleteDatamapData(Segment segment) throws IOException {
/////////////////////////////////////////////////////////////////////////
1:     } catch (InterruptedException ex) {
1:       throw new IOException("drop datamap failed, failed to delete datamap directory");
1:   @Override
1:   public void deleteDatamapData() {
commit:d8562e5
/////////////////////////////////////////////////////////////////////////
1:         deleteDatamapData(segment);
1:     } catch (IOException | RuntimeException ex) {
/////////////////////////////////////////////////////////////////////////
0:   @Override public void deleteDatamapData(Segment segment) {
1:     try {
1:       String segmentId = segment.getSegmentNo();
1:       String datamapPath = CarbonTablePath
1:           .getDataMapStorePath(tableIdentifier.getTablePath(), segmentId, dataMapName);
1:       if (FileFactory.isFileExist(datamapPath)) {
1:         CarbonFile file = FileFactory.getCarbonFile(datamapPath,
1:             FileFactory.getFileType(datamapPath));
1:         CarbonUtil.deleteFoldersAndFilesSilent(file);
1:       }
0:     } catch (IOException | InterruptedException ex) {
0:       throw new RuntimeException(
1:           "drop datamap failed, failed to delete datamap directory");
1:     }
1:   }
1: 
commit:cd7c210
/////////////////////////////////////////////////////////////////////////
1: import org.apache.carbondata.core.datastore.block.SegmentProperties;
/////////////////////////////////////////////////////////////////////////
1:   public DataMapWriter createWriter(Segment segment, String shardName,
1:       SegmentProperties segmentProperties) {
/////////////////////////////////////////////////////////////////////////
1:   public DataMapBuilder createBuilder(Segment segment, String shardName,
1:       SegmentProperties segmentProperties) {
commit:5397c05
/////////////////////////////////////////////////////////////////////////
1: import org.apache.carbondata.core.datamap.Segment;
/////////////////////////////////////////////////////////////////////////
1:   @Override
0:   public DataMapWriter createWriter(Segment segment, String writeDirectoryPath) {
0:         tableIdentifier, dataMapName, segment, writeDirectoryPath, true);
1:   @Override
1:   public List<DataMapDistributable> toDistributable(Segment segment) {
0:         CarbonTablePath.getSegmentPath(tableIdentifier.getTablePath(), segment.getSegmentNo()));
1:   @Override
/////////////////////////////////////////////////////////////////////////
1:   @Override
1:   public void clear(Segment segment) {
1:   @Override
author:ravipesala
-------------------------------------------------------------------------------
commit:60dfdd3
/////////////////////////////////////////////////////////////////////////
/////////////////////////////////////////////////////////////////////////
0:         String datamapPath = CarbonTablePath
0:             .getDataMapStorePath(tableIdentifier.getTablePath(), segmentId, dataMapName);
/////////////////////////////////////////////////////////////////////////
1:         DataMapDistributable luceneDataMapDistributable =
1:             new LuceneDataMapDistributable(tableIdentifier.getTablePath(),
1:                 indexDir.getAbsolutePath());
/////////////////////////////////////////////////////////////////////////
0:         String dmPath = CarbonTablePath
0:             .getDataMapStorePath(tablePath, segmentId, dataMap.getDataMapSchema().getDataMapName());
commit:f184de8
/////////////////////////////////////////////////////////////////////////
1:    * Size of the cache to maintain in Lucene writer, if specified then it tries to aggregate the
1:    * unique data till the cache limit and flush to Lucene.
1:    * It is best suitable for low cardinality dimensions.
1:    */
1:   static final String FLUSH_CACHE = "flush_cache";
1: 
1:   /**
1:    * By default it does not use any cache.
1:    */
1:   static final String FLUSH_CACHE_DEFAULT_SIZE = "-1";
1: 
1:   /**
1:    * when made as true then store the data in blocklet wise in lucene , it means new folder will be
1:    * created for each blocklet thus it eliminates storing on blockletid in lucene.
1:    * And also it makes lucene small chuns of data
1:    */
1:   static final String SPLIT_BLOCKLET = "split_blocklet";
1: 
1:   /**
1:    * By default it is false
1:    */
1:   static final String SPLIT_BLOCKLET_DEFAULT = "true";
1:   /**
/////////////////////////////////////////////////////////////////////////
1:   List<CarbonColumn> indexedCarbonColumns = null;
1: 
1:   int flushCacheSize;
1: 
1:   boolean storeBlockletWise;
1: 
/////////////////////////////////////////////////////////////////////////
1:     indexedCarbonColumns =  carbonTable.getIndexedColumns(dataMapSchema);;
1:     flushCacheSize = validateAndGetWriteCacheSize(dataMapSchema);
1:     storeBlockletWise = validateAndGetStoreBlockletWise(dataMapSchema);
/////////////////////////////////////////////////////////////////////////
1:     this.dataMapMeta = new DataMapMeta(indexedCarbonColumns, optimizedOperations);
1:   public static int validateAndGetWriteCacheSize(DataMapSchema schema) {
1:     String cacheStr = schema.getProperties().get(FLUSH_CACHE);
1:     if (cacheStr == null) {
1:       cacheStr = FLUSH_CACHE_DEFAULT_SIZE;
1:     }
1:     int cacheSize;
1:     try {
1:       cacheSize = Integer.parseInt(cacheStr);
1:     } catch (NumberFormatException e) {
1:       cacheSize = -1;
1:     }
1:     return cacheSize;
1:   }
1: 
1:   public static boolean validateAndGetStoreBlockletWise(DataMapSchema schema) {
1:     String splitBlockletStr = schema.getProperties().get(SPLIT_BLOCKLET);
1:     if (splitBlockletStr == null) {
1:       splitBlockletStr = SPLIT_BLOCKLET_DEFAULT;
1:     }
1:     boolean splitBlockletWise;
1:     try {
1:       splitBlockletWise = Boolean.parseBoolean(splitBlockletStr);
1:     } catch (NumberFormatException e) {
1:       splitBlockletWise = true;
1:     }
1:     return splitBlockletWise;
1:   }
/////////////////////////////////////////////////////////////////////////
1:         dataMapMeta.getIndexedColumns(), segment, shardName, flushCacheSize,
1:         storeBlockletWise);
1:         segment, shardName, dataMapMeta.getIndexedColumns(), flushCacheSize, storeBlockletWise);
commit:8b33ab2
/////////////////////////////////////////////////////////////////////////
1:     List<DataMapDistributable> lstDataMapDistribute = new ArrayList<>();
1:       // Filter out the tasks which are filtered through CG datamap.
0:       if (!segment.getFilteredIndexShardNames().contains(indexDir.getName())) {
1:         continue;
1:       }
author:xubo245
-------------------------------------------------------------------------------
commit:b338459
/////////////////////////////////////////////////////////////////////////
1: import org.apache.carbondata.core.datamap.DataMapLevel;
/////////////////////////////////////////////////////////////////////////
1:       if (getDataMapLevel() != DataMapLevel.FG &&
1:           !segment.getFilteredIndexShardNames().contains(indexDir.getName())) {
author:akashrn5
-------------------------------------------------------------------------------
commit:2018048
/////////////////////////////////////////////////////////////////////////
1:     if (segment.getFilteredIndexShardNames().size() == 0) {
1:       for (CarbonFile indexDir : indexDirs) {
1:         DataMapDistributable luceneDataMapDistributable = new LuceneDataMapDistributable(
1:             CarbonTablePath.getSegmentPath(tableIdentifier.getTablePath(), segment.getSegmentNo()),
1:             indexDir.getAbsolutePath());
1:         lstDataMapDistribute.add(luceneDataMapDistributable);
1:       }
1:       return lstDataMapDistribute;
1:     }
commit:5229443
/////////////////////////////////////////////////////////////////////////
1: import java.util.Arrays;
/////////////////////////////////////////////////////////////////////////
1: import org.apache.carbondata.core.datamap.DataMapStoreManager;
1: import org.apache.carbondata.core.datamap.TableDataMap;
1: import org.apache.carbondata.core.datastore.filesystem.CarbonFileFilter;
/////////////////////////////////////////////////////////////////////////
0:   CarbonTable carbonTable = null;
1: 
/////////////////////////////////////////////////////////////////////////
0:     this.carbonTable = carbonTable;
/////////////////////////////////////////////////////////////////////////
0:   @Override public List<DataMapDistributable> toDistributable(Segment segment) {
1:     CarbonFile[] indexDirs =
1:         getAllIndexDirs(tableIdentifier.getTablePath(), segment.getSegmentNo());
/////////////////////////////////////////////////////////////////////////
1: 
1:   /**
1:    * returns all the directories of lucene index files for query
1:    * @param tablePath
1:    * @param segmentId
1:    * @return
1:    */
1:   private CarbonFile[] getAllIndexDirs(String tablePath, String segmentId) {
1:     List<CarbonFile> indexDirs = new ArrayList<>();
1:     List<TableDataMap> dataMaps = new ArrayList<>();
1:     try {
1:       // there can be multiple lucene datamaps present on a table, so get all datamaps and form
1:       // the path till the index file directories in all datamaps folders present in each segment
0:       dataMaps = DataMapStoreManager.getInstance().getAllDataMap(carbonTable);
1:     } catch (IOException ex) {
1:       LOGGER.error("failed to get datamaps");
1:     }
1:     if (dataMaps.size() > 0) {
1:       for (TableDataMap dataMap : dataMaps) {
0:         List<CarbonFile> indexFiles;
0:         String dmPath =
0:             CarbonTablePath.getSegmentPath(tablePath, segmentId) + File.separator + dataMap
0:                 .getDataMapSchema().getDataMapName();
0:         FileFactory.FileType fileType = FileFactory.getFileType(dmPath);
0:         final CarbonFile dirPath = FileFactory.getCarbonFile(dmPath, fileType);
0:         indexFiles = Arrays.asList(dirPath.listFiles(new CarbonFileFilter() {
0:           @Override public boolean accept(CarbonFile file) {
0:             return file.isDirectory();
1:           }
0:         }));
0:         indexDirs.addAll(indexFiles);
1:       }
1:     }
1:     return indexDirs.toArray(new CarbonFile[0]);
1:   }
commit:860e144
/////////////////////////////////////////////////////////////////////////
0: import java.io.File;
/////////////////////////////////////////////////////////////////////////
1: import org.apache.carbondata.core.datastore.filesystem.CarbonFile;
1: import org.apache.carbondata.core.datastore.impl.FileFactory;
1: import org.apache.carbondata.core.metadata.encoder.Encoding;
1: import org.apache.carbondata.core.statusmanager.SegmentStatusManager;
1: import org.apache.carbondata.core.util.CarbonUtil;
/////////////////////////////////////////////////////////////////////////
1:   /**
0:    * indexed carbon columns for lucene
1:    */
0:   List<String> indexedCarbonColumns = null;
1: 
1: 
0:   public void init(CarbonTable carbonTable, DataMapSchema dataMapSchema)
1:     Objects.requireNonNull(carbonTable.getAbsoluteTableIdentifier());
1:     this.tableIdentifier = carbonTable.getAbsoluteTableIdentifier();
/////////////////////////////////////////////////////////////////////////
0:     indexedCarbonColumns = new ArrayList<>(textColumns.length);
/////////////////////////////////////////////////////////////////////////
1:       } else if (column.getEncoder().contains(Encoding.DICTIONARY)) {
1:         throw new MalformedDataMapCommandException(
0:             "TEXT_COLUMNS cannot contain dictionary column " + column.getColName());
0:       indexedCarbonColumns.add(column.getColName());
0:     return indexedCarbonColumns;
1:   }
1: 
1:   /**
1:    * this method will delete the datamap folders during drop datamap
1:    * @throws MalformedDataMapCommandException
1:    */
1:   private void deleteDatamap() throws MalformedDataMapCommandException {
1:     SegmentStatusManager ssm = new SegmentStatusManager(tableIdentifier);
1:     try {
1:       List<Segment> validSegments = ssm.getValidAndInvalidSegments().getValidSegments();
1:       for (Segment segment : validSegments) {
0:         String segmentId = segment.getSegmentNo();
0:         String datamapPath =
0:             CarbonTablePath.getSegmentPath(tableIdentifier.getTablePath(), segmentId)
0:                 + File.separator + dataMapName;
0:         if (FileFactory.isFileExist(datamapPath)) {
0:           CarbonFile file =
0:               FileFactory.getCarbonFile(datamapPath, FileFactory.getFileType(datamapPath));
0:           CarbonUtil.deleteFoldersAndFilesSilent(file);
1:         }
1:       }
0:     } catch (IOException ex) {
1:       throw new MalformedDataMapCommandException(
0:           "drop datamap failed, failed to delete datamap directory");
0:     } catch (InterruptedException ex) {
1:       throw new MalformedDataMapCommandException(
0:           "drop datamap failed, failed to delete datamap directory");
1:     }
/////////////////////////////////////////////////////////////////////////
0:     return new LuceneDataMapWriter(tableIdentifier, dataMapName, segment, writeDirectoryPath, true,
0:         indexedCarbonColumns);
/////////////////////////////////////////////////////////////////////////
0:     CarbonFile[] indexDirs = LuceneDataMapWriter
0:         .getAllIndexDirs(tableIdentifier.getTablePath(), segment.getSegmentNo(), dataMapName);
1:     for (CarbonFile indexDir : indexDirs) {
0:       DataMapDistributable luceneDataMapDistributable = new LuceneDataMapDistributable(
0:           CarbonTablePath.getSegmentPath(tableIdentifier.getTablePath(), segment.getSegmentNo()),
0:           indexDir.getAbsolutePath());
1:       lstDataMapDistribute.add(luceneDataMapDistributable);
1:     }
/////////////////////////////////////////////////////////////////////////
0:   @Override public void deleteDatamapData() {
1:     try {
1:       deleteDatamap();
1:     } catch (MalformedDataMapCommandException ex) {
1:       LOGGER.error(ex, "failed to delete datamap directory ");
1:     }
1:   }
1: 
author:Jacky Li
-------------------------------------------------------------------------------
commit:747be9b
/////////////////////////////////////////////////////////////////////////
1: import org.apache.carbondata.core.datamap.dev.DataMapBuilder;
/////////////////////////////////////////////////////////////////////////
0:   public DataMapBuilder createBuilder(Segment segment, String shardName) {
1:     return new LuceneDataMapBuilder(getCarbonTable().getTablePath(), dataMapName,
1:   @Override
0:   public List<DataMapDistributable> toDistributable(Segment segment) {
commit:9db662a
/////////////////////////////////////////////////////////////////////////
0: import org.apache.carbondata.core.datamap.dev.DataMapRefresher;
/////////////////////////////////////////////////////////////////////////
/////////////////////////////////////////////////////////////////////////
1: abstract class LuceneDataMapFactoryBase<T extends DataMap> extends DataMapFactory<T> {
/////////////////////////////////////////////////////////////////////////
1:   public LuceneDataMapFactoryBase(CarbonTable carbonTable, DataMapSchema dataMapSchema)
1:       throws MalformedDataMapCommandException {
1:     super(carbonTable, dataMapSchema);
0:     List<CarbonColumn> indexedColumns =  carbonTable.getIndexedColumns(dataMapSchema);
/////////////////////////////////////////////////////////////////////////
/////////////////////////////////////////////////////////////////////////
0:   public DataMapWriter createWriter(Segment segment, String shardName) {
1:     LOGGER.info("lucene data write to " + shardName);
1:     return new LuceneDataMapWriter(getCarbonTable().getTablePath(), dataMapName,
0:         dataMapMeta.getIndexedColumns(), segment, shardName, true);
1:   }
1: 
1:   @Override
0:   public DataMapRefresher createRefresher(Segment segment, String shardName) {
0:     return new LuceneDataMapRefresher(getCarbonTable().getTablePath(), dataMapName,
0:         segment, shardName, dataMapMeta.getIndexedColumns());
/////////////////////////////////////////////////////////////////////////
1:       dataMaps = DataMapStoreManager.getInstance().getAllDataMap(getCarbonTable());
/////////////////////////////////////////////////////////////////////////
1: 
1:   /**
1:    * Further validate whether it is string column and dictionary column.
1:    * Currently only string and non-dictionary column is supported for Lucene DataMap
1:    */
0:   @Override
1:   public void validate() throws MalformedDataMapCommandException {
1:     super.validate();
1:     List<CarbonColumn> indexColumns = getCarbonTable().getIndexedColumns(getDataMapSchema());
1: 
1:     for (CarbonColumn column : indexColumns) {
1:       if (column.getDataType() != DataTypes.STRING) {
1:         throw new MalformedDataMapCommandException(String.format(
1:             "Only String column is supported, column '%s' is %s type. ",
1:             column.getColName(), column.getDataType()));
0:       } else if (column.getEncoder().contains(Encoding.DICTIONARY)) {
1:         throw new MalformedDataMapCommandException(String.format(
1:             "Dictionary column is not supported, column '%s' is dictionary column",
1:             column.getColName()));
1:       }
1:     }
1:   }
commit:bbb1092
/////////////////////////////////////////////////////////////////////////
1: /*
1:  * Licensed to the Apache Software Foundation (ASF) under one or more
1:  * contributor license agreements.  See the NOTICE file distributed with
1:  * this work for additional information regarding copyright ownership.
1:  * The ASF licenses this file to You under the Apache License, Version 2.0
1:  * (the "License"); you may not use this file except in compliance with
1:  * the License.  You may obtain a copy of the License at
1:  *
1:  *    http://www.apache.org/licenses/LICENSE-2.0
1:  *
1:  * Unless required by applicable law or agreed to in writing, software
1:  * distributed under the License is distributed on an "AS IS" BASIS,
1:  * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
1:  * See the License for the specific language governing permissions and
1:  * limitations under the License.
1:  */
1: 
1: package org.apache.carbondata.datamap.lucene;
1: 
1: import java.io.IOException;
1: import java.util.ArrayList;
1: import java.util.List;
1: import java.util.Objects;
1: 
1: import org.apache.carbondata.common.annotations.InterfaceAudience;
1: import org.apache.carbondata.common.logging.LogService;
1: import org.apache.carbondata.common.logging.LogServiceFactory;
1: import org.apache.carbondata.core.datamap.DataMapDistributable;
1: import org.apache.carbondata.core.datamap.DataMapMeta;
1: import org.apache.carbondata.core.datamap.dev.DataMap;
1: import org.apache.carbondata.core.datamap.dev.DataMapFactory;
1: import org.apache.carbondata.core.datamap.dev.DataMapWriter;
1: import org.apache.carbondata.core.metadata.AbsoluteTableIdentifier;
0: import org.apache.carbondata.core.metadata.CarbonMetadata;
1: import org.apache.carbondata.core.metadata.schema.table.CarbonTable;
1: import org.apache.carbondata.core.metadata.schema.table.DataMapSchema;
0: import org.apache.carbondata.core.metadata.schema.table.TableInfo;
0: import org.apache.carbondata.core.metadata.schema.table.column.ColumnSchema;
1: import org.apache.carbondata.core.scan.filter.intf.ExpressionType;
1: import org.apache.carbondata.core.util.path.CarbonTablePath;
1: import org.apache.carbondata.events.Event;
1: 
1: import org.apache.lucene.analysis.Analyzer;
1: import org.apache.lucene.analysis.standard.StandardAnalyzer;
1: 
1: /**
1:  * Base implementation for CG and FG lucene DataMapFactory.
1:  */
1: @InterfaceAudience.Internal
0: abstract class LuceneDataMapFactoryBase<T extends DataMap> implements DataMapFactory<T> {
1: 
1:   /**
1:    * Logger
1:    */
1:   final LogService LOGGER = LogServiceFactory.getLogService(this.getClass().getName());
1: 
1:   /**
1:    * table's index columns
1:    */
1:   DataMapMeta dataMapMeta = null;
1: 
1:   /**
1:    * analyzer for lucene
1:    */
1:   Analyzer analyzer = null;
1: 
1:   /**
1:    * index name
1:    */
1:   String dataMapName = null;
1: 
1:   /**
1:    * table identifier
1:    */
1:   AbsoluteTableIdentifier tableIdentifier = null;
1: 
0:   @Override
0:   public void init(AbsoluteTableIdentifier identifier, DataMapSchema dataMapSchema)
0:       throws IOException {
0:     Objects.requireNonNull(identifier);
1:     Objects.requireNonNull(dataMapSchema);
1: 
0:     this.tableIdentifier = identifier;
1:     this.dataMapName = dataMapSchema.getDataMapName();
1: 
0:     // get carbonmetadata from carbonmetadata instance
0:     CarbonMetadata carbonMetadata = CarbonMetadata.getInstance();
1: 
0:     String tableUniqueName = identifier.getCarbonTableIdentifier().getTableUniqueName();
1: 
0:     // get carbon table
0:     CarbonTable carbonTable = carbonMetadata.getCarbonTable(tableUniqueName);
0:     if (carbonTable == null) {
0:       String errorMessage =
0:           String.format("failed to get carbon table with name %s", tableUniqueName);
0:       LOGGER.error(errorMessage);
0:       throw new IOException(errorMessage);
1:     }
1: 
0:     TableInfo tableInfo = carbonTable.getTableInfo();
0:     List<ColumnSchema> lstCoumnSchemas = tableInfo.getFactTable().getListOfColumns();
1: 
0:     // currently add all columns into lucene indexer
0:     // TODO:only add index columns
0:     List<String> indexedColumns = new ArrayList<String>();
0:     for (ColumnSchema columnSchema : lstCoumnSchemas) {
0:       if (!columnSchema.isInvisible()) {
0:         indexedColumns.add(columnSchema.getColumnName());
1:       }
1:     }
1: 
0:     // get indexed columns
0:     //    Map<String, String> properties = dataMapSchema.getProperties();
0:     //    String columns = properties.get("text_column");
0:     //    if (columns != null) {
0:     //      String[] columnArray = columns.split(CarbonCommonConstants.COMMA, -1);
0:     //      Collections.addAll(indexedColumns, columnArray);
0:     //    }
1: 
1:     // add optimizedOperations
1:     List<ExpressionType> optimizedOperations = new ArrayList<ExpressionType>();
0:     //    optimizedOperations.add(ExpressionType.EQUALS);
0:     //    optimizedOperations.add(ExpressionType.GREATERTHAN);
0:     //    optimizedOperations.add(ExpressionType.GREATERTHAN_EQUALTO);
0:     //    optimizedOperations.add(ExpressionType.LESSTHAN);
0:     //    optimizedOperations.add(ExpressionType.LESSTHAN_EQUALTO);
0:     //    optimizedOperations.add(ExpressionType.NOT);
1:     optimizedOperations.add(ExpressionType.TEXT_MATCH);
0:     this.dataMapMeta = new DataMapMeta(indexedColumns, optimizedOperations);
1: 
1:     // get analyzer
1:     // TODO: how to get analyzer ?
1:     analyzer = new StandardAnalyzer();
1:   }
1: 
1:   /**
1:    * Return a new write for this datamap
1:    */
0:   public DataMapWriter createWriter(String segmentId, String writeDirectoryPath) {
0:     LOGGER.info("lucene data write to " + writeDirectoryPath);
0:     return new LuceneDataMapWriter(
0:         tableIdentifier, dataMapName, segmentId, writeDirectoryPath, true);
1:   }
1: 
1:   /**
1:    * Get all distributable objects of a segmentid
1:    */
0:   public List<DataMapDistributable> toDistributable(String segmentId) {
0:     List<DataMapDistributable> lstDataMapDistribute = new ArrayList<DataMapDistributable>();
0:     DataMapDistributable luceneDataMapDistributable = new LuceneDataMapDistributable(
0:         CarbonTablePath.getSegmentPath(tableIdentifier.getTablePath(), segmentId));
0:     lstDataMapDistribute.add(luceneDataMapDistributable);
1:     return lstDataMapDistribute;
1:   }
1: 
1:   public void fireEvent(Event event) {
1: 
1:   }
1: 
1:   /**
1:    * Clears datamap of the segment
1:    */
0:   public void clear(String segmentId) {
1: 
1:   }
1: 
1:   /**
1:    * Clear all datamaps from memory
1:    */
1:   public void clear() {
1: 
1:   }
1: 
1:   /**
1:    * Return metadata of this datamap
1:    */
1:   public DataMapMeta getMeta() {
1:     return dataMapMeta;
1:   }
1: }
author:QiangCai
-------------------------------------------------------------------------------
commit:21c5fb1
/////////////////////////////////////////////////////////////////////////
0:   public static List<String> validateAndGetIndexedColumns(DataMapSchema dataMapSchema,
/////////////////////////////////////////////////////////////////////////
0:     List<String> indexedCarbonColumns = new ArrayList<>(textColumns.length);
commit:d23f7fa
/////////////////////////////////////////////////////////////////////////
1: import org.apache.carbondata.common.exceptions.sql.MalformedDataMapCommandException;
/////////////////////////////////////////////////////////////////////////
1: import org.apache.carbondata.core.metadata.datatype.DataTypes;
1: import org.apache.carbondata.core.metadata.schema.table.column.CarbonColumn;
0: import org.apache.commons.lang.StringUtils;
/////////////////////////////////////////////////////////////////////////
0:   static final String TEXT_COLUMNS = "text_columns";
0: 
/////////////////////////////////////////////////////////////////////////
0:       throws IOException, MalformedDataMapCommandException {
/////////////////////////////////////////////////////////////////////////
1:     // validate DataMapSchema and get index columns
0:     List<String> indexedColumns =  validateAndGetIndexedColumns(dataMapSchema, carbonTable);
1:     // optimizedOperations.add(ExpressionType.EQUALS);
1:     // optimizedOperations.add(ExpressionType.GREATERTHAN);
1:     // optimizedOperations.add(ExpressionType.GREATERTHAN_EQUALTO);
1:     // optimizedOperations.add(ExpressionType.LESSTHAN);
1:     // optimizedOperations.add(ExpressionType.LESSTHAN_EQUALTO);
1:     // optimizedOperations.add(ExpressionType.NOT);
/////////////////////////////////////////////////////////////////////////
0:    * validate Lucene DataMap
0:    * 1. require TEXT_COLUMNS property
0:    * 2. TEXT_COLUMNS can't contains illegal argument(empty, blank)
0:    * 3. TEXT_COLUMNS can't contains duplicate same columns
0:    * 4. TEXT_COLUMNS should be exists in table columns
0:    * 5. TEXT_COLUMNS support only String DataType columns
0:    */
0:   private List<String> validateAndGetIndexedColumns(DataMapSchema dataMapSchema,
0:       CarbonTable carbonTable) throws MalformedDataMapCommandException {
0:     String textColumnsStr = dataMapSchema.getProperties().get(TEXT_COLUMNS);
0:     if (textColumnsStr == null || StringUtils.isBlank(textColumnsStr)) {
0:       throw new MalformedDataMapCommandException(
0:           "Lucene DataMap require proper TEXT_COLUMNS property.");
0:     }
0:     String[] textColumns = textColumnsStr.split(",", -1);
0:     for (int i = 0; i < textColumns.length; i++) {
0:       textColumns[i] = textColumns[i].trim().toLowerCase();
0:     }
0:     for (int i = 0; i < textColumns.length; i++) {
0:       if (textColumns[i].isEmpty()) {
0:         throw new MalformedDataMapCommandException("TEXT_COLUMNS contains illegal argument.");
0:       }
0:       for (int j = i + 1; j < textColumns.length; j++) {
0:         if (textColumns[i].equals(textColumns[j])) {
0:           throw new MalformedDataMapCommandException(
0:               "TEXT_COLUMNS has duplicate columns :" + textColumns[i]);
0:         }
0:       }
0:     }
0:     List<String> textColumnList = new ArrayList<String>(textColumns.length);
0:     for (int i = 0; i < textColumns.length; i++) {
0:       CarbonColumn column = carbonTable.getColumnByName(carbonTable.getTableName(), textColumns[i]);
0:       if (null == column) {
0:         throw new MalformedDataMapCommandException("TEXT_COLUMNS: " + textColumns[i]
0:             + " does not exist in table. Please check create DataMap statement.");
0:       } else if (column.getDataType() != DataTypes.STRING) {
0:         throw new MalformedDataMapCommandException(
0:             "TEXT_COLUMNS only supports String column. " + "Unsupported column: " + textColumns[i]
0:                 + ", DataType: " + column.getDataType());
0:       }
0:       textColumnList.add(column.getColName());
0:     }
0:     return textColumnList;
0:   }
0: 
0:   /**
============================================================================