1:40c31e8: /*
1:40c31e8:  * Licensed to the Apache Software Foundation (ASF) under one or more
1:40c31e8:  * contributor license agreements.  See the NOTICE file distributed with
1:40c31e8:  * this work for additional information regarding copyright ownership.
1:40c31e8:  * The ASF licenses this file to You under the Apache License, Version 2.0
1:40c31e8:  * (the "License"); you may not use this file except in compliance with
1:40c31e8:  * the License.  You may obtain a copy of the License at
1:40c31e8:  *
1:40c31e8:  *    http://www.apache.org/licenses/LICENSE-2.0
1:40c31e8:  *
1:40c31e8:  * Unless required by applicable law or agreed to in writing, software
1:40c31e8:  * distributed under the License is distributed on an "AS IS" BASIS,
1:40c31e8:  * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
1:40c31e8:  * See the License for the specific language governing permissions and
1:40c31e8:  * limitations under the License.
1:40c31e8:  */
1:40c31e8: 
1:74c3eb1: package org.apache.carbondata.stream;
1:40c31e8: 
1:40c31e8: import java.io.File;
1:40c31e8: import java.io.IOException;
1:40c31e8: import java.util.ArrayList;
1:40c31e8: import java.util.Date;
1:40c31e8: import java.util.List;
1:40c31e8: import java.util.UUID;
1:40c31e8: 
1:40c31e8: import org.apache.carbondata.core.datastore.impl.FileFactory;
1:40c31e8: import org.apache.carbondata.core.metadata.AbsoluteTableIdentifier;
1:40c31e8: import org.apache.carbondata.core.metadata.CarbonTableIdentifier;
1:40c31e8: import org.apache.carbondata.core.statusmanager.FileFormat;
1:40c31e8: import org.apache.carbondata.hadoop.CarbonInputSplit;
1:40c31e8: import org.apache.carbondata.hadoop.CarbonMultiBlockSplit;
1:4c48148: import org.apache.carbondata.hadoop.util.CarbonInputFormatUtil;
1:74c3eb1: import org.apache.carbondata.streaming.CarbonStreamInputFormat;
1:40c31e8: 
1:40c31e8: import junit.framework.TestCase;
1:40c31e8: import org.apache.hadoop.conf.Configuration;
1:40c31e8: import org.apache.hadoop.mapreduce.InputSplit;
1:40c31e8: import org.apache.hadoop.mapreduce.JobID;
1:40c31e8: import org.apache.hadoop.mapreduce.RecordReader;
1:40c31e8: import org.apache.hadoop.mapreduce.TaskAttemptContext;
1:40c31e8: import org.apache.hadoop.mapreduce.TaskAttemptID;
1:40c31e8: import org.apache.hadoop.mapreduce.TaskID;
1:40c31e8: import org.apache.hadoop.mapreduce.TaskType;
1:40c31e8: import org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl;
1:40c31e8: import org.junit.Assert;
1:40c31e8: import org.junit.Test;
1:40c31e8: 
1:74c3eb1: public class CarbonStreamRecordReaderTest extends TestCase {
1:40c31e8: 
1:40c31e8:   private TaskAttemptID taskAttemptId;
1:40c31e8:   private TaskAttemptContext taskAttemptContext;
1:40c31e8:   private Configuration hadoopConf;
1:40c31e8:   private AbsoluteTableIdentifier identifier;
1:2fe7758:   private String tablePath;
1:4c48148: 
1:40c31e8: 
1:40c31e8:   @Override protected void setUp() throws Exception {
1:2fe7758:     tablePath = new File("target/stream_input").getCanonicalPath();
1:40c31e8:     String dbName = "default";
1:40c31e8:     String tableName = "stream_table_input";
1:2fe7758:     identifier = AbsoluteTableIdentifier.from(
1:2fe7758:         tablePath,
1:40c31e8:         new CarbonTableIdentifier(dbName, tableName, UUID.randomUUID().toString()));
1:40c31e8: 
1:4c48148:     JobID jobId = CarbonInputFormatUtil.getJobId(new Date(), 0);
1:40c31e8:     TaskID taskId = new TaskID(jobId, TaskType.MAP, 0);
1:40c31e8:     taskAttemptId = new TaskAttemptID(taskId, 0);
1:40c31e8: 
1:40c31e8:     hadoopConf = new Configuration();
1:40c31e8:     taskAttemptContext = new TaskAttemptContextImpl(hadoopConf, taskAttemptId);
1:40c31e8:   }
1:40c31e8: 
1:40c31e8:   private InputSplit buildInputSplit() throws IOException {
1:40c31e8:     CarbonInputSplit carbonInputSplit = new CarbonInputSplit();
1:40c31e8:     List<CarbonInputSplit> splitList = new ArrayList<>();
1:40c31e8:     splitList.add(carbonInputSplit);
1:44ffaf5:     return new CarbonMultiBlockSplit(splitList, new String[] { "localhost" },
1:ee71610:         FileFormat.ROW_V1);
1:40c31e8:   }
1:40c31e8: 
1:40c31e8:   @Test public void testCreateRecordReader() {
1:40c31e8:     try {
1:40c31e8:       InputSplit inputSplit = buildInputSplit();
1:40c31e8:       CarbonStreamInputFormat inputFormat = new CarbonStreamInputFormat();
1:40c31e8:       RecordReader recordReader = inputFormat.createRecordReader(inputSplit, taskAttemptContext);
1:40c31e8:       Assert.assertNotNull("Failed to create record reader", recordReader);
1:40c31e8:     } catch (Exception e) {
1:40c31e8:       e.printStackTrace();
1:40c31e8:       Assert.assertTrue(e.getMessage(), false);
1:40c31e8:     }
1:40c31e8:   }
1:40c31e8: 
1:40c31e8:   @Override protected void tearDown() throws Exception {
1:40c31e8:     super.tearDown();
1:2fe7758:     if (tablePath != null) {
1:2fe7758:       FileFactory.deleteAllFilesOfDir(new File(tablePath));
1:40c31e8:     }
1:40c31e8:   }
1:40c31e8: }
============================================================================
author:sandeep-katta
-------------------------------------------------------------------------------
commit:74c3eb1
/////////////////////////////////////////////////////////////////////////
1: package org.apache.carbondata.stream;
/////////////////////////////////////////////////////////////////////////
1: import org.apache.carbondata.streaming.CarbonStreamInputFormat;
/////////////////////////////////////////////////////////////////////////
1: public class CarbonStreamRecordReaderTest extends TestCase {
author:Jacky Li
-------------------------------------------------------------------------------
commit:c723947
/////////////////////////////////////////////////////////////////////////
0: package org.apache.carbondata.streaming;
commit:2fe7758
/////////////////////////////////////////////////////////////////////////
1:   private String tablePath;
1:     tablePath = new File("target/stream_input").getCanonicalPath();
1:     identifier = AbsoluteTableIdentifier.from(
1:         tablePath,
/////////////////////////////////////////////////////////////////////////
1:     if (tablePath != null) {
1:       FileFactory.deleteAllFilesOfDir(new File(tablePath));
commit:ee71610
/////////////////////////////////////////////////////////////////////////
1:         FileFormat.ROW_V1);
author:ravipesala
-------------------------------------------------------------------------------
commit:44ffaf5
/////////////////////////////////////////////////////////////////////////
1:     return new CarbonMultiBlockSplit(splitList, new String[] { "localhost" },
author:sounakr
-------------------------------------------------------------------------------
commit:4c48148
/////////////////////////////////////////////////////////////////////////
1: import org.apache.carbondata.hadoop.util.CarbonInputFormatUtil;
/////////////////////////////////////////////////////////////////////////
/////////////////////////////////////////////////////////////////////////
1: 
/////////////////////////////////////////////////////////////////////////
1:     JobID jobId = CarbonInputFormatUtil.getJobId(new Date(), 0);
author:QiangCai
-------------------------------------------------------------------------------
commit:40c31e8
/////////////////////////////////////////////////////////////////////////
1: /*
1:  * Licensed to the Apache Software Foundation (ASF) under one or more
1:  * contributor license agreements.  See the NOTICE file distributed with
1:  * this work for additional information regarding copyright ownership.
1:  * The ASF licenses this file to You under the Apache License, Version 2.0
1:  * (the "License"); you may not use this file except in compliance with
1:  * the License.  You may obtain a copy of the License at
1:  *
1:  *    http://www.apache.org/licenses/LICENSE-2.0
1:  *
1:  * Unless required by applicable law or agreed to in writing, software
1:  * distributed under the License is distributed on an "AS IS" BASIS,
1:  * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
1:  * See the License for the specific language governing permissions and
1:  * limitations under the License.
1:  */
1: 
0: package org.apache.carbondata.hadoop.streaming;
1: 
1: import java.io.File;
1: import java.io.IOException;
1: import java.util.ArrayList;
1: import java.util.Date;
1: import java.util.List;
1: import java.util.UUID;
1: 
1: import org.apache.carbondata.core.datastore.impl.FileFactory;
1: import org.apache.carbondata.core.metadata.AbsoluteTableIdentifier;
1: import org.apache.carbondata.core.metadata.CarbonTableIdentifier;
1: import org.apache.carbondata.core.statusmanager.FileFormat;
1: import org.apache.carbondata.hadoop.CarbonInputSplit;
1: import org.apache.carbondata.hadoop.CarbonMultiBlockSplit;
1: 
1: import junit.framework.TestCase;
1: import org.apache.hadoop.conf.Configuration;
1: import org.apache.hadoop.mapreduce.InputSplit;
1: import org.apache.hadoop.mapreduce.JobID;
1: import org.apache.hadoop.mapreduce.RecordReader;
1: import org.apache.hadoop.mapreduce.TaskAttemptContext;
1: import org.apache.hadoop.mapreduce.TaskAttemptID;
1: import org.apache.hadoop.mapreduce.TaskID;
1: import org.apache.hadoop.mapreduce.TaskType;
1: import org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl;
0: import org.apache.spark.SparkHadoopWriter;
1: import org.junit.Assert;
1: import org.junit.Test;
1: 
0: public class CarbonStreamInputFormatTest extends TestCase {
1: 
1:   private TaskAttemptID taskAttemptId;
1:   private TaskAttemptContext taskAttemptContext;
1:   private Configuration hadoopConf;
1:   private AbsoluteTableIdentifier identifier;
0:   private String storePath;
1: 
1:   @Override protected void setUp() throws Exception {
0:     storePath = new File("target/stream_input").getCanonicalPath();
1:     String dbName = "default";
1:     String tableName = "stream_table_input";
0:     identifier = new AbsoluteTableIdentifier(storePath,
1:         new CarbonTableIdentifier(dbName, tableName, UUID.randomUUID().toString()));
1: 
0:     JobID jobId = SparkHadoopWriter.createJobID(new Date(), 0);
1:     TaskID taskId = new TaskID(jobId, TaskType.MAP, 0);
1:     taskAttemptId = new TaskAttemptID(taskId, 0);
1: 
1:     hadoopConf = new Configuration();
1:     taskAttemptContext = new TaskAttemptContextImpl(hadoopConf, taskAttemptId);
1:   }
1: 
1:   private InputSplit buildInputSplit() throws IOException {
1:     CarbonInputSplit carbonInputSplit = new CarbonInputSplit();
1:     List<CarbonInputSplit> splitList = new ArrayList<>();
1:     splitList.add(carbonInputSplit);
0:     return new CarbonMultiBlockSplit(identifier, splitList, new String[] { "localhost" },
0:         FileFormat.rowformat);
1:   }
1: 
1:   @Test public void testCreateRecordReader() {
1:     try {
1:       InputSplit inputSplit = buildInputSplit();
1:       CarbonStreamInputFormat inputFormat = new CarbonStreamInputFormat();
1:       RecordReader recordReader = inputFormat.createRecordReader(inputSplit, taskAttemptContext);
1:       Assert.assertNotNull("Failed to create record reader", recordReader);
1:     } catch (Exception e) {
1:       e.printStackTrace();
1:       Assert.assertTrue(e.getMessage(), false);
1:     }
1:   }
1: 
1:   @Override protected void tearDown() throws Exception {
1:     super.tearDown();
0:     if (storePath != null) {
0:       FileFactory.deleteAllFilesOfDir(new File(storePath));
1:     }
1:   }
1: }
============================================================================