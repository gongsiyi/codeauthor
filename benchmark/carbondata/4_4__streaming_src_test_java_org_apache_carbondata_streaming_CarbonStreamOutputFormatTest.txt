1:40c31e8: /*
1:40c31e8:  * Licensed to the Apache Software Foundation (ASF) under one or more
1:40c31e8:  * contributor license agreements.  See the NOTICE file distributed with
1:40c31e8:  * this work for additional information regarding copyright ownership.
1:40c31e8:  * The ASF licenses this file to You under the Apache License, Version 2.0
1:40c31e8:  * (the "License"); you may not use this file except in compliance with
1:40c31e8:  * the License.  You may obtain a copy of the License at
1:40c31e8:  *
1:40c31e8:  *    http://www.apache.org/licenses/LICENSE-2.0
1:40c31e8:  *
1:40c31e8:  * Unless required by applicable law or agreed to in writing, software
1:40c31e8:  * distributed under the License is distributed on an "AS IS" BASIS,
1:40c31e8:  * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
1:40c31e8:  * See the License for the specific language governing permissions and
1:40c31e8:  * limitations under the License.
1:40c31e8:  */
1:40c31e8: 
1:c723947: package org.apache.carbondata.streaming;
1:40c31e8: 
1:40c31e8: import java.io.File;
1:40c31e8: import java.io.IOException;
1:40c31e8: import java.util.Date;
1:40c31e8: import java.util.UUID;
1:40c31e8: 
1:40c31e8: import org.apache.carbondata.core.datastore.impl.FileFactory;
1:40c31e8: import org.apache.carbondata.core.metadata.AbsoluteTableIdentifier;
1:40c31e8: import org.apache.carbondata.core.metadata.CarbonTableIdentifier;
1:40c31e8: import org.apache.carbondata.core.metadata.schema.table.CarbonTable;
1:c723947: import org.apache.carbondata.hadoop.testutil.StoreCreator;
1:4c48148: import org.apache.carbondata.hadoop.util.CarbonInputFormatUtil;
1:40c31e8: import org.apache.carbondata.processing.loading.model.CarbonLoadModel;
1:40c31e8: 
1:40c31e8: import junit.framework.TestCase;
1:40c31e8: import org.apache.hadoop.conf.Configuration;
1:40c31e8: import org.apache.hadoop.mapreduce.JobID;
1:40c31e8: import org.apache.hadoop.mapreduce.RecordWriter;
1:40c31e8: import org.apache.hadoop.mapreduce.TaskAttemptContext;
1:40c31e8: import org.apache.hadoop.mapreduce.TaskAttemptID;
1:40c31e8: import org.apache.hadoop.mapreduce.TaskID;
1:40c31e8: import org.apache.hadoop.mapreduce.TaskType;
1:40c31e8: import org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl;
1:40c31e8: import org.junit.Assert;
1:40c31e8: import org.junit.Test;
1:40c31e8: 
1:40c31e8: public class CarbonStreamOutputFormatTest extends TestCase {
1:40c31e8: 
1:40c31e8:   private Configuration hadoopConf;
1:40c31e8:   private TaskAttemptID taskAttemptId;
1:40c31e8:   private CarbonLoadModel carbonLoadModel;
1:2fe7758:   private String tablePath;
1:40c31e8: 
1:40c31e8:   @Override protected void setUp() throws Exception {
1:40c31e8:     super.setUp();
1:4c48148:     JobID jobId = CarbonInputFormatUtil.getJobId(new Date(), 0);
1:40c31e8:     TaskID taskId = new TaskID(jobId, TaskType.MAP, 0);
1:40c31e8:     taskAttemptId = new TaskAttemptID(taskId, 0);
1:40c31e8: 
1:40c31e8:     hadoopConf = new Configuration();
1:40c31e8:     hadoopConf.set("mapred.job.id", jobId.toString());
1:40c31e8:     hadoopConf.set("mapred.tip.id", taskAttemptId.getTaskID().toString());
1:40c31e8:     hadoopConf.set("mapred.task.id", taskAttemptId.toString());
1:40c31e8:     hadoopConf.setBoolean("mapred.task.is.map", true);
1:40c31e8:     hadoopConf.setInt("mapred.task.partition", 0);
1:40c31e8: 
1:2fe7758:     tablePath = new File("target/stream_output").getCanonicalPath();
1:40c31e8:     String dbName = "default";
1:40c31e8:     String tableName = "stream_table_output";
1:2fe7758:     AbsoluteTableIdentifier identifier =
1:2fe7758:         AbsoluteTableIdentifier.from(
1:2fe7758:             tablePath,
1:2fe7758:             new CarbonTableIdentifier(dbName, tableName, UUID.randomUUID().toString()));
1:40c31e8: 
1:3894e1d:     CarbonTable table = new StoreCreator(new File("target/store").getAbsolutePath(),
1:3894e1d:         new File("../hadoop/src/test/resources/data.csv").getCanonicalPath()).createTable(identifier);
1:40c31e8: 
1:40c31e8:     String factFilePath = new File("../hadoop/src/test/resources/data.csv").getCanonicalPath();
1:40c31e8:     carbonLoadModel = StoreCreator.buildCarbonLoadModel(table, factFilePath, identifier);
1:40c31e8:   }
1:40c31e8: 
1:40c31e8:   @Test public void testSetCarbonLoadModel() {
1:40c31e8:     try {
1:40c31e8:       CarbonStreamOutputFormat.setCarbonLoadModel(hadoopConf, carbonLoadModel);
1:40c31e8:     } catch (IOException e) {
1:40c31e8:       Assert.assertTrue("Failed to config CarbonLoadModel for CarbonStreamOutputFromat", false);
1:40c31e8:     }
1:40c31e8:   }
1:40c31e8: 
1:40c31e8:   @Test public void testGetCarbonLoadModel() {
1:40c31e8:     try {
1:40c31e8:       CarbonStreamOutputFormat.setCarbonLoadModel(hadoopConf, carbonLoadModel);
1:40c31e8:       CarbonLoadModel model = CarbonStreamOutputFormat.getCarbonLoadModel(hadoopConf);
1:40c31e8: 
1:40c31e8:       Assert.assertNotNull("Failed to get CarbonLoadModel", model);
1:40c31e8:       Assert.assertTrue("CarbonLoadModel should be same with previous",
1:40c31e8:           carbonLoadModel.getFactTimeStamp() == model.getFactTimeStamp());
1:40c31e8: 
1:40c31e8:     } catch (IOException e) {
1:40c31e8:       Assert.assertTrue("Failed to get CarbonLoadModel for CarbonStreamOutputFromat", false);
1:40c31e8:     }
1:40c31e8:   }
1:40c31e8: 
1:40c31e8:   @Test public void testGetRecordWriter() {
1:40c31e8:     CarbonStreamOutputFormat outputFormat = new CarbonStreamOutputFormat();
1:40c31e8:     try {
1:40c31e8:       CarbonStreamOutputFormat.setCarbonLoadModel(hadoopConf, carbonLoadModel);
1:40c31e8:       TaskAttemptContext taskAttemptContext =
1:40c31e8:           new TaskAttemptContextImpl(hadoopConf, taskAttemptId);
1:40c31e8:       RecordWriter recordWriter = outputFormat.getRecordWriter(taskAttemptContext);
1:40c31e8:       Assert.assertNotNull("Failed to get CarbonStreamRecordWriter", recordWriter);
1:40c31e8:     } catch (Exception e) {
1:40c31e8:       e.printStackTrace();
1:40c31e8:       Assert.assertTrue(e.getMessage(), false);
1:40c31e8:     }
1:40c31e8:   }
1:40c31e8: 
1:40c31e8:   @Override protected void tearDown() throws Exception {
1:40c31e8:     super.tearDown();
1:2fe7758:     if (tablePath != null) {
1:2fe7758:       FileFactory.deleteAllFilesOfDir(new File(tablePath));
1:40c31e8:     }
1:40c31e8:   }
1:40c31e8: }
============================================================================
author:ravipesala
-------------------------------------------------------------------------------
commit:3894e1d
/////////////////////////////////////////////////////////////////////////
1:     CarbonTable table = new StoreCreator(new File("target/store").getAbsolutePath(),
1:         new File("../hadoop/src/test/resources/data.csv").getCanonicalPath()).createTable(identifier);
author:Jacky Li
-------------------------------------------------------------------------------
commit:c723947
/////////////////////////////////////////////////////////////////////////
1: package org.apache.carbondata.streaming;
/////////////////////////////////////////////////////////////////////////
1: import org.apache.carbondata.hadoop.testutil.StoreCreator;
commit:2fe7758
/////////////////////////////////////////////////////////////////////////
1:   private String tablePath;
/////////////////////////////////////////////////////////////////////////
1:     tablePath = new File("target/stream_output").getCanonicalPath();
1:     AbsoluteTableIdentifier identifier =
1:         AbsoluteTableIdentifier.from(
1:             tablePath,
1:             new CarbonTableIdentifier(dbName, tableName, UUID.randomUUID().toString()));
/////////////////////////////////////////////////////////////////////////
1:     if (tablePath != null) {
1:       FileFactory.deleteAllFilesOfDir(new File(tablePath));
author:sounakr
-------------------------------------------------------------------------------
commit:4c48148
/////////////////////////////////////////////////////////////////////////
1: import org.apache.carbondata.hadoop.util.CarbonInputFormatUtil;
/////////////////////////////////////////////////////////////////////////
/////////////////////////////////////////////////////////////////////////
1:     JobID jobId = CarbonInputFormatUtil.getJobId(new Date(), 0);
author:QiangCai
-------------------------------------------------------------------------------
commit:40c31e8
/////////////////////////////////////////////////////////////////////////
1: /*
1:  * Licensed to the Apache Software Foundation (ASF) under one or more
1:  * contributor license agreements.  See the NOTICE file distributed with
1:  * this work for additional information regarding copyright ownership.
1:  * The ASF licenses this file to You under the Apache License, Version 2.0
1:  * (the "License"); you may not use this file except in compliance with
1:  * the License.  You may obtain a copy of the License at
1:  *
1:  *    http://www.apache.org/licenses/LICENSE-2.0
1:  *
1:  * Unless required by applicable law or agreed to in writing, software
1:  * distributed under the License is distributed on an "AS IS" BASIS,
1:  * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
1:  * See the License for the specific language governing permissions and
1:  * limitations under the License.
1:  */
1: 
0: package org.apache.carbondata.hadoop.streaming;
1: 
1: import java.io.File;
1: import java.io.IOException;
1: import java.util.Date;
1: import java.util.UUID;
1: 
1: import org.apache.carbondata.core.datastore.impl.FileFactory;
1: import org.apache.carbondata.core.metadata.AbsoluteTableIdentifier;
1: import org.apache.carbondata.core.metadata.CarbonTableIdentifier;
1: import org.apache.carbondata.core.metadata.schema.table.CarbonTable;
0: import org.apache.carbondata.hadoop.test.util.StoreCreator;
1: import org.apache.carbondata.processing.loading.model.CarbonLoadModel;
1: 
1: import junit.framework.TestCase;
1: import org.apache.hadoop.conf.Configuration;
1: import org.apache.hadoop.mapreduce.JobID;
1: import org.apache.hadoop.mapreduce.RecordWriter;
1: import org.apache.hadoop.mapreduce.TaskAttemptContext;
1: import org.apache.hadoop.mapreduce.TaskAttemptID;
1: import org.apache.hadoop.mapreduce.TaskID;
1: import org.apache.hadoop.mapreduce.TaskType;
1: import org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl;
0: import org.apache.spark.SparkHadoopWriter;
1: import org.junit.Assert;
1: import org.junit.Test;
1: 
1: public class CarbonStreamOutputFormatTest extends TestCase {
1: 
1:   private Configuration hadoopConf;
1:   private TaskAttemptID taskAttemptId;
1:   private CarbonLoadModel carbonLoadModel;
0:   private String storePath;
1: 
1:   @Override protected void setUp() throws Exception {
1:     super.setUp();
0:     JobID jobId = SparkHadoopWriter.createJobID(new Date(), 0);
1:     TaskID taskId = new TaskID(jobId, TaskType.MAP, 0);
1:     taskAttemptId = new TaskAttemptID(taskId, 0);
1: 
1:     hadoopConf = new Configuration();
1:     hadoopConf.set("mapred.job.id", jobId.toString());
1:     hadoopConf.set("mapred.tip.id", taskAttemptId.getTaskID().toString());
1:     hadoopConf.set("mapred.task.id", taskAttemptId.toString());
1:     hadoopConf.setBoolean("mapred.task.is.map", true);
1:     hadoopConf.setInt("mapred.task.partition", 0);
1: 
0:     storePath = new File("target/stream_output").getCanonicalPath();
1:     String dbName = "default";
1:     String tableName = "stream_table_output";
0:     AbsoluteTableIdentifier identifier = new AbsoluteTableIdentifier(storePath,
0:         new CarbonTableIdentifier(dbName, tableName, UUID.randomUUID().toString()));
1: 
0:     CarbonTable table = StoreCreator.createTable(identifier);
1: 
1:     String factFilePath = new File("../hadoop/src/test/resources/data.csv").getCanonicalPath();
1:     carbonLoadModel = StoreCreator.buildCarbonLoadModel(table, factFilePath, identifier);
1:   }
1: 
1:   @Test public void testSetCarbonLoadModel() {
1:     try {
1:       CarbonStreamOutputFormat.setCarbonLoadModel(hadoopConf, carbonLoadModel);
1:     } catch (IOException e) {
1:       Assert.assertTrue("Failed to config CarbonLoadModel for CarbonStreamOutputFromat", false);
1:     }
1:   }
1: 
1:   @Test public void testGetCarbonLoadModel() {
1:     try {
1:       CarbonStreamOutputFormat.setCarbonLoadModel(hadoopConf, carbonLoadModel);
1:       CarbonLoadModel model = CarbonStreamOutputFormat.getCarbonLoadModel(hadoopConf);
1: 
1:       Assert.assertNotNull("Failed to get CarbonLoadModel", model);
1:       Assert.assertTrue("CarbonLoadModel should be same with previous",
1:           carbonLoadModel.getFactTimeStamp() == model.getFactTimeStamp());
1: 
1:     } catch (IOException e) {
1:       Assert.assertTrue("Failed to get CarbonLoadModel for CarbonStreamOutputFromat", false);
1:     }
1:   }
1: 
1:   @Test public void testGetRecordWriter() {
1:     CarbonStreamOutputFormat outputFormat = new CarbonStreamOutputFormat();
1:     try {
1:       CarbonStreamOutputFormat.setCarbonLoadModel(hadoopConf, carbonLoadModel);
1:       TaskAttemptContext taskAttemptContext =
1:           new TaskAttemptContextImpl(hadoopConf, taskAttemptId);
1:       RecordWriter recordWriter = outputFormat.getRecordWriter(taskAttemptContext);
1:       Assert.assertNotNull("Failed to get CarbonStreamRecordWriter", recordWriter);
1:     } catch (Exception e) {
1:       e.printStackTrace();
1:       Assert.assertTrue(e.getMessage(), false);
1:     }
1:   }
1: 
1:   @Override protected void tearDown() throws Exception {
1:     super.tearDown();
0:     if (storePath != null) {
0:       FileFactory.deleteAllFilesOfDir(new File(storePath));
1:     }
1:   }
1: }
============================================================================