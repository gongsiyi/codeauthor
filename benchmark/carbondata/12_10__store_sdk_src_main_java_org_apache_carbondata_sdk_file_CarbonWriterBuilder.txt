1:1d827c7: /*
1:1d827c7:  * Licensed to the Apache Software Foundation (ASF) under one or more
1:1d827c7:  * contributor license agreements.  See the NOTICE file distributed with
1:1d827c7:  * this work for additional information regarding copyright ownership.
1:1d827c7:  * The ASF licenses this file to You under the Apache License, Version 2.0
1:1d827c7:  * (the "License"); you may not use this file except in compliance with
1:1d827c7:  * the License.  You may obtain a copy of the License at
1:242c08b:  *
1:1d827c7:  *    http://www.apache.org/licenses/LICENSE-2.0
1:242c08b:  *
1:1d827c7:  * Unless required by applicable law or agreed to in writing, software
1:1d827c7:  * distributed under the License is distributed on an "AS IS" BASIS,
1:1d827c7:  * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
1:1d827c7:  * See the License for the specific language governing permissions and
1:1d827c7:  * limitations under the License.
2:1d827c7:  */
12:1d827c7: 
1:1d827c7: package org.apache.carbondata.sdk.file;
1:1d827c7: 
1:1d827c7: import java.io.IOException;
1:5f32647: import java.util.ArrayList;
1:1d827c7: import java.util.Arrays;
1:1d827c7: import java.util.HashMap;
1:cf666c1: import java.util.HashSet;
1:1d827c7: import java.util.List;
1:1d827c7: import java.util.Map;
1:1d827c7: import java.util.Objects;
1:cf666c1: import java.util.Set;
1:5f32647: import java.util.TreeMap;
1:35a7b5e: import java.util.concurrent.atomic.AtomicInteger;
1:1d827c7: 
1:1d827c7: import org.apache.carbondata.common.annotations.InterfaceAudience;
1:1d827c7: import org.apache.carbondata.common.annotations.InterfaceStability;
1:89cfd8e: import org.apache.carbondata.common.exceptions.sql.InvalidLoadOptionException;
1:1d827c7: import org.apache.carbondata.core.datastore.impl.FileFactory;
1:1d827c7: import org.apache.carbondata.core.metadata.CarbonMetadata;
1:1d827c7: import org.apache.carbondata.core.metadata.converter.SchemaConverter;
1:1d827c7: import org.apache.carbondata.core.metadata.converter.ThriftWrapperSchemaConverterImpl;
1:3202cf5: import org.apache.carbondata.core.metadata.datatype.DataType;
1:5f32647: import org.apache.carbondata.core.metadata.datatype.DataTypes;
1:68b359e: import org.apache.carbondata.core.metadata.datatype.MapType;
1:1d827c7: import org.apache.carbondata.core.metadata.datatype.StructField;
1:1d827c7: import org.apache.carbondata.core.metadata.schema.table.CarbonTable;
1:1d827c7: import org.apache.carbondata.core.metadata.schema.table.TableInfo;
1:1d827c7: import org.apache.carbondata.core.metadata.schema.table.TableSchema;
1:1d827c7: import org.apache.carbondata.core.metadata.schema.table.TableSchemaBuilder;
1:93724ec: import org.apache.carbondata.core.metadata.schema.table.column.ColumnSchema;
1:f5c7a19: import org.apache.carbondata.core.util.CarbonUtil;
1:1d827c7: import org.apache.carbondata.core.util.path.CarbonTablePath;
1:1d827c7: import org.apache.carbondata.core.writer.ThriftWriter;
1:1d827c7: import org.apache.carbondata.processing.loading.model.CarbonLoadModel;
1:89cfd8e: import org.apache.carbondata.processing.loading.model.CarbonLoadModelBuilder;
1:1d827c7: 
1:8f1a029: import org.apache.hadoop.conf.Configuration;
1:242c08b: import org.apache.hadoop.fs.s3a.Constants;
1:242c08b: 
1:cf1e4d4: /**
1:4b98af2:  * Builder for {@link CarbonWriter}
1:cf1e4d4:  */
1:1d827c7: @InterfaceAudience.User
1:1d827c7: @InterfaceStability.Unstable
1:1d827c7: public class CarbonWriterBuilder {
1:1d827c7:   private Schema schema;
1:1d827c7:   private String path;
1:1d827c7:   private String[] sortColumns;
1:1d827c7:   private boolean persistSchemaFile;
1:6cb6f83:   private int blockletSize;
1:6cb6f83:   private int blockSize;
1:b7b8073:   private boolean isTransactionalTable;
1:280a400:   private long UUID;
1:5f32647:   private Map<String, String> options;
1:b7b8073:   private String taskNo;
1:3109d04:   private int localDictionaryThreshold;
1:3109d04:   private boolean isLocalDictionaryEnabled;
1:2a9604c: 
1:cf1e4d4:   /**
1:cf1e4d4:    * Sets the output path of the writer builder
1:cf1e4d4:    * @param path is the absolute path where output files are written
1:4b98af2:    * This method must be called when building CarbonWriterBuilder
1:cf1e4d4:    * @return updated CarbonWriterBuilder
1:cf1e4d4:    */
1:1d827c7:   public CarbonWriterBuilder outputPath(String path) {
1:1d827c7:     Objects.requireNonNull(path, "path should not be null");
1:1d827c7:     this.path = path;
2:1d827c7:     return this;
1:2a9604c:   }
1:6b70b7e: 
1:17a4b48:   /**
1:cf1e4d4:    * sets the list of columns that needs to be in sorted order
1:5f32647:    * @param sortColumns is a string array of columns that needs to be sorted.
1:4b98af2:    * If it is null or by default all dimensions are selected for sorting
1:4b98af2:    * If it is empty array, no columns are sorted
1:cf1e4d4:    * @return updated CarbonWriterBuilder
1:cf1e4d4:    */
1:1d827c7:   public CarbonWriterBuilder sortBy(String[] sortColumns) {
1:f5c7a19:     if (sortColumns != null) {
1:f5c7a19:       for (int i = 0; i < sortColumns.length; i++) {
1:f5c7a19:         sortColumns[i] = sortColumns[i].toLowerCase();
1:6b70b7e:       }
1:6b70b7e:     }
1:1d827c7:     this.sortColumns = sortColumns;
1:280a400:     return this;
1:f5c7a19:   }
1:5f32647: 
1:5804d75:   /**
1:b7b8073:    * sets the taskNo for the writer. SDKs concurrently running
1:4b98af2:    * will set taskNo in order to avoid conflicts in file's name during write.
1:4b98af2:    * @param taskNo is the TaskNo user wants to specify.
1:4b98af2:    * by default it is system time in nano seconds.
1:cf1e4d4:    * @return updated CarbonWriterBuilder
1:b7b8073:    */
1:fc4b7f9:   public CarbonWriterBuilder taskNo(long taskNo) {
1:fc4b7f9:     this.taskNo = String.valueOf(taskNo);
1:b7b8073:     return this;
1:f5c7a19:   }
1:280a400: 
1:b7b8073: 
1:b7b8073: 
1:cf1e4d4:   /**
1:cf1e4d4:    * If set, create a schema file in metadata folder.
1:4b98af2:    * @param persist is a boolean value, If set to true, creates a schema file in metadata folder.
1:4b98af2:    * By default set to false. will not create metadata folder
1:cf1e4d4:    * @return updated CarbonWriterBuilder
1:cf1e4d4:    */
1:1d827c7:   public CarbonWriterBuilder persistSchemaFile(boolean persist) {
1:1d827c7:     this.persistSchemaFile = persist;
1:1d827c7:     return this;
1:5f32647:   }
1:b7b8073: 
1:cf1e4d4:   /**
1:b7b8073:    * If set false, writes the carbondata and carbonindex files in a flat folder structure
1:4b98af2:    * @param isTransactionalTable is a boolelan value
1:ddf3e85:    * If set to false, then writes the carbondata and carbonindex files
1:4b98af2:    * in a flat folder structure.
1:ddf3e85:    * If set to true, then writes the carbondata and carbonindex files
1:ddf3e85:    * in segment folder structure.
1:4b98af2:    * By default set to false.
1:cf1e4d4:    * @return updated CarbonWriterBuilder
1:cf1e4d4:    */
1:b7b8073:   public CarbonWriterBuilder isTransactionalTable(boolean isTransactionalTable) {
1:b7b8073:     Objects.requireNonNull(isTransactionalTable, "Transactional Table should not be null");
1:b7b8073:     this.isTransactionalTable = isTransactionalTable;
1:1d827c7:     return this;
1:280a400:   }
1:1d827c7: 
1:cf1e4d4:   /**
1:242c08b:    * Set the access key for S3
1:17a4b48:    *
1:242c08b:    * @param key   the string of access key for different S3 type,like: fs.s3a.access.key
1:242c08b:    * @param value the value of access key
1:242c08b:    * @return CarbonWriterBuilder
1:242c08b:    */
1:242c08b:   public CarbonWriterBuilder setAccessKey(String key, String value) {
1:242c08b:     FileFactory.getConfiguration().set(key, value);
1:242c08b:     return this;
1:242c08b:   }
1:242c08b: 
1:cf1e4d4:   /**
1:242c08b:    * Set the access key for S3.
1:17a4b48:    *
1:242c08b:    * @param value the value of access key
1:242c08b:    * @return CarbonWriterBuilder
1:242c08b:    */
1:242c08b:   public CarbonWriterBuilder setAccessKey(String value) {
1:242c08b:     return setAccessKey(Constants.ACCESS_KEY, value);
1:242c08b:   }
1:242c08b: 
1:cf1e4d4:   /**
1:242c08b:    * Set the secret key for S3
1:242c08b:    *
1:242c08b:    * @param key   the string of secret key for different S3 type,like: fs.s3a.secret.key
1:242c08b:    * @param value the value of secret key
1:242c08b:    * @return CarbonWriterBuilder
1:242c08b:    */
1:242c08b:   public CarbonWriterBuilder setSecretKey(String key, String value) {
1:242c08b:     FileFactory.getConfiguration().set(key, value);
1:242c08b:     return this;
1:242c08b:   }
1:242c08b: 
1:242c08b:   /**
1:242c08b:    * Set the secret key for S3
1:242c08b:    *
1:242c08b:    * @param value the value of secret key
1:242c08b:    * @return CarbonWriterBuilder
1:242c08b:    */
1:242c08b:   public CarbonWriterBuilder setSecretKey(String value) {
1:242c08b:     return setSecretKey(Constants.SECRET_KEY, value);
1:242c08b:   }
1:242c08b: 
1:242c08b:   /**
1:242c08b:    * Set the endpoint for S3
1:242c08b:    *
1:242c08b:    * @param key   the string of endpoint for different S3 type,like: fs.s3a.endpoint
1:242c08b:    * @param value the value of endpoint
1:242c08b:    * @return CarbonWriterBuilder
1:242c08b:    */
1:242c08b:   public CarbonWriterBuilder setEndPoint(String key, String value) {
1:242c08b:     FileFactory.getConfiguration().set(key, value);
1:242c08b:     return this;
1:242c08b:   }
1:242c08b: 
1:242c08b:   /**
1:242c08b:    * Set the endpoint for S3
1:242c08b:    *
1:242c08b:    * @param value the value of endpoint
1:242c08b:    * @return CarbonWriterBuilder
1:242c08b:    */
1:242c08b:   public CarbonWriterBuilder setEndPoint(String value) {
1:242c08b:     FileFactory.getConfiguration().set(Constants.ENDPOINT, value);
1:242c08b:     return this;
1:242c08b:   }
1:242c08b: 
1:242c08b:   /**
1:cf1e4d4:    * to set the timestamp in the carbondata and carbonindex index files
1:4b98af2:    * @param UUID is a timestamp to be used in the carbondata and carbonindex index files.
1:4b98af2:    * By default set to zero.
1:cf1e4d4:    * @return updated CarbonWriterBuilder
1:cf1e4d4:    */
1:280a400:   public CarbonWriterBuilder uniqueIdentifier(long UUID) {
1:280a400:     Objects.requireNonNull(UUID, "Unique Identifier should not be null");
1:280a400:     this.UUID = UUID;
1:280a400:     return this;
1:280a400:   }
1:280a400: 
1:cf1e4d4:   /**
1:5f32647:    * To support the load options for sdk writer
1:5f32647:    * @param options key,value pair of load options.
1:4b98af2:    * supported keys values are
1:4b98af2:    * a. bad_records_logger_enable -- true (write into separate logs), false
1:4b98af2:    * b. bad_records_action -- FAIL, FORCE, IGNORE, REDIRECT
1:4b98af2:    * c. bad_record_path -- path
1:4b98af2:    * d. dateformat -- same as JAVA SimpleDateFormat
1:4b98af2:    * e. timestampformat -- same as JAVA SimpleDateFormat
1:4b98af2:    * f. complex_delimiter_level_1 -- value to Split the complexTypeData
1:4b98af2:    * g. complex_delimiter_level_2 -- value to Split the nested complexTypeData
1:4b98af2:    * h. quotechar
1:4b98af2:    * i. escapechar
1:4b98af2:    *
1:4b98af2:    * Default values are as follows.
1:4b98af2:    *
1:4b98af2:    * a. bad_records_logger_enable -- "false"
1:4b98af2:    * b. bad_records_action -- "FAIL"
1:4b98af2:    * c. bad_record_path -- ""
1:4b98af2:    * d. dateformat -- "" , uses from carbon.properties file
1:4b98af2:    * e. timestampformat -- "", uses from carbon.properties file
1:4b98af2:    * f. complex_delimiter_level_1 -- "$"
1:4b98af2:    * g. complex_delimiter_level_2 -- ":"
1:4b98af2:    * h. quotechar -- "\""
1:4b98af2:    * i. escapechar -- "\\"
1:5f32647:    *
1:5f32647:    * @return updated CarbonWriterBuilder
1:5f32647:    */
1:5f32647:   public CarbonWriterBuilder withLoadOptions(Map<String, String> options) {
1:5f32647:     Objects.requireNonNull(options, "Load options should not be null");
1:5f32647:     //validate the options.
1:5f32647:     for (String option: options.keySet()) {
1:5f32647:       if (!option.equalsIgnoreCase("bad_records_logger_enable") &&
1:5f32647:           !option.equalsIgnoreCase("bad_records_action") &&
1:5f32647:           !option.equalsIgnoreCase("bad_record_path") &&
1:5f32647:           !option.equalsIgnoreCase("dateformat") &&
1:5f32647:           !option.equalsIgnoreCase("timestampformat") &&
1:5f32647:           !option.equalsIgnoreCase("complex_delimiter_level_1") &&
1:5f32647:           !option.equalsIgnoreCase("complex_delimiter_level_2") &&
1:5f32647:           !option.equalsIgnoreCase("quotechar") &&
1:a9cc434:           !option.equalsIgnoreCase("escapechar")) {
1:f5c7a19:         throw new IllegalArgumentException("Unsupported option:" + option
1:f5c7a19:             + ". Refer method header or documentation");
1:f5c7a19:       }
1:f5c7a19:     }
1:5f32647: 
1:a9cc434:     if (this.options == null) {
1:a9cc434:       // convert it to treeMap as keys need to be case insensitive
1:a9cc434:       this.options = new TreeMap<>(String.CASE_INSENSITIVE_ORDER);
1:a9cc434:     }
1:a9cc434:     this.options.putAll(options);
1:5f32647:     return this;
1:5f32647:   }
1:5f32647: 
1:5f32647:   /**
1:1372450:    * To support the table properties for sdk writer
1:1372450:    *
1:1372450:    * @param options key,value pair of create table properties.
1:1372450:    * supported keys values are
1:f5c7a19:    * a. table_blocksize -- [1-2048] values in MB. Default value is 1024
1:f5c7a19:    * b. table_blocklet_size -- values in MB. Default value is 64 MB
1:f5c7a19:    * c. local_dictionary_threshold -- positive value, default is 10000
1:f5c7a19:    * d. local_dictionary_enable -- true / false. Default is false
1:f5c7a19:    * e. sort_columns -- comma separated column. "c1,c2". Default all dimensions are sorted.
1:a9cc434:    * j. sort_scope -- "local_sort", "no_sort", "batch_sort". default value is "local_sort"
1:a9cc434:    * k. long_string_columns -- comma separated string columns which are more than 32k length.
1:a9cc434:    *                           default value is null.
1:1372450:    *
1:1372450:    * @return updated CarbonWriterBuilder
1:1372450:    */
1:1372450:   public CarbonWriterBuilder withTableProperties(Map<String, String> options) {
1:1372450:     Objects.requireNonNull(options, "Table properties should not be null");
1:1372450:     //validate the options.
1:1372450:     if (options.size() > 5) {
1:1372450:       throw new IllegalArgumentException("Supports only 5 options now. "
1:1372450:           + "Refer method header or documentation");
1:1372450:     }
1:1372450: 
1:1372450:     Set<String> supportedOptions = new HashSet<>(Arrays
1:f5c7a19:         .asList("table_blocksize", "table_blocklet_size", "local_dictionary_threshold",
1:a9cc434:             "local_dictionary_enable", "sort_columns", "sort_scope", "long_string_columns"));
1:1372450: 
1:1372450:     for (String key : options.keySet()) {
1:1372450:       if (!supportedOptions.contains(key.toLowerCase())) {
1:1372450:         throw new IllegalArgumentException(
1:1372450:             "Unsupported options. " + "Refer method header or documentation");
1:1372450:       }
1:1372450:     }
1:1372450: 
1:1372450:     for (Map.Entry<String, String> entry : options.entrySet()) {
1:f5c7a19:       if (entry.getKey().equalsIgnoreCase("table_blocksize")) {
1:1372450:         this.withBlockSize(Integer.parseInt(entry.getValue()));
1:f5c7a19:       } else if (entry.getKey().equalsIgnoreCase("table_blocklet_size")) {
1:1372450:         this.withBlockletSize(Integer.parseInt(entry.getValue()));
1:f5c7a19:       } else if (entry.getKey().equalsIgnoreCase("local_dictionary_threshold")) {
1:1372450:         this.localDictionaryThreshold(Integer.parseInt(entry.getValue()));
1:f5c7a19:       } else if (entry.getKey().equalsIgnoreCase("local_dictionary_enable")) {
1:1372450:         this.enableLocalDictionary((entry.getValue().equalsIgnoreCase("true")));
1:f5c7a19:       } else if (entry.getKey().equalsIgnoreCase("sort_columns")) {
1:1372450:         //sort columns
1:1372450:         String[] sortColumns = entry.getValue().split(",");
1:1372450:         this.sortBy(sortColumns);
1:a9cc434:       } else if (entry.getKey().equalsIgnoreCase("sort_scope")) {
1:a9cc434:         this.withSortScope(entry);
1:a9cc434:       } else if (entry.getKey().equalsIgnoreCase("long_string_columns")) {
1:a9cc434:         updateToLoadOptions(entry);
1:1372450:       }
1:1372450:     }
1:1372450:     return this;
1:1372450:   }
1:1372450: 
1:1372450:   /**
1:cf1e4d4:    * To set the carbondata file size in MB between 1MB-2048MB
1:cf1e4d4:    * @param blockSize is size in MB between 1MB to 2048 MB
1:4b98af2:    * default value is 1024 MB
1:cf1e4d4:    * @return updated CarbonWriterBuilder
1:cf1e4d4:    */
1:1d827c7:   public CarbonWriterBuilder withBlockSize(int blockSize) {
1:280a400:     if (blockSize <= 0 || blockSize > 2048) {
1:280a400:       throw new IllegalArgumentException("blockSize should be between 1 MB to 2048 MB");
1:5f32647:     }
1:6cb6f83:     this.blockSize = blockSize;
1:6cb6f83:     return this;
1:5f32647:   }
1:1d827c7: 
1:242c08b:   /**
1:3109d04:    * @param localDictionaryThreshold is localDictionaryThreshold,default is 10000
1:3109d04:    * @return updated CarbonWriterBuilder
1:3109d04:    */
1:3109d04:   public CarbonWriterBuilder localDictionaryThreshold(int localDictionaryThreshold) {
1:3109d04:     if (localDictionaryThreshold <= 0) {
1:3109d04:       throw new IllegalArgumentException(
1:1372450:           "Local Dictionary Threshold should be greater than 0");
1:3109d04:     }
1:3109d04:     this.localDictionaryThreshold = localDictionaryThreshold;
1:3109d04:     return this;
1:3109d04:   }
1:3109d04: 
1:3109d04:   /**
1:3109d04:    * @param enableLocalDictionary enable local dictionary  , default is false
1:3109d04:    * @return updated CarbonWriterBuilder
1:3109d04:    */
1:3109d04:   public CarbonWriterBuilder enableLocalDictionary(boolean enableLocalDictionary) {
1:3109d04:     this.isLocalDictionaryEnabled = enableLocalDictionary;
1:3109d04:     return this;
1:3109d04:   }
1:3109d04: 
1:3109d04: 
1:3109d04:   /**
1:ddf3e85:    * To set the blocklet size of CarbonData file
1:cf1e4d4:    * @param blockletSize is blocklet size in MB
1:4b98af2:    * default value is 64 MB
1:cf1e4d4:    * @return updated CarbonWriterBuilder
1:cf1e4d4:    */
1:1d827c7:   public CarbonWriterBuilder withBlockletSize(int blockletSize) {
1:1d827c7:     if (blockletSize <= 0) {
1:1d827c7:       throw new IllegalArgumentException("blockletSize should be greater than zero");
1:b7b8073:     }
1:6cb6f83:     this.blockletSize = blockletSize;
1:6cb6f83:     return this;
16:1d827c7:   }
1:1d827c7: 
1:242c08b:   /**
1:17a4b48:    * This writer is not thread safe,
1:17a4b48:    * use buildThreadSafeWriterForCSVInput in multi thread environment
1:17a4b48:    * Build a {@link CarbonWriter}, which accepts row in CSV format
1:26eb2d0:    * @param schema carbon Schema object {org.apache.carbondata.sdk.file.Schema}
1:4b98af2:    * @return CSVCarbonWriter
1:4b98af2:    * @throws IOException
1:4b98af2:    * @throws InvalidLoadOptionException
1:1d827c7:    */
1:8f1a029:   public CarbonWriter buildWriterForCSVInput(Schema schema, Configuration configuration)
1:26eb2d0:       throws IOException, InvalidLoadOptionException {
1:e39b0a1:     Objects.requireNonNull(schema, "schema should not be null");
1:e39b0a1:     Objects.requireNonNull(path, "path should not be null");
1:26eb2d0:     this.schema = schema;
1:347b8e1:     CarbonLoadModel loadModel = buildLoadModel(schema);
1:8f1a029:     return new CSVCarbonWriter(loadModel, configuration);
1:e39b0a1:   }
1:ec33c11: 
1:b7b8073:   /**
1:1d827c7:    *
1:89cfd8e:    * Build a {@link CarbonWriter}, which accepts row in CSV format
1:17a4b48:    * @param schema carbon Schema object {org.apache.carbondata.sdk.file.Schema}
1:17a4b48:    * @param numOfThreads number of threads() in which .write will be called.
1:17a4b48:    * @return CSVCarbonWriter
1:17a4b48:    * @throws IOException
1:17a4b48:    * @throws InvalidLoadOptionException
1:17a4b48:    */
1:8f1a029:   public CarbonWriter buildThreadSafeWriterForCSVInput(Schema schema, short numOfThreads,
1:8f1a029:       Configuration configuration) throws IOException, InvalidLoadOptionException {
1:17a4b48:     Objects.requireNonNull(schema, "schema should not be null");
1:17a4b48:     Objects.requireNonNull(numOfThreads, "numOfThreads should not be null");
1:17a4b48:     Objects.requireNonNull(path, "path should not be null");
1:17a4b48:     this.schema = schema;
1:17a4b48:     if (numOfThreads <= 0) {
1:17a4b48:       throw new IllegalArgumentException(" numOfThreads must be greater than 0");
1:17a4b48:     }
1:17a4b48:     CarbonLoadModel loadModel = buildLoadModel(schema);
1:94d2089:     loadModel.setSdkWriterCores(numOfThreads);
1:8f1a029:     return new CSVCarbonWriter(loadModel, configuration);
1:17a4b48:   }
1:17a4b48: 
1:17a4b48:   /**
1:17a4b48:    * This writer is not thread safe,
1:17a4b48:    * use buildThreadSafeWriterForAvroInput in multi thread environment
1:17a4b48:    * Build a {@link CarbonWriter}, which accepts Avro object
1:26eb2d0:    * @param avroSchema avro Schema object {org.apache.avro.Schema}
1:17a4b48:    * @return AvroCarbonWriter
1:17a4b48:    * @throws IOException
1:17a4b48:    * @throws InvalidLoadOptionException
1:17a4b48:    */
1:8f1a029:   public CarbonWriter buildWriterForAvroInput(org.apache.avro.Schema avroSchema,
1:8f1a029:       Configuration configuration) throws IOException, InvalidLoadOptionException {
1:26eb2d0:     this.schema = AvroCarbonWriter.getCarbonSchemaFromAvroSchema(avroSchema);
2:1d827c7:     Objects.requireNonNull(schema, "schema should not be null");
1:1d827c7:     Objects.requireNonNull(path, "path should not be null");
1:347b8e1:     CarbonLoadModel loadModel = buildLoadModel(schema);
1:ec33c11:     // AVRO records are pushed to Carbon as Object not as Strings. This was done in order to
1:ec33c11:     // handle multi level complex type support. As there are no conversion converter step is
1:ec33c11:     // removed from the load. LoadWithoutConverter flag is going to point to the Loader Builder
1:ec33c11:     // which will skip Conversion Step.
1:cf55028:     loadModel.setLoadWithoutConverterStep(true);
1:8f1a029:     return new AvroCarbonWriter(loadModel, configuration);
1:17a4b48:   }
1:e39b0a1: 
2:1d827c7:   /**
1:89cfd8e:    * Build a {@link CarbonWriter}, which accepts Avro object
1:17a4b48:    * @param avroSchema avro Schema object {org.apache.avro.Schema}
1:17a4b48:    * @param numOfThreads number of threads() in which .write will be called.
1:4b98af2:    * @return AvroCarbonWriter
1:1d827c7:    * @throws IOException
1:4b98af2:    * @throws InvalidLoadOptionException
1:1d827c7:    */
1:17a4b48:   public CarbonWriter buildThreadSafeWriterForAvroInput(org.apache.avro.Schema avroSchema,
1:8f1a029:       short numOfThreads, Configuration configuration)
1:26eb2d0:       throws IOException, InvalidLoadOptionException {
1:17a4b48:     this.schema = AvroCarbonWriter.getCarbonSchemaFromAvroSchema(avroSchema);
1:17a4b48:     Objects.requireNonNull(schema, "schema should not be null");
1:17a4b48:     Objects.requireNonNull(path, "path should not be null");
1:17a4b48:     Objects.requireNonNull(numOfThreads, "numOfThreads should not be null");
1:17a4b48:     if (numOfThreads <= 0) {
1:17a4b48:       throw new IllegalArgumentException(" numOfThreads must be greater than 0");
1:17a4b48:     }
1:17a4b48:     CarbonLoadModel loadModel = buildLoadModel(schema);
1:17a4b48:     // AVRO records are pushed to Carbon as Object not as Strings. This was done in order to
1:17a4b48:     // handle multi level complex type support. As there are no conversion converter step is
1:17a4b48:     // removed from the load. LoadWithoutConverter flag is going to point to the Loader Builder
1:17a4b48:     // which will skip Conversion Step.
1:17a4b48:     loadModel.setLoadWithoutConverterStep(true);
1:94d2089:     loadModel.setSdkWriterCores(numOfThreads);
1:8f1a029:     return new AvroCarbonWriter(loadModel, configuration);
1:1d827c7:   }
1:17a4b48: 
1:17a4b48:   /**
1:17a4b48:    * This writer is not thread safe,
1:17a4b48:    * use buildThreadSafeWriterForJsonInput in multi thread environment
1:5804d75:    * Build a {@link CarbonWriter}, which accepts Json object
1:5804d75:    * @param carbonSchema carbon Schema object
1:5804d75:    * @return JsonCarbonWriter
1:5804d75:    * @throws IOException
1:5804d75:    * @throws InvalidLoadOptionException
1:5804d75:    */
1:8f1a029:   public JsonCarbonWriter buildWriterForJsonInput(Schema carbonSchema, Configuration configuration)
1:17a4b48:       throws IOException, InvalidLoadOptionException {
1:5804d75:     Objects.requireNonNull(carbonSchema, "schema should not be null");
1:5804d75:     Objects.requireNonNull(path, "path should not be null");
1:5804d75:     this.schema = carbonSchema;
1:347b8e1:     CarbonLoadModel loadModel = buildLoadModel(carbonSchema);
1:5804d75:     loadModel.setJsonFileLoad(true);
1:8f1a029:     return new JsonCarbonWriter(loadModel, configuration);
1:5804d75:   }
1:5804d75: 
1:1d827c7:   /**
1:17a4b48:    * Can use this writer in multi-thread instance.
1:1d827c7:    *
1:17a4b48:    * Build a {@link CarbonWriter}, which accepts Json object
1:17a4b48:    * @param carbonSchema carbon Schema object
1:17a4b48:    * @param numOfThreads number of threads() in which .write will be called.
1:17a4b48:    * @return JsonCarbonWriter
1:17a4b48:    * @throws IOException
1:17a4b48:    * @throws InvalidLoadOptionException
1:17a4b48:    */
1:8f1a029:   public JsonCarbonWriter buildThreadSafeWriterForJsonInput(Schema carbonSchema, short numOfThreads,
1:8f1a029:       Configuration configuration) throws IOException, InvalidLoadOptionException {
1:17a4b48:     Objects.requireNonNull(carbonSchema, "schema should not be null");
1:17a4b48:     Objects.requireNonNull(path, "path should not be null");
1:17a4b48:     if (numOfThreads <= 0) {
1:17a4b48:       throw new IllegalArgumentException(" numOfThreads must be greater than 0");
1:17a4b48:     }
1:17a4b48:     this.schema = carbonSchema;
1:17a4b48:     CarbonLoadModel loadModel = buildLoadModel(schema);
1:17a4b48:     loadModel.setJsonFileLoad(true);
1:94d2089:     loadModel.setSdkWriterCores(numOfThreads);
1:8f1a029:     return new JsonCarbonWriter(loadModel, configuration);
1:17a4b48:   }
1:17a4b48: 
1:b1c85fa:   private void setCsvHeader(CarbonLoadModel model) {
1:b1c85fa:     Field[] fields = schema.getFields();
1:b1c85fa:     StringBuilder builder = new StringBuilder();
1:b1c85fa:     String[] columns = new String[fields.length];
1:b1c85fa:     int i = 0;
1:b1c85fa:     for (Field field : fields) {
1:3ea2a1d:       if (null != field) {
1:b1c85fa:         builder.append(field.getFieldName());
1:b1c85fa:         builder.append(",");
1:b1c85fa:         columns[i++] = field.getFieldName();
1:b1c85fa:       }
1:b1c85fa:     }
1:b1c85fa:     String header = builder.toString();
1:b1c85fa:     model.setCsvHeader(header.substring(0, header.length() - 1));
1:b1c85fa:     model.setCsvHeaderColumns(columns);
1:b1c85fa:   }
1:b1c85fa: 
1:347b8e1:   public CarbonLoadModel buildLoadModel(Schema carbonSchema)
1:17a4b48:       throws IOException, InvalidLoadOptionException {
1:a9cc434:     Set<String> longStringColumns = null;
1:a9cc434:     if (options != null && options.get("long_string_columns") != null) {
1:a9cc434:       longStringColumns =
1:a9cc434:           new HashSet<>(Arrays.asList(options.get("long_string_columns").toLowerCase().split(",")));
1:a9cc434:       validateLongStringColumns(carbonSchema, longStringColumns);
1:a9cc434:     }
1:a9cc434:     this.schema = updateSchemaFields(carbonSchema, longStringColumns);
1:e39b0a1:     // build CarbonTable using schema
1:e39b0a1:     CarbonTable table = buildCarbonTable();
1:e39b0a1:     if (persistSchemaFile) {
1:e39b0a1:       // we are still using the traditional carbon table folder structure
1:e39b0a1:       persistSchemaFile(table, CarbonTablePath.getSchemaFilePath(path));
1:e39b0a1:     }
1:e39b0a1:     // build LoadModel
1:5f32647:     return buildLoadModel(table, UUID, taskNo, options);
1:1d827c7:   }
1:b1c85fa: 
1:a9cc434:   private void validateLongStringColumns(Schema carbonSchema, Set<String> longStringColumns) {
1:a9cc434:     // long string columns must be string or varchar type
1:a9cc434:     for (Field field :carbonSchema.getFields()) {
1:a9cc434:       if (longStringColumns.contains(field.getFieldName().toLowerCase()) && (
1:a9cc434:           (field.getDataType() != DataTypes.STRING) && field.getDataType() != DataTypes.VARCHAR)) {
1:a9cc434:         throw new RuntimeException(
1:a9cc434:             "long string column : " + field.getFieldName() + "is not supported for data type: "
1:a9cc434:                 + field.getDataType());
1:a9cc434:       }
1:a9cc434:     }
1:a9cc434:     // long string columns must not be present in sort columns
1:a9cc434:     if (sortColumns != null) {
1:a9cc434:       for (String col : sortColumns) {
1:a9cc434:         // already will be in lower case
1:a9cc434:         if (longStringColumns.contains(col)) {
1:a9cc434:           throw new RuntimeException(
1:a9cc434:               "long string column : " + col + "must not be present in sort columns");
1:a9cc434:         }
1:a9cc434:       }
1:a9cc434:     }
1:a9cc434:   }
1:a9cc434: 
1:1d827c7:   /**
1:1d827c7:    * Build a {@link CarbonTable}
1:1d827c7:    */
1:1d827c7:   private CarbonTable buildCarbonTable() {
1:1d827c7:     TableSchemaBuilder tableSchemaBuilder = TableSchema.builder();
1:cf1e4d4:     if (blockSize > 0) {
1:6cb6f83:       tableSchemaBuilder = tableSchemaBuilder.blockSize(blockSize);
1:6cb6f83:     }
1:e39b0a1: 
1:cf1e4d4:     if (blockletSize > 0) {
1:cf1e4d4:       tableSchemaBuilder = tableSchemaBuilder.blockletSize(blockletSize);
1:cf1e4d4:     }
1:3109d04:     tableSchemaBuilder.enableLocalDictionary(isLocalDictionaryEnabled);
1:3109d04:     tableSchemaBuilder.localDictionaryThreshold(localDictionaryThreshold);
1:5f32647:     List<String> sortColumnsList = new ArrayList<>();
1:5f32647:     if (sortColumns == null) {
1:5f32647:       // If sort columns are not specified, default set all dimensions to sort column.
1:5f32647:       // When dimensions are default set to sort column,
1:5f32647:       // Inverted index will be supported by default for sort columns.
1:3ea2a1d:       //Null check for field to handle hole in field[] ex.
1:3ea2a1d:       //  user passed size 4 but supplied only 2 fileds
1:5f32647:       for (Field field : schema.getFields()) {
1:3ea2a1d:         if (null != field) {
1:5f32647:           if (field.getDataType() == DataTypes.STRING ||
1:b1c85fa:               field.getDataType() == DataTypes.DATE  ||
1:5f32647:               field.getDataType() == DataTypes.TIMESTAMP) {
1:5f32647:             sortColumnsList.add(field.getFieldName());
1:5f32647:           }
1:5f32647:         }
1:3ea2a1d:       }
1:5f32647:       sortColumns = new String[sortColumnsList.size()];
1:5f32647:       sortColumns = sortColumnsList.toArray(sortColumns);
1:1372450:     } else {
1:5f32647:       sortColumnsList = Arrays.asList(sortColumns);
1:3ea2a1d:     }
1:93724ec:     ColumnSchema[] sortColumnsSchemaList = new ColumnSchema[sortColumnsList.size()];
1:b1c85fa:     Field[] fields = schema.getFields();
1:b1c85fa:     buildTableSchema(fields, tableSchemaBuilder, sortColumnsList, sortColumnsSchemaList);
1:cf1e4d4: 
1:93724ec:     tableSchemaBuilder.setSortColumns(Arrays.asList(sortColumnsSchemaList));
1:280a400:     String tableName;
1:280a400:     String dbName;
1:b7b8073:     if (isTransactionalTable) {
1:280a400:       tableName = "_tempTable";
1:280a400:       dbName = "_tempDB";
1:280a400:     } else {
1:78efc7f:       dbName = "";
1:78efc7f:       tableName = "_tempTable_" + String.valueOf(UUID);
1:280a400:     }
1:1d827c7:     TableSchema schema = tableSchemaBuilder.build();
1:1d827c7:     schema.setTableName(tableName);
1:b1c85fa:     CarbonTable table =
1:b1c85fa:         CarbonTable.builder().tableName(schema.getTableName()).databaseName(dbName).tablePath(path)
1:b1c85fa:             .tableSchema(schema).isTransactionalTable(isTransactionalTable).build();
1:1d827c7:     return table;
1:1d827c7:   }
1:b1c85fa: 
1:b1c85fa:   private void buildTableSchema(Field[] fields, TableSchemaBuilder tableSchemaBuilder,
1:b1c85fa:       List<String> sortColumnsList, ColumnSchema[] sortColumnsSchemaList) {
1:cf666c1:     Set<String> uniqueFields = new HashSet<>();
1:35a7b5e:     // a counter which will be used in case of complex array type. This valIndex will be assigned
1:35a7b5e:     // to child of complex array type in the order val1, val2 so that each array type child is
1:35a7b5e:     // differentiated to any level
1:35a7b5e:     AtomicInteger valIndex = new AtomicInteger(0);
1:6b70b7e:     // Check if any of the columns specified in sort columns are missing from schema.
1:6b70b7e:     for (String sortColumn: sortColumnsList) {
1:6b70b7e:       boolean exists = false;
1:6b70b7e:       for (Field field : fields) {
1:6b70b7e:         if (field.getFieldName().equalsIgnoreCase(sortColumn)) {
1:6b70b7e:           exists = true;
1:6b70b7e:           break;
1:6b70b7e:         }
1:6b70b7e:       }
1:6b70b7e:       if (!exists) {
1:6b70b7e:         throw new RuntimeException(
1:6b70b7e:             "column: " + sortColumn + " specified in sort columns does not exist in schema");
1:6b70b7e:       }
1:6b70b7e:     }
1:1c5b526:     int i = 0;
1:b1c85fa:     for (Field field : fields) {
1:b1c85fa:       if (null != field) {
1:cf666c1:         if (!uniqueFields.add(field.getFieldName())) {
1:cf666c1:           throw new RuntimeException(
1:cf666c1:               "Duplicate column " + field.getFieldName() + " found in table schema");
1:cf666c1:         }
1:93724ec:         int isSortColumn = sortColumnsList.indexOf(field.getFieldName());
2:b1c85fa:         if (isSortColumn > -1) {
1:b1c85fa:           // unsupported types for ("array", "struct", "double", "float", "decimal")
1:b1c85fa:           if (field.getDataType() == DataTypes.DOUBLE || field.getDataType() == DataTypes.FLOAT
1:1345dc6:               || DataTypes.isDecimal(field.getDataType()) || field.getDataType().isComplexType()
1:1345dc6:               || field.getDataType() == DataTypes.VARCHAR) {
1:b1c85fa:             throw new RuntimeException(
1:1345dc6:                 " sort columns not supported for array, struct, double, float, decimal, varchar");
1:b1c85fa:           }
1:b1c85fa:         }
1:b1c85fa:         if (field.getChildren() != null && field.getChildren().size() > 0) {
1:b1c85fa:           if (field.getDataType().getName().equalsIgnoreCase("ARRAY")) {
1:b1c85fa:             // Loop through the inner columns and for a StructData
1:b1c85fa:             DataType complexType =
1:b1c85fa:                 DataTypes.createArrayType(field.getChildren().get(0).getDataType());
1:35a7b5e:             tableSchemaBuilder
1:35a7b5e:                 .addColumn(new StructField(field.getFieldName(), complexType), valIndex, false);
1:b1c85fa:           } else if (field.getDataType().getName().equalsIgnoreCase("STRUCT")) {
1:b1c85fa:             // Loop through the inner columns and for a StructData
1:b1c85fa:             List<StructField> structFieldsArray =
1:b1c85fa:                 new ArrayList<StructField>(field.getChildren().size());
1:b1c85fa:             for (StructField childFld : field.getChildren()) {
1:b1c85fa:               structFieldsArray
1:b1c85fa:                   .add(new StructField(childFld.getFieldName(), childFld.getDataType()));
1:b1c85fa:             }
1:b1c85fa:             DataType complexType = DataTypes.createStructType(structFieldsArray);
1:35a7b5e:             tableSchemaBuilder
1:35a7b5e:                 .addColumn(new StructField(field.getFieldName(), complexType), valIndex, false);
1:fb6dffe:           } else if (field.getDataType().getName().equalsIgnoreCase("MAP")) {
1:fb6dffe:             // Loop through the inner columns for MapType
1:68b359e:             DataType mapType = DataTypes.createMapType(((MapType) field.getDataType()).getKeyType(),
1:68b359e:                 field.getChildren().get(0).getDataType());
1:fb6dffe:             tableSchemaBuilder
1:fb6dffe:                 .addColumn(new StructField(field.getFieldName(), mapType), valIndex, false);
1:b1c85fa:           }
1:b1c85fa:         } else {
1:93724ec:           ColumnSchema columnSchema = tableSchemaBuilder
1:93724ec:               .addColumn(new StructField(field.getFieldName(), field.getDataType()),
1:35a7b5e:                   valIndex, isSortColumn > -1);
1:93724ec:           if (isSortColumn > -1) {
2:1c5b526:             columnSchema.setSortColumn(true);
1:93724ec:             sortColumnsSchemaList[isSortColumn] = columnSchema;
1:93724ec:           }
1:b1c85fa:         }
1:b1c85fa:       }
1:b1c85fa:     }
1:b1c85fa:   }
1:b1c85fa: 
1:1d827c7:   /**
1:1d827c7:    * Save the schema of the {@param table} to {@param persistFilePath}
1:1d827c7:    * @param table table object containing schema
1:1d827c7:    * @param persistFilePath absolute file path with file name
1:1d827c7:    */
1:1d827c7:   private void persistSchemaFile(CarbonTable table, String persistFilePath) throws IOException {
1:1d827c7:     TableInfo tableInfo = table.getTableInfo();
1:1d827c7:     String schemaMetadataPath = CarbonTablePath.getFolderContainingFile(persistFilePath);
1:1d827c7:     CarbonMetadata.getInstance().loadTableMetadata(tableInfo);
1:1d827c7:     SchemaConverter schemaConverter = new ThriftWrapperSchemaConverterImpl();
1:1d827c7:     org.apache.carbondata.format.TableInfo thriftTableInfo =
1:1d827c7:         schemaConverter.fromWrapperToExternalTableInfo(
1:1d827c7:             tableInfo,
1:1d827c7:             tableInfo.getDatabaseName(),
1:1d827c7:             tableInfo.getFactTable().getTableName());
1:1d827c7:     org.apache.carbondata.format.SchemaEvolutionEntry schemaEvolutionEntry =
1:1d827c7:         new org.apache.carbondata.format.SchemaEvolutionEntry(
1:1d827c7:             tableInfo.getLastUpdatedTime());
1:1d827c7:     thriftTableInfo.getFact_table().getSchema_evolution().getSchema_evolution_history()
1:1d827c7:         .add(schemaEvolutionEntry);
1:1d827c7:     FileFactory.FileType fileType = FileFactory.getFileType(schemaMetadataPath);
1:1d827c7:     if (!FileFactory.isFileExist(schemaMetadataPath, fileType)) {
1:1d827c7:       FileFactory.mkdirs(schemaMetadataPath, fileType);
1:b1c85fa:     }
1:1d827c7:     ThriftWriter thriftWriter = new ThriftWriter(persistFilePath, false);
1:1d827c7:     thriftWriter.open();
1:1d827c7:     thriftWriter.write(thriftTableInfo);
1:1d827c7:     thriftWriter.close();
1:3202cf5:   }
1:6cb6f83: 
1:1d827c7:   /**
1:1d827c7:    * Build a {@link CarbonLoadModel}
1:1d827c7:    */
1:5f32647:   private CarbonLoadModel buildLoadModel(CarbonTable table, long UUID, String taskNo,
1:5f32647:       Map<String, String> options) throws InvalidLoadOptionException, IOException {
1:5f32647:     if (options == null) {
1:5f32647:       options = new HashMap<>();
1:3202cf5:     }
1:89cfd8e:     CarbonLoadModelBuilder builder = new CarbonLoadModelBuilder(table);
1:b1c85fa:     CarbonLoadModel build = builder.build(options, UUID, taskNo);
1:b1c85fa:     setCsvHeader(build);
1:b1c85fa:     return build;
1:26eb2d0:   }
1:f5c7a19: 
1:a9cc434:   /* loop through all the parent column and
1:a9cc434:   a) change fields name to lower case.
1:a9cc434:   this is to match with sort column case.
1:a9cc434:   b) change string fields to varchar type */
1:a9cc434:   private Schema updateSchemaFields(Schema schema, Set<String> longStringColumns) {
1:f5c7a19:     if (schema == null) {
1:f5c7a19:       return null;
1:f5c7a19:     }
1:f5c7a19:     Field[] fields =  schema.getFields();
1:f5c7a19:     for (int i = 0; i < fields.length; i++) {
1:f5c7a19:       if (fields[i] != null) {
1:f5c7a19:         fields[i].updateNameToLowerCase();
1:f5c7a19:       }
1:a9cc434: 
1:a9cc434:       if (longStringColumns != null) {
1:a9cc434:         /* Also update the string type to varchar */
1:a9cc434:         if (longStringColumns.contains(fields[i].getFieldName())) {
1:a9cc434:           fields[i].updateDataTypeToVarchar();
1:a9cc434:         }
1:a9cc434:       }
1:f5c7a19:     }
1:f5c7a19:     return new Schema(fields);
1:f5c7a19:   }
1:a9cc434: 
1:a9cc434:   private void updateToLoadOptions(Map.Entry<String, String> entry) {
1:a9cc434:     if (this.options == null) {
1:a9cc434:       // convert it to treeMap as keys need to be case insensitive
1:a9cc434:       this.options = new TreeMap<>(String.CASE_INSENSITIVE_ORDER);
1:a9cc434:     }
1:a9cc434:     // update it to load options
1:a9cc434:     this.options.put(entry.getKey(), entry.getValue());
1:a9cc434:   }
1:a9cc434: 
1:a9cc434:   private void withSortScope(Map.Entry<String, String> entry) {
1:a9cc434:     String sortScope = entry.getValue();
1:a9cc434:     if (sortScope != null) {
1:a9cc434:       if ((!CarbonUtil.isValidSortOption(sortScope))) {
1:a9cc434:         throw new IllegalArgumentException("Invalid Sort Scope Option: " + sortScope);
1:a9cc434:       } else if (sortScope.equalsIgnoreCase("global_sort")) {
1:a9cc434:         throw new IllegalArgumentException("global sort is not supported");
1:a9cc434:       }
1:a9cc434:     }
1:a9cc434:     // update it to load options
1:a9cc434:     updateToLoadOptions(entry);
1:a9cc434:   }
1:1d827c7: }
============================================================================
author:kunal642
-------------------------------------------------------------------------------
commit:8f1a029
/////////////////////////////////////////////////////////////////////////
1: import org.apache.hadoop.conf.Configuration;
/////////////////////////////////////////////////////////////////////////
/////////////////////////////////////////////////////////////////////////
1:   public CarbonWriter buildWriterForCSVInput(Schema schema, Configuration configuration)
1:     return new CSVCarbonWriter(loadModel, configuration);
/////////////////////////////////////////////////////////////////////////
1:   public CarbonWriter buildThreadSafeWriterForCSVInput(Schema schema, short numOfThreads,
1:       Configuration configuration) throws IOException, InvalidLoadOptionException {
/////////////////////////////////////////////////////////////////////////
1:     return new CSVCarbonWriter(loadModel, configuration);
/////////////////////////////////////////////////////////////////////////
1:   public CarbonWriter buildWriterForAvroInput(org.apache.avro.Schema avroSchema,
1:       Configuration configuration) throws IOException, InvalidLoadOptionException {
/////////////////////////////////////////////////////////////////////////
1:     return new AvroCarbonWriter(loadModel, configuration);
/////////////////////////////////////////////////////////////////////////
1:       short numOfThreads, Configuration configuration)
/////////////////////////////////////////////////////////////////////////
1:     return new AvroCarbonWriter(loadModel, configuration);
/////////////////////////////////////////////////////////////////////////
1:   public JsonCarbonWriter buildWriterForJsonInput(Schema carbonSchema, Configuration configuration)
1:     return new JsonCarbonWriter(loadModel, configuration);
/////////////////////////////////////////////////////////////////////////
1:   public JsonCarbonWriter buildThreadSafeWriterForJsonInput(Schema carbonSchema, short numOfThreads,
1:       Configuration configuration) throws IOException, InvalidLoadOptionException {
/////////////////////////////////////////////////////////////////////////
1:     return new JsonCarbonWriter(loadModel, configuration);
commit:2a9604c
/////////////////////////////////////////////////////////////////////////
0: import org.apache.carbondata.core.util.CarbonSessionInfo;
0: import org.apache.carbondata.core.util.ThreadLocalSessionInfo;
/////////////////////////////////////////////////////////////////////////
0:   public CarbonWriterBuilder() {
0:     ThreadLocalSessionInfo.setCarbonSessionInfo(new CarbonSessionInfo());
1:   }
1: 
commit:cf666c1
/////////////////////////////////////////////////////////////////////////
1: import java.util.HashSet;
1: import java.util.Set;
/////////////////////////////////////////////////////////////////////////
1:     Set<String> uniqueFields = new HashSet<>();
/////////////////////////////////////////////////////////////////////////
1:         if (!uniqueFields.add(field.getFieldName())) {
1:           throw new RuntimeException(
1:               "Duplicate column " + field.getFieldName() + " found in table schema");
1:         }
commit:26eb2d0
/////////////////////////////////////////////////////////////////////////
/////////////////////////////////////////////////////////////////////////
1:    * @param schema carbon Schema object {org.apache.carbondata.sdk.file.Schema}
0:   public CarbonWriter buildWriterForCSVInput(Schema schema)
1:       throws IOException, InvalidLoadOptionException {
1:     this.schema = schema;
1:    * @param avroSchema avro Schema object {org.apache.avro.Schema}
0:   public CarbonWriter buildWriterForAvroInput(org.apache.avro.Schema avroSchema)
1:       throws IOException, InvalidLoadOptionException {
1:     this.schema = AvroCarbonWriter.getCarbonSchemaFromAvroSchema(avroSchema);
/////////////////////////////////////////////////////////////////////////
1: }
commit:6b70b7e
/////////////////////////////////////////////////////////////////////////
1:     // Check if any of the columns specified in sort columns are missing from schema.
1:     for (String sortColumn: sortColumnsList) {
1:       boolean exists = false;
1:       for (Field field : fields) {
1:         if (field.getFieldName().equalsIgnoreCase(sortColumn)) {
1:           exists = true;
1:           break;
1:         }
1:       }
1:       if (!exists) {
1:         throw new RuntimeException(
1:             "column: " + sortColumn + " specified in sort columns does not exist in schema");
1:       }
1:     }
/////////////////////////////////////////////////////////////////////////
0:             checkForUnsupportedDataTypes(field.getChildren().get(0).getDataType());
/////////////////////////////////////////////////////////////////////////
0:               checkForUnsupportedDataTypes(childFld.getDataType());
/////////////////////////////////////////////////////////////////////////
0:   private void checkForUnsupportedDataTypes(DataType dataType) {
0:     if (dataType == DataTypes.DOUBLE || dataType == DataTypes.DATE || DataTypes
0:         .isDecimal(dataType)) {
0:       throw new RuntimeException("Unsupported data type: " + dataType.getName());
1:     }
1:   }
1: 
author:ajantha-bhat
-------------------------------------------------------------------------------
commit:a9cc434
/////////////////////////////////////////////////////////////////////////
/////////////////////////////////////////////////////////////////////////
/////////////////////////////////////////////////////////////////////////
1:           !option.equalsIgnoreCase("escapechar")) {
1:     if (this.options == null) {
1:       // convert it to treeMap as keys need to be case insensitive
1:       this.options = new TreeMap<>(String.CASE_INSENSITIVE_ORDER);
1:     }
1:     this.options.putAll(options);
/////////////////////////////////////////////////////////////////////////
1:    * j. sort_scope -- "local_sort", "no_sort", "batch_sort". default value is "local_sort"
1:    * k. long_string_columns -- comma separated string columns which are more than 32k length.
1:    *                           default value is null.
/////////////////////////////////////////////////////////////////////////
1:             "local_dictionary_enable", "sort_columns", "sort_scope", "long_string_columns"));
/////////////////////////////////////////////////////////////////////////
1:       } else if (entry.getKey().equalsIgnoreCase("sort_scope")) {
1:         this.withSortScope(entry);
1:       } else if (entry.getKey().equalsIgnoreCase("long_string_columns")) {
1:         updateToLoadOptions(entry);
/////////////////////////////////////////////////////////////////////////
1:     Set<String> longStringColumns = null;
1:     if (options != null && options.get("long_string_columns") != null) {
1:       longStringColumns =
1:           new HashSet<>(Arrays.asList(options.get("long_string_columns").toLowerCase().split(",")));
1:       validateLongStringColumns(carbonSchema, longStringColumns);
1:     }
1:     this.schema = updateSchemaFields(carbonSchema, longStringColumns);
/////////////////////////////////////////////////////////////////////////
1:   private void validateLongStringColumns(Schema carbonSchema, Set<String> longStringColumns) {
1:     // long string columns must be string or varchar type
1:     for (Field field :carbonSchema.getFields()) {
1:       if (longStringColumns.contains(field.getFieldName().toLowerCase()) && (
1:           (field.getDataType() != DataTypes.STRING) && field.getDataType() != DataTypes.VARCHAR)) {
1:         throw new RuntimeException(
1:             "long string column : " + field.getFieldName() + "is not supported for data type: "
1:                 + field.getDataType());
1:       }
1:     }
1:     // long string columns must not be present in sort columns
1:     if (sortColumns != null) {
1:       for (String col : sortColumns) {
1:         // already will be in lower case
1:         if (longStringColumns.contains(col)) {
1:           throw new RuntimeException(
1:               "long string column : " + col + "must not be present in sort columns");
1:         }
1:       }
1:     }
1:   }
1: 
/////////////////////////////////////////////////////////////////////////
1:   /* loop through all the parent column and
1:   a) change fields name to lower case.
1:   this is to match with sort column case.
1:   b) change string fields to varchar type */
1:   private Schema updateSchemaFields(Schema schema, Set<String> longStringColumns) {
/////////////////////////////////////////////////////////////////////////
1: 
1:       if (longStringColumns != null) {
1:         /* Also update the string type to varchar */
1:         if (longStringColumns.contains(fields[i].getFieldName())) {
1:           fields[i].updateDataTypeToVarchar();
1:         }
1:       }
1: 
1:   private void updateToLoadOptions(Map.Entry<String, String> entry) {
1:     if (this.options == null) {
1:       // convert it to treeMap as keys need to be case insensitive
1:       this.options = new TreeMap<>(String.CASE_INSENSITIVE_ORDER);
1:     }
1:     // update it to load options
1:     this.options.put(entry.getKey(), entry.getValue());
1:   }
1: 
1:   private void withSortScope(Map.Entry<String, String> entry) {
1:     String sortScope = entry.getValue();
1:     if (sortScope != null) {
1:       if ((!CarbonUtil.isValidSortOption(sortScope))) {
1:         throw new IllegalArgumentException("Invalid Sort Scope Option: " + sortScope);
1:       } else if (sortScope.equalsIgnoreCase("global_sort")) {
1:         throw new IllegalArgumentException("global sort is not supported");
1:       }
1:     }
1:     // update it to load options
1:     updateToLoadOptions(entry);
1:   }
commit:f5c7a19
/////////////////////////////////////////////////////////////////////////
1: import org.apache.carbondata.core.util.CarbonUtil;
/////////////////////////////////////////////////////////////////////////
1:     if (sortColumns != null) {
1:       for (int i = 0; i < sortColumns.length; i++) {
1:         sortColumns[i] = sortColumns[i].toLowerCase();
1:       }
1:     }
/////////////////////////////////////////////////////////////////////////
0:    * j. sort_scope -- "local_sort", "no_sort", "batch_sort"
/////////////////////////////////////////////////////////////////////////
0:    * j. sort_scope -- "local_sort"
/////////////////////////////////////////////////////////////////////////
0:           !option.equalsIgnoreCase("escapechar") &&
0:           !option.equalsIgnoreCase("sort_scope")) {
1:         throw new IllegalArgumentException("Unsupported option:" + option
1:             + ". Refer method header or documentation");
1:       }
1:     }
0:     // validate sort scope
0:     String sortScope = options.get("sort_scope");
0:     if (sortScope != null) {
0:       if ((!CarbonUtil.isValidSortOption(sortScope))) {
0:         throw new IllegalArgumentException("Invalid Sort Scope Option: " + sortScope);
0:       } else if (sortScope.equalsIgnoreCase("global_sort")) {
0:         throw new IllegalArgumentException("global sort is not supported");
/////////////////////////////////////////////////////////////////////////
1:    * a. table_blocksize -- [1-2048] values in MB. Default value is 1024
1:    * b. table_blocklet_size -- values in MB. Default value is 64 MB
1:    * c. local_dictionary_threshold -- positive value, default is 10000
1:    * d. local_dictionary_enable -- true / false. Default is false
1:    * e. sort_columns -- comma separated column. "c1,c2". Default all dimensions are sorted.
/////////////////////////////////////////////////////////////////////////
1:         .asList("table_blocksize", "table_blocklet_size", "local_dictionary_threshold",
0:             "local_dictionary_enable", "sort_columns"));
/////////////////////////////////////////////////////////////////////////
1:       if (entry.getKey().equalsIgnoreCase("table_blocksize")) {
1:       } else if (entry.getKey().equalsIgnoreCase("table_blocklet_size")) {
1:       } else if (entry.getKey().equalsIgnoreCase("local_dictionary_threshold")) {
1:       } else if (entry.getKey().equalsIgnoreCase("local_dictionary_enable")) {
1:       } else if (entry.getKey().equalsIgnoreCase("sort_columns")) {
/////////////////////////////////////////////////////////////////////////
0:     this.schema = schemaFieldNameToLowerCase(carbonSchema);
/////////////////////////////////////////////////////////////////////////
1: 
0:   /* loop through all the parent column and change fields name lower case.
0:   * this is to match with sort column case */
0:   private Schema schemaFieldNameToLowerCase(Schema schema) {
1:     if (schema == null) {
1:       return null;
1:     }
1:     Field[] fields =  schema.getFields();
1:     for (int i = 0; i < fields.length; i++) {
1:       if (fields[i] != null) {
1:         fields[i].updateNameToLowerCase();
1:       }
1:     }
1:     return new Schema(fields);
1:   }
commit:94d2089
/////////////////////////////////////////////////////////////////////////
1:     loadModel.setSdkWriterCores(numOfThreads);
/////////////////////////////////////////////////////////////////////////
1:     loadModel.setSdkWriterCores(numOfThreads);
/////////////////////////////////////////////////////////////////////////
1:     loadModel.setSdkWriterCores(numOfThreads);
commit:17a4b48
/////////////////////////////////////////////////////////////////////////
1:    * This writer is not thread safe,
1:    * use buildThreadSafeWriterForCSVInput in multi thread environment
/////////////////////////////////////////////////////////////////////////
1:    *
1:    * Build a {@link CarbonWriter}, which accepts row in CSV format
1:    * @param schema carbon Schema object {org.apache.carbondata.sdk.file.Schema}
1:    * @param numOfThreads number of threads() in which .write will be called.
1:    * @return CSVCarbonWriter
1:    * @throws IOException
1:    * @throws InvalidLoadOptionException
1:    */
0:   public CarbonWriter buildThreadSafeWriterForCSVInput(Schema schema, short numOfThreads)
1:       throws IOException, InvalidLoadOptionException {
1:     Objects.requireNonNull(schema, "schema should not be null");
1:     Objects.requireNonNull(numOfThreads, "numOfThreads should not be null");
1:     Objects.requireNonNull(path, "path should not be null");
1:     this.schema = schema;
1:     if (numOfThreads <= 0) {
1:       throw new IllegalArgumentException(" numOfThreads must be greater than 0");
1:     }
1:     CarbonLoadModel loadModel = buildLoadModel(schema);
0:     loadModel.setSdkUserCores(numOfThreads);
0:     return new CSVCarbonWriter(loadModel);
1:   }
1: 
1:   /**
1:    * This writer is not thread safe,
1:    * use buildThreadSafeWriterForAvroInput in multi thread environment
/////////////////////////////////////////////////////////////////////////
1:    * Build a {@link CarbonWriter}, which accepts Avro object
1:    * @param avroSchema avro Schema object {org.apache.avro.Schema}
1:    * @param numOfThreads number of threads() in which .write will be called.
1:    * @return AvroCarbonWriter
1:    * @throws IOException
1:    * @throws InvalidLoadOptionException
1:    */
1:   public CarbonWriter buildThreadSafeWriterForAvroInput(org.apache.avro.Schema avroSchema,
0:       short numOfThreads)
1:       throws IOException, InvalidLoadOptionException {
1:     this.schema = AvroCarbonWriter.getCarbonSchemaFromAvroSchema(avroSchema);
1:     Objects.requireNonNull(schema, "schema should not be null");
1:     Objects.requireNonNull(path, "path should not be null");
1:     Objects.requireNonNull(numOfThreads, "numOfThreads should not be null");
1:     if (numOfThreads <= 0) {
1:       throw new IllegalArgumentException(" numOfThreads must be greater than 0");
1:     }
1:     CarbonLoadModel loadModel = buildLoadModel(schema);
1:     // AVRO records are pushed to Carbon as Object not as Strings. This was done in order to
1:     // handle multi level complex type support. As there are no conversion converter step is
1:     // removed from the load. LoadWithoutConverter flag is going to point to the Loader Builder
1:     // which will skip Conversion Step.
1:     loadModel.setLoadWithoutConverterStep(true);
0:     loadModel.setSdkUserCores(numOfThreads);
0:     return new AvroCarbonWriter(loadModel);
1:   }
1: 
1:   /**
1:    * This writer is not thread safe,
1:    * use buildThreadSafeWriterForJsonInput in multi thread environment
/////////////////////////////////////////////////////////////////////////
1:   /**
1:    * Can use this writer in multi-thread instance.
1:    *
1:    * Build a {@link CarbonWriter}, which accepts Json object
1:    * @param carbonSchema carbon Schema object
1:    * @param numOfThreads number of threads() in which .write will be called.
1:    * @return JsonCarbonWriter
1:    * @throws IOException
1:    * @throws InvalidLoadOptionException
1:    */
0:   public JsonCarbonWriter buildThreadSafeWriterForJsonInput(Schema carbonSchema, short numOfThreads)
0:       throws IOException, InvalidLoadOptionException {
1:     Objects.requireNonNull(carbonSchema, "schema should not be null");
1:     Objects.requireNonNull(path, "path should not be null");
0:     Objects.requireNonNull(numOfThreads, "numOfThreads should not be null");
1:     if (numOfThreads <= 0) {
1:       throw new IllegalArgumentException(" numOfThreads must be greater than 0");
1:     }
1:     this.schema = carbonSchema;
1:     CarbonLoadModel loadModel = buildLoadModel(schema);
1:     loadModel.setJsonFileLoad(true);
0:     loadModel.setSdkUserCores(numOfThreads);
0:     return new JsonCarbonWriter(loadModel);
1:   }
1: 
commit:1372450
/////////////////////////////////////////////////////////////////////////
1:    * To support the table properties for sdk writer
1:    *
1:    * @param options key,value pair of create table properties.
1:    * supported keys values are
0:    * a. blocksize -- [1-2048] values in MB. Default value is 1024
0:    * b. blockletsize -- values in MB. Default value is 64 MB
0:    * c. localDictionaryThreshold -- positive value, default is 10000
0:    * d. enableLocalDictionary -- true / false. Default is false
0:    * e. sortcolumns -- comma separated column. "c1,c2". Default all dimensions are sorted.
1:    *
1:    * @return updated CarbonWriterBuilder
1:    */
1:   public CarbonWriterBuilder withTableProperties(Map<String, String> options) {
1:     Objects.requireNonNull(options, "Table properties should not be null");
1:     //validate the options.
1:     if (options.size() > 5) {
1:       throw new IllegalArgumentException("Supports only 5 options now. "
1:           + "Refer method header or documentation");
1:     }
1: 
1:     Set<String> supportedOptions = new HashSet<>(Arrays
0:         .asList("blocksize", "blockletsize", "localdictionarythreshold", "enablelocaldictionary",
0:             "sortcolumns"));
1: 
1:     for (String key : options.keySet()) {
1:       if (!supportedOptions.contains(key.toLowerCase())) {
1:         throw new IllegalArgumentException(
1:             "Unsupported options. " + "Refer method header or documentation");
1:       }
1:     }
1: 
1:     for (Map.Entry<String, String> entry : options.entrySet()) {
0:       if (entry.getKey().equalsIgnoreCase("equalsIgnoreCase")) {
1:         this.withBlockSize(Integer.parseInt(entry.getValue()));
0:       } else if (entry.getKey().equalsIgnoreCase("blockletsize")) {
1:         this.withBlockletSize(Integer.parseInt(entry.getValue()));
0:       } else if (entry.getKey().equalsIgnoreCase("localDictionaryThreshold")) {
1:         this.localDictionaryThreshold(Integer.parseInt(entry.getValue()));
0:       } else if (entry.getKey().equalsIgnoreCase("enableLocalDictionary")) {
1:         this.enableLocalDictionary((entry.getValue().equalsIgnoreCase("true")));
1:       } else {
1:         //sort columns
1:         String[] sortColumns = entry.getValue().split(",");
1:         this.sortBy(sortColumns);
1:       }
1:     }
1:     return this;
1:   }
1: 
1:   /**
/////////////////////////////////////////////////////////////////////////
1:           "Local Dictionary Threshold should be greater than 0");
commit:5804d75
/////////////////////////////////////////////////////////////////////////
/////////////////////////////////////////////////////////////////////////
1:   /**
1:    * Build a {@link CarbonWriter}, which accepts Json object
1:    * @param carbonSchema carbon Schema object
1:    * @return JsonCarbonWriter
1:    * @throws IOException
1:    * @throws InvalidLoadOptionException
1:    */
0:   public JsonCarbonWriter buildWriterForJsonInput(Schema carbonSchema)
0:       throws IOException, InvalidLoadOptionException {
1:     Objects.requireNonNull(carbonSchema, "schema should not be null");
1:     Objects.requireNonNull(path, "path should not be null");
1:     this.schema = carbonSchema;
0:     CarbonLoadModel loadModel = createLoadModel();
1:     loadModel.setJsonFileLoad(true);
0:     return new JsonCarbonWriter(loadModel);
1:   }
1: 
commit:4b98af2
/////////////////////////////////////////////////////////////////////////
1:  * Builder for {@link CarbonWriter}
/////////////////////////////////////////////////////////////////////////
1:    * This method must be called when building CarbonWriterBuilder
/////////////////////////////////////////////////////////////////////////
0:    * This method must be called when building CarbonWriterBuilder
/////////////////////////////////////////////////////////////////////////
1:    * If it is null or by default all dimensions are selected for sorting
1:    * If it is empty array, no columns are sorted
/////////////////////////////////////////////////////////////////////////
1:    * will set taskNo in order to avoid conflicts in file's name during write.
1:    * @param taskNo is the TaskNo user wants to specify.
1:    * by default it is system time in nano seconds.
/////////////////////////////////////////////////////////////////////////
1:    * @param persist is a boolean value, If set to true, creates a schema file in metadata folder.
1:    * By default set to false. will not create metadata folder
/////////////////////////////////////////////////////////////////////////
1:    * @param isTransactionalTable is a boolelan value
0:    * if set to false, then writes the carbondata and carbonindex files
1:    * in a flat folder structure.
0:    * if set to true, then writes the carbondata and carbonindex files
0:    * in segment folder structure..
1:    * By default set to false.
/////////////////////////////////////////////////////////////////////////
1:    * @param UUID is a timestamp to be used in the carbondata and carbonindex index files.
1:    * By default set to zero.
/////////////////////////////////////////////////////////////////////////
1:    * supported keys values are
1:    * a. bad_records_logger_enable -- true (write into separate logs), false
1:    * b. bad_records_action -- FAIL, FORCE, IGNORE, REDIRECT
1:    * c. bad_record_path -- path
1:    * d. dateformat -- same as JAVA SimpleDateFormat
1:    * e. timestampformat -- same as JAVA SimpleDateFormat
1:    * f. complex_delimiter_level_1 -- value to Split the complexTypeData
1:    * g. complex_delimiter_level_2 -- value to Split the nested complexTypeData
1:    * h. quotechar
1:    * i. escapechar
1:    *
1:    * Default values are as follows.
1:    *
1:    * a. bad_records_logger_enable -- "false"
1:    * b. bad_records_action -- "FAIL"
1:    * c. bad_record_path -- ""
1:    * d. dateformat -- "" , uses from carbon.properties file
1:    * e. timestampformat -- "", uses from carbon.properties file
1:    * f. complex_delimiter_level_1 -- "$"
1:    * g. complex_delimiter_level_2 -- ":"
1:    * h. quotechar -- "\""
1:    * i. escapechar -- "\\"
/////////////////////////////////////////////////////////////////////////
1:    * default value is 1024 MB
/////////////////////////////////////////////////////////////////////////
1:    * default value is 64 MB
/////////////////////////////////////////////////////////////////////////
1:    * @return CSVCarbonWriter
1:    * @throws IOException
1:    * @throws InvalidLoadOptionException
/////////////////////////////////////////////////////////////////////////
1:    * @return AvroCarbonWriter
1:    * @throws InvalidLoadOptionException
commit:5f32647
/////////////////////////////////////////////////////////////////////////
1: import java.util.ArrayList;
1: import java.util.TreeMap;
/////////////////////////////////////////////////////////////////////////
1: import org.apache.carbondata.core.metadata.datatype.DataTypes;
/////////////////////////////////////////////////////////////////////////
1:   private Map<String, String> options;
/////////////////////////////////////////////////////////////////////////
1:    * @param sortColumns is a string array of columns that needs to be sorted.
0:    *                    If it is null, all dimensions are selected for sorting
0:    *                    If it is empty array, no columns are sorted
/////////////////////////////////////////////////////////////////////////
1:    * To support the load options for sdk writer
1:    * @param options key,value pair of load options.
0:    *                supported keys values are
0:    *                a. bad_records_logger_enable -- true (write into separate logs), false
0:    *                b. bad_records_action -- FAIL, FORCE, IGNORE, REDIRECT
0:    *                c. bad_record_path -- path
0:    *                d. dateformat -- same as JAVA SimpleDateFormat
0:    *                e. timestampformat -- same as JAVA SimpleDateFormat
0:    *                f. complex_delimiter_level_1 -- value to Split the complexTypeData
0:    *                g. complex_delimiter_level_2 -- value to Split the nested complexTypeData
0:    *                h. quotechar
0:    *                i. escapechar
1:    *
1:    * @return updated CarbonWriterBuilder
1:    */
1:   public CarbonWriterBuilder withLoadOptions(Map<String, String> options) {
1:     Objects.requireNonNull(options, "Load options should not be null");
1:     //validate the options.
0:     if (options.size() > 9) {
0:       throw new IllegalArgumentException("Supports only nine options now. "
0:           + "Refer method header or documentation");
1:     }
1: 
1:     for (String option: options.keySet()) {
1:       if (!option.equalsIgnoreCase("bad_records_logger_enable") &&
1:           !option.equalsIgnoreCase("bad_records_action") &&
1:           !option.equalsIgnoreCase("bad_record_path") &&
1:           !option.equalsIgnoreCase("dateformat") &&
1:           !option.equalsIgnoreCase("timestampformat") &&
1:           !option.equalsIgnoreCase("complex_delimiter_level_1") &&
1:           !option.equalsIgnoreCase("complex_delimiter_level_2") &&
1:           !option.equalsIgnoreCase("quotechar") &&
0:           !option.equalsIgnoreCase("escapechar")) {
0:         throw new IllegalArgumentException("Unsupported options. "
0:             + "Refer method header or documentation");
1:       }
1:     }
1: 
0:     // convert it to treeMap as keys need to be case insensitive
0:     Map<String, String> optionsTreeMap = new TreeMap<>(String.CASE_INSENSITIVE_ORDER);
0:     optionsTreeMap.putAll(options);
0:     this.options = optionsTreeMap;
1:     return this;
1:   }
1: 
1:   /**
/////////////////////////////////////////////////////////////////////////
1:     return buildLoadModel(table, UUID, taskNo, options);
/////////////////////////////////////////////////////////////////////////
1:     List<String> sortColumnsList = new ArrayList<>();
1:     if (sortColumns == null) {
1:       // If sort columns are not specified, default set all dimensions to sort column.
1:       // When dimensions are default set to sort column,
1:       // Inverted index will be supported by default for sort columns.
1:       for (Field field : schema.getFields()) {
1:         if (field.getDataType() == DataTypes.STRING ||
0:             field.getDataType() == DataTypes.DATE ||
1:             field.getDataType() == DataTypes.TIMESTAMP) {
1:           sortColumnsList.add(field.getFieldName());
1:         }
1:       }
1:       sortColumns = new String[sortColumnsList.size()];
1:       sortColumns = sortColumnsList.toArray(sortColumns);
1:       sortColumnsList = Arrays.asList(sortColumns);
/////////////////////////////////////////////////////////////////////////
1:   private CarbonLoadModel buildLoadModel(CarbonTable table, long UUID, String taskNo,
1:       Map<String, String> options) throws InvalidLoadOptionException, IOException {
1:     if (options == null) {
1:       options = new HashMap<>();
commit:cf1e4d4
/////////////////////////////////////////////////////////////////////////
1:   /**
0:    * prepares the builder with the schema provided
0:    * @param schema is instance of Schema
1:    * @return updated CarbonWriterBuilder
1:    */
1:   /**
1:    * Sets the output path of the writer builder
1:    * @param path is the absolute path where output files are written
1:    * @return updated CarbonWriterBuilder
1:    */
1:   /**
1:    * sets the list of columns that needs to be in sorted order
0:    * @param sortColumns is a string array of columns that needs to be sorted
1:    * @return updated CarbonWriterBuilder
1:    */
1:   /**
1:    * If set, create a schema file in metadata folder.
0:    * @param persist is a boolean value, If set, create a schema file in metadata folder
1:    * @return updated CarbonWriterBuilder
1:    */
1:   /**
0:    * If set true, writes the carbondata and carbonindex files in a flat folder structure
0:    * @param isUnManagedTable is a boolelan value if set writes
0:    *                     the carbondata and carbonindex files in a flat folder structure
1:    * @return updated CarbonWriterBuilder
1:    */
1:   /**
1:    * to set the timestamp in the carbondata and carbonindex index files
0:    * @param UUID is a timestamp to be used in the carbondata and carbonindex index files
1:    * @return updated CarbonWriterBuilder
1:    */
1:   /**
1:    * To set the carbondata file size in MB between 1MB-2048MB
1:    * @param blockSize is size in MB between 1MB to 2048 MB
1:    * @return updated CarbonWriterBuilder
1:    */
/////////////////////////////////////////////////////////////////////////
1:   /**
0:    * To set the blocklet size of carbondata file
1:    * @param blockletSize is blocklet size in MB
1:    * @return updated CarbonWriterBuilder
1:    */
/////////////////////////////////////////////////////////////////////////
1:     if (blockSize > 0) {
1:     if (blockletSize > 0) {
1:       tableSchemaBuilder = tableSchemaBuilder.blockletSize(blockletSize);
1:     }
1: 
commit:280a400
/////////////////////////////////////////////////////////////////////////
0:   private boolean isUnManagedTable;
1:   private long UUID;
/////////////////////////////////////////////////////////////////////////
0:   public CarbonWriterBuilder unManagedTable(boolean isUnManagedTable) {
0:     Objects.requireNonNull(isUnManagedTable, "UnManaged Table should not be null");
0:     this.isUnManagedTable = isUnManagedTable;
1:     return this;
1:   }
1: 
1:   public CarbonWriterBuilder uniqueIdentifier(long UUID) {
1:     Objects.requireNonNull(UUID, "Unique Identifier should not be null");
1:     this.UUID = UUID;
1:     return this;
1:   }
1: 
1:     if (blockSize <= 0 || blockSize > 2048) {
1:       throw new IllegalArgumentException("blockSize should be between 1 MB to 2048 MB");
/////////////////////////////////////////////////////////////////////////
0:     return buildLoadModel(table, UUID);
/////////////////////////////////////////////////////////////////////////
1:     String tableName;
1:     String dbName;
0:     if (!isUnManagedTable) {
1:       tableName = "_tempTable";
1:       dbName = "_tempDB";
1:     } else {
0:       dbName = null;
0:       tableName = null;
1:     }
/////////////////////////////////////////////////////////////////////////
0:         .isUnManagedTable(isUnManagedTable)
/////////////////////////////////////////////////////////////////////////
0:   private CarbonLoadModel buildLoadModel(CarbonTable table, long UUID)
0:     return builder.build(options, UUID);
author:manishgupta88
-------------------------------------------------------------------------------
commit:68b359e
/////////////////////////////////////////////////////////////////////////
1: import org.apache.carbondata.core.metadata.datatype.MapType;
/////////////////////////////////////////////////////////////////////////
1:             DataType mapType = DataTypes.createMapType(((MapType) field.getDataType()).getKeyType(),
1:                 field.getChildren().get(0).getDataType());
commit:fb6dffe
/////////////////////////////////////////////////////////////////////////
1:           } else if (field.getDataType().getName().equalsIgnoreCase("MAP")) {
1:             // Loop through the inner columns for MapType
0:             DataType mapType =
0:                 DataTypes.createMapType(DataTypes.STRING, field.getChildren().get(0).getDataType());
1:             tableSchemaBuilder
1:                 .addColumn(new StructField(field.getFieldName(), mapType), valIndex, false);
commit:35a7b5e
/////////////////////////////////////////////////////////////////////////
1: import java.util.concurrent.atomic.AtomicInteger;
/////////////////////////////////////////////////////////////////////////
1:     // a counter which will be used in case of complex array type. This valIndex will be assigned
1:     // to child of complex array type in the order val1, val2 so that each array type child is
1:     // differentiated to any level
1:     AtomicInteger valIndex = new AtomicInteger(0);
/////////////////////////////////////////////////////////////////////////
1:             tableSchemaBuilder
1:                 .addColumn(new StructField(field.getFieldName(), complexType), valIndex, false);
/////////////////////////////////////////////////////////////////////////
1:             tableSchemaBuilder
1:                 .addColumn(new StructField(field.getFieldName(), complexType), valIndex, false);
1:                   valIndex, isSortColumn > -1);
author:ravipesala
-------------------------------------------------------------------------------
commit:347b8e1
/////////////////////////////////////////////////////////////////////////
1:     CarbonLoadModel loadModel = buildLoadModel(schema);
/////////////////////////////////////////////////////////////////////////
1:     CarbonLoadModel loadModel = buildLoadModel(schema);
/////////////////////////////////////////////////////////////////////////
1:     CarbonLoadModel loadModel = buildLoadModel(carbonSchema);
/////////////////////////////////////////////////////////////////////////
1:   public CarbonLoadModel buildLoadModel(Schema carbonSchema)
0:       throws IOException, InvalidLoadOptionException {
0:     this.schema = carbonSchema;
author:xuchuanyin
-------------------------------------------------------------------------------
commit:1345dc6
/////////////////////////////////////////////////////////////////////////
1:               || DataTypes.isDecimal(field.getDataType()) || field.getDataType().isComplexType()
1:               || field.getDataType() == DataTypes.VARCHAR) {
1:                 " sort columns not supported for array, struct, double, float, decimal, varchar");
author:BJangir
-------------------------------------------------------------------------------
commit:3109d04
/////////////////////////////////////////////////////////////////////////
1:   private int localDictionaryThreshold;
1:   private boolean isLocalDictionaryEnabled;
/////////////////////////////////////////////////////////////////////////
1:    * @param localDictionaryThreshold is localDictionaryThreshold,default is 10000
1:    * @return updated CarbonWriterBuilder
1:    */
1:   public CarbonWriterBuilder localDictionaryThreshold(int localDictionaryThreshold) {
1:     if (localDictionaryThreshold <= 0) {
1:       throw new IllegalArgumentException(
0:           "Local Dictionary Threshold should be between greater than 0");
1:     }
1:     this.localDictionaryThreshold = localDictionaryThreshold;
1:     return this;
1:   }
1: 
1:   /**
1:    * @param enableLocalDictionary enable local dictionary  , default is false
1:    * @return updated CarbonWriterBuilder
1:    */
1:   public CarbonWriterBuilder enableLocalDictionary(boolean enableLocalDictionary) {
1:     this.isLocalDictionaryEnabled = enableLocalDictionary;
1:     return this;
1:   }
1: 
1: 
1:   /**
/////////////////////////////////////////////////////////////////////////
1:     tableSchemaBuilder.enableLocalDictionary(isLocalDictionaryEnabled);
1:     tableSchemaBuilder.localDictionaryThreshold(localDictionaryThreshold);
commit:3ea2a1d
/////////////////////////////////////////////////////////////////////////
1:       //Null check for field to handle hole in field[] ex.
1:       //  user passed size 4 but supplied only 2 fileds
1:         if (null != field) {
0:           if (field.getDataType() == DataTypes.STRING ||
0:               field.getDataType() == DataTypes.DATE ||
0:               field.getDataType() == DataTypes.TIMESTAMP) {
0:             sortColumnsList.add(field.getFieldName());
1:           }
/////////////////////////////////////////////////////////////////////////
1:       if (null != field) {
0:         tableSchemaBuilder.addColumn(
0:             new StructField(field.getFieldName(), field.getDataType()),
0:             sortColumnsList.contains(field.getFieldName()));
1:       }
author:rahul
-------------------------------------------------------------------------------
commit:19312ab
/////////////////////////////////////////////////////////////////////////
author:rahulforallp
-------------------------------------------------------------------------------
commit:92d9b92
/////////////////////////////////////////////////////////////////////////
0:     if (sortColumns == null) {
/////////////////////////////////////////////////////////////////////////
0:           } else if (!sortColumnsList.isEmpty() && columnSchema.isDimensionColumn()
commit:9aa3a8c
/////////////////////////////////////////////////////////////////////////
0:     if (sortColumns == null || sortColumns.length == 0) {
commit:78efc7f
/////////////////////////////////////////////////////////////////////////
1:       dbName = "";
1:       tableName = "_tempTable_" + String.valueOf(UUID);
commit:fc4b7f9
/////////////////////////////////////////////////////////////////////////
1:   public CarbonWriterBuilder taskNo(long taskNo) {
1:     this.taskNo = String.valueOf(taskNo);
commit:1c5b526
/////////////////////////////////////////////////////////////////////////
1:     int i = 0;
/////////////////////////////////////////////////////////////////////////
1:             columnSchema.setSortColumn(true);
0:           } else if (sortColumnsList.isEmpty() && columnSchema.isDimensionColumn()
0:               && columnSchema.getNumberOfChild() < 1) {
1:             columnSchema.setSortColumn(true);
0:             sortColumnsSchemaList[i] = columnSchema;
0:             i++;
commit:93724ec
/////////////////////////////////////////////////////////////////////////
1: import org.apache.carbondata.core.metadata.schema.table.column.ColumnSchema;
/////////////////////////////////////////////////////////////////////////
1:     ColumnSchema[] sortColumnsSchemaList = new ColumnSchema[sortColumnsList.size()];
/////////////////////////////////////////////////////////////////////////
1:           int isSortColumn = sortColumnsList.indexOf(field.getFieldName());
1:           ColumnSchema columnSchema = tableSchemaBuilder
1:               .addColumn(new StructField(field.getFieldName(), field.getDataType()),
0:                   isSortColumn > -1);
1:           if (isSortColumn > -1) {
0:             columnSchema.setSortColumn(true);
1:             sortColumnsSchemaList[isSortColumn] = columnSchema;
1:           }
1:     tableSchemaBuilder.setSortColumns(Arrays.asList(sortColumnsSchemaList));
author:xubo245
-------------------------------------------------------------------------------
commit:ddf3e85
/////////////////////////////////////////////////////////////////////////
1:    * If set to false, then writes the carbondata and carbonindex files
1:    * If set to true, then writes the carbondata and carbonindex files
1:    * in segment folder structure.
/////////////////////////////////////////////////////////////////////////
1:    * To set the blocklet size of CarbonData file
commit:cf55028
/////////////////////////////////////////////////////////////////////////
1:     loadModel.setLoadWithoutConverterStep(true);
commit:242c08b
/////////////////////////////////////////////////////////////////////////
1: import org.apache.hadoop.fs.s3a.Constants;
1: 
/////////////////////////////////////////////////////////////////////////
1:    * Set the access key for S3
1:    *
1:    * @param key   the string of access key for different S3 type,like: fs.s3a.access.key
1:    * @param value the value of access key
1:    * @return CarbonWriterBuilder
1:    */
1:   public CarbonWriterBuilder setAccessKey(String key, String value) {
1:     FileFactory.getConfiguration().set(key, value);
1:     return this;
1:   }
1: 
1:   /**
1:    * Set the access key for S3.
1:    *
1:    * @param value the value of access key
1:    * @return CarbonWriterBuilder
1:    */
1:   public CarbonWriterBuilder setAccessKey(String value) {
1:     return setAccessKey(Constants.ACCESS_KEY, value);
1:   }
1: 
1:   /**
1:    * Set the secret key for S3
1:    *
1:    * @param key   the string of secret key for different S3 type,like: fs.s3a.secret.key
1:    * @param value the value of secret key
1:    * @return CarbonWriterBuilder
1:    */
1:   public CarbonWriterBuilder setSecretKey(String key, String value) {
1:     FileFactory.getConfiguration().set(key, value);
1:     return this;
1:   }
1: 
1:   /**
1:    * Set the secret key for S3
1:    *
1:    * @param value the value of secret key
1:    * @return CarbonWriterBuilder
1:    */
1:   public CarbonWriterBuilder setSecretKey(String value) {
1:     return setSecretKey(Constants.SECRET_KEY, value);
1:   }
1: 
1:   /**
1:    * Set the endpoint for S3
1:    *
1:    * @param key   the string of endpoint for different S3 type,like: fs.s3a.endpoint
1:    * @param value the value of endpoint
1:    * @return CarbonWriterBuilder
1:    */
1:   public CarbonWriterBuilder setEndPoint(String key, String value) {
1:     FileFactory.getConfiguration().set(key, value);
1:     return this;
1:   }
1: 
1:   /**
1:    * Set the endpoint for S3
1:    *
1:    * @param value the value of endpoint
1:    * @return CarbonWriterBuilder
1:    */
1:   public CarbonWriterBuilder setEndPoint(String value) {
1:     FileFactory.getConfiguration().set(Constants.ENDPOINT, value);
1:     return this;
1:   }
1: 
1:   /**
author:kumarvishal09
-------------------------------------------------------------------------------
commit:6297ea0
/////////////////////////////////////////////////////////////////////////
/////////////////////////////////////////////////////////////////////////
/////////////////////////////////////////////////////////////////////////
author:sounakr
-------------------------------------------------------------------------------
commit:ec33c11
/////////////////////////////////////////////////////////////////////////
1: 
1:     // AVRO records are pushed to Carbon as Object not as Strings. This was done in order to
1:     // handle multi level complex type support. As there are no conversion converter step is
1:     // removed from the load. LoadWithoutConverter flag is going to point to the Loader Builder
1:     // which will skip Conversion Step.
0:     loadModel.setLoadWithoutCoverterStep(true);
commit:b1c85fa
/////////////////////////////////////////////////////////////////////////
1:   private void setCsvHeader(CarbonLoadModel model) {
1:     Field[] fields = schema.getFields();
1:     StringBuilder builder = new StringBuilder();
1:     String[] columns = new String[fields.length];
1:     int i = 0;
1:     for (Field field : fields) {
1:       if (null != field) {
1:         builder.append(field.getFieldName());
1:         builder.append(",");
1:         columns[i++] = field.getFieldName();
1:       }
1:     }
1:     String header = builder.toString();
1:     model.setCsvHeader(header.substring(0, header.length() - 1));
1:     model.setCsvHeaderColumns(columns);
1:   }
1: 
/////////////////////////////////////////////////////////////////////////
1:               field.getDataType() == DataTypes.DATE  ||
/////////////////////////////////////////////////////////////////////////
1:     Field[] fields = schema.getFields();
1:     buildTableSchema(fields, tableSchemaBuilder, sortColumnsList, sortColumnsSchemaList);
1: 
/////////////////////////////////////////////////////////////////////////
1:     CarbonTable table =
1:         CarbonTable.builder().tableName(schema.getTableName()).databaseName(dbName).tablePath(path)
1:             .tableSchema(schema).isTransactionalTable(isTransactionalTable).build();
1:   private void buildTableSchema(Field[] fields, TableSchemaBuilder tableSchemaBuilder,
1:       List<String> sortColumnsList, ColumnSchema[] sortColumnsSchemaList) {
1:     for (Field field : fields) {
0:       if (null != field) {
0:         int isSortColumn = sortColumnsList.indexOf(field.getFieldName());
1:         if (isSortColumn > -1) {
1:           // unsupported types for ("array", "struct", "double", "float", "decimal")
1:           if (field.getDataType() == DataTypes.DOUBLE || field.getDataType() == DataTypes.FLOAT
0:               || DataTypes.isDecimal(field.getDataType()) || field.getDataType().isComplexType()) {
1:             throw new RuntimeException(
0:                 " sort columns not supported for " + "array, struct, double, float, decimal ");
1:           }
1:         }
1: 
1:         if (field.getChildren() != null && field.getChildren().size() > 0) {
1:           if (field.getDataType().getName().equalsIgnoreCase("ARRAY")) {
1:             // Loop through the inner columns and for a StructData
1:             DataType complexType =
1:                 DataTypes.createArrayType(field.getChildren().get(0).getDataType());
0:             tableSchemaBuilder.addColumn(new StructField(field.getFieldName(), complexType), false);
1:           } else if (field.getDataType().getName().equalsIgnoreCase("STRUCT")) {
1:             // Loop through the inner columns and for a StructData
1:             List<StructField> structFieldsArray =
1:                 new ArrayList<StructField>(field.getChildren().size());
1:             for (StructField childFld : field.getChildren()) {
1:               structFieldsArray
1:                   .add(new StructField(childFld.getFieldName(), childFld.getDataType()));
1:             }
1:             DataType complexType = DataTypes.createStructType(structFieldsArray);
0:             tableSchemaBuilder.addColumn(new StructField(field.getFieldName(), complexType), false);
1:           }
1:         } else {
0:           ColumnSchema columnSchema = tableSchemaBuilder
0:               .addColumn(new StructField(field.getFieldName(), field.getDataType()),
0:                   isSortColumn > -1);
0:           columnSchema.setSortColumn(true);
1:           if (isSortColumn > -1) {
0:             sortColumnsSchemaList[isSortColumn] = columnSchema;
1:           }
1:         }
1:       }
1:     }
1:   }
1: 
/////////////////////////////////////////////////////////////////////////
1:     CarbonLoadModel build = builder.build(options, UUID, taskNo);
1:     setCsvHeader(build);
1:     return build;
commit:3202cf5
/////////////////////////////////////////////////////////////////////////
1: import org.apache.carbondata.core.metadata.datatype.DataType;
/////////////////////////////////////////////////////////////////////////
0:         if (field.getChildren() != null && field.getChildren().size() > 0) {
0:           // Loop through the inner columns and for a StructData
0:           List<StructField> structFieldsArray =
0:               new ArrayList<StructField>(field.getChildren().size());
0:           String parentName = field.getFieldName();
0:           for (StructField childFld : field.getChildren()) {
0:             structFieldsArray.add(new StructField(childFld.getFieldName(), childFld.getDataType()));
1:           }
0:           DataType complexType = DataTypes.createStructType(structFieldsArray);
0:           tableSchemaBuilder.addColumn(new StructField(field.getFieldName(), complexType), false);
0:         } else {
0:           tableSchemaBuilder.addColumn(new StructField(field.getFieldName(), field.getDataType()),
0:               sortColumnsList.contains(field.getFieldName()));
1:         }
commit:b7b8073
/////////////////////////////////////////////////////////////////////////
1:   private boolean isTransactionalTable;
1:   private String taskNo;
/////////////////////////////////////////////////////////////////////////
1:    * sets the taskNo for the writer. SDKs concurrently running
0:    * will set taskNo in order to avoid conflits in file write.
0:    * @param taskNo is the TaskNo user wants to specify. Mostly it system time.
0:    * @return updated CarbonWriterBuilder
1:    */
0:   public CarbonWriterBuilder taskNo(String taskNo) {
0:     this.taskNo = taskNo;
1:     return this;
1:   }
1: 
1: 
1: 
1:   /**
/////////////////////////////////////////////////////////////////////////
1:    * If set false, writes the carbondata and carbonindex files in a flat folder structure
0:    * @param isTransactionalTable is a boolelan value if set to false then writes
1:   public CarbonWriterBuilder isTransactionalTable(boolean isTransactionalTable) {
1:     Objects.requireNonNull(isTransactionalTable, "Transactional Table should not be null");
1:     this.isTransactionalTable = isTransactionalTable;
/////////////////////////////////////////////////////////////////////////
0:     return buildLoadModel(table, UUID, taskNo);
/////////////////////////////////////////////////////////////////////////
1:     if (isTransactionalTable) {
/////////////////////////////////////////////////////////////////////////
0:         .isTransactionalTable(isTransactionalTable)
/////////////////////////////////////////////////////////////////////////
0:   private CarbonLoadModel buildLoadModel(CarbonTable table, long UUID, String taskNo)
0:     return builder.build(options, UUID, taskNo);
author:Jacky Li
-------------------------------------------------------------------------------
commit:e39b0a1
/////////////////////////////////////////////////////////////////////////
0:     CarbonLoadModel loadModel = createLoadModel();
/////////////////////////////////////////////////////////////////////////
0:   public CarbonWriter buildWriterForAvroInput() throws IOException, InvalidLoadOptionException {
1:     Objects.requireNonNull(schema, "schema should not be null");
1:     Objects.requireNonNull(path, "path should not be null");
0:     CarbonLoadModel loadModel = createLoadModel();
0:     return new AvroCarbonWriter(loadModel);
1:   }
1: 
0:   private CarbonLoadModel createLoadModel() throws IOException, InvalidLoadOptionException {
1:     // build CarbonTable using schema
1:     CarbonTable table = buildCarbonTable();
1:     if (persistSchemaFile) {
1:       // we are still using the traditional carbon table folder structure
1:       persistSchemaFile(table, CarbonTablePath.getSchemaFilePath(path));
1:     }
1: 
1:     // build LoadModel
0:     return buildLoadModel(table);
commit:6cb6f83
/////////////////////////////////////////////////////////////////////////
1:   private int blockletSize;
1:   private int blockSize;
/////////////////////////////////////////////////////////////////////////
1:     this.blockSize = blockSize;
1:     return this;
1:     this.blockletSize = blockletSize;
1:     return this;
/////////////////////////////////////////////////////////////////////////
0:     if (blockletSize > 0) {
1:       tableSchemaBuilder = tableSchemaBuilder.blockSize(blockSize);
1:     }
1: 
commit:89cfd8e
/////////////////////////////////////////////////////////////////////////
1: import org.apache.carbondata.common.exceptions.sql.InvalidLoadOptionException;
/////////////////////////////////////////////////////////////////////////
1: import org.apache.carbondata.processing.loading.model.CarbonLoadModelBuilder;
/////////////////////////////////////////////////////////////////////////
1:    * Build a {@link CarbonWriter}, which accepts row in CSV format
0:   public CarbonWriter buildWriterForCSVInput() throws IOException, InvalidLoadOptionException {
/////////////////////////////////////////////////////////////////////////
1:    * Build a {@link CarbonWriter}, which accepts Avro object
/////////////////////////////////////////////////////////////////////////
0:   private CarbonLoadModel buildLoadModel(CarbonTable table)
0:       throws InvalidLoadOptionException, IOException {
1:     CarbonLoadModelBuilder builder = new CarbonLoadModelBuilder(table);
0:     return builder.build(options);
commit:1d827c7
/////////////////////////////////////////////////////////////////////////
1: /*
1:  * Licensed to the Apache Software Foundation (ASF) under one or more
1:  * contributor license agreements.  See the NOTICE file distributed with
1:  * this work for additional information regarding copyright ownership.
1:  * The ASF licenses this file to You under the Apache License, Version 2.0
1:  * (the "License"); you may not use this file except in compliance with
1:  * the License.  You may obtain a copy of the License at
1:  *
1:  *    http://www.apache.org/licenses/LICENSE-2.0
1:  *
1:  * Unless required by applicable law or agreed to in writing, software
1:  * distributed under the License is distributed on an "AS IS" BASIS,
1:  * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
1:  * See the License for the specific language governing permissions and
1:  * limitations under the License.
1:  */
1: 
1: package org.apache.carbondata.sdk.file;
1: 
1: import java.io.IOException;
1: import java.util.Arrays;
1: import java.util.HashMap;
0: import java.util.LinkedList;
1: import java.util.List;
1: import java.util.Map;
1: import java.util.Objects;
1: 
0: import org.apache.carbondata.common.Strings;
1: import org.apache.carbondata.common.annotations.InterfaceAudience;
1: import org.apache.carbondata.common.annotations.InterfaceStability;
1: import org.apache.carbondata.core.datastore.impl.FileFactory;
1: import org.apache.carbondata.core.metadata.CarbonMetadata;
1: import org.apache.carbondata.core.metadata.converter.SchemaConverter;
1: import org.apache.carbondata.core.metadata.converter.ThriftWrapperSchemaConverterImpl;
1: import org.apache.carbondata.core.metadata.datatype.StructField;
1: import org.apache.carbondata.core.metadata.schema.table.CarbonTable;
1: import org.apache.carbondata.core.metadata.schema.table.TableInfo;
1: import org.apache.carbondata.core.metadata.schema.table.TableSchema;
1: import org.apache.carbondata.core.metadata.schema.table.TableSchemaBuilder;
1: import org.apache.carbondata.core.util.path.CarbonTablePath;
1: import org.apache.carbondata.core.writer.ThriftWriter;
1: import org.apache.carbondata.processing.loading.model.CarbonLoadModel;
0: import org.apache.carbondata.spark.util.DataLoadingUtil;
1: 
1: /**
0:  * Biulder for {@link CarbonWriter}
1:  */
1: @InterfaceAudience.User
1: @InterfaceStability.Unstable
1: public class CarbonWriterBuilder {
1:   private Schema schema;
1:   private String path;
1:   private String[] sortColumns;
1:   private boolean persistSchemaFile;
1: 
0:   public CarbonWriterBuilder withSchema(Schema schema) {
1:     Objects.requireNonNull(schema, "schema should not be null");
0:     this.schema = schema;
1:     return this;
1:   }
1: 
1:   public CarbonWriterBuilder outputPath(String path) {
1:     Objects.requireNonNull(path, "path should not be null");
1:     this.path = path;
1:     return this;
1:   }
1: 
1:   public CarbonWriterBuilder sortBy(String[] sortColumns) {
1:     this.sortColumns = sortColumns;
1:     return this;
1:   }
1: 
0:   public CarbonWriterBuilder partitionBy(String[] partitionColumns) {
0:     throw new UnsupportedOperationException();
1:   }
1: 
1:   public CarbonWriterBuilder persistSchemaFile(boolean persist) {
1:     this.persistSchemaFile = persist;
1:     return this;
1:   }
1: 
1:   public CarbonWriterBuilder withBlockSize(int blockSize) {
0:     if (blockSize <= 0) {
0:       throw new IllegalArgumentException("blockSize should be greater than zero");
1:     }
0:     throw new UnsupportedOperationException();
1:   }
1: 
1:   public CarbonWriterBuilder withBlockletSize(int blockletSize) {
1:     if (blockletSize <= 0) {
1:       throw new IllegalArgumentException("blockletSize should be greater than zero");
1:     }
0:     throw new UnsupportedOperationException();
1:   }
1: 
1:   /**
0:    * Build a {@link CSVCarbonWriter}, which accepts row in CSV format
1:    */
0:   public CarbonWriter buildWriterForCSVInput() throws IOException {
1:     Objects.requireNonNull(schema, "schema should not be null");
1:     Objects.requireNonNull(path, "path should not be null");
1: 
0:     // build CarbonTable using schema
0:     CarbonTable table = buildCarbonTable();
0:     if (persistSchemaFile) {
0:       // we are still using the traditional carbon table folder structure
0:       persistSchemaFile(table, CarbonTablePath.getSchemaFilePath(path));
1:     }
1: 
0:     // build LoadModel
0:     CarbonLoadModel loadModel = buildLoadModel(table);
0:     return new CSVCarbonWriter(loadModel);
1:   }
1: 
1:   /**
0:    * Build a {@link AvroCarbonWriter}, which accepts Avro object
0:    * @return
1:    * @throws IOException
1:    */
0:   public CarbonWriter buildWriterForAvroInput() throws IOException {
0:     // TODO
0:     throw new UnsupportedOperationException();
1:   }
1: 
1:   /**
1:    * Build a {@link CarbonTable}
1:    */
1:   private CarbonTable buildCarbonTable() {
1:     TableSchemaBuilder tableSchemaBuilder = TableSchema.builder();
0:     List<String> sortColumnsList;
0:     if (sortColumns != null) {
0:       sortColumnsList = Arrays.asList(sortColumns);
0:     } else {
0:       sortColumnsList = new LinkedList<>();
1:     }
0:     for (Field field : schema.getFields()) {
0:       tableSchemaBuilder.addColumn(
0:           new StructField(field.getFieldName(), field.getDataType()),
0:           sortColumnsList.contains(field.getFieldName()));
1:     }
0:     String tableName = "_tempTable";
0:     String dbName = "_tempDB";
1:     TableSchema schema = tableSchemaBuilder.build();
1:     schema.setTableName(tableName);
0:     CarbonTable table = CarbonTable.builder()
0:         .tableName(schema.getTableName())
0:         .databaseName(dbName)
0:         .tablePath(path)
0:         .tableSchema(schema)
0:         .build();
1:     return table;
1:   }
1: 
1:   /**
1:    * Save the schema of the {@param table} to {@param persistFilePath}
1:    * @param table table object containing schema
1:    * @param persistFilePath absolute file path with file name
1:    */
1:   private void persistSchemaFile(CarbonTable table, String persistFilePath) throws IOException {
1:     TableInfo tableInfo = table.getTableInfo();
1:     String schemaMetadataPath = CarbonTablePath.getFolderContainingFile(persistFilePath);
1:     CarbonMetadata.getInstance().loadTableMetadata(tableInfo);
1:     SchemaConverter schemaConverter = new ThriftWrapperSchemaConverterImpl();
1:     org.apache.carbondata.format.TableInfo thriftTableInfo =
1:         schemaConverter.fromWrapperToExternalTableInfo(
1:             tableInfo,
1:             tableInfo.getDatabaseName(),
1:             tableInfo.getFactTable().getTableName());
1:     org.apache.carbondata.format.SchemaEvolutionEntry schemaEvolutionEntry =
1:         new org.apache.carbondata.format.SchemaEvolutionEntry(
1:             tableInfo.getLastUpdatedTime());
1:     thriftTableInfo.getFact_table().getSchema_evolution().getSchema_evolution_history()
1:         .add(schemaEvolutionEntry);
1:     FileFactory.FileType fileType = FileFactory.getFileType(schemaMetadataPath);
1:     if (!FileFactory.isFileExist(schemaMetadataPath, fileType)) {
1:       FileFactory.mkdirs(schemaMetadataPath, fileType);
1:     }
1:     ThriftWriter thriftWriter = new ThriftWriter(persistFilePath, false);
1:     thriftWriter.open();
1:     thriftWriter.write(thriftTableInfo);
1:     thriftWriter.close();
1:   }
1: 
1:   /**
1:    * Build a {@link CarbonLoadModel}
1:    */
0:   private CarbonLoadModel buildLoadModel(CarbonTable table) {
0:     Map<String, String> options = new HashMap<>();
0:     if (sortColumns != null) {
0:       options.put("sort_columns", Strings.mkString(sortColumns, ","));
1:     }
0:     return DataLoadingUtil.buildCarbonLoadModelJava(table, options);
1:   }
1: }
============================================================================