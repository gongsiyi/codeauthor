1:b86ff92: /*
1:b86ff92:  * Licensed to the Apache Software Foundation (ASF) under one or more
1:b86ff92:  * contributor license agreements.  See the NOTICE file distributed with
1:b86ff92:  * this work for additional information regarding copyright ownership.
1:b86ff92:  * The ASF licenses this file to You under the Apache License, Version 2.0
1:b86ff92:  * (the "License"); you may not use this file except in compliance with
1:b86ff92:  * the License.  You may obtain a copy of the License at
2:b86ff92:  *
1:b86ff92:  *    http://www.apache.org/licenses/LICENSE-2.0
1:b86ff92:  *
1:b86ff92:  * Unless required by applicable law or agreed to in writing, software
1:b86ff92:  * distributed under the License is distributed on an "AS IS" BASIS,
1:b86ff92:  * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
1:b86ff92:  * See the License for the specific language governing permissions and
1:b86ff92:  * limitations under the License.
2:b86ff92:  */
1:b86ff92: package org.apache.carbondata.datamap.bloom;
1:cd7c210: 
1:b86ff92: import java.io.IOException;
1:cd7c210: import java.util.HashMap;
1:b86ff92: import java.util.List;
1:cd7c210: import java.util.Map;
14:b86ff92: 
1:b86ff92: import org.apache.carbondata.common.annotations.InterfaceAudience;
1:b86ff92: import org.apache.carbondata.core.datamap.Segment;
1:cd7c210: import org.apache.carbondata.core.datastore.block.SegmentProperties;
1:cd7c210: import org.apache.carbondata.core.keygenerator.columnar.ColumnarSplitter;
1:b86ff92: import org.apache.carbondata.core.metadata.datatype.DataTypes;
1:9db662a: import org.apache.carbondata.core.metadata.schema.table.column.CarbonColumn;
1:cd7c210: import org.apache.carbondata.core.metadata.schema.table.column.CarbonDimension;
1:b86ff92: import org.apache.carbondata.core.util.CarbonUtil;
1:b86ff92: 
1:cd7c210: import org.apache.commons.collections.CollectionUtils;
1:cd7c210: import org.apache.commons.collections.Predicate;
1:b86ff92: 
2:b86ff92: /**
1:9db662a:  * BloomDataMap is constructed in CG level (blocklet level).
1:9db662a:  * For each indexed column, a bloom filter is constructed to indicate whether a value
1:9db662a:  * belongs to this blocklet. Bloom filter of blocklet that belongs to same block will
1:9db662a:  * be written to one index file suffixed with .bloomindex. So the number
1:b86ff92:  * of bloom index file will be equal to that of the blocks.
1:b86ff92:  */
2:b86ff92: @InterfaceAudience.Internal
1:5c483f3: public class BloomDataMapWriter extends AbstractBloomDataMapWriter {
1:cd7c210:   private ColumnarSplitter columnarSplitter;
1:cd7c210:   // for the dict/sort/date column, they are encoded in MDK,
1:cd7c210:   // this maps the index column name to the index in MDK
1:cd7c210:   private Map<String, Integer> indexCol2MdkIdx;
1:b86ff92: 
1:9db662a:   BloomDataMapWriter(String tablePath, String dataMapName, List<CarbonColumn> indexColumns,
1:cd7c210:       Segment segment, String shardName, SegmentProperties segmentProperties,
1:cd7c210:       int bloomFilterSize, double bloomFilterFpp, boolean compressBloom)
1:6b94971:       throws IOException {
1:5c483f3:     super(tablePath, dataMapName, indexColumns, segment, shardName, segmentProperties,
1:5c483f3:         bloomFilterSize, bloomFilterFpp, compressBloom);
1:b86ff92: 
1:cd7c210:     columnarSplitter = segmentProperties.getFixedLengthKeySplitter();
1:cd7c210:     this.indexCol2MdkIdx = new HashMap<>();
1:cd7c210:     int idx = 0;
1:cd7c210:     for (final CarbonDimension dimension : segmentProperties.getDimensions()) {
1:cd7c210:       if (!dimension.isGlobalDictionaryEncoding() && !dimension.isDirectDictionaryEncoding()) {
1:cd7c210:         continue;
1:5c483f3:       }
1:cd7c210:       boolean isExistInIndex = CollectionUtils.exists(indexColumns, new Predicate() {
1:cd7c210:         @Override public boolean evaluate(Object object) {
1:cd7c210:           return ((CarbonColumn) object).getColName().equalsIgnoreCase(dimension.getColName());
1:cd7c210:         }
1:cd7c210:       });
1:cd7c210:       if (isExistInIndex) {
1:cd7c210:         this.indexCol2MdkIdx.put(dimension.getColName(), idx);
1:cd7c210:       }
1:cd7c210:       idx++;
1:cd7c210:     }
1:cd7c210:   }
1:b86ff92: 
1:5c483f3:   protected byte[] convertNonDictionaryValue(int indexColIdx, byte[] value) {
1:5c483f3:     if (DataTypes.VARCHAR == indexColumns.get(indexColIdx).getDataType()) {
1:5c483f3:       return DataConvertUtil.getRawBytesForVarchar(value);
1:5c483f3:     } else {
1:5c483f3:       return DataConvertUtil.getRawBytes(value);
3:cd7c210:     }
1:cd7c210:   }
1:b86ff92: 
7:b86ff92:   @Override
1:81038f5:   protected byte[] convertDictionaryValue(int indexColIdx, Object value) {
1:81038f5:     // input value from onPageAdded in load process is byte[]
1:b86ff92: 
1:7fc0c6b:     // for dict columns including dictionary and date columns decode value to get the surrogate key
1:7fc0c6b:     int thisKeyIdx = indexCol2MdkIdx.get(indexColumns.get(indexColIdx).getColName());
1:7fc0c6b:     int surrogateKey = CarbonUtil.getSurrogateInternal((byte[]) value, 0,
1:7fc0c6b:         columnarSplitter.getBlockKeySize()[thisKeyIdx]);
1:cd7c210:     // store the dictionary key in bloom
1:81038f5:     return CarbonUtil.getValueAsBytes(DataTypes.INT, surrogateKey);
1:cd7c210:   }
1:cd7c210: }
============================================================================
author:xuchuanyin
-------------------------------------------------------------------------------
commit:7fc0c6b
/////////////////////////////////////////////////////////////////////////
/////////////////////////////////////////////////////////////////////////
/////////////////////////////////////////////////////////////////////////
/////////////////////////////////////////////////////////////////////////
/////////////////////////////////////////////////////////////////////////
1:     // for dict columns including dictionary and date columns decode value to get the surrogate key
1:     int thisKeyIdx = indexCol2MdkIdx.get(indexColumns.get(indexColIdx).getColName());
1:     int surrogateKey = CarbonUtil.getSurrogateInternal((byte[]) value, 0,
1:         columnarSplitter.getBlockKeySize()[thisKeyIdx]);
commit:5c483f3
/////////////////////////////////////////////////////////////////////////
/////////////////////////////////////////////////////////////////////////
1: public class BloomDataMapWriter extends AbstractBloomDataMapWriter {
1:     super(tablePath, dataMapName, indexColumns, segment, shardName, segmentProperties,
1:         bloomFilterSize, bloomFilterFpp, compressBloom);
1:   }
1:   protected byte[] convertNonDictionaryValue(int indexColIdx, byte[] value) {
1:     if (DataTypes.VARCHAR == indexColumns.get(indexColIdx).getDataType()) {
1:       return DataConvertUtil.getRawBytesForVarchar(value);
1:     } else {
1:       return DataConvertUtil.getRawBytes(value);
commit:cd7c210
/////////////////////////////////////////////////////////////////////////
1: import java.util.HashMap;
1: import java.util.Map;
0: import org.apache.carbondata.core.constants.CarbonCommonConstants;
1: import org.apache.carbondata.core.datastore.block.SegmentProperties;
0: import org.apache.carbondata.core.keygenerator.KeyGenerator;
1: import org.apache.carbondata.core.keygenerator.columnar.ColumnarSplitter;
0: import org.apache.carbondata.core.metadata.encoder.Encoding;
1: import org.apache.carbondata.core.metadata.schema.table.column.CarbonDimension;
1: import org.apache.commons.collections.CollectionUtils;
1: import org.apache.commons.collections.Predicate;
/////////////////////////////////////////////////////////////////////////
0:   private KeyGenerator keyGenerator;
1:   private ColumnarSplitter columnarSplitter;
1:   // for the dict/sort/date column, they are encoded in MDK,
1:   // this maps the index column name to the index in MDK
1:   private Map<String, Integer> indexCol2MdkIdx;
0:   // this gives the reverse map to indexCol2MdkIdx
0:   private Map<Integer, String> mdkIdx2IndexCol;
1:       Segment segment, String shardName, SegmentProperties segmentProperties,
1:       int bloomFilterSize, double bloomFilterFpp, boolean compressBloom)
/////////////////////////////////////////////////////////////////////////
1: 
0:     keyGenerator = segmentProperties.getDimensionKeyGenerator();
1:     columnarSplitter = segmentProperties.getFixedLengthKeySplitter();
1:     this.indexCol2MdkIdx = new HashMap<>();
0:     this.mdkIdx2IndexCol = new HashMap<>();
1:     int idx = 0;
1:     for (final CarbonDimension dimension : segmentProperties.getDimensions()) {
1:       if (!dimension.isGlobalDictionaryEncoding() && !dimension.isDirectDictionaryEncoding()) {
1:         continue;
1:       }
1:       boolean isExistInIndex = CollectionUtils.exists(indexColumns, new Predicate() {
1:         @Override public boolean evaluate(Object object) {
1:           return ((CarbonColumn) object).getColName().equalsIgnoreCase(dimension.getColName());
1:         }
1:       });
1:       if (isExistInIndex) {
1:         this.indexCol2MdkIdx.put(dimension.getColName(), idx);
0:         this.mdkIdx2IndexCol.put(idx, dimension.getColName());
1:       }
1:       idx++;
1:     }
/////////////////////////////////////////////////////////////////////////
0:         // convert measure to bytes
0:         // convert non-dict dimensions to simple bytes without length
0:         // convert internal-dict dimensions to simple bytes without any encode
0:         if (indexColumns.get(i).isMeasure()) {
0:         } else {
0:           if (indexColumns.get(i).hasEncoding(Encoding.DICTIONARY)
0:               || indexColumns.get(i).hasEncoding(Encoding.DIRECT_DICTIONARY)) {
0:             byte[] mdkBytes;
0:             // this means that we need to pad some fake bytes
0:             // to get the whole MDK in corresponding position
0:             if (columnarSplitter.getBlockKeySize().length > indexCol2MdkIdx.size()) {
0:               int totalSize = 0;
0:               for (int size : columnarSplitter.getBlockKeySize()) {
0:                 totalSize += size;
1:               }
0:               mdkBytes = new byte[totalSize];
0:               int startPos = 0;
0:               int destPos = 0;
0:               for (int keyIdx = 0; keyIdx < columnarSplitter.getBlockKeySize().length; keyIdx++) {
0:                 if (mdkIdx2IndexCol.containsKey(keyIdx)) {
0:                   int size = columnarSplitter.getBlockKeySize()[keyIdx];
0:                   System.arraycopy(data, startPos, mdkBytes, destPos, size);
0:                   startPos += size;
1:                 }
0:                 destPos += columnarSplitter.getBlockKeySize()[keyIdx];
1:               }
0:             } else {
0:               mdkBytes = (byte[]) data;
1:             }
0:             // for dict columns including dictionary and date columns
0:             // decode value to get the surrogate key
0:             int surrogateKey = (int) keyGenerator.getKey(mdkBytes,
0:                 indexCol2MdkIdx.get(indexColumns.get(i).getColName()));
1:             // store the dictionary key in bloom
0:             indexValue = CarbonUtil.getValueAsBytes(DataTypes.INT, surrogateKey);
0:           } else if (DataTypes.VARCHAR == dataType) {
0:             indexValue = DataConvertUtil.getRawBytesForVarchar((byte[]) data);
0:           } else {
0:             indexValue = DataConvertUtil.getRawBytes((byte[]) data);
1:           }
1:         }
0:         if (indexValue.length == 0) {
0:           indexValue = CarbonCommonConstants.MEMBER_DEFAULT_VAL_ARRAY;
commit:dac5d3c
/////////////////////////////////////////////////////////////////////////
commit:d14c403
/////////////////////////////////////////////////////////////////////////
/////////////////////////////////////////////////////////////////////////
0:       String dmFile = BloomCoarseGrainDataMap.getBloomIndexFile(dataMapPath,
0:           indexColumns.get(indexColId).getColName());
commit:6b94971
/////////////////////////////////////////////////////////////////////////
0:   private double bloomFilterFpp;
/////////////////////////////////////////////////////////////////////////
0:       Segment segment, String shardName, int bloomFilterSize, double bloomFilterFpp)
1:       throws IOException {
0:     this.bloomFilterFpp = bloomFilterFpp;
/////////////////////////////////////////////////////////////////////////
0:           bloomFilterSize, bloomFilterFpp));
commit:b86ff92
/////////////////////////////////////////////////////////////////////////
1: /*
1:  * Licensed to the Apache Software Foundation (ASF) under one or more
1:  * contributor license agreements.  See the NOTICE file distributed with
1:  * this work for additional information regarding copyright ownership.
1:  * The ASF licenses this file to You under the Apache License, Version 2.0
1:  * (the "License"); you may not use this file except in compliance with
1:  * the License.  You may obtain a copy of the License at
1:  *
1:  *    http://www.apache.org/licenses/LICENSE-2.0
1:  *
1:  * Unless required by applicable law or agreed to in writing, software
1:  * distributed under the License is distributed on an "AS IS" BASIS,
1:  * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
1:  * See the License for the specific language governing permissions and
1:  * limitations under the License.
1:  */
1: package org.apache.carbondata.datamap.bloom;
1: 
0: import java.io.DataOutputStream;
0: import java.io.File;
1: import java.io.IOException;
0: import java.io.ObjectOutputStream;
0: import java.util.ArrayList;
0: import java.util.HashMap;
1: import java.util.List;
0: import java.util.Map;
1: 
1: import org.apache.carbondata.common.annotations.InterfaceAudience;
0: import org.apache.carbondata.core.datamap.DataMapMeta;
1: import org.apache.carbondata.core.datamap.Segment;
0: import org.apache.carbondata.core.datamap.dev.DataMapWriter;
0: import org.apache.carbondata.core.datastore.impl.FileFactory;
0: import org.apache.carbondata.core.datastore.page.ColumnPage;
0: import org.apache.carbondata.core.metadata.AbsoluteTableIdentifier;
0: import org.apache.carbondata.core.metadata.datatype.DataType;
1: import org.apache.carbondata.core.metadata.datatype.DataTypes;
1: import org.apache.carbondata.core.util.CarbonUtil;
1: 
0: import com.google.common.hash.BloomFilter;
0: import com.google.common.hash.Funnels;
0: import org.apache.hadoop.fs.FileSystem;
0: import org.apache.hadoop.fs.Path;
1: 
1: /**
0:  * BloomDataMap is constructed in blocklet level. For each indexed column, a bloom filter is
0:  * constructed to indicate whether a value belongs to this blocklet. Bloom filter of blocklet that
0:  * belongs to same block will be written to one index file suffixed with .bloomindex. So the number
1:  * of bloom index file will be equal to that of the blocks.
1:  */
1: @InterfaceAudience.Internal
0: public class BloomDataMapWriter extends DataMapWriter {
0:   private String dataMapName;
0:   private List<String> indexedColumns;
0:   private int bloomFilterSize;
0:   // map column name to ordinal in pages
0:   private Map<String, Integer> col2Ordianl;
0:   private Map<String, DataType> col2DataType;
0:   private String currentBlockId;
0:   private int currentBlockletId;
0:   private List<String> currentDMFiles;
0:   private List<DataOutputStream> currentDataOutStreams;
0:   private List<ObjectOutputStream> currentObjectOutStreams;
0:   private List<BloomFilter<byte[]>> indexBloomFilters;
1: 
1:   @InterfaceAudience.Internal
0:   public BloomDataMapWriter(AbsoluteTableIdentifier identifier, DataMapMeta dataMapMeta,
0:       int bloomFilterSize, Segment segment, String writeDirectoryPath) {
0:     super(identifier, segment, writeDirectoryPath);
0:     dataMapName = dataMapMeta.getDataMapName();
0:     indexedColumns = dataMapMeta.getIndexedColumns();
0:     this.bloomFilterSize = bloomFilterSize;
0:     col2Ordianl = new HashMap<String, Integer>(indexedColumns.size());
0:     col2DataType = new HashMap<String, DataType>(indexedColumns.size());
1: 
0:     currentDMFiles = new ArrayList<String>(indexedColumns.size());
0:     currentDataOutStreams = new ArrayList<DataOutputStream>(indexedColumns.size());
0:     currentObjectOutStreams = new ArrayList<ObjectOutputStream>(indexedColumns.size());
1: 
0:     indexBloomFilters = new ArrayList<BloomFilter<byte[]>>(indexedColumns.size());
0:   }
1: 
1:   @Override
0:   public void onBlockStart(String blockId, long taskId) throws IOException {
0:     this.currentBlockId = blockId;
0:     this.currentBlockletId = 0;
0:     currentDMFiles.clear();
0:     currentDataOutStreams.clear();
0:     currentObjectOutStreams.clear();
0:     initDataMapFile();
0:   }
1: 
1:   @Override
0:   public void onBlockEnd(String blockId) throws IOException {
0:     for (int indexColId = 0; indexColId < indexedColumns.size(); indexColId++) {
0:       CarbonUtil.closeStreams(this.currentDataOutStreams.get(indexColId),
0:           this.currentObjectOutStreams.get(indexColId));
0:       commitFile(this.currentDMFiles.get(indexColId));
0:     }
0:   }
1: 
1:   @Override
0:   public void onBlockletStart(int blockletId) {
0:     this.currentBlockletId = blockletId;
0:     indexBloomFilters.clear();
0:     for (int i = 0; i < indexedColumns.size(); i++) {
0:       indexBloomFilters.add(BloomFilter.create(Funnels.byteArrayFunnel(),
0:           bloomFilterSize, 0.00001d));
0:     }
0:   }
1: 
1:   @Override
0:   public void onBlockletEnd(int blockletId) {
0:     try {
0:       writeBloomDataMapFile();
0:     } catch (Exception e) {
0:       for (ObjectOutputStream objectOutputStream : currentObjectOutStreams) {
0:         CarbonUtil.closeStreams(objectOutputStream);
0:       }
0:       for (DataOutputStream dataOutputStream : currentDataOutStreams) {
0:         CarbonUtil.closeStreams(dataOutputStream);
0:       }
0:       throw new RuntimeException(e);
0:     }
0:   }
1: 
0:   // notice that the input pages only contains the indexed columns
1:   @Override
0:   public void onPageAdded(int blockletId, int pageId, ColumnPage[] pages)
0:       throws IOException {
0:     col2Ordianl.clear();
0:     col2DataType.clear();
0:     for (int colId = 0; colId < pages.length; colId++) {
0:       String columnName = pages[colId].getColumnSpec().getFieldName().toLowerCase();
0:       col2Ordianl.put(columnName, colId);
0:       DataType columnType = pages[colId].getColumnSpec().getSchemaDataType();
0:       col2DataType.put(columnName, columnType);
0:     }
1: 
0:     // for each row
0:     for (int rowId = 0; rowId < pages[0].getPageSize(); rowId++) {
0:       // for each indexed column
0:       for (int indexColId = 0; indexColId < indexedColumns.size(); indexColId++) {
0:         String indexedCol = indexedColumns.get(indexColId);
0:         byte[] indexValue;
0:         if (DataTypes.STRING == col2DataType.get(indexedCol)
0:             || DataTypes.BYTE_ARRAY == col2DataType.get(indexedCol)) {
0:           byte[] originValue = (byte[]) pages[col2Ordianl.get(indexedCol)].getData(rowId);
0:           indexValue = new byte[originValue.length - 2];
0:           System.arraycopy(originValue, 2, indexValue, 0, originValue.length - 2);
0:         } else {
0:           Object originValue = pages[col2Ordianl.get(indexedCol)].getData(rowId);
0:           indexValue = CarbonUtil.getValueAsBytes(col2DataType.get(indexedCol), originValue);
0:         }
1: 
0:         indexBloomFilters.get(indexColId).put(indexValue);
0:       }
0:     }
0:   }
1: 
0:   private void initDataMapFile() throws IOException {
0:     String dataMapDir = genDataMapStorePath(this.writeDirectoryPath, this.dataMapName);
0:     for (int indexColId = 0; indexColId < indexedColumns.size(); indexColId++) {
0:       String dmFile = dataMapDir + File.separator + this.currentBlockId
0:           + '.' + indexedColumns.get(indexColId) + BloomCoarseGrainDataMap.BLOOM_INDEX_SUFFIX;
0:       DataOutputStream dataOutStream = null;
0:       ObjectOutputStream objectOutStream = null;
0:       try {
0:         FileFactory.createNewFile(dmFile, FileFactory.getFileType(dmFile));
0:         dataOutStream = FileFactory.getDataOutputStream(dmFile,
0:             FileFactory.getFileType(dmFile));
0:         objectOutStream = new ObjectOutputStream(dataOutStream);
0:       } catch (IOException e) {
0:         CarbonUtil.closeStreams(objectOutStream, dataOutStream);
0:         throw new IOException(e);
0:       }
1: 
0:       this.currentDMFiles.add(dmFile);
0:       this.currentDataOutStreams.add(dataOutStream);
0:       this.currentObjectOutStreams.add(objectOutStream);
0:     }
0:   }
1: 
0:   private void writeBloomDataMapFile() throws IOException {
0:     for (int indexColId = 0; indexColId < indexedColumns.size(); indexColId++) {
0:       BloomDMModel model = new BloomDMModel(this.currentBlockId, this.currentBlockletId,
0:           indexBloomFilters.get(indexColId));
0:       // only in higher version of guava-bloom-filter, it provides readFrom/writeTo interface.
0:       // In lower version, we use default java serializer to write bloomfilter.
0:       this.currentObjectOutStreams.get(indexColId).writeObject(model);
0:       this.currentObjectOutStreams.get(indexColId).flush();
0:       this.currentDataOutStreams.get(indexColId).flush();
0:     }
0:   }
1: 
1:   @Override
0:   public void finish() throws IOException {
1: 
0:   }
1: 
1:   @Override
0:   protected void commitFile(String dataMapFile) throws IOException {
0:     super.commitFile(dataMapFile);
0:   }
1: 
1:   /**
0:    * create and return path that will store the datamap
1:    *
0:    * @param dataPath patch to store the carbondata factdata
0:    * @param dataMapName datamap name
0:    * @return path to store the datamap
0:    * @throws IOException
1:    */
0:   public static String genDataMapStorePath(String dataPath, String dataMapName)
0:       throws IOException {
0:     String dmDir = dataPath + File.separator + dataMapName;
0:     Path dmPath = FileFactory.getPath(dmDir);
0:     FileSystem fs = FileFactory.getFileSystem(dmPath);
0:     if (!fs.exists(dmPath)) {
0:       fs.mkdirs(dmPath);
0:     }
0:     return dmDir;
0:   }
0: }
author:Manhua
-------------------------------------------------------------------------------
commit:81038f5
/////////////////////////////////////////////////////////////////////////
0: import java.util.HashMap;
0: import java.util.Map;
0: import org.apache.carbondata.core.keygenerator.KeyGenerator;
0: import org.apache.carbondata.core.keygenerator.columnar.ColumnarSplitter;
0: import org.apache.carbondata.core.metadata.schema.table.column.CarbonDimension;
0: import org.apache.carbondata.core.util.CarbonUtil;
0: 
0: import org.apache.commons.collections.CollectionUtils;
0: import org.apache.commons.collections.Predicate;
/////////////////////////////////////////////////////////////////////////
0:   private KeyGenerator keyGenerator;
0:   private ColumnarSplitter columnarSplitter;
0:   // for the dict/sort/date column, they are encoded in MDK,
0:   // this maps the index column name to the index in MDK
0:   private Map<String, Integer> indexCol2MdkIdx;
0:   // this gives the reverse map to indexCol2MdkIdx
0:   private Map<Integer, String> mdkIdx2IndexCol;
/////////////////////////////////////////////////////////////////////////
0: 
0:     keyGenerator = segmentProperties.getDimensionKeyGenerator();
0:     columnarSplitter = segmentProperties.getFixedLengthKeySplitter();
0:     this.indexCol2MdkIdx = new HashMap<>();
0:     this.mdkIdx2IndexCol = new HashMap<>();
0:     int idx = 0;
0:     for (final CarbonDimension dimension : segmentProperties.getDimensions()) {
0:       if (!dimension.isGlobalDictionaryEncoding() && !dimension.isDirectDictionaryEncoding()) {
0:         continue;
0:       }
0:       boolean isExistInIndex = CollectionUtils.exists(indexColumns, new Predicate() {
0:         @Override public boolean evaluate(Object object) {
0:           return ((CarbonColumn) object).getColName().equalsIgnoreCase(dimension.getColName());
0:         }
0:       });
0:       if (isExistInIndex) {
0:         this.indexCol2MdkIdx.put(dimension.getColName(), idx);
0:         this.mdkIdx2IndexCol.put(idx, dimension.getColName());
0:       }
0:       idx++;
0:     }
/////////////////////////////////////////////////////////////////////////
0: 
0:   @Override
1:   protected byte[] convertDictionaryValue(int indexColIdx, Object value) {
1:     // input value from onPageAdded in load process is byte[]
0:     byte[] fakeMdkBytes;
0:     // this means that we need to pad some fake bytes
0:     // to get the whole MDK in corresponding position
0:     if (columnarSplitter.getBlockKeySize().length > indexCol2MdkIdx.size()) {
0:       int totalSize = 0;
0:       for (int size : columnarSplitter.getBlockKeySize()) {
0:         totalSize += size;
0:       }
0:       fakeMdkBytes = new byte[totalSize];
0: 
0:       // put this bytes to corresponding position
0:       int thisKeyIdx = indexCol2MdkIdx.get(indexColumns.get(indexColIdx).getColName());
0:       int destPos = 0;
0:       for (int keyIdx = 0; keyIdx < columnarSplitter.getBlockKeySize().length; keyIdx++) {
0:         if (thisKeyIdx == keyIdx) {
0:           System.arraycopy(value, 0,
0:               fakeMdkBytes, destPos, columnarSplitter.getBlockKeySize()[thisKeyIdx]);
0:           break;
0:         }
0:         destPos += columnarSplitter.getBlockKeySize()[keyIdx];
0:       }
0:     } else {
0:       fakeMdkBytes = (byte[])value;
0:     }
0:     // for dict columns including dictionary and date columns
0:     // decode value to get the surrogate key
0:     int surrogateKey = (int) keyGenerator.getKey(fakeMdkBytes,
0:         indexCol2MdkIdx.get(indexColumns.get(indexColIdx).getColName()));
0:     // store the dictionary key in bloom
1:     return CarbonUtil.getValueAsBytes(DataTypes.INT, surrogateKey);
0:   }
author:ravipesala
-------------------------------------------------------------------------------
commit:047c502
/////////////////////////////////////////////////////////////////////////
0:         CarbonBloomFilter bloomFilter = indexBloomFilters.get(indexColId);
0:         bloomFilter.setBlockletNo(currentBlockletId);
0:         bloomFilter.write(this.currentDataOutStreams.get(indexColId));
commit:77a1110
/////////////////////////////////////////////////////////////////////////
/////////////////////////////////////////////////////////////////////////
0: import org.apache.hadoop.util.bloom.CarbonBloomFilter;
0: import org.apache.hadoop.util.bloom.Key;
0: import org.apache.hadoop.util.hash.Hash;
/////////////////////////////////////////////////////////////////////////
0:   private boolean compressBloom;
0:   protected List<CarbonBloomFilter> indexBloomFilters;
0:       Segment segment, String shardName, int bloomFilterSize, double bloomFilterFpp,
0:       boolean compressBloom)
0:     this.compressBloom = compressBloom;
0:     currentDMFiles = new ArrayList<>(indexColumns.size());
0:     currentDataOutStreams = new ArrayList<>(indexColumns.size());
0:     indexBloomFilters = new ArrayList<>(indexColumns.size());
/////////////////////////////////////////////////////////////////////////
0:     int[] stats = calculateBloomStats();
0:       indexBloomFilters
0:           .add(new CarbonBloomFilter(stats[0], stats[1], Hash.MURMUR_HASH, compressBloom));
0:   /**
0:    * It calculates the bits size and number of hash functions to calculate bloom.
0:    */
0:   private int[] calculateBloomStats() {
0:     /*
0:      * n: how many items you expect to have in your filter
0:      * p: your acceptable false positive rate
0:      * Number of bits (m) = -n*ln(p) / (ln(2)^2)
0:      * Number of hashes(k) = m/n * ln(2)
0:      */
0:     double sizeinBits = -bloomFilterSize * Math.log(bloomFilterFpp) / (Math.pow(Math.log(2), 2));
0:     double numberOfHashes = sizeinBits / bloomFilterSize * Math.log(2);
0:     int[] stats = new int[2];
0:     stats[0] = (int) Math.ceil(sizeinBits);
0:     stats[1] = (int) Math.ceil(numberOfHashes);
0:     return stats;
0:   }
0: 
/////////////////////////////////////////////////////////////////////////
0:         indexBloomFilters.get(i).add(new Key(indexValue));
/////////////////////////////////////////////////////////////////////////
0:         CarbonUtil.closeStreams(dataOutStream);
/////////////////////////////////////////////////////////////////////////
0:         model.write(this.currentDataOutStreams.get(indexColId));
/////////////////////////////////////////////////////////////////////////
0:           currentDataOutStreams.get(indexColId));
commit:8b33ab2
/////////////////////////////////////////////////////////////////////////
0: import org.apache.carbondata.core.constants.CarbonCommonConstants;
/////////////////////////////////////////////////////////////////////////
/////////////////////////////////////////////////////////////////////////
0:   private String indexShardName;
/////////////////////////////////////////////////////////////////////////
0:   public void onBlockStart(String blockId, String indexShardName) throws IOException {
0:     if (this.indexShardName == null) {
0:       this.indexShardName = indexShardName;
0:       initDataMapFile();
0:     }
0: 
/////////////////////////////////////////////////////////////////////////
0:     dataMapDir = dataMapDir + CarbonCommonConstants.FILE_SEPARATOR + this.indexShardName;
0:     FileFactory.mkdirs(dataMapDir, FileFactory.getFileType(dataMapDir));
0:       String dmFile = dataMapDir + CarbonCommonConstants.FILE_SEPARATOR +
0:           indexedColumns.get(indexColId) + BloomCoarseGrainDataMap.BLOOM_INDEX_SUFFIX;
/////////////////////////////////////////////////////////////////////////
0:       BloomDMModel model = new BloomDMModel(this.currentBlockletId,
/////////////////////////////////////////////////////////////////////////
0:     for (int indexColId = 0; indexColId < indexedColumns.size(); indexColId++) {
0:       CarbonUtil.closeStreams(this.currentDataOutStreams.get(indexColId),
0:           this.currentObjectOutStreams.get(indexColId));
0:       commitFile(this.currentDMFiles.get(indexColId));
0:     }
/////////////////////////////////////////////////////////////////////////
0:     FileFactory.mkdirs(dmDir, FileFactory.getFileType(dmDir));
author:akashrn5
-------------------------------------------------------------------------------
commit:7f4bd3d
/////////////////////////////////////////////////////////////////////////
0:     if (!isWritingFinished()) {
0:       if (indexBloomFilters.size() > 0) {
0:         writeBloomDataMapFile();
0:       }
0:       releaseResouce();
0:       setWritingFinished(true);
author:Jacky Li
-------------------------------------------------------------------------------
commit:9db662a
/////////////////////////////////////////////////////////////////////////
0: import org.apache.carbondata.common.logging.LogService;
0: import org.apache.carbondata.common.logging.LogServiceFactory;
1: import org.apache.carbondata.core.metadata.schema.table.column.CarbonColumn;
1:  * BloomDataMap is constructed in CG level (blocklet level).
1:  * For each indexed column, a bloom filter is constructed to indicate whether a value
1:  * belongs to this blocklet. Bloom filter of blocklet that belongs to same block will
1:  * be written to one index file suffixed with .bloomindex. So the number
0:   private static final LogService LOG = LogServiceFactory.getLogService(
0:       BloomDataMapWriter.class.getCanonicalName());
0:   protected int currentBlockletId;
0:   protected List<BloomFilter<byte[]>> indexBloomFilters;
1:   BloomDataMapWriter(String tablePath, String dataMapName, List<CarbonColumn> indexColumns,
0:       Segment segment, String shardName, int bloomFilterSize) throws IOException {
0:     super(tablePath, dataMapName, indexColumns, segment, shardName);
0:     currentDMFiles = new ArrayList<String>(indexColumns.size());
0:     currentDataOutStreams = new ArrayList<DataOutputStream>(indexColumns.size());
0:     currentObjectOutStreams = new ArrayList<ObjectOutputStream>(indexColumns.size());
0:     indexBloomFilters = new ArrayList<BloomFilter<byte[]>>(indexColumns.size());
0:     initDataMapFile();
0:     resetBloomFilters();
0:   public void onBlockStart(String blockId) throws IOException {
0:   }
0: 
0:   protected void resetBloomFilters() {
0:     List<CarbonColumn> indexColumns = getIndexColumns();
0:     for (int i = 0; i < indexColumns.size(); i++) {
/////////////////////////////////////////////////////////////////////////
0:     writeBloomDataMapFile();
0:     currentBlockletId++;
0:   public void onPageAdded(int blockletId, int pageId, int pageSize, ColumnPage[] pages) {
0:     List<CarbonColumn> indexColumns = getIndexColumns();
0:     for (int rowId = 0; rowId < pageSize; rowId++) {
0:       // for each indexed column, add the data to bloom filter
0:       for (int i = 0; i < indexColumns.size(); i++) {
0:         Object data = pages[i].getData(rowId);
0:         DataType dataType = indexColumns.get(i).getDataType();
0:         if (DataTypes.STRING == dataType) {
0:           indexValue = getStringData(data);
0:         } else if (DataTypes.BYTE_ARRAY == dataType) {
0:           byte[] originValue = (byte[]) data;
0:           // String and byte array is LV encoded, L is short type
0:           indexValue = CarbonUtil.getValueAsBytes(dataType, data);
0:         indexBloomFilters.get(i).put(indexValue);
0:   protected byte[] getStringData(Object data) {
0:     byte[] lvData = (byte[]) data;
0:     byte[] indexValue = new byte[lvData.length - 2];
0:     System.arraycopy(lvData, 2, indexValue, 0, lvData.length - 2);
0:     return indexValue;
0:   }
0: 
0:     if (!FileFactory.isFileExist(dataMapPath)) {
0:       if (!FileFactory.mkdirs(dataMapPath, FileFactory.getFileType(dataMapPath))) {
0:         throw new IOException("Failed to create directory " + dataMapPath);
0:       }
0:     }
0:     List<CarbonColumn> indexColumns = getIndexColumns();
0:     for (int indexColId = 0; indexColId < indexColumns.size(); indexColId++) {
0:       String dmFile = dataMapPath + CarbonCommonConstants.FILE_SEPARATOR +
0:           indexColumns.get(indexColId).getColName() + BloomCoarseGrainDataMap.BLOOM_INDEX_SUFFIX;
/////////////////////////////////////////////////////////////////////////
0:   protected void writeBloomDataMapFile() {
0:     List<CarbonColumn> indexColumns = getIndexColumns();
0:     try {
0:       for (int indexColId = 0; indexColId < indexColumns.size(); indexColId++) {
0:         BloomDMModel model =
0:             new BloomDMModel(this.currentBlockletId, indexBloomFilters.get(indexColId));
0:         // only in higher version of guava-bloom-filter, it provides readFrom/writeTo interface.
0:         // In lower version, we use default java serializer to write bloomfilter.
0:         this.currentObjectOutStreams.get(indexColId).writeObject(model);
0:         this.currentObjectOutStreams.get(indexColId).flush();
0:         this.currentDataOutStreams.get(indexColId).flush();
0:       }
0:     } catch (Exception e) {
0:       for (ObjectOutputStream objectOutputStream : currentObjectOutStreams) {
0:         CarbonUtil.closeStreams(objectOutputStream);
0:       }
0:       for (DataOutputStream dataOutputStream : currentDataOutStreams) {
0:         CarbonUtil.closeStreams(dataOutputStream);
0:       }
0:       throw new RuntimeException(e);
0:     } finally {
0:       resetBloomFilters();
0:     if (indexBloomFilters.size() > 0) {
0:       writeBloomDataMapFile();
0:     }
0:     releaseResouce();
0:   }
0: 
0:   protected void releaseResouce() {
0:     List<CarbonColumn> indexColumns = getIndexColumns();
0:     for (int indexColId = 0; indexColId < indexColumns.size(); indexColId++) {
0:       CarbonUtil.closeStreams(
0:           currentDataOutStreams.get(indexColId), currentObjectOutStreams.get(indexColId));
============================================================================