1:2cf1104: /*
1:2cf1104:  * Licensed to the Apache Software Foundation (ASF) under one or more
1:2cf1104:  * contributor license agreements.  See the NOTICE file distributed with
1:2cf1104:  * this work for additional information regarding copyright ownership.
1:2cf1104:  * The ASF licenses this file to You under the Apache License, Version 2.0
1:2cf1104:  * (the "License"); you may not use this file except in compliance with
1:2cf1104:  * the License.  You may obtain a copy of the License at
1:2cf1104:  *
1:2cf1104:  *    http://www.apache.org/licenses/LICENSE-2.0
1:2cf1104:  *
1:2cf1104:  * Unless required by applicable law or agreed to in writing, software
1:2cf1104:  * distributed under the License is distributed on an "AS IS" BASIS,
1:2cf1104:  * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
1:2cf1104:  * See the License for the specific language governing permissions and
1:2cf1104:  * limitations under the License.
1:2cf1104:  */
1:2cf1104: package org.apache.carbondata.core.datastore.chunk.reader.dimension.v3;
2:2cf1104: 
1:2cf1104: import java.io.IOException;
1:2cf1104: import java.nio.ByteBuffer;
1:e6a4f64: import java.util.List;
1:2cf1104: 
1:43285bb: import org.apache.carbondata.core.constants.CarbonCommonConstants;
1:daa6465: import org.apache.carbondata.core.datastore.FileReader;
1:daa6465: import org.apache.carbondata.core.datastore.chunk.DimensionColumnPage;
1:2cf1104: import org.apache.carbondata.core.datastore.chunk.impl.DimensionRawColumnChunk;
1:daa6465: import org.apache.carbondata.core.datastore.chunk.impl.FixedLengthDimensionColumnPage;
1:daa6465: import org.apache.carbondata.core.datastore.chunk.impl.VariableLengthDimensionColumnPage;
1:2cf1104: import org.apache.carbondata.core.datastore.chunk.reader.dimension.AbstractChunkReaderV2V3Format;
1:e6a4f64: import org.apache.carbondata.core.datastore.chunk.store.ColumnPageWrapper;
1:dc53dee: import org.apache.carbondata.core.datastore.chunk.store.DimensionChunkStoreFactory;
1:2cf1104: import org.apache.carbondata.core.datastore.columnar.UnBlockIndexer;
1:8f08c4a: import org.apache.carbondata.core.datastore.compression.CompressorFactory;
1:e6a4f64: import org.apache.carbondata.core.datastore.page.ColumnPage;
1:e6a4f64: import org.apache.carbondata.core.datastore.page.encoding.ColumnPageDecoder;
1:8c1ddbf: import org.apache.carbondata.core.datastore.page.encoding.DefaultEncodingFactory;
1:8c1ddbf: import org.apache.carbondata.core.datastore.page.encoding.EncodingFactory;
1:e6a4f64: import org.apache.carbondata.core.memory.MemoryException;
1:2cf1104: import org.apache.carbondata.core.metadata.blocklet.BlockletInfo;
1:438b442: import org.apache.carbondata.core.scan.executor.util.QueryUtil;
1:8f08c4a: import org.apache.carbondata.core.util.CarbonMetadataUtil;
1:2cf1104: import org.apache.carbondata.core.util.CarbonUtil;
1:2cf1104: import org.apache.carbondata.format.DataChunk2;
1:2cf1104: import org.apache.carbondata.format.DataChunk3;
1:2cf1104: import org.apache.carbondata.format.Encoding;
1:2cf1104: 
1:2cf1104: import org.apache.commons.lang.ArrayUtils;
1:2cf1104: 
1:2cf1104: /**
1:2cf1104:  * Dimension column V3 Reader class which will be used to read and uncompress
1:2cf1104:  * V3 format data
1:2cf1104:  * data format
1:2cf1104:  * Data Format
1:b41e48f:  * <FileHeader>
1:2cf1104:  * <Column1 Data ChunkV3><Column1<Page1><Page2><Page3><Page4>>
1:2cf1104:  * <Column2 Data ChunkV3><Column2<Page1><Page2><Page3><Page4>>
1:2cf1104:  * <Column3 Data ChunkV3><Column3<Page1><Page2><Page3><Page4>>
1:2cf1104:  * <Column4 Data ChunkV3><Column4<Page1><Page2><Page3><Page4>>
1:b41e48f:  * <File Footer>
1:2cf1104:  */
1:2cf1104: public class CompressedDimensionChunkFileBasedReaderV3 extends AbstractChunkReaderV2V3Format {
1:e6a4f64: 
1:8c1ddbf:   private EncodingFactory encodingFactory = DefaultEncodingFactory.getInstance();
1:2cf1104: 
1:2cf1104:   /**
1:2cf1104:    * end position of last dimension in carbon data file
1:2cf1104:    */
1:2cf1104:   private long lastDimensionOffsets;
1:2cf1104: 
1:2cf1104:   public CompressedDimensionChunkFileBasedReaderV3(BlockletInfo blockletInfo,
1:2cf1104:       int[] eachColumnValueSize, String filePath) {
1:2cf1104:     super(blockletInfo, eachColumnValueSize, filePath);
1:2cf1104:     lastDimensionOffsets = blockletInfo.getDimensionOffset();
3:2cf1104:   }
1:2cf1104: 
1:2cf1104:   /**
1:2cf1104:    * Below method will be used to read the dimension column data form carbon data file
1:2cf1104:    * Steps for reading
1:2cf1104:    * 1. Get the length of the data to be read
1:2cf1104:    * 2. Allocate the direct buffer
1:2cf1104:    * 3. read the data from file
1:2cf1104:    * 4. Get the data chunk object from data read
1:2cf1104:    * 5. Create the raw chunk object and fill the details
1:2cf1104:    *
1:2cf1104:    * @param fileReader          reader for reading the column from carbon data file
1:daa6465:    * @param columnIndex blocklet index of the column in carbon data file
1:2cf1104:    * @return dimension raw chunk
1:2cf1104:    */
1:daa6465:   public DimensionRawColumnChunk readRawDimensionChunk(FileReader fileReader,
1:daa6465:       int columnIndex) throws IOException {
1:2cf1104:     // get the current dimension offset
1:daa6465:     long currentDimensionOffset = dimensionChunksOffset.get(columnIndex);
1:2cf1104:     int length = 0;
1:2cf1104:     // to calculate the length of the data to be read
1:2cf1104:     // column other than last column we can subtract the offset of current column with
1:2cf1104:     // next column and get the total length.
1:2cf1104:     // but for last column we need to use lastDimensionOffset which is the end position
1:2cf1104:     // of the last dimension, we can subtract current dimension offset from lastDimesionOffset
1:daa6465:     if (dimensionChunksOffset.size() - 1 == columnIndex) {
1:2cf1104:       length = (int) (lastDimensionOffsets - currentDimensionOffset);
1:2cf1104:     } else {
1:daa6465:       length = (int) (dimensionChunksOffset.get(columnIndex + 1) - currentDimensionOffset);
1:2cf1104:     }
1:8f59a32:     ByteBuffer buffer = null;
1:2cf1104:     // read the data from carbon data file
1:2cf1104:     synchronized (fileReader) {
1:8f59a32:       buffer = fileReader.readByteBuffer(filePath, currentDimensionOffset, length);
1:2cf1104:     }
1:2cf1104:     // get the data chunk which will have all the details about the data pages
1:2cf1104:     DataChunk3 dataChunk = CarbonUtil.readDataChunk3(buffer, 0, length);
1:daa6465:     return getDimensionRawColumnChunk(fileReader, columnIndex, 0, length, buffer,
1:d509f17:         dataChunk);
1:d509f17:   }
1:d509f17: 
1:daa6465:   protected DimensionRawColumnChunk getDimensionRawColumnChunk(FileReader fileReader,
1:daa6465:       int columnIndex, long offset, int length, ByteBuffer buffer, DataChunk3 dataChunk) {
1:2cf1104:     // creating a raw chunks instance and filling all the details
1:2cf1104:     DimensionRawColumnChunk rawColumnChunk =
1:daa6465:         new DimensionRawColumnChunk(columnIndex, buffer, offset, length, this);
2:2cf1104:     int numberOfPages = dataChunk.getPage_length().size();
2:2cf1104:     byte[][] maxValueOfEachPage = new byte[numberOfPages][];
2:2cf1104:     byte[][] minValueOfEachPage = new byte[numberOfPages][];
2:2cf1104:     int[] eachPageLength = new int[numberOfPages];
1:2cf1104:     for (int i = 0; i < minValueOfEachPage.length; i++) {
1:2cf1104:       maxValueOfEachPage[i] =
1:2cf1104:           dataChunk.getData_chunk_list().get(i).getMin_max().getMax_values().get(0).array();
1:2cf1104:       minValueOfEachPage[i] =
1:2cf1104:           dataChunk.getData_chunk_list().get(i).getMin_max().getMin_values().get(0).array();
1:2cf1104:       eachPageLength[i] = dataChunk.getData_chunk_list().get(i).getNumberOfRowsInpage();
1:2cf1104:     }
1:2cf1104:     rawColumnChunk.setDataChunkV3(dataChunk);
1:daa6465:     rawColumnChunk.setFileReader(fileReader);
1:2cf1104:     rawColumnChunk.setPagesCount(dataChunk.getPage_length().size());
1:2cf1104:     rawColumnChunk.setMaxValues(maxValueOfEachPage);
1:2cf1104:     rawColumnChunk.setMinValues(minValueOfEachPage);
1:2cf1104:     rawColumnChunk.setRowCount(eachPageLength);
1:2cf1104:     rawColumnChunk.setOffsets(ArrayUtils
2:2cf1104:         .toPrimitive(dataChunk.page_offset.toArray(new Integer[dataChunk.page_offset.size()])));
1:2cf1104:     return rawColumnChunk;
1:2cf1104:   }
1:2cf1104: 
1:2cf1104:   /**
1:2cf1104:    * Below method will be used to read the multiple dimension column data in group
1:2cf1104:    * and divide into dimension raw chunk object
1:2cf1104:    * Steps for reading
1:2cf1104:    * 1. Get the length of the data to be read
1:2cf1104:    * 2. Allocate the direct buffer
1:2cf1104:    * 3. read the data from file
1:2cf1104:    * 4. Get the data chunk object from file for each column
1:2cf1104:    * 5. Create the raw chunk object and fill the details for each column
1:2cf1104:    * 6. increment the offset of the data
1:2cf1104:    *
1:2cf1104:    * @param fileReader
1:2cf1104:    *        reader which will be used to read the dimension columns data from file
1:2cf1104:    * @param startBlockletColumnIndex
1:2cf1104:    *        blocklet index of the first dimension column
1:2cf1104:    * @param endBlockletColumnIndex
1:2cf1104:    *        blocklet index of the last dimension column
1:2cf1104:    * @ DimensionRawColumnChunk array
1:2cf1104:    */
1:daa6465:   protected DimensionRawColumnChunk[] readRawDimensionChunksInGroup(FileReader fileReader,
1:2cf1104:       int startBlockletColumnIndex, int endBlockletColumnIndex) throws IOException {
1:2cf1104:     // to calculate the length of the data to be read
1:2cf1104:     // column we can subtract the offset of start column offset with
1:2cf1104:     // end column+1 offset and get the total length.
1:2cf1104:     long currentDimensionOffset = dimensionChunksOffset.get(startBlockletColumnIndex);
1:8f59a32:     ByteBuffer buffer = null;
1:2cf1104:     // read the data from carbon data file
1:2cf1104:     synchronized (fileReader) {
1:8f59a32:       buffer = fileReader.readByteBuffer(filePath, currentDimensionOffset,
2:2cf1104:           (int) (dimensionChunksOffset.get(endBlockletColumnIndex + 1) - currentDimensionOffset));
1:2cf1104:     }
1:2cf1104:     // create raw chunk for each dimension column
1:2cf1104:     DimensionRawColumnChunk[] dimensionDataChunks =
1:2cf1104:         new DimensionRawColumnChunk[endBlockletColumnIndex - startBlockletColumnIndex + 1];
1:2cf1104:     int index = 0;
1:2cf1104:     int runningLength = 0;
1:2cf1104:     for (int i = startBlockletColumnIndex; i <= endBlockletColumnIndex; i++) {
1:2cf1104:       int currentLength = (int) (dimensionChunksOffset.get(i + 1) - dimensionChunksOffset.get(i));
1:2cf1104:       DataChunk3 dataChunk =
1:2cf1104:           CarbonUtil.readDataChunk3(buffer, runningLength, dimensionChunksLength.get(i));
1:d509f17:       dimensionDataChunks[index] =
1:d509f17:           getDimensionRawColumnChunk(fileReader, i, runningLength, currentLength, buffer,
1:d509f17:               dataChunk);
1:2cf1104:       runningLength += currentLength;
1:2cf1104:       index++;
1:2cf1104:     }
1:2cf1104:     return dimensionDataChunks;
1:2cf1104:   }
1:2cf1104: 
1:2cf1104:   /**
1:2cf1104:    * Below method will be used to convert the compressed dimension chunk raw data to actual data
1:2cf1104:    *
1:e6a4f64:    * @param rawColumnPage dimension raw chunk
1:086b06d:    * @param pageNumber              number
1:daa6465:    * @return DimensionColumnPage
1:2cf1104:    */
1:daa6465:   @Override public DimensionColumnPage decodeColumnPage(
1:e6a4f64:       DimensionRawColumnChunk rawColumnPage, int pageNumber) throws IOException, MemoryException {
1:2cf1104:     // data chunk of blocklet column
1:e6a4f64:     DataChunk3 dataChunk3 = rawColumnPage.getDataChunkV3();
1:2cf1104:     // get the data buffer
1:e6a4f64:     ByteBuffer rawData = rawColumnPage.getRawData();
1:e6a4f64:     DataChunk2 pageMetadata = dataChunk3.getData_chunk_list().get(pageNumber);
1:8f08c4a:     String compressorName = CarbonMetadataUtil.getCompressorNameFromChunkMeta(
1:8f08c4a:         pageMetadata.getChunk_meta());
1:8f08c4a:     this.compressor = CompressorFactory.getInstance().getCompressor(compressorName);
1:2cf1104:     // calculating the start point of data
1:2cf1104:     // as buffer can contain multiple column data, start point will be datachunkoffset +
1:2cf1104:     // data chunk length + page offset
1:d509f17:     int offset = (int) rawColumnPage.getOffSet() + dimensionChunksLength
1:8c1ddbf:         .get(rawColumnPage.getColumnIndex()) + dataChunk3.getPage_offset().get(pageNumber);
1:2cf1104:     // first read the data and uncompressed it
1:e6a4f64:     return decodeDimension(rawColumnPage, rawData, pageMetadata, offset);
1:e6a4f64:   }
1:e6a4f64: 
1:8c1ddbf:   private ColumnPage decodeDimensionByMeta(DataChunk2 pageMetadata,
1:43285bb:       ByteBuffer pageData, int offset, boolean isLocalDictEncodedPage)
1:e6a4f64:       throws IOException, MemoryException {
1:e6a4f64:     List<Encoding> encodings = pageMetadata.getEncoders();
1:e6a4f64:     List<ByteBuffer> encoderMetas = pageMetadata.getEncoder_meta();
1:8f08c4a:     String compressorName = CarbonMetadataUtil.getCompressorNameFromChunkMeta(
1:8f08c4a:         pageMetadata.getChunk_meta());
1:8f08c4a:     ColumnPageDecoder decoder = encodingFactory.createDecoder(encodings, encoderMetas,
1:8f08c4a:         compressorName);
1:43285bb:     return decoder
1:43285bb:         .decode(pageData.array(), offset, pageMetadata.data_page_length, isLocalDictEncodedPage);
1:e6a4f64:   }
1:e6a4f64: 
1:e6a4f64:   private boolean isEncodedWithMeta(DataChunk2 pageMetadata) {
1:e6a4f64:     List<Encoding> encodings = pageMetadata.getEncoders();
1:e6a4f64:     if (encodings != null && encodings.size() == 1) {
1:e6a4f64:       Encoding encoding = encodings.get(0);
1:e6a4f64:       switch (encoding) {
1:e6a4f64:         case DIRECT_COMPRESS:
1:e6a4f64:         case DIRECT_STRING:
1:438b442:         case ADAPTIVE_INTEGRAL:
1:438b442:         case ADAPTIVE_DELTA_INTEGRAL:
1:438b442:         case ADAPTIVE_FLOATING:
1:438b442:         case ADAPTIVE_DELTA_FLOATING:
1:e6a4f64:           return true;
1:e6a4f64:       }
1:e6a4f64:     }
1:e6a4f64:     return false;
1:e6a4f64:   }
1:e6a4f64: 
1:daa6465:   protected DimensionColumnPage decodeDimension(DimensionRawColumnChunk rawColumnPage,
1:e6a4f64:       ByteBuffer pageData, DataChunk2 pageMetadata, int offset)
1:e6a4f64:       throws IOException, MemoryException {
1:e6a4f64:     if (isEncodedWithMeta(pageMetadata)) {
1:43285bb:       ColumnPage decodedPage = decodeDimensionByMeta(pageMetadata, pageData, offset,
1:43285bb:           null != rawColumnPage.getLocalDictionary());
1:8f08c4a:       decodedPage.setNullBits(QueryUtil.getNullBitSet(pageMetadata.presence, this.compressor));
1:438b442:       return new ColumnPageWrapper(decodedPage, rawColumnPage.getLocalDictionary(),
1:438b442:           isEncodedWithAdaptiveMeta(pageMetadata));
1:e6a4f64:     } else {
1:e6a4f64:       // following code is for backward compatibility
1:e6a4f64:       return decodeDimensionLegacy(rawColumnPage, pageData, pageMetadata, offset);
1:438b442:     }
1:e6a4f64:   }
1:e6a4f64: 
1:438b442:   private boolean isEncodedWithAdaptiveMeta(DataChunk2 pageMetadata) {
1:438b442:     List<Encoding> encodings = pageMetadata.getEncoders();
1:438b442:     if (encodings != null && encodings.size() == 1) {
1:438b442:       Encoding encoding = encodings.get(0);
1:438b442:       switch (encoding) {
1:438b442:         case ADAPTIVE_INTEGRAL:
1:438b442:         case ADAPTIVE_DELTA_INTEGRAL:
1:438b442:         case ADAPTIVE_FLOATING:
1:438b442:         case ADAPTIVE_DELTA_FLOATING:
1:438b442:           return true;
1:438b442:       }
1:438b442:     }
1:438b442:     return false;
1:e6a4f64:   }
1:438b442: 
1:daa6465:   private DimensionColumnPage decodeDimensionLegacy(DimensionRawColumnChunk rawColumnPage,
1:e710339:       ByteBuffer pageData, DataChunk2 pageMetadata, int offset) throws IOException,
1:e710339:       MemoryException {
1:e6a4f64:     byte[] dataPage;
1:e6a4f64:     int[] rlePage;
1:7ef9164:     int[] invertedIndexes = new int[0];
1:7ef9164:     int[] invertedIndexesReverse = new int[0];
1:8f08c4a:     dataPage = compressor.unCompressByte(pageData.array(), offset, pageMetadata.data_page_length);
1:e6a4f64:     offset += pageMetadata.data_page_length;
1:2cf1104:     // if row id block is present then read the row id chunk and uncompress it
1:2ccdbb7:     if (CarbonUtil.hasEncoding(pageMetadata.encoders, Encoding.INVERTED_INDEX)) {
1:2cf1104:       invertedIndexes = CarbonUtil
1:e6a4f64:           .getUnCompressColumnIndex(pageMetadata.rowid_page_length, pageData, offset);
1:e6a4f64:       offset += pageMetadata.rowid_page_length;
1:2cf1104:       // get the reverse index
1:2ccdbb7:       invertedIndexesReverse = CarbonUtil.getInvertedReverseIndex(invertedIndexes);
1:2cf1104:     }
1:2cf1104:     // if rle is applied then read the rle block chunk and then uncompress
1:2cf1104:     //then actual data based on rle block
1:2ccdbb7:     if (CarbonUtil.hasEncoding(pageMetadata.encoders, Encoding.RLE)) {
1:2cf1104:       rlePage =
1:e6a4f64:           CarbonUtil.getIntArray(pageData, offset, pageMetadata.rle_page_length);
1:2cf1104:       // uncompress the data with rle indexes
1:2cf1104:       dataPage = UnBlockIndexer.uncompressData(dataPage, rlePage,
1:3a4b881:           null == rawColumnPage.getLocalDictionary() ?
1:3a4b881:               eachColumnValueSize[rawColumnPage.getColumnIndex()] :
1:43285bb:               CarbonCommonConstants.LOCAL_DICT_ENCODED_BYTEARRAY_SIZE);
1:2cf1104:     }
1:e6a4f64: 
1:daa6465:     DimensionColumnPage columnDataChunk = null;
1:2cf1104:     // if no dictionary column then first create a no dictionary column chunk
1:2cf1104:     // and set to data chunk instance
1:2ccdbb7:     if (!CarbonUtil.hasEncoding(pageMetadata.encoders, Encoding.DICTIONARY)) {
1:dc53dee:       DimensionChunkStoreFactory.DimensionStoreType dimStoreType =
1:3a4b881:           null != rawColumnPage.getLocalDictionary() ?
1:3a4b881:               DimensionChunkStoreFactory.DimensionStoreType.LOCAL_DICT :
1:2ccdbb7:               (CarbonUtil.hasEncoding(pageMetadata.encoders, Encoding.DIRECT_COMPRESS_VARCHAR) ?
1:dc53dee:                   DimensionChunkStoreFactory.DimensionStoreType.VARIABLE_INT_LENGTH :
1:3a4b881:                   DimensionChunkStoreFactory.DimensionStoreType.VARIABLE_SHORT_LENGTH);
1:2cf1104:       columnDataChunk =
1:daa6465:           new VariableLengthDimensionColumnPage(dataPage, invertedIndexes, invertedIndexesReverse,
1:3a4b881:               pageMetadata.getNumberOfRowsInpage(), dimStoreType,
1:3a4b881:               rawColumnPage.getLocalDictionary());
1:2cf1104:     } else {
2:2cf1104:       // to store fixed length column chunk values
1:2cf1104:       columnDataChunk =
1:daa6465:           new FixedLengthDimensionColumnPage(dataPage, invertedIndexes, invertedIndexesReverse,
1:e6a4f64:               pageMetadata.getNumberOfRowsInpage(),
1:8c1ddbf:               eachColumnValueSize[rawColumnPage.getColumnIndex()]);
1:2cf1104:     }
1:2cf1104:     return columnDataChunk;
1:2cf1104:   }
1:2cf1104: }
============================================================================
author:xuchuanyin
-------------------------------------------------------------------------------
commit:8f08c4a
/////////////////////////////////////////////////////////////////////////
1: import org.apache.carbondata.core.datastore.compression.CompressorFactory;
/////////////////////////////////////////////////////////////////////////
1: import org.apache.carbondata.core.util.CarbonMetadataUtil;
/////////////////////////////////////////////////////////////////////////
1:     String compressorName = CarbonMetadataUtil.getCompressorNameFromChunkMeta(
1:         pageMetadata.getChunk_meta());
1:     this.compressor = CompressorFactory.getInstance().getCompressor(compressorName);
/////////////////////////////////////////////////////////////////////////
1:     String compressorName = CarbonMetadataUtil.getCompressorNameFromChunkMeta(
1:         pageMetadata.getChunk_meta());
1:     ColumnPageDecoder decoder = encodingFactory.createDecoder(encodings, encoderMetas,
1:         compressorName);
/////////////////////////////////////////////////////////////////////////
1:       decodedPage.setNullBits(QueryUtil.getNullBitSet(pageMetadata.presence, this.compressor));
/////////////////////////////////////////////////////////////////////////
1:     dataPage = compressor.unCompressByte(pageData.array(), offset, pageMetadata.data_page_length);
commit:dc53dee
/////////////////////////////////////////////////////////////////////////
1: import org.apache.carbondata.core.datastore.chunk.store.DimensionChunkStoreFactory;
/////////////////////////////////////////////////////////////////////////
1:       DimensionChunkStoreFactory.DimensionStoreType dimStoreType =
0:           hasEncoding(pageMetadata.encoders, Encoding.DIRECT_COMPRESS_VARCHAR) ?
1:               DimensionChunkStoreFactory.DimensionStoreType.VARIABLE_INT_LENGTH :
0:               DimensionChunkStoreFactory.DimensionStoreType.VARIABLE_SHORT_LENGTH;
0:               pageMetadata.getNumberOfRowsInpage(), dimStoreType);
author:akashrn5
-------------------------------------------------------------------------------
commit:2ccdbb7
/////////////////////////////////////////////////////////////////////////
1:     if (CarbonUtil.hasEncoding(pageMetadata.encoders, Encoding.INVERTED_INDEX)) {
1:       invertedIndexesReverse = CarbonUtil.getInvertedReverseIndex(invertedIndexes);
1:     if (CarbonUtil.hasEncoding(pageMetadata.encoders, Encoding.RLE)) {
/////////////////////////////////////////////////////////////////////////
1:     if (!CarbonUtil.hasEncoding(pageMetadata.encoders, Encoding.DICTIONARY)) {
1:               (CarbonUtil.hasEncoding(pageMetadata.encoders, Encoding.DIRECT_COMPRESS_VARCHAR) ?
author:kumarvishal09
-------------------------------------------------------------------------------
commit:43285bb
/////////////////////////////////////////////////////////////////////////
1: import org.apache.carbondata.core.constants.CarbonCommonConstants;
/////////////////////////////////////////////////////////////////////////
1:       ByteBuffer pageData, int offset, boolean isLocalDictEncodedPage)
1:     return decoder
1:         .decode(pageData.array(), offset, pageMetadata.data_page_length, isLocalDictEncodedPage);
/////////////////////////////////////////////////////////////////////////
1:       ColumnPage decodedPage = decodeDimensionByMeta(pageMetadata, pageData, offset,
1:           null != rawColumnPage.getLocalDictionary());
/////////////////////////////////////////////////////////////////////////
1:               CarbonCommonConstants.LOCAL_DICT_ENCODED_BYTEARRAY_SIZE);
commit:3a4b881
/////////////////////////////////////////////////////////////////////////
0:       return new ColumnPageWrapper(decodedPage, rawColumnPage.getLocalDictionary());
/////////////////////////////////////////////////////////////////////////
1:           null == rawColumnPage.getLocalDictionary() ?
1:               eachColumnValueSize[rawColumnPage.getColumnIndex()] :
0:               3);
1:           null != rawColumnPage.getLocalDictionary() ?
1:               DimensionChunkStoreFactory.DimensionStoreType.LOCAL_DICT :
0:               (hasEncoding(pageMetadata.encoders, Encoding.DIRECT_COMPRESS_VARCHAR) ?
0:                   DimensionChunkStoreFactory.DimensionStoreType.VARIABLE_INT_LENGTH :
1:                   DimensionChunkStoreFactory.DimensionStoreType.VARIABLE_SHORT_LENGTH);
1:               pageMetadata.getNumberOfRowsInpage(), dimStoreType,
1:               rawColumnPage.getLocalDictionary());
commit:e710339
/////////////////////////////////////////////////////////////////////////
1:       ByteBuffer pageData, DataChunk2 pageMetadata, int offset) throws IOException,
1:       MemoryException {
author:ajantha-bhat
-------------------------------------------------------------------------------
commit:438b442
/////////////////////////////////////////////////////////////////////////
1: import org.apache.carbondata.core.scan.executor.util.QueryUtil;
/////////////////////////////////////////////////////////////////////////
1:         case ADAPTIVE_INTEGRAL:
1:         case ADAPTIVE_DELTA_INTEGRAL:
1:         case ADAPTIVE_FLOATING:
1:         case ADAPTIVE_DELTA_FLOATING:
/////////////////////////////////////////////////////////////////////////
0:       decodedPage.setNullBits(QueryUtil.getNullBitSet(pageMetadata.presence));
1:       return new ColumnPageWrapper(decodedPage, rawColumnPage.getLocalDictionary(),
1:           isEncodedWithAdaptiveMeta(pageMetadata));
1:   private boolean isEncodedWithAdaptiveMeta(DataChunk2 pageMetadata) {
1:     List<Encoding> encodings = pageMetadata.getEncoders();
1:     if (encodings != null && encodings.size() == 1) {
1:       Encoding encoding = encodings.get(0);
1:       switch (encoding) {
1:         case ADAPTIVE_INTEGRAL:
1:         case ADAPTIVE_DELTA_INTEGRAL:
1:         case ADAPTIVE_FLOATING:
1:         case ADAPTIVE_DELTA_FLOATING:
1:           return true;
1:       }
1:     }
1:     return false;
1:   }
1: 
author:Raghunandan S
-------------------------------------------------------------------------------
commit:7ef9164
/////////////////////////////////////////////////////////////////////////
1:     int[] invertedIndexes = new int[0];
1:     int[] invertedIndexesReverse = new int[0];
author:Jacky Li
-------------------------------------------------------------------------------
commit:daa6465
/////////////////////////////////////////////////////////////////////////
1: import org.apache.carbondata.core.datastore.FileReader;
1: import org.apache.carbondata.core.datastore.chunk.DimensionColumnPage;
1: import org.apache.carbondata.core.datastore.chunk.impl.FixedLengthDimensionColumnPage;
1: import org.apache.carbondata.core.datastore.chunk.impl.VariableLengthDimensionColumnPage;
/////////////////////////////////////////////////////////////////////////
1:    * @param columnIndex blocklet index of the column in carbon data file
1:   public DimensionRawColumnChunk readRawDimensionChunk(FileReader fileReader,
1:       int columnIndex) throws IOException {
1:     long currentDimensionOffset = dimensionChunksOffset.get(columnIndex);
1:     if (dimensionChunksOffset.size() - 1 == columnIndex) {
1:       length = (int) (dimensionChunksOffset.get(columnIndex + 1) - currentDimensionOffset);
/////////////////////////////////////////////////////////////////////////
1:     return getDimensionRawColumnChunk(fileReader, columnIndex, 0, length, buffer,
1:   protected DimensionRawColumnChunk getDimensionRawColumnChunk(FileReader fileReader,
1:       int columnIndex, long offset, int length, ByteBuffer buffer, DataChunk3 dataChunk) {
1:         new DimensionRawColumnChunk(columnIndex, buffer, offset, length, this);
/////////////////////////////////////////////////////////////////////////
1:     rawColumnChunk.setFileReader(fileReader);
/////////////////////////////////////////////////////////////////////////
1:   protected DimensionRawColumnChunk[] readRawDimensionChunksInGroup(FileReader fileReader,
/////////////////////////////////////////////////////////////////////////
1:    * @return DimensionColumnPage
1:   @Override public DimensionColumnPage decodeColumnPage(
/////////////////////////////////////////////////////////////////////////
1:   protected DimensionColumnPage decodeDimension(DimensionRawColumnChunk rawColumnPage,
0:       return new ColumnPageWrapper(decodedPage);
1:   private DimensionColumnPage decodeDimensionLegacy(DimensionRawColumnChunk rawColumnPage,
/////////////////////////////////////////////////////////////////////////
1:     DimensionColumnPage columnDataChunk = null;
1:           new VariableLengthDimensionColumnPage(dataPage, invertedIndexes, invertedIndexesReverse,
1:           new FixedLengthDimensionColumnPage(dataPage, invertedIndexes, invertedIndexesReverse,
commit:8c1ddbf
/////////////////////////////////////////////////////////////////////////
1: import org.apache.carbondata.core.datastore.page.encoding.DefaultEncodingFactory;
1: import org.apache.carbondata.core.datastore.page.encoding.EncodingFactory;
/////////////////////////////////////////////////////////////////////////
1:   private EncodingFactory encodingFactory = DefaultEncodingFactory.getInstance();
/////////////////////////////////////////////////////////////////////////
1:         .get(rawColumnPage.getColumnIndex()) + dataChunk3.getPage_offset().get(pageNumber);
1:   private ColumnPage decodeDimensionByMeta(DataChunk2 pageMetadata,
0:     ColumnPageDecoder decoder = encodingFactory.createDecoder(encodings, encoderMetas);
0:     return decoder.decode(pageData.array(), offset, pageMetadata.data_page_length);
/////////////////////////////////////////////////////////////////////////
0:       ColumnPage decodedPage = decodeDimensionByMeta(pageMetadata, pageData, offset);
0:       return new ColumnPageWrapper(decodedPage,
1:           eachColumnValueSize[rawColumnPage.getColumnIndex()]);
/////////////////////////////////////////////////////////////////////////
0:           eachColumnValueSize[rawColumnPage.getColumnIndex()]);
/////////////////////////////////////////////////////////////////////////
0:               eachColumnValueSize[rawColumnPage.getColumnIndex()]);
commit:e6a4f64
/////////////////////////////////////////////////////////////////////////
1: import java.util.List;
1: import org.apache.carbondata.core.datastore.chunk.store.ColumnPageWrapper;
1: import org.apache.carbondata.core.datastore.page.ColumnPage;
1: import org.apache.carbondata.core.datastore.page.encoding.ColumnPageDecoder;
0: import org.apache.carbondata.core.datastore.page.encoding.EncodingStrategy;
0: import org.apache.carbondata.core.datastore.page.encoding.EncodingStrategyFactory;
1: import org.apache.carbondata.core.memory.MemoryException;
/////////////////////////////////////////////////////////////////////////
0:   private EncodingStrategy strategy = EncodingStrategyFactory.getStrategy();
1: 
/////////////////////////////////////////////////////////////////////////
/////////////////////////////////////////////////////////////////////////
/////////////////////////////////////////////////////////////////////////
1:    * @param rawColumnPage dimension raw chunk
1:       DimensionRawColumnChunk rawColumnPage, int pageNumber) throws IOException, MemoryException {
1:     DataChunk3 dataChunk3 = rawColumnPage.getDataChunkV3();
1:     ByteBuffer rawData = rawColumnPage.getRawData();
1:     DataChunk2 pageMetadata = dataChunk3.getData_chunk_list().get(pageNumber);
0:     int offset = rawColumnPage.getOffSet() + dimensionChunksLength
0:         .get(rawColumnPage.getBlockletId()) + dataChunk3.getPage_offset().get(pageNumber);
1:     return decodeDimension(rawColumnPage, rawData, pageMetadata, offset);
1:   }
1: 
0:   private DimensionColumnDataChunk decodeDimensionByMeta(DataChunk2 pageMetadata,
0:       ByteBuffer pageData, int offset)
1:       throws IOException, MemoryException {
1:     List<Encoding> encodings = pageMetadata.getEncoders();
1:     List<ByteBuffer> encoderMetas = pageMetadata.getEncoder_meta();
0:     ColumnPageDecoder decoder = strategy.createDecoder(encodings, encoderMetas);
0:     ColumnPage decodedPage = decoder.decode(
0:         pageData.array(), offset, pageMetadata.data_page_length);
0:     return new ColumnPageWrapper(decodedPage);
1:   }
1: 
1:   private boolean isEncodedWithMeta(DataChunk2 pageMetadata) {
1:     List<Encoding> encodings = pageMetadata.getEncoders();
1:     if (encodings != null && encodings.size() == 1) {
1:       Encoding encoding = encodings.get(0);
1:       switch (encoding) {
1:         case DIRECT_COMPRESS:
1:         case DIRECT_STRING:
1:           return true;
1:       }
1:     }
1:     return false;
1:   }
1: 
0:   private DimensionColumnDataChunk decodeDimension(DimensionRawColumnChunk rawColumnPage,
1:       ByteBuffer pageData, DataChunk2 pageMetadata, int offset)
1:       throws IOException, MemoryException {
1:     if (isEncodedWithMeta(pageMetadata)) {
0:       return decodeDimensionByMeta(pageMetadata, pageData, offset);
1:     } else {
1:       // following code is for backward compatibility
1:       return decodeDimensionLegacy(rawColumnPage, pageData, pageMetadata, offset);
1:     }
1:   }
1: 
0:   private DimensionColumnDataChunk decodeDimensionLegacy(DimensionRawColumnChunk rawColumnPage,
0:       ByteBuffer pageData, DataChunk2 pageMetadata, int offset) {
1:     byte[] dataPage;
1:     int[] rlePage;
0:     int[] invertedIndexes = null;
0:     int[] invertedIndexesReverse = null;
0:     dataPage = COMPRESSOR.unCompressByte(pageData.array(), offset, pageMetadata.data_page_length);
1:     offset += pageMetadata.data_page_length;
0:     if (hasEncoding(pageMetadata.encoders, Encoding.INVERTED_INDEX)) {
1:           .getUnCompressColumnIndex(pageMetadata.rowid_page_length, pageData, offset);
1:       offset += pageMetadata.rowid_page_length;
0:     if (hasEncoding(pageMetadata.encoders, Encoding.RLE)) {
1:           CarbonUtil.getIntArray(pageData, offset, pageMetadata.rle_page_length);
0:           eachColumnValueSize[rawColumnPage.getBlockletId()]);
1: 
0:     if (!hasEncoding(pageMetadata.encoders, Encoding.DICTIONARY)) {
0:               pageMetadata.getNumberOfRowsInpage());
1:               pageMetadata.getNumberOfRowsInpage(),
0:               eachColumnValueSize[rawColumnPage.getBlockletId()]);
author:ravipesala
-------------------------------------------------------------------------------
commit:d509f17
/////////////////////////////////////////////////////////////////////////
0:     return getDimensionRawColumnChunk(fileReader, blockletColumnIndex, 0, length, buffer,
1:         dataChunk);
1:   }
1: 
0:   protected DimensionRawColumnChunk getDimensionRawColumnChunk(FileHolder fileReader,
0:       int blockletColumnIndex, long offset, int length, ByteBuffer buffer, DataChunk3 dataChunk) {
0:         new DimensionRawColumnChunk(blockletColumnIndex, buffer, offset, length, this);
/////////////////////////////////////////////////////////////////////////
1:       dimensionDataChunks[index] =
1:           getDimensionRawColumnChunk(fileReader, i, runningLength, currentLength, buffer,
1:               dataChunk);
/////////////////////////////////////////////////////////////////////////
1:     int offset = (int) rawColumnPage.getOffSet() + dimensionChunksLength
/////////////////////////////////////////////////////////////////////////
0:   protected DimensionColumnDataChunk decodeDimension(DimensionRawColumnChunk rawColumnPage,
commit:8f59a32
/////////////////////////////////////////////////////////////////////////
1:     ByteBuffer buffer = null;
1:       buffer = fileReader.readByteBuffer(filePath, currentDimensionOffset, length);
/////////////////////////////////////////////////////////////////////////
1:     ByteBuffer buffer = null;
1:       buffer = fileReader.readByteBuffer(filePath, currentDimensionOffset,
/////////////////////////////////////////////////////////////////////////
0:     dataPage = COMPRESSOR
0:         .unCompressByte(rawData.array(), copySourcePoint, dimensionColumnChunk.data_page_length);
author:QiangCai
-------------------------------------------------------------------------------
commit:086b06d
/////////////////////////////////////////////////////////////////////////
1:    * @param pageNumber              number
author:kumarvishal
-------------------------------------------------------------------------------
commit:b41e48f
/////////////////////////////////////////////////////////////////////////
1:  * <FileHeader>
1:  * <File Footer>
commit:2cf1104
/////////////////////////////////////////////////////////////////////////
1: /*
1:  * Licensed to the Apache Software Foundation (ASF) under one or more
1:  * contributor license agreements.  See the NOTICE file distributed with
1:  * this work for additional information regarding copyright ownership.
1:  * The ASF licenses this file to You under the Apache License, Version 2.0
1:  * (the "License"); you may not use this file except in compliance with
1:  * the License.  You may obtain a copy of the License at
1:  *
1:  *    http://www.apache.org/licenses/LICENSE-2.0
1:  *
1:  * Unless required by applicable law or agreed to in writing, software
1:  * distributed under the License is distributed on an "AS IS" BASIS,
1:  * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
1:  * See the License for the specific language governing permissions and
1:  * limitations under the License.
1:  */
1: package org.apache.carbondata.core.datastore.chunk.reader.dimension.v3;
1: 
1: import java.io.IOException;
1: import java.nio.ByteBuffer;
1: 
0: import org.apache.carbondata.core.datastore.FileHolder;
0: import org.apache.carbondata.core.datastore.chunk.DimensionColumnDataChunk;
0: import org.apache.carbondata.core.datastore.chunk.impl.ColumnGroupDimensionDataChunk;
1: import org.apache.carbondata.core.datastore.chunk.impl.DimensionRawColumnChunk;
0: import org.apache.carbondata.core.datastore.chunk.impl.FixedLengthDimensionDataChunk;
0: import org.apache.carbondata.core.datastore.chunk.impl.VariableLengthDimensionDataChunk;
1: import org.apache.carbondata.core.datastore.chunk.reader.dimension.AbstractChunkReaderV2V3Format;
1: import org.apache.carbondata.core.datastore.columnar.UnBlockIndexer;
1: import org.apache.carbondata.core.metadata.blocklet.BlockletInfo;
1: import org.apache.carbondata.core.util.CarbonUtil;
1: import org.apache.carbondata.format.DataChunk2;
1: import org.apache.carbondata.format.DataChunk3;
1: import org.apache.carbondata.format.Encoding;
1: 
1: import org.apache.commons.lang.ArrayUtils;
1: 
1: /**
1:  * Dimension column V3 Reader class which will be used to read and uncompress
1:  * V3 format data
1:  * data format
1:  * Data Format
1:  * <Column1 Data ChunkV3><Column1<Page1><Page2><Page3><Page4>>
1:  * <Column2 Data ChunkV3><Column2<Page1><Page2><Page3><Page4>>
1:  * <Column3 Data ChunkV3><Column3<Page1><Page2><Page3><Page4>>
1:  * <Column4 Data ChunkV3><Column4<Page1><Page2><Page3><Page4>>
1:  */
1: public class CompressedDimensionChunkFileBasedReaderV3 extends AbstractChunkReaderV2V3Format {
1: 
1:   /**
1:    * end position of last dimension in carbon data file
1:    */
1:   private long lastDimensionOffsets;
1: 
1:   public CompressedDimensionChunkFileBasedReaderV3(BlockletInfo blockletInfo,
1:       int[] eachColumnValueSize, String filePath) {
1:     super(blockletInfo, eachColumnValueSize, filePath);
1:     lastDimensionOffsets = blockletInfo.getDimensionOffset();
1:   }
1: 
1:   /**
1:    * Below method will be used to read the dimension column data form carbon data file
1:    * Steps for reading
1:    * 1. Get the length of the data to be read
1:    * 2. Allocate the direct buffer
1:    * 3. read the data from file
1:    * 4. Get the data chunk object from data read
1:    * 5. Create the raw chunk object and fill the details
1:    *
1:    * @param fileReader          reader for reading the column from carbon data file
0:    * @param blockletColumnIndex blocklet index of the column in carbon data file
1:    * @return dimension raw chunk
1:    */
0:   public DimensionRawColumnChunk readRawDimensionChunk(FileHolder fileReader,
0:       int blockletColumnIndex) throws IOException {
1:     // get the current dimension offset
0:     long currentDimensionOffset = dimensionChunksOffset.get(blockletColumnIndex);
1:     int length = 0;
1:     // to calculate the length of the data to be read
1:     // column other than last column we can subtract the offset of current column with
1:     // next column and get the total length.
1:     // but for last column we need to use lastDimensionOffset which is the end position
1:     // of the last dimension, we can subtract current dimension offset from lastDimesionOffset
0:     if (dimensionChunksOffset.size() - 1 == blockletColumnIndex) {
1:       length = (int) (lastDimensionOffsets - currentDimensionOffset);
1:     } else {
0:       length = (int) (dimensionChunksOffset.get(blockletColumnIndex + 1) - currentDimensionOffset);
1:     }
0:     // allocate the buffer
0:     ByteBuffer buffer = ByteBuffer.allocateDirect(length);
1:     // read the data from carbon data file
1:     synchronized (fileReader) {
0:       fileReader.readByteBuffer(filePath, buffer, currentDimensionOffset, length);
1:     }
1:     // get the data chunk which will have all the details about the data pages
1:     DataChunk3 dataChunk = CarbonUtil.readDataChunk3(buffer, 0, length);
1:     // creating a raw chunks instance and filling all the details
1:     DimensionRawColumnChunk rawColumnChunk =
0:         new DimensionRawColumnChunk(blockletColumnIndex, buffer, 0, length, this);
1:     int numberOfPages = dataChunk.getPage_length().size();
1:     byte[][] maxValueOfEachPage = new byte[numberOfPages][];
1:     byte[][] minValueOfEachPage = new byte[numberOfPages][];
1:     int[] eachPageLength = new int[numberOfPages];
1:     for (int i = 0; i < minValueOfEachPage.length; i++) {
1:       maxValueOfEachPage[i] =
1:           dataChunk.getData_chunk_list().get(i).getMin_max().getMax_values().get(0).array();
1:       minValueOfEachPage[i] =
1:           dataChunk.getData_chunk_list().get(i).getMin_max().getMin_values().get(0).array();
1:       eachPageLength[i] = dataChunk.getData_chunk_list().get(i).getNumberOfRowsInpage();
1:     }
1:     rawColumnChunk.setDataChunkV3(dataChunk);
0:     rawColumnChunk.setFileHolder(fileReader);
1:     rawColumnChunk.setPagesCount(dataChunk.getPage_length().size());
1:     rawColumnChunk.setMaxValues(maxValueOfEachPage);
1:     rawColumnChunk.setMinValues(minValueOfEachPage);
1:     rawColumnChunk.setRowCount(eachPageLength);
0:     rawColumnChunk.setLengths(ArrayUtils
0:         .toPrimitive(dataChunk.page_length.toArray(new Integer[dataChunk.page_length.size()])));
1:     rawColumnChunk.setOffsets(ArrayUtils
1:         .toPrimitive(dataChunk.page_offset.toArray(new Integer[dataChunk.page_offset.size()])));
1:     return rawColumnChunk;
1:   }
1: 
1:   /**
1:    * Below method will be used to read the multiple dimension column data in group
1:    * and divide into dimension raw chunk object
1:    * Steps for reading
1:    * 1. Get the length of the data to be read
1:    * 2. Allocate the direct buffer
1:    * 3. read the data from file
1:    * 4. Get the data chunk object from file for each column
1:    * 5. Create the raw chunk object and fill the details for each column
1:    * 6. increment the offset of the data
1:    *
1:    * @param fileReader
1:    *        reader which will be used to read the dimension columns data from file
1:    * @param startBlockletColumnIndex
1:    *        blocklet index of the first dimension column
1:    * @param endBlockletColumnIndex
1:    *        blocklet index of the last dimension column
1:    * @ DimensionRawColumnChunk array
1:    */
0:   protected DimensionRawColumnChunk[] readRawDimensionChunksInGroup(FileHolder fileReader,
1:       int startBlockletColumnIndex, int endBlockletColumnIndex) throws IOException {
1:     // to calculate the length of the data to be read
1:     // column we can subtract the offset of start column offset with
1:     // end column+1 offset and get the total length.
1:     long currentDimensionOffset = dimensionChunksOffset.get(startBlockletColumnIndex);
0:     ByteBuffer buffer = ByteBuffer.allocateDirect(
1:         (int) (dimensionChunksOffset.get(endBlockletColumnIndex + 1) - currentDimensionOffset));
1:     // read the data from carbon data file
1:     synchronized (fileReader) {
0:       fileReader.readByteBuffer(filePath, buffer, currentDimensionOffset,
1:           (int) (dimensionChunksOffset.get(endBlockletColumnIndex + 1) - currentDimensionOffset));
1:     }
1:     // create raw chunk for each dimension column
1:     DimensionRawColumnChunk[] dimensionDataChunks =
1:         new DimensionRawColumnChunk[endBlockletColumnIndex - startBlockletColumnIndex + 1];
1:     int index = 0;
1:     int runningLength = 0;
1:     for (int i = startBlockletColumnIndex; i <= endBlockletColumnIndex; i++) {
1:       int currentLength = (int) (dimensionChunksOffset.get(i + 1) - dimensionChunksOffset.get(i));
0:       dimensionDataChunks[index] =
0:           new DimensionRawColumnChunk(i, buffer, runningLength, currentLength, this);
1:       DataChunk3 dataChunk =
1:           CarbonUtil.readDataChunk3(buffer, runningLength, dimensionChunksLength.get(i));
1:       int numberOfPages = dataChunk.getPage_length().size();
1:       byte[][] maxValueOfEachPage = new byte[numberOfPages][];
1:       byte[][] minValueOfEachPage = new byte[numberOfPages][];
1:       int[] eachPageLength = new int[numberOfPages];
0:       for (int j = 0; j < minValueOfEachPage.length; j++) {
0:         maxValueOfEachPage[j] =
0:             dataChunk.getData_chunk_list().get(j).getMin_max().getMax_values().get(0).array();
0:         minValueOfEachPage[j] =
0:             dataChunk.getData_chunk_list().get(j).getMin_max().getMin_values().get(0).array();
0:         eachPageLength[j] = dataChunk.getData_chunk_list().get(j).getNumberOfRowsInpage();
1:       }
0:       dimensionDataChunks[index].setDataChunkV3(dataChunk);
0:       dimensionDataChunks[index].setFileHolder(fileReader);
0:       dimensionDataChunks[index].setPagesCount(dataChunk.getPage_length().size());
0:       dimensionDataChunks[index].setMaxValues(maxValueOfEachPage);
0:       dimensionDataChunks[index].setMinValues(minValueOfEachPage);
0:       dimensionDataChunks[index].setRowCount(eachPageLength);
0:       dimensionDataChunks[index].setLengths(ArrayUtils
0:           .toPrimitive(dataChunk.page_length.toArray(new Integer[dataChunk.page_length.size()])));
0:       dimensionDataChunks[index].setOffsets(ArrayUtils
1:           .toPrimitive(dataChunk.page_offset.toArray(new Integer[dataChunk.page_offset.size()])));
1:       runningLength += currentLength;
1:       index++;
1:     }
1:     return dimensionDataChunks;
1:   }
1: 
1:   /**
1:    * Below method will be used to convert the compressed dimension chunk raw data to actual data
1:    *
0:    * @param dimensionRawColumnChunk dimension raw chunk
0:    * @param page                    number
0:    * @return DimensionColumnDataChunk
1:    */
0:   @Override public DimensionColumnDataChunk convertToDimensionChunk(
0:       DimensionRawColumnChunk dimensionRawColumnChunk, int pageNumber) throws IOException {
0:     byte[] dataPage = null;
0:     int[] invertedIndexes = null;
0:     int[] invertedIndexesReverse = null;
0:     int[] rlePage = null;
0:     // data chunk of page
0:     DataChunk2 dimensionColumnChunk = null;
1:     // data chunk of blocklet column
0:     DataChunk3 dataChunk3 = dimensionRawColumnChunk.getDataChunkV3();
1:     // get the data buffer
0:     ByteBuffer rawData = dimensionRawColumnChunk.getRawData();
0:     dimensionColumnChunk = dataChunk3.getData_chunk_list().get(pageNumber);
1:     // calculating the start point of data
1:     // as buffer can contain multiple column data, start point will be datachunkoffset +
1:     // data chunk length + page offset
0:     int copySourcePoint = dimensionRawColumnChunk.getOffSet() + dimensionChunksLength
0:         .get(dimensionRawColumnChunk.getBlockletId()) + dataChunk3.getPage_offset().get(pageNumber);
0:     byte[] data = new byte[dimensionColumnChunk.data_page_length];
0:     rawData.position(copySourcePoint);
0:     rawData.get(data);
1:     // first read the data and uncompressed it
0:     dataPage = COMPRESSOR.unCompressByte(data, 0, dimensionColumnChunk.data_page_length);
0:     copySourcePoint += dimensionColumnChunk.data_page_length;
1:     // if row id block is present then read the row id chunk and uncompress it
0:     if (hasEncoding(dimensionColumnChunk.encoders, Encoding.INVERTED_INDEX)) {
1:       invertedIndexes = CarbonUtil
0:           .getUnCompressColumnIndex(dimensionColumnChunk.rowid_page_length, rawData,
0:               copySourcePoint);
0:       copySourcePoint += dimensionColumnChunk.rowid_page_length;
1:       // get the reverse index
0:       invertedIndexesReverse = getInvertedReverseIndex(invertedIndexes);
1:     }
1:     // if rle is applied then read the rle block chunk and then uncompress
1:     //then actual data based on rle block
0:     if (hasEncoding(dimensionColumnChunk.encoders, Encoding.RLE)) {
1:       rlePage =
0:           CarbonUtil.getIntArray(rawData, copySourcePoint, dimensionColumnChunk.rle_page_length);
1:       // uncompress the data with rle indexes
1:       dataPage = UnBlockIndexer.uncompressData(dataPage, rlePage,
0:           eachColumnValueSize[dimensionRawColumnChunk.getBlockletId()]);
0:       rlePage = null;
1:     }
0:     // fill chunk attributes
0:     DimensionColumnDataChunk columnDataChunk = null;
1: 
0:     if (dimensionColumnChunk.isRowMajor()) {
1:       // to store fixed length column chunk values
0:       columnDataChunk = new ColumnGroupDimensionDataChunk(dataPage,
0:           eachColumnValueSize[dimensionRawColumnChunk.getBlockletId()],
0:           dimensionRawColumnChunk.getRowCount()[pageNumber]);
1:     }
1:     // if no dictionary column then first create a no dictionary column chunk
1:     // and set to data chunk instance
0:     else if (!hasEncoding(dimensionColumnChunk.encoders, Encoding.DICTIONARY)) {
1:       columnDataChunk =
0:           new VariableLengthDimensionDataChunk(dataPage, invertedIndexes, invertedIndexesReverse,
0:               dimensionRawColumnChunk.getRowCount()[pageNumber]);
1:     } else {
1:       // to store fixed length column chunk values
1:       columnDataChunk =
0:           new FixedLengthDimensionDataChunk(dataPage, invertedIndexes, invertedIndexesReverse,
0:               dimensionRawColumnChunk.getRowCount()[pageNumber],
0:               eachColumnValueSize[dimensionRawColumnChunk.getBlockletId()]);
1:     }
1:     return columnDataChunk;
1:   }
1: }
============================================================================