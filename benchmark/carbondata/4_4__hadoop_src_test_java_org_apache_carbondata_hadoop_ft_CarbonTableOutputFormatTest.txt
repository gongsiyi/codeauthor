1:91e6f6f: /*
1:91e6f6f:  * Licensed to the Apache Software Foundation (ASF) under one or more
1:91e6f6f:  * contributor license agreements.  See the NOTICE file distributed with
1:91e6f6f:  * this work for additional information regarding copyright ownership.
1:91e6f6f:  * The ASF licenses this file to You under the Apache License, Version 2.0
1:91e6f6f:  * (the "License"); you may not use this file except in compliance with
1:91e6f6f:  * the License.  You may obtain a copy of the License at
1:91e6f6f:  *
1:91e6f6f:  *    http://www.apache.org/licenses/LICENSE-2.0
1:91e6f6f:  *
1:91e6f6f:  * Unless required by applicable law or agreed to in writing, software
1:91e6f6f:  * distributed under the License is distributed on an "AS IS" BASIS,
1:91e6f6f:  * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
1:91e6f6f:  * See the License for the specific language governing permissions and
1:91e6f6f:  * limitations under the License.
1:91e6f6f:  */
1:91e6f6f: package org.apache.carbondata.hadoop.ft;
2:91e6f6f: 
1:91e6f6f: import java.io.File;
1:91e6f6f: import java.io.FilenameFilter;
1:91e6f6f: import java.io.IOException;
1:91e6f6f: 
1:91e6f6f: import org.apache.carbondata.core.constants.CarbonCommonConstants;
1:91e6f6f: import org.apache.carbondata.core.util.CarbonProperties;
1:91e6f6f: import org.apache.carbondata.core.util.CarbonUtil;
1:91e6f6f: import org.apache.carbondata.core.util.path.CarbonTablePath;
1:91e6f6f: import org.apache.carbondata.hadoop.api.CarbonTableOutputFormat;
1:dded5d5: import org.apache.carbondata.hadoop.internal.ObjectArrayWritable;
1:c723947: import org.apache.carbondata.hadoop.testutil.StoreCreator;
1:91e6f6f: import org.apache.carbondata.processing.loading.csvinput.CSVInputFormat;
1:91e6f6f: import org.apache.carbondata.processing.loading.csvinput.StringArrayWritable;
1:91e6f6f: import org.apache.carbondata.processing.loading.model.CarbonLoadModel;
1:91e6f6f: 
1:91e6f6f: import org.apache.hadoop.conf.Configuration;
1:91e6f6f: import org.apache.hadoop.fs.Path;
1:91e6f6f: import org.apache.hadoop.io.NullWritable;
1:91e6f6f: import org.apache.hadoop.mapreduce.Job;
1:91e6f6f: import org.apache.hadoop.mapreduce.Mapper;
1:91e6f6f: import org.apache.hadoop.mapreduce.lib.input.FileInputFormat;
1:91e6f6f: import org.apache.hadoop.mapreduce.lib.output.FileOutputFormat;
1:6e77f2b: import org.junit.After;
1:6e77f2b: import org.junit.Assert;
1:6e77f2b: import org.junit.Before;
1:91e6f6f: import org.junit.Test;
1:91e6f6f: 
1:6e77f2b: public class CarbonTableOutputFormatTest {
1:91e6f6f: 
1:6e77f2b:   static CarbonLoadModel carbonLoadModel;
1:91e6f6f: 
1:91e6f6f:   // changed setUp to static init block to avoid un wanted multiple time store creation
1:91e6f6f:   static {
1:91e6f6f:     CarbonProperties.getInstance().
1:91e6f6f:         addProperty(CarbonCommonConstants.CARBON_BADRECORDS_LOC, "/tmp/carbon/badrecords");
1:bea277f:     CarbonProperties.getInstance()
1:bea277f:         .addProperty(CarbonCommonConstants.CARBON_SYSTEM_FOLDER_LOCATION, "/tmp/carbon/");
1:6e77f2b:     try {
1:3894e1d:       carbonLoadModel = new StoreCreator(new File("target/store").getAbsolutePath(),
1:3894e1d:           new File("../hadoop/src/test/resources/data.csv").getCanonicalPath()).createTableAndLoadModel();
1:6e77f2b:     } catch (Exception e) {
1:6e77f2b:       Assert.fail("create table failed: " + e.getMessage());
1:91e6f6f:     }
1:6e77f2b:   }
1:91e6f6f: 
1:91e6f6f: 
1:91e6f6f:   @Test public void testOutputFormat() throws Exception {
1:91e6f6f:     runJob("");
1:91e6f6f:     String segmentPath = CarbonTablePath.getSegmentPath(carbonLoadModel.getTablePath(), "0");
1:91e6f6f:     File file = new File(segmentPath);
1:29be1d0:     Assert.assertTrue(file.exists());
1:91e6f6f:     File[] listFiles = file.listFiles(new FilenameFilter() {
1:91e6f6f:       @Override public boolean accept(File dir, String name) {
1:3ff55a2:         return name.endsWith(".carbondata") ||
1:3ff55a2:             name.endsWith(".carbonindex") ||
1:3ff55a2:             name.endsWith(".carbonindexmerge");
1:91e6f6f:       }
1:91e6f6f:     });
1:91e6f6f: 
1:29be1d0:     Assert.assertTrue(listFiles.length == 2);
1:91e6f6f:   }
1:91e6f6f: 
1:6e77f2b:   @After
1:6e77f2b:   public void tearDown() throws Exception {
1:91e6f6f:     CarbonProperties.getInstance()
1:91e6f6f:         .addProperty(CarbonCommonConstants.ENABLE_QUERY_STATISTICS, "true");
1:91e6f6f:   }
1:91e6f6f: 
1:6e77f2b:   @Before
1:6e77f2b:   public void setUp() throws Exception {
1:91e6f6f:     CarbonProperties.getInstance()
1:91e6f6f:         .addProperty(CarbonCommonConstants.ENABLE_QUERY_STATISTICS, "false");
1:91e6f6f:   }
1:91e6f6f: 
1:dded5d5:  public static class Map extends Mapper<NullWritable, StringArrayWritable, NullWritable, ObjectArrayWritable> {
1:91e6f6f: 
1:dded5d5:    private ObjectArrayWritable writable = new ObjectArrayWritable();
1:91e6f6f:    @Override protected void map(NullWritable key, StringArrayWritable value, Context context)
1:91e6f6f:        throws IOException, InterruptedException {
1:dded5d5:      writable.set(value.get());
1:dded5d5:      context.write(key, writable);
1:91e6f6f:    }
1:91e6f6f:  }
1:91e6f6f: 
1:91e6f6f:   private void runJob(String outPath) throws Exception {
1:91e6f6f:     Configuration configuration = new Configuration();
1:75126c6:     String mrLocalDir = new File(outPath + "1").getCanonicalPath();
1:75126c6:     configuration.set("mapreduce.cluster.local.dir", mrLocalDir);
1:91e6f6f:     Job job = Job.getInstance(configuration);
1:6e77f2b:     job.setJarByClass(CarbonTableOutputFormatTest.class);
1:91e6f6f:     job.setOutputKeyClass(NullWritable.class);
1:dded5d5:     job.setOutputValueClass(ObjectArrayWritable.class);
1:91e6f6f:     job.setMapperClass(Map.class);
1:91e6f6f:     job.setNumReduceTasks(0);
1:91e6f6f: 
1:91e6f6f:     FileInputFormat.addInputPath(job, new Path(carbonLoadModel.getFactFilePath()));
1:91e6f6f:     CarbonTableOutputFormat.setLoadModel(job.getConfiguration(), carbonLoadModel);
1:91e6f6f:     CarbonTableOutputFormat.setCarbonTable(job.getConfiguration(), carbonLoadModel.getCarbonDataLoadSchema().getCarbonTable());
1:91e6f6f:     CSVInputFormat.setHeaderExtractionEnabled(job.getConfiguration(), true);
1:91e6f6f:     job.setInputFormatClass(CSVInputFormat.class);
1:91e6f6f:     job.setOutputFormatClass(CarbonTableOutputFormat.class);
1:91e6f6f:     CarbonUtil.deleteFoldersAndFiles(new File(carbonLoadModel.getTablePath() + "1"));
1:91e6f6f:     FileOutputFormat.setOutputPath(job, new Path(carbonLoadModel.getTablePath() + "1"));
1:91e6f6f:     job.getConfiguration().set("outpath", outPath);
1:91e6f6f:     job.getConfiguration().set("query.id", String.valueOf(System.nanoTime()));
1:91e6f6f:     job.waitForCompletion(true);
1:91e6f6f: 
1:75126c6:     CarbonUtil.deleteFoldersAndFiles(new File(mrLocalDir));
1:91e6f6f:   }
1:91e6f6f: 
1:91e6f6f: }
============================================================================
author:ravipesala
-------------------------------------------------------------------------------
commit:3894e1d
/////////////////////////////////////////////////////////////////////////
1:       carbonLoadModel = new StoreCreator(new File("target/store").getAbsolutePath(),
1:           new File("../hadoop/src/test/resources/data.csv").getCanonicalPath()).createTableAndLoadModel();
commit:dded5d5
/////////////////////////////////////////////////////////////////////////
1: import org.apache.carbondata.hadoop.internal.ObjectArrayWritable;
/////////////////////////////////////////////////////////////////////////
1:  public static class Map extends Mapper<NullWritable, StringArrayWritable, NullWritable, ObjectArrayWritable> {
1:    private ObjectArrayWritable writable = new ObjectArrayWritable();
1:      writable.set(value.get());
1:      context.write(key, writable);
/////////////////////////////////////////////////////////////////////////
1:     job.setOutputValueClass(ObjectArrayWritable.class);
commit:3ff55a2
/////////////////////////////////////////////////////////////////////////
1:         return name.endsWith(".carbondata") ||
1:             name.endsWith(".carbonindex") ||
1:             name.endsWith(".carbonindexmerge");
commit:91e6f6f
/////////////////////////////////////////////////////////////////////////
1: /*
1:  * Licensed to the Apache Software Foundation (ASF) under one or more
1:  * contributor license agreements.  See the NOTICE file distributed with
1:  * this work for additional information regarding copyright ownership.
1:  * The ASF licenses this file to You under the Apache License, Version 2.0
1:  * (the "License"); you may not use this file except in compliance with
1:  * the License.  You may obtain a copy of the License at
1:  *
1:  *    http://www.apache.org/licenses/LICENSE-2.0
1:  *
1:  * Unless required by applicable law or agreed to in writing, software
1:  * distributed under the License is distributed on an "AS IS" BASIS,
1:  * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
1:  * See the License for the specific language governing permissions and
1:  * limitations under the License.
1:  */
1: package org.apache.carbondata.hadoop.ft;
1: 
1: import java.io.File;
1: import java.io.FilenameFilter;
1: import java.io.IOException;
1: 
1: import org.apache.carbondata.core.constants.CarbonCommonConstants;
1: import org.apache.carbondata.core.util.CarbonProperties;
1: import org.apache.carbondata.core.util.CarbonUtil;
1: import org.apache.carbondata.core.util.path.CarbonTablePath;
1: import org.apache.carbondata.hadoop.api.CarbonTableOutputFormat;
0: import org.apache.carbondata.hadoop.test.util.StoreCreator;
1: import org.apache.carbondata.processing.loading.csvinput.CSVInputFormat;
1: import org.apache.carbondata.processing.loading.csvinput.StringArrayWritable;
1: import org.apache.carbondata.processing.loading.model.CarbonLoadModel;
1: 
0: import junit.framework.TestCase;
1: import org.apache.hadoop.conf.Configuration;
1: import org.apache.hadoop.fs.Path;
1: import org.apache.hadoop.io.NullWritable;
1: import org.apache.hadoop.mapreduce.Job;
1: import org.apache.hadoop.mapreduce.Mapper;
1: import org.apache.hadoop.mapreduce.lib.input.FileInputFormat;
1: import org.apache.hadoop.mapreduce.lib.output.FileOutputFormat;
1: import org.junit.Test;
1: 
0: public class CarbonOutputMapperTest extends TestCase {
1: 
0:   CarbonLoadModel carbonLoadModel;
1: 
1:   // changed setUp to static init block to avoid un wanted multiple time store creation
1:   static {
1:     CarbonProperties.getInstance().
1:         addProperty(CarbonCommonConstants.CARBON_BADRECORDS_LOC, "/tmp/carbon/badrecords");
1:   }
1: 
1: 
1:   @Test public void testOutputFormat() throws Exception {
1:     runJob("");
1:     String segmentPath = CarbonTablePath.getSegmentPath(carbonLoadModel.getTablePath(), "0");
1:     File file = new File(segmentPath);
0:     assert (file.exists());
1:     File[] listFiles = file.listFiles(new FilenameFilter() {
1:       @Override public boolean accept(File dir, String name) {
0:         return name.endsWith(".carbondata") || name.endsWith(".carbonindex");
1:       }
1:     });
1: 
0:     assert (listFiles.length == 2);
1: 
1:   }
1: 
1: 
0:   @Override public void tearDown() throws Exception {
0:     super.tearDown();
1:     CarbonProperties.getInstance()
1:         .addProperty(CarbonCommonConstants.ENABLE_QUERY_STATISTICS, "true");
1:   }
1: 
0:   @Override public void setUp() throws Exception {
0:     super.setUp();
1:     CarbonProperties.getInstance()
1:         .addProperty(CarbonCommonConstants.ENABLE_QUERY_STATISTICS, "false");
0:     carbonLoadModel = StoreCreator.getCarbonLoadModel();
1:   }
1: 
0:  public static class Map extends Mapper<NullWritable, StringArrayWritable, NullWritable, StringArrayWritable> {
1: 
1:    @Override protected void map(NullWritable key, StringArrayWritable value, Context context)
1:        throws IOException, InterruptedException {
0:      context.write(key, value);
1:    }
1:  }
1: 
1:   private void runJob(String outPath) throws Exception {
1:     Configuration configuration = new Configuration();
0:     configuration.set("mapreduce.cluster.local.dir", new File(outPath + "1").getCanonicalPath());
1:     Job job = Job.getInstance(configuration);
0:     job.setJarByClass(CarbonOutputMapperTest.class);
1:     job.setOutputKeyClass(NullWritable.class);
0:     job.setOutputValueClass(StringArrayWritable.class);
1:     job.setMapperClass(Map.class);
1:     job.setNumReduceTasks(0);
1: 
1:     FileInputFormat.addInputPath(job, new Path(carbonLoadModel.getFactFilePath()));
1:     CarbonTableOutputFormat.setLoadModel(job.getConfiguration(), carbonLoadModel);
1:     CarbonTableOutputFormat.setCarbonTable(job.getConfiguration(), carbonLoadModel.getCarbonDataLoadSchema().getCarbonTable());
1:     CSVInputFormat.setHeaderExtractionEnabled(job.getConfiguration(), true);
1:     job.setInputFormatClass(CSVInputFormat.class);
1:     job.setOutputFormatClass(CarbonTableOutputFormat.class);
1:     CarbonUtil.deleteFoldersAndFiles(new File(carbonLoadModel.getTablePath() + "1"));
1:     FileOutputFormat.setOutputPath(job, new Path(carbonLoadModel.getTablePath() + "1"));
1:     job.getConfiguration().set("outpath", outPath);
1:     job.getConfiguration().set("query.id", String.valueOf(System.nanoTime()));
1:     job.waitForCompletion(true);
1:   }
1: 
1: }
author:mohammadshahidkhan
-------------------------------------------------------------------------------
commit:bea277f
/////////////////////////////////////////////////////////////////////////
1:     CarbonProperties.getInstance()
1:         .addProperty(CarbonCommonConstants.CARBON_SYSTEM_FOLDER_LOCATION, "/tmp/carbon/");
author:xuchuanyin
-------------------------------------------------------------------------------
commit:75126c6
/////////////////////////////////////////////////////////////////////////
1:     String mrLocalDir = new File(outPath + "1").getCanonicalPath();
1:     configuration.set("mapreduce.cluster.local.dir", mrLocalDir);
/////////////////////////////////////////////////////////////////////////
0: 
1:     CarbonUtil.deleteFoldersAndFiles(new File(mrLocalDir));
commit:29be1d0
/////////////////////////////////////////////////////////////////////////
1:     Assert.assertTrue(file.exists());
/////////////////////////////////////////////////////////////////////////
1:     Assert.assertTrue(listFiles.length == 2);
author:Jacky Li
-------------------------------------------------------------------------------
commit:c723947
/////////////////////////////////////////////////////////////////////////
1: import org.apache.carbondata.hadoop.testutil.StoreCreator;
commit:6e77f2b
/////////////////////////////////////////////////////////////////////////
/////////////////////////////////////////////////////////////////////////
1: import org.junit.After;
1: import org.junit.Assert;
1: import org.junit.Before;
1: public class CarbonTableOutputFormatTest {
1:   static CarbonLoadModel carbonLoadModel;
1:     try {
0:       carbonLoadModel = StoreCreator.createTableAndLoadModel();
1:     } catch (Exception e) {
1:       Assert.fail("create table failed: " + e.getMessage());
1:     }
/////////////////////////////////////////////////////////////////////////
1:   @After
1:   public void tearDown() throws Exception {
1:   @Before
1:   public void setUp() throws Exception {
/////////////////////////////////////////////////////////////////////////
1:     job.setJarByClass(CarbonTableOutputFormatTest.class);
============================================================================