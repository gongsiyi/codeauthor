1:d509f17: /*
1:d509f17:  * Licensed to the Apache Software Foundation (ASF) under one or more
1:d509f17:  * contributor license agreements.  See the NOTICE file distributed with
1:d509f17:  * this work for additional information regarding copyright ownership.
1:d509f17:  * The ASF licenses this file to You under the Apache License, Version 2.0
1:d509f17:  * (the "License"); you may not use this file except in compliance with
1:d509f17:  * the License.  You may obtain a copy of the License at
1:d509f17:  *
1:d509f17:  *    http://www.apache.org/licenses/LICENSE-2.0
1:d509f17:  *
1:d509f17:  * Unless required by applicable law or agreed to in writing, software
1:d509f17:  * distributed under the License is distributed on an "AS IS" BASIS,
1:d509f17:  * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
1:d509f17:  * See the License for the specific language governing permissions and
1:d509f17:  * limitations under the License.
1:d509f17:  */
1:d509f17: package org.apache.carbondata.core.datastore.chunk.reader.dimension.v3;
1:d509f17: 
1:d509f17: import java.io.ByteArrayInputStream;
1:d509f17: import java.io.IOException;
1:d509f17: import java.nio.ByteBuffer;
1:d509f17: 
1:daa6465: import org.apache.carbondata.core.datastore.FileReader;
1:daa6465: import org.apache.carbondata.core.datastore.chunk.DimensionColumnPage;
1:d509f17: import org.apache.carbondata.core.datastore.chunk.impl.DimensionRawColumnChunk;
1:d509f17: import org.apache.carbondata.core.memory.MemoryException;
1:d509f17: import org.apache.carbondata.core.metadata.blocklet.BlockletInfo;
1:d509f17: import org.apache.carbondata.core.util.CarbonUtil;
1:d509f17: import org.apache.carbondata.format.DataChunk2;
1:d509f17: import org.apache.carbondata.format.DataChunk3;
1:d509f17: import org.apache.carbondata.format.Encoding;
1:d509f17: 
1:d509f17: /**
1:d509f17:  * Dimension column V3 Reader class which will be used to read and uncompress
1:d509f17:  * V3 format data. It reads the data in each page at once unlike whole blocklet. It is
1:d509f17:  * used for memory constraint operations like compaction.
1:d509f17:  * data format
1:d509f17:  * Data Format
1:d509f17:  * <FileHeader>
1:d509f17:  * <Column1 Data ChunkV3><Column1<Page1><Page2><Page3><Page4>>
1:d509f17:  * <Column2 Data ChunkV3><Column2<Page1><Page2><Page3><Page4>>
1:d509f17:  * <Column3 Data ChunkV3><Column3<Page1><Page2><Page3><Page4>>
1:d509f17:  * <Column4 Data ChunkV3><Column4<Page1><Page2><Page3><Page4>>
1:d509f17:  * <File Footer>
1:d509f17:  */
1:d509f17: public class CompressedDimChunkFileBasedPageLevelReaderV3
1:d509f17:     extends CompressedDimensionChunkFileBasedReaderV3 {
1:d509f17: 
1:d509f17:   /**
1:d509f17:    * end position of last dimension in carbon data file
1:d509f17:    */
1:d509f17:   private long lastDimensionOffsets;
1:d509f17: 
1:d509f17:   public CompressedDimChunkFileBasedPageLevelReaderV3(BlockletInfo blockletInfo,
1:d509f17:       int[] eachColumnValueSize, String filePath) {
1:d509f17:     super(blockletInfo, eachColumnValueSize, filePath);
1:d509f17:     lastDimensionOffsets = blockletInfo.getDimensionOffset();
1:d509f17:   }
1:d509f17: 
1:d509f17:   /**
1:d509f17:    * Below method will be used to read the dimension column data form carbon data file
1:d509f17:    * Steps for reading
1:d509f17:    * 1. Get the length of the data to be read
1:d509f17:    * 2. Allocate the direct buffer
1:d509f17:    * 3. read the data from file
1:d509f17:    * 4. Get the data chunk object from data read
1:d509f17:    * 5. Create the raw chunk object and fill the details
1:d509f17:    *
1:d509f17:    * @param fileReader          reader for reading the column from carbon data file
1:d509f17:    * @param blockletColumnIndex blocklet index of the column in carbon data file
1:d509f17:    * @return dimension raw chunk
1:d509f17:    */
1:daa6465:   @Override
1:daa6465:   public DimensionRawColumnChunk readRawDimensionChunk(FileReader fileReader,
1:d509f17:       int blockletColumnIndex) throws IOException {
1:d509f17:     // get the current dimension offset
1:d509f17:     long currentDimensionOffset = dimensionChunksOffset.get(blockletColumnIndex);
1:d509f17:     int length = 0;
1:d509f17:     // to calculate the length of the data to be read
1:d509f17:     // column other than last column we can subtract the offset of current column with
1:d509f17:     // next column and get the total length.
1:d509f17:     // but for last column we need to use lastDimensionOffset which is the end position
1:e8da880:     // of the last dimension, we can subtract current dimension offset from lastDimensionOffset
1:d509f17:     if (dimensionChunksOffset.size() - 1 == blockletColumnIndex) {
1:d509f17:       length = (int) (lastDimensionOffsets - currentDimensionOffset);
1:d509f17:     } else {
1:d509f17:       length = (int) (dimensionChunksOffset.get(blockletColumnIndex + 1) - currentDimensionOffset);
1:d509f17:     }
1:d509f17:     ByteBuffer buffer;
1:d509f17:     // read the data from carbon data file
1:d509f17:     synchronized (fileReader) {
1:d509f17:       buffer = fileReader.readByteBuffer(filePath, currentDimensionOffset,
1:d509f17:           dimensionChunksLength.get(blockletColumnIndex));
1:d509f17:     }
1:d509f17:     // get the data chunk which will have all the details about the data pages
1:d509f17:     DataChunk3 dataChunk = CarbonUtil.readDataChunk3(new ByteArrayInputStream(buffer.array()));
1:d509f17:     DimensionRawColumnChunk rawColumnChunk =
1:d509f17:         getDimensionRawColumnChunk(fileReader, blockletColumnIndex, currentDimensionOffset, length,
1:d509f17:             null, dataChunk);
1:d509f17: 
1:d509f17:     return rawColumnChunk;
1:d509f17:   }
1:d509f17: 
1:d509f17:   /**
1:d509f17:    * Below method will be used to read the multiple dimension column data in group
1:d509f17:    * and divide into dimension raw chunk object
1:d509f17:    * Steps for reading
1:d509f17:    * 1. Get the length of the data to be read
1:d509f17:    * 2. Allocate the direct buffer
1:d509f17:    * 3. read the data from file
1:d509f17:    * 4. Get the data chunk object from file for each column
1:d509f17:    * 5. Create the raw chunk object and fill the details for each column
1:d509f17:    * 6. increment the offset of the data
1:d509f17:    *
1:d509f17:    * @param fileReader      reader which will be used to read the dimension columns data from file
1:d509f17:    * @param startBlockletColumnIndex blocklet index of the first dimension column
1:d509f17:    * @param endBlockletColumnIndex   blocklet index of the last dimension column
1:d509f17:    * @ DimensionRawColumnChunk array
1:d509f17:    */
1:daa6465:   protected DimensionRawColumnChunk[] readRawDimensionChunksInGroup(FileReader fileReader,
1:d509f17:       int startBlockletColumnIndex, int endBlockletColumnIndex) throws IOException {
1:d509f17:     // create raw chunk for each dimension column
1:d509f17:     DimensionRawColumnChunk[] dimensionDataChunks =
1:d509f17:         new DimensionRawColumnChunk[endBlockletColumnIndex - startBlockletColumnIndex + 1];
1:d509f17:     int index = 0;
1:d509f17:     for (int i = startBlockletColumnIndex; i <= endBlockletColumnIndex; i++) {
1:d509f17:       dimensionDataChunks[index] = readRawDimensionChunk(fileReader, i);
1:d509f17:       index++;
1:d509f17:     }
1:d509f17:     return dimensionDataChunks;
1:d509f17:   }
1:d509f17: 
1:d509f17:   /**
1:d509f17:    * Below method will be used to convert the compressed dimension chunk raw data to actual data
1:d509f17:    *
1:d509f17:    * @param dimensionRawColumnChunk dimension raw chunk
1:d509f17:    * @param pageNumber              number
1:d509f17:    * @return DimensionColumnDataChunk
1:d509f17:    */
1:daa6465:   @Override public DimensionColumnPage decodeColumnPage(
1:d509f17:       DimensionRawColumnChunk dimensionRawColumnChunk, int pageNumber)
1:d509f17:       throws IOException, MemoryException {
1:d509f17:     // data chunk of page
1:d509f17:     DataChunk2 pageMetadata = null;
1:d509f17:     // data chunk of blocklet column
1:d509f17:     DataChunk3 dataChunk3 = dimensionRawColumnChunk.getDataChunkV3();
1:d509f17: 
1:d509f17:     pageMetadata = dataChunk3.getData_chunk_list().get(pageNumber);
1:d509f17:     // calculating the start point of data
1:d509f17:     // as buffer can contain multiple column data, start point will be datachunkoffset +
1:d509f17:     // data chunk length + page offset
1:d509f17:     long offset = dimensionRawColumnChunk.getOffSet() + dimensionChunksLength
1:d509f17:         .get(dimensionRawColumnChunk.getColumnIndex()) + dataChunk3.getPage_offset()
1:d509f17:         .get(pageNumber);
1:d509f17:     int length = pageMetadata.data_page_length;
1:2ccdbb7:     if (CarbonUtil.hasEncoding(pageMetadata.encoders, Encoding.INVERTED_INDEX)) {
1:d509f17:       length += pageMetadata.rowid_page_length;
1:d509f17:     }
1:d509f17: 
1:2ccdbb7:     if (CarbonUtil.hasEncoding(pageMetadata.encoders, Encoding.RLE)) {
1:d509f17:       length += pageMetadata.rle_page_length;
1:d509f17:     }
1:d509f17:     // get the data buffer
1:d509f17:     ByteBuffer rawData = dimensionRawColumnChunk.getFileReader()
1:d509f17:         .readByteBuffer(filePath, offset, length);
1:d509f17: 
1:d509f17:     return decodeDimension(dimensionRawColumnChunk, rawData, pageMetadata, 0);
1:d509f17:   }
1:d509f17: }
============================================================================
author:akashrn5
-------------------------------------------------------------------------------
commit:2ccdbb7
/////////////////////////////////////////////////////////////////////////
1:     if (CarbonUtil.hasEncoding(pageMetadata.encoders, Encoding.INVERTED_INDEX)) {
1:     if (CarbonUtil.hasEncoding(pageMetadata.encoders, Encoding.RLE)) {
author:xubo245
-------------------------------------------------------------------------------
commit:e8da880
/////////////////////////////////////////////////////////////////////////
1:     // of the last dimension, we can subtract current dimension offset from lastDimensionOffset
author:Jacky Li
-------------------------------------------------------------------------------
commit:daa6465
/////////////////////////////////////////////////////////////////////////
1: import org.apache.carbondata.core.datastore.FileReader;
1: import org.apache.carbondata.core.datastore.chunk.DimensionColumnPage;
/////////////////////////////////////////////////////////////////////////
1:   @Override
1:   public DimensionRawColumnChunk readRawDimensionChunk(FileReader fileReader,
/////////////////////////////////////////////////////////////////////////
1:   protected DimensionRawColumnChunk[] readRawDimensionChunksInGroup(FileReader fileReader,
/////////////////////////////////////////////////////////////////////////
1:   @Override public DimensionColumnPage decodeColumnPage(
author:ravipesala
-------------------------------------------------------------------------------
commit:d509f17
/////////////////////////////////////////////////////////////////////////
1: /*
1:  * Licensed to the Apache Software Foundation (ASF) under one or more
1:  * contributor license agreements.  See the NOTICE file distributed with
1:  * this work for additional information regarding copyright ownership.
1:  * The ASF licenses this file to You under the Apache License, Version 2.0
1:  * (the "License"); you may not use this file except in compliance with
1:  * the License.  You may obtain a copy of the License at
1:  *
1:  *    http://www.apache.org/licenses/LICENSE-2.0
1:  *
1:  * Unless required by applicable law or agreed to in writing, software
1:  * distributed under the License is distributed on an "AS IS" BASIS,
1:  * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
1:  * See the License for the specific language governing permissions and
1:  * limitations under the License.
1:  */
1: package org.apache.carbondata.core.datastore.chunk.reader.dimension.v3;
1: 
1: import java.io.ByteArrayInputStream;
1: import java.io.IOException;
1: import java.nio.ByteBuffer;
1: 
0: import org.apache.carbondata.core.datastore.FileHolder;
0: import org.apache.carbondata.core.datastore.chunk.DimensionColumnDataChunk;
1: import org.apache.carbondata.core.datastore.chunk.impl.DimensionRawColumnChunk;
1: import org.apache.carbondata.core.memory.MemoryException;
1: import org.apache.carbondata.core.metadata.blocklet.BlockletInfo;
1: import org.apache.carbondata.core.util.CarbonUtil;
1: import org.apache.carbondata.format.DataChunk2;
1: import org.apache.carbondata.format.DataChunk3;
1: import org.apache.carbondata.format.Encoding;
1: 
1: /**
1:  * Dimension column V3 Reader class which will be used to read and uncompress
1:  * V3 format data. It reads the data in each page at once unlike whole blocklet. It is
1:  * used for memory constraint operations like compaction.
1:  * data format
1:  * Data Format
1:  * <FileHeader>
1:  * <Column1 Data ChunkV3><Column1<Page1><Page2><Page3><Page4>>
1:  * <Column2 Data ChunkV3><Column2<Page1><Page2><Page3><Page4>>
1:  * <Column3 Data ChunkV3><Column3<Page1><Page2><Page3><Page4>>
1:  * <Column4 Data ChunkV3><Column4<Page1><Page2><Page3><Page4>>
1:  * <File Footer>
1:  */
1: public class CompressedDimChunkFileBasedPageLevelReaderV3
1:     extends CompressedDimensionChunkFileBasedReaderV3 {
1: 
1:   /**
1:    * end position of last dimension in carbon data file
1:    */
1:   private long lastDimensionOffsets;
1: 
1:   public CompressedDimChunkFileBasedPageLevelReaderV3(BlockletInfo blockletInfo,
1:       int[] eachColumnValueSize, String filePath) {
1:     super(blockletInfo, eachColumnValueSize, filePath);
1:     lastDimensionOffsets = blockletInfo.getDimensionOffset();
1:   }
1: 
1:   /**
1:    * Below method will be used to read the dimension column data form carbon data file
1:    * Steps for reading
1:    * 1. Get the length of the data to be read
1:    * 2. Allocate the direct buffer
1:    * 3. read the data from file
1:    * 4. Get the data chunk object from data read
1:    * 5. Create the raw chunk object and fill the details
1:    *
1:    * @param fileReader          reader for reading the column from carbon data file
1:    * @param blockletColumnIndex blocklet index of the column in carbon data file
1:    * @return dimension raw chunk
1:    */
0:   public DimensionRawColumnChunk readRawDimensionChunk(FileHolder fileReader,
1:       int blockletColumnIndex) throws IOException {
1:     // get the current dimension offset
1:     long currentDimensionOffset = dimensionChunksOffset.get(blockletColumnIndex);
1:     int length = 0;
1:     // to calculate the length of the data to be read
1:     // column other than last column we can subtract the offset of current column with
1:     // next column and get the total length.
1:     // but for last column we need to use lastDimensionOffset which is the end position
0:     // of the last dimension, we can subtract current dimension offset from lastDimesionOffset
1:     if (dimensionChunksOffset.size() - 1 == blockletColumnIndex) {
1:       length = (int) (lastDimensionOffsets - currentDimensionOffset);
1:     } else {
1:       length = (int) (dimensionChunksOffset.get(blockletColumnIndex + 1) - currentDimensionOffset);
1:     }
1:     ByteBuffer buffer;
1:     // read the data from carbon data file
1:     synchronized (fileReader) {
1:       buffer = fileReader.readByteBuffer(filePath, currentDimensionOffset,
1:           dimensionChunksLength.get(blockletColumnIndex));
1:     }
1:     // get the data chunk which will have all the details about the data pages
1:     DataChunk3 dataChunk = CarbonUtil.readDataChunk3(new ByteArrayInputStream(buffer.array()));
1:     DimensionRawColumnChunk rawColumnChunk =
1:         getDimensionRawColumnChunk(fileReader, blockletColumnIndex, currentDimensionOffset, length,
1:             null, dataChunk);
1: 
1:     return rawColumnChunk;
1:   }
1: 
1:   /**
1:    * Below method will be used to read the multiple dimension column data in group
1:    * and divide into dimension raw chunk object
1:    * Steps for reading
1:    * 1. Get the length of the data to be read
1:    * 2. Allocate the direct buffer
1:    * 3. read the data from file
1:    * 4. Get the data chunk object from file for each column
1:    * 5. Create the raw chunk object and fill the details for each column
1:    * 6. increment the offset of the data
1:    *
1:    * @param fileReader      reader which will be used to read the dimension columns data from file
1:    * @param startBlockletColumnIndex blocklet index of the first dimension column
1:    * @param endBlockletColumnIndex   blocklet index of the last dimension column
1:    * @ DimensionRawColumnChunk array
1:    */
0:   protected DimensionRawColumnChunk[] readRawDimensionChunksInGroup(FileHolder fileReader,
1:       int startBlockletColumnIndex, int endBlockletColumnIndex) throws IOException {
1:     // create raw chunk for each dimension column
1:     DimensionRawColumnChunk[] dimensionDataChunks =
1:         new DimensionRawColumnChunk[endBlockletColumnIndex - startBlockletColumnIndex + 1];
1:     int index = 0;
1:     for (int i = startBlockletColumnIndex; i <= endBlockletColumnIndex; i++) {
1:       dimensionDataChunks[index] = readRawDimensionChunk(fileReader, i);
1:       index++;
1:     }
1:     return dimensionDataChunks;
1:   }
1: 
1:   /**
1:    * Below method will be used to convert the compressed dimension chunk raw data to actual data
1:    *
1:    * @param dimensionRawColumnChunk dimension raw chunk
1:    * @param pageNumber              number
1:    * @return DimensionColumnDataChunk
1:    */
0:   @Override public DimensionColumnDataChunk convertToDimensionChunk(
1:       DimensionRawColumnChunk dimensionRawColumnChunk, int pageNumber)
1:       throws IOException, MemoryException {
1:     // data chunk of page
1:     DataChunk2 pageMetadata = null;
1:     // data chunk of blocklet column
1:     DataChunk3 dataChunk3 = dimensionRawColumnChunk.getDataChunkV3();
1: 
1:     pageMetadata = dataChunk3.getData_chunk_list().get(pageNumber);
1:     // calculating the start point of data
1:     // as buffer can contain multiple column data, start point will be datachunkoffset +
1:     // data chunk length + page offset
1:     long offset = dimensionRawColumnChunk.getOffSet() + dimensionChunksLength
1:         .get(dimensionRawColumnChunk.getColumnIndex()) + dataChunk3.getPage_offset()
1:         .get(pageNumber);
1:     int length = pageMetadata.data_page_length;
0:     if (hasEncoding(pageMetadata.encoders, Encoding.INVERTED_INDEX)) {
1:       length += pageMetadata.rowid_page_length;
1:     }
1: 
0:     if (hasEncoding(pageMetadata.encoders, Encoding.RLE)) {
1:       length += pageMetadata.rle_page_length;
1:     }
1:     // get the data buffer
1:     ByteBuffer rawData = dimensionRawColumnChunk.getFileReader()
1:         .readByteBuffer(filePath, offset, length);
1: 
1:     return decodeDimension(dimensionRawColumnChunk, rawData, pageMetadata, 0);
1:   }
1: }
============================================================================