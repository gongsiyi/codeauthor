1:dded5d5: /*
1:dded5d5:  * Licensed to the Apache Software Foundation (ASF) under one or more
1:dded5d5:  * contributor license agreements.  See the NOTICE file distributed with
1:dded5d5:  * this work for additional information regarding copyright ownership.
1:dded5d5:  * The ASF licenses this file to You under the Apache License, Version 2.0
1:dded5d5:  * (the "License"); you may not use this file except in compliance with
1:dded5d5:  * the License.  You may obtain a copy of the License at
2:dded5d5:  *
1:dded5d5:  *    http://www.apache.org/licenses/LICENSE-2.0
1:dded5d5:  *
1:dded5d5:  * Unless required by applicable law or agreed to in writing, software
1:dded5d5:  * distributed under the License is distributed on an "AS IS" BASIS,
1:dded5d5:  * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
1:dded5d5:  * See the License for the specific language governing permissions and
1:dded5d5:  * limitations under the License.
2:dded5d5:  */
1:dded5d5: package org.apache.carbondata.processing.loading.steps;
1:dded5d5: 
1:ec33c11: import java.io.ByteArrayOutputStream;
1:ec33c11: import java.io.DataOutputStream;
1:dded5d5: import java.io.IOException;
1:ec33c11: import java.util.HashMap;
1:dded5d5: import java.util.Iterator;
1:dded5d5: import java.util.List;
1:ec33c11: import java.util.Map;
1:dded5d5: import java.util.concurrent.atomic.AtomicLong;
1:dded5d5: 
1:dded5d5: import org.apache.carbondata.common.CarbonIterator;
1:dded5d5: import org.apache.carbondata.core.datastore.row.CarbonRow;
1:2f23486: import org.apache.carbondata.core.keygenerator.directdictionary.DirectDictionaryGenerator;
1:2f23486: import org.apache.carbondata.core.keygenerator.directdictionary.DirectDictionaryKeyGeneratorFactory;
1:dded5d5: import org.apache.carbondata.core.metadata.datatype.DataType;
1:dded5d5: import org.apache.carbondata.core.metadata.datatype.DataTypes;
1:dded5d5: import org.apache.carbondata.core.metadata.encoder.Encoding;
1:dded5d5: import org.apache.carbondata.core.util.CarbonProperties;
1:dded5d5: import org.apache.carbondata.core.util.DataTypeUtil;
1:ec33c11: import org.apache.carbondata.processing.datatypes.GenericDataType;
1:dded5d5: import org.apache.carbondata.processing.loading.AbstractDataLoadProcessorStep;
1:dded5d5: import org.apache.carbondata.processing.loading.CarbonDataLoadConfiguration;
1:dded5d5: import org.apache.carbondata.processing.loading.DataField;
1:ec33c11: import org.apache.carbondata.processing.loading.constants.DataLoadProcessorConstants;
1:6b70b7e: import org.apache.carbondata.processing.loading.converter.BadRecordLogHolder;
1:ec33c11: import org.apache.carbondata.processing.loading.converter.impl.FieldEncoderFactory;
1:dded5d5: import org.apache.carbondata.processing.loading.converter.impl.RowConverterImpl;
1:6b70b7e: import org.apache.carbondata.processing.loading.exception.BadRecordFoundException;
1:ec33c11: import org.apache.carbondata.processing.loading.exception.CarbonDataLoadingException;
1:dded5d5: import org.apache.carbondata.processing.loading.row.CarbonRowBatch;
1:dded5d5: import org.apache.carbondata.processing.util.CarbonDataProcessorUtil;
1:5804d75: 
1:dded5d5: 
2:dded5d5: /**
1:dded5d5:  * It reads data from record reader and sends data to next step.
1:dded5d5:  */
1:ec33c11: public class InputProcessorStepWithNoConverterImpl extends AbstractDataLoadProcessorStep {
1:dded5d5: 
1:dded5d5:   private CarbonIterator<Object[]>[] inputIterators;
1:dded5d5: 
1:dded5d5:   private boolean[] noDictionaryMapping;
1:dded5d5: 
1:dded5d5:   private DataType[] dataTypes;
1:dded5d5: 
1:dded5d5:   private int[] orderOfData;
1:dded5d5: 
1:ec33c11:   private Map<Integer, GenericDataType> dataFieldsWithComplexDataType;
1:6b70b7e: 
1:94d2089:   // cores used in SDK writer, set by the user
1:94d2089:   private short sdkWriterCores;
1:94d2089: 
1:ec33c11:   public InputProcessorStepWithNoConverterImpl(CarbonDataLoadConfiguration configuration,
1:dded5d5:       CarbonIterator<Object[]>[] inputIterators) {
1:dded5d5:     super(configuration, null);
1:dded5d5:     this.inputIterators = inputIterators;
1:94d2089:     sdkWriterCores = configuration.getWritingCoresCount();
1:dded5d5:   }
1:dded5d5: 
1:dded5d5:   @Override public DataField[] getOutput() {
1:dded5d5:     return configuration.getDataFields();
1:dded5d5:   }
1:dded5d5: 
1:dded5d5:   @Override public void initialize() throws IOException {
1:dded5d5:     super.initialize();
1:dded5d5:     // if logger is enabled then raw data will be required.
1:dded5d5:     RowConverterImpl rowConverter =
1:dded5d5:         new RowConverterImpl(configuration.getDataFields(), configuration, null);
1:dded5d5:     rowConverter.initialize();
1:dded5d5:     configuration.setCardinalityFinder(rowConverter);
1:dded5d5:     noDictionaryMapping =
1:dded5d5:         CarbonDataProcessorUtil.getNoDictionaryMapping(configuration.getDataFields());
1:dded5d5: 
1:ec33c11:     dataFieldsWithComplexDataType = new HashMap<>();
1:ec33c11:     convertComplexDataType(dataFieldsWithComplexDataType);
1:ec33c11: 
1:dded5d5:     dataTypes = new DataType[configuration.getDataFields().length];
1:dded5d5:     for (int i = 0; i < dataTypes.length; i++) {
1:dded5d5:       if (configuration.getDataFields()[i].getColumn().hasEncoding(Encoding.DICTIONARY)) {
1:dded5d5:         dataTypes[i] = DataTypes.INT;
1:dded5d5:       } else {
1:dded5d5:         dataTypes[i] = configuration.getDataFields()[i].getColumn().getDataType();
1:dded5d5:       }
1:dded5d5:     }
1:dded5d5:     orderOfData = arrangeData(configuration.getDataFields(), configuration.getHeader());
1:dded5d5:   }
1:dded5d5: 
1:ec33c11:   private void convertComplexDataType(Map<Integer, GenericDataType> dataFieldsWithComplexDataType) {
1:ec33c11:     DataField[] srcDataField = configuration.getDataFields();
1:ec33c11:     FieldEncoderFactory fieldConverterFactory = FieldEncoderFactory.getInstance();
1:ec33c11:     String nullFormat =
1:ec33c11:         configuration.getDataLoadProperty(DataLoadProcessorConstants.SERIALIZATION_NULL_FORMAT)
1:ec33c11:             .toString();
1:ec33c11:     boolean isEmptyBadRecord = Boolean.parseBoolean(
1:ec33c11:         configuration.getDataLoadProperty(DataLoadProcessorConstants.IS_EMPTY_DATA_BAD_RECORD)
1:ec33c11:             .toString());
1:ec33c11:     for (int i = 0; i < srcDataField.length; i++) {
1:ec33c11:       if (srcDataField[i].getColumn().isComplex()) {
1:ec33c11:         // create a ComplexDataType
1:ec33c11:         dataFieldsWithComplexDataType.put(srcDataField[i].getColumn().getOrdinal(),
1:ec33c11:             fieldConverterFactory
1:5804d75:                 .createComplexDataType(srcDataField[i], configuration.getTableIdentifier(), null,
1:5804d75:                     false, null, i, nullFormat, isEmptyBadRecord));
1:dded5d5:       }
1:dded5d5:     }
1:dded5d5:   }
1:dded5d5: 
1:dded5d5:   private int[] arrangeData(DataField[] dataFields, String[] header) {
1:dded5d5:     int[] data = new int[dataFields.length];
1:dded5d5:     for (int i = 0; i < dataFields.length; i++) {
1:dded5d5:       for (int j = 0; j < header.length; j++) {
1:dded5d5:         if (dataFields[i].getColumn().getColName().equalsIgnoreCase(header[j])) {
1:dded5d5:           data[i] = j;
1:dded5d5:           break;
1:dded5d5:         }
1:dded5d5:       }
1:dded5d5:     }
1:dded5d5:     return data;
1:dded5d5:   }
1:dded5d5: 
1:dded5d5:   @Override public Iterator<CarbonRowBatch>[] execute() {
1:dded5d5:     int batchSize = CarbonProperties.getInstance().getBatchSize();
1:94d2089:     List<CarbonIterator<Object[]>>[] readerIterators =
1:94d2089:         CarbonDataProcessorUtil.partitionInputReaderIterators(this.inputIterators, sdkWriterCores);
1:dded5d5:     Iterator<CarbonRowBatch>[] outIterators = new Iterator[readerIterators.length];
1:dded5d5:     for (int i = 0; i < outIterators.length; i++) {
1:dded5d5:       outIterators[i] =
1:dded5d5:           new InputProcessorIterator(readerIterators[i], batchSize, configuration.isPreFetch(),
1:5804d75:               rowCounter, orderOfData, noDictionaryMapping, dataTypes, configuration,
1:5804d75:               dataFieldsWithComplexDataType);
1:dded5d5:     }
1:dded5d5:     return outIterators;
1:dded5d5:   }
1:dded5d5: 
1:dded5d5:   @Override public void close() {
1:dded5d5:     if (!closed) {
1:dded5d5:       super.close();
1:dded5d5:       for (CarbonIterator inputIterator : inputIterators) {
1:dded5d5:         inputIterator.close();
1:dded5d5:       }
1:dded5d5:     }
1:dded5d5:   }
1:dded5d5: 
1:dded5d5:   @Override protected String getStepName() {
1:dded5d5:     return "Input Processor";
1:dded5d5:   }
1:dded5d5: 
1:dded5d5:   /**
1:dded5d5:    * This iterator wraps the list of iterators and it starts iterating the each
1:dded5d5:    * iterator of the list one by one. It also parse the data while iterating it.
1:dded5d5:    */
1:dded5d5:   private static class InputProcessorIterator extends CarbonIterator<CarbonRowBatch> {
1:dded5d5: 
1:dded5d5:     private List<CarbonIterator<Object[]>> inputIterators;
1:dded5d5: 
1:dded5d5:     private CarbonIterator<Object[]> currentIterator;
1:dded5d5: 
1:dded5d5:     private int counter;
1:dded5d5: 
1:dded5d5:     private int batchSize;
1:dded5d5: 
1:dded5d5:     private boolean nextBatch;
1:dded5d5: 
1:dded5d5:     private boolean firstTime;
1:dded5d5: 
1:dded5d5:     private AtomicLong rowCounter;
1:dded5d5: 
1:dded5d5:     private boolean[] noDictionaryMapping;
1:dded5d5: 
1:dded5d5:     private DataType[] dataTypes;
1:dded5d5: 
1:ec33c11:     private DataField[] dataFields;
1:dded5d5: 
1:dded5d5:     private int[] orderOfData;
1:dded5d5: 
1:ec33c11:     private Map<Integer, GenericDataType> dataFieldsWithComplexDataType;
2:ec33c11: 
1:2f23486:     private DirectDictionaryGenerator dateDictionaryGenerator;
1:2f23486: 
1:2f23486:     private DirectDictionaryGenerator timestampDictionaryGenerator;
1:2f23486: 
1:2f537b7:     private BadRecordLogHolder logHolder = new BadRecordLogHolder();
1:2f537b7: 
1:dded5d5:     public InputProcessorIterator(List<CarbonIterator<Object[]>> inputIterators, int batchSize,
1:dded5d5:         boolean preFetch, AtomicLong rowCounter, int[] orderOfData, boolean[] noDictionaryMapping,
1:6b70b7e:         DataType[] dataTypes, CarbonDataLoadConfiguration configuration,
1:ec33c11:         Map<Integer, GenericDataType> dataFieldsWithComplexDataType) {
1:dded5d5:       this.inputIterators = inputIterators;
1:dded5d5:       this.batchSize = batchSize;
1:dded5d5:       this.counter = 0;
1:dded5d5:       // Get the first iterator from the list.
1:dded5d5:       currentIterator = inputIterators.get(counter++);
1:dded5d5:       this.rowCounter = rowCounter;
1:dded5d5:       this.nextBatch = false;
1:dded5d5:       this.firstTime = true;
1:dded5d5:       this.noDictionaryMapping = noDictionaryMapping;
1:dded5d5:       this.dataTypes = dataTypes;
1:6b70b7e:       this.dataFields = configuration.getDataFields();
1:dded5d5:       this.orderOfData = orderOfData;
1:ec33c11:       this.dataFieldsWithComplexDataType = dataFieldsWithComplexDataType;
1:dded5d5:     }
1:dded5d5: 
1:dded5d5:     @Override public boolean hasNext() {
1:dded5d5:       return nextBatch || internalHasNext();
1:dded5d5:     }
1:dded5d5: 
1:dded5d5:     private boolean internalHasNext() {
1:dded5d5:       if (firstTime) {
1:dded5d5:         firstTime = false;
1:dded5d5:         currentIterator.initialize();
1:dded5d5:       }
1:dded5d5:       boolean hasNext = currentIterator.hasNext();
1:dded5d5:       // If iterator is finished then check for next iterator.
1:dded5d5:       if (!hasNext) {
1:dded5d5:         currentIterator.close();
1:dded5d5:         // Check next iterator is available in the list.
1:dded5d5:         if (counter < inputIterators.size()) {
1:dded5d5:           // Get the next iterator from the list.
1:dded5d5:           currentIterator = inputIterators.get(counter++);
1:dded5d5:           // Initialize the new iterator
1:dded5d5:           currentIterator.initialize();
1:dded5d5:           hasNext = internalHasNext();
1:dded5d5:         }
1:dded5d5:       }
1:dded5d5:       return hasNext;
1:dded5d5:     }
1:dded5d5: 
1:dded5d5:     @Override public CarbonRowBatch next() {
1:dded5d5:       return getBatch();
1:dded5d5:     }
1:dded5d5: 
1:dded5d5:     private CarbonRowBatch getBatch() {
1:dded5d5:       // Create batch and fill it.
1:dded5d5:       CarbonRowBatch carbonRowBatch = new CarbonRowBatch(batchSize);
1:dded5d5:       int count = 0;
1:5804d75: 
1:dded5d5:       while (internalHasNext() && count < batchSize) {
1:ec33c11:         carbonRowBatch.addRow(
1:ec33c11:             new CarbonRow(convertToNoDictionaryToBytes(currentIterator.next(), dataFields)));
1:dded5d5:         count++;
1:dded5d5:       }
1:dded5d5:       rowCounter.getAndAdd(carbonRowBatch.getSize());
1:5804d75: 
1:5804d75: 
1:dded5d5:       return carbonRowBatch;
1:5804d75: 
1:dded5d5:     }
1:dded5d5: 
1:ec33c11:     private Object[] convertToNoDictionaryToBytes(Object[] data, DataField[] dataFields) {
1:dded5d5:       Object[] newData = new Object[data.length];
1:ec33c11:       for (int i = 0; i < data.length; i++) {
1:ec33c11:         if (i < noDictionaryMapping.length && noDictionaryMapping[i]) {
1:dded5d5:           newData[i] = DataTypeUtil
1:dded5d5:               .getBytesDataDataTypeForNoDictionaryColumn(data[orderOfData[i]], dataTypes[i]);
1:dded5d5:         } else {
1:ec33c11:           // if this is a complex column then recursively comver the data into Byte Array.
1:ec33c11:           if (dataTypes[i].isComplexType()) {
1:ec33c11:             ByteArrayOutputStream byteArray = new ByteArrayOutputStream();
1:ec33c11:             DataOutputStream dataOutputStream = new DataOutputStream(byteArray);
1:ec33c11:             try {
1:ec33c11:               GenericDataType complextType =
1:ec33c11:                   dataFieldsWithComplexDataType.get(dataFields[i].getColumn().getOrdinal());
1:6b70b7e:               complextType.writeByteArray(data[orderOfData[i]], dataOutputStream, logHolder);
1:ec33c11:               dataOutputStream.close();
1:ec33c11:               newData[i] = byteArray.toByteArray();
1:6b70b7e:             } catch (BadRecordFoundException e) {
1:6b70b7e:               throw new CarbonDataLoadingException("Loading Exception: " + e.getMessage(), e);
1:ec33c11:             } catch (Exception e) {
1:ec33c11:               throw new CarbonDataLoadingException("Loading Exception", e);
1:dded5d5:             }
1:ec33c11:           } else {
1:2f23486:             DataType dataType = dataFields[i].getColumn().getDataType();
1:2f23486:             if (dataType == DataTypes.DATE && data[orderOfData[i]] instanceof Long) {
1:2f23486:               if (dateDictionaryGenerator == null) {
1:2f23486:                 dateDictionaryGenerator = DirectDictionaryKeyGeneratorFactory
1:2f23486:                     .getDirectDictionaryGenerator(dataType, dataFields[i].getDateFormat());
1:dded5d5:               }
1:2f23486:               newData[i] = dateDictionaryGenerator.generateKey((long) data[orderOfData[i]]);
1:2f23486:             } else if (dataType == DataTypes.TIMESTAMP && data[orderOfData[i]] instanceof Long) {
1:2f23486:               if (timestampDictionaryGenerator == null) {
1:5804d75:                 timestampDictionaryGenerator = DirectDictionaryKeyGeneratorFactory
1:5804d75:                     .getDirectDictionaryGenerator(dataType, dataFields[i].getTimestampFormat());
1:dded5d5:               }
1:2f23486:               newData[i] = timestampDictionaryGenerator.generateKey((long) data[orderOfData[i]]);
1:2f23486:             } else {
2:dded5d5:               newData[i] = data[orderOfData[i]];
1:dded5d5:             }
1:dded5d5:           }
2:2f23486:         }
1:2f23486:       }
1:dded5d5:       return newData;
1:dded5d5:     }
1:dded5d5: 
1:dded5d5:   }
1:dded5d5: 
1:dded5d5: }
============================================================================
author:ajantha-bhat
-------------------------------------------------------------------------------
commit:94d2089
/////////////////////////////////////////////////////////////////////////
/////////////////////////////////////////////////////////////////////////
1:   // cores used in SDK writer, set by the user
1:   private short sdkWriterCores;
1: 
1:     sdkWriterCores = configuration.getWritingCoresCount();
/////////////////////////////////////////////////////////////////////////
1:     List<CarbonIterator<Object[]>>[] readerIterators =
1:         CarbonDataProcessorUtil.partitionInputReaderIterators(this.inputIterators, sdkWriterCores);
/////////////////////////////////////////////////////////////////////////
commit:5804d75
/////////////////////////////////////////////////////////////////////////
1: 
/////////////////////////////////////////////////////////////////////////
1:                 .createComplexDataType(srcDataField[i], configuration.getTableIdentifier(), null,
1:                     false, null, i, nullFormat, isEmptyBadRecord));
/////////////////////////////////////////////////////////////////////////
1:               rowCounter, orderOfData, noDictionaryMapping, dataTypes, configuration,
1:               dataFieldsWithComplexDataType);
/////////////////////////////////////////////////////////////////////////
1: 
1: 
1: 
1: 
/////////////////////////////////////////////////////////////////////////
1:                 timestampDictionaryGenerator = DirectDictionaryKeyGeneratorFactory
1:                     .getDirectDictionaryGenerator(dataType, dataFields[i].getTimestampFormat());
commit:ff5166e
/////////////////////////////////////////////////////////////////////////
0:                 .createComplexDataType(srcDataField[i], configuration.getTableIdentifier(),
author:ravipesala
-------------------------------------------------------------------------------
commit:2f537b7
/////////////////////////////////////////////////////////////////////////
/////////////////////////////////////////////////////////////////////////
1:     private BadRecordLogHolder logHolder = new BadRecordLogHolder();
1: 
/////////////////////////////////////////////////////////////////////////
/////////////////////////////////////////////////////////////////////////
/////////////////////////////////////////////////////////////////////////
/////////////////////////////////////////////////////////////////////////
commit:dded5d5
/////////////////////////////////////////////////////////////////////////
1: /*
1:  * Licensed to the Apache Software Foundation (ASF) under one or more
1:  * contributor license agreements.  See the NOTICE file distributed with
1:  * this work for additional information regarding copyright ownership.
1:  * The ASF licenses this file to You under the Apache License, Version 2.0
1:  * (the "License"); you may not use this file except in compliance with
1:  * the License.  You may obtain a copy of the License at
1:  *
1:  *    http://www.apache.org/licenses/LICENSE-2.0
1:  *
1:  * Unless required by applicable law or agreed to in writing, software
1:  * distributed under the License is distributed on an "AS IS" BASIS,
1:  * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
1:  * See the License for the specific language governing permissions and
1:  * limitations under the License.
1:  */
1: package org.apache.carbondata.processing.loading.steps;
1: 
1: import java.io.IOException;
0: import java.util.ArrayList;
1: import java.util.Iterator;
1: import java.util.List;
1: import java.util.concurrent.atomic.AtomicLong;
1: 
1: import org.apache.carbondata.common.CarbonIterator;
1: import org.apache.carbondata.core.datastore.row.CarbonRow;
1: import org.apache.carbondata.core.metadata.datatype.DataType;
1: import org.apache.carbondata.core.metadata.datatype.DataTypes;
1: import org.apache.carbondata.core.metadata.encoder.Encoding;
1: import org.apache.carbondata.core.util.CarbonProperties;
1: import org.apache.carbondata.core.util.DataTypeUtil;
1: import org.apache.carbondata.processing.loading.AbstractDataLoadProcessorStep;
1: import org.apache.carbondata.processing.loading.CarbonDataLoadConfiguration;
1: import org.apache.carbondata.processing.loading.DataField;
1: import org.apache.carbondata.processing.loading.converter.impl.RowConverterImpl;
1: import org.apache.carbondata.processing.loading.row.CarbonRowBatch;
1: import org.apache.carbondata.processing.util.CarbonDataProcessorUtil;
1: 
1: /**
1:  * It reads data from record reader and sends data to next step.
1:  */
0: public class InputProcessorStepForPartitionImpl extends AbstractDataLoadProcessorStep {
1: 
1:   private CarbonIterator<Object[]>[] inputIterators;
1: 
1:   private boolean[] noDictionaryMapping;
1: 
1:   private DataType[] dataTypes;
1: 
1:   private int[] orderOfData;
1: 
0:   public InputProcessorStepForPartitionImpl(CarbonDataLoadConfiguration configuration,
1:       CarbonIterator<Object[]>[] inputIterators) {
1:     super(configuration, null);
1:     this.inputIterators = inputIterators;
1:   }
1: 
1:   @Override public DataField[] getOutput() {
1:     return configuration.getDataFields();
1:   }
1: 
1:   @Override public void initialize() throws IOException {
1:     super.initialize();
1:     // if logger is enabled then raw data will be required.
1:     RowConverterImpl rowConverter =
1:         new RowConverterImpl(configuration.getDataFields(), configuration, null);
1:     rowConverter.initialize();
1:     configuration.setCardinalityFinder(rowConverter);
1:     noDictionaryMapping =
1:         CarbonDataProcessorUtil.getNoDictionaryMapping(configuration.getDataFields());
1:     dataTypes = new DataType[configuration.getDataFields().length];
1:     for (int i = 0; i < dataTypes.length; i++) {
1:       if (configuration.getDataFields()[i].getColumn().hasEncoding(Encoding.DICTIONARY)) {
1:         dataTypes[i] = DataTypes.INT;
1:       } else {
1:         dataTypes[i] = configuration.getDataFields()[i].getColumn().getDataType();
1:       }
1:     }
1:     orderOfData = arrangeData(configuration.getDataFields(), configuration.getHeader());
1:   }
1: 
1:   private int[] arrangeData(DataField[] dataFields, String[] header) {
1:     int[] data = new int[dataFields.length];
1:     for (int i = 0; i < dataFields.length; i++) {
1:       for (int j = 0; j < header.length; j++) {
1:         if (dataFields[i].getColumn().getColName().equalsIgnoreCase(header[j])) {
1:           data[i] = j;
1:           break;
1:         }
1:       }
1:     }
1:     return data;
1:   }
1: 
1:   @Override public Iterator<CarbonRowBatch>[] execute() {
1:     int batchSize = CarbonProperties.getInstance().getBatchSize();
0:     List<CarbonIterator<Object[]>>[] readerIterators = partitionInputReaderIterators();
1:     Iterator<CarbonRowBatch>[] outIterators = new Iterator[readerIterators.length];
1:     for (int i = 0; i < outIterators.length; i++) {
1:       outIterators[i] =
1:           new InputProcessorIterator(readerIterators[i], batchSize, configuration.isPreFetch(),
0:               rowCounter, orderOfData, noDictionaryMapping, dataTypes);
1:     }
1:     return outIterators;
1:   }
1: 
1:   /**
0:    * Partition input iterators equally as per the number of threads.
1:    *
0:    * @return
1:    */
0:   private List<CarbonIterator<Object[]>>[] partitionInputReaderIterators() {
0:     // Get the number of cores configured in property.
0:     int numberOfCores = CarbonProperties.getInstance().getNumberOfCores();
0:     // Get the minimum of number of cores and iterators size to get the number of parallel threads
0:     // to be launched.
0:     int parallelThreadNumber = Math.min(inputIterators.length, numberOfCores);
1: 
0:     List<CarbonIterator<Object[]>>[] iterators = new List[parallelThreadNumber];
0:     for (int i = 0; i < parallelThreadNumber; i++) {
0:       iterators[i] = new ArrayList<>();
1:     }
0:     // Equally partition the iterators as per number of threads
0:     for (int i = 0; i < inputIterators.length; i++) {
0:       iterators[i % parallelThreadNumber].add(inputIterators[i]);
1:     }
0:     return iterators;
1:   }
1: 
0:   @Override protected CarbonRow processRow(CarbonRow row) {
0:     return null;
1:   }
1: 
1:   @Override public void close() {
1:     if (!closed) {
1:       super.close();
1:       for (CarbonIterator inputIterator : inputIterators) {
1:         inputIterator.close();
1:       }
1:     }
1:   }
1: 
1:   @Override protected String getStepName() {
1:     return "Input Processor";
1:   }
1: 
1:   /**
1:    * This iterator wraps the list of iterators and it starts iterating the each
1:    * iterator of the list one by one. It also parse the data while iterating it.
1:    */
1:   private static class InputProcessorIterator extends CarbonIterator<CarbonRowBatch> {
1: 
1:     private List<CarbonIterator<Object[]>> inputIterators;
1: 
1:     private CarbonIterator<Object[]> currentIterator;
1: 
1:     private int counter;
1: 
1:     private int batchSize;
1: 
1:     private boolean nextBatch;
1: 
1:     private boolean firstTime;
1: 
1:     private AtomicLong rowCounter;
1: 
1:     private boolean[] noDictionaryMapping;
1: 
1:     private DataType[] dataTypes;
1: 
1:     private int[] orderOfData;
1: 
1:     public InputProcessorIterator(List<CarbonIterator<Object[]>> inputIterators, int batchSize,
1:         boolean preFetch, AtomicLong rowCounter, int[] orderOfData, boolean[] noDictionaryMapping,
0:         DataType[] dataTypes) {
1:       this.inputIterators = inputIterators;
1:       this.batchSize = batchSize;
1:       this.counter = 0;
1:       // Get the first iterator from the list.
1:       currentIterator = inputIterators.get(counter++);
1:       this.rowCounter = rowCounter;
1:       this.nextBatch = false;
1:       this.firstTime = true;
1:       this.noDictionaryMapping = noDictionaryMapping;
1:       this.dataTypes = dataTypes;
1:       this.orderOfData = orderOfData;
1:     }
1: 
1:     @Override public boolean hasNext() {
1:       return nextBatch || internalHasNext();
1:     }
1: 
1:     private boolean internalHasNext() {
1:       if (firstTime) {
1:         firstTime = false;
1:         currentIterator.initialize();
1:       }
1:       boolean hasNext = currentIterator.hasNext();
1:       // If iterator is finished then check for next iterator.
1:       if (!hasNext) {
1:         currentIterator.close();
1:         // Check next iterator is available in the list.
1:         if (counter < inputIterators.size()) {
1:           // Get the next iterator from the list.
1:           currentIterator = inputIterators.get(counter++);
1:           // Initialize the new iterator
1:           currentIterator.initialize();
1:           hasNext = internalHasNext();
1:         }
1:       }
1:       return hasNext;
1:     }
1: 
1:     @Override public CarbonRowBatch next() {
1:       return getBatch();
1:     }
1: 
1:     private CarbonRowBatch getBatch() {
1:       // Create batch and fill it.
1:       CarbonRowBatch carbonRowBatch = new CarbonRowBatch(batchSize);
1:       int count = 0;
1:       while (internalHasNext() && count < batchSize) {
0:         carbonRowBatch.addRow(new CarbonRow(convertToNoDictionaryToBytes(currentIterator.next())));
1:         count++;
1:       }
1:       rowCounter.getAndAdd(carbonRowBatch.getSize());
1:       return carbonRowBatch;
1:     }
1: 
0:     private Object[] convertToNoDictionaryToBytes(Object[] data) {
1:       Object[] newData = new Object[data.length];
0:       for (int i = 0; i < noDictionaryMapping.length; i++) {
0:         if (noDictionaryMapping[i]) {
1:           newData[i] = DataTypeUtil
1:               .getBytesDataDataTypeForNoDictionaryColumn(data[orderOfData[i]], dataTypes[i]);
1:         } else {
1:           newData[i] = data[orderOfData[i]];
1:         }
1:       }
0:       if (newData.length > noDictionaryMapping.length) {
0:         for (int i = noDictionaryMapping.length; i < newData.length; i++) {
1:           newData[i] = data[orderOfData[i]];
1:         }
1:       }
0:       //      System.out.println(Arrays.toString(data));
1:       return newData;
1:     }
1: 
1:   }
1: 
1: }
author:sraghunandan
-------------------------------------------------------------------------------
commit:f911403
/////////////////////////////////////////////////////////////////////////
author:kunal642
-------------------------------------------------------------------------------
commit:2f23486
/////////////////////////////////////////////////////////////////////////
1: import org.apache.carbondata.core.keygenerator.directdictionary.DirectDictionaryGenerator;
1: import org.apache.carbondata.core.keygenerator.directdictionary.DirectDictionaryKeyGeneratorFactory;
/////////////////////////////////////////////////////////////////////////
1:     private DirectDictionaryGenerator dateDictionaryGenerator;
1: 
1:     private DirectDictionaryGenerator timestampDictionaryGenerator;
1: 
/////////////////////////////////////////////////////////////////////////
1:             DataType dataType = dataFields[i].getColumn().getDataType();
1:             if (dataType == DataTypes.DATE && data[orderOfData[i]] instanceof Long) {
1:               if (dateDictionaryGenerator == null) {
1:                 dateDictionaryGenerator = DirectDictionaryKeyGeneratorFactory
1:                     .getDirectDictionaryGenerator(dataType, dataFields[i].getDateFormat());
1:               }
1:               newData[i] = dateDictionaryGenerator.generateKey((long) data[orderOfData[i]]);
1:             } else if (dataType == DataTypes.TIMESTAMP && data[orderOfData[i]] instanceof Long) {
1:               if (timestampDictionaryGenerator == null) {
0:                 timestampDictionaryGenerator =
0:                     DirectDictionaryKeyGeneratorFactory
0:                         .getDirectDictionaryGenerator(dataType, dataFields[i].getTimestampFormat());
1:               }
1:               newData[i] = timestampDictionaryGenerator.generateKey((long) data[orderOfData[i]]);
1:             } else {
0:               newData[i] = data[orderOfData[i]];
1:             }
commit:6b70b7e
/////////////////////////////////////////////////////////////////////////
0: import org.apache.carbondata.processing.loading.BadRecordsLogger;
0: import org.apache.carbondata.processing.loading.BadRecordsLoggerProvider;
1: import org.apache.carbondata.processing.loading.converter.BadRecordLogHolder;
1: import org.apache.carbondata.processing.loading.exception.BadRecordFoundException;
/////////////////////////////////////////////////////////////////////////
0:               configuration, dataFieldsWithComplexDataType);
/////////////////////////////////////////////////////////////////////////
0:     private CarbonDataLoadConfiguration configuration;
1: 
1:         DataType[] dataTypes, CarbonDataLoadConfiguration configuration,
/////////////////////////////////////////////////////////////////////////
1:       this.dataFields = configuration.getDataFields();
0:       this.configuration = configuration;
/////////////////////////////////////////////////////////////////////////
0:       BadRecordLogHolder logHolder = new BadRecordLogHolder();
0:       BadRecordsLogger badRecordLogger =
0:           BadRecordsLoggerProvider.createBadRecordLogger(configuration);
/////////////////////////////////////////////////////////////////////////
1:               complextType.writeByteArray(data[orderOfData[i]], dataOutputStream, logHolder);
0:               if (!logHolder.isLogged() && logHolder.isBadRecordNotAdded()) {
0:                 badRecordLogger.addBadRecordsToBuilder(data, logHolder.getReason());
0:                 if (badRecordLogger.isDataLoadFail()) {
0:                   String error = "Data load failed due to bad record: " + logHolder.getReason();
0:                   if (!badRecordLogger.isBadRecordLoggerEnable()) {
0:                     error += "Please enable bad record logger to know the detail reason.";
0:                   }
0:                   throw new BadRecordFoundException(error);
0:                 }
0:               }
1:             } catch (BadRecordFoundException e) {
1:               throw new CarbonDataLoadingException("Loading Exception: " + e.getMessage(), e);
author:sounakr
-------------------------------------------------------------------------------
commit:ec33c11
/////////////////////////////////////////////////////////////////////////
1: import java.io.ByteArrayOutputStream;
1: import java.io.DataOutputStream;
1: import java.util.HashMap;
1: import java.util.Map;
/////////////////////////////////////////////////////////////////////////
1: import org.apache.carbondata.processing.datatypes.GenericDataType;
1: import org.apache.carbondata.processing.loading.constants.DataLoadProcessorConstants;
1: import org.apache.carbondata.processing.loading.converter.impl.FieldEncoderFactory;
1: import org.apache.carbondata.processing.loading.exception.CarbonDataLoadingException;
1: public class InputProcessorStepWithNoConverterImpl extends AbstractDataLoadProcessorStep {
/////////////////////////////////////////////////////////////////////////
1:   private Map<Integer, GenericDataType> dataFieldsWithComplexDataType;
1: 
1:   public InputProcessorStepWithNoConverterImpl(CarbonDataLoadConfiguration configuration,
/////////////////////////////////////////////////////////////////////////
1: 
1:     dataFieldsWithComplexDataType = new HashMap<>();
1:     convertComplexDataType(dataFieldsWithComplexDataType);
1: 
/////////////////////////////////////////////////////////////////////////
1:   private void convertComplexDataType(Map<Integer, GenericDataType> dataFieldsWithComplexDataType) {
1:     DataField[] srcDataField = configuration.getDataFields();
1:     FieldEncoderFactory fieldConverterFactory = FieldEncoderFactory.getInstance();
1:     String nullFormat =
1:         configuration.getDataLoadProperty(DataLoadProcessorConstants.SERIALIZATION_NULL_FORMAT)
1:             .toString();
1:     boolean isEmptyBadRecord = Boolean.parseBoolean(
1:         configuration.getDataLoadProperty(DataLoadProcessorConstants.IS_EMPTY_DATA_BAD_RECORD)
1:             .toString());
1:     for (int i = 0; i < srcDataField.length; i++) {
1:       if (srcDataField[i].getColumn().isComplex()) {
1:         // create a ComplexDataType
1:         dataFieldsWithComplexDataType.put(srcDataField[i].getColumn().getOrdinal(),
1:             fieldConverterFactory
0:                 .createComplexDataType(srcDataField[i], null, configuration.getTableIdentifier(),
0:                     null, false, null, i, nullFormat, isEmptyBadRecord));
0:       }
0:     }
0:   }
0: 
/////////////////////////////////////////////////////////////////////////
0:               rowCounter, orderOfData, noDictionaryMapping, dataTypes,
0:               configuration.getDataFields(), dataFieldsWithComplexDataType);
/////////////////////////////////////////////////////////////////////////
1:     private DataField[] dataFields;
0: 
1:     private Map<Integer, GenericDataType> dataFieldsWithComplexDataType;
0: 
0:         DataType[] dataTypes, DataField[] dataFields,
1:         Map<Integer, GenericDataType> dataFieldsWithComplexDataType) {
/////////////////////////////////////////////////////////////////////////
0:       this.dataFields = dataFields;
1:       this.dataFieldsWithComplexDataType = dataFieldsWithComplexDataType;
/////////////////////////////////////////////////////////////////////////
1:         carbonRowBatch.addRow(
1:             new CarbonRow(convertToNoDictionaryToBytes(currentIterator.next(), dataFields)));
1:     private Object[] convertToNoDictionaryToBytes(Object[] data, DataField[] dataFields) {
1:       for (int i = 0; i < data.length; i++) {
1:         if (i < noDictionaryMapping.length && noDictionaryMapping[i]) {
1:           // if this is a complex column then recursively comver the data into Byte Array.
1:           if (dataTypes[i].isComplexType()) {
1:             ByteArrayOutputStream byteArray = new ByteArrayOutputStream();
1:             DataOutputStream dataOutputStream = new DataOutputStream(byteArray);
1:             try {
1:               GenericDataType complextType =
1:                   dataFieldsWithComplexDataType.get(dataFields[i].getColumn().getOrdinal());
0: 
0:               complextType.writeByteArray(data[orderOfData[i]], dataOutputStream);
0: 
1:               dataOutputStream.close();
1:               newData[i] = byteArray.toByteArray();
1:             } catch (Exception e) {
1:               throw new CarbonDataLoadingException("Loading Exception", e);
0:             }
1:           } else {
0:             newData[i] = data[orderOfData[i]];
0:           }
0:       // System.out.println(Arrays.toString(data));
============================================================================