1:bcef656: /*
1:bcef656:  * Licensed to the Apache Software Foundation (ASF) under one or more
1:bcef656:  * contributor license agreements.  See the NOTICE file distributed with
1:bcef656:  * this work for additional information regarding copyright ownership.
1:bcef656:  * The ASF licenses this file to You under the Apache License, Version 2.0
1:bcef656:  * (the "License"); you may not use this file except in compliance with
1:bcef656:  * the License.  You may obtain a copy of the License at
1:bcef656:  *
1:bcef656:  *    http://www.apache.org/licenses/LICENSE-2.0
1:bcef656:  *
1:bcef656:  * Unless required by applicable law or agreed to in writing, software
1:bcef656:  * distributed under the License is distributed on an "AS IS" BASIS,
1:bcef656:  * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
1:bcef656:  * See the License for the specific language governing permissions and
1:bcef656:  * limitations under the License.
4:bcef656:  */
1:bcef656: package org.apache.spark.sql;
6:bcef656: 
1:bcef656: import java.math.BigInteger;
1:bcef656: 
1:bcef656: import org.apache.spark.memory.MemoryMode;
1:bcef656: import org.apache.spark.sql.catalyst.InternalRow;
1:74c3eb1: import org.apache.spark.sql.execution.vectorized.Dictionary;
1:74c3eb1: import org.apache.spark.sql.execution.vectorized.WritableColumnVector;
1:74c3eb1: import org.apache.spark.sql.types.*;
1:74c3eb1: import org.apache.spark.sql.vectorized.ColumnarBatch;
1:bcef656: import org.apache.spark.unsafe.types.CalendarInterval;
1:bcef656: import org.apache.spark.unsafe.types.UTF8String;
1:bcef656: 
4:bcef656: /**
1:bcef656:  * Adapter class which handles the columnar vector reading of the carbondata
1:bcef656:  * based on the spark ColumnVector and ColumnarBatch API. This proxy class
1:74c3eb1:  * handles the complexity of spark 2.3 version related api changes since
1:bcef656:  * spark ColumnVector and ColumnarBatch interfaces are still evolving.
1:bcef656:  */
1:bcef656: public class CarbonVectorProxy {
1:bcef656: 
1:bcef656:     private ColumnarBatch columnarBatch;
1:74c3eb1:     private WritableColumnVector[] columnVectors;
1:bcef656: 
1:bcef656:     /**
1:bcef656:      * Adapter class which handles the columnar vector reading of the carbondata
1:bcef656:      * based on the spark ColumnVector and ColumnarBatch API. This proxy class
1:bcef656:      * handles the complexity of spark 2.3 version related api changes since
1:bcef656:      * spark ColumnVector and ColumnarBatch interfaces are still evolving.
1:bcef656:      *
1:bcef656:      * @param memMode       which represent the type onheap or offheap vector.
1:bcef656:      * @param rowNum        rows number for vector reading
1:bcef656:      * @param structFileds, metadata related to current schema of table.
1:bcef656:      */
1:bcef656:     public CarbonVectorProxy(MemoryMode memMode, int rowNum, StructField[] structFileds) {
1:74c3eb1:         columnVectors = ColumnVectorFactory
1:74c3eb1:                 .getColumnVector(memMode, new StructType(structFileds), rowNum);
1:74c3eb1:         columnarBatch = new ColumnarBatch(columnVectors);
1:74c3eb1:         columnarBatch.setNumRows(rowNum);
6:bcef656:     }
1:bcef656: 
1:bcef656:     public CarbonVectorProxy(MemoryMode memMode, StructType outputSchema, int rowNum) {
1:74c3eb1:         columnVectors = ColumnVectorFactory
1:74c3eb1:                 .getColumnVector(memMode, outputSchema, rowNum);
1:74c3eb1:         columnarBatch = new ColumnarBatch(columnVectors);
1:74c3eb1:         columnarBatch.setNumRows(rowNum);
1:bcef656:     }
1:bcef656: 
1:bcef656:     /**
1:bcef656:      * Returns the number of rows for read, including filtered rows.
1:bcef656:      */
1:bcef656:     public int numRows() {
1:74c3eb1:         return columnarBatch.numRows();
1:74c3eb1:     }
1:74c3eb1: 
1:74c3eb1:     public Object reserveDictionaryIds(int capacity, int ordinal) {
1:74c3eb1:         return columnVectors[ordinal].reserveDictionaryIds(capacity);
1:74c3eb1:     }
1:74c3eb1: 
1:74c3eb1:     /**
1:74c3eb1:      * This API will return a columnvector from a batch of column vector rows
1:74c3eb1:      * based on the ordinal
1:74c3eb1:      *
1:74c3eb1:      * @param ordinal
1:74c3eb1:      * @return
1:74c3eb1:      */
1:74c3eb1:     public WritableColumnVector column(int ordinal) {
1:74c3eb1:         return (WritableColumnVector) columnarBatch.column(ordinal);
1:74c3eb1:     }
1:74c3eb1: 
1:74c3eb1:     public WritableColumnVector getColumnVector(int ordinal) {
1:74c3eb1:         return columnVectors[ordinal];
1:74c3eb1:     }
1:74c3eb1: 
1:74c3eb1:     /**
1:74c3eb1:      * Resets this column for writing. The currently stored values are no longer accessible.
1:74c3eb1:      */
1:74c3eb1:     public void reset() {
1:74c3eb1:         for (WritableColumnVector col : columnVectors) {
1:74c3eb1:             col.reset();
1:74c3eb1:         }
1:74c3eb1:     }
1:74c3eb1: 
1:74c3eb1:     public void resetDictionaryIds(int ordinal) {
1:74c3eb1:         columnVectors[ordinal].getDictionaryIds().reset();
1:74c3eb1:     }
1:74c3eb1: 
1:74c3eb1:     /**
1:74c3eb1:      * Returns the row in this batch at `rowId`. Returned row is reused across calls.
1:74c3eb1:      */
1:74c3eb1:     public InternalRow getRow(int rowId) {
1:74c3eb1:         return columnarBatch.getRow(rowId);
1:74c3eb1:     }
1:74c3eb1: 
1:74c3eb1: 
1:74c3eb1:     /**
1:74c3eb1:      * Returns the row in this batch at `rowId`. Returned row is reused across calls.
1:74c3eb1:      */
1:74c3eb1:     public Object getColumnarBatch() {
1:74c3eb1:         return columnarBatch;
1:bcef656:     }
1:bcef656: 
1:bcef656:     /**
1:bcef656:      * Called to close all the columns in this batch. It is not valid to access the data after
1:bcef656:      * calling this. This must be called at the end to clean up memory allocations.
1:bcef656:      */
1:bcef656:     public void close() {
1:bcef656:         columnarBatch.close();
1:bcef656:     }
1:bcef656: 
1:bcef656:     /**
1:74c3eb1:      * Sets the number of rows in this batch.
1:bcef656:      */
1:74c3eb1:     public void setNumRows(int numRows) {
1:74c3eb1:         columnarBatch.setNumRows(numRows);
1:bcef656:     }
1:bcef656: 
1:bcef656:     public void putRowToColumnBatch(int rowId, Object value, int offset) {
1:bcef656:         org.apache.spark.sql.types.DataType t = dataType(offset);
1:bcef656:         if (null == value) {
1:bcef656:             putNull(rowId, offset);
1:bcef656:         } else {
1:bcef656:             if (t == org.apache.spark.sql.types.DataTypes.BooleanType) {
1:bcef656:                 putBoolean(rowId, (boolean) value, offset);
1:bcef656:             } else if (t == org.apache.spark.sql.types.DataTypes.ByteType) {
1:bcef656:                 putByte(rowId, (byte) value, offset);
1:bcef656:             } else if (t == org.apache.spark.sql.types.DataTypes.ShortType) {
1:bcef656:                 putShort(rowId, (short) value, offset);
1:bcef656:             } else if (t == org.apache.spark.sql.types.DataTypes.IntegerType) {
1:bcef656:                 putInt(rowId, (int) value, offset);
1:bcef656:             } else if (t == org.apache.spark.sql.types.DataTypes.LongType) {
1:bcef656:                 putLong(rowId, (long) value, offset);
1:bcef656:             } else if (t == org.apache.spark.sql.types.DataTypes.FloatType) {
1:bcef656:                 putFloat(rowId, (float) value, offset);
1:bcef656:             } else if (t == org.apache.spark.sql.types.DataTypes.DoubleType) {
1:bcef656:                 putDouble(rowId, (double) value, offset);
1:bcef656:             } else if (t == org.apache.spark.sql.types.DataTypes.StringType) {
1:bcef656:                 UTF8String v = (UTF8String) value;
1:bcef656:                 putByteArray(rowId, v.getBytes(), offset);
1:74c3eb1:             } else if (t instanceof DecimalType) {
1:bcef656:                 DecimalType dt = (DecimalType) t;
1:bcef656:                 Decimal d = Decimal.fromDecimal(value);
1:bcef656:                 if (dt.precision() <= Decimal.MAX_INT_DIGITS()) {
1:bcef656:                     putInt(rowId, (int) d.toUnscaledLong(), offset);
1:bcef656:                 } else if (dt.precision() <= Decimal.MAX_LONG_DIGITS()) {
1:bcef656:                     putLong(rowId, d.toUnscaledLong(), offset);
1:bcef656:                 } else {
1:bcef656:                     final BigInteger integer = d.toJavaBigDecimal().unscaledValue();
1:bcef656:                     byte[] bytes = integer.toByteArray();
1:bcef656:                     putByteArray(rowId, bytes, 0, bytes.length, offset);
1:bcef656:                 }
1:bcef656:             } else if (t instanceof CalendarIntervalType) {
1:bcef656:                 CalendarInterval c = (CalendarInterval) value;
1:74c3eb1:                 columnVectors[offset].getChild(0).putInt(rowId, c.months);
1:74c3eb1:                 columnVectors[offset].getChild(1).putLong(rowId, c.microseconds);
1:bcef656:             } else if (t instanceof org.apache.spark.sql.types.DateType) {
1:bcef656:                 putInt(rowId, (int) value, offset);
1:bcef656:             } else if (t instanceof org.apache.spark.sql.types.TimestampType) {
1:bcef656:                 putLong(rowId, (long) value, offset);
1:bcef656:             }
1:bcef656:         }
1:bcef656:     }
1:bcef656: 
1:bcef656:     public void putBoolean(int rowId, boolean value, int ordinal) {
1:74c3eb1:         columnVectors[ordinal].putBoolean(rowId, (boolean) value);
1:bcef656:     }
1:bcef656: 
1:bcef656:     public void putByte(int rowId, byte value, int ordinal) {
1:74c3eb1:         columnVectors[ordinal].putByte(rowId, (byte) value);
1:bcef656:     }
1:bcef656: 
1:bcef656:     public void putShort(int rowId, short value, int ordinal) {
1:74c3eb1:         columnVectors[ordinal].putShort(rowId, (short) value);
1:bcef656:     }
1:bcef656: 
1:bcef656:     public void putInt(int rowId, int value, int ordinal) {
1:74c3eb1:         columnVectors[ordinal].putInt(rowId, (int) value);
1:74c3eb1:     }
1:74c3eb1: 
1:74c3eb1:     public void putDictionaryInt(int rowId, int value, int ordinal) {
1:74c3eb1:         columnVectors[ordinal].getDictionaryIds().putInt(rowId, (int) value);
1:bcef656:     }
1:bcef656: 
1:bcef656:     public void putFloat(int rowId, float value, int ordinal) {
1:74c3eb1:         columnVectors[ordinal].putFloat(rowId, (float) value);
1:bcef656:     }
1:bcef656: 
1:bcef656:     public void putLong(int rowId, long value, int ordinal) {
1:74c3eb1:         columnVectors[ordinal].putLong(rowId, (long) value);
1:bcef656:     }
1:bcef656: 
1:bcef656:     public void putDouble(int rowId, double value, int ordinal) {
1:74c3eb1:         columnVectors[ordinal].putDouble(rowId, (double) value);
1:bcef656:     }
1:bcef656: 
1:bcef656:     public void putByteArray(int rowId, byte[] value, int ordinal) {
1:74c3eb1:         columnVectors[ordinal].putByteArray(rowId, (byte[]) value);
1:bcef656:     }
1:bcef656: 
1:bcef656:     public void putInts(int rowId, int count, int value, int ordinal) {
1:74c3eb1:         columnVectors[ordinal].putInts(rowId, count, value);
1:bcef656:     }
1:bcef656: 
1:bcef656:     public void putShorts(int rowId, int count, short value, int ordinal) {
1:74c3eb1:         columnVectors[ordinal].putShorts(rowId, count, value);
1:bcef656:     }
1:bcef656: 
1:bcef656:     public void putLongs(int rowId, int count, long value, int ordinal) {
1:74c3eb1:         columnVectors[ordinal].putLongs(rowId, count, value);
1:bcef656:     }
1:bcef656: 
1:bcef656:     public void putDecimal(int rowId, Decimal value, int precision, int ordinal) {
1:74c3eb1:         columnVectors[ordinal].putDecimal(rowId, value, precision);
1:74c3eb1: 
1:bcef656:     }
1:bcef656: 
1:bcef656:     public void putDoubles(int rowId, int count, double value, int ordinal) {
1:74c3eb1:         columnVectors[ordinal].putDoubles(rowId, count, value);
1:bcef656:     }
1:bcef656: 
1:bcef656:     public void putByteArray(int rowId, byte[] value, int offset, int length, int ordinal) {
1:74c3eb1:         columnVectors[ordinal].putByteArray(rowId, (byte[]) value, offset, length);
1:bcef656:     }
1:bcef656: 
1:bcef656:     public void putNull(int rowId, int ordinal) {
1:74c3eb1:         columnVectors[ordinal].putNull(rowId);
1:bcef656:     }
1:bcef656: 
1:bcef656:     public void putNulls(int rowId, int count, int ordinal) {
1:74c3eb1:         columnVectors[ordinal].putNulls(rowId, count);
1:74c3eb1:     }
1:74c3eb1: 
1:74c3eb1:     public void putNotNull(int rowId, int ordinal) {
1:74c3eb1:         columnVectors[ordinal].putNotNull(rowId);
1:74c3eb1:     }
1:74c3eb1: 
1:74c3eb1:     public void putNotNulls(int rowId, int count, int ordinal) {
1:74c3eb1:         columnVectors[ordinal].putNotNulls(rowId, count);
1:bcef656:     }
1:bcef656: 
1:bcef656:     public boolean isNullAt(int rowId, int ordinal) {
1:74c3eb1:         return columnVectors[ordinal].isNullAt(rowId);
1:74c3eb1:     }
1:74c3eb1: 
1:74c3eb1:     public boolean hasDictionary(int ordinal) {
1:74c3eb1:         return columnVectors[ordinal].hasDictionary();
1:74c3eb1:     }
1:74c3eb1: 
1:74c3eb1:     public void setDictionary(Object dictionary, int ordinal) {
1:74c3eb1:         if (dictionary instanceof Dictionary) {
1:74c3eb1:             columnVectors[ordinal].setDictionary((Dictionary) dictionary);
1:74c3eb1:         } else {
1:74c3eb1:             columnVectors[ordinal].setDictionary(null);
1:74c3eb1:         }
1:bcef656:     }
1:bcef656: 
1:bcef656:     public DataType dataType(int ordinal) {
1:74c3eb1:         return columnVectors[ordinal].dataType();
1:bcef656:     }
1:bcef656: }
============================================================================
author:sandeep-katta
-------------------------------------------------------------------------------
commit:74c3eb1
/////////////////////////////////////////////////////////////////////////
1: import org.apache.spark.sql.execution.vectorized.Dictionary;
1: import org.apache.spark.sql.execution.vectorized.WritableColumnVector;
1: import org.apache.spark.sql.types.*;
1: import org.apache.spark.sql.vectorized.ColumnarBatch;
1:  * handles the complexity of spark 2.3 version related api changes since
1:     private WritableColumnVector[] columnVectors;
/////////////////////////////////////////////////////////////////////////
1:         columnVectors = ColumnVectorFactory
1:                 .getColumnVector(memMode, new StructType(structFileds), rowNum);
1:         columnarBatch = new ColumnarBatch(columnVectors);
1:         columnarBatch.setNumRows(rowNum);
1:         columnVectors = ColumnVectorFactory
1:                 .getColumnVector(memMode, outputSchema, rowNum);
1:         columnarBatch = new ColumnarBatch(columnVectors);
1:         columnarBatch.setNumRows(rowNum);
1:         return columnarBatch.numRows();
1:     }
1: 
1:     public Object reserveDictionaryIds(int capacity, int ordinal) {
1:         return columnVectors[ordinal].reserveDictionaryIds(capacity);
1:     }
1: 
1:     /**
1:      * This API will return a columnvector from a batch of column vector rows
1:      * based on the ordinal
1:      *
1:      * @param ordinal
1:      * @return
1:      */
1:     public WritableColumnVector column(int ordinal) {
1:         return (WritableColumnVector) columnarBatch.column(ordinal);
1:     }
1: 
1:     public WritableColumnVector getColumnVector(int ordinal) {
1:         return columnVectors[ordinal];
1:     }
1: 
1:     /**
1:      * Resets this column for writing. The currently stored values are no longer accessible.
1:      */
1:     public void reset() {
1:         for (WritableColumnVector col : columnVectors) {
1:             col.reset();
1:         }
1:     }
1: 
1:     public void resetDictionaryIds(int ordinal) {
1:         columnVectors[ordinal].getDictionaryIds().reset();
1:     }
1: 
1:     /**
1:      * Returns the row in this batch at `rowId`. Returned row is reused across calls.
1:      */
1:     public InternalRow getRow(int rowId) {
1:         return columnarBatch.getRow(rowId);
1:     }
1: 
1: 
1:     /**
1:      * Returns the row in this batch at `rowId`. Returned row is reused across calls.
1:      */
1:     public Object getColumnarBatch() {
1:         return columnarBatch;
/////////////////////////////////////////////////////////////////////////
1:      * Sets the number of rows in this batch.
1:     public void setNumRows(int numRows) {
1:         columnarBatch.setNumRows(numRows);
/////////////////////////////////////////////////////////////////////////
1:             } else if (t instanceof DecimalType) {
/////////////////////////////////////////////////////////////////////////
1:                 columnVectors[offset].getChild(0).putInt(rowId, c.months);
1:                 columnVectors[offset].getChild(1).putLong(rowId, c.microseconds);
/////////////////////////////////////////////////////////////////////////
1:         columnVectors[ordinal].putBoolean(rowId, (boolean) value);
1:         columnVectors[ordinal].putByte(rowId, (byte) value);
1:         columnVectors[ordinal].putShort(rowId, (short) value);
1:         columnVectors[ordinal].putInt(rowId, (int) value);
1:     }
1: 
1:     public void putDictionaryInt(int rowId, int value, int ordinal) {
1:         columnVectors[ordinal].getDictionaryIds().putInt(rowId, (int) value);
1:         columnVectors[ordinal].putFloat(rowId, (float) value);
1:         columnVectors[ordinal].putLong(rowId, (long) value);
1:         columnVectors[ordinal].putDouble(rowId, (double) value);
1:         columnVectors[ordinal].putByteArray(rowId, (byte[]) value);
1:         columnVectors[ordinal].putInts(rowId, count, value);
1:         columnVectors[ordinal].putShorts(rowId, count, value);
1:         columnVectors[ordinal].putLongs(rowId, count, value);
1:         columnVectors[ordinal].putDecimal(rowId, value, precision);
1: 
1:         columnVectors[ordinal].putDoubles(rowId, count, value);
1:         columnVectors[ordinal].putByteArray(rowId, (byte[]) value, offset, length);
1:         columnVectors[ordinal].putNull(rowId);
1:         columnVectors[ordinal].putNulls(rowId, count);
1:     }
1: 
1:     public void putNotNull(int rowId, int ordinal) {
1:         columnVectors[ordinal].putNotNull(rowId);
1:     }
1: 
1:     public void putNotNulls(int rowId, int count, int ordinal) {
1:         columnVectors[ordinal].putNotNulls(rowId, count);
1:         return columnVectors[ordinal].isNullAt(rowId);
1:     }
1: 
1:     public boolean hasDictionary(int ordinal) {
1:         return columnVectors[ordinal].hasDictionary();
1:     }
1: 
1:     public void setDictionary(Object dictionary, int ordinal) {
1:         if (dictionary instanceof Dictionary) {
1:             columnVectors[ordinal].setDictionary((Dictionary) dictionary);
1:         } else {
1:             columnVectors[ordinal].setDictionary(null);
1:         }
1:         return columnVectors[ordinal].dataType();
author:sujith71955
-------------------------------------------------------------------------------
commit:bcef656
/////////////////////////////////////////////////////////////////////////
1: /*
1:  * Licensed to the Apache Software Foundation (ASF) under one or more
1:  * contributor license agreements.  See the NOTICE file distributed with
1:  * this work for additional information regarding copyright ownership.
1:  * The ASF licenses this file to You under the Apache License, Version 2.0
1:  * (the "License"); you may not use this file except in compliance with
1:  * the License.  You may obtain a copy of the License at
1:  *
1:  *    http://www.apache.org/licenses/LICENSE-2.0
1:  *
1:  * Unless required by applicable law or agreed to in writing, software
1:  * distributed under the License is distributed on an "AS IS" BASIS,
1:  * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
1:  * See the License for the specific language governing permissions and
1:  * limitations under the License.
1:  */
1: package org.apache.spark.sql;
1: 
1: import java.math.BigInteger;
1: 
1: import org.apache.spark.memory.MemoryMode;
1: import org.apache.spark.sql.catalyst.InternalRow;
0: import org.apache.spark.sql.execution.vectorized.ColumnarBatch;
0: import org.apache.spark.sql.types.CalendarIntervalType;
0: import org.apache.spark.sql.types.DataType;
0: import org.apache.spark.sql.types.Decimal;
0: import org.apache.spark.sql.types.DecimalType;
0: import org.apache.spark.sql.types.StructField;
0: import org.apache.spark.sql.types.StructType;
1: import org.apache.spark.unsafe.types.CalendarInterval;
1: import org.apache.spark.unsafe.types.UTF8String;
1: 
1: /**
1:  * Adapter class which handles the columnar vector reading of the carbondata
1:  * based on the spark ColumnVector and ColumnarBatch API. This proxy class
0:  * handles the complexity of spark 2.2 version related api changes since
1:  * spark ColumnVector and ColumnarBatch interfaces are still evolving.
1:  */
1: public class CarbonVectorProxy {
1: 
1:     private ColumnarBatch columnarBatch;
1: 
1:     /**
1:      * Adapter class which handles the columnar vector reading of the carbondata
1:      * based on the spark ColumnVector and ColumnarBatch API. This proxy class
1:      * handles the complexity of spark 2.3 version related api changes since
1:      * spark ColumnVector and ColumnarBatch interfaces are still evolving.
1:      *
1:      * @param memMode       which represent the type onheap or offheap vector.
1:      * @param rowNum        rows number for vector reading
1:      * @param structFileds, metadata related to current schema of table.
1:      */
1:     public CarbonVectorProxy(MemoryMode memMode, int rowNum, StructField[] structFileds) {
0:         columnarBatch = ColumnarBatch.allocate(new StructType(structFileds), memMode, rowNum);
1:     }
1: 
1:     public CarbonVectorProxy(MemoryMode memMode, StructType outputSchema, int rowNum) {
0:         columnarBatch = ColumnarBatch.allocate(outputSchema, memMode, rowNum);
1:     }
1: 
1:     /**
0:      * Sets the number of rows in this batch.
1:      */
0:     public void setNumRows(int numRows) {
0:         columnarBatch.setNumRows(numRows);
1:     }
1: 
1:     /**
1:      * Returns the number of rows for read, including filtered rows.
1:      */
1:     public int numRows() {
0:         return columnarBatch.capacity();
1:     }
1: 
1:     /**
1:      * Called to close all the columns in this batch. It is not valid to access the data after
1:      * calling this. This must be called at the end to clean up memory allocations.
1:      */
1:     public void close() {
1:         columnarBatch.close();
1:     }
1: 
1:     /**
0:      * Returns the row in this batch at `rowId`. Returned row is reused across calls.
1:      */
0:     public InternalRow getRow(int rowId) {
0:         return columnarBatch.getRow(rowId);
1:     }
1: 
1:     /**
0:      * Returns the row in this batch at `rowId`. Returned row is reused across calls.
1:      */
0:     public Object getColumnarBatch() {
0:         return columnarBatch;
1:     }
1: 
0:     public Object reserveDictionaryIds(int capacity , int dummyOrdinal) {
0:         return columnarBatch.column(ordinal).reserveDictionaryIds(capacity);
1:     }
1: 
0:     public void resetDictionaryIds(int ordinal) {
0:         columnarBatch.column(ordinal).getDictionaryIds().reset();
1:     }
1: 
1:     /**
0:      * Resets this column for writing. The currently stored values are no longer accessible.
1:      */
0:     public void reset() {
0:         columnarBatch.reset();
1:     }
1: 
1:     public void putRowToColumnBatch(int rowId, Object value, int offset) {
1:         org.apache.spark.sql.types.DataType t = dataType(offset);
1:         if (null == value) {
1:             putNull(rowId, offset);
1:         } else {
1:             if (t == org.apache.spark.sql.types.DataTypes.BooleanType) {
1:                 putBoolean(rowId, (boolean) value, offset);
1:             } else if (t == org.apache.spark.sql.types.DataTypes.ByteType) {
1:                 putByte(rowId, (byte) value, offset);
1:             } else if (t == org.apache.spark.sql.types.DataTypes.ShortType) {
1:                 putShort(rowId, (short) value, offset);
1:             } else if (t == org.apache.spark.sql.types.DataTypes.IntegerType) {
1:                 putInt(rowId, (int) value, offset);
1:             } else if (t == org.apache.spark.sql.types.DataTypes.LongType) {
1:                 putLong(rowId, (long) value, offset);
1:             } else if (t == org.apache.spark.sql.types.DataTypes.FloatType) {
1:                 putFloat(rowId, (float) value, offset);
1:             } else if (t == org.apache.spark.sql.types.DataTypes.DoubleType) {
1:                 putDouble(rowId, (double) value, offset);
1:             } else if (t == org.apache.spark.sql.types.DataTypes.StringType) {
1:                 UTF8String v = (UTF8String) value;
1:                 putByteArray(rowId, v.getBytes(), offset);
0:             } else if (t instanceof org.apache.spark.sql.types.DecimalType) {
1:                 DecimalType dt = (DecimalType) t;
1:                 Decimal d = Decimal.fromDecimal(value);
1:                 if (dt.precision() <= Decimal.MAX_INT_DIGITS()) {
1:                     putInt(rowId, (int) d.toUnscaledLong(), offset);
1:                 } else if (dt.precision() <= Decimal.MAX_LONG_DIGITS()) {
1:                     putLong(rowId, d.toUnscaledLong(), offset);
1:                 } else {
1:                     final BigInteger integer = d.toJavaBigDecimal().unscaledValue();
1:                     byte[] bytes = integer.toByteArray();
1:                     putByteArray(rowId, bytes, 0, bytes.length, offset);
1:                 }
1:             } else if (t instanceof CalendarIntervalType) {
1:                 CalendarInterval c = (CalendarInterval) value;
0:                 columnarBatch.column(offset).getChildColumn(0).putInt(rowId, c.months);
0:                 columnarBatch.column(offset).getChildColumn(1).putLong(rowId, c.microseconds);
1:             } else if (t instanceof org.apache.spark.sql.types.DateType) {
1:                 putInt(rowId, (int) value, offset);
1:             } else if (t instanceof org.apache.spark.sql.types.TimestampType) {
1:                 putLong(rowId, (long) value, offset);
1:             }
1:         }
1:     }
1: 
1:     public void putBoolean(int rowId, boolean value, int ordinal) {
0:         columnarBatch.column(ordinal).putBoolean(rowId, (boolean) value);
1:     }
1: 
1:     public void putByte(int rowId, byte value, int ordinal) {
0:         columnarBatch.column(ordinal).putByte(rowId, (byte) value);
1:     }
1: 
1:     public void putShort(int rowId, short value, int ordinal) {
0:         columnarBatch.column(ordinal).putShort(rowId, (short) value);
1:     }
1: 
1:     public void putInt(int rowId, int value, int ordinal) {
0:         columnarBatch.column(ordinal).putInt(rowId, (int) value);
1:     }
1: 
1:     public void putFloat(int rowId, float value, int ordinal) {
0:         columnarBatch.column(ordinal).putFloat(rowId, (float) value);
1:     }
1: 
1:     public void putLong(int rowId, long value, int ordinal) {
0:         columnarBatch.column(ordinal).putLong(rowId, (long) value);
1:     }
1: 
1:     public void putDouble(int rowId, double value, int ordinal) {
0:         columnarBatch.column(ordinal).putDouble(rowId, (double) value);
1:     }
1: 
1:     public void putByteArray(int rowId, byte[] value, int ordinal) {
0:         columnarBatch.column(ordinal).putByteArray(rowId, (byte[]) value);
1:     }
1: 
1:     public void putInts(int rowId, int count, int value, int ordinal) {
0:         columnarBatch.column(ordinal).putInts(rowId, count, value);
1:     }
1: 
1:     public void putShorts(int rowId, int count, short value, int ordinal) {
0:         columnarBatch.column(ordinal).putShorts(rowId, count, value);
1:     }
1: 
1:     public void putLongs(int rowId, int count, long value, int ordinal) {
0:         columnarBatch.column(ordinal).putLongs(rowId, count, value);
1:     }
1: 
1:     public void putDecimal(int rowId, Decimal value, int precision, int ordinal) {
0:         columnarBatch.column(ordinal).putDecimal(rowId, value, precision);
1:     }
1: 
1:     public void putDoubles(int rowId, int count, double value, int ordinal) {
0:         columnarBatch.column(ordinal).putDoubles(rowId, count, value);
1:     }
1: 
1:     public void putByteArray(int rowId, byte[] value, int offset, int length, int ordinal) {
0:         columnarBatch.column(ordinal).putByteArray(rowId, (byte[]) value, offset, length);
1:     }
1: 
1:     public void putNull(int rowId, int ordinal) {
0:         columnarBatch.column(ordinal).putNull(rowId);
1:     }
1: 
1:     public void putNulls(int rowId, int count, int ordinal) {
0:         columnarBatch.column(ordinal).putNulls(rowId, count);
1:     }
1: 
1:     public boolean isNullAt(int rowId, int ordinal) {
0:         return columnarBatch.column(ordinal).isNullAt(rowId);
1:     }
1: 
1:     public DataType dataType(int ordinal) {
0:         return columnarBatch.column(ordinal).dataType();
1:     }
1: }
============================================================================