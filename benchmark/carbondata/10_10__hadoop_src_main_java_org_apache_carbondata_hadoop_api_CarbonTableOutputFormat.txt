1:edb02ab: /*
1:41347d8:  * Licensed to the Apache Software Foundation (ASF) under one or more
1:41347d8:  * contributor license agreements.  See the NOTICE file distributed with
1:41347d8:  * this work for additional information regarding copyright ownership.
1:41347d8:  * The ASF licenses this file to You under the Apache License, Version 2.0
1:41347d8:  * (the "License"); you may not use this file except in compliance with
1:41347d8:  * the License.  You may obtain a copy of the License at
1:edb02ab:  *
1:edb02ab:  *    http://www.apache.org/licenses/LICENSE-2.0
1:edb02ab:  *
1:41347d8:  * Unless required by applicable law or agreed to in writing, software
1:41347d8:  * distributed under the License is distributed on an "AS IS" BASIS,
1:41347d8:  * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
1:41347d8:  * See the License for the specific language governing permissions and
1:41347d8:  * limitations under the License.
1:edb02ab:  */
3:edb02ab: 
1:edb02ab: package org.apache.carbondata.hadoop.api;
1:edb02ab: 
1:edb02ab: import java.io.IOException;
1:91e6f6f: import java.util.List;
1:4430178: import java.util.concurrent.ExecutionException;
1:4430178: import java.util.concurrent.ExecutorService;
1:4430178: import java.util.concurrent.Executors;
1:4430178: import java.util.concurrent.Future;
1:94d2089: import java.util.concurrent.atomic.AtomicLong;
1:edb02ab: 
1:91e6f6f: import org.apache.carbondata.core.constants.CarbonCommonConstants;
1:91e6f6f: import org.apache.carbondata.core.constants.CarbonLoadOptionConstants;
1:8f08c4a: import org.apache.carbondata.core.datastore.compression.CompressorFactory;
1:91e6f6f: import org.apache.carbondata.core.metadata.datatype.StructField;
1:91e6f6f: import org.apache.carbondata.core.metadata.datatype.StructType;
1:91e6f6f: import org.apache.carbondata.core.metadata.schema.table.CarbonTable;
1:91e6f6f: import org.apache.carbondata.core.metadata.schema.table.TableInfo;
1:91e6f6f: import org.apache.carbondata.core.util.CarbonProperties;
1:4430178: import org.apache.carbondata.core.util.CarbonThreadFactory;
1:2018048: import org.apache.carbondata.core.util.ObjectSerializationUtil;
1:2a9604c: import org.apache.carbondata.core.util.ThreadLocalSessionInfo;
1:dded5d5: import org.apache.carbondata.hadoop.internal.ObjectArrayWritable;
1:91e6f6f: import org.apache.carbondata.processing.loading.DataLoadExecutor;
1:9550e69: import org.apache.carbondata.processing.loading.TableProcessingOperations;
1:91e6f6f: import org.apache.carbondata.processing.loading.iterator.CarbonOutputIteratorWrapper;
1:91e6f6f: import org.apache.carbondata.processing.loading.model.CarbonDataLoadSchema;
1:91e6f6f: import org.apache.carbondata.processing.loading.model.CarbonLoadModel;
1:91e6f6f: 
1:7a1d12a: import org.apache.commons.lang3.StringUtils;
1:a89587e: import org.apache.commons.logging.Log;
1:a89587e: import org.apache.commons.logging.LogFactory;
1:91e6f6f: import org.apache.hadoop.conf.Configuration;
1:91e6f6f: import org.apache.hadoop.fs.Path;
1:91e6f6f: import org.apache.hadoop.io.NullWritable;
1:91e6f6f: import org.apache.hadoop.mapreduce.OutputCommitter;
1:91e6f6f: import org.apache.hadoop.mapreduce.RecordWriter;
1:91e6f6f: import org.apache.hadoop.mapreduce.TaskAttemptContext;
1:91e6f6f: import org.apache.hadoop.mapreduce.lib.output.FileOutputFormat;
1:91e6f6f: 
1:edb02ab: /**
1:91e6f6f:  * This is table level output format which writes the data to store in new segment. Each load
1:91e6f6f:  * creates new segment folder and manages the folder through tablestatus file.
1:91e6f6f:  * It also generate and writes dictionary data during load only if dictionary server is configured.
1:edb02ab:  */
1:91e6f6f: // TODO Move dictionary generater which is coded in spark to MR framework.
1:dded5d5: public class CarbonTableOutputFormat extends FileOutputFormat<NullWritable, ObjectArrayWritable> {
1:91e6f6f: 
1:91e6f6f:   private static final String LOAD_MODEL = "mapreduce.carbontable.load.model";
1:91e6f6f:   private static final String DATABASE_NAME = "mapreduce.carbontable.databaseName";
1:91e6f6f:   private static final String TABLE_NAME = "mapreduce.carbontable.tableName";
1:91e6f6f:   private static final String TABLE = "mapreduce.carbontable.table";
1:91e6f6f:   private static final String TABLE_PATH = "mapreduce.carbontable.tablepath";
1:91e6f6f:   private static final String INPUT_SCHEMA = "mapreduce.carbontable.inputschema";
1:91e6f6f:   private static final String TEMP_STORE_LOCATIONS = "mapreduce.carbontable.tempstore.locations";
1:91e6f6f:   private static final String OVERWRITE_SET = "mapreduce.carbontable.set.overwrite";
1:91e6f6f:   public static final String COMPLEX_DELIMITERS = "mapreduce.carbontable.complex_delimiters";
1:b7b8073:   private static final String CARBON_TRANSACTIONAL_TABLE =
1:b7b8073:       "mapreduce.input.carboninputformat.transactional";
1:91e6f6f:   public static final String SERIALIZATION_NULL_FORMAT =
1:91e6f6f:       "mapreduce.carbontable.serialization.null.format";
1:91e6f6f:   public static final String BAD_RECORDS_LOGGER_ENABLE =
1:91e6f6f:       "mapreduce.carbontable.bad.records.logger.enable";
1:91e6f6f:   public static final String BAD_RECORDS_LOGGER_ACTION =
1:91e6f6f:       "mapreduce.carbontable.bad.records.logger.action";
1:91e6f6f:   public static final String IS_EMPTY_DATA_BAD_RECORD =
1:91e6f6f:       "mapreduce.carbontable.empty.data.bad.record";
1:91e6f6f:   public static final String SKIP_EMPTY_LINE = "mapreduce.carbontable.skip.empty.line";
1:91e6f6f:   public static final String SORT_SCOPE = "mapreduce.carbontable.load.sort.scope";
1:91e6f6f:   public static final String BATCH_SORT_SIZE_INMB =
1:91e6f6f:       "mapreduce.carbontable.batch.sort.size.inmb";
1:91e6f6f:   public static final String GLOBAL_SORT_PARTITIONS =
1:91e6f6f:       "mapreduce.carbontable.global.sort.partitions";
1:91e6f6f:   public static final String BAD_RECORD_PATH = "mapreduce.carbontable.bad.record.path";
1:91e6f6f:   public static final String DATE_FORMAT = "mapreduce.carbontable.date.format";
1:91e6f6f:   public static final String TIMESTAMP_FORMAT = "mapreduce.carbontable.timestamp.format";
1:91e6f6f:   public static final String IS_ONE_PASS_LOAD = "mapreduce.carbontable.one.pass.load";
1:91e6f6f:   public static final String DICTIONARY_SERVER_HOST =
1:91e6f6f:       "mapreduce.carbontable.dict.server.host";
1:91e6f6f:   public static final String DICTIONARY_SERVER_PORT =
1:91e6f6f:       "mapreduce.carbontable.dict.server.port";
1:a89587e:   /**
1:a89587e:    * Set the update timestamp if user sets in case of update query. It needs to be updated
1:a89587e:    * in load status update time
1:a89587e:    */
1:a89587e:   public static final String UPADTE_TIMESTAMP = "mapreduce.carbontable.update.timestamp";
1:a89587e: 
1:cdf2c02:   /**
1:cdf2c02:    * During update query we first delete the old data and then add updated data to new segment, so
1:cdf2c02:    * sometimes there is a chance that complete segments needs to removed during deletion. We should
1:cdf2c02:    * do 'Mark for delete' for those segments during table status update.
1:cdf2c02:    */
1:cdf2c02:   public static final String SEGMENTS_TO_BE_DELETED =
1:cdf2c02:       "mapreduce.carbontable.segments.to.be.removed";
1:cdf2c02: 
1:829e7aa:   /**
1:829e7aa:    * It is used only to fire events in case of any child tables to be loaded.
1:829e7aa:    */
1:829e7aa:   public static final String OPERATION_CONTEXT = "mapreduce.carbontable.operation.context";
1:829e7aa: 
1:a89587e:   private static final Log LOG = LogFactory.getLog(CarbonTableOutputFormat.class);
1:91e6f6f: 
1:91e6f6f:   private CarbonOutputCommitter committer;
1:91e6f6f: 
1:91e6f6f:   public static void setDatabaseName(Configuration configuration, String databaseName) {
1:91e6f6f:     if (null != databaseName) {
1:91e6f6f:       configuration.set(DATABASE_NAME, databaseName);
1:91e6f6f:     }
1:91e6f6f:   }
1:91e6f6f: 
1:91e6f6f:   public static String getDatabaseName(Configuration configuration) {
1:91e6f6f:     return configuration.get(DATABASE_NAME);
1:91e6f6f:   }
1:91e6f6f: 
1:91e6f6f:   public static void setTableName(Configuration configuration, String tableName) {
1:91e6f6f:     if (null != tableName) {
1:91e6f6f:       configuration.set(TABLE_NAME, tableName);
1:91e6f6f:     }
1:91e6f6f:   }
1:91e6f6f: 
1:91e6f6f:   public static String getTableName(Configuration configuration) {
1:91e6f6f:     return configuration.get(TABLE_NAME);
1:91e6f6f:   }
1:91e6f6f: 
1:91e6f6f:   public static void setTablePath(Configuration configuration, String tablePath) {
1:91e6f6f:     if (null != tablePath) {
1:91e6f6f:       configuration.set(TABLE_PATH, tablePath);
1:91e6f6f:     }
1:91e6f6f:   }
1:91e6f6f: 
1:91e6f6f:   public static String getTablePath(Configuration configuration) {
1:91e6f6f:     return configuration.get(TABLE_PATH);
1:91e6f6f:   }
1:91e6f6f: 
1:91e6f6f:   public static void setCarbonTable(Configuration configuration, CarbonTable carbonTable)
1:91e6f6f:       throws IOException {
1:91e6f6f:     if (carbonTable != null) {
1:91e6f6f:       configuration.set(TABLE,
1:91e6f6f:           ObjectSerializationUtil.convertObjectToString(carbonTable.getTableInfo().serialize()));
1:91e6f6f:     }
1:91e6f6f:   }
1:91e6f6f: 
1:91e6f6f:   public static CarbonTable getCarbonTable(Configuration configuration) throws IOException {
1:91e6f6f:     CarbonTable carbonTable = null;
1:91e6f6f:     String encodedString = configuration.get(TABLE);
1:91e6f6f:     if (encodedString != null) {
1:91e6f6f:       byte[] bytes = (byte[]) ObjectSerializationUtil.convertStringToObject(encodedString);
1:91e6f6f:       TableInfo tableInfo = TableInfo.deserialize(bytes);
1:91e6f6f:       carbonTable = CarbonTable.buildFromTableInfo(tableInfo);
1:91e6f6f:     }
1:91e6f6f:     return carbonTable;
1:91e6f6f:   }
1:91e6f6f: 
1:91e6f6f:   public static void setLoadModel(Configuration configuration, CarbonLoadModel loadModel)
1:91e6f6f:       throws IOException {
1:91e6f6f:     if (loadModel != null) {
1:91e6f6f:       configuration.set(LOAD_MODEL, ObjectSerializationUtil.convertObjectToString(loadModel));
1:91e6f6f:     }
1:91e6f6f:   }
1:91e6f6f: 
1:91e6f6f:   public static void setInputSchema(Configuration configuration, StructType inputSchema)
1:91e6f6f:       throws IOException {
1:91e6f6f:     if (inputSchema != null && inputSchema.getFields().size() > 0) {
1:91e6f6f:       configuration.set(INPUT_SCHEMA, ObjectSerializationUtil.convertObjectToString(inputSchema));
1:91e6f6f:     } else {
1:91e6f6f:       throw new UnsupportedOperationException("Input schema must be set");
1:91e6f6f:     }
1:91e6f6f:   }
1:91e6f6f: 
1:91e6f6f:   private static StructType getInputSchema(Configuration configuration) throws IOException {
1:91e6f6f:     String encodedString = configuration.get(INPUT_SCHEMA);
1:91e6f6f:     if (encodedString != null) {
1:91e6f6f:       return (StructType) ObjectSerializationUtil.convertStringToObject(encodedString);
1:91e6f6f:     }
1:91e6f6f:     return null;
1:91e6f6f:   }
1:91e6f6f: 
1:91e6f6f:   public static boolean isOverwriteSet(Configuration configuration) {
1:91e6f6f:     String overwrite = configuration.get(OVERWRITE_SET);
1:91e6f6f:     if (overwrite != null) {
1:91e6f6f:       return Boolean.parseBoolean(overwrite);
1:91e6f6f:     }
1:91e6f6f:     return false;
1:91e6f6f:   }
1:91e6f6f: 
1:91e6f6f:   public static void setOverwrite(Configuration configuration, boolean overwrite) {
1:91e6f6f:     configuration.set(OVERWRITE_SET, String.valueOf(overwrite));
1:91e6f6f:   }
1:91e6f6f: 
1:91e6f6f:   public static void setTempStoreLocations(Configuration configuration, String[] tempLocations)
1:91e6f6f:       throws IOException {
1:91e6f6f:     if (tempLocations != null && tempLocations.length > 0) {
1:91e6f6f:       configuration
1:91e6f6f:           .set(TEMP_STORE_LOCATIONS, ObjectSerializationUtil.convertObjectToString(tempLocations));
1:91e6f6f:     }
1:91e6f6f:   }
1:91e6f6f: 
1:91e6f6f:   private static String[] getTempStoreLocations(TaskAttemptContext taskAttemptContext)
1:91e6f6f:       throws IOException {
1:91e6f6f:     String encodedString = taskAttemptContext.getConfiguration().get(TEMP_STORE_LOCATIONS);
1:91e6f6f:     if (encodedString != null) {
1:91e6f6f:       return (String[]) ObjectSerializationUtil.convertStringToObject(encodedString);
1:91e6f6f:     }
1:91e6f6f:     return new String[] {
1:91e6f6f:         System.getProperty("java.io.tmpdir") + "/" + System.nanoTime() + "_" + taskAttemptContext
1:91e6f6f:             .getTaskAttemptID().toString() };
1:91e6f6f:   }
1:91e6f6f: 
2:91e6f6f:   @Override
1:91e6f6f:   public synchronized OutputCommitter getOutputCommitter(TaskAttemptContext context)
1:91e6f6f:       throws IOException {
1:91e6f6f:     if (this.committer == null) {
1:91e6f6f:       Path output = getOutputPath(context);
1:91e6f6f:       this.committer = new CarbonOutputCommitter(output, context);
1:91e6f6f:     }
1:91e6f6f:     return this.committer;
1:91e6f6f:   }
1:91e6f6f: 
1:91e6f6f:   @Override
1:dded5d5:   public RecordWriter<NullWritable, ObjectArrayWritable> getRecordWriter(
1:2a9604c:       final TaskAttemptContext taskAttemptContext) throws IOException {
1:91e6f6f:     final CarbonLoadModel loadModel = getLoadModel(taskAttemptContext.getConfiguration());
1:1613471:     //if loadModel having taskNo already(like in SDK) then no need to overwrite
1:94d2089:     short sdkWriterCores = loadModel.getSdkWriterCores();
1:94d2089:     int itrSize = (sdkWriterCores > 0) ? sdkWriterCores : 1;
1:17a4b48:     final CarbonOutputIteratorWrapper[] iterators = new CarbonOutputIteratorWrapper[itrSize];
1:17a4b48:     for (int i = 0; i < itrSize; i++) {
1:17a4b48:       iterators[i] = new CarbonOutputIteratorWrapper();
1:17a4b48:     }
1:1613471:     if (null == loadModel.getTaskNo() || loadModel.getTaskNo().isEmpty()) {
1:1613471:       loadModel.setTaskNo(taskAttemptContext.getConfiguration()
1:1613471:           .get("carbon.outputformat.taskno", String.valueOf(System.nanoTime())));
1:1613471:     }
1:8d3c774:     loadModel.setDataWritePath(
1:8d3c774:         taskAttemptContext.getConfiguration().get("carbon.outputformat.writepath"));
1:91e6f6f:     final String[] tempStoreLocations = getTempStoreLocations(taskAttemptContext);
1:91e6f6f:     final DataLoadExecutor dataLoadExecutor = new DataLoadExecutor();
1:7edef8f:     final ExecutorService executorService = Executors.newFixedThreadPool(1,
1:2a9604c:         new CarbonThreadFactory("CarbonRecordWriter:" + loadModel.getTableName()));
1:4430178:     // It should be started in new thread as the underlying iterator uses blocking queue.
1:4430178:     Future future = executorService.submit(new Thread() {
1:91e6f6f:       @Override public void run() {
1:2a9604c:         ThreadLocalSessionInfo.setConfigurationToCurrentThread(taskAttemptContext
1:2a9604c:             .getConfiguration());
1:91e6f6f:         try {
1:4430178:           dataLoadExecutor
1:17a4b48:               .execute(loadModel, tempStoreLocations, iterators);
1:91e6f6f:         } catch (Exception e) {
1:7edef8f:           executorService.shutdownNow();
1:17a4b48:           for (CarbonOutputIteratorWrapper iterator : iterators) {
1:17a4b48:             iterator.closeWriter(true);
1:17a4b48:           }
2:91e6f6f:           dataLoadExecutor.close();
1:9550e69:           // clean up the folders and files created locally for data load operation
1:9550e69:           TableProcessingOperations.deleteLocalDataLoadFolderLocation(loadModel, false, false);
1:7edef8f: 
1:91e6f6f:           throw new RuntimeException(e);
1:8f1a029:         } finally {
1:8f1a029:           ThreadLocalSessionInfo.unsetAll();
1:91e6f6f:         }
1:91e6f6f:       }
1:4430178:     });
1:91e6f6f: 
1:94d2089:     if (sdkWriterCores > 0) {
1:17a4b48:       // CarbonMultiRecordWriter handles the load balancing of the write rows in round robin.
1:17a4b48:       return new CarbonMultiRecordWriter(iterators, dataLoadExecutor, loadModel, future,
1:17a4b48:           executorService);
1:17a4b48:     } else {
1:17a4b48:       return new CarbonRecordWriter(iterators[0], dataLoadExecutor, loadModel, future,
1:17a4b48:           executorService);
1:17a4b48:     }
1:91e6f6f:   }
1:91e6f6f: 
1:91e6f6f:   public static CarbonLoadModel getLoadModel(Configuration conf) throws IOException {
1:91e6f6f:     CarbonLoadModel model;
1:91e6f6f:     String encodedString = conf.get(LOAD_MODEL);
1:91e6f6f:     if (encodedString != null) {
1:91e6f6f:       model = (CarbonLoadModel) ObjectSerializationUtil.convertStringToObject(encodedString);
1:91e6f6f:       return model;
1:91e6f6f:     }
1:91e6f6f:     model = new CarbonLoadModel();
1:91e6f6f:     CarbonProperties carbonProperty = CarbonProperties.getInstance();
1:91e6f6f:     model.setDatabaseName(CarbonTableOutputFormat.getDatabaseName(conf));
1:91e6f6f:     model.setTableName(CarbonTableOutputFormat.getTableName(conf));
1:b7b8073:     model.setCarbonTransactionalTable(true);
1:7a1d12a:     CarbonTable carbonTable = getCarbonTable(conf);
1:8f08c4a:     String columnCompressor = carbonTable.getTableInfo().getFactTable().getTableProperties().get(
1:8f08c4a:         CarbonCommonConstants.COMPRESSOR);
1:8f08c4a:     if (null == columnCompressor) {
1:8f08c4a:       columnCompressor = CompressorFactory.getInstance().getCompressor().getName();
1:8f08c4a:     }
1:8f08c4a:     model.setColumnCompressor(columnCompressor);
1:7a1d12a:     model.setCarbonDataLoadSchema(new CarbonDataLoadSchema(carbonTable));
1:91e6f6f:     model.setTablePath(getTablePath(conf));
1:91e6f6f:     setFileHeader(conf, model);
1:91e6f6f:     model.setSerializationNullFormat(conf.get(SERIALIZATION_NULL_FORMAT, "\\N"));
1:91e6f6f:     model.setBadRecordsLoggerEnable(
1:91e6f6f:         conf.get(
1:91e6f6f:             BAD_RECORDS_LOGGER_ENABLE,
3:91e6f6f:             carbonProperty.getProperty(
1:91e6f6f:                 CarbonLoadOptionConstants.CARBON_OPTIONS_BAD_RECORDS_LOGGER_ENABLE,
1:91e6f6f:                 CarbonLoadOptionConstants.CARBON_OPTIONS_BAD_RECORDS_LOGGER_ENABLE_DEFAULT)));
1:91e6f6f:     model.setBadRecordsAction(
1:91e6f6f:         conf.get(
1:91e6f6f:             BAD_RECORDS_LOGGER_ACTION,
1:91e6f6f:             carbonProperty.getProperty(
1:91e6f6f:                 CarbonCommonConstants.CARBON_BAD_RECORDS_ACTION,
1:91e6f6f:                 CarbonCommonConstants.CARBON_BAD_RECORDS_ACTION_DEFAULT)));
1:91e6f6f: 
1:91e6f6f:     model.setIsEmptyDataBadRecord(
1:91e6f6f:         conf.get(
1:91e6f6f:             IS_EMPTY_DATA_BAD_RECORD,
1:91e6f6f:             carbonProperty.getProperty(
1:91e6f6f:                 CarbonLoadOptionConstants.CARBON_OPTIONS_IS_EMPTY_DATA_BAD_RECORD,
1:91e6f6f:                 CarbonLoadOptionConstants.CARBON_OPTIONS_IS_EMPTY_DATA_BAD_RECORD_DEFAULT)));
1:91e6f6f: 
1:91e6f6f:     model.setSkipEmptyLine(
1:91e6f6f:         conf.get(
1:91e6f6f:             SKIP_EMPTY_LINE,
1:91e6f6f:             carbonProperty.getProperty(CarbonLoadOptionConstants.CARBON_OPTIONS_SKIP_EMPTY_LINE)));
1:91e6f6f: 
1:088465f:     String complexDelim = conf.get(COMPLEX_DELIMITERS, "$" + "," + ":");
1:91e6f6f:     String[] split = complexDelim.split(",");
1:91e6f6f:     model.setComplexDelimiterLevel1(split[0]);
1:91e6f6f:     if (split.length > 1) {
1:088465f:       model.setComplexDelimiterLevel2(split[1]);
1:91e6f6f:     }
1:91e6f6f:     model.setDateFormat(
1:91e6f6f:         conf.get(
1:91e6f6f:             DATE_FORMAT,
1:91e6f6f:             carbonProperty.getProperty(
1:91e6f6f:                 CarbonLoadOptionConstants.CARBON_OPTIONS_DATEFORMAT,
1:91e6f6f:                 CarbonLoadOptionConstants.CARBON_OPTIONS_DATEFORMAT_DEFAULT)));
1:91e6f6f: 
1:91e6f6f:     model.setTimestampformat(
1:91e6f6f:         conf.get(
1:91e6f6f:             TIMESTAMP_FORMAT,
1:91e6f6f:             carbonProperty.getProperty(
1:91e6f6f:                 CarbonLoadOptionConstants.CARBON_OPTIONS_TIMESTAMPFORMAT,
1:91e6f6f:                 CarbonLoadOptionConstants.CARBON_OPTIONS_TIMESTAMPFORMAT_DEFAULT)));
1:91e6f6f: 
1:91e6f6f:     model.setGlobalSortPartitions(
1:91e6f6f:         conf.get(
1:91e6f6f:             GLOBAL_SORT_PARTITIONS,
1:91e6f6f:             carbonProperty.getProperty(
1:91e6f6f:                 CarbonLoadOptionConstants.CARBON_OPTIONS_GLOBAL_SORT_PARTITIONS,
1:91e6f6f:                 null)));
1:91e6f6f: 
1:91e6f6f:     model.setBatchSortSizeInMb(
1:91e6f6f:         conf.get(
1:91e6f6f:             BATCH_SORT_SIZE_INMB,
1:91e6f6f:             carbonProperty.getProperty(
1:91e6f6f:                 CarbonLoadOptionConstants.CARBON_OPTIONS_BATCH_SORT_SIZE_INMB,
1:91e6f6f:                 carbonProperty.getProperty(
1:91e6f6f:                     CarbonCommonConstants.LOAD_BATCH_SORT_SIZE_INMB,
1:91e6f6f:                     CarbonCommonConstants.LOAD_BATCH_SORT_SIZE_INMB_DEFAULT))));
1:91e6f6f: 
1:7a1d12a:     String badRecordsPath = conf.get(BAD_RECORD_PATH);
1:7a1d12a:     if (StringUtils.isEmpty(badRecordsPath)) {
1:7a1d12a:       badRecordsPath =
1:7a1d12a:           carbonTable.getTableInfo().getFactTable().getTableProperties().get("bad_records_path");
1:7a1d12a:       if (StringUtils.isEmpty(badRecordsPath)) {
1:7a1d12a:         badRecordsPath = carbonProperty
1:7a1d12a:             .getProperty(CarbonLoadOptionConstants.CARBON_OPTIONS_BAD_RECORD_PATH, carbonProperty
1:7a1d12a:                 .getProperty(CarbonCommonConstants.CARBON_BADRECORDS_LOC,
1:7a1d12a:                     CarbonCommonConstants.CARBON_BADRECORDS_LOC_DEFAULT_VAL));
1:7a1d12a:       }
1:7a1d12a:     }
1:7a1d12a:     model.setBadRecordsLocation(badRecordsPath);
1:91e6f6f:     model.setUseOnePass(
1:91e6f6f:         conf.getBoolean(IS_ONE_PASS_LOAD,
1:91e6f6f:             Boolean.parseBoolean(
1:91e6f6f:                 carbonProperty.getProperty(
1:91e6f6f:                     CarbonLoadOptionConstants.CARBON_OPTIONS_SINGLE_PASS,
1:91e6f6f:                     CarbonLoadOptionConstants.CARBON_OPTIONS_SINGLE_PASS_DEFAULT))));
1:91e6f6f:     return model;
1:91e6f6f:   }
1:91e6f6f: 
1:91e6f6f:   private static void setFileHeader(Configuration configuration, CarbonLoadModel model)
1:91e6f6f:       throws IOException {
1:91e6f6f:     StructType inputSchema = getInputSchema(configuration);
1:91e6f6f:     if (inputSchema == null || inputSchema.getFields().size() == 0) {
1:91e6f6f:       throw new UnsupportedOperationException("Input schema must be set");
1:91e6f6f:     }
1:91e6f6f:     List<StructField> fields = inputSchema.getFields();
1:91e6f6f:     StringBuilder builder = new StringBuilder();
1:91e6f6f:     String[] columns = new String[fields.size()];
1:91e6f6f:     int i = 0;
1:91e6f6f:     for (StructField field : fields) {
1:91e6f6f:       builder.append(field.getFieldName());
1:91e6f6f:       builder.append(",");
1:91e6f6f:       columns[i++] = field.getFieldName();
1:91e6f6f:     }
1:91e6f6f:     String header = builder.toString();
1:91e6f6f:     model.setCsvHeader(header.substring(0, header.length() - 1));
1:91e6f6f:     model.setCsvHeaderColumns(columns);
1:91e6f6f:   }
1:91e6f6f: 
1:dded5d5:   public static class CarbonRecordWriter extends RecordWriter<NullWritable, ObjectArrayWritable> {
1:91e6f6f: 
1:91e6f6f:     private CarbonOutputIteratorWrapper iteratorWrapper;
1:91e6f6f: 
1:91e6f6f:     private DataLoadExecutor dataLoadExecutor;
1:91e6f6f: 
1:4430178:     private CarbonLoadModel loadModel;
1:4430178: 
1:4430178:     private ExecutorService executorService;
1:4430178: 
1:4430178:     private Future future;
1:4430178: 
1:91e6f6f:     public CarbonRecordWriter(CarbonOutputIteratorWrapper iteratorWrapper,
1:4430178:         DataLoadExecutor dataLoadExecutor, CarbonLoadModel loadModel, Future future,
1:4430178:         ExecutorService executorService) {
1:91e6f6f:       this.iteratorWrapper = iteratorWrapper;
1:91e6f6f:       this.dataLoadExecutor = dataLoadExecutor;
1:4430178:       this.loadModel = loadModel;
1:4430178:       this.executorService = executorService;
1:4430178:       this.future = future;
1:91e6f6f:     }
1:91e6f6f: 
1:dded5d5:     @Override public void write(NullWritable aVoid, ObjectArrayWritable objects)
1:91e6f6f:         throws InterruptedException {
1:17a4b48:       if (iteratorWrapper != null) {
1:17a4b48:         iteratorWrapper.write(objects.get());
1:17a4b48:       }
1:91e6f6f:     }
1:91e6f6f: 
1:4430178:     @Override public void close(TaskAttemptContext taskAttemptContext) throws InterruptedException {
1:17a4b48:       if (iteratorWrapper != null) {
1:17a4b48:         iteratorWrapper.closeWriter(false);
1:17a4b48:       }
1:4430178:       try {
1:4430178:         future.get();
1:4430178:       } catch (ExecutionException e) {
1:a89587e:         LOG.error("Error while loading data", e);
1:4430178:         throw new InterruptedException(e.getMessage());
1:4430178:       } finally {
1:4430178:         executorService.shutdownNow();
1:4430178:         dataLoadExecutor.close();
1:8f1a029:         ThreadLocalSessionInfo.unsetAll();
1:9550e69:         // clean up the folders and files created locally for data load operation
1:9550e69:         TableProcessingOperations.deleteLocalDataLoadFolderLocation(loadModel, false, false);
1:4430178:       }
1:7edef8f:       LOG.info("Closed writer task " + taskAttemptContext.getTaskAttemptID());
1:4430178:     }
1:4430178: 
1:4430178:     public CarbonLoadModel getLoadModel() {
1:4430178:       return loadModel;
1:91e6f6f:     }
1:edb02ab:   }
1:17a4b48: 
1:17a4b48:   /* CarbonMultiRecordWriter takes multiple iterators
1:17a4b48:   and handles the load balancing of the write rows in round robin. */
1:17a4b48:   public static class CarbonMultiRecordWriter extends CarbonRecordWriter {
1:17a4b48: 
1:17a4b48:     private CarbonOutputIteratorWrapper[] iterators;
1:17a4b48: 
1:94d2089:     // keep counts of number of writes called
1:94d2089:     // and it is used to load balance each write call to one iterator.
1:94d2089:     private AtomicLong counter;
1:17a4b48: 
1:17a4b48:     CarbonMultiRecordWriter(CarbonOutputIteratorWrapper[] iterators,
1:17a4b48:         DataLoadExecutor dataLoadExecutor, CarbonLoadModel loadModel, Future future,
1:17a4b48:         ExecutorService executorService) {
1:17a4b48:       super(null, dataLoadExecutor, loadModel, future, executorService);
1:17a4b48:       this.iterators = iterators;
1:94d2089:       counter = new AtomicLong(0);
1:17a4b48:     }
1:17a4b48: 
1:94d2089:     @Override public void write(NullWritable aVoid, ObjectArrayWritable objects)
1:17a4b48:         throws InterruptedException {
1:94d2089:       int iteratorNum = (int) (counter.incrementAndGet() % iterators.length);
1:94d2089:       synchronized (iterators[iteratorNum]) {
1:94d2089:         iterators[iteratorNum].write(objects.get());
1:17a4b48:       }
1:17a4b48:     }
1:17a4b48: 
1:17a4b48:     @Override public void close(TaskAttemptContext taskAttemptContext) throws InterruptedException {
1:94d2089:       for (int i = 0; i < iterators.length; i++) {
1:94d2089:         synchronized (iterators[i]) {
1:94d2089:           iterators[i].closeWriter(false);
1:94d2089:         }
1:17a4b48:       }
1:17a4b48:       super.close(taskAttemptContext);
1:17a4b48:     }
1:17a4b48:   }
1:edb02ab: }
============================================================================
author:xuchuanyin
-------------------------------------------------------------------------------
commit:8f08c4a
/////////////////////////////////////////////////////////////////////////
1: import org.apache.carbondata.core.datastore.compression.CompressorFactory;
/////////////////////////////////////////////////////////////////////////
1:     String columnCompressor = carbonTable.getTableInfo().getFactTable().getTableProperties().get(
1:         CarbonCommonConstants.COMPRESSOR);
1:     if (null == columnCompressor) {
1:       columnCompressor = CompressorFactory.getInstance().getCompressor().getName();
1:     }
1:     model.setColumnCompressor(columnCompressor);
author:kunal642
-------------------------------------------------------------------------------
commit:8f1a029
/////////////////////////////////////////////////////////////////////////
1:         } finally {
1:           ThreadLocalSessionInfo.unsetAll();
/////////////////////////////////////////////////////////////////////////
1:         ThreadLocalSessionInfo.unsetAll();
commit:2a9604c
/////////////////////////////////////////////////////////////////////////
1: import org.apache.carbondata.core.util.ThreadLocalSessionInfo;
/////////////////////////////////////////////////////////////////////////
1:       final TaskAttemptContext taskAttemptContext) throws IOException {
/////////////////////////////////////////////////////////////////////////
1:         new CarbonThreadFactory("CarbonRecordWriter:" + loadModel.getTableName()));
1:         ThreadLocalSessionInfo.setConfigurationToCurrentThread(taskAttemptContext
1:             .getConfiguration());
commit:7a1d12a
/////////////////////////////////////////////////////////////////////////
1: import org.apache.commons.lang3.StringUtils;
/////////////////////////////////////////////////////////////////////////
1:     CarbonTable carbonTable = getCarbonTable(conf);
1:     model.setCarbonDataLoadSchema(new CarbonDataLoadSchema(carbonTable));
/////////////////////////////////////////////////////////////////////////
1:     String badRecordsPath = conf.get(BAD_RECORD_PATH);
1:     if (StringUtils.isEmpty(badRecordsPath)) {
1:       badRecordsPath =
1:           carbonTable.getTableInfo().getFactTable().getTableProperties().get("bad_records_path");
1:       if (StringUtils.isEmpty(badRecordsPath)) {
1:         badRecordsPath = carbonProperty
1:             .getProperty(CarbonLoadOptionConstants.CARBON_OPTIONS_BAD_RECORD_PATH, carbonProperty
1:                 .getProperty(CarbonCommonConstants.CARBON_BADRECORDS_LOC,
1:                     CarbonCommonConstants.CARBON_BADRECORDS_LOC_DEFAULT_VAL));
1:       }
1:     }
1:     model.setBadRecordsLocation(badRecordsPath);
author:ajantha-bhat
-------------------------------------------------------------------------------
commit:94d2089
/////////////////////////////////////////////////////////////////////////
1: import java.util.concurrent.atomic.AtomicLong;
/////////////////////////////////////////////////////////////////////////
1:     short sdkWriterCores = loadModel.getSdkWriterCores();
1:     int itrSize = (sdkWriterCores > 0) ? sdkWriterCores : 1;
/////////////////////////////////////////////////////////////////////////
1:     if (sdkWriterCores > 0) {
/////////////////////////////////////////////////////////////////////////
1:     // keep counts of number of writes called
1:     // and it is used to load balance each write call to one iterator.
1:     private AtomicLong counter;
1:       counter = new AtomicLong(0);
1:     @Override public void write(NullWritable aVoid, ObjectArrayWritable objects)
1:       int iteratorNum = (int) (counter.incrementAndGet() % iterators.length);
1:       synchronized (iterators[iteratorNum]) {
1:         iterators[iteratorNum].write(objects.get());
1:       for (int i = 0; i < iterators.length; i++) {
1:         synchronized (iterators[i]) {
1:           iterators[i].closeWriter(false);
1:         }
commit:17a4b48
/////////////////////////////////////////////////////////////////////////
/////////////////////////////////////////////////////////////////////////
0:     short sdkUserCore = loadModel.getSdkUserCores();
0:     int itrSize = (sdkUserCore > 0) ? sdkUserCore : 1;
1:     final CarbonOutputIteratorWrapper[] iterators = new CarbonOutputIteratorWrapper[itrSize];
1:     for (int i = 0; i < itrSize; i++) {
1:       iterators[i] = new CarbonOutputIteratorWrapper();
1:     }
/////////////////////////////////////////////////////////////////////////
/////////////////////////////////////////////////////////////////////////
1:               .execute(loadModel, tempStoreLocations, iterators);
1:           for (CarbonOutputIteratorWrapper iterator : iterators) {
1:             iterator.closeWriter(true);
1:           }
/////////////////////////////////////////////////////////////////////////
0:     if (sdkUserCore > 0) {
1:       // CarbonMultiRecordWriter handles the load balancing of the write rows in round robin.
1:       return new CarbonMultiRecordWriter(iterators, dataLoadExecutor, loadModel, future,
1:           executorService);
1:     } else {
1:       return new CarbonRecordWriter(iterators[0], dataLoadExecutor, loadModel, future,
1:           executorService);
1:     }
/////////////////////////////////////////////////////////////////////////
1:       if (iteratorWrapper != null) {
1:         iteratorWrapper.write(objects.get());
1:       }
1:       if (iteratorWrapper != null) {
1:         iteratorWrapper.closeWriter(false);
1:       }
/////////////////////////////////////////////////////////////////////////
1: 
1:   /* CarbonMultiRecordWriter takes multiple iterators
1:   and handles the load balancing of the write rows in round robin. */
1:   public static class CarbonMultiRecordWriter extends CarbonRecordWriter {
1: 
1:     private CarbonOutputIteratorWrapper[] iterators;
1: 
0:     private int counter;
1: 
1:     CarbonMultiRecordWriter(CarbonOutputIteratorWrapper[] iterators,
1:         DataLoadExecutor dataLoadExecutor, CarbonLoadModel loadModel, Future future,
1:         ExecutorService executorService) {
1:       super(null, dataLoadExecutor, loadModel, future, executorService);
1:       this.iterators = iterators;
1:     }
1: 
0:     @Override public synchronized void write(NullWritable aVoid, ObjectArrayWritable objects)
1:         throws InterruptedException {
0:       iterators[counter].write(objects.get());
0:       if (++counter == iterators.length) {
0:         //round robin reset
0:         counter = 0;
1:       }
1:     }
1: 
1:     @Override public void close(TaskAttemptContext taskAttemptContext) throws InterruptedException {
0:       for (CarbonOutputIteratorWrapper itr : iterators) {
0:         itr.closeWriter(false);
1:       }
1:       super.close(taskAttemptContext);
1:     }
1:   }
commit:7edef8f
/////////////////////////////////////////////////////////////////////////
1:     final ExecutorService executorService = Executors.newFixedThreadPool(1,
/////////////////////////////////////////////////////////////////////////
1:           executorService.shutdownNow();
1: 
0:           iteratorWrapper.closeWriter(true);
/////////////////////////////////////////////////////////////////////////
0:       iteratorWrapper.closeWriter(false);
/////////////////////////////////////////////////////////////////////////
1:       LOG.info("Closed writer task " + taskAttemptContext.getTaskAttemptID());
author:BJangir
-------------------------------------------------------------------------------
commit:3f33276
/////////////////////////////////////////////////////////////////////////
0:           iteratorWrapper.closeWriter(true);
commit:1613471
/////////////////////////////////////////////////////////////////////////
1:     //if loadModel having taskNo already(like in SDK) then no need to overwrite
1:     if (null == loadModel.getTaskNo() || loadModel.getTaskNo().isEmpty()) {
1:       loadModel.setTaskNo(taskAttemptContext.getConfiguration()
1:           .get("carbon.outputformat.taskno", String.valueOf(System.nanoTime())));
1:     }
author:akashrn5
-------------------------------------------------------------------------------
commit:2018048
/////////////////////////////////////////////////////////////////////////
1: import org.apache.carbondata.core.util.ObjectSerializationUtil;
author:sounakr
-------------------------------------------------------------------------------
commit:b7b8073
/////////////////////////////////////////////////////////////////////////
1:   private static final String CARBON_TRANSACTIONAL_TABLE =
1:       "mapreduce.input.carboninputformat.transactional";
/////////////////////////////////////////////////////////////////////////
1:     model.setCarbonTransactionalTable(true);
author:ravipesala
-------------------------------------------------------------------------------
commit:8d3c774
/////////////////////////////////////////////////////////////////////////
1:     loadModel.setDataWritePath(
1:         taskAttemptContext.getConfiguration().get("carbon.outputformat.writepath"));
commit:dded5d5
/////////////////////////////////////////////////////////////////////////
1: import org.apache.carbondata.hadoop.internal.ObjectArrayWritable;
/////////////////////////////////////////////////////////////////////////
1: public class CarbonTableOutputFormat extends FileOutputFormat<NullWritable, ObjectArrayWritable> {
/////////////////////////////////////////////////////////////////////////
1:   public RecordWriter<NullWritable, ObjectArrayWritable> getRecordWriter(
/////////////////////////////////////////////////////////////////////////
1:   public static class CarbonRecordWriter extends RecordWriter<NullWritable, ObjectArrayWritable> {
/////////////////////////////////////////////////////////////////////////
1:     @Override public void write(NullWritable aVoid, ObjectArrayWritable objects)
0:       iteratorWrapper.write(objects.get());
commit:9550e69
/////////////////////////////////////////////////////////////////////////
1: import org.apache.carbondata.processing.loading.TableProcessingOperations;
/////////////////////////////////////////////////////////////////////////
0:     loadModel.setTaskNo(taskAttemptContext.getConfiguration().get(
0:         "carbon.outputformat.taskno",
0:         String.valueOf(System.nanoTime())));
/////////////////////////////////////////////////////////////////////////
1:           // clean up the folders and files created locally for data load operation
1:           TableProcessingOperations.deleteLocalDataLoadFolderLocation(loadModel, false, false);
/////////////////////////////////////////////////////////////////////////
1:         // clean up the folders and files created locally for data load operation
1:         TableProcessingOperations.deleteLocalDataLoadFolderLocation(loadModel, false, false);
commit:829e7aa
/////////////////////////////////////////////////////////////////////////
1:   /**
1:    * It is used only to fire events in case of any child tables to be loaded.
1:    */
1:   public static final String OPERATION_CONTEXT = "mapreduce.carbontable.operation.context";
1: 
commit:cdf2c02
/////////////////////////////////////////////////////////////////////////
1:   /**
1:    * During update query we first delete the old data and then add updated data to new segment, so
1:    * sometimes there is a chance that complete segments needs to removed during deletion. We should
1:    * do 'Mark for delete' for those segments during table status update.
1:    */
1:   public static final String SEGMENTS_TO_BE_DELETED =
1:       "mapreduce.carbontable.segments.to.be.removed";
1: 
commit:a89587e
/////////////////////////////////////////////////////////////////////////
1: import org.apache.commons.logging.Log;
1: import org.apache.commons.logging.LogFactory;
/////////////////////////////////////////////////////////////////////////
1:   /**
1:    * Set the update timestamp if user sets in case of update query. It needs to be updated
1:    * in load status update time
1:    */
1:   public static final String UPADTE_TIMESTAMP = "mapreduce.carbontable.update.timestamp";
1: 
1:   private static final Log LOG = LogFactory.getLog(CarbonTableOutputFormat.class);
/////////////////////////////////////////////////////////////////////////
0:       iteratorWrapper.closeWriter();
1:         LOG.error("Error while loading data", e);
0:       LOG.info("Closed partition writer task " + taskAttemptContext.getTaskAttemptID());
commit:4430178
/////////////////////////////////////////////////////////////////////////
1: import java.util.concurrent.ExecutionException;
1: import java.util.concurrent.ExecutorService;
1: import java.util.concurrent.Executors;
1: import java.util.concurrent.Future;
/////////////////////////////////////////////////////////////////////////
1: import org.apache.carbondata.core.util.CarbonThreadFactory;
/////////////////////////////////////////////////////////////////////////
0:     loadModel.setTaskNo(System.nanoTime() + "");
0:     ExecutorService executorService = Executors.newFixedThreadPool(1,
0:         new CarbonThreadFactory("CarbonRecordWriter:" + loadModel.getTableName()));;
1:     // It should be started in new thread as the underlying iterator uses blocking queue.
1:     Future future = executorService.submit(new Thread() {
1:           dataLoadExecutor
0:               .execute(loadModel, tempStoreLocations, new CarbonIterator[] { iteratorWrapper });
1:     });
0:     return new CarbonRecordWriter(iteratorWrapper, dataLoadExecutor, loadModel, future,
0:         executorService);
/////////////////////////////////////////////////////////////////////////
0:   public static class CarbonRecordWriter extends RecordWriter<NullWritable, StringArrayWritable> {
1:     private CarbonLoadModel loadModel;
1: 
1:     private ExecutorService executorService;
1: 
1:     private Future future;
1: 
1:         DataLoadExecutor dataLoadExecutor, CarbonLoadModel loadModel, Future future,
1:         ExecutorService executorService) {
1:       this.loadModel = loadModel;
1:       this.executorService = executorService;
1:       this.future = future;
0:     @Override public void write(NullWritable aVoid, StringArrayWritable strings)
1:     @Override public void close(TaskAttemptContext taskAttemptContext) throws InterruptedException {
1:       try {
1:         future.get();
1:       } catch (ExecutionException e) {
1:         throw new InterruptedException(e.getMessage());
1:       } finally {
1:         executorService.shutdownNow();
1:         dataLoadExecutor.close();
1:       }
1:     }
1: 
1:     public CarbonLoadModel getLoadModel() {
1:       return loadModel;
commit:91e6f6f
/////////////////////////////////////////////////////////////////////////
1: import java.util.List;
0: import org.apache.carbondata.common.CarbonIterator;
1: import org.apache.carbondata.core.constants.CarbonCommonConstants;
1: import org.apache.carbondata.core.constants.CarbonLoadOptionConstants;
1: import org.apache.carbondata.core.metadata.datatype.StructField;
1: import org.apache.carbondata.core.metadata.datatype.StructType;
1: import org.apache.carbondata.core.metadata.schema.table.CarbonTable;
1: import org.apache.carbondata.core.metadata.schema.table.TableInfo;
1: import org.apache.carbondata.core.util.CarbonProperties;
0: import org.apache.carbondata.hadoop.util.ObjectSerializationUtil;
1: import org.apache.carbondata.processing.loading.DataLoadExecutor;
0: import org.apache.carbondata.processing.loading.csvinput.StringArrayWritable;
1: import org.apache.carbondata.processing.loading.iterator.CarbonOutputIteratorWrapper;
1: import org.apache.carbondata.processing.loading.model.CarbonDataLoadSchema;
1: import org.apache.carbondata.processing.loading.model.CarbonLoadModel;
1: 
1: import org.apache.hadoop.conf.Configuration;
1: import org.apache.hadoop.fs.Path;
1: import org.apache.hadoop.io.NullWritable;
1: import org.apache.hadoop.mapreduce.OutputCommitter;
1: import org.apache.hadoop.mapreduce.RecordWriter;
1: import org.apache.hadoop.mapreduce.TaskAttemptContext;
1: import org.apache.hadoop.mapreduce.lib.output.FileOutputFormat;
1:  * This is table level output format which writes the data to store in new segment. Each load
1:  * creates new segment folder and manages the folder through tablestatus file.
1:  * It also generate and writes dictionary data during load only if dictionary server is configured.
1: // TODO Move dictionary generater which is coded in spark to MR framework.
0: public class CarbonTableOutputFormat extends FileOutputFormat<NullWritable, StringArrayWritable> {
1: 
1:   private static final String LOAD_MODEL = "mapreduce.carbontable.load.model";
1:   private static final String DATABASE_NAME = "mapreduce.carbontable.databaseName";
1:   private static final String TABLE_NAME = "mapreduce.carbontable.tableName";
1:   private static final String TABLE = "mapreduce.carbontable.table";
1:   private static final String TABLE_PATH = "mapreduce.carbontable.tablepath";
1:   private static final String INPUT_SCHEMA = "mapreduce.carbontable.inputschema";
1:   private static final String TEMP_STORE_LOCATIONS = "mapreduce.carbontable.tempstore.locations";
1:   private static final String OVERWRITE_SET = "mapreduce.carbontable.set.overwrite";
1:   public static final String COMPLEX_DELIMITERS = "mapreduce.carbontable.complex_delimiters";
1:   public static final String SERIALIZATION_NULL_FORMAT =
1:       "mapreduce.carbontable.serialization.null.format";
1:   public static final String BAD_RECORDS_LOGGER_ENABLE =
1:       "mapreduce.carbontable.bad.records.logger.enable";
1:   public static final String BAD_RECORDS_LOGGER_ACTION =
1:       "mapreduce.carbontable.bad.records.logger.action";
1:   public static final String IS_EMPTY_DATA_BAD_RECORD =
1:       "mapreduce.carbontable.empty.data.bad.record";
1:   public static final String SKIP_EMPTY_LINE = "mapreduce.carbontable.skip.empty.line";
1:   public static final String SORT_SCOPE = "mapreduce.carbontable.load.sort.scope";
1:   public static final String BATCH_SORT_SIZE_INMB =
1:       "mapreduce.carbontable.batch.sort.size.inmb";
1:   public static final String GLOBAL_SORT_PARTITIONS =
1:       "mapreduce.carbontable.global.sort.partitions";
1:   public static final String BAD_RECORD_PATH = "mapreduce.carbontable.bad.record.path";
1:   public static final String DATE_FORMAT = "mapreduce.carbontable.date.format";
1:   public static final String TIMESTAMP_FORMAT = "mapreduce.carbontable.timestamp.format";
1:   public static final String IS_ONE_PASS_LOAD = "mapreduce.carbontable.one.pass.load";
1:   public static final String DICTIONARY_SERVER_HOST =
1:       "mapreduce.carbontable.dict.server.host";
1:   public static final String DICTIONARY_SERVER_PORT =
1:       "mapreduce.carbontable.dict.server.port";
1: 
1:   private CarbonOutputCommitter committer;
1: 
1:   public static void setDatabaseName(Configuration configuration, String databaseName) {
1:     if (null != databaseName) {
1:       configuration.set(DATABASE_NAME, databaseName);
1:     }
1:   }
1: 
1:   public static String getDatabaseName(Configuration configuration) {
1:     return configuration.get(DATABASE_NAME);
1:   }
1: 
1:   public static void setTableName(Configuration configuration, String tableName) {
1:     if (null != tableName) {
1:       configuration.set(TABLE_NAME, tableName);
1:     }
1:   }
1: 
1:   public static String getTableName(Configuration configuration) {
1:     return configuration.get(TABLE_NAME);
1:   }
1: 
1:   public static void setTablePath(Configuration configuration, String tablePath) {
1:     if (null != tablePath) {
1:       configuration.set(TABLE_PATH, tablePath);
1:     }
1:   }
1: 
1:   public static String getTablePath(Configuration configuration) {
1:     return configuration.get(TABLE_PATH);
1:   }
1: 
1:   public static void setCarbonTable(Configuration configuration, CarbonTable carbonTable)
1:       throws IOException {
1:     if (carbonTable != null) {
1:       configuration.set(TABLE,
1:           ObjectSerializationUtil.convertObjectToString(carbonTable.getTableInfo().serialize()));
1:     }
1:   }
1: 
1:   public static CarbonTable getCarbonTable(Configuration configuration) throws IOException {
1:     CarbonTable carbonTable = null;
1:     String encodedString = configuration.get(TABLE);
1:     if (encodedString != null) {
1:       byte[] bytes = (byte[]) ObjectSerializationUtil.convertStringToObject(encodedString);
1:       TableInfo tableInfo = TableInfo.deserialize(bytes);
1:       carbonTable = CarbonTable.buildFromTableInfo(tableInfo);
1:     }
1:     return carbonTable;
1:   }
1: 
1:   public static void setLoadModel(Configuration configuration, CarbonLoadModel loadModel)
1:       throws IOException {
1:     if (loadModel != null) {
1:       configuration.set(LOAD_MODEL, ObjectSerializationUtil.convertObjectToString(loadModel));
1:     }
1:   }
1: 
1:   public static void setInputSchema(Configuration configuration, StructType inputSchema)
1:       throws IOException {
1:     if (inputSchema != null && inputSchema.getFields().size() > 0) {
1:       configuration.set(INPUT_SCHEMA, ObjectSerializationUtil.convertObjectToString(inputSchema));
1:     } else {
1:       throw new UnsupportedOperationException("Input schema must be set");
1:     }
1:   }
1: 
1:   private static StructType getInputSchema(Configuration configuration) throws IOException {
1:     String encodedString = configuration.get(INPUT_SCHEMA);
1:     if (encodedString != null) {
1:       return (StructType) ObjectSerializationUtil.convertStringToObject(encodedString);
1:     }
1:     return null;
1:   }
1: 
1:   public static boolean isOverwriteSet(Configuration configuration) {
1:     String overwrite = configuration.get(OVERWRITE_SET);
1:     if (overwrite != null) {
1:       return Boolean.parseBoolean(overwrite);
1:     }
1:     return false;
1:   }
1: 
1:   public static void setOverwrite(Configuration configuration, boolean overwrite) {
1:     configuration.set(OVERWRITE_SET, String.valueOf(overwrite));
1:   }
1: 
1:   public static void setTempStoreLocations(Configuration configuration, String[] tempLocations)
1:       throws IOException {
1:     if (tempLocations != null && tempLocations.length > 0) {
1:       configuration
1:           .set(TEMP_STORE_LOCATIONS, ObjectSerializationUtil.convertObjectToString(tempLocations));
1:     }
1:   }
1: 
1:   private static String[] getTempStoreLocations(TaskAttemptContext taskAttemptContext)
1:       throws IOException {
1:     String encodedString = taskAttemptContext.getConfiguration().get(TEMP_STORE_LOCATIONS);
1:     if (encodedString != null) {
1:       return (String[]) ObjectSerializationUtil.convertStringToObject(encodedString);
1:     }
1:     return new String[] {
1:         System.getProperty("java.io.tmpdir") + "/" + System.nanoTime() + "_" + taskAttemptContext
1:             .getTaskAttemptID().toString() };
1:   }
1:   public synchronized OutputCommitter getOutputCommitter(TaskAttemptContext context)
1:       throws IOException {
1:     if (this.committer == null) {
1:       Path output = getOutputPath(context);
1:       this.committer = new CarbonOutputCommitter(output, context);
1:     }
1:     return this.committer;
1:   }
1: 
1:   @Override
0:   public RecordWriter<NullWritable, StringArrayWritable> getRecordWriter(
0:       TaskAttemptContext taskAttemptContext) throws IOException {
1:     final CarbonLoadModel loadModel = getLoadModel(taskAttemptContext.getConfiguration());
0:     loadModel.setTaskNo(taskAttemptContext.getTaskAttemptID().getTaskID().getId() + "");
1:     final String[] tempStoreLocations = getTempStoreLocations(taskAttemptContext);
0:     final CarbonOutputIteratorWrapper iteratorWrapper = new CarbonOutputIteratorWrapper();
1:     final DataLoadExecutor dataLoadExecutor = new DataLoadExecutor();
0:     CarbonRecordWriter recordWriter = new CarbonRecordWriter(iteratorWrapper, dataLoadExecutor);
0:     new Thread() {
1:       @Override public void run() {
1:         try {
0:           dataLoadExecutor.execute(
0:               loadModel,
0:               tempStoreLocations,
0:               new CarbonIterator[] { iteratorWrapper });
1:         } catch (Exception e) {
1:           dataLoadExecutor.close();
1:           throw new RuntimeException(e);
1:         }
1:       }
0:     }.start();
1: 
0:     return recordWriter;
1:   }
1: 
1:   public static CarbonLoadModel getLoadModel(Configuration conf) throws IOException {
1:     CarbonLoadModel model;
1:     String encodedString = conf.get(LOAD_MODEL);
1:     if (encodedString != null) {
1:       model = (CarbonLoadModel) ObjectSerializationUtil.convertStringToObject(encodedString);
1:       return model;
1:     }
1:     model = new CarbonLoadModel();
1:     CarbonProperties carbonProperty = CarbonProperties.getInstance();
1:     model.setDatabaseName(CarbonTableOutputFormat.getDatabaseName(conf));
1:     model.setTableName(CarbonTableOutputFormat.getTableName(conf));
0:     model.setCarbonDataLoadSchema(new CarbonDataLoadSchema(getCarbonTable(conf)));
1:     model.setTablePath(getTablePath(conf));
1: 
1:     setFileHeader(conf, model);
1:     model.setSerializationNullFormat(conf.get(SERIALIZATION_NULL_FORMAT, "\\N"));
1:     model.setBadRecordsLoggerEnable(
1:         conf.get(
1:             BAD_RECORDS_LOGGER_ENABLE,
1:             carbonProperty.getProperty(
1:                 CarbonLoadOptionConstants.CARBON_OPTIONS_BAD_RECORDS_LOGGER_ENABLE,
1:                 CarbonLoadOptionConstants.CARBON_OPTIONS_BAD_RECORDS_LOGGER_ENABLE_DEFAULT)));
1:     model.setBadRecordsAction(
1:         conf.get(
1:             BAD_RECORDS_LOGGER_ACTION,
1:             carbonProperty.getProperty(
1:                 CarbonCommonConstants.CARBON_BAD_RECORDS_ACTION,
1:                 CarbonCommonConstants.CARBON_BAD_RECORDS_ACTION_DEFAULT)));
1: 
1:     model.setIsEmptyDataBadRecord(
1:         conf.get(
1:             IS_EMPTY_DATA_BAD_RECORD,
1:             carbonProperty.getProperty(
1:                 CarbonLoadOptionConstants.CARBON_OPTIONS_IS_EMPTY_DATA_BAD_RECORD,
1:                 CarbonLoadOptionConstants.CARBON_OPTIONS_IS_EMPTY_DATA_BAD_RECORD_DEFAULT)));
1: 
1:     model.setSkipEmptyLine(
1:         conf.get(
1:             SKIP_EMPTY_LINE,
1:             carbonProperty.getProperty(CarbonLoadOptionConstants.CARBON_OPTIONS_SKIP_EMPTY_LINE)));
1: 
0:     String complexDelim = conf.get(COMPLEX_DELIMITERS, "\\$" + "," + "\\:");
1:     String[] split = complexDelim.split(",");
1:     model.setComplexDelimiterLevel1(split[0]);
1:     if (split.length > 1) {
0:       model.setComplexDelimiterLevel1(split[1]);
1:     }
1:     model.setDateFormat(
1:         conf.get(
1:             DATE_FORMAT,
1:             carbonProperty.getProperty(
1:                 CarbonLoadOptionConstants.CARBON_OPTIONS_DATEFORMAT,
1:                 CarbonLoadOptionConstants.CARBON_OPTIONS_DATEFORMAT_DEFAULT)));
1: 
1:     model.setTimestampformat(
1:         conf.get(
1:             TIMESTAMP_FORMAT,
1:             carbonProperty.getProperty(
1:                 CarbonLoadOptionConstants.CARBON_OPTIONS_TIMESTAMPFORMAT,
1:                 CarbonLoadOptionConstants.CARBON_OPTIONS_TIMESTAMPFORMAT_DEFAULT)));
1: 
1:     model.setGlobalSortPartitions(
1:         conf.get(
1:             GLOBAL_SORT_PARTITIONS,
1:             carbonProperty.getProperty(
1:                 CarbonLoadOptionConstants.CARBON_OPTIONS_GLOBAL_SORT_PARTITIONS,
1:                 null)));
1: 
1:     model.setBatchSortSizeInMb(
1:         conf.get(
1:             BATCH_SORT_SIZE_INMB,
1:             carbonProperty.getProperty(
1:                 CarbonLoadOptionConstants.CARBON_OPTIONS_BATCH_SORT_SIZE_INMB,
1:                 carbonProperty.getProperty(
1:                     CarbonCommonConstants.LOAD_BATCH_SORT_SIZE_INMB,
1:                     CarbonCommonConstants.LOAD_BATCH_SORT_SIZE_INMB_DEFAULT))));
1: 
0:     model.setBadRecordsLocation(
0:         conf.get(BAD_RECORD_PATH,
1:             carbonProperty.getProperty(
0:                 CarbonLoadOptionConstants.CARBON_OPTIONS_BAD_RECORD_PATH,
1:                 carbonProperty.getProperty(
0:                     CarbonCommonConstants.CARBON_BADRECORDS_LOC,
0:                     CarbonCommonConstants.CARBON_BADRECORDS_LOC_DEFAULT_VAL))));
1: 
1:     model.setUseOnePass(
1:         conf.getBoolean(IS_ONE_PASS_LOAD,
1:             Boolean.parseBoolean(
1:                 carbonProperty.getProperty(
1:                     CarbonLoadOptionConstants.CARBON_OPTIONS_SINGLE_PASS,
1:                     CarbonLoadOptionConstants.CARBON_OPTIONS_SINGLE_PASS_DEFAULT))));
1:     return model;
1:   }
1: 
1:   private static void setFileHeader(Configuration configuration, CarbonLoadModel model)
1:       throws IOException {
1:     StructType inputSchema = getInputSchema(configuration);
1:     if (inputSchema == null || inputSchema.getFields().size() == 0) {
1:       throw new UnsupportedOperationException("Input schema must be set");
1:     }
1:     List<StructField> fields = inputSchema.getFields();
1:     StringBuilder builder = new StringBuilder();
1:     String[] columns = new String[fields.size()];
1:     int i = 0;
1:     for (StructField field : fields) {
1:       builder.append(field.getFieldName());
1:       builder.append(",");
1:       columns[i++] = field.getFieldName();
1:     }
1:     String header = builder.toString();
1:     model.setCsvHeader(header.substring(0, header.length() - 1));
1:     model.setCsvHeaderColumns(columns);
1:   }
1: 
0:   private static class CarbonRecordWriter extends RecordWriter<NullWritable, StringArrayWritable> {
1: 
1:     private CarbonOutputIteratorWrapper iteratorWrapper;
1: 
1:     private DataLoadExecutor dataLoadExecutor;
1: 
1:     public CarbonRecordWriter(CarbonOutputIteratorWrapper iteratorWrapper,
0:         DataLoadExecutor dataLoadExecutor) {
1:       this.iteratorWrapper = iteratorWrapper;
1:       this.dataLoadExecutor = dataLoadExecutor;
1:     }
1: 
1:     @Override
0:     public void write(NullWritable aVoid, StringArrayWritable strings)
1:         throws InterruptedException {
0:       iteratorWrapper.write(strings.get());
1:     }
1: 
1:     @Override
0:     public void close(TaskAttemptContext taskAttemptContext) {
0:       iteratorWrapper.close();
1:       dataLoadExecutor.close();
1:     }
author:Zhang Zhichao
-------------------------------------------------------------------------------
commit:088465f
/////////////////////////////////////////////////////////////////////////
1:     String complexDelim = conf.get(COMPLEX_DELIMITERS, "$" + "," + ":");
1:       model.setComplexDelimiterLevel2(split[1]);
author:QiangCai
-------------------------------------------------------------------------------
commit:41347d8
/////////////////////////////////////////////////////////////////////////
1:  * Licensed to the Apache Software Foundation (ASF) under one or more
1:  * contributor license agreements.  See the NOTICE file distributed with
1:  * this work for additional information regarding copyright ownership.
1:  * The ASF licenses this file to You under the Apache License, Version 2.0
1:  * (the "License"); you may not use this file except in compliance with
1:  * the License.  You may obtain a copy of the License at
1:  * Unless required by applicable law or agreed to in writing, software
1:  * distributed under the License is distributed on an "AS IS" BASIS,
1:  * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
1:  * See the License for the specific language governing permissions and
1:  * limitations under the License.
author:jackylk
-------------------------------------------------------------------------------
commit:edb02ab
/////////////////////////////////////////////////////////////////////////
1: /*
0:  * Licensed to the Apache Software Foundation (ASF) under one
0:  * or more contributor license agreements.  See the NOTICE file
0:  * distributed with this work for additional information
0:  * regarding copyright ownership.  The ASF licenses this file
0:  * to you under the Apache License, Version 2.0 (the
0:  * "License"); you may not use this file except in compliance
0:  * with the License.  You may obtain a copy of the License at
1:  *
1:  *    http://www.apache.org/licenses/LICENSE-2.0
1:  *
0:  * Unless required by applicable law or agreed to in writing,
0:  * software distributed under the License is distributed on an
0:  * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
0:  * KIND, either express or implied.  See the License for the
0:  * specific language governing permissions and limitations
0:  * under the License.
1:  */
1: 
1: package org.apache.carbondata.hadoop.api;
1: 
1: import java.io.IOException;
1: 
0: import org.apache.hadoop.fs.FileSystem;
0: import org.apache.hadoop.mapred.FileOutputFormat;
0: import org.apache.hadoop.mapred.JobConf;
0: import org.apache.hadoop.mapred.RecordWriter;
0: import org.apache.hadoop.util.Progressable;
1: 
1: /**
0:  * Base class for all output format for CarbonData file.
0:  * @param <T>
1:  */
0: public abstract class CarbonTableOutputFormat<T> extends FileOutputFormat<Void, T> {
1: 
0:   @Override
0:   public RecordWriter<Void, T> getRecordWriter(FileSystem ignored, JobConf job, String name,
0:       Progressable progress) throws IOException {
0:     return null;
1:   }
1: }
============================================================================