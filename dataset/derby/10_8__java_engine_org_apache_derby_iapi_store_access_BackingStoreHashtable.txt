1:eac0369: /*
1:7ecc1f2: 
1:7ecc1f2:    Derby - Class org.apache.derby.iapi.store.access.BackingStoreHashtable
1:7ecc1f2: 
1:75c7276:    Licensed to the Apache Software Foundation (ASF) under one or more
1:75c7276:    contributor license agreements.  See the NOTICE file distributed with
1:75c7276:    this work for additional information regarding copyright ownership.
1:75c7276:    The ASF licenses this file to you under the Apache License, Version 2.0
1:75c7276:    (the "License"); you may not use this file except in compliance with
1:75c7276:    the License.  You may obtain a copy of the License at
1:7ecc1f2: 
1:7ecc1f2:       http://www.apache.org/licenses/LICENSE-2.0
1:7ecc1f2: 
1:7ecc1f2:    Unless required by applicable law or agreed to in writing, software
1:7ecc1f2:    distributed under the License is distributed on an "AS IS" BASIS,
1:7ecc1f2:    WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
1:7ecc1f2:    See the License for the specific language governing permissions and
1:7ecc1f2:    limitations under the License.
1:eac0369: 
1:eac0369:  */
1:eac0369: 
1:eac0369: package org.apache.derby.iapi.store.access;
1:eac0369: 
1:eac0369: import org.apache.derby.iapi.error.StandardException; 
1:eac0369: 
1:eac0369: import org.apache.derby.iapi.types.DataValueDescriptor;
1:e81e52c: import org.apache.derby.iapi.types.LocatedRow;
1:e81e52c: import org.apache.derby.iapi.types.RowLocation;
1:eac0369: 
1:b223f72: import org.apache.derby.iapi.services.cache.ClassSize;
1:eac0369: 
1:e81e52c: import org.apache.derby.shared.common.sanity.SanityManager;
1:e81e52c: 
1:bd34a3a: import java.util.ArrayList;
1:0b3c2c9: import java.util.Collections;
1:eac0369: import java.util.Enumeration;
1:0b3c2c9: import java.util.HashMap;
1:0b3c2c9: import java.util.Iterator;
1:bd34a3a: import java.util.List;
1:eac0369: import java.util.Properties; 
1:b223f72: import java.util.NoSuchElementException;
1:eac0369: 
1:eac0369: /**
1:e81e52c: <p>
1:eac0369: A BackingStoreHashtable is a utility class which will store a set of rows into
1:eac0369: an in memory hash table, or overflow the hash table to a tempory on disk 
1:eac0369: structure.
1:e81e52c: </p>
1:e81e52c: 
2:eac0369: <p>
1:eac0369: All rows must contain the same number of columns, and the column at position
1:eac0369: N of all the rows must have the same format id.  If the BackingStoreHashtable needs to be
1:eac0369: overflowed to disk, then an arbitrary row will be chosen and used as a template
1:eac0369: for creating the underlying overflow container.
1:e81e52c: </p>
1:eac0369: 
1:eac0369: <p>
1:eac0369: The hash table will be built logically as follows (actual implementation
1:eac0369: may differ).  The important points are that the hash value is the standard
1:eac0369: java hash value on the row[key_column_numbers[0], if key_column_numbers.length is 1,
1:dbed020: or row[key_column_numbers[0, 1, ...]] if key_column_numbers.length &gt; 1, 
1:eac0369: and that duplicate detection is done by the standard java duplicate detection provided by 
1:eac0369: java.util.Hashtable.
1:e81e52c: </p>
1:e81e52c: 
1:ba10aba: <pre>
2:eac0369: import java.util.Hashtable;
1:eac0369: 
1:eac0369: hash_table = new Hashtable();
1:eac0369: 
1:e81e52c: Object row; // is a DataValueDescriptor[] or a LocatedRow
1:e81e52c: 
1:eac0369: boolean  needsToClone = rowSource.needsToClone();
1:eac0369: 
1:eac0369: while((row = rowSource.getNextRowFromRowSource()) != null)
1:eac0369: {
2:eac0369:     if (needsToClone)
1:eac0369:         row = clone_row_from_row(row);
1:eac0369: 
1:fcc3580:     Object key = KeyHasher.buildHashKey(row, key_column_numbers);
1:eac0369: 
1:eac0369:     if ((duplicate_value = 
1:eac0369:         hash_table.put(key, row)) != null)
1:eac0369:     {
2:eac0369:         Vector row_vec;
1:eac0369: 
1:eac0369:         // inserted a duplicate
1:eac0369:         if ((duplicate_value instanceof vector))
1:eac0369:         {
2:eac0369:             row_vec = (Vector) duplicate_value;
1:eac0369:         }
1:eac0369:         else
1:eac0369:         {
2:eac0369:             // allocate vector to hold duplicates
2:eac0369:             row_vec = new Vector(2);
1:fcc3580: 
1:eac0369:             // insert original row into vector
2:eac0369:             row_vec.addElement(duplicate_value);
1:eac0369: 
1:eac0369:             // put the vector as the data rather than the row
1:eac0369:             hash_table.put(key, row_vec);
1:eac0369:         }
1:eac0369:         
2:eac0369:         // insert new row into vector
2:eac0369:         row_vec.addElement(row);
1:eac0369:     }
1:eac0369: }
1:ba10aba: </pre>
1:eac0369: 
1:e81e52c: <p>
1:e81e52c: What actually goes into the hash table is a little complicated. That is because
1:e81e52c: the row may either be an array of column values (i.e. DataValueDescriptor[])
1:e81e52c: or a LocatedRow (i.e., a structure holding the columns plus a RowLocation).
1:e81e52c: In addition, the hash value itself may either be one of these rows or
1:e81e52c: (in the case of multiple rows which hash to the same value) a bucket (List)
1:e81e52c: of rows. To sum this up, the values in a hash table which does not spill
1:e81e52c: to disk may be the following:
1:e81e52c: </p>
1:e81e52c: 
1:e81e52c: <ul>
1:e81e52c: <li>DataValueDescriptor[] and ArrayList<DataValueDescriptor></li>
1:e81e52c: <li>or LocatedRow and ArrayList<LocatedRow></li>
1:e81e52c: </ul>
1:e81e52c: 
1:e81e52c: <p>
1:e81e52c: If rows spill to disk, then they just become arrays of columns. In this case,
1:e81e52c: a LocatedRow becomes a DataValueDescriptor[], where the last cell contains
1:e81e52c: the RowLocation.
1:e81e52c: </p>
1:e81e52c: 
1:eac0369: **/
1:eac0369: 
1:eac0369: public class BackingStoreHashtable
1:eac0369: {
1:eac0369: 
1:eac0369:     /**************************************************************************
1:eac0369:      * Fields of the class
1:eac0369:      **************************************************************************
1:eac0369:      */
1:b223f72:     private TransactionController tc;
1:073b862:     private HashMap<Object,Object>     hash_table;
1:eac0369:     private int[]       key_column_numbers;
1:eac0369:     private boolean     remove_duplicates;
1:eac0369: 	private boolean		skipNullKeyColumns;
1:eac0369:     private Properties  auxillary_runtimestats;
1:eac0369: 	private RowSource	row_source;
1:b223f72:     /* If max_inmemory_rowcnt > 0 then use that to decide when to spill to disk.
1:b223f72:      * Otherwise compute max_inmemory_size based on the JVM memory size when the BackingStoreHashtable
1:b223f72:      * is constructed and use that to decide when to spill to disk.
1:eac0369:      */
1:b223f72:     private long max_inmemory_rowcnt;
1:b223f72:     private long inmemory_rowcnt;
1:b223f72:     private long max_inmemory_size;
1:b223f72:     private boolean keepAfterCommit;
1:eac0369: 
1:381dfaf:     /**
1:bd34a3a:      * The estimated number of bytes used by ArrayList(0)
1:381dfaf:      */  
1:bd34a3a:     private final static int ARRAY_LIST_SIZE =
1:bd34a3a:         ClassSize.estimateBaseFromCatalog(ArrayList.class);
1:eac0369:     
1:b223f72:     private DiskHashtable diskHashtable;
1:eac0369: 
1:eac0369:     /**************************************************************************
1:eac0369:      * Constructors for This class:
1:eac0369:      **************************************************************************
1:eac0369:      */
1:eac0369:     private BackingStoreHashtable(){}
1:eac0369: 
1:eac0369:     /**
1:eac0369:      * Create the BackingStoreHashtable from a row source.
1:e81e52c:      * <p>
1:eac0369:      * This routine drains the RowSource.  The performance characteristics
1:eac0369:      * depends on the number of rows inserted and the parameters to the 
1:e81e52c:      * constructor. RowLocations are supported iff row_source is null.
1:e81e52c:      * RowLocations in a non-null row_source can be added later
1:e81e52c:      * if there is a use-case that stresses this behavior.
1:eac0369:      * <p>
1:dbed020:      * If the number of rows is &lt;= "max_inmemory_rowcnt", then the rows are
1:0b3c2c9:      * inserted into a java.util.HashMap. In this case no
1:eac0369:      * TransactionController is necessary, a "null" tc is valid.
1:eac0369:      * <p>
1:dbed020:      * If the number of rows is &gt; "max_inmemory_rowcnt", then the rows will
1:eac0369:      * be all placed in some sort of Access temporary file on disk.  This 
1:eac0369:      * case requires a valid TransactionController.
1:eac0369:      *
1:eac0369:      * @param tc                An open TransactionController to be used if the
1:eac0369:      *                          hash table needs to overflow to disk.
1:eac0369:      *
1:eac0369:      * @param row_source        RowSource to read rows from.
1:eac0369:      *
1:eac0369:      * @param key_column_numbers The column numbers of the columns in the
1:0b3c2c9:      *                          scan result row to be the key to the HashMap.
1:eac0369:      *                          "0" is the first column in the scan result
1:eac0369:      *                          row (which may be different than the first
1:eac0369:      *                          row in the table of the scan).
1:eac0369:      *
1:0b3c2c9:      * @param remove_duplicates Should the HashMap automatically remove
1:bd34a3a:      *                          duplicates, or should it create the list of
1:eac0369:      *                          duplicates?
1:eac0369:      *
1:eac0369:      * @param estimated_rowcnt  The estimated number of rows in the hash table.
1:eac0369:      *                          Pass in -1 if there is no estimate.
1:eac0369:      *
1:eac0369:      * @param max_inmemory_rowcnt
1:eac0369:      *                          The maximum number of rows to insert into the 
1:eac0369:      *                          inmemory Hash table before overflowing to disk.
1:eac0369:      *                          Pass in -1 if there is no maximum.
1:eac0369:      *
1:0b3c2c9:      * @param initialCapacity   If not "-1" used to initialize the java HashMap
1:eac0369:      *
1:0b3c2c9:      * @param loadFactor        If not "-1" used to initialize the java HashMap
1:eac0369: 	 *
1:eac0369: 	 * @param skipNullKeyColumns	Skip rows with a null key column, if true.
1:eac0369:      *
1:b223f72:      * @param keepAfterCommit If true the hash table is kept after a commit,
1:b223f72:      *                        if false the hash table is dropped on the next commit.
1:eac0369:      *
1:eac0369:      *
1:eac0369: 	 * @exception  StandardException  Standard exception policy.
1:eac0369:      **/
1:eac0369:     public BackingStoreHashtable(
1:eac0369:     TransactionController   tc,
1:eac0369:     RowSource               row_source,
1:eac0369:     int[]                   key_column_numbers,
1:eac0369:     boolean                 remove_duplicates,
1:eac0369:     long                    estimated_rowcnt,
1:eac0369:     long                    max_inmemory_rowcnt,
1:eac0369:     int                     initialCapacity,
1:eac0369:     float                   loadFactor,
1:b223f72: 	boolean					skipNullKeyColumns,
1:b223f72:     boolean                 keepAfterCommit)
1:eac0369:         throws StandardException
1:eac0369:     {
1:eac0369:         this.key_column_numbers    = key_column_numbers;
1:eac0369:         this.remove_duplicates    = remove_duplicates;
1:eac0369: 		this.row_source			   = row_source;
1:eac0369: 		this.skipNullKeyColumns	   = skipNullKeyColumns;
1:b223f72:         this.max_inmemory_rowcnt = max_inmemory_rowcnt;
1:e81e52c:         if ( max_inmemory_rowcnt > 0)
1:e81e52c:         {
1:b223f72:             max_inmemory_size = Long.MAX_VALUE;
1:e81e52c:         }
1:b223f72:         else
1:e81e52c:         {
1:b223f72:             max_inmemory_size = Runtime.getRuntime().totalMemory()/100;
1:e81e52c:         }
1:b223f72:         this.tc = tc;
1:b223f72:         this.keepAfterCommit = keepAfterCommit;
1:eac0369: 
1:e81e52c:         if (SanityManager.DEBUG)
1:e81e52c:         {
1:e81e52c:             // RowLocations are not currently supported if the
1:e81e52c:             // hash table is being filled from a non-null
1:e81e52c:             // row_source arg.
1:e81e52c:             if ( row_source != null )
1:e81e52c:             {
1:e81e52c:                 SanityManager.ASSERT( !includeRowLocations() );
1:e81e52c:             }
1:e81e52c:         }
1:e81e52c: 
1:eac0369:         // use passed in capacity and loadfactor if not -1, you must specify
1:eac0369:         // capacity if you want to specify loadfactor.
1:eac0369:         if (initialCapacity != -1)
1:eac0369:         {
1:eac0369:             hash_table = 
1:eac0369:                 ((loadFactor == -1) ? 
1:073b862:                      new HashMap<Object,Object>(initialCapacity) :
1:073b862:                      new HashMap<Object,Object>(initialCapacity, loadFactor));
1:eac0369:         }
1:eac0369:         else
1:eac0369:         {
1:e540aee:             /* We want to create the hash table based on the estimated row
1:e540aee:              * count if a) we have an estimated row count (i.e. it's greater
1:e540aee:              * than zero) and b) we think we can create a hash table to
1:e540aee:              * hold the estimated row count without running out of memory.
1:e540aee:              * The check for "b" is required because, for deeply nested
1:e540aee:              * queries and/or queries with a high number of tables in
1:e540aee:              * their FROM lists, the optimizer can end up calculating
1:e540aee:              * some very high row count estimates--even up to the point of
1:e540aee:              * Double.POSITIVE_INFINITY (see DERBY-1259 for an explanation
1:e540aee:              * of how that can happen).  In that case any attempts to
1:0b3c2c9:              * create a hash table of size estimated_rowcnt can cause
1:0b3c2c9:              * OutOfMemory errors when we try to create the hash table.
1:e540aee:              * So as a "red flag" for that kind of situation, we check to
1:e540aee:              * see if the estimated row count is greater than the max
1:e540aee:              * in-memory size for this table.  Unit-wise this comparison
1:e540aee:              * is relatively meaningless: rows vs bytes.  But if our
1:e540aee:              * estimated row count is greater than the max number of
1:e540aee:              * in-memory bytes that we're allowed to consume, then
1:0b3c2c9:              * it's very likely that creating a hash table with a capacity
1:e540aee:              * of estimated_rowcnt will lead to memory problems.  So in
1:e540aee:              * that particular case we leave hash_table null here and
1:e540aee:              * initialize it further below, using the estimated in-memory
1:e540aee:              * size of the first row to figure out what a reasonable size
1:0b3c2c9:              * for the hash table might be.
1:b223f72:              */
1:eac0369:             hash_table = 
1:e540aee:                 (((estimated_rowcnt <= 0) || (row_source == null)) ?
1:073b862:                      new HashMap<Object,Object>() :
1:e540aee:                      (estimated_rowcnt < max_inmemory_size) ?
1:073b862:                          new HashMap<Object,Object>((int) estimated_rowcnt) :
1:e540aee:                          null);
1:eac0369:         }
1:eac0369: 
1:eac0369:         if (row_source != null)
1:eac0369:         {
1:eac0369:             boolean needsToClone = row_source.needsToClone();
1:eac0369: 
1:e8e04bf:             DataValueDescriptor[] row;
1:eac0369:             while ((row = getNextRowFromRowSource()) != null)
1:eac0369:             {
1:e540aee:                 // If we haven't initialized the hash_table yet then that's
1:0b3c2c9:                 // because a hash table with capacity estimated_rowcnt would
1:e540aee:                 // probably cause memory problems.  So look at the first row
1:e540aee:                 // that we found and use that to create the hash table with
1:e540aee:                 // an initial capacity such that, if it was completely full,
1:e540aee:                 // it would still satisfy the max_inmemory condition.  Note
1:e540aee:                 // that this isn't a hard limit--the hash table can grow if
1:e540aee:                 // needed.
1:e540aee:                 if (hash_table == null)
1:eac0369:                 {
1:985a9b0:                     // Check to see how much memory we think the first row
1:e540aee:                     // is going to take, and then use that to set the initial
1:0b3c2c9:                     // capacity of the hash table.
1:e540aee:                     double rowUsage = getEstimatedMemUsage(row);
1:0b3c2c9:                     hash_table =
1:073b862:                         new HashMap<Object,Object>((int)(max_inmemory_size / rowUsage));
1:eac0369:                 }
1:eac0369:                
1:e81e52c:                 add_row_to_hash_table(row, null, needsToClone);
1:eac0369:             }
1:eac0369:         }
1:eac0369: 
1:985a9b0:         // In the (unlikely) event that we received a "red flag" estimated_rowcnt
1:985a9b0:         // that is too big (see comments above), it's possible that, if row_source
1:985a9b0:         // was null or else didn't have any rows, hash_table could still be null
1:985a9b0:         // at this point.  So we initialize it to an empty hashtable (representing
1:985a9b0:         // an empty result set) so that calls to other methods on this
1:985a9b0:         // BackingStoreHashtable (ex. "size()") will have a working hash_table
1:985a9b0:         // on which to operate.
1:985a9b0:         if (hash_table == null)
1:e81e52c:         {
1:073b862:             hash_table = new HashMap<Object,Object>();
1:e81e52c:         }
1:e81e52c:     }
1:e81e52c: 
1:e81e52c:     /**
1:e81e52c:      * Return true if we should include RowLocations with the rows
1:e81e52c:      * stored in this hash table.
1:e81e52c:      */
1:e81e52c:     public  boolean includeRowLocations()
1:e81e52c:     {
1:e81e52c:         return false;
1:eac0369:     }
1:eac0369: 
1:eac0369:     /**************************************************************************
1:eac0369:      * Private/Protected methods of This class:
1:eac0369:      **************************************************************************
1:eac0369:      */
1:eac0369: 
1:eac0369: 	/**
1:eac0369: 	 * Call method to either get next row or next row with non-null
1:e81e52c: 	 * key columns. Currently, RowLocation information is not included in
1:e81e52c:      * rows siphoned out of a RowSource. That functionality is only supported
1:e81e52c:      * if the rows come from the scan of a base table.
1:eac0369: 	 *
1:eac0369:      *
1:eac0369: 	 * @exception  StandardException  Standard exception policy.
1:eac0369: 	 */
1:e8e04bf: 	private DataValueDescriptor[] getNextRowFromRowSource()
1:eac0369: 		throws StandardException
1:eac0369: 	{
1:e8e04bf: 		DataValueDescriptor[] row = row_source.getNextRowFromRowSource();
1:eac0369: 
1:eac0369: 		if (skipNullKeyColumns)
1:eac0369: 		{
1:eac0369: 			while (row != null)
1:eac0369: 			{
1:eac0369: 				// Are any key columns null?
1:eac0369: 				int index = 0;
1:eac0369: 				for ( ; index < key_column_numbers.length; index++)
1:eac0369: 				{
1:e8e04bf: 					if (row[key_column_numbers[index]].isNull())
1:eac0369: 					{
1:eac0369: 						break;
1:eac0369: 					}
1:eac0369: 				}
1:eac0369: 				// No null key columns
1:eac0369: 				if (index == key_column_numbers.length)
1:eac0369: 				{
1:eac0369: 					return row;
1:eac0369: 				}
1:eac0369: 				// 1 or more null key columns
1:eac0369: 				row = row_source.getNextRowFromRowSource();
1:eac0369: 			}
1:eac0369: 		}
1:eac0369: 		return row;
1:eac0369: 	}
1:eac0369: 
1:eac0369:     /**
1:eac0369:      * Return a cloned copy of the row.
1:eac0369:      *
1:eac0369: 	 * @return The cloned row row to use.
1:eac0369:      *
1:eac0369: 	 * @exception  StandardException  Standard exception policy.
1:eac0369:      **/
1:e8e04bf:     private static DataValueDescriptor[] cloneRow(DataValueDescriptor[] old_row)
1:eac0369:         throws StandardException
1:eac0369:     {
1:e8e04bf:         DataValueDescriptor[] new_row = new DataValueDescriptor[old_row.length];
1:eac0369: 
1:b4e2eb7:         // History: We used to materialize streams when getting a clone
1:b4e2eb7:         //          here (i.e. used getClone, not cloneObject). We still do.
1:b4e2eb7:         // Beetle 4896.
1:eac0369:         for (int i = 0; i < old_row.length; i++)
1:eac0369:         {
1:e81e52c:             if ( old_row[i] != null)
1:e81e52c:             {
1:854dd10:                 new_row[i] = old_row[i].cloneValue(false);
1:e81e52c:             }
1:eac0369:         }
1:eac0369: 
1:eac0369:         return(new_row);
1:eac0369:     }
1:eac0369: 
1:eac0369:     /**
1:b7c1f3b:      * Return a shallow cloned row
1:eac0369:      *
1:b7c1f3b:      * @return The cloned row row to use.
1:b223f72:      *
1:b7c1f3b:      * @exception  StandardException  Standard exception policy.
1:b7c1f3b:      **/
1:b7c1f3b:     static DataValueDescriptor[] shallowCloneRow(DataValueDescriptor[] old_row)
1:b223f72:         throws StandardException
1:eac0369:     {
1:b7c1f3b:         DataValueDescriptor[] new_row = new DataValueDescriptor[old_row.length];
1:b4e2eb7:         // History: We used to *not* materialize streams when getting a clone
1:d7aa761:         //          here (i.e. used cloneObject, not getClone).
1:d7aa761:         //          We still don't materialize, just clone the holder.
1:b4e2eb7:         // DERBY-802
1:b7c1f3b:         for (int i = 0; i < old_row.length; i++)
1:e81e52c:         {
1:e81e52c:             if ( old_row[i] != null)
1:eac0369:             {
1:d7aa761:                 new_row[i] = old_row[i].cloneHolder();
1:e81e52c:             }
1:eac0369:         }
1:eac0369: 
1:b7c1f3b:         return(new_row);
1:eac0369:     }
1:eac0369: 
1:eac0369:     /**
1:eac0369:      * Do the work to add one row to the hash table.
1:eac0369:      * <p>
1:eac0369:      *
1:e81e52c:      * @param columnValues               Row to add to the hash table.
1:e81e52c:      * @param rowLocation   Location of row in conglomerate; could be null.
1:b7c1f3b:      * @param needsToClone      If the row needs to be cloned
1:eac0369:      *
1:eac0369: 	 * @exception  StandardException  Standard exception policy.
1:eac0369:      **/
1:e81e52c:     private void add_row_to_hash_table
1:e81e52c:         (
1:e81e52c:          DataValueDescriptor[] columnValues,
1:e81e52c:          RowLocation rowLocation,
1:e81e52c:          boolean needsToClone
1:e81e52c:          )
1:eac0369: 		throws StandardException
1:eac0369:     {
1:e81e52c:         if (spillToDisk( columnValues, rowLocation ))
1:e81e52c:         {
1:b223f72:             return;
1:e81e52c:         }
1:eac0369:         
1:eac0369:         if (needsToClone)
1:eac0369:         {
1:e81e52c:             columnValues = cloneRow( columnValues );
1:b223f72:         }
1:eac0369: 
1:e81e52c:         Object key = KeyHasher.buildHashKey( columnValues, key_column_numbers );
1:e81e52c:         Object hashValue = !includeRowLocations() ?
1:e81e52c:             columnValues : new LocatedRow( columnValues, rowLocation );
1:e81e52c:         Object duplicate_value = hash_table.put( key, hashValue );
1:eac0369: 
1:fcc3580:         if (duplicate_value == null)
1:e81e52c:         {
1:e81e52c:             doSpaceAccounting( hashValue, false );
1:e81e52c:         }
1:b223f72:         else
1:eac0369:         {
1:eac0369:             if (!remove_duplicates)
1:eac0369:             {
1:fcc3580:                 RowList row_vec;
1:eac0369: 
1:eac0369:                 // inserted a duplicate
1:fcc3580:                 if (duplicate_value instanceof RowList)
1:b223f72:                 {
1:e81e52c:                     doSpaceAccounting( hashValue, false );
1:fcc3580:                     row_vec = (RowList) duplicate_value;
1:eac0369:                 }
1:eac0369:                 else
1:b223f72:                 {
1:bd34a3a:                     // allocate list to hold duplicates
1:fcc3580:                     row_vec = new RowList(2);
1:eac0369: 
1:eac0369:                     // insert original row into vector
1:e81e52c:                     row_vec.add( duplicate_value );
1:e81e52c:                     doSpaceAccounting( hashValue, true );
1:b223f72:                 }
1:eac0369: 
1:bd34a3a:                 // insert new row into list
1:e81e52c:                 row_vec.add( hashValue );
1:b223f72: 
1:bd34a3a:                 // store list of rows back into hash table,
1:eac0369:                 // overwriting the duplicate key that was 
1:eac0369:                 // inserted.
1:eac0369:                 hash_table.put(key, row_vec);
1:eac0369:             }
1:eac0369:         }
1:eac0369:     }
1:eac0369: 
1:e81e52c:     private void doSpaceAccounting(Object hashValue,
1:b223f72:                                     boolean firstDuplicate)
1:b223f72:     {
1:b223f72:         inmemory_rowcnt++;
1:e81e52c:         if ( max_inmemory_rowcnt <= 0)
1:b223f72:         {
1:e81e52c:             max_inmemory_size -= getEstimatedMemUsage( hashValue );
1:e81e52c:             if ( firstDuplicate)
1:e81e52c:             {
1:bd34a3a:                 max_inmemory_size -= ARRAY_LIST_SIZE;
1:e81e52c:             }
1:b223f72:         }
1:b223f72:     } // end of doSpaceAccounting
1:eac0369: 
1:b223f72:     /**
1:b223f72:      * Determine whether a new row should be spilled to disk and, if so, do it.
1:b223f72:      *
1:e81e52c:      * @param columnValues  Actual columns from source row.
1:e81e52c:      * @param rowLocation       Optional row location.
1:b223f72:      *
1:b223f72:      * @return true if the row was spilled to disk, false if not
1:b223f72:      *
1:b223f72:      * @exception  StandardException  Standard exception policy.
1:b223f72:      */
1:e81e52c:     private boolean spillToDisk
1:e81e52c:         (
1:e81e52c:          DataValueDescriptor[] columnValues,
1:e81e52c:          RowLocation rowLocation
1:e81e52c:          )
1:e81e52c:         throws StandardException
1:e81e52c:     {
1:b223f72:         // Once we have started spilling all new rows will go to disk, even if we have freed up some
1:b223f72:         // memory by moving duplicates to disk. This simplifies handling of duplicates and accounting.
1:e81e52c: 
1:e81e52c:         DataValueDescriptor[]   diskRow = null;
1:e81e52c:         
1:e81e52c:         if ( diskHashtable == null)
1:b223f72:         {
1:e81e52c:             if ( max_inmemory_rowcnt > 0)
1:b223f72:             {
1:e81e52c:                 if ( inmemory_rowcnt < max_inmemory_rowcnt)
1:e81e52c:                 {
1:b223f72:                     return false; // Do not spill
1:e81e52c:                 }
1:b223f72:             }
1:e81e52c:             else if
1:e81e52c:                 (
1:e81e52c:                  max_inmemory_size >
1:e81e52c:                  getEstimatedMemUsage
1:e81e52c:                  (
1:e81e52c:                   !includeRowLocations() ?
1:e81e52c:                   columnValues : new LocatedRow( columnValues, rowLocation )
1:e81e52c:                  )
1:e81e52c:                 )
1:e81e52c:             {
1:b223f72:                 return false;
1:e81e52c:             }
1:e81e52c:             
1:b223f72:             // Want to start spilling
1:e81e52c:             diskRow = makeDiskRow( columnValues, rowLocation );
1:e8e04bf:  
1:b61f876:             diskHashtable = 
1:b61f876:                 new DiskHashtable(
1:b61f876:                        tc,
1:e81e52c:                        diskRow,
1:b61f876:                        (int[]) null, //TODO-COLLATION, set non default collation if necessary.
1:b61f876:                        key_column_numbers,
1:b61f876:                        remove_duplicates,
1:b61f876:                        keepAfterCommit);
1:b223f72:         }
1:e81e52c:         Object key = KeyHasher.buildHashKey( columnValues, key_column_numbers );
1:b223f72:         Object duplicateValue = hash_table.get( key);
1:e81e52c:         if ( duplicateValue != null)
1:b223f72:         {
1:e81e52c:             if ( remove_duplicates)
1:b223f72:                 return true; // a degenerate case of spilling
1:b223f72:             // If we are keeping duplicates then move all the duplicates from memory to disk
1:b223f72:             // This simplifies finding duplicates: they are either all in memory or all on disk.
1:bd34a3a:             if (duplicateValue instanceof List)
1:b223f72:             {
1:bd34a3a:                 List duplicateVec = (List) duplicateValue;
1:b223f72:                 for( int i = duplicateVec.size() - 1; i >= 0; i--)
1:b223f72:                 {
1:e81e52c:                     diskHashtable.put
1:e81e52c:                         ( key, makeDiskRow( duplicateVec.get( i ) ));
1:b223f72:                 }
1:b223f72:             }
1:b223f72:             else
1:e81e52c:             {
1:e81e52c:                 diskHashtable.put( key, makeDiskRow( duplicateValue ) );
1:e81e52c:             }
1:b223f72:             hash_table.remove( key);
1:b223f72:         }
1:e81e52c: 
1:e81e52c:         if ( diskRow == null )
1:e81e52c:         { diskRow = makeDiskRow( columnValues, rowLocation ); }
1:e81e52c:         
1:e81e52c:         diskHashtable.put( key, diskRow );
1:b223f72:         return true;
1:b223f72:     } // end of spillToDisk
1:b223f72: 
1:b7c1f3b:     /**
1:e81e52c:      * <p>
1:e81e52c:      * Make a full set of columns from an object which is either already
1:e81e52c:      * an array of column or otherwise a LocatedRow. The full set of columns
1:e81e52c:      * is what's stored on disk when we spill to disk. This is the inverse of
1:e81e52c:      * makeInMemoryRow().
1:e81e52c:      * </p>
1:e81e52c:      */
1:e81e52c:     private DataValueDescriptor[]   makeDiskRow( Object raw )
1:e81e52c:     {
1:e81e52c:         DataValueDescriptor[]   allColumns = null;
1:e81e52c:         if ( includeRowLocations() )
1:e81e52c:         {
1:e81e52c:             LocatedRow  locatedRow = (LocatedRow) raw;
1:e81e52c:             allColumns = makeDiskRow
1:e81e52c:                 ( locatedRow.columnValues(), locatedRow.rowLocation() );
1:e81e52c:         }
1:e81e52c:         else { allColumns = (DataValueDescriptor[]) raw; }
1:e81e52c: 
1:e81e52c:         return allColumns;
1:e81e52c:     }
1:e81e52c: 
1:e81e52c:     /**
1:e81e52c:      * <p>
1:e81e52c:      * Turn a list of disk rows into a list of in-memory rows. The on disk
1:e81e52c:      * rows are always of type DataValueDescriptor[]. But the in-memory rows
1:e81e52c:      * could be of type LocatedRow.
1:e81e52c:      * </p>
1:e81e52c:      */
1:e81e52c:     private List    makeInMemoryRows( List diskRows )
1:e81e52c:     {
1:e81e52c:         if ( !includeRowLocations() )
1:e81e52c:         {
1:e81e52c:             return diskRows;
1:e81e52c:         }
1:e81e52c:         else
1:e81e52c:         {
1:e81e52c:             ArrayList<Object>   result = new ArrayList<Object>();
1:e81e52c:             for ( Object diskRow : diskRows )
1:e81e52c:             {
1:e81e52c:                 result.add
1:e81e52c:                     ( makeInMemoryRow( (DataValueDescriptor[]) diskRow ) );
1:e81e52c:             }
1:e81e52c: 
1:e81e52c:             return result;
1:e81e52c:         }
1:e81e52c:     }
1:e81e52c: 
1:e81e52c:     /**
1:e81e52c:      * <p>
1:e81e52c:      * Make an in-memory row from an on-disk row. This is the inverse
1:e81e52c:      * of makeDiskRow().
1:e81e52c:      * </p>
1:e81e52c:      */
1:e81e52c:     private Object  makeInMemoryRow( DataValueDescriptor[] diskRow )
1:e81e52c:     {
1:e81e52c:         if ( !includeRowLocations() )
1:e81e52c:         {
1:e81e52c:             return diskRow;
1:e81e52c:         }
1:e81e52c:         else
1:e81e52c:         {
1:e81e52c:             return new LocatedRow( diskRow );
1:e81e52c:         }
1:e81e52c:     }
1:e81e52c: 
1:e81e52c:     /**
1:e81e52c:      * <p>
1:e81e52c:      * Construct a full set of columns, which may need to end
1:e81e52c:      * with the row location.The full set of columns is what's
1:e81e52c:      * stored on disk when we spill to disk.
1:e81e52c:      * </p>
1:e81e52c:      */
1:e81e52c:     private DataValueDescriptor[]   makeDiskRow
1:e81e52c:         ( DataValueDescriptor[] columnValues, RowLocation rowLocation )
1:e81e52c:     {
1:e81e52c:         if ( !includeRowLocations() )
1:e81e52c:         {
1:e81e52c:             return columnValues;
1:e81e52c:         }
1:e81e52c:         else
1:e81e52c:         {
1:e81e52c:             return LocatedRow.flatten( columnValues, rowLocation );
1:e81e52c:         }
1:e81e52c:     }
1:e81e52c: 
1:e81e52c:     /**
1:e81e52c:      * Take a value which will go into the hash table and return an estimate
1:e81e52c:      * of how much memory that value will consume. The hash value could
1:e81e52c:      * be either an array of columns or a LocatedRow.
2:b7c1f3b:      * 
1:e81e52c:      * @param hashValue The object for which we want to know the memory usage.
1:e81e52c:      * @return A guess as to how much memory the current hash value will
1:e540aee:      *  use.
1:e540aee:      */
1:e81e52c:     private long getEstimatedMemUsage( Object hashValue )
1:b223f72:     {
1:e540aee:         long rowMem = 0;
1:e81e52c:         DataValueDescriptor[] row = null;
1:e81e52c: 
1:e81e52c:         if ( hashValue instanceof DataValueDescriptor[] )
1:e81e52c:         {
1:e81e52c:             row = (DataValueDescriptor[]) hashValue;
1:e81e52c:         }
1:e81e52c:         else
1:e81e52c:         {
1:e81e52c:             LocatedRow  locatedRow = (LocatedRow) hashValue;
1:e81e52c:             row = locatedRow.columnValues();
1:e81e52c: 
1:e81e52c:             // account for the RowLocation size and class overhead
1:e81e52c:             RowLocation rowLocation = locatedRow.rowLocation();
1:e81e52c:             if ( rowLocation != null )
1:e81e52c:             {
1:e81e52c:                 rowMem += locatedRow.rowLocation().estimateMemoryUsage();
1:e81e52c:                 rowMem += ClassSize.refSize;
1:e81e52c:             }
1:e81e52c: 
1:e81e52c:             // account for class overhead of the LocatedRow itself
1:e81e52c:             rowMem += ClassSize.refSize;
1:e81e52c:         }
1:e81e52c:         
1:b223f72:         for( int i = 0; i < row.length; i++)
1:b223f72:         {
1:e81e52c:             // account for the column's size and class overhead
1:e8e04bf:             rowMem += row[i].estimateMemoryUsage();
1:e540aee:             rowMem += ClassSize.refSize;
1:b223f72:         }
1:b223f72: 
1:e81e52c:         // account for the class overhead of the array itself
1:e540aee:         rowMem += ClassSize.refSize;
1:e540aee:         return rowMem;
1:b223f72:     }
1:b223f72: 
1:eac0369:     /**************************************************************************
1:eac0369:      * Public Methods of This class:
1:eac0369:      **************************************************************************
1:eac0369:      */
1:eac0369: 
1:eac0369:     /**
1:eac0369:      * Close the BackingStoreHashtable.
1:eac0369:      * <p>
1:eac0369:      * Perform any necessary cleanup after finishing with the hashtable.  Will
1:eac0369:      * deallocate/dereference objects as necessary.  If the table has gone
1:eac0369:      * to disk this will drop any on disk files used to support the hash table.
1:eac0369:      * <p>
1:eac0369:      *
1:eac0369: 	 * @exception  StandardException  Standard exception policy.
1:eac0369:      **/
1:eac0369:     public void close() 
1:eac0369: 		throws StandardException
1:eac0369:     {
1:eac0369:         hash_table = null;
1:e81e52c:         if ( diskHashtable != null)
1:b223f72:         {
1:b223f72:             diskHashtable.close();
1:b223f72:             diskHashtable = null;
1:eac0369:         }
1:eac0369:         return;
1:b223f72:     }
1:eac0369: 
1:eac0369:     /**
1:eac0369:      * <p>
1:e81e52c:      * Return an Enumeration that can be used to scan the entire table. The objects
1:e81e52c:      * in the Enumeration can be either of the following:
1:e81e52c:      * </p>
1:e81e52c:      *
1:e81e52c:      * <ul>
1:e81e52c:      * <li>a row - This is a single row with a unique hash key.</li>
1:e81e52c:      * <li>or a bucket of rows - This is a list of rows which all have the same hash key.</li>
1:e81e52c:      * </ul>
1:e81e52c:      *
1:e81e52c:      * <p>
1:e81e52c:      * The situation is a little more complicated because the row representation
1:e81e52c:      * is different depending on whether the row includes a RowLocation.
1:e81e52c:      * If includeRowLocations()== true, then the row is a LocatedRow. Otherwise,
1:e81e52c:      * the row is an array of DataValueDescriptor. Putting all of this together,
1:e81e52c:      * if the row contains a RowLocation, then the objects in the Enumeration returned
1:e81e52c:      * by this method can be either of the following:
1:e81e52c:      * </p>
1:e81e52c:      *
1:e81e52c:      * <ul>
1:e81e52c:      * <li>a LocatedRow</li>
1:e81e52c:      * <li>or a List&lt;LocatedRow&gt;</li>
1:e81e52c:      * </ul>
1:e81e52c:      *
1:e81e52c:      * <p>
1:e81e52c:      * But if the row does not contain a RowLocation, then the objects in the
1:e81e52c:      * Enumeration returned by this method can be either of the following:
1:e81e52c:      * </p>
1:e81e52c:      *
1:e81e52c:      * <ul>
1:e81e52c:      * <li>a DataValueDescriptor[]</li>
1:e81e52c:      * <li>or a List&lt;DataValueDescriptor[]&gt;</li>
1:e81e52c:      * </ul>
1:e81e52c:      *
1:eac0369:      * <p>
1:eac0369:      * RESOLVE - is it worth it to support this routine when we have a
1:eac0369:      *           disk overflow hash table?
1:e81e52c:      * </p>
1:eac0369:      *
1:eac0369: 	 * @return The Enumeration.
1:eac0369:      *
1:eac0369: 	 * @exception  StandardException  Standard exception policy.
1:eac0369:      **/
1:073b862:     public Enumeration<Object> elements()
1:eac0369:         throws StandardException
1:e81e52c:     {
1:e81e52c:         if ( diskHashtable == null)
1:eac0369:         {
1:0b3c2c9:             return Collections.enumeration(hash_table.values());
1:e81e52c:         }
1:b223f72:         return new BackingStoreHashtableEnumeration();
1:eac0369:     }
1:eac0369: 
1:eac0369:     /**
1:e81e52c:      * <p>
1:e81e52c:      * Get data associated with given key.
1:e81e52c:      * </p>
1:e81e52c:      *
1:eac0369:      * <p>
1:eac0369:      * There are 2 different types of objects returned from this routine.
1:e81e52c:      * </p>
1:e81e52c:      *
1:eac0369:      * <p>
1:eac0369: 	 * In both cases, the key value is either the object stored in 
1:eac0369:      * row[key_column_numbers[0]], if key_column_numbers.length is 1, 
1:eac0369:      * otherwise it is a KeyHasher containing
1:eac0369: 	 * the objects stored in row[key_column_numbers[0, 1, ...]].
1:eac0369:      * For every qualifying unique row value an entry is placed into the 
1:0b3c2c9:      * hash table.
1:e81e52c:      * </p>
1:e81e52c:      *
1:eac0369:      * <p>
1:bd34a3a:      * For row values with duplicates, the value of the data is a list of
1:eac0369:      * rows.
1:e81e52c:      * </p>
1:e81e52c:      *
1:e81e52c:      * <p>
1:e81e52c:      * The situation is a little more complicated because the row representation
1:e81e52c:      * is different depending on whether the row includes a RowLocation.
1:e81e52c:      * If includeRowLocations() == true, then the row is a LocatedRow. Otherwise,
1:e81e52c:      * the row is an array of DataValueDescriptor. Putting all of this together,
1:e81e52c:      * if the row contains a RowLocation, then the objects returned by this method
1:e81e52c:      * can be either of the following:
1:e81e52c:      * </p>
1:e81e52c:      *
1:e81e52c:      * <ul>
1:e81e52c:      * <li>a LocatedRow</li>
1:e81e52c:      * <li>or a List&lt;LocatedRow&gt;</li>
1:e81e52c:      * </ul>
1:e81e52c:      *
1:e81e52c:      * <p>
1:e81e52c:      * But if the row does not contain a RowLocation, then the objects
1:e81e52c:      * returned by this method can be either of the following:
1:e81e52c:      * </p>
1:e81e52c:      *
1:e81e52c:      * <ul>
1:e81e52c:      * <li>a DataValueDescriptor[]</li>
1:e81e52c:      * <li>or a List&lt;DataValueDescriptor[]&gt;</li>
1:e81e52c:      * </ul>
1:e81e52c:      *
1:eac0369:      * <p>
1:eac0369:      * The caller will have to call "instanceof" on the data value
1:eac0369:      * object if duplicates are expected, to determine if the data value
1:bd34a3a:      * of the hash table entry is a row or is a list of rows.
1:e81e52c:      * </p>
1:e81e52c:      *
1:e81e52c:      * <p>
1:e81e52c:      * See the javadoc for elements() for more information on the objects
1:e81e52c:      * returned by this method.
1:e81e52c:      * </p>
1:e81e52c:      *
1:eac0369:      * <p>
1:eac0369:      * The BackingStoreHashtable "owns" the objects returned from the get()
1:eac0369:      * routine.  They remain valid until the next access to the 
1:eac0369:      * BackingStoreHashtable.  If the client needs to keep references to these
1:eac0369:      * objects, it should clone copies of the objects.  A valid 
1:eac0369:      * BackingStoreHashtable can place all rows into a disk based conglomerate,
1:eac0369:      * declare a row buffer and then reuse that row buffer for every get()
1:eac0369:      * call.
1:e81e52c:      * </p>
1:eac0369:      *
1:eac0369: 	 * @return The value to which the key is mapped in this hashtable; 
1:eac0369:      *         null if the key is not mapped to any value in this hashtable.
1:eac0369:      *
1:eac0369:      * @param key    The key to hash on.
1:eac0369:      *
1:eac0369: 	 * @exception  StandardException  Standard exception policy.
1:eac0369:      **/
1:eac0369:     public Object get(Object key)
1:eac0369: 		throws StandardException
1:eac0369:     {
1:b223f72:         Object obj = hash_table.get(key);
1:e81e52c:         if ( diskHashtable == null || obj != null)
1:e81e52c:         {
1:b223f72:             return obj;
1:e81e52c:         }
1:e81e52c: 
1:e81e52c:         Object  diskHashtableValue = diskHashtable.get( key );
1:e81e52c:         if ( diskHashtableValue == null )
1:e81e52c:         { return null; }
1:e81e52c: 
1:e81e52c:         if ( diskHashtableValue instanceof List )
1:e81e52c:         {
1:e81e52c:             return makeInMemoryRows( (List) diskHashtableValue );
1:e81e52c:         }
1:e81e52c:         else
1:e81e52c:         {
1:e81e52c:             return makeInMemoryRow
1:e81e52c:                 ( (DataValueDescriptor[]) diskHashtableValue );
1:e81e52c:         }
1:eac0369:     }
1:eac0369: 
1:eac0369:     /**
1:eac0369:      * Return runtime stats to caller by adding them to prop.
1:eac0369:      * <p>
1:eac0369:      *
1:eac0369:      * @param prop   The set of properties to append to.
1:eac0369:      *
1:eac0369: 	 * @exception  StandardException  Standard exception policy.
1:eac0369:      **/
1:eac0369:     public void getAllRuntimeStats(Properties   prop)
1:eac0369: 		throws StandardException
1:eac0369:     {
1:eac0369:         if (auxillary_runtimestats != null)
1:e81e52c:         {
1:e81e52c:             org.apache.derby.iapi.util.PropertyUtil.copyProperties
1:e81e52c:                 (auxillary_runtimestats, prop);
1:e81e52c:         }
1:eac0369:     }
1:eac0369: 
1:eac0369:     /**
1:eac0369:      * remove a row from the hash table.
1:eac0369:      * <p>
1:eac0369:      * a remove of a duplicate removes the entire duplicate list.
1:eac0369:      *
1:eac0369:      * @param key          The key of the row to remove.
1:eac0369:      *
1:eac0369: 	 * @exception  StandardException  Standard exception policy.
1:eac0369:      **/
1:eac0369:     public Object remove(
1:eac0369:     Object      key)
1:eac0369: 		throws StandardException
1:eac0369:     {
1:b223f72:         Object obj = hash_table.remove(key);
1:e81e52c:         if ( obj != null || diskHashtable == null)
1:e81e52c:         {
1:b223f72:             return obj;
1:e81e52c:         }
1:b223f72:         return diskHashtable.remove(key);
1:eac0369:     }
1:eac0369: 
1:eac0369:     /**
1:eac0369:      * Set the auxillary runtime stats.
1:eac0369:      * <p>
1:eac0369:      * getRuntimeStats() will return both the auxillary stats and any
1:eac0369:      * BackingStoreHashtable() specific stats.  Note that each call to
1:eac0369:      * setAuxillaryRuntimeStats() overwrites the Property set that was
1:eac0369:      * set previously.
1:eac0369:      *
1:eac0369:      * @param prop   The set of properties to append from.
1:eac0369:      *
1:eac0369: 	 * @exception  StandardException  Standard exception policy.
1:eac0369:      **/
1:eac0369:     public void setAuxillaryRuntimeStats(Properties   prop)
1:eac0369: 		throws StandardException
1:eac0369:     {
1:eac0369:         auxillary_runtimestats = prop;
1:eac0369:     }
1:eac0369: 
1:eac0369:     /**
1:eac0369:      * Put a row into the hash table.
1:eac0369:      * <p>
1:eac0369:      * The in memory hash table will need to keep a reference to the row
1:eac0369:      * after the put call has returned.  If "needsToClone" is true then the
1:eac0369:      * hash table will make a copy of the row and put that, else if 
1:eac0369:      * "needsToClone" is false then the hash table will keep a reference to
1:eac0369:      * the row passed in and no copy will be made.
1:eac0369:      * <p>
1:af1c18c:      * If routine returns false, then no reference is kept to the duplicate
1:eac0369:      * row which was rejected (thus allowing caller to reuse the object).
1:eac0369:      *
1:eac0369:      * @param needsToClone does this routine have to make a copy of the row,
1:eac0369:      *                     in order to keep a reference to it after return?
1:eac0369:      * @param row          The row to insert into the table.
1:af1c18c:      * @param rowLocation  Location of row in conglomerate; could be null.
1:eac0369:      *
1:eac0369: 	 * @return true if row was inserted into the hash table.  Returns
1:eac0369:      *              false if the BackingStoreHashtable is eliminating 
1:eac0369:      *              duplicates, and the row being inserted is a duplicate,
1:eac0369: 	 *				or if we are skipping rows with 1 or more null key columns
1:eac0369: 	 *				and we find a null key column.
1:eac0369:      *
1:eac0369: 	 * @exception  StandardException  Standard exception policy.
1:eac0369:      **/
1:e81e52c:     public boolean putRow
1:e81e52c:         (
1:e81e52c:          boolean     needsToClone,
1:e81e52c:          DataValueDescriptor[]    row,
1:e81e52c:          RowLocation    rowLocation
1:e81e52c:          )
1:eac0369: 		throws StandardException
1:eac0369:     {
1:eac0369: 		// Are any key columns null?
1:eac0369: 		if (skipNullKeyColumns)
1:eac0369: 		{
1:eac0369: 			int index = 0;
1:eac0369: 			for ( ; index < key_column_numbers.length; index++)
1:eac0369: 			{
1:e8e04bf: 				if (row[key_column_numbers[index]].isNull())
1:eac0369: 				{
1:eac0369: 					return false;
1:eac0369: 				}
1:eac0369: 			}
1:eac0369: 		}
1:eac0369: 
1:ba10aba:         Object key = KeyHasher.buildHashKey(row, key_column_numbers);
1:eac0369: 
1:eac0369:         if ((remove_duplicates) && (get(key) != null))
1:eac0369:         {
1:eac0369:             return(false);
1:eac0369:         }
1:eac0369:         else
1:eac0369:         {
1:e81e52c:             add_row_to_hash_table( row, rowLocation, needsToClone );
1:eac0369:             return(true);
1:eac0369:         }
1:eac0369:     }
1:eac0369: 
1:eac0369:     /**
1:eac0369:      * Return number of unique rows in the hash table.
1:eac0369:      * <p>
1:eac0369:      *
1:eac0369: 	 * @return The number of unique rows in the hash table.
1:eac0369:      *
1:eac0369: 	 * @exception  StandardException  Standard exception policy.
1:eac0369:      **/
1:eac0369:     public int size()
1:eac0369: 		throws StandardException
1:e81e52c:     {
1:e81e52c:         if ( diskHashtable == null)
1:eac0369:         {
1:eac0369:             return(hash_table.size());
1:e81e52c:         }
1:b223f72:         return hash_table.size() + diskHashtable.size();
1:eac0369:     }
1:eac0369: 
1:073b862:     private class BackingStoreHashtableEnumeration implements Enumeration<Object>
1:eac0369:     {
1:073b862:         private Iterator<Object> memoryIterator;
1:073b862:         private Enumeration<Object> diskEnumeration;
1:eac0369: 
1:b223f72:         BackingStoreHashtableEnumeration()
1:eac0369:         {
1:0b3c2c9:             memoryIterator = hash_table.values().iterator();
1:e81e52c:             if ( diskHashtable != null)
1:eac0369:             {
1:b223f72:                 try
1:b223f72:                 {
1:b223f72:                     diskEnumeration = diskHashtable.elements();
1:eac0369:                 }
1:b223f72:                 catch( StandardException se)
7:b223f72:                 {
1:b223f72:                     diskEnumeration = null;
1:eac0369:                 }
1:eac0369:             }
1:eac0369:         }
1:b223f72:         
1:b223f72:         public boolean hasMoreElements()
1:b223f72:         {
1:0b3c2c9:             if (memoryIterator != null) {
1:0b3c2c9:                 if (memoryIterator.hasNext()) {
1:b223f72:                     return true;
1:0b3c2c9:                 }
1:0b3c2c9:                 memoryIterator = null;
4:b223f72:             }
1:e81e52c:             if ( diskEnumeration == null)
1:e81e52c:             {
1:b223f72:                 return false;
1:e81e52c:             }
1:b223f72:             return diskEnumeration.hasMoreElements();
1:b223f72:         }
1:b223f72: 
1:b223f72:         public Object nextElement() throws NoSuchElementException
1:b223f72:         {
1:0b3c2c9:             if (memoryIterator != null) {
1:0b3c2c9:                 if (memoryIterator.hasNext()) {
1:0b3c2c9:                     return memoryIterator.next();
1:0b3c2c9:                 }
1:0b3c2c9:                 memoryIterator = null;
1:b223f72:             }
1:e81e52c:             return makeInMemoryRow
1:e81e52c:                 ( ((DataValueDescriptor[]) diskEnumeration.nextElement()) );
1:b223f72:         }
1:b223f72:     } // end of class BackingStoreHashtableEnumeration
1:fcc3580: 
1:fcc3580:     /**
1:fcc3580:      * List of {@code DataValueDescriptor[]} instances that represent rows.
1:fcc3580:      * This class is used when the hash table contains multiple rows for the
1:fcc3580:      * same hash key.
1:fcc3580:      */
1:e81e52c:     private static class RowList extends ArrayList<Object> {
1:fcc3580: 
1:fcc3580:         private RowList(int initialCapacity) {
1:fcc3580:             super(initialCapacity);
1:fcc3580:         }
1:fcc3580: 
1:fcc3580:         // The class is mostly empty and provides no functionality in addition
1:e81e52c:         // to what's provided by ArrayList<Object>. The main
1:fcc3580:         // purpose of the class is to allow type-safe casts from Object. These
1:fcc3580:         // casts are needed because the hash table can store both DVD[] and
1:fcc3580:         // List<DVD[]>, so its declared type is HashMap<Object, Object>.
1:fcc3580:         // Because of type erasure, casts to ArrayList<DataValueDescriptor[]>
1:fcc3580:         // will make the compiler generate unchecked conversion warnings.
1:fcc3580:         // Casts to RowList, on the other hand, won't cause warnings, as there
1:fcc3580:         // are no parameterized types and type erasure doesn't come into play.
1:fcc3580:     }
1:b223f72: }
============================================================================
author:Richard N. Hillegas
-------------------------------------------------------------------------------
commit:dbed020
/////////////////////////////////////////////////////////////////////////
1: or row[key_column_numbers[0, 1, ...]] if key_column_numbers.length &gt; 1, 
/////////////////////////////////////////////////////////////////////////
1:      * If the number of rows is &lt;= "max_inmemory_rowcnt", then the rows are
1:      * If the number of rows is &gt; "max_inmemory_rowcnt", then the rows will
commit:e81e52c
/////////////////////////////////////////////////////////////////////////
1: import org.apache.derby.iapi.types.LocatedRow;
1: import org.apache.derby.iapi.types.RowLocation;
1: import org.apache.derby.shared.common.sanity.SanityManager;
1: 
/////////////////////////////////////////////////////////////////////////
1: <p>
1: </p>
1: 
1: </p>
/////////////////////////////////////////////////////////////////////////
1: </p>
1: 
1: Object row; // is a DataValueDescriptor[] or a LocatedRow
1: 
/////////////////////////////////////////////////////////////////////////
1: <p>
1: What actually goes into the hash table is a little complicated. That is because
1: the row may either be an array of column values (i.e. DataValueDescriptor[])
1: or a LocatedRow (i.e., a structure holding the columns plus a RowLocation).
1: In addition, the hash value itself may either be one of these rows or
1: (in the case of multiple rows which hash to the same value) a bucket (List)
1: of rows. To sum this up, the values in a hash table which does not spill
1: to disk may be the following:
1: </p>
1: 
1: <ul>
1: <li>DataValueDescriptor[] and ArrayList<DataValueDescriptor></li>
1: <li>or LocatedRow and ArrayList<LocatedRow></li>
1: </ul>
1: 
1: <p>
1: If rows spill to disk, then they just become arrays of columns. In this case,
1: a LocatedRow becomes a DataValueDescriptor[], where the last cell contains
1: the RowLocation.
1: </p>
1: 
/////////////////////////////////////////////////////////////////////////
1:      * constructor. RowLocations are supported iff row_source is null.
1:      * RowLocations in a non-null row_source can be added later
1:      * if there is a use-case that stresses this behavior.
/////////////////////////////////////////////////////////////////////////
1:         if ( max_inmemory_rowcnt > 0)
1:         {
1:         }
1:         {
1:         }
1:         if (SanityManager.DEBUG)
1:         {
1:             // RowLocations are not currently supported if the
1:             // hash table is being filled from a non-null
1:             // row_source arg.
1:             if ( row_source != null )
1:             {
1:                 SanityManager.ASSERT( !includeRowLocations() );
1:             }
1:         }
1: 
/////////////////////////////////////////////////////////////////////////
1:                 add_row_to_hash_table(row, null, needsToClone);
/////////////////////////////////////////////////////////////////////////
1:         {
1:         }
1:     }
1: 
1:     /**
1:      * Return true if we should include RowLocations with the rows
1:      * stored in this hash table.
1:      */
1:     public  boolean includeRowLocations()
1:     {
1:         return false;
/////////////////////////////////////////////////////////////////////////
1: 	 * key columns. Currently, RowLocation information is not included in
1:      * rows siphoned out of a RowSource. That functionality is only supported
1:      * if the rows come from the scan of a base table.
/////////////////////////////////////////////////////////////////////////
1:             if ( old_row[i] != null)
1:             {
1:             }
/////////////////////////////////////////////////////////////////////////
1:             if ( old_row[i] != null)
1:             {
1:             }
/////////////////////////////////////////////////////////////////////////
1:      * @param columnValues               Row to add to the hash table.
1:      * @param rowLocation   Location of row in conglomerate; could be null.
1:     private void add_row_to_hash_table
1:         (
1:          DataValueDescriptor[] columnValues,
1:          RowLocation rowLocation,
1:          boolean needsToClone
1:          )
1:         if (spillToDisk( columnValues, rowLocation ))
1:         {
1:         }
1:             columnValues = cloneRow( columnValues );
1:         Object key = KeyHasher.buildHashKey( columnValues, key_column_numbers );
1:         Object hashValue = !includeRowLocations() ?
1:             columnValues : new LocatedRow( columnValues, rowLocation );
1:         Object duplicate_value = hash_table.put( key, hashValue );
1:         {
1:             doSpaceAccounting( hashValue, false );
1:         }
/////////////////////////////////////////////////////////////////////////
1:                     doSpaceAccounting( hashValue, false );
/////////////////////////////////////////////////////////////////////////
1:                     row_vec.add( duplicate_value );
1:                     doSpaceAccounting( hashValue, true );
1:                 row_vec.add( hashValue );
/////////////////////////////////////////////////////////////////////////
1:     private void doSpaceAccounting(Object hashValue,
1:         if ( max_inmemory_rowcnt <= 0)
1:             max_inmemory_size -= getEstimatedMemUsage( hashValue );
1:             if ( firstDuplicate)
1:             {
1:             }
1:      * @param columnValues  Actual columns from source row.
1:      * @param rowLocation       Optional row location.
1:     private boolean spillToDisk
1:         (
1:          DataValueDescriptor[] columnValues,
1:          RowLocation rowLocation
1:          )
1:         throws StandardException
1:     {
1: 
1:         DataValueDescriptor[]   diskRow = null;
1:         
1:         if ( diskHashtable == null)
1:             if ( max_inmemory_rowcnt > 0)
1:                 if ( inmemory_rowcnt < max_inmemory_rowcnt)
1:                 {
1:                 }
1:             else if
1:                 (
1:                  max_inmemory_size >
1:                  getEstimatedMemUsage
1:                  (
1:                   !includeRowLocations() ?
1:                   columnValues : new LocatedRow( columnValues, rowLocation )
1:                  )
1:                 )
1:             {
1:             }
1:             
1:             diskRow = makeDiskRow( columnValues, rowLocation );
1:                        diskRow,
1:         Object key = KeyHasher.buildHashKey( columnValues, key_column_numbers );
1:         if ( duplicateValue != null)
1:             if ( remove_duplicates)
/////////////////////////////////////////////////////////////////////////
1:                     diskHashtable.put
1:                         ( key, makeDiskRow( duplicateVec.get( i ) ));
1:             {
1:                 diskHashtable.put( key, makeDiskRow( duplicateValue ) );
1:             }
1: 
1:         if ( diskRow == null )
1:         { diskRow = makeDiskRow( columnValues, rowLocation ); }
1:         
1:         diskHashtable.put( key, diskRow );
1:      * <p>
1:      * Make a full set of columns from an object which is either already
1:      * an array of column or otherwise a LocatedRow. The full set of columns
1:      * is what's stored on disk when we spill to disk. This is the inverse of
1:      * makeInMemoryRow().
1:      * </p>
1:      */
1:     private DataValueDescriptor[]   makeDiskRow( Object raw )
1:     {
1:         DataValueDescriptor[]   allColumns = null;
1:         if ( includeRowLocations() )
1:         {
1:             LocatedRow  locatedRow = (LocatedRow) raw;
1:             allColumns = makeDiskRow
1:                 ( locatedRow.columnValues(), locatedRow.rowLocation() );
1:         }
1:         else { allColumns = (DataValueDescriptor[]) raw; }
1: 
1:         return allColumns;
1:     }
1: 
1:     /**
1:      * <p>
1:      * Turn a list of disk rows into a list of in-memory rows. The on disk
1:      * rows are always of type DataValueDescriptor[]. But the in-memory rows
1:      * could be of type LocatedRow.
1:      * </p>
1:      */
1:     private List    makeInMemoryRows( List diskRows )
1:     {
1:         if ( !includeRowLocations() )
1:         {
1:             return diskRows;
1:         }
1:         else
1:         {
1:             ArrayList<Object>   result = new ArrayList<Object>();
1:             for ( Object diskRow : diskRows )
1:             {
1:                 result.add
1:                     ( makeInMemoryRow( (DataValueDescriptor[]) diskRow ) );
1:             }
1: 
1:             return result;
1:         }
1:     }
1: 
1:     /**
1:      * <p>
1:      * Make an in-memory row from an on-disk row. This is the inverse
1:      * of makeDiskRow().
1:      * </p>
1:      */
1:     private Object  makeInMemoryRow( DataValueDescriptor[] diskRow )
1:     {
1:         if ( !includeRowLocations() )
1:         {
1:             return diskRow;
1:         }
1:         else
1:         {
1:             return new LocatedRow( diskRow );
1:         }
1:     }
1: 
1:     /**
1:      * <p>
1:      * Construct a full set of columns, which may need to end
1:      * with the row location.The full set of columns is what's
1:      * stored on disk when we spill to disk.
1:      * </p>
1:      */
1:     private DataValueDescriptor[]   makeDiskRow
1:         ( DataValueDescriptor[] columnValues, RowLocation rowLocation )
1:     {
1:         if ( !includeRowLocations() )
1:         {
1:             return columnValues;
1:         }
1:         else
1:         {
1:             return LocatedRow.flatten( columnValues, rowLocation );
1:         }
1:     }
1: 
1:     /**
1:      * Take a value which will go into the hash table and return an estimate
1:      * of how much memory that value will consume. The hash value could
1:      * be either an array of columns or a LocatedRow.
1:      * @param hashValue The object for which we want to know the memory usage.
1:      * @return A guess as to how much memory the current hash value will
1:     private long getEstimatedMemUsage( Object hashValue )
1:         DataValueDescriptor[] row = null;
1: 
1:         if ( hashValue instanceof DataValueDescriptor[] )
1:         {
1:             row = (DataValueDescriptor[]) hashValue;
1:         }
1:         else
1:         {
1:             LocatedRow  locatedRow = (LocatedRow) hashValue;
1:             row = locatedRow.columnValues();
1: 
1:             // account for the RowLocation size and class overhead
1:             RowLocation rowLocation = locatedRow.rowLocation();
1:             if ( rowLocation != null )
1:             {
1:                 rowMem += locatedRow.rowLocation().estimateMemoryUsage();
1:                 rowMem += ClassSize.refSize;
1:             }
1: 
1:             // account for class overhead of the LocatedRow itself
1:             rowMem += ClassSize.refSize;
1:         }
1:         
1:             // account for the column's size and class overhead
1:         // account for the class overhead of the array itself
/////////////////////////////////////////////////////////////////////////
1:         if ( diskHashtable != null)
/////////////////////////////////////////////////////////////////////////
1:      * <p>
1:      * Return an Enumeration that can be used to scan the entire table. The objects
1:      * in the Enumeration can be either of the following:
1:      * </p>
1:      *
1:      * <ul>
1:      * <li>a row - This is a single row with a unique hash key.</li>
1:      * <li>or a bucket of rows - This is a list of rows which all have the same hash key.</li>
1:      * </ul>
1:      *
1:      * <p>
1:      * The situation is a little more complicated because the row representation
1:      * is different depending on whether the row includes a RowLocation.
1:      * If includeRowLocations()== true, then the row is a LocatedRow. Otherwise,
1:      * the row is an array of DataValueDescriptor. Putting all of this together,
1:      * if the row contains a RowLocation, then the objects in the Enumeration returned
1:      * by this method can be either of the following:
1:      * </p>
1:      *
1:      * <ul>
1:      * <li>a LocatedRow</li>
1:      * <li>or a List&lt;LocatedRow&gt;</li>
1:      * </ul>
1:      *
1:      * <p>
1:      * But if the row does not contain a RowLocation, then the objects in the
1:      * Enumeration returned by this method can be either of the following:
1:      * </p>
1:      *
1:      * <ul>
1:      * <li>a DataValueDescriptor[]</li>
1:      * <li>or a List&lt;DataValueDescriptor[]&gt;</li>
1:      * </ul>
1:      *
1:      * </p>
/////////////////////////////////////////////////////////////////////////
1:         if ( diskHashtable == null)
1:         {
1:         }
1:      * <p>
1:      * Get data associated with given key.
1:      * </p>
1:      *
1:      * </p>
1:      *
/////////////////////////////////////////////////////////////////////////
1:      * </p>
1:      *
1:      * </p>
1:      *
1:      * <p>
1:      * The situation is a little more complicated because the row representation
1:      * is different depending on whether the row includes a RowLocation.
1:      * If includeRowLocations() == true, then the row is a LocatedRow. Otherwise,
1:      * the row is an array of DataValueDescriptor. Putting all of this together,
1:      * if the row contains a RowLocation, then the objects returned by this method
1:      * can be either of the following:
1:      * </p>
1:      *
1:      * <ul>
1:      * <li>a LocatedRow</li>
1:      * <li>or a List&lt;LocatedRow&gt;</li>
1:      * </ul>
1:      *
1:      * <p>
1:      * But if the row does not contain a RowLocation, then the objects
1:      * returned by this method can be either of the following:
1:      * </p>
1:      *
1:      * <ul>
1:      * <li>a DataValueDescriptor[]</li>
1:      * <li>or a List&lt;DataValueDescriptor[]&gt;</li>
1:      * </ul>
1:      *
1:      * </p>
1:      *
1:      * <p>
1:      * See the javadoc for elements() for more information on the objects
1:      * returned by this method.
1:      * </p>
1:      *
/////////////////////////////////////////////////////////////////////////
1:      * </p>
/////////////////////////////////////////////////////////////////////////
1:         if ( diskHashtable == null || obj != null)
1:         {
1:         }
1: 
1:         Object  diskHashtableValue = diskHashtable.get( key );
1:         if ( diskHashtableValue == null )
1:         { return null; }
1: 
1:         if ( diskHashtableValue instanceof List )
1:         {
1:             return makeInMemoryRows( (List) diskHashtableValue );
1:         }
1:         else
1:         {
1:             return makeInMemoryRow
1:                 ( (DataValueDescriptor[]) diskHashtableValue );
1:         }
/////////////////////////////////////////////////////////////////////////
1:         {
1:             org.apache.derby.iapi.util.PropertyUtil.copyProperties
1:                 (auxillary_runtimestats, prop);
1:         }
/////////////////////////////////////////////////////////////////////////
1:         if ( obj != null || diskHashtable == null)
1:         {
1:         }
/////////////////////////////////////////////////////////////////////////
1:     public boolean putRow
1:         (
1:          boolean     needsToClone,
1:          DataValueDescriptor[]    row,
1:          RowLocation    rowLocation
1:          )
/////////////////////////////////////////////////////////////////////////
1:             add_row_to_hash_table( row, rowLocation, needsToClone );
/////////////////////////////////////////////////////////////////////////
1:         if ( diskHashtable == null)
1:         {
1:         }
/////////////////////////////////////////////////////////////////////////
1:             if ( diskHashtable != null)
/////////////////////////////////////////////////////////////////////////
1:             if ( diskEnumeration == null)
1:             {
1:             }
/////////////////////////////////////////////////////////////////////////
1:             return makeInMemoryRow
1:                 ( ((DataValueDescriptor[]) diskEnumeration.nextElement()) );
/////////////////////////////////////////////////////////////////////////
1:     private static class RowList extends ArrayList<Object> {
1:         // to what's provided by ArrayList<Object>. The main
commit:073b862
/////////////////////////////////////////////////////////////////////////
1:     private HashMap<Object,Object>     hash_table;
/////////////////////////////////////////////////////////////////////////
1:                      new HashMap<Object,Object>(initialCapacity) :
1:                      new HashMap<Object,Object>(initialCapacity, loadFactor));
/////////////////////////////////////////////////////////////////////////
1:                      new HashMap<Object,Object>() :
1:                          new HashMap<Object,Object>((int) estimated_rowcnt) :
/////////////////////////////////////////////////////////////////////////
1:                         new HashMap<Object,Object>((int)(max_inmemory_size / rowUsage));
/////////////////////////////////////////////////////////////////////////
1:             hash_table = new HashMap<Object,Object>();
/////////////////////////////////////////////////////////////////////////
0:     @SuppressWarnings("unchecked")
/////////////////////////////////////////////////////////////////////////
0:                 List<Object> row_vec;
0:                     row_vec = (List<Object>) duplicate_value;
0:                     row_vec = new ArrayList<Object>(2);
/////////////////////////////////////////////////////////////////////////
1:     public Enumeration<Object> elements()
/////////////////////////////////////////////////////////////////////////
1:     private class BackingStoreHashtableEnumeration implements Enumeration<Object>
1:         private Iterator<Object> memoryIterator;
1:         private Enumeration<Object> diskEnumeration;
commit:75c7276
/////////////////////////////////////////////////////////////////////////
1:    Licensed to the Apache Software Foundation (ASF) under one or more
1:    contributor license agreements.  See the NOTICE file distributed with
1:    this work for additional information regarding copyright ownership.
1:    The ASF licenses this file to you under the Apache License, Version 2.0
1:    (the "License"); you may not use this file except in compliance with
1:    the License.  You may obtain a copy of the License at
author:Dag H. Wanvik
-------------------------------------------------------------------------------
commit:af1c18c
/////////////////////////////////////////////////////////////////////////
1:      * If routine returns false, then no reference is kept to the duplicate
1:      * @param rowLocation  Location of row in conglomerate; could be null.
author:Knut Anders Hatlen
-------------------------------------------------------------------------------
commit:fcc3580
/////////////////////////////////////////////////////////////////////////
/////////////////////////////////////////////////////////////////////////
1:         Object key = KeyHasher.buildHashKey(row, key_column_numbers);
0:         Object duplicate_value = hash_table.put(key, row);
1: 
1:         if (duplicate_value == null)
1:                 RowList row_vec;
1:                 if (duplicate_value instanceof RowList)
1:                     row_vec = (RowList) duplicate_value;
1:                     row_vec = new RowList(2);
0:                     row_vec.add((DataValueDescriptor[]) duplicate_value);
/////////////////////////////////////////////////////////////////////////
/////////////////////////////////////////////////////////////////////////
1: 
1:     /**
1:      * List of {@code DataValueDescriptor[]} instances that represent rows.
1:      * This class is used when the hash table contains multiple rows for the
1:      * same hash key.
1:      */
0:     private static class RowList extends ArrayList<DataValueDescriptor[]> {
1: 
1:         private RowList(int initialCapacity) {
1:             super(initialCapacity);
1:         }
1: 
1:         // The class is mostly empty and provides no functionality in addition
0:         // to what's provided by ArrayList<DataValueDescriptor[]>. The main
1:         // purpose of the class is to allow type-safe casts from Object. These
1:         // casts are needed because the hash table can store both DVD[] and
1:         // List<DVD[]>, so its declared type is HashMap<Object, Object>.
1:         // Because of type erasure, casts to ArrayList<DataValueDescriptor[]>
1:         // will make the compiler generate unchecked conversion warnings.
1:         // Casts to RowList, on the other hand, won't cause warnings, as there
1:         // are no parameterized types and type erasure doesn't come into play.
1:     }
commit:bd34a3a
/////////////////////////////////////////////////////////////////////////
1: import java.util.ArrayList;
1: import java.util.List;
/////////////////////////////////////////////////////////////////////////
1:      * The estimated number of bytes used by ArrayList(0)
1:     private final static int ARRAY_LIST_SIZE =
1:         ClassSize.estimateBaseFromCatalog(ArrayList.class);
/////////////////////////////////////////////////////////////////////////
1:      *                          duplicates, or should it create the list of
/////////////////////////////////////////////////////////////////////////
0:                 List row_vec;
0:                 if (duplicate_value instanceof List)
0:                     row_vec = (List) duplicate_value;
1:                     // allocate list to hold duplicates
0:                     row_vec = new ArrayList(2);
0:                     row_vec.add(duplicate_value);
1:                 // insert new row into list
0:                 row_vec.add(row);
1:                 // store list of rows back into hash table,
/////////////////////////////////////////////////////////////////////////
1:                 max_inmemory_size -= ARRAY_LIST_SIZE;
/////////////////////////////////////////////////////////////////////////
1:             if (duplicateValue instanceof List)
1:                 List duplicateVec = (List) duplicateValue;
0:                     Object[] dupRow = (Object[]) duplicateVec.get(i);
/////////////////////////////////////////////////////////////////////////
1:      * For row values with duplicates, the value of the data is a list of
1:      * of the hash table entry is a row or is a list of rows.
commit:0b3c2c9
/////////////////////////////////////////////////////////////////////////
1: import java.util.Collections;
1: import java.util.HashMap;
1: import java.util.Iterator;
/////////////////////////////////////////////////////////////////////////
0:     private HashMap     hash_table;
/////////////////////////////////////////////////////////////////////////
1:      * inserted into a java.util.HashMap. In this case no
/////////////////////////////////////////////////////////////////////////
1:      *                          scan result row to be the key to the HashMap.
1:      * @param remove_duplicates Should the HashMap automatically remove
/////////////////////////////////////////////////////////////////////////
1:      * @param initialCapacity   If not "-1" used to initialize the java HashMap
1:      * @param loadFactor        If not "-1" used to initialize the java HashMap
/////////////////////////////////////////////////////////////////////////
0:                      new HashMap(initialCapacity) :
0:                      new HashMap(initialCapacity, loadFactor));
/////////////////////////////////////////////////////////////////////////
1:              * create a hash table of size estimated_rowcnt can cause
1:              * OutOfMemory errors when we try to create the hash table.
1:              * it's very likely that creating a hash table with a capacity
1:              * for the hash table might be.
0:                      new HashMap() :
0:                          new HashMap((int) estimated_rowcnt) :
/////////////////////////////////////////////////////////////////////////
1:                 // because a hash table with capacity estimated_rowcnt would
/////////////////////////////////////////////////////////////////////////
1:                     // capacity of the hash table.
1:                     hash_table =
0:                         new HashMap((int)(max_inmemory_size / rowUsage));
0:                 add_row_to_hash_table(row, needsToClone);
/////////////////////////////////////////////////////////////////////////
0:             hash_table = new HashMap();
/////////////////////////////////////////////////////////////////////////
0:     private void add_row_to_hash_table(Object[] row, boolean needsToClone)
0:         if (spillToDisk(row))
/////////////////////////////////////////////////////////////////////////
0:     private boolean spillToDisk(Object[] row) throws StandardException {
/////////////////////////////////////////////////////////////////////////
1:             return Collections.enumeration(hash_table.values());
/////////////////////////////////////////////////////////////////////////
1:      * hash table.
0:      * of the hash table entry is a row or is a Vector of rows.
/////////////////////////////////////////////////////////////////////////
0:             add_row_to_hash_table(row, needsToClone);
/////////////////////////////////////////////////////////////////////////
0:         private Iterator memoryIterator;
1:             memoryIterator = hash_table.values().iterator();
/////////////////////////////////////////////////////////////////////////
1:             if (memoryIterator != null) {
1:                 if (memoryIterator.hasNext()) {
1:                 }
1:                 memoryIterator = null;
/////////////////////////////////////////////////////////////////////////
1:             if (memoryIterator != null) {
1:                 if (memoryIterator.hasNext()) {
1:                     return memoryIterator.next();
1:                 }
1:                 memoryIterator = null;
commit:ba10aba
/////////////////////////////////////////////////////////////////////////
1: <pre>
/////////////////////////////////////////////////////////////////////////
1:     Object key = KeyHasher.buildHashKey(row, key_column_numbers);
/////////////////////////////////////////////////////////////////////////
1: </pre>
author:Kristian Waagan
-------------------------------------------------------------------------------
commit:854dd10
/////////////////////////////////////////////////////////////////////////
1:                 new_row[i] = old_row[i].cloneValue(false);
commit:d7aa761
/////////////////////////////////////////////////////////////////////////
1:         //          here (i.e. used cloneObject, not getClone).
1:         //          We still don't materialize, just clone the holder.
1:                 new_row[i] = old_row[i].cloneHolder();
commit:b4e2eb7
/////////////////////////////////////////////////////////////////////////
/////////////////////////////////////////////////////////////////////////
1:         // History: We used to materialize streams when getting a clone
1:         //          here (i.e. used getClone, not cloneObject). We still do.
1:         // Beetle 4896.
/////////////////////////////////////////////////////////////////////////
1:         // History: We used to *not* materialize streams when getting a clone
0:         //          here (i.e. used cloneObject, not getClone). We still do.
1:         // DERBY-802
0:                 new_row[i] = old_row[i].cloneObject();
author:Daniel John Debrunner
-------------------------------------------------------------------------------
commit:e8e04bf
/////////////////////////////////////////////////////////////////////////
/////////////////////////////////////////////////////////////////////////
1:             DataValueDescriptor[] row;
/////////////////////////////////////////////////////////////////////////
1: 	private DataValueDescriptor[] getNextRowFromRowSource()
1: 		DataValueDescriptor[] row = row_source.getNextRowFromRowSource();
/////////////////////////////////////////////////////////////////////////
1: 					if (row[key_column_numbers[index]].isNull())
/////////////////////////////////////////////////////////////////////////
1:     private static DataValueDescriptor[] cloneRow(DataValueDescriptor[] old_row)
1:         DataValueDescriptor[] new_row = new DataValueDescriptor[old_row.length];
0:                 new_row[i] = old_row[i].getClone();
/////////////////////////////////////////////////////////////////////////
0:     private void add_row_to_hash_table(DataValueDescriptor[] row, boolean needsToClone)
/////////////////////////////////////////////////////////////////////////
0:     private void doSpaceAccounting(DataValueDescriptor[] row,
/////////////////////////////////////////////////////////////////////////
0:     private boolean spillToDisk(DataValueDescriptor[] row) throws StandardException {
/////////////////////////////////////////////////////////////////////////
1:  
0:                        row,
/////////////////////////////////////////////////////////////////////////
0:                     DataValueDescriptor[] dupRow = (DataValueDescriptor[]) duplicateVec.get(i);
0:                 diskHashtable.put( key, (DataValueDescriptor[]) duplicateValue);
/////////////////////////////////////////////////////////////////////////
0:     private long getEstimatedMemUsage(DataValueDescriptor[] row)
1:             rowMem += row[i].estimateMemoryUsage();
/////////////////////////////////////////////////////////////////////////
0:     public boolean putRow(
0:     DataValueDescriptor[]    row)
/////////////////////////////////////////////////////////////////////////
1: 				if (row[key_column_numbers[index]].isNull())
commit:381dfaf
/////////////////////////////////////////////////////////////////////////
1:     /**
0:      * The estimated number of bytes used by Vector(0)
1:      */  
0:     private final static int vectorSize = ClassSize.estimateBaseFromCatalog(java.util.Vector.class);
commit:7ecc1f2
/////////////////////////////////////////////////////////////////////////
1:    Derby - Class org.apache.derby.iapi.store.access.BackingStoreHashtable
1: 
0:    Copyright 1999, 2004 The Apache Software Foundation or its licensors, as applicable.
1: 
0:    Licensed under the Apache License, Version 2.0 (the "License");
0:    you may not use this file except in compliance with the License.
0:    You may obtain a copy of the License at
1: 
1:       http://www.apache.org/licenses/LICENSE-2.0
1: 
1:    Unless required by applicable law or agreed to in writing, software
1:    distributed under the License is distributed on an "AS IS" BASIS,
1:    WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
1:    See the License for the specific language governing permissions and
1:    limitations under the License.
commit:76addbc
/////////////////////////////////////////////////////////////////////////
commit:eac0369
/////////////////////////////////////////////////////////////////////////
1: /*
1: 
0:    Licensed Materials - Property of IBM
0:    Cloudscape - Package org.apache.derby.iapi.store.access
0:    (C) Copyright IBM Corp. 1999, 2004. All Rights Reserved.
0:    US Government Users Restricted Rights - Use, duplication or
0:    disclosure restricted by GSA ADP Schedule Contract with IBM Corp.
1: 
1:  */
1: 
1: package org.apache.derby.iapi.store.access;
1: 
0: import org.apache.derby.iapi.services.sanity.SanityManager;
1: 
0: import org.apache.derby.iapi.services.io.Storable;
1: 
1: import org.apache.derby.iapi.error.StandardException; 
1: 
0: import org.apache.derby.iapi.types.CloneableObject;
1: import org.apache.derby.iapi.types.DataValueDescriptor;
1: 
1: import java.util.Enumeration;
1: import java.util.Hashtable;
1: import java.util.Properties; 
0: import java.util.Vector;
1: 
1: /**
1: A BackingStoreHashtable is a utility class which will store a set of rows into
1: an in memory hash table, or overflow the hash table to a tempory on disk 
1: structure.
1: <p>
1: All rows must contain the same number of columns, and the column at position
1: N of all the rows must have the same format id.  If the BackingStoreHashtable needs to be
1: overflowed to disk, then an arbitrary row will be chosen and used as a template
1: for creating the underlying overflow container.
1: 
1: <p>
1: The hash table will be built logically as follows (actual implementation
1: may differ).  The important points are that the hash value is the standard
1: java hash value on the row[key_column_numbers[0], if key_column_numbers.length is 1,
0: or row[key_column_numbers[0, 1, ...]] if key_column_numbers.length > 1, 
1: and that duplicate detection is done by the standard java duplicate detection provided by 
1: java.util.Hashtable.
1: <p>
1: import java.util.Hashtable;
1: 
1: hash_table = new Hashtable();
1: 
0: Object[] row;
1: boolean  needsToClone = rowSource.needsToClone();
1: 
1: while((row = rowSource.getNextRowFromRowSource()) != null)
1: {
1:     if (needsToClone)
1:         row = clone_row_from_row(row);
1: 
0: 	Object key = KeyHasher.buildHashKey(row, key_column_numbers);
1: 
1:     if ((duplicate_value = 
1:         hash_table.put(key, row)) != null)
1:     {
1:         Vector row_vec;
1: 
1:         // inserted a duplicate
1:         if ((duplicate_value instanceof vector))
1:         {
1:             row_vec = (Vector) duplicate_value;
1:         }
1:         else
1:         {
1:             // allocate vector to hold duplicates
1:             row_vec = new Vector(2);
1: 
1:             // insert original row into vector
1:             row_vec.addElement(duplicate_value);
1: 
1:             // put the vector as the data rather than the row
1:             hash_table.put(key, row_vec);
1:         }
1:         
1:         // insert new row into vector
1:         row_vec.addElement(row);
1:     }
1: }
1: 
1: **/
1: 
1: public class BackingStoreHashtable
1: {
1: 	/**
0: 		IBM Copyright &copy notice.
1: 	*/
0: 	public static final String copyrightNotice = org.apache.derby.iapi.reference.Copyright.SHORT_1999_2004;
1: 
1:     /**************************************************************************
1:      * Fields of the class
1:      **************************************************************************
1:      */
0:     private Hashtable   hash_table;
1:     private int[]       key_column_numbers;
1:     private boolean     remove_duplicates;
1: 	private boolean		skipNullKeyColumns;
1:     private Properties  auxillary_runtimestats;
1: 	private RowSource	row_source;
1: 
1:     /**************************************************************************
1:      * Constructors for This class:
1:      **************************************************************************
1:      */
1:     private BackingStoreHashtable(){}
1: 
1:     /**
1:      * Create the BackingStoreHashtable from a row source.
1:      * <p>
1:      * This routine drains the RowSource.  The performance characteristics
1:      * depends on the number of rows inserted and the parameters to the 
0:      * constructor.  
1:      * <p>
0:      * If the number of rows is <= "max_inmemory_rowcnt", then the rows are
0:      * inserted into a java.util.Hashtable.  In this case no 
1:      * TransactionController is necessary, a "null" tc is valid.
1:      * <p>
0:      * If the number of rows is > "max_inmemory_rowcnt", then the rows will
1:      * be all placed in some sort of Access temporary file on disk.  This 
1:      * case requires a valid TransactionController.
1:      *
0: 	 * @return The identifier to be used to open the conglomerate later.
1:      *
1:      * @param tc                An open TransactionController to be used if the
1:      *                          hash table needs to overflow to disk.
1:      *
1:      * @param row_source        RowSource to read rows from.
1:      *
1:      * @param key_column_numbers The column numbers of the columns in the
0:      *                          scan result row to be the key to the Hashtable.
1:      *                          "0" is the first column in the scan result
1:      *                          row (which may be different than the first
1:      *                          row in the table of the scan).
1:      *
0:      * @param remove_duplicates Should the Hashtable automatically remove
0:      *                          duplicates, or should it create the Vector of
1:      *                          duplicates?
1:      *
1:      * @param estimated_rowcnt  The estimated number of rows in the hash table.
1:      *                          Pass in -1 if there is no estimate.
1:      *
1:      * @param max_inmemory_rowcnt
1:      *                          The maximum number of rows to insert into the 
1:      *                          inmemory Hash table before overflowing to disk.
1:      *                          Pass in -1 if there is no maximum.
1:      *
0:      * @param initialCapacity   If not "-1" used to initialize the java 
0:      *                          Hashtable.
1:      *
0:      * @param loadFactor        If not "-1" used to initialize the java 
0:      *                          Hashtable.
1: 	 *
1: 	 * @param skipNullKeyColumns	Skip rows with a null key column, if true.
1:      *
1:      *
1: 	 * @exception  StandardException  Standard exception policy.
1:      **/
1:     public BackingStoreHashtable(
1:     TransactionController   tc,
1:     RowSource               row_source,
1:     int[]                   key_column_numbers,
1:     boolean                 remove_duplicates,
1:     long                    estimated_rowcnt,
1:     long                    max_inmemory_rowcnt,
1:     int                     initialCapacity,
1:     float                   loadFactor,
0: 	boolean					skipNullKeyColumns)
1:         throws StandardException
1:     {
1:         this.key_column_numbers    = key_column_numbers;
1:         this.remove_duplicates    = remove_duplicates;
1: 		this.row_source			   = row_source;
1: 		this.skipNullKeyColumns	   = skipNullKeyColumns;
1: 
0:         Object[] row;
1: 
1:         // use passed in capacity and loadfactor if not -1, you must specify
1:         // capacity if you want to specify loadfactor.
1:         if (initialCapacity != -1)
1:         {
1:             hash_table = 
1:                 ((loadFactor == -1) ? 
0:                      new Hashtable(initialCapacity) : 
0:                      new Hashtable(initialCapacity, loadFactor));
1:         }
1:         else
1:         {
1:             hash_table = 
0:                 ((estimated_rowcnt <= 0) ? 
0:                      new Hashtable() : new Hashtable((int) estimated_rowcnt));
1:         }
1: 
1:         if (row_source != null)
1:         {
1:             boolean needsToClone = row_source.needsToClone();
1: 
1:             while ((row = getNextRowFromRowSource()) != null)
1:             {
1: 
1:                 if (needsToClone)
1:                 {
0:                     row = cloneRow(row);
1:                 }
1: 
0:                 Object key = 
0:                     KeyHasher.buildHashKey(row, key_column_numbers);
1: 
0:                 add_row_to_hash_table(hash_table, key, row);
1:             }
1:         }
1:     }
1: 
1:     /**************************************************************************
1:      * Private/Protected methods of This class:
1:      **************************************************************************
1:      */
1: 
1: 	/**
1: 	 * Call method to either get next row or next row with non-null
0: 	 * key columns.
1: 	 *
1:      *
1: 	 * @exception  StandardException  Standard exception policy.
1: 	 */
0: 	private Object[] getNextRowFromRowSource()
1: 		throws StandardException
1: 	{
0: 		Object[] row = row_source.getNextRowFromRowSource();
1: 
1: 		if (skipNullKeyColumns)
1: 		{
1: 			while (row != null)
1: 			{
1: 				// Are any key columns null?
1: 				int index = 0;
1: 				for ( ; index < key_column_numbers.length; index++)
1: 				{
0: 					if (SanityManager.DEBUG)
1: 					{
0: 						if (! (row[key_column_numbers[index]] instanceof Storable))
1: 						{
0: 							SanityManager.THROWASSERT(
0: 								"row[key_column_numbers[index]] expected to be Storable, not " +
0: 								row[key_column_numbers[index]].getClass().getName());
1: 						}
1: 					}
0: 					Storable storable = (Storable) row[key_column_numbers[index]];
0: 					if (storable.isNull())
1: 					{
1: 						break;
1: 					}
1: 				}
1: 				// No null key columns
1: 				if (index == key_column_numbers.length)
1: 				{
1: 					return row;
1: 				}
1: 				// 1 or more null key columns
1: 				row = row_source.getNextRowFromRowSource();
1: 			}
1: 		}
1: 		return row;
1: 	}
1: 
1:     /**
1:      * Return a cloned copy of the row.
1:      *
1: 	 * @return The cloned row row to use.
1:      *
1: 	 * @exception  StandardException  Standard exception policy.
1:      **/
0:     private Object[] cloneRow(Object[] old_row)
1:         throws StandardException
1:     {
0:         Object[] new_row = new DataValueDescriptor[old_row.length];
1: 
0: 		// the only difference between getClone and cloneObject is cloneObject does
0: 		// not objectify a stream.  We use getClone here.  Beetle 4896.
1:         for (int i = 0; i < old_row.length; i++)
0:             new_row[i] = ((DataValueDescriptor) old_row[i]).getClone();
1: 
1:         return(new_row);
1:     }
1: 
1:     /**
1:      * Do the work to add one row to the hash table.
1:      * <p>
1:      *
0:      * @param row               Row to add to the hash table.
0:      * @param hash_table        The java HashTable to load into.
1:      *
0: 	 * @return true if successful, false if heap add fails.
1:      *
1: 	 * @exception  StandardException  Standard exception policy.
1:      **/
0:     private void add_row_to_hash_table(
0:     Hashtable   hash_table,
0:     Object      key,
0:     Object[]    row)
1: 		throws StandardException
1:     {
0:         Object  duplicate_value = null;
1: 
0:         if ((duplicate_value = hash_table.put(key, row)) != null)
1:         {
1:             if (!remove_duplicates)
1:             {
1:                 Vector row_vec;
1: 
1:                 // inserted a duplicate
0:                 if ((duplicate_value instanceof Vector))
1:                 {
1:                     row_vec = (Vector) duplicate_value;
1:                 }
1:                 else
1:                 {
1:                     // allocate vector to hold duplicates
1:                     row_vec = new Vector(2);
1: 
1:                     // insert original row into vector
1:                     row_vec.addElement(duplicate_value);
1:                 }
1: 
1:                 // insert new row into vector
1:                 row_vec.addElement(row);
1: 
0:                 // store vector of rows back into hash table,
1:                 // overwriting the duplicate key that was 
1:                 // inserted.
1:                 hash_table.put(key, row_vec);
1:             }
1:         }
1: 
0:         row = null;
1:     }
1: 
1:     /**************************************************************************
1:      * Public Methods of This class:
1:      **************************************************************************
1:      */
1: 
1:     /**
1:      * Close the BackingStoreHashtable.
1:      * <p>
1:      * Perform any necessary cleanup after finishing with the hashtable.  Will
1:      * deallocate/dereference objects as necessary.  If the table has gone
1:      * to disk this will drop any on disk files used to support the hash table.
1:      * <p>
1:      *
1: 	 * @exception  StandardException  Standard exception policy.
1:      **/
1:     public void close() 
1: 		throws StandardException
1:     {
1:         hash_table = null;
1:         return;
1:     }
1: 
1:     /**
0:      * Return an Enumeration that can be used to scan entire table.
1:      * <p>
1:      * RESOLVE - is it worth it to support this routine when we have a
1:      *           disk overflow hash table?
1:      *
1: 	 * @return The Enumeration.
1:      *
1: 	 * @exception  StandardException  Standard exception policy.
1:      **/
0:     public Enumeration elements()
1:         throws StandardException
1:     {
0:         return(hash_table.elements());
1:     }
1: 
1:     /**
0:      * get data associated with given key.
1:      * <p>
1:      * There are 2 different types of objects returned from this routine.
1:      * <p>
1: 	 * In both cases, the key value is either the object stored in 
1:      * row[key_column_numbers[0]], if key_column_numbers.length is 1, 
1:      * otherwise it is a KeyHasher containing
1: 	 * the objects stored in row[key_column_numbers[0, 1, ...]].
1:      * For every qualifying unique row value an entry is placed into the 
0:      * Hashtable.
1:      * <p>
0:      * For row values with duplicates, the value of the data is a Vector of
1:      * rows.
1:      * <p>
1:      * The caller will have to call "instanceof" on the data value
1:      * object if duplicates are expected, to determine if the data value
0:      * of the Hashtable entry is a row or is a Vector of rows.
1:      * <p>
1:      * The BackingStoreHashtable "owns" the objects returned from the get()
1:      * routine.  They remain valid until the next access to the 
1:      * BackingStoreHashtable.  If the client needs to keep references to these
1:      * objects, it should clone copies of the objects.  A valid 
1:      * BackingStoreHashtable can place all rows into a disk based conglomerate,
1:      * declare a row buffer and then reuse that row buffer for every get()
1:      * call.
1:      *
1: 	 * @return The value to which the key is mapped in this hashtable; 
1:      *         null if the key is not mapped to any value in this hashtable.
1:      *
1:      * @param key    The key to hash on.
1:      *
1: 	 * @exception  StandardException  Standard exception policy.
1:      **/
1:     public Object get(Object key)
1: 		throws StandardException
1:     {
0:         return(hash_table.get(key));
1:     }
1: 
1:     /**
1:      * Return runtime stats to caller by adding them to prop.
1:      * <p>
1:      *
1:      * @param prop   The set of properties to append to.
1:      *
1: 	 * @exception  StandardException  Standard exception policy.
1:      **/
1:     public void getAllRuntimeStats(Properties   prop)
1: 		throws StandardException
1:     {
1:         if (auxillary_runtimestats != null)
0:             org.apache.derby.iapi.util.PropertyUtil.copyProperties(auxillary_runtimestats, prop);
1:     }
1: 
1:     /**
1:      * remove a row from the hash table.
1:      * <p>
1:      * a remove of a duplicate removes the entire duplicate list.
1:      *
1:      * @param key          The key of the row to remove.
1:      *
1: 	 * @exception  StandardException  Standard exception policy.
1:      **/
1:     public Object remove(
1:     Object      key)
1: 		throws StandardException
1:     {
0:         return(hash_table.remove(key));
1:     }
1: 
1:     /**
1:      * Set the auxillary runtime stats.
1:      * <p>
1:      * getRuntimeStats() will return both the auxillary stats and any
1:      * BackingStoreHashtable() specific stats.  Note that each call to
1:      * setAuxillaryRuntimeStats() overwrites the Property set that was
1:      * set previously.
1:      *
1:      * @param prop   The set of properties to append from.
1:      *
1: 	 * @exception  StandardException  Standard exception policy.
1:      **/
1:     public void setAuxillaryRuntimeStats(Properties   prop)
1: 		throws StandardException
1:     {
1:         auxillary_runtimestats = prop;
1:     }
1: 
1:     /**
1:      * Put a row into the hash table.
1:      * <p>
1:      * The in memory hash table will need to keep a reference to the row
1:      * after the put call has returned.  If "needsToClone" is true then the
1:      * hash table will make a copy of the row and put that, else if 
1:      * "needsToClone" is false then the hash table will keep a reference to
1:      * the row passed in and no copy will be made.
1:      * <p>
0:      * If rouine returns false, then no reference is kept to the duplicate
1:      * row which was rejected (thus allowing caller to reuse the object).
1:      *
1:      * @param needsToClone does this routine have to make a copy of the row,
1:      *                     in order to keep a reference to it after return?
1:      * @param row          The row to insert into the table.
1:      *
1: 	 * @return true if row was inserted into the hash table.  Returns
1:      *              false if the BackingStoreHashtable is eliminating 
1:      *              duplicates, and the row being inserted is a duplicate,
1: 	 *				or if we are skipping rows with 1 or more null key columns
1: 	 *				and we find a null key column.
1:      *
1: 	 * @exception  StandardException  Standard exception policy.
1:      **/
0:     public boolean put(
0:     boolean     needsToClone,
0:     Object[]    row)
1: 		throws StandardException
1:     {
1: 		// Are any key columns null?
1: 		if (skipNullKeyColumns)
1: 		{
1: 			int index = 0;
1: 			for ( ; index < key_column_numbers.length; index++)
1: 			{
0: 				if (SanityManager.DEBUG)
1: 				{
0: 					if (! (row[key_column_numbers[index]] instanceof Storable))
1: 					{
0: 						SanityManager.THROWASSERT(
0: 							"row[key_column_numbers[index]] expected to be Storable, not " +
0: 							row[key_column_numbers[index]].getClass().getName());
1: 					}
1: 				}
0: 				Storable storable = (Storable) row[key_column_numbers[index]];
0: 				if (storable.isNull())
1: 				{
1: 					return false;
1: 				}
1: 			}
1: 		}
1: 
1:         if (needsToClone)
1:         {
0:             row = cloneRow(row);
1:         }
1: 
0:         Object key = KeyHasher.buildHashKey(row, key_column_numbers);
1: 
1:         if ((remove_duplicates) && (get(key) != null))
1:         {
1:             return(false);
1:         }
1:         else
1:         {
0:             add_row_to_hash_table(hash_table, key, row);
1:             return(true);
1:         }
1:     }
1: 
1:     /**
1:      * Return number of unique rows in the hash table.
1:      * <p>
1:      *
1: 	 * @return The number of unique rows in the hash table.
1:      *
1: 	 * @exception  StandardException  Standard exception policy.
1:      **/
1:     public int size()
1: 		throws StandardException
1:     {
1:         return(hash_table.size());
1:     }
1: 
1: }
author:Mike Matrigali
-------------------------------------------------------------------------------
commit:b61f876
/////////////////////////////////////////////////////////////////////////
1:             diskHashtable = 
1:                 new DiskHashtable(
1:                        tc,
0:                        (DataValueDescriptor[]) row,
1:                        (int[]) null, //TODO-COLLATION, set non default collation if necessary.
1:                        key_column_numbers,
1:                        remove_duplicates,
1:                        keepAfterCommit);
commit:f2ee915
/////////////////////////////////////////////////////////////////////////
commit:b223f72
/////////////////////////////////////////////////////////////////////////
1: import org.apache.derby.iapi.services.cache.ClassSize;
1: 
1: import java.util.NoSuchElementException;
/////////////////////////////////////////////////////////////////////////
1:     private TransactionController tc;
1:     /* If max_inmemory_rowcnt > 0 then use that to decide when to spill to disk.
1:      * Otherwise compute max_inmemory_size based on the JVM memory size when the BackingStoreHashtable
1:      * is constructed and use that to decide when to spill to disk.
1:      */
1:     private long max_inmemory_rowcnt;
1:     private long inmemory_rowcnt;
1:     private long max_inmemory_size;
1:     private boolean keepAfterCommit;
1: 
0:     private static int vectorSize; // The estimated number of bytes used by Vector(0)
0:     static {
1:         try
1:         {
0:             vectorSize = ClassSize.estimateBase( java.util.Vector.class);
1:         }
0:         catch( SecurityException se)
1:         {
0:             vectorSize = 4*ClassSize.refSize;
1:         }
0:     };
1:     
1:     private DiskHashtable diskHashtable;
/////////////////////////////////////////////////////////////////////////
1:      * @param keepAfterCommit If true the hash table is kept after a commit,
1:      *                        if false the hash table is dropped on the next commit.
1:      *
/////////////////////////////////////////////////////////////////////////
1: 	boolean					skipNullKeyColumns,
1:     boolean                 keepAfterCommit)
1:         this.max_inmemory_rowcnt = max_inmemory_rowcnt;
0:         if( max_inmemory_rowcnt > 0)
1:             max_inmemory_size = Long.MAX_VALUE;
1:         else
1:             max_inmemory_size = Runtime.getRuntime().totalMemory()/100;
1:         this.tc = tc;
1:         this.keepAfterCommit = keepAfterCommit;
/////////////////////////////////////////////////////////////////////////
0:     static Object[] cloneRow(Object[] old_row)
/////////////////////////////////////////////////////////////////////////
/////////////////////////////////////////////////////////////////////////
0:         if( spillToDisk( hash_table, key, row))
1:             return;
1:         
0:         if ((duplicate_value = hash_table.put(key, row)) == null)
0:             doSpaceAccounting( row, false);
1:         else
/////////////////////////////////////////////////////////////////////////
0:                     doSpaceAccounting( row, false);
/////////////////////////////////////////////////////////////////////////
0:                     doSpaceAccounting( row, true);
/////////////////////////////////////////////////////////////////////////
0:     private void doSpaceAccounting( Object[] row,
1:                                     boolean firstDuplicate)
1:     {
1:         inmemory_rowcnt++;
0:         if( max_inmemory_rowcnt <= 0)
1:         {
1:             for( int i = 0; i < row.length; i++)
1:             {
0:                 if( row[i] instanceof DataValueDescriptor)
0:                     max_inmemory_size -= ((DataValueDescriptor) row[i]).estimateMemoryUsage();
0:                 max_inmemory_size -= ClassSize.refSize;
1:             }
0:             max_inmemory_size -= ClassSize.refSize;
0:             if( firstDuplicate)
0:                 max_inmemory_size -= vectorSize;
1:         }
1:     } // end of doSpaceAccounting
1: 
1:     /**
1:      * Determine whether a new row should be spilled to disk and, if so, do it.
1:      *
0:      * @param hash_table The in-memory hash table
0:      * @param key The row's key
0:      * @param row
1:      *
1:      * @return true if the row was spilled to disk, false if not
1:      *
1:      * @exception  StandardException  Standard exception policy.
1:      */
0:     private boolean spillToDisk( Hashtable   hash_table,
0:                                  Object      key,
0:                                  Object[]    row)
1: 		throws StandardException
1:     {
1:         // Once we have started spilling all new rows will go to disk, even if we have freed up some
1:         // memory by moving duplicates to disk. This simplifies handling of duplicates and accounting.
0:         if( diskHashtable == null)
1:         {
0:             if( max_inmemory_rowcnt > 0)
1:             {
0:                 if( inmemory_rowcnt < max_inmemory_rowcnt)
1:                     return false; // Do not spill
1:             }
0:             else if( max_inmemory_size > 0)
1:                 return false;
1:             // Want to start spilling
0:             if( ! (row instanceof DataValueDescriptor[]))
1:             {
0:                 if( SanityManager.DEBUG)
0:                     SanityManager.THROWASSERT( "BackingStoreHashtable row is not DataValueDescriptor[]");
0:                 // Do not know how to put it on disk
1:                 return false;
1:             }
0:             diskHashtable = new DiskHashtable( tc,
0:                                                (DataValueDescriptor[]) row,
0:                                                key_column_numbers,
0:                                                remove_duplicates,
0:                                                keepAfterCommit);
1:         }
1:         
1:         Object duplicateValue = hash_table.get( key);
0:         if( duplicateValue != null)
1:         {
0:             if( remove_duplicates)
1:                 return true; // a degenerate case of spilling
1:             // If we are keeping duplicates then move all the duplicates from memory to disk
1:             // This simplifies finding duplicates: they are either all in memory or all on disk.
0:             if( duplicateValue instanceof Vector)
1:             {
0:                 Vector duplicateVec = (Vector) duplicateValue;
1:                 for( int i = duplicateVec.size() - 1; i >= 0; i--)
1:                 {
0:                     Object[] dupRow = (Object[]) duplicateVec.elementAt(i);
0:                     diskHashtable.put( key, dupRow);
1:                 }
1:             }
1:             else
0:                 diskHashtable.put( key, (Object []) duplicateValue);
1:             hash_table.remove( key);
1:         }
0:         diskHashtable.put( key, row);
1:         return true;
1:     } // end of spillToDisk
/////////////////////////////////////////////////////////////////////////
0:         if( diskHashtable != null)
1:         {
1:             diskHashtable.close();
1:             diskHashtable = null;
1:         }
/////////////////////////////////////////////////////////////////////////
0:         if( diskHashtable == null)
0:             return(hash_table.elements());
1:         return new BackingStoreHashtableEnumeration();
/////////////////////////////////////////////////////////////////////////
1:         Object obj = hash_table.get(key);
0:         if( diskHashtable == null || obj != null)
1:             return obj;
0:         return diskHashtable.get( key);
/////////////////////////////////////////////////////////////////////////
1:         Object obj = hash_table.remove(key);
0:         if( obj != null || diskHashtable == null)
1:             return obj;
1:         return diskHashtable.remove(key);
/////////////////////////////////////////////////////////////////////////
0:         if( diskHashtable == null)
0:             return(hash_table.size());
1:         return hash_table.size() + diskHashtable.size();
0:     private class BackingStoreHashtableEnumeration implements Enumeration
1:     {
0:         private Enumeration memoryEnumeration;
0:         private Enumeration diskEnumeration;
0: 
1:         BackingStoreHashtableEnumeration()
1:         {
0:             memoryEnumeration = hash_table.elements();
0:             if( diskHashtable != null)
1:             {
0:                 try
1:                 {
1:                     diskEnumeration = diskHashtable.elements();
1:                 }
1:                 catch( StandardException se)
1:                 {
1:                     diskEnumeration = null;
1:                 }
1:             }
1:         }
0:         
1:         public boolean hasMoreElements()
1:         {
0:             if( memoryEnumeration != null)
1:             {
0:                 if( memoryEnumeration.hasMoreElements())
1:                     return true;
0:                 memoryEnumeration = null;
1:             }
0:             if( diskEnumeration == null)
0:                 return false;
1:             return diskEnumeration.hasMoreElements();
1:         }
0: 
1:         public Object nextElement() throws NoSuchElementException
1:         {
0:             if( memoryEnumeration != null)
1:             {
0:                 if( memoryEnumeration.hasMoreElements())
0:                     return memoryEnumeration.nextElement();
0:                 memoryEnumeration = null;
1:             }
0:             return diskEnumeration.nextElement();
1:         }
1:     } // end of class BackingStoreHashtableEnumeration
author:Andreas Korneliussen
-------------------------------------------------------------------------------
commit:b7c1f3b
/////////////////////////////////////////////////////////////////////////
0:                
0:                 add_row_to_hash_table(hash_table, row, needsToClone);
/////////////////////////////////////////////////////////////////////////
1:      * Return a shallow cloned row
1:      *
1:      * @return The cloned row row to use.
1:      *
1:      * @exception  StandardException  Standard exception policy.
1:      **/
1:     static DataValueDescriptor[] shallowCloneRow(DataValueDescriptor[] old_row)
0:         throws StandardException
0:     {
1:         DataValueDescriptor[] new_row = new DataValueDescriptor[old_row.length];
0:         // the only difference between getClone and cloneObject is cloneObject does
0:         // not objectify a stream.  We use cloneObject here.  DERBY-802
1:         for (int i = 0; i < old_row.length; i++)
0:         {
0:             if( old_row[i] != null)
0:                 new_row[i] = (DataValueDescriptor) 
0:                     ((CloneableObject) old_row[i]).cloneObject();
0:         }
0: 
1:         return(new_row);
0:     }
0: 
1:     /**
1:      * @param needsToClone      If the row needs to be cloned
0:     Object[]    row,
0:     boolean needsToClone )
0:         if( spillToDisk( hash_table, row))
0:         if (needsToClone)
0:         {
0:             row = cloneRow(row);
0:         }
0:         Object key = KeyHasher.buildHashKey(row, key_column_numbers);
/////////////////////////////////////////////////////////////////////////
/////////////////////////////////////////////////////////////////////////
/////////////////////////////////////////////////////////////////////////
0:             else if( max_inmemory_size > getEstimatedMemUsage(row))
0:                 
/////////////////////////////////////////////////////////////////////////
0:         Object key = KeyHasher.buildHashKey(row, key_column_numbers);
/////////////////////////////////////////////////////////////////////////
/////////////////////////////////////////////////////////////////////////
0:             add_row_to_hash_table(hash_table, row, needsToClone);
author:Satheesh E. Bandaram
-------------------------------------------------------------------------------
commit:985a9b0
/////////////////////////////////////////////////////////////////////////
1:                     // Check to see how much memory we think the first row
/////////////////////////////////////////////////////////////////////////
0: 
1:         // In the (unlikely) event that we received a "red flag" estimated_rowcnt
1:         // that is too big (see comments above), it's possible that, if row_source
1:         // was null or else didn't have any rows, hash_table could still be null
1:         // at this point.  So we initialize it to an empty hashtable (representing
1:         // an empty result set) so that calls to other methods on this
1:         // BackingStoreHashtable (ex. "size()") will have a working hash_table
1:         // on which to operate.
1:         if (hash_table == null)
0:             hash_table = new Hashtable();
commit:e540aee
/////////////////////////////////////////////////////////////////////////
1:             /* We want to create the hash table based on the estimated row
1:              * count if a) we have an estimated row count (i.e. it's greater
1:              * than zero) and b) we think we can create a hash table to
1:              * hold the estimated row count without running out of memory.
1:              * The check for "b" is required because, for deeply nested
1:              * queries and/or queries with a high number of tables in
1:              * their FROM lists, the optimizer can end up calculating
1:              * some very high row count estimates--even up to the point of
1:              * Double.POSITIVE_INFINITY (see DERBY-1259 for an explanation
1:              * of how that can happen).  In that case any attempts to
0:              * create a Hashtable of size estimated_rowcnt can cause
0:              * OutOfMemory errors when we try to create the Hashtable.
1:              * So as a "red flag" for that kind of situation, we check to
1:              * see if the estimated row count is greater than the max
1:              * in-memory size for this table.  Unit-wise this comparison
1:              * is relatively meaningless: rows vs bytes.  But if our
1:              * estimated row count is greater than the max number of
1:              * in-memory bytes that we're allowed to consume, then
0:              * it's very likely that creating a Hashtable with a capacity
1:              * of estimated_rowcnt will lead to memory problems.  So in
1:              * that particular case we leave hash_table null here and
1:              * initialize it further below, using the estimated in-memory
1:              * size of the first row to figure out what a reasonable size
0:              * for the Hashtable might be.
1:              */
1:                 (((estimated_rowcnt <= 0) || (row_source == null)) ?
0:                      new Hashtable() :
1:                      (estimated_rowcnt < max_inmemory_size) ?
0:                          new Hashtable((int) estimated_rowcnt) :
1:                          null);
/////////////////////////////////////////////////////////////////////////
1:                 // If we haven't initialized the hash_table yet then that's
0:                 // because a Hashtable with capacity estimated_rowcnt would
1:                 // probably cause memory problems.  So look at the first row
1:                 // that we found and use that to create the hash table with
1:                 // an initial capacity such that, if it was completely full,
1:                 // it would still satisfy the max_inmemory condition.  Note
1:                 // that this isn't a hard limit--the hash table can grow if
1:                 // needed.
1:                 if (hash_table == null)
0:                 {
0: 					// Check to see how much memory we think the first row
1:                     // is going to take, and then use that to set the initial
0:                     // capacity of the Hashtable.
1:                     double rowUsage = getEstimatedMemUsage(row);
0:                     hash_table = new Hashtable((int)(max_inmemory_size / rowUsage));
0:                 }
/////////////////////////////////////////////////////////////////////////
0:             max_inmemory_size -= getEstimatedMemUsage(row);
/////////////////////////////////////////////////////////////////////////
0: 
0:     /**
0:      * Take a row and return an estimate as to how much memory that
0:      * row will consume.
0:      * 
0:      * @param row The row for which we want to know the memory usage.
0:      * @return A guess as to how much memory the current row will
1:      *  use.
0:      */
0:     private long getEstimatedMemUsage(Object [] row)
0:     {
1:         long rowMem = 0;
0:         for( int i = 0; i < row.length; i++)
0:         {
0:             if (row[i] instanceof DataValueDescriptor)
0:                 rowMem += ((DataValueDescriptor) row[i]).estimateMemoryUsage();
1:             rowMem += ClassSize.refSize;
0:         }
0: 
1:         rowMem += ClassSize.refSize;
1:         return rowMem;
0:     }
0: 
commit:ae13c42
/////////////////////////////////////////////////////////////////////////
0:         {
0:             if( old_row[i] != null)
0:                 new_row[i] = ((DataValueDescriptor) old_row[i]).getClone();
0:         }
author:Oyvind Bakksjo
-------------------------------------------------------------------------------
commit:aaea357
author:Ken Coar
-------------------------------------------------------------------------------
commit:95e7b46
/////////////////////////////////////////////////////////////////////////
0: /*
0: 
0:    Licensed Materials - Property of IBM
0:    Cloudscape - Package org.apache.derby.iapi.store.access
0:    (C) Copyright IBM Corp. 1999, 2004. All Rights Reserved.
0:    US Government Users Restricted Rights - Use, duplication or
0:    disclosure restricted by GSA ADP Schedule Contract with IBM Corp.
0: 
0:  */
0: 
0: package org.apache.derby.iapi.store.access;
0: 
0: import org.apache.derby.iapi.services.sanity.SanityManager;
0: 
0: import org.apache.derby.iapi.services.io.Storable;
0: 
0: import org.apache.derby.iapi.error.StandardException; 
0: 
0: import org.apache.derby.iapi.types.CloneableObject;
0: import org.apache.derby.iapi.types.DataValueDescriptor;
0: 
0: import java.util.Enumeration;
0: import java.util.Hashtable;
0: import java.util.Properties; 
0: import java.util.Vector;
0: 
0: /**
0: A BackingStoreHashtable is a utility class which will store a set of rows into
0: an in memory hash table, or overflow the hash table to a tempory on disk 
0: structure.
0: <p>
0: All rows must contain the same number of columns, and the column at position
0: N of all the rows must have the same format id.  If the BackingStoreHashtable needs to be
0: overflowed to disk, then an arbitrary row will be chosen and used as a template
0: for creating the underlying overflow container.
0: 
0: <p>
0: The hash table will be built logically as follows (actual implementation
0: may differ).  The important points are that the hash value is the standard
0: java hash value on the row[key_column_numbers[0], if key_column_numbers.length is 1,
0: or row[key_column_numbers[0, 1, ...]] if key_column_numbers.length > 1, 
0: and that duplicate detection is done by the standard java duplicate detection provided by 
0: java.util.Hashtable.
0: <p>
0: import java.util.Hashtable;
0: 
0: hash_table = new Hashtable();
0: 
0: Object[] row;
0: boolean  needsToClone = rowSource.needsToClone();
0: 
0: while((row = rowSource.getNextRowFromRowSource()) != null)
0: {
0:     if (needsToClone)
0:         row = clone_row_from_row(row);
0: 
0: 	Object key = KeyHasher.buildHashKey(row, key_column_numbers);
0: 
0:     if ((duplicate_value = 
0:         hash_table.put(key, row)) != null)
0:     {
0:         Vector row_vec;
0: 
0:         // inserted a duplicate
0:         if ((duplicate_value instanceof vector))
0:         {
0:             row_vec = (Vector) duplicate_value;
0:         }
0:         else
0:         {
0:             // allocate vector to hold duplicates
0:             row_vec = new Vector(2);
0: 
0:             // insert original row into vector
0:             row_vec.addElement(duplicate_value);
0: 
0:             // put the vector as the data rather than the row
0:             hash_table.put(key, row_vec);
0:         }
0:         
0:         // insert new row into vector
0:         row_vec.addElement(row);
0:     }
0: }
0: 
0: **/
0: 
0: public class BackingStoreHashtable
0: {
0: 	/**
0: 		IBM Copyright &copy notice.
0: 	*/
0: 	public static final String copyrightNotice = org.apache.derby.iapi.reference.Copyright.SHORT_1999_2004;
0: 
0:     /**************************************************************************
0:      * Fields of the class
0:      **************************************************************************
0:      */
0:     private Hashtable   hash_table;
0:     private int[]       key_column_numbers;
0:     private boolean     remove_duplicates;
0: 	private boolean		skipNullKeyColumns;
0:     private Properties  auxillary_runtimestats;
0: 	private RowSource	row_source;
0: 
0:     /**************************************************************************
0:      * Constructors for This class:
0:      **************************************************************************
0:      */
0:     private BackingStoreHashtable(){}
0: 
0:     /**
0:      * Create the BackingStoreHashtable from a row source.
0:      * <p>
0:      * This routine drains the RowSource.  The performance characteristics
0:      * depends on the number of rows inserted and the parameters to the 
0:      * constructor.  
0:      * <p>
0:      * If the number of rows is <= "max_inmemory_rowcnt", then the rows are
0:      * inserted into a java.util.Hashtable.  In this case no 
0:      * TransactionController is necessary, a "null" tc is valid.
0:      * <p>
0:      * If the number of rows is > "max_inmemory_rowcnt", then the rows will
0:      * be all placed in some sort of Access temporary file on disk.  This 
0:      * case requires a valid TransactionController.
0:      *
0: 	 * @return The identifier to be used to open the conglomerate later.
0:      *
0:      * @param tc                An open TransactionController to be used if the
0:      *                          hash table needs to overflow to disk.
0:      *
0:      * @param row_source        RowSource to read rows from.
0:      *
0:      * @param key_column_numbers The column numbers of the columns in the
0:      *                          scan result row to be the key to the Hashtable.
0:      *                          "0" is the first column in the scan result
0:      *                          row (which may be different than the first
0:      *                          row in the table of the scan).
0:      *
0:      * @param remove_duplicates Should the Hashtable automatically remove
0:      *                          duplicates, or should it create the Vector of
0:      *                          duplicates?
0:      *
0:      * @param estimated_rowcnt  The estimated number of rows in the hash table.
0:      *                          Pass in -1 if there is no estimate.
0:      *
0:      * @param max_inmemory_rowcnt
0:      *                          The maximum number of rows to insert into the 
0:      *                          inmemory Hash table before overflowing to disk.
0:      *                          Pass in -1 if there is no maximum.
0:      *
0:      * @param initialCapacity   If not "-1" used to initialize the java 
0:      *                          Hashtable.
0:      *
0:      * @param loadFactor        If not "-1" used to initialize the java 
0:      *                          Hashtable.
0: 	 *
0: 	 * @param skipNullKeyColumns	Skip rows with a null key column, if true.
0:      *
0:      *
0: 	 * @exception  StandardException  Standard exception policy.
0:      **/
0:     public BackingStoreHashtable(
0:     TransactionController   tc,
0:     RowSource               row_source,
0:     int[]                   key_column_numbers,
0:     boolean                 remove_duplicates,
0:     long                    estimated_rowcnt,
0:     long                    max_inmemory_rowcnt,
0:     int                     initialCapacity,
0:     float                   loadFactor,
0: 	boolean					skipNullKeyColumns)
0:         throws StandardException
0:     {
0:         this.key_column_numbers    = key_column_numbers;
0:         this.remove_duplicates    = remove_duplicates;
0: 		this.row_source			   = row_source;
0: 		this.skipNullKeyColumns	   = skipNullKeyColumns;
0: 
0:         Object[] row;
0: 
0:         // use passed in capacity and loadfactor if not -1, you must specify
0:         // capacity if you want to specify loadfactor.
0:         if (initialCapacity != -1)
0:         {
0:             hash_table = 
0:                 ((loadFactor == -1) ? 
0:                      new Hashtable(initialCapacity) : 
0:                      new Hashtable(initialCapacity, loadFactor));
0:         }
0:         else
0:         {
0:             hash_table = 
0:                 ((estimated_rowcnt <= 0) ? 
0:                      new Hashtable() : new Hashtable((int) estimated_rowcnt));
0:         }
0: 
0:         if (row_source != null)
0:         {
0:             boolean needsToClone = row_source.needsToClone();
0: 
0:             while ((row = getNextRowFromRowSource()) != null)
0:             {
0: 
0:                 if (needsToClone)
0:                 {
0:                     row = cloneRow(row);
0:                 }
0: 
0:                 Object key = 
0:                     KeyHasher.buildHashKey(row, key_column_numbers);
0: 
0:                 add_row_to_hash_table(hash_table, key, row);
0:             }
0:         }
0:     }
0: 
0:     /**************************************************************************
0:      * Private/Protected methods of This class:
0:      **************************************************************************
0:      */
0: 
0: 	/**
0: 	 * Call method to either get next row or next row with non-null
0: 	 * key columns.
0: 	 *
0:      *
0: 	 * @exception  StandardException  Standard exception policy.
0: 	 */
0: 	private Object[] getNextRowFromRowSource()
0: 		throws StandardException
0: 	{
0: 		Object[] row = row_source.getNextRowFromRowSource();
0: 
0: 		if (skipNullKeyColumns)
0: 		{
0: 			while (row != null)
0: 			{
0: 				// Are any key columns null?
0: 				int index = 0;
0: 				for ( ; index < key_column_numbers.length; index++)
0: 				{
0: 					if (SanityManager.DEBUG)
0: 					{
0: 						if (! (row[key_column_numbers[index]] instanceof Storable))
0: 						{
0: 							SanityManager.THROWASSERT(
0: 								"row[key_column_numbers[index]] expected to be Storable, not " +
0: 								row[key_column_numbers[index]].getClass().getName());
0: 						}
0: 					}
0: 					Storable storable = (Storable) row[key_column_numbers[index]];
0: 					if (storable.isNull())
0: 					{
0: 						break;
0: 					}
0: 				}
0: 				// No null key columns
0: 				if (index == key_column_numbers.length)
0: 				{
0: 					return row;
0: 				}
0: 				// 1 or more null key columns
0: 				row = row_source.getNextRowFromRowSource();
0: 			}
0: 		}
0: 		return row;
0: 	}
0: 
0:     /**
0:      * Return a cloned copy of the row.
0:      *
0: 	 * @return The cloned row row to use.
0:      *
0: 	 * @exception  StandardException  Standard exception policy.
0:      **/
0:     private Object[] cloneRow(Object[] old_row)
0:         throws StandardException
0:     {
0:         Object[] new_row = new DataValueDescriptor[old_row.length];
0: 
0: 		// the only difference between getClone and cloneObject is cloneObject does
0: 		// not objectify a stream.  We use getClone here.  Beetle 4896.
0:         for (int i = 0; i < old_row.length; i++)
0:             new_row[i] = ((DataValueDescriptor) old_row[i]).getClone();
0: 
0:         return(new_row);
0:     }
0: 
0:     /**
0:      * Do the work to add one row to the hash table.
0:      * <p>
0:      *
0:      * @param row               Row to add to the hash table.
0:      * @param hash_table        The java HashTable to load into.
0:      *
0: 	 * @return true if successful, false if heap add fails.
0:      *
0: 	 * @exception  StandardException  Standard exception policy.
0:      **/
0:     private void add_row_to_hash_table(
0:     Hashtable   hash_table,
0:     Object      key,
0:     Object[]    row)
0: 		throws StandardException
0:     {
0:         Object  duplicate_value = null;
0: 
0:         if ((duplicate_value = hash_table.put(key, row)) != null)
0:         {
0:             if (!remove_duplicates)
0:             {
0:                 Vector row_vec;
0: 
0:                 // inserted a duplicate
0:                 if ((duplicate_value instanceof Vector))
0:                 {
0:                     row_vec = (Vector) duplicate_value;
0:                 }
0:                 else
0:                 {
0:                     // allocate vector to hold duplicates
0:                     row_vec = new Vector(2);
0: 
0:                     // insert original row into vector
0:                     row_vec.addElement(duplicate_value);
0:                 }
0: 
0:                 // insert new row into vector
0:                 row_vec.addElement(row);
0: 
0:                 // store vector of rows back into hash table,
0:                 // overwriting the duplicate key that was 
0:                 // inserted.
0:                 hash_table.put(key, row_vec);
0:             }
0:         }
0: 
0:         row = null;
0:     }
0: 
0:     /**************************************************************************
0:      * Public Methods of This class:
0:      **************************************************************************
0:      */
0: 
0:     /**
0:      * Close the BackingStoreHashtable.
0:      * <p>
0:      * Perform any necessary cleanup after finishing with the hashtable.  Will
0:      * deallocate/dereference objects as necessary.  If the table has gone
0:      * to disk this will drop any on disk files used to support the hash table.
0:      * <p>
0:      *
0: 	 * @exception  StandardException  Standard exception policy.
0:      **/
0:     public void close() 
0: 		throws StandardException
0:     {
0:         hash_table = null;
0:         return;
0:     }
0: 
0:     /**
0:      * Return an Enumeration that can be used to scan entire table.
0:      * <p>
0:      * RESOLVE - is it worth it to support this routine when we have a
0:      *           disk overflow hash table?
0:      *
0: 	 * @return The Enumeration.
0:      *
0: 	 * @exception  StandardException  Standard exception policy.
0:      **/
0:     public Enumeration elements()
0:         throws StandardException
0:     {
0:         return(hash_table.elements());
0:     }
0: 
0:     /**
0:      * get data associated with given key.
0:      * <p>
0:      * There are 2 different types of objects returned from this routine.
0:      * <p>
0: 	 * In both cases, the key value is either the object stored in 
0:      * row[key_column_numbers[0]], if key_column_numbers.length is 1, 
0:      * otherwise it is a KeyHasher containing
0: 	 * the objects stored in row[key_column_numbers[0, 1, ...]].
0:      * For every qualifying unique row value an entry is placed into the 
0:      * Hashtable.
0:      * <p>
0:      * For row values with duplicates, the value of the data is a Vector of
0:      * rows.
0:      * <p>
0:      * The caller will have to call "instanceof" on the data value
0:      * object if duplicates are expected, to determine if the data value
0:      * of the Hashtable entry is a row or is a Vector of rows.
0:      * <p>
0:      * The BackingStoreHashtable "owns" the objects returned from the get()
0:      * routine.  They remain valid until the next access to the 
0:      * BackingStoreHashtable.  If the client needs to keep references to these
0:      * objects, it should clone copies of the objects.  A valid 
0:      * BackingStoreHashtable can place all rows into a disk based conglomerate,
0:      * declare a row buffer and then reuse that row buffer for every get()
0:      * call.
0:      *
0: 	 * @return The value to which the key is mapped in this hashtable; 
0:      *         null if the key is not mapped to any value in this hashtable.
0:      *
0:      * @param key    The key to hash on.
0:      *
0: 	 * @exception  StandardException  Standard exception policy.
0:      **/
0:     public Object get(Object key)
0: 		throws StandardException
0:     {
0:         return(hash_table.get(key));
0:     }
0: 
0:     /**
0:      * Return runtime stats to caller by adding them to prop.
0:      * <p>
0:      *
0:      * @param prop   The set of properties to append to.
0:      *
0: 	 * @exception  StandardException  Standard exception policy.
0:      **/
0:     public void getAllRuntimeStats(Properties   prop)
0: 		throws StandardException
0:     {
0:         if (auxillary_runtimestats != null)
0:             org.apache.derby.iapi.util.PropertyUtil.copyProperties(auxillary_runtimestats, prop);
0:     }
0: 
0:     /**
0:      * remove a row from the hash table.
0:      * <p>
0:      * a remove of a duplicate removes the entire duplicate list.
0:      *
0:      * @param key          The key of the row to remove.
0:      *
0: 	 * @exception  StandardException  Standard exception policy.
0:      **/
0:     public Object remove(
0:     Object      key)
0: 		throws StandardException
0:     {
0:         return(hash_table.remove(key));
0:     }
0: 
0:     /**
0:      * Set the auxillary runtime stats.
0:      * <p>
0:      * getRuntimeStats() will return both the auxillary stats and any
0:      * BackingStoreHashtable() specific stats.  Note that each call to
0:      * setAuxillaryRuntimeStats() overwrites the Property set that was
0:      * set previously.
0:      *
0:      * @param prop   The set of properties to append from.
0:      *
0: 	 * @exception  StandardException  Standard exception policy.
0:      **/
0:     public void setAuxillaryRuntimeStats(Properties   prop)
0: 		throws StandardException
0:     {
0:         auxillary_runtimestats = prop;
0:     }
0: 
0:     /**
0:      * Put a row into the hash table.
0:      * <p>
0:      * The in memory hash table will need to keep a reference to the row
0:      * after the put call has returned.  If "needsToClone" is true then the
0:      * hash table will make a copy of the row and put that, else if 
0:      * "needsToClone" is false then the hash table will keep a reference to
0:      * the row passed in and no copy will be made.
0:      * <p>
0:      * If rouine returns false, then no reference is kept to the duplicate
0:      * row which was rejected (thus allowing caller to reuse the object).
0:      *
0:      * @param needsToClone does this routine have to make a copy of the row,
0:      *                     in order to keep a reference to it after return?
0:      * @param row          The row to insert into the table.
0:      *
0: 	 * @return true if row was inserted into the hash table.  Returns
0:      *              false if the BackingStoreHashtable is eliminating 
0:      *              duplicates, and the row being inserted is a duplicate,
0: 	 *				or if we are skipping rows with 1 or more null key columns
0: 	 *				and we find a null key column.
0:      *
0: 	 * @exception  StandardException  Standard exception policy.
0:      **/
0:     public boolean put(
0:     boolean     needsToClone,
0:     Object[]    row)
0: 		throws StandardException
0:     {
0: 		// Are any key columns null?
0: 		if (skipNullKeyColumns)
0: 		{
0: 			int index = 0;
0: 			for ( ; index < key_column_numbers.length; index++)
0: 			{
0: 				if (SanityManager.DEBUG)
0: 				{
0: 					if (! (row[key_column_numbers[index]] instanceof Storable))
0: 					{
0: 						SanityManager.THROWASSERT(
0: 							"row[key_column_numbers[index]] expected to be Storable, not " +
0: 							row[key_column_numbers[index]].getClass().getName());
0: 					}
0: 				}
0: 				Storable storable = (Storable) row[key_column_numbers[index]];
0: 				if (storable.isNull())
0: 				{
0: 					return false;
0: 				}
0: 			}
0: 		}
0: 
0:         if (needsToClone)
0:         {
0:             row = cloneRow(row);
0:         }
0: 
0:         Object key = KeyHasher.buildHashKey(row, key_column_numbers);
0: 
0:         if ((remove_duplicates) && (get(key) != null))
0:         {
0:             return(false);
0:         }
0:         else
0:         {
0:             add_row_to_hash_table(hash_table, key, row);
0:             return(true);
0:         }
0:     }
0: 
0:     /**
0:      * Return number of unique rows in the hash table.
0:      * <p>
0:      *
0: 	 * @return The number of unique rows in the hash table.
0:      *
0: 	 * @exception  StandardException  Standard exception policy.
0:      **/
0:     public int size()
0: 		throws StandardException
0:     {
0:         return(hash_table.size());
0:     }
0: 
0: }
============================================================================