1:58ac441: /*
1:58ac441: 
1:58ac441:    Derby - Class org.apache.derbyTesting.perf.basic.jdbc.BlobAccessTest
1:58ac441: 
1:58ac441:    Licensed to the Apache Software Foundation (ASF) under one or more
1:58ac441:    contributor license agreements.  See the NOTICE file distributed with
1:58ac441:    this work for additional information regarding copyright ownership.
1:58ac441:    The ASF licenses this file to you under the Apache License, Version 2.0
1:58ac441:    (the "License"); you may not use this file except in compliance with
1:58ac441:    the License.  You may obtain a copy of the License at
1:58ac441: 
1:58ac441:       http://www.apache.org/licenses/LICENSE-2.0
1:58ac441: 
1:58ac441:    Unless required by applicable law or agreed to in writing, software
1:58ac441:    distributed under the License is distributed on an "AS IS" BASIS,
1:58ac441:    WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
1:58ac441:    See the License for the specific language governing permissions and
1:58ac441:    limitations under the License.
1:58ac441: 
1:58ac441:  */
1:58ac441: package org.apache.derbyTesting.perf.basic.jdbc;
1:58ac441: 
1:58ac441: import java.io.BufferedInputStream;
1:58ac441: import java.io.IOException;
1:58ac441: import java.io.InputStream;
1:58ac441: import java.io.UnsupportedEncodingException;
1:58ac441: import java.sql.Blob;
1:58ac441: import java.sql.Connection;
1:58ac441: import java.sql.PreparedStatement;
1:58ac441: import java.sql.ResultSet;
1:58ac441: import java.sql.SQLException;
1:58ac441: import java.sql.Statement;
1:58ac441: import junit.framework.Test;
1:58ac441: import org.apache.derbyTesting.functionTests.util.streams.LoopingAlphabetStream;
1:1ae02c9: import org.apache.derbyTesting.junit.BaseTestSuite;
1:58ac441: import org.apache.derbyTesting.junit.CleanDatabaseTestSetup;
1:58ac441: import org.apache.derbyTesting.junit.JDBCPerfTestCase;
1:58ac441: import org.apache.derbyTesting.perf.clients.BackToBackLoadGenerator;
1:58ac441: import org.apache.derbyTesting.perf.clients.Client;
1:58ac441: import org.apache.derbyTesting.perf.clients.DBFiller;
1:58ac441: import org.apache.derbyTesting.perf.clients.LoadGenerator;
1:58ac441: import org.apache.derbyTesting.perf.clients.SingleRecordFiller;
1:58ac441: import org.apache.derbyTesting.perf.clients.SingleRecordSelectClient;
1:58ac441: 
1:58ac441: /**
1:58ac441:  * A series of tests accessing Blobs in various ways.
1:58ac441:  * <p>
1:58ac441:  * These tests are intended to detect Blob performance regressions. Before
1:58ac441:  * committing a patch that might change the Blob performance characteristics,
1:58ac441:  * first run these tests on a clean build and then with the patch applied. The
1:58ac441:  * results can only be compared when both runs are done on the same machine.
1:58ac441:  * <p>
1:58ac441:  * The results are the time taken to execute the test. Lower duration is better
1:58ac441:  * (improvement). Currently the results are printed to standard out. There is
1:58ac441:  * one exception, which is {@code testConcurrency}. For this test, the
1:58ac441:  * throughput is printed and it will always run for a fixed amount of time.
1:58ac441:  * <p>
1:58ac441:  * The tests are written with two axis in mind: read-only vs update and small vs
1:58ac441:  * large. These axis were chosen based on the Blob implementation at the time.
1:58ac441:  * In the context of this test, small means the Blob is represented as a string
1:58ac441:  * by the Derby store and large means the Blob is represtend as a stream into
1:58ac441:  * the Derby store. When a Blob is modified, an in-memory or on disk temporary
1:58ac441:  * copy is created. The performance of these temporary representations are
1:58ac441:  * tested with the tests that modify the Blob content.
1:58ac441:  * <p>
1:58ac441:  * System properties controlling test behavior:
1:58ac441:  * <ul><li>derby.tests.disableSmallBlobs</li>
1:58ac441:  *     <li>derby.tests.disableLargeBlobs</li>
1:58ac441:  *     <li>derby.tests.disableConcurrencyTest</li>
1:58ac441:  *     <li>derby.tests.largeBlobSize (in MB, 15 is the default)</li>
1:58ac441:  * </ul>
1:58ac441:  *
1:58ac441:  * <p>
1:58ac441:  * <b>NOTE</b>: Currently there are no tests for the client driver (network)
1:58ac441:  * or for encrypted Blobs.
1:58ac441:  */
1:58ac441: public class BlobAccessTest
1:58ac441:         extends JDBCPerfTestCase {
1:58ac441: 
1:58ac441:     private static final boolean disableSmallBlobs =
1:58ac441:             Boolean.getBoolean("derby.tests.disableSmallBlobs");
1:58ac441:     private static final boolean disableLargeBlobs =
1:58ac441:             Boolean.getBoolean("derby.tests.disableLargeBlobs");
1:58ac441:     private static final boolean disableConcurrencyTest =
1:58ac441:             Boolean.getBoolean("derby.tests.disableConcurrencyTest");
1:58ac441:     private static final int largeBlobSizeMB =
1:58ac441:             Integer.getInteger("derby.tests.largeBlobSize", 15).intValue();
1:58ac441: 
1:58ac441:     private static final int FETCH_GETBYTES = 0;
1:58ac441:     private static final int FETCH_GETBINARYSTREAM = 1;
1:58ac441: 
1:58ac441:     /**
1:58ac441:      * Instantiates a new test that will be run the specified number of
1:58ac441:      * iterations and repeated as specified.
1:58ac441:      *
1:58ac441:      * @param name name of the test to instantiate
1:58ac441:      * @param iterations number of iterations per repetition
1:58ac441:      * @param repeats number of repetitions
1:58ac441:      */
1:58ac441:     public BlobAccessTest(String name, int iterations, int repeats) {
1:58ac441:         super(name, iterations, repeats);
1:58ac441:     }
1:58ac441: 
1:58ac441:     /**
1:58ac441:      * Set autocommit to false by default.
1:58ac441:      */
1:58ac441:     public void initializeConnection(Connection conn)
1:58ac441:             throws SQLException {
1:58ac441:         conn.setAutoCommit(false);
1:58ac441:     }
1:58ac441: 
1:58ac441:     /**
1:58ac441:      * Generates a suite of tests.
1:58ac441:      * <p>
1:58ac441:      * The required test data will be generated. Note that a subset of the
1:58ac441:      * tests can be disabled by using a system property.
1:58ac441:      *
1:58ac441:      * @return A suite of tests.
1:58ac441:      */
1:58ac441:     public static Test suite() {
1:1ae02c9:         BaseTestSuite mainSuite = new BaseTestSuite("BlobAccessTest suite");
1:58ac441:         if (!disableSmallBlobs) {
1:58ac441:             int iters = 50;
1:58ac441:             int reps = 3;
1:58ac441:             println("Adding small Blob tests.");
1:1ae02c9:             BaseTestSuite smallSuite = new BaseTestSuite("Small Blob suite");
1:58ac441:             smallSuite.addTest(new BlobAccessTest(
1:58ac441:                     "testFetchSmallBlobs", iters, reps));
1:58ac441:             smallSuite.addTest(new BlobAccessTest(
1:58ac441:                     "testFetchSmallBlobsInaccurateLength", iters, reps));
1:58ac441:             smallSuite.addTest(new BlobAccessTest(
1:58ac441:                     "testModifySmallBlobs", iters, reps));
1:58ac441:             mainSuite.addTest(smallSuite);
1:58ac441:         }
1:58ac441:         if (!disableLargeBlobs) {
1:58ac441:             int iters = 5;
1:58ac441:             int reps = 3;
1:58ac441:             println("Adding large Blob tests.");
1:1ae02c9:             BaseTestSuite largeSuite = new BaseTestSuite("Large Blob suite");
1:58ac441:             largeSuite.addTest(new BlobAccessTest(
1:58ac441:                     "testFetchLargeBlobs", iters, reps));
1:58ac441:             largeSuite.addTest(new BlobAccessTest(
1:58ac441:                     "testFetchLargeBlobOneByOneByteBaseline", iters, reps));
1:58ac441:             largeSuite.addTest(new BlobAccessTest(
1:58ac441:                     "testFetchLargeBlobOneByOneByteModified", iters, reps));
1:58ac441:             largeSuite.addTest(new BlobAccessTest(
1:58ac441:                     "testFetchLargeBlobOneByOneByte", iters, reps));
1:58ac441:             largeSuite.addTest(new BlobAccessTest(
1:58ac441:                     "testFetchLargeBlob", iters, reps));
1:58ac441:             largeSuite.addTest(new BlobAccessTest(
1:58ac441:                     "testFetchLargeBlobModified", iters, reps));
1:58ac441:             largeSuite.addTest(new BlobAccessTest(
1:58ac441:                     "testFetchLargeBlobPieceByPiece", iters, reps));
1:58ac441:             largeSuite.addTest(new BlobAccessTest(
1:58ac441:                     "testFetchLargeBlobPieceByPieceModified", iters, reps));
1:58ac441:             largeSuite.addTest(new BlobAccessTest(
1:58ac441:                     "testLargeBlobGetLength", iters, reps));
1:58ac441:             mainSuite.addTest(largeSuite);
1:58ac441:         }
1:58ac441:         if (!disableConcurrencyTest) {
1:58ac441:             mainSuite.addTest(new BlobAccessTest("testConcurrency", 1, 1));
1:58ac441:         }
1:58ac441:         return new CleanDatabaseTestSetup(mainSuite) {
1:58ac441:             protected void decorateSQL(Statement stmt)
1:58ac441:                     throws SQLException {
1:58ac441:                 try {
1:58ac441:                     initializeBlobData(stmt);
1:58ac441:                 } catch (UnsupportedEncodingException uee) {
1:58ac441:                     // Compiled with JDK 1.4, can't use constructor.
1:58ac441:                     SQLException sqle = new SQLException();
1:58ac441:                     sqle.initCause(uee);
1:58ac441:                     throw sqle;
1:58ac441:                 }
1:58ac441:             }
1:58ac441:         };
1:58ac441:     }
1:58ac441: 
1:58ac441:     /**
1:58ac441:      * Fetches a number of small Blobs, getting the content using getBytes.
1:58ac441:      * <p>
1:58ac441:      * The exact length of the Blob is used when getting the bytes.
1:58ac441:      */
1:58ac441:     public void testFetchSmallBlobs()
1:58ac441:             throws SQLException {
1:58ac441:         PreparedStatement ps = prepareStatement(
1:58ac441:                 "select dBlob, length from smallBlobs");
1:58ac441:         ResultSet rs = ps.executeQuery();
1:58ac441:         while (rs.next()) {
1:58ac441:             Blob Blob = rs.getBlob(1);
1:58ac441:             int blobLength = rs.getInt(2);
1:58ac441:             byte[] content = Blob.getBytes(1, blobLength);
1:58ac441:         }
1:58ac441:         rs.close();
1:58ac441:     }
1:58ac441: 
1:58ac441:     /**
1:58ac441:      * Fetches a number of small Blobs, getting the content using getBytes.
1:58ac441:      * <p>
1:58ac441:      * A too long length of the Blob is used when getting the bytes.
1:58ac441:      */
1:58ac441:     public void testFetchSmallBlobsInaccurateLength()
1:58ac441:             throws SQLException {
1:58ac441:         PreparedStatement ps = prepareStatement(
1:58ac441:                 "select dBlob, length from smallBlobs");
1:58ac441:         ResultSet rs = ps.executeQuery();
1:58ac441:         while (rs.next()) {
1:58ac441:             Blob Blob = rs.getBlob(1);
1:58ac441:             int unusedLength = rs.getInt(2);
1:58ac441:             byte[] content = Blob.getBytes(1, 100);
1:58ac441:         }
1:58ac441:         rs.close();
1:58ac441:     }
1:58ac441: 
1:58ac441:     /**
1:58ac441:      * Test fetching the content after adding a single byte at the end.
1:58ac441:      */
1:58ac441:     public void testModifySmallBlobs()
1:58ac441:             throws SQLException, UnsupportedEncodingException {
1:58ac441:         PreparedStatement ps = prepareStatement(
1:58ac441:                 "select dBlob, length from smallBlobs");
1:58ac441:         ResultSet rs = ps.executeQuery();
1:58ac441:         while (rs.next()) {
1:58ac441:             Blob Blob = rs.getBlob(1);
1:58ac441:             int length = rs.getInt(2);
1:58ac441:             Blob.setBytes(length, "X".getBytes("US-ASCII"));
1:58ac441:             byte[] content = Blob.getBytes(1, 100);
1:58ac441:         }
1:58ac441:         rs.close();
1:58ac441:     }
1:58ac441: 
1:58ac441:     /**
1:58ac441:      * Fetches a number of Blobs using a rather large read buffer with
1:58ac441:      * {@code getBinaryStream}.
1:58ac441:      */
1:58ac441:     public void testFetchLargeBlobs()
1:58ac441:             throws IOException, SQLException {
1:58ac441:         PreparedStatement ps = prepareStatement(
1:58ac441:                 "select dBlob, length from largeBlobs");
1:58ac441:         ResultSet rs = ps.executeQuery();
1:58ac441:         byte[] byteBuf = new byte[16*1024]; // 16 KB
1:58ac441:         while (rs.next()) {
1:58ac441:             Blob Blob = rs.getBlob(1);
1:58ac441:             InputStream content = Blob.getBinaryStream();
1:58ac441:             long remaining = rs.getInt(2);
1:58ac441:             while (remaining > 0) {
1:58ac441:                 remaining -= content.read(byteBuf);
1:58ac441:             }
1:58ac441:             content.close();
1:58ac441:         }
1:58ac441:         rs.close();
1:58ac441:     }
1:58ac441: 
1:58ac441:     /**
1:58ac441:      * Fetches a single Blob and reads it byte by byte, but utilizing a
1:58ac441:      * buffered stream to get a lower time bound on the read operation.
1:58ac441:      */
1:58ac441:     public void testFetchLargeBlobOneByOneByteBaseline()
1:58ac441:             throws IOException, SQLException {
1:58ac441:         // Select just one Blob.
1:58ac441:         PreparedStatement ps = prepareStatement(
1:58ac441:                 "select dBlob, length from largeBlobs where id = 4");
1:58ac441:         ResultSet rs = ps.executeQuery();
1:58ac441:         while (rs.next()) {
1:58ac441:             Blob Blob = rs.getBlob(1);
1:58ac441:             InputStream content = Blob.getBinaryStream();
1:58ac441:             BufferedInputStream bufferedContent =
1:58ac441:                     new BufferedInputStream(content);
1:58ac441:             long remaining = rs.getInt(2);
1:58ac441:             while (bufferedContent.read() != -1) {
1:58ac441:                 remaining--;
1:58ac441:             }
1:58ac441:             content.close();
1:58ac441:             assertEquals(0, remaining);
1:58ac441:         }
1:58ac441:         rs.close();
1:58ac441:     }
1:58ac441: 
1:58ac441:     /**
1:58ac441:      * Fetches a single Blob and reads it byte by byte.
1:58ac441:      */
1:58ac441:     public void testFetchLargeBlobOneByOneByte()
1:58ac441:             throws IOException, SQLException {
1:58ac441:         // Select just one Blob.
1:58ac441:         PreparedStatement ps = prepareStatement(
1:58ac441:                 "select dBlob, length from largeBlobs where id = 4");
1:58ac441:         ResultSet rs = ps.executeQuery();
1:58ac441:         while (rs.next()) {
1:58ac441:             Blob Blob = rs.getBlob(1);
1:58ac441:             InputStream content = Blob.getBinaryStream();
1:58ac441:             long remaining = rs.getInt(2);
1:58ac441:             while (content.read() != -1) {
1:58ac441:                 remaining--;
1:58ac441:             }
1:58ac441:             content.close();
1:58ac441:             assertEquals(0, remaining);
1:58ac441:         }
1:58ac441:         rs.close();
1:58ac441:     }
1:58ac441: 
1:58ac441:     /**
1:58ac441:      * Fetches a single Blob and reads it byte by byte after it has first been
1:58ac441:      * modified.
1:58ac441:      * <p>
1:58ac441:      * The point of modifiying the Blob is to make Derby use the writable Blob
1:58ac441:      * representation (different implementation).
1:58ac441:      */
1:58ac441:     public void testFetchLargeBlobOneByOneByteModified()
1:58ac441:             throws IOException, SQLException {
1:58ac441:         // Select just one Blob.
1:58ac441:         PreparedStatement ps = prepareStatement(
1:58ac441:                 "select dBlob, length from largeBlobs where id = 4");
1:58ac441:         ResultSet rs = ps.executeQuery();
1:58ac441:         while (rs.next()) {
1:58ac441:             Blob Blob = rs.getBlob(1);
1:58ac441:             long remaining = rs.getInt(2);
1:58ac441:             Blob.setBytes(++remaining, "X".getBytes("US-ASCII"));
1:58ac441:             InputStream content = Blob.getBinaryStream();
1:58ac441:             while (content.read() != -1) {
1:58ac441:                 remaining --;
1:58ac441:             }
1:58ac441:             content.close();
1:58ac441:             assertEquals(0, remaining);
1:58ac441:         }
1:58ac441:         rs.close();
1:58ac441:     }
1:58ac441: 
1:58ac441:     /**
1:58ac441:      * Fetches a single Blob by reading it piece by piece with {@code getBytes}.
1:58ac441:      */
1:58ac441:     public void testFetchLargeBlobPieceByPiece()
1:58ac441:             throws IOException, SQLException {
1:58ac441:         fetchBlobPieceByPiece(false, FETCH_GETBYTES);
1:58ac441:     }
1:58ac441: 
1:58ac441:     /**
1:58ac441:      * Fetches a single Blob by reading it piece by piece with {@code getBytes}.
1:58ac441:      */
1:58ac441:     public void testFetchLargeBlobPieceByPieceModified()
1:58ac441:             throws IOException, SQLException {
1:8be1f95:         fetchBlobPieceByPiece(true, FETCH_GETBYTES);
1:58ac441:     }
1:58ac441: 
1:58ac441:     /**
1:58ac441:      * Fetches a single Blob by reading it in chunks with
1:58ac441:      * {@code getBinaryStream}.
1:58ac441:      */
1:58ac441:     public void testFetchLargeBlob()
1:58ac441:             throws IOException, SQLException {
1:58ac441:         fetchBlobPieceByPiece(false, FETCH_GETBINARYSTREAM);
1:58ac441:     }
1:58ac441: 
1:58ac441:     /**
1:58ac441:      * Fetches a single Blob by reading it in chunks with
1:58ac441:      * {@code getBinaryStream}.
1:58ac441:      */
1:58ac441:     public void testFetchLargeBlobModified()
1:58ac441:             throws IOException, SQLException {
2:58ac441:         fetchBlobPieceByPiece(true, FETCH_GETBINARYSTREAM);
1:58ac441:     }
1:58ac441: 
1:58ac441:     /**
1:58ac441:      * Fetches a "large" Blob piece by piece using getBytes.
1:58ac441:      *
1:58ac441:      * @param modifyBlob whether to modify the Blob before fetching it
1:58ac441:      *      (determines the internal Derby Blob representation)
1:58ac441:      */
1:58ac441:     private void fetchBlobPieceByPiece(boolean modifyBlob, int fetchMode)
1:58ac441:             throws IOException, SQLException {
1:58ac441:         // Select just one Blob.
1:58ac441:         PreparedStatement ps = prepareStatement(
1:58ac441:                 "select dBlob, length from largeBlobs where id = 4");
1:58ac441:         ResultSet rs = ps.executeQuery();
1:58ac441:         while (rs.next()) {
1:58ac441:             Blob blob = rs.getBlob(1);
1:58ac441:             long remaining = rs.getInt(2);
1:58ac441:             if (modifyBlob) {
1:58ac441:                 // Modify the Blob to create a temporary copy in memory or on
1:58ac441:                 // disk (depends on the Blob size).
1:58ac441:                 long modifyStart = System.currentTimeMillis();
1:58ac441:                 blob.setBytes(++remaining, new byte[] {(byte)'X'});
1:58ac441:                 println("Blob modification duration: " +
1:58ac441:                         (System.currentTimeMillis() - modifyStart) + " ms");
1:58ac441:             }
1:58ac441:             long pos = 1;
1:58ac441:             int MAX_SIZE = 32676;
1:58ac441:             switch (fetchMode) {
1:58ac441:                 case FETCH_GETBYTES:
1:58ac441:                     while (remaining > 0) {
1:58ac441:                         byte[] bytes = blob.getBytes(
1:58ac441:                                 pos, (int)Math.min(MAX_SIZE, remaining));
1:58ac441:                         pos += bytes.length;
1:58ac441:                         remaining -= bytes.length;
1:58ac441:                     }
1:58ac441:                     break;
1:58ac441:                 case FETCH_GETBINARYSTREAM:
1:58ac441:                     InputStream stream = blob.getBinaryStream();
1:58ac441:                     byte[] buf = new byte[MAX_SIZE];
1:58ac441:                     while (remaining > 0) {
1:58ac441:                         int read = stream.read(buf);
1:58ac441:                         pos += read;
1:58ac441:                         remaining -= read;
1:58ac441:                     }
1:58ac441:                     stream.close();
1:58ac441:                     break;
1:58ac441:                 default:
1:58ac441:                     fail("Unknown fetch mode: " + fetchMode);
1:58ac441:             }
1:58ac441:         }
1:58ac441:         rs.close();
1:58ac441:     }
1:58ac441: 
1:58ac441:     /**
1:58ac441:      * Tests if the Blob length is cached.
1:58ac441:      */
1:58ac441:     public void testLargeBlobGetLength() throws SQLException {
1:58ac441:         // Select just one Blob.
1:58ac441:         PreparedStatement ps = prepareStatement(
1:58ac441:                 "select dBlob, length from largeBlobs where id = 7");
1:58ac441:         ResultSet rs = ps.executeQuery();
1:58ac441:         while (rs.next()) {
1:58ac441:             Blob blob = rs.getBlob(1);
1:58ac441:             long length = rs.getInt(2);
1:58ac441:             // This should be cached. Have to skip lots of data otherwise.
1:58ac441:             for (int i=0; i < 50; i++) {
1:58ac441:                 assertEquals(length, blob.length());
1:58ac441:             }
1:58ac441:         }
1:58ac441:         rs.close();
1:58ac441:     }
1:58ac441: 
1:58ac441:     /**
1:58ac441:      * Tests if the Blob length is cached.
1:58ac441:      */
1:58ac441:     public void testLargeBlobGetLengthModified() throws SQLException {
1:58ac441:         // Select just one Blob.
1:58ac441:         PreparedStatement ps = prepareStatement(
1:58ac441:                 "select dBlob, length from largeBlobs where id = 7");
1:58ac441:         ResultSet rs = ps.executeQuery();
1:58ac441:         while (rs.next()) {
1:58ac441:             Blob blob = rs.getBlob(1);
1:58ac441:             blob.setBytes(1, new byte[] {(byte)'X'});
1:58ac441:             long length = rs.getInt(2);
1:58ac441:             // This should be cached. Have to skip lots of data otherwise.
1:58ac441:             for (int i=0; i < 50; i++) {
1:58ac441:                 assertEquals(length, blob.length());
1:58ac441:             }
1:58ac441:         }
1:58ac441:         rs.close();
1:58ac441:     }
1:58ac441: 
1:58ac441:     /**
1:58ac441:      * Runs a test using multiple threads.
1:58ac441:      * <p>
1:58ac441:      * This test intends to detect problems with small Blobs and general
1:58ac441:      * problems with concurrency.
1:58ac441:      * <p>
1:58ac441:      * <b>NOTE</b>: To produce more reliable numbers, please run the performance
1:58ac441:      * client independently outside this JUnit test framework. Performance also
1:58ac441:      * suffers greatly with SANE builds.
1:58ac441:      */
1:58ac441:     public void testConcurrency()
1:58ac441:             throws InterruptedException, SQLException {
1:58ac441: 
1:58ac441:         final int records = 100000;
1:58ac441:         final int tables = 1;
1:58ac441:         final int threads = 16;
1:58ac441:         DBFiller filler = new SingleRecordFiller(
1:58ac441:                 records, tables, java.sql.Types.BLOB, false, false);
1:58ac441:         Connection conn = getConnection();
1:58ac441:         println("initializing database...");
1:58ac441:         filler.fill(conn);
1:58ac441:         conn.close();
1:58ac441: 
1:58ac441:         Client[] clients = new Client[threads];
1:58ac441:         for (int i = 0; i < clients.length; i++) {
1:58ac441:             Connection c = openDefaultConnection();
1:58ac441:             c.setTransactionIsolation(Connection.TRANSACTION_READ_COMMITTED);
1:58ac441:             clients[i] = new SingleRecordSelectClient(
1:58ac441:                     records, tables, java.sql.Types.BLOB, false, false);
1:58ac441:             clients[i].init(c);
1:58ac441:         }
1:58ac441: 
1:58ac441:         final int warmupSec = 30;
1:58ac441:         final int steadySec = 60;
1:58ac441:         LoadGenerator gen = new BackToBackLoadGenerator();
1:58ac441:         gen.init(clients);
1:58ac441:         println("starting warmup...");
1:58ac441:         gen.startWarmup();
1:58ac441:         Thread.sleep(1000L * warmupSec);
1:58ac441:         println("entering steady state...");
1:58ac441:         gen.startSteadyState();
1:58ac441:         Thread.sleep(1000L * steadySec);
1:58ac441:         println("stopping threads...");
1:58ac441:         gen.stop();
1:58ac441:         // Should get the printstream used by the test harness here.
1:58ac441:         gen.printReport(System.out);
1:58ac441:     }
1:58ac441: 
1:58ac441:     /**
1:58ac441:      * Generates test data.
1:58ac441:      */
1:58ac441:     private static void initializeBlobData(Statement stmt)
1:58ac441:             throws SQLException, UnsupportedEncodingException {
1:58ac441:         Connection con = stmt.getConnection();
1:58ac441:         con.setAutoCommit(false);
1:58ac441:         if (!disableSmallBlobs) {
1:58ac441:             println("Generating small Blobs test data.");
1:58ac441:             // Insert small Blob data.
1:58ac441:             try {
1:58ac441:                 stmt.executeUpdate("drop table smallBlobs");
1:58ac441:             } catch (SQLException sqle) {
1:58ac441:                 assertSQLState("42Y55", sqle);
1:58ac441:             }
1:58ac441:             stmt.executeUpdate(
1:58ac441:                     "create table smallBlobs (dBlob Blob, length int)");
1:58ac441:             PreparedStatement smallBlobInsert = con.prepareStatement(
1:58ac441:                     "insert into smallBlobs values (?,?)");
1:58ac441:             // Insert 15 000 small Blobs.
1:58ac441:             for (int BlobCounter = 1; BlobCounter < 15001; BlobCounter++) {
1:58ac441:                 byte[] content =
1:58ac441:                         Integer.toString(BlobCounter).getBytes("US-ASCII");
1:58ac441:                 smallBlobInsert.setBytes(1, content);
1:58ac441:                 smallBlobInsert.setInt(2, content.length);
1:58ac441:                 smallBlobInsert.executeUpdate();
1:58ac441:                 if (BlobCounter % 1000 == 0) {
1:58ac441:                     con.commit();
1:58ac441:                 }
1:58ac441:             }
1:58ac441:             con.commit();
1:58ac441:         }
1:58ac441: 
1:58ac441:         if (!disableLargeBlobs) {
1:58ac441:             println("Generating large Blobs test data.");
1:58ac441:             // Insert large Blob data.
1:58ac441:             try {
1:58ac441:                 stmt.executeUpdate("drop table largeBlobs");
1:58ac441:             } catch (SQLException sqle) {
1:58ac441:                 assertSQLState("42Y55", sqle);
1:58ac441:             }
1:58ac441:             stmt.executeUpdate("create table largeBlobs (" +
1:58ac441:                     "id int unique not null, dBlob Blob, length int)");
1:58ac441:             PreparedStatement largeBlobInsert = con.prepareStatement(
1:58ac441:                     "insert into largeBlobs values (?,?,?)");
1:58ac441:             // Insert some large Blobs.
1:58ac441:             final int size = largeBlobSizeMB*1024*1024; // 15 MB default
1:58ac441:             for (int BlobCounter = 1; BlobCounter < 11; BlobCounter++) {
1:58ac441:                 largeBlobInsert.setInt(1, BlobCounter);
1:58ac441:                 largeBlobInsert.setBinaryStream(
1:58ac441:                         2, new LoopingAlphabetStream(size), size);
1:58ac441:                 largeBlobInsert.setInt(3, size);
1:58ac441:                 largeBlobInsert.executeUpdate();
1:58ac441:             }
1:58ac441:             con.commit();
1:58ac441:         }
1:58ac441:     }
1:58ac441: }
============================================================================
author:Dag H. Wanvik
-------------------------------------------------------------------------------
commit:1ae02c9
/////////////////////////////////////////////////////////////////////////
1: import org.apache.derbyTesting.junit.BaseTestSuite;
/////////////////////////////////////////////////////////////////////////
1:         BaseTestSuite mainSuite = new BaseTestSuite("BlobAccessTest suite");
1:             BaseTestSuite smallSuite = new BaseTestSuite("Small Blob suite");
/////////////////////////////////////////////////////////////////////////
1:             BaseTestSuite largeSuite = new BaseTestSuite("Large Blob suite");
author:Kristian Waagan
-------------------------------------------------------------------------------
commit:8be1f95
/////////////////////////////////////////////////////////////////////////
1:         fetchBlobPieceByPiece(true, FETCH_GETBYTES);
commit:58ac441
/////////////////////////////////////////////////////////////////////////
1: /*
1: 
1:    Derby - Class org.apache.derbyTesting.perf.basic.jdbc.BlobAccessTest
1: 
1:    Licensed to the Apache Software Foundation (ASF) under one or more
1:    contributor license agreements.  See the NOTICE file distributed with
1:    this work for additional information regarding copyright ownership.
1:    The ASF licenses this file to you under the Apache License, Version 2.0
1:    (the "License"); you may not use this file except in compliance with
1:    the License.  You may obtain a copy of the License at
1: 
1:       http://www.apache.org/licenses/LICENSE-2.0
1: 
1:    Unless required by applicable law or agreed to in writing, software
1:    distributed under the License is distributed on an "AS IS" BASIS,
1:    WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
1:    See the License for the specific language governing permissions and
1:    limitations under the License.
1: 
1:  */
1: package org.apache.derbyTesting.perf.basic.jdbc;
1: 
1: import java.io.BufferedInputStream;
1: import java.io.IOException;
1: import java.io.InputStream;
1: import java.io.UnsupportedEncodingException;
1: import java.sql.Blob;
1: import java.sql.Connection;
1: import java.sql.PreparedStatement;
1: import java.sql.ResultSet;
1: import java.sql.SQLException;
1: import java.sql.Statement;
1: import junit.framework.Test;
0: import junit.framework.TestSuite;
1: import org.apache.derbyTesting.functionTests.util.streams.LoopingAlphabetStream;
1: import org.apache.derbyTesting.junit.CleanDatabaseTestSetup;
1: import org.apache.derbyTesting.junit.JDBCPerfTestCase;
1: import org.apache.derbyTesting.perf.clients.BackToBackLoadGenerator;
1: import org.apache.derbyTesting.perf.clients.Client;
1: import org.apache.derbyTesting.perf.clients.DBFiller;
1: import org.apache.derbyTesting.perf.clients.LoadGenerator;
1: import org.apache.derbyTesting.perf.clients.SingleRecordFiller;
1: import org.apache.derbyTesting.perf.clients.SingleRecordSelectClient;
1: 
1: /**
1:  * A series of tests accessing Blobs in various ways.
1:  * <p>
1:  * These tests are intended to detect Blob performance regressions. Before
1:  * committing a patch that might change the Blob performance characteristics,
1:  * first run these tests on a clean build and then with the patch applied. The
1:  * results can only be compared when both runs are done on the same machine.
1:  * <p>
1:  * The results are the time taken to execute the test. Lower duration is better
1:  * (improvement). Currently the results are printed to standard out. There is
1:  * one exception, which is {@code testConcurrency}. For this test, the
1:  * throughput is printed and it will always run for a fixed amount of time.
1:  * <p>
1:  * The tests are written with two axis in mind: read-only vs update and small vs
1:  * large. These axis were chosen based on the Blob implementation at the time.
1:  * In the context of this test, small means the Blob is represented as a string
1:  * by the Derby store and large means the Blob is represtend as a stream into
1:  * the Derby store. When a Blob is modified, an in-memory or on disk temporary
1:  * copy is created. The performance of these temporary representations are
1:  * tested with the tests that modify the Blob content.
1:  * <p>
1:  * System properties controlling test behavior:
1:  * <ul><li>derby.tests.disableSmallBlobs</li>
1:  *     <li>derby.tests.disableLargeBlobs</li>
1:  *     <li>derby.tests.disableConcurrencyTest</li>
1:  *     <li>derby.tests.largeBlobSize (in MB, 15 is the default)</li>
1:  * </ul>
1:  *
1:  * <p>
1:  * <b>NOTE</b>: Currently there are no tests for the client driver (network)
1:  * or for encrypted Blobs.
1:  */
1: public class BlobAccessTest
1:         extends JDBCPerfTestCase {
1: 
1:     private static final boolean disableSmallBlobs =
1:             Boolean.getBoolean("derby.tests.disableSmallBlobs");
1:     private static final boolean disableLargeBlobs =
1:             Boolean.getBoolean("derby.tests.disableLargeBlobs");
1:     private static final boolean disableConcurrencyTest =
1:             Boolean.getBoolean("derby.tests.disableConcurrencyTest");
1:     private static final int largeBlobSizeMB =
1:             Integer.getInteger("derby.tests.largeBlobSize", 15).intValue();
1: 
1:     private static final int FETCH_GETBYTES = 0;
1:     private static final int FETCH_GETBINARYSTREAM = 1;
1: 
1:     /**
1:      * Instantiates a new test that will be run the specified number of
1:      * iterations and repeated as specified.
1:      *
1:      * @param name name of the test to instantiate
1:      * @param iterations number of iterations per repetition
1:      * @param repeats number of repetitions
1:      */
1:     public BlobAccessTest(String name, int iterations, int repeats) {
1:         super(name, iterations, repeats);
1:     }
1: 
1:     /**
1:      * Set autocommit to false by default.
1:      */
1:     public void initializeConnection(Connection conn)
1:             throws SQLException {
1:         conn.setAutoCommit(false);
1:     }
1: 
1:     /**
1:      * Generates a suite of tests.
1:      * <p>
1:      * The required test data will be generated. Note that a subset of the
1:      * tests can be disabled by using a system property.
1:      *
1:      * @return A suite of tests.
1:      */
1:     public static Test suite() {
0:         TestSuite mainSuite = new TestSuite("BlobAccessTest suite");
1:         if (!disableSmallBlobs) {
1:             int iters = 50;
1:             int reps = 3;
1:             println("Adding small Blob tests.");
0:             TestSuite smallSuite = new TestSuite("Small Blob suite");
1:             smallSuite.addTest(new BlobAccessTest(
1:                     "testFetchSmallBlobs", iters, reps));
1:             smallSuite.addTest(new BlobAccessTest(
1:                     "testFetchSmallBlobsInaccurateLength", iters, reps));
1:             smallSuite.addTest(new BlobAccessTest(
1:                     "testModifySmallBlobs", iters, reps));
1:             mainSuite.addTest(smallSuite);
1:         }
1:         if (!disableLargeBlobs) {
1:             int iters = 5;
1:             int reps = 3;
1:             println("Adding large Blob tests.");
0:             TestSuite largeSuite = new TestSuite("Large Blob suite");
1:             largeSuite.addTest(new BlobAccessTest(
1:                     "testFetchLargeBlobs", iters, reps));
1:             largeSuite.addTest(new BlobAccessTest(
1:                     "testFetchLargeBlobOneByOneByteBaseline", iters, reps));
1:             largeSuite.addTest(new BlobAccessTest(
1:                     "testFetchLargeBlobOneByOneByteModified", iters, reps));
1:             largeSuite.addTest(new BlobAccessTest(
1:                     "testFetchLargeBlobOneByOneByte", iters, reps));
1:             largeSuite.addTest(new BlobAccessTest(
1:                     "testFetchLargeBlob", iters, reps));
1:             largeSuite.addTest(new BlobAccessTest(
1:                     "testFetchLargeBlobModified", iters, reps));
1:             largeSuite.addTest(new BlobAccessTest(
1:                     "testFetchLargeBlobPieceByPiece", iters, reps));
1:             largeSuite.addTest(new BlobAccessTest(
1:                     "testFetchLargeBlobPieceByPieceModified", iters, reps));
1:             largeSuite.addTest(new BlobAccessTest(
1:                     "testLargeBlobGetLength", iters, reps));
1:             mainSuite.addTest(largeSuite);
1:         }
1:         if (!disableConcurrencyTest) {
1:             mainSuite.addTest(new BlobAccessTest("testConcurrency", 1, 1));
1:         }
1:         return new CleanDatabaseTestSetup(mainSuite) {
1:             protected void decorateSQL(Statement stmt)
1:                     throws SQLException {
1:                 try {
1:                     initializeBlobData(stmt);
1:                 } catch (UnsupportedEncodingException uee) {
1:                     // Compiled with JDK 1.4, can't use constructor.
1:                     SQLException sqle = new SQLException();
1:                     sqle.initCause(uee);
1:                     throw sqle;
1:                 }
1:             }
1:         };
1:     }
1: 
1:     /**
1:      * Fetches a number of small Blobs, getting the content using getBytes.
1:      * <p>
1:      * The exact length of the Blob is used when getting the bytes.
1:      */
1:     public void testFetchSmallBlobs()
1:             throws SQLException {
1:         PreparedStatement ps = prepareStatement(
1:                 "select dBlob, length from smallBlobs");
1:         ResultSet rs = ps.executeQuery();
1:         while (rs.next()) {
1:             Blob Blob = rs.getBlob(1);
1:             int blobLength = rs.getInt(2);
1:             byte[] content = Blob.getBytes(1, blobLength);
1:         }
1:         rs.close();
1:     }
1: 
1:     /**
1:      * Fetches a number of small Blobs, getting the content using getBytes.
1:      * <p>
1:      * A too long length of the Blob is used when getting the bytes.
1:      */
1:     public void testFetchSmallBlobsInaccurateLength()
1:             throws SQLException {
1:         PreparedStatement ps = prepareStatement(
1:                 "select dBlob, length from smallBlobs");
1:         ResultSet rs = ps.executeQuery();
1:         while (rs.next()) {
1:             Blob Blob = rs.getBlob(1);
1:             int unusedLength = rs.getInt(2);
1:             byte[] content = Blob.getBytes(1, 100);
1:         }
1:         rs.close();
1:     }
1: 
1:     /**
1:      * Test fetching the content after adding a single byte at the end.
1:      */
1:     public void testModifySmallBlobs()
1:             throws SQLException, UnsupportedEncodingException {
1:         PreparedStatement ps = prepareStatement(
1:                 "select dBlob, length from smallBlobs");
1:         ResultSet rs = ps.executeQuery();
1:         while (rs.next()) {
1:             Blob Blob = rs.getBlob(1);
1:             int length = rs.getInt(2);
1:             Blob.setBytes(length, "X".getBytes("US-ASCII"));
1:             byte[] content = Blob.getBytes(1, 100);
1:         }
1:         rs.close();
1:     }
1: 
1:     /**
1:      * Fetches a number of Blobs using a rather large read buffer with
1:      * {@code getBinaryStream}.
1:      */
1:     public void testFetchLargeBlobs()
1:             throws IOException, SQLException {
1:         PreparedStatement ps = prepareStatement(
1:                 "select dBlob, length from largeBlobs");
1:         ResultSet rs = ps.executeQuery();
1:         byte[] byteBuf = new byte[16*1024]; // 16 KB
1:         while (rs.next()) {
1:             Blob Blob = rs.getBlob(1);
1:             InputStream content = Blob.getBinaryStream();
1:             long remaining = rs.getInt(2);
1:             while (remaining > 0) {
1:                 remaining -= content.read(byteBuf);
1:             }
1:             content.close();
1:         }
1:         rs.close();
1:     }
1: 
1:     /**
1:      * Fetches a single Blob and reads it byte by byte, but utilizing a
1:      * buffered stream to get a lower time bound on the read operation.
1:      */
1:     public void testFetchLargeBlobOneByOneByteBaseline()
1:             throws IOException, SQLException {
1:         // Select just one Blob.
1:         PreparedStatement ps = prepareStatement(
1:                 "select dBlob, length from largeBlobs where id = 4");
1:         ResultSet rs = ps.executeQuery();
1:         while (rs.next()) {
1:             Blob Blob = rs.getBlob(1);
1:             InputStream content = Blob.getBinaryStream();
1:             BufferedInputStream bufferedContent =
1:                     new BufferedInputStream(content);
1:             long remaining = rs.getInt(2);
1:             while (bufferedContent.read() != -1) {
1:                 remaining--;
1:             }
1:             content.close();
1:             assertEquals(0, remaining);
1:         }
1:         rs.close();
1:     }
1: 
1:     /**
1:      * Fetches a single Blob and reads it byte by byte.
1:      */
1:     public void testFetchLargeBlobOneByOneByte()
1:             throws IOException, SQLException {
1:         // Select just one Blob.
1:         PreparedStatement ps = prepareStatement(
1:                 "select dBlob, length from largeBlobs where id = 4");
1:         ResultSet rs = ps.executeQuery();
1:         while (rs.next()) {
1:             Blob Blob = rs.getBlob(1);
1:             InputStream content = Blob.getBinaryStream();
1:             long remaining = rs.getInt(2);
1:             while (content.read() != -1) {
1:                 remaining--;
1:             }
1:             content.close();
1:             assertEquals(0, remaining);
1:         }
1:         rs.close();
1:     }
1: 
1:     /**
1:      * Fetches a single Blob and reads it byte by byte after it has first been
1:      * modified.
1:      * <p>
1:      * The point of modifiying the Blob is to make Derby use the writable Blob
1:      * representation (different implementation).
1:      */
1:     public void testFetchLargeBlobOneByOneByteModified()
1:             throws IOException, SQLException {
1:         // Select just one Blob.
1:         PreparedStatement ps = prepareStatement(
1:                 "select dBlob, length from largeBlobs where id = 4");
1:         ResultSet rs = ps.executeQuery();
1:         while (rs.next()) {
1:             Blob Blob = rs.getBlob(1);
1:             long remaining = rs.getInt(2);
1:             Blob.setBytes(++remaining, "X".getBytes("US-ASCII"));
1:             InputStream content = Blob.getBinaryStream();
1:             while (content.read() != -1) {
1:                 remaining --;
1:             }
1:             content.close();
1:             assertEquals(0, remaining);
1:         }
1:         rs.close();
1:     }
1: 
1:     /**
1:      * Fetches a single Blob by reading it piece by piece with {@code getBytes}.
1:      */
1:     public void testFetchLargeBlobPieceByPiece()
1:             throws IOException, SQLException {
1:         fetchBlobPieceByPiece(false, FETCH_GETBYTES);
1:     }
1: 
1:     /**
1:      * Fetches a single Blob by reading it piece by piece with {@code getBytes}.
1:      */
1:     public void testFetchLargeBlobPieceByPieceModified()
1:             throws IOException, SQLException {
1:         fetchBlobPieceByPiece(true, FETCH_GETBINARYSTREAM);
1:     }
1: 
1:     /**
1:      * Fetches a single Blob by reading it in chunks with
1:      * {@code getBinaryStream}.
1:      */
1:     public void testFetchLargeBlob()
1:             throws IOException, SQLException {
1:         fetchBlobPieceByPiece(false, FETCH_GETBINARYSTREAM);
1:     }
1: 
1:     /**
1:      * Fetches a single Blob by reading it in chunks with
1:      * {@code getBinaryStream}.
1:      */
1:     public void testFetchLargeBlobModified()
1:             throws IOException, SQLException {
1:         fetchBlobPieceByPiece(true, FETCH_GETBINARYSTREAM);
1:     }
1: 
1:     /**
1:      * Fetches a "large" Blob piece by piece using getBytes.
1:      *
1:      * @param modifyBlob whether to modify the Blob before fetching it
1:      *      (determines the internal Derby Blob representation)
1:      */
1:     private void fetchBlobPieceByPiece(boolean modifyBlob, int fetchMode)
1:             throws IOException, SQLException {
1:         // Select just one Blob.
1:         PreparedStatement ps = prepareStatement(
1:                 "select dBlob, length from largeBlobs where id = 4");
1:         ResultSet rs = ps.executeQuery();
1:         while (rs.next()) {
1:             Blob blob = rs.getBlob(1);
1:             long remaining = rs.getInt(2);
1:             if (modifyBlob) {
1:                 // Modify the Blob to create a temporary copy in memory or on
1:                 // disk (depends on the Blob size).
1:                 long modifyStart = System.currentTimeMillis();
1:                 blob.setBytes(++remaining, new byte[] {(byte)'X'});
1:                 println("Blob modification duration: " +
1:                         (System.currentTimeMillis() - modifyStart) + " ms");
1:             }
1:             long pos = 1;
1:             int MAX_SIZE = 32676;
1:             switch (fetchMode) {
1:                 case FETCH_GETBYTES:
1:                     while (remaining > 0) {
1:                         byte[] bytes = blob.getBytes(
1:                                 pos, (int)Math.min(MAX_SIZE, remaining));
1:                         pos += bytes.length;
1:                         remaining -= bytes.length;
1:                     }
1:                     break;
1:                 case FETCH_GETBINARYSTREAM:
1:                     InputStream stream = blob.getBinaryStream();
1:                     byte[] buf = new byte[MAX_SIZE];
1:                     while (remaining > 0) {
1:                         int read = stream.read(buf);
1:                         pos += read;
1:                         remaining -= read;
1:                     }
1:                     stream.close();
1:                     break;
1:                 default:
1:                     fail("Unknown fetch mode: " + fetchMode);
1:             }
1:         }
1:         rs.close();
1:     }
1: 
1:     /**
1:      * Tests if the Blob length is cached.
1:      */
1:     public void testLargeBlobGetLength() throws SQLException {
1:         // Select just one Blob.
1:         PreparedStatement ps = prepareStatement(
1:                 "select dBlob, length from largeBlobs where id = 7");
1:         ResultSet rs = ps.executeQuery();
1:         while (rs.next()) {
1:             Blob blob = rs.getBlob(1);
1:             long length = rs.getInt(2);
1:             // This should be cached. Have to skip lots of data otherwise.
1:             for (int i=0; i < 50; i++) {
1:                 assertEquals(length, blob.length());
1:             }
1:         }
1:         rs.close();
1:     }
1: 
1:     /**
1:      * Tests if the Blob length is cached.
1:      */
1:     public void testLargeBlobGetLengthModified() throws SQLException {
1:         // Select just one Blob.
1:         PreparedStatement ps = prepareStatement(
1:                 "select dBlob, length from largeBlobs where id = 7");
1:         ResultSet rs = ps.executeQuery();
1:         while (rs.next()) {
1:             Blob blob = rs.getBlob(1);
1:             blob.setBytes(1, new byte[] {(byte)'X'});
1:             long length = rs.getInt(2);
1:             // This should be cached. Have to skip lots of data otherwise.
1:             for (int i=0; i < 50; i++) {
1:                 assertEquals(length, blob.length());
1:             }
1:         }
1:         rs.close();
1:     }
1: 
1:     /**
1:      * Runs a test using multiple threads.
1:      * <p>
1:      * This test intends to detect problems with small Blobs and general
1:      * problems with concurrency.
1:      * <p>
1:      * <b>NOTE</b>: To produce more reliable numbers, please run the performance
1:      * client independently outside this JUnit test framework. Performance also
1:      * suffers greatly with SANE builds.
1:      */
1:     public void testConcurrency()
1:             throws InterruptedException, SQLException {
1: 
1:         final int records = 100000;
1:         final int tables = 1;
1:         final int threads = 16;
1:         DBFiller filler = new SingleRecordFiller(
1:                 records, tables, java.sql.Types.BLOB, false, false);
1:         Connection conn = getConnection();
1:         println("initializing database...");
1:         filler.fill(conn);
1:         conn.close();
1: 
1:         Client[] clients = new Client[threads];
1:         for (int i = 0; i < clients.length; i++) {
1:             Connection c = openDefaultConnection();
1:             c.setTransactionIsolation(Connection.TRANSACTION_READ_COMMITTED);
1:             clients[i] = new SingleRecordSelectClient(
1:                     records, tables, java.sql.Types.BLOB, false, false);
1:             clients[i].init(c);
1:         }
1: 
1:         final int warmupSec = 30;
1:         final int steadySec = 60;
1:         LoadGenerator gen = new BackToBackLoadGenerator();
1:         gen.init(clients);
1:         println("starting warmup...");
1:         gen.startWarmup();
1:         Thread.sleep(1000L * warmupSec);
1:         println("entering steady state...");
1:         gen.startSteadyState();
1:         Thread.sleep(1000L * steadySec);
1:         println("stopping threads...");
1:         gen.stop();
1:         // Should get the printstream used by the test harness here.
1:         gen.printReport(System.out);
1:     }
1: 
1:     /**
1:      * Generates test data.
1:      */
1:     private static void initializeBlobData(Statement stmt)
1:             throws SQLException, UnsupportedEncodingException {
1:         Connection con = stmt.getConnection();
1:         con.setAutoCommit(false);
1:         if (!disableSmallBlobs) {
1:             println("Generating small Blobs test data.");
1:             // Insert small Blob data.
1:             try {
1:                 stmt.executeUpdate("drop table smallBlobs");
1:             } catch (SQLException sqle) {
1:                 assertSQLState("42Y55", sqle);
1:             }
1:             stmt.executeUpdate(
1:                     "create table smallBlobs (dBlob Blob, length int)");
1:             PreparedStatement smallBlobInsert = con.prepareStatement(
1:                     "insert into smallBlobs values (?,?)");
1:             // Insert 15 000 small Blobs.
1:             for (int BlobCounter = 1; BlobCounter < 15001; BlobCounter++) {
1:                 byte[] content =
1:                         Integer.toString(BlobCounter).getBytes("US-ASCII");
1:                 smallBlobInsert.setBytes(1, content);
1:                 smallBlobInsert.setInt(2, content.length);
1:                 smallBlobInsert.executeUpdate();
1:                 if (BlobCounter % 1000 == 0) {
1:                     con.commit();
1:                 }
1:             }
1:             con.commit();
1:         }
1: 
1:         if (!disableLargeBlobs) {
1:             println("Generating large Blobs test data.");
1:             // Insert large Blob data.
1:             try {
1:                 stmt.executeUpdate("drop table largeBlobs");
1:             } catch (SQLException sqle) {
1:                 assertSQLState("42Y55", sqle);
1:             }
1:             stmt.executeUpdate("create table largeBlobs (" +
1:                     "id int unique not null, dBlob Blob, length int)");
1:             PreparedStatement largeBlobInsert = con.prepareStatement(
1:                     "insert into largeBlobs values (?,?,?)");
1:             // Insert some large Blobs.
1:             final int size = largeBlobSizeMB*1024*1024; // 15 MB default
1:             for (int BlobCounter = 1; BlobCounter < 11; BlobCounter++) {
1:                 largeBlobInsert.setInt(1, BlobCounter);
1:                 largeBlobInsert.setBinaryStream(
1:                         2, new LoopingAlphabetStream(size), size);
1:                 largeBlobInsert.setInt(3, size);
1:                 largeBlobInsert.executeUpdate();
1:             }
1:             con.commit();
1:         }
1:     }
1: }
============================================================================