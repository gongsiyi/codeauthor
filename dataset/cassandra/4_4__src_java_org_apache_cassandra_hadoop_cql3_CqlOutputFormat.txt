1:56e0ad1: /*
1:56e0ad1:  * Licensed to the Apache Software Foundation (ASF) under one
1:56e0ad1:  * or more contributor license agreements.  See the NOTICE file
1:56e0ad1:  * distributed with this work for additional information
1:56e0ad1:  * regarding copyright ownership.  The ASF licenses this file
1:56e0ad1:  * to you under the Apache License, Version 2.0 (the
1:56e0ad1:  * "License"); you may not use this file except in compliance
1:56e0ad1:  * with the License.  You may obtain a copy of the License at
1:56e0ad1:  *
1:56e0ad1:  *     http://www.apache.org/licenses/LICENSE-2.0
1:56e0ad1:  *
1:56e0ad1:  * Unless required by applicable law or agreed to in writing, software
1:56e0ad1:  * distributed under the License is distributed on an "AS IS" BASIS,
1:56e0ad1:  * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
1:56e0ad1:  * See the License for the specific language governing permissions and
1:56e0ad1:  * limitations under the License.
1:56e0ad1:  */
1:56e0ad1: package org.apache.cassandra.hadoop.cql3;
1:56e0ad1: 
1:56e0ad1: 
1:56e0ad1: import java.io.IOException;
1:56e0ad1: import java.nio.ByteBuffer;
1:56e0ad1: import java.util.List;
1:56e0ad1: import java.util.Map;
1:56e0ad1: 
1:f698cc2: import org.apache.cassandra.hadoop.*;
1:f698cc2: import org.apache.hadoop.conf.*;
1:56e0ad1: import org.apache.hadoop.mapreduce.*;
1:56e0ad1: 
1:56e0ad1: /**
1:f698cc2:  * The <code>CqlOutputFormat</code> acts as a Hadoop-specific
1:56e0ad1:  * OutputFormat that allows reduce tasks to store keys (and corresponding
1:88ad4f4:  * bound variable values) as CQL rows (and respective columns) in a given
1:f698cc2:  * table.
1:56e0ad1:  *
1:56e0ad1:  * <p>
1:5fe804c:  * As is the case with the {@link org.apache.cassandra.hadoop.ColumnFamilyInputFormat}, 
1:5fe804c:  * you need to set the prepared statement in your
1:a004779:  * Hadoop job Configuration. The {@link CqlConfigHelper} class, through its
1:bef913a:  * {@link CqlConfigHelper#setOutputCql} method, is provided to make this
1:56e0ad1:  * simple.
1:56e0ad1:  * you need to set the Keyspace. The {@link ConfigHelper} class, through its
1:56e0ad1:  * {@link ConfigHelper#setOutputColumnFamily} method, is provided to make this
1:56e0ad1:  * simple.
1:56e0ad1:  * </p>
1:56e0ad1:  * 
1:56e0ad1:  * <p>
1:56e0ad1:  * For the sake of performance, this class employs a lazy write-back caching
1:56e0ad1:  * mechanism, where its record writer prepared statement binded variable values
1:56e0ad1:  * created based on the reduce's inputs (in a task-specific map), and periodically 
1:56e0ad1:  * makes the changes official by sending a execution of prepared statement request 
1:56e0ad1:  * to Cassandra.
1:56e0ad1:  * </p>
1:56e0ad1:  */
1:f698cc2: public class CqlOutputFormat extends OutputFormat<Map<String, ByteBuffer>, List<ByteBuffer>>
1:f698cc2:         implements org.apache.hadoop.mapred.OutputFormat<Map<String, ByteBuffer>, List<ByteBuffer>>
1:f698cc2: {
1:446e253:     public static final String BATCH_THRESHOLD = "mapreduce.output.columnfamilyoutputformat.batch.threshold";
1:446e253:     public static final String QUEUE_SIZE = "mapreduce.output.columnfamilyoutputformat.queue.size";
1:446e253: 
1:f698cc2:     /**
1:f698cc2:      * Check for validity of the output-specification for the job.
1:f698cc2:      *
1:f698cc2:      * @param context
1:f698cc2:      *            information about the job
1:f698cc2:      */
1:f698cc2:     public void checkOutputSpecs(JobContext context)
1:f698cc2:     {
1:f698cc2:         checkOutputSpecs(HadoopCompat.getConfiguration(context));
1:f698cc2:     }
1:f698cc2: 
1:f698cc2:     protected void checkOutputSpecs(Configuration conf)
1:f698cc2:     {
1:f698cc2:         if (ConfigHelper.getOutputKeyspace(conf) == null)
1:f698cc2:             throw new UnsupportedOperationException("You must set the keyspace with setOutputKeyspace()");
1:f698cc2:         if (ConfigHelper.getOutputPartitioner(conf) == null)
1:f698cc2:             throw new UnsupportedOperationException("You must set the output partitioner to the one used by your Cassandra cluster");
1:f698cc2:         if (ConfigHelper.getOutputInitialAddress(conf) == null)
1:f698cc2:             throw new UnsupportedOperationException("You must set the initial output address to a Cassandra node");
1:f698cc2:     }
1:f698cc2: 
1:f698cc2:     /** Fills the deprecated OutputFormat interface for streaming. */
1:f698cc2:     @Deprecated
1:f698cc2:     public void checkOutputSpecs(org.apache.hadoop.fs.FileSystem filesystem, org.apache.hadoop.mapred.JobConf job) throws IOException
1:f698cc2:     {
1:f698cc2:         checkOutputSpecs(job);
1:f698cc2:     }
1:f698cc2: 
1:f698cc2:     /**
1:f698cc2:      * The OutputCommitter for this format does not write any data to the DFS.
1:f698cc2:      *
1:f698cc2:      * @param context
1:f698cc2:      *            the task context
1:f698cc2:      * @return an output committer
1:f698cc2:      * @throws IOException
1:f698cc2:      * @throws InterruptedException
1:f698cc2:      */
1:f698cc2:     public OutputCommitter getOutputCommitter(TaskAttemptContext context) throws IOException, InterruptedException
1:f698cc2:     {
1:f698cc2:         return new NullOutputCommitter();
1:f698cc2:     }
1:f698cc2: 
1:56e0ad1:     /** Fills the deprecated OutputFormat interface for streaming. */
1:56e0ad1:     @Deprecated
1:a004779:     public CqlRecordWriter getRecordWriter(org.apache.hadoop.fs.FileSystem filesystem, org.apache.hadoop.mapred.JobConf job, String name, org.apache.hadoop.util.Progressable progress) throws IOException
2:56e0ad1:     {
1:4cf8a8a:         return new CqlRecordWriter(job, progress);
1:56e0ad1:     }
1:56e0ad1: 
1:56e0ad1:     /**
1:56e0ad1:      * Get the {@link RecordWriter} for the given task.
1:56e0ad1:      *
1:56e0ad1:      * @param context
1:56e0ad1:      *            the information about the current task.
1:56e0ad1:      * @return a {@link RecordWriter} to write the output for the job.
1:56e0ad1:      * @throws IOException
1:56e0ad1:      */
1:a004779:     public CqlRecordWriter getRecordWriter(final TaskAttemptContext context) throws IOException, InterruptedException
1:56e0ad1:     {
1:a004779:         return new CqlRecordWriter(context);
1:56e0ad1:     }
1:f698cc2: 
1:f698cc2:     /**
1:f698cc2:      * An {@link OutputCommitter} that does nothing.
1:f698cc2:      */
1:f698cc2:     private static class NullOutputCommitter extends OutputCommitter
1:f698cc2:     {
1:f698cc2:         public void abortTask(TaskAttemptContext taskContext) { }
1:f698cc2: 
1:f698cc2:         public void cleanupJob(JobContext jobContext) { }
1:f698cc2: 
1:f698cc2:         public void commitTask(TaskAttemptContext taskContext) { }
1:f698cc2: 
1:f698cc2:         public boolean needsTaskCommit(TaskAttemptContext taskContext)
1:f698cc2:         {
1:f698cc2:             return false;
1:f698cc2:         }
1:f698cc2: 
1:f698cc2:         public void setupJob(JobContext jobContext) { }
1:f698cc2: 
1:f698cc2:         public void setupTask(TaskAttemptContext taskContext) { }
1:f698cc2:     }
1:56e0ad1: }
============================================================================
author:Philip Thompson
-------------------------------------------------------------------------------
commit:446e253
/////////////////////////////////////////////////////////////////////////
1:     public static final String BATCH_THRESHOLD = "mapreduce.output.columnfamilyoutputformat.batch.threshold";
1:     public static final String QUEUE_SIZE = "mapreduce.output.columnfamilyoutputformat.queue.size";
1: 
commit:f698cc2
/////////////////////////////////////////////////////////////////////////
1: import org.apache.cassandra.hadoop.*;
1: import org.apache.hadoop.conf.*;
1:  * The <code>CqlOutputFormat</code> acts as a Hadoop-specific
1:  * table.
/////////////////////////////////////////////////////////////////////////
1: public class CqlOutputFormat extends OutputFormat<Map<String, ByteBuffer>, List<ByteBuffer>>
1:         implements org.apache.hadoop.mapred.OutputFormat<Map<String, ByteBuffer>, List<ByteBuffer>>
1: {
1:     /**
1:      * Check for validity of the output-specification for the job.
1:      *
1:      * @param context
1:      *            information about the job
1:      */
1:     public void checkOutputSpecs(JobContext context)
1:     {
1:         checkOutputSpecs(HadoopCompat.getConfiguration(context));
1:     }
1: 
1:     protected void checkOutputSpecs(Configuration conf)
1:     {
1:         if (ConfigHelper.getOutputKeyspace(conf) == null)
1:             throw new UnsupportedOperationException("You must set the keyspace with setOutputKeyspace()");
1:         if (ConfigHelper.getOutputPartitioner(conf) == null)
1:             throw new UnsupportedOperationException("You must set the output partitioner to the one used by your Cassandra cluster");
1:         if (ConfigHelper.getOutputInitialAddress(conf) == null)
1:             throw new UnsupportedOperationException("You must set the initial output address to a Cassandra node");
1:     }
1: 
1:     /** Fills the deprecated OutputFormat interface for streaming. */
1:     @Deprecated
1:     public void checkOutputSpecs(org.apache.hadoop.fs.FileSystem filesystem, org.apache.hadoop.mapred.JobConf job) throws IOException
1:     {
1:         checkOutputSpecs(job);
1:     }
1: 
1:     /**
1:      * The OutputCommitter for this format does not write any data to the DFS.
1:      *
1:      * @param context
1:      *            the task context
1:      * @return an output committer
1:      * @throws IOException
1:      * @throws InterruptedException
1:      */
1:     public OutputCommitter getOutputCommitter(TaskAttemptContext context) throws IOException, InterruptedException
1:     {
1:         return new NullOutputCommitter();
1:     }
1: 
/////////////////////////////////////////////////////////////////////////
1: 
1:     /**
1:      * An {@link OutputCommitter} that does nothing.
1:      */
1:     private static class NullOutputCommitter extends OutputCommitter
1:     {
1:         public void abortTask(TaskAttemptContext taskContext) { }
1: 
1:         public void cleanupJob(JobContext jobContext) { }
1: 
1:         public void commitTask(TaskAttemptContext taskContext) { }
1: 
1:         public boolean needsTaskCommit(TaskAttemptContext taskContext)
1:         {
1:             return false;
1:         }
1: 
1:         public void setupJob(JobContext jobContext) { }
1: 
1:         public void setupTask(TaskAttemptContext taskContext) { }
1:     }
author:Jonathan Ellis
-------------------------------------------------------------------------------
commit:88ad4f4
/////////////////////////////////////////////////////////////////////////
1:  * bound variable values) as CQL rows (and respective columns) in a given
commit:a004779
/////////////////////////////////////////////////////////////////////////
1:  * Hadoop job Configuration. The {@link CqlConfigHelper} class, through its
/////////////////////////////////////////////////////////////////////////
0: public class CqlOutputFormat extends AbstractColumnFamilyOutputFormat<Map<String, ByteBuffer>, List<ByteBuffer>>
1:     public CqlRecordWriter getRecordWriter(org.apache.hadoop.fs.FileSystem filesystem, org.apache.hadoop.mapred.JobConf job, String name, org.apache.hadoop.util.Progressable progress) throws IOException
0:         return new CqlRecordWriter(job, new Progressable(progress));
/////////////////////////////////////////////////////////////////////////
1:     public CqlRecordWriter getRecordWriter(final TaskAttemptContext context) throws IOException, InterruptedException
1:         return new CqlRecordWriter(context);
commit:02a7ba8
/////////////////////////////////////////////////////////////////////////
0:  * Hadoop job Configuration. The {@link CqlConfigHelper} class, through its
/////////////////////////////////////////////////////////////////////////
0: public class CqlOutputFormat extends AbstractColumnFamilyOutputFormat<Map<String, ByteBuffer>, List<ByteBuffer>>
0:     public CqlRecordWriter getRecordWriter(org.apache.hadoop.fs.FileSystem filesystem, org.apache.hadoop.mapred.JobConf job, String name, org.apache.hadoop.util.Progressable progress) throws IOException
0:         return new CqlRecordWriter(job, new Progressable(progress));
/////////////////////////////////////////////////////////////////////////
0:     public CqlRecordWriter getRecordWriter(final TaskAttemptContext context) throws IOException, InterruptedException
0:         return new CqlRecordWriter(context);
commit:56e0ad1
/////////////////////////////////////////////////////////////////////////
1: /*
1:  * Licensed to the Apache Software Foundation (ASF) under one
1:  * or more contributor license agreements.  See the NOTICE file
1:  * distributed with this work for additional information
1:  * regarding copyright ownership.  The ASF licenses this file
1:  * to you under the Apache License, Version 2.0 (the
1:  * "License"); you may not use this file except in compliance
1:  * with the License.  You may obtain a copy of the License at
1:  *
1:  *     http://www.apache.org/licenses/LICENSE-2.0
1:  *
1:  * Unless required by applicable law or agreed to in writing, software
1:  * distributed under the License is distributed on an "AS IS" BASIS,
1:  * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
1:  * See the License for the specific language governing permissions and
1:  * limitations under the License.
1:  */
1: package org.apache.cassandra.hadoop.cql3;
1: 
1: 
1: import java.io.IOException;
1: import java.nio.ByteBuffer;
1: import java.util.List;
1: import java.util.Map;
1: 
0: import org.apache.cassandra.hadoop.AbstractColumnFamilyOutputFormat;
0: import org.apache.cassandra.hadoop.ColumnFamilyInputFormat;
0: import org.apache.cassandra.hadoop.ConfigHelper;
0: import org.apache.cassandra.hadoop.Progressable;
1: import org.apache.hadoop.mapreduce.*;
1: 
1: /**
0:  * The <code>ColumnFamilyOutputFormat</code> acts as a Hadoop-specific
1:  * OutputFormat that allows reduce tasks to store keys (and corresponding
0:  *  binded variable values) as CQL rows (and respective columns) in a given
0:  * ColumnFamily.
1:  *
1:  * <p>
0:  * As is the case with the {@link ColumnFamilyInputFormat}, you need to set the
0:  * prepared statement in your
0:  * Hadoop job Configuration. The {@link CQLConfigHelper} class, through its
0:  * {@link ConfigHelper#setOutputPreparedStatement} method, is provided to make this
1:  * simple.
1:  * you need to set the Keyspace. The {@link ConfigHelper} class, through its
1:  * {@link ConfigHelper#setOutputColumnFamily} method, is provided to make this
1:  * simple.
1:  * </p>
1:  * 
1:  * <p>
1:  * For the sake of performance, this class employs a lazy write-back caching
1:  * mechanism, where its record writer prepared statement binded variable values
1:  * created based on the reduce's inputs (in a task-specific map), and periodically 
1:  * makes the changes official by sending a execution of prepared statement request 
1:  * to Cassandra.
1:  * </p>
1:  */
0: public class ColumnFamilyOutputFormat extends AbstractColumnFamilyOutputFormat<Map<String, ByteBuffer>, List<ByteBuffer>>
1: {   
1:     /** Fills the deprecated OutputFormat interface for streaming. */
1:     @Deprecated
0:     public ColumnFamilyRecordWriter getRecordWriter(org.apache.hadoop.fs.FileSystem filesystem, org.apache.hadoop.mapred.JobConf job, String name, org.apache.hadoop.util.Progressable progress) throws IOException
1:     {
0:         return new ColumnFamilyRecordWriter(job, new Progressable(progress));
1:     }
1: 
1:     /**
1:      * Get the {@link RecordWriter} for the given task.
1:      *
1:      * @param context
1:      *            the information about the current task.
1:      * @return a {@link RecordWriter} to write the output for the job.
1:      * @throws IOException
1:      */
0:     public ColumnFamilyRecordWriter getRecordWriter(final TaskAttemptContext context) throws IOException, InterruptedException
1:     {
0:         return new ColumnFamilyRecordWriter(context);
1:     }
1: }
author:Dave Brosius
-------------------------------------------------------------------------------
commit:bef913a
/////////////////////////////////////////////////////////////////////////
1:  * {@link CqlConfigHelper#setOutputCql} method, is provided to make this
commit:6f217f7
/////////////////////////////////////////////////////////////////////////
commit:5fe804c
/////////////////////////////////////////////////////////////////////////
1:  * As is the case with the {@link org.apache.cassandra.hadoop.ColumnFamilyInputFormat}, 
1:  * you need to set the prepared statement in your
author:Brandon Williams
-------------------------------------------------------------------------------
commit:4cf8a8a
/////////////////////////////////////////////////////////////////////////
/////////////////////////////////////////////////////////////////////////
1:         return new CqlRecordWriter(job, progress);
============================================================================