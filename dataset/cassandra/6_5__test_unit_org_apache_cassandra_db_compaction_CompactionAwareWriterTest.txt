1:910170c: /*
1:910170c:  * Licensed to the Apache Software Foundation (ASF) under one
1:910170c:  * or more contributor license agreements.  See the NOTICE file
1:910170c:  * distributed with this work for additional information
1:910170c:  * regarding copyright ownership.  The ASF licenses this file
1:910170c:  * to you under the Apache License, Version 2.0 (the
1:910170c:  * "License"); you may not use this file except in compliance
1:910170c:  * with the License.  You may obtain a copy of the License at
1:910170c:  *
1:910170c:  *     http://www.apache.org/licenses/LICENSE-2.0
1:910170c:  *
1:910170c:  * Unless required by applicable law or agreed to in writing, software
1:910170c:  * distributed under the License is distributed on an "AS IS" BASIS,
1:910170c:  * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
1:910170c:  * See the License for the specific language governing permissions and
1:910170c:  * limitations under the License.
1:910170c:  */
1:910170c: package org.apache.cassandra.db.compaction;
2:910170c: 
1:e87008b: import java.nio.ByteBuffer;
1:e87008b: import java.util.*;
1:910170c: 
1:910170c: import com.google.common.primitives.Longs;
1:a991b64: import org.junit.*;
1:910170c: 
1:a991b64: import org.apache.cassandra.cql3.CQLTester;
1:a991b64: import org.apache.cassandra.cql3.QueryProcessor;
1:a991b64: import org.apache.cassandra.db.*;
1:910170c: import org.apache.cassandra.db.compaction.writers.CompactionAwareWriter;
1:910170c: import org.apache.cassandra.db.compaction.writers.DefaultCompactionWriter;
1:910170c: import org.apache.cassandra.db.compaction.writers.MajorLeveledCompactionWriter;
1:910170c: import org.apache.cassandra.db.compaction.writers.MaxSSTableSizeWriter;
1:910170c: import org.apache.cassandra.db.compaction.writers.SplittingSizeTieredCompactionWriter;
1:e5a76bd: import org.apache.cassandra.db.lifecycle.LifecycleTransaction;
1:910170c: import org.apache.cassandra.io.sstable.format.SSTableReader;
1:a991b64: import org.apache.cassandra.utils.FBUtilities;
1:a991b64: import org.apache.cassandra.utils.UUIDGen;
1:e5a76bd: 
1:910170c: import static org.junit.Assert.assertEquals;
1:910170c: 
1:a991b64: public class CompactionAwareWriterTest extends CQLTester
3:910170c: {
1:a991b64:     private static final String KEYSPACE = "cawt_keyspace";
1:a991b64:     private static final String TABLE = "cawt_table";
1:a991b64: 
1:a991b64:     private static final int ROW_PER_PARTITION = 10;
1:910170c: 
1:910170c:     @BeforeClass
1:a991b64:     public static void beforeClass() throws Throwable
1:910170c:     {
1:a991b64:         // Disabling durable write since we don't care
1:a991b64:         schemaChange("CREATE KEYSPACE IF NOT EXISTS " + KEYSPACE + " WITH replication = {'class': 'SimpleStrategy', 'replication_factor': '1'} AND durable_writes=false");
1:a991b64:         schemaChange(String.format("CREATE TABLE %s.%s (k int, t int, v blob, PRIMARY KEY (k, t))", KEYSPACE, TABLE));
3:910170c:     }
1:a991b64: 
1:a991b64:     @AfterClass
1:a991b64:     public static void tearDownClass()
1:e87008b:     {
1:a991b64:         QueryProcessor.executeInternal("DROP KEYSPACE IF EXISTS " + KEYSPACE);
1:a991b64:     }
1:a991b64: 
1:a991b64:     private ColumnFamilyStore getColumnFamilyStore()
1:a991b64:     {
1:a991b64:         return Keyspace.open(KEYSPACE).getColumnFamilyStore(TABLE);
1:e87008b:     }
1:e87008b: 
1:910170c:     @Test
1:a991b64:     public void testDefaultCompactionWriter() throws Throwable
1:910170c:     {
1:a991b64:         Keyspace ks = Keyspace.open(KEYSPACE);
1:a991b64:         ColumnFamilyStore cfs = ks.getColumnFamilyStore(TABLE);
1:a991b64: 
1:910170c:         int rowCount = 1000;
1:910170c:         cfs.disableAutoCompaction();
1:a991b64:         populate(rowCount);
1:ad8cad7:         LifecycleTransaction txn = cfs.getTracker().tryModify(cfs.getLiveSSTables(), OperationType.COMPACTION);
1:e5a76bd:         long beforeSize = txn.originals().iterator().next().onDiskLength();
1:9ed2727:         CompactionAwareWriter writer = new DefaultCompactionWriter(cfs, cfs.getDirectories(), txn, txn.originals());
1:e5a76bd:         int rows = compact(cfs, txn, writer);
1:ad8cad7:         assertEquals(1, cfs.getLiveSSTables().size());
1:910170c:         assertEquals(rowCount, rows);
1:ad8cad7:         assertEquals(beforeSize, cfs.getLiveSSTables().iterator().next().onDiskLength());
1:910170c:         validateData(cfs, rowCount);
1:910170c:         cfs.truncateBlocking();
1:910170c:     }
1:a991b64: 
1:910170c:     @Test
1:a991b64:     public void testMaxSSTableSizeWriter() throws Throwable
1:910170c:     {
1:a991b64:         ColumnFamilyStore cfs = getColumnFamilyStore();
1:910170c:         cfs.disableAutoCompaction();
1:910170c:         int rowCount = 1000;
1:a991b64:         populate(rowCount);
1:ad8cad7:         LifecycleTransaction txn = cfs.getTracker().tryModify(cfs.getLiveSSTables(), OperationType.COMPACTION);
1:e5a76bd:         long beforeSize = txn.originals().iterator().next().onDiskLength();
1:e87008b:         int sstableSize = (int)beforeSize/10;
1:9ed2727:         CompactionAwareWriter writer = new MaxSSTableSizeWriter(cfs, cfs.getDirectories(), txn, txn.originals(), sstableSize, 0);
1:e5a76bd:         int rows = compact(cfs, txn, writer);
1:ad8cad7:         assertEquals(10, cfs.getLiveSSTables().size());
1:910170c:         assertEquals(rowCount, rows);
1:910170c:         validateData(cfs, rowCount);
1:910170c:         cfs.truncateBlocking();
1:910170c:     }
1:e87008b: 
1:910170c:     @Test
1:a991b64:     public void testSplittingSizeTieredCompactionWriter() throws Throwable
1:910170c:     {
1:a991b64:         ColumnFamilyStore cfs = getColumnFamilyStore();
1:910170c:         cfs.disableAutoCompaction();
2:910170c:         int rowCount = 10000;
1:a991b64:         populate(rowCount);
1:ad8cad7:         LifecycleTransaction txn = cfs.getTracker().tryModify(cfs.getLiveSSTables(), OperationType.COMPACTION);
1:e5a76bd:         long beforeSize = txn.originals().iterator().next().onDiskLength();
1:9ed2727:         CompactionAwareWriter writer = new SplittingSizeTieredCompactionWriter(cfs, cfs.getDirectories(), txn, txn.originals(), 0);
1:e5a76bd:         int rows = compact(cfs, txn, writer);
1:910170c:         long expectedSize = beforeSize / 2;
1:ad8cad7:         List<SSTableReader> sortedSSTables = new ArrayList<>(cfs.getLiveSSTables());
1:910170c: 
1:910170c:         Collections.sort(sortedSSTables, new Comparator<SSTableReader>()
1:910170c:                                 {
1:910170c:                                     @Override
1:910170c:                                     public int compare(SSTableReader o1, SSTableReader o2)
1:910170c:                                     {
1:910170c:                                         return Longs.compare(o2.onDiskLength(), o1.onDiskLength());
1:910170c:                                     }
1:910170c:                                 });
1:910170c:         for (SSTableReader sstable : sortedSSTables)
1:910170c:         {
1:e87008b:             // we dont create smaller files than this, everything will be in the last file
1:e87008b:             if (expectedSize > SplittingSizeTieredCompactionWriter.DEFAULT_SMALLEST_SSTABLE_BYTES)
1:e87008b:                 assertEquals(expectedSize, sstable.onDiskLength(), expectedSize / 100); // allow 1% diff in estimated vs actual size
1:910170c:             expectedSize /= 2;
1:910170c:         }
1:910170c:         assertEquals(rowCount, rows);
1:910170c:         validateData(cfs, rowCount);
1:910170c:         cfs.truncateBlocking();
1:910170c:     }
1:910170c: 
1:910170c:     @Test
1:a991b64:     public void testMajorLeveledCompactionWriter() throws Throwable
1:910170c:     {
1:a991b64:         ColumnFamilyStore cfs = getColumnFamilyStore();
1:910170c:         cfs.disableAutoCompaction();
1:e87008b:         int rowCount = 20000;
1:e87008b:         int targetSSTableCount = 50;
1:a991b64:         populate(rowCount);
1:ad8cad7:         LifecycleTransaction txn = cfs.getTracker().tryModify(cfs.getLiveSSTables(), OperationType.COMPACTION);
1:e5a76bd:         long beforeSize = txn.originals().iterator().next().onDiskLength();
1:e87008b:         int sstableSize = (int)beforeSize/targetSSTableCount;
1:9ed2727:         CompactionAwareWriter writer = new MajorLeveledCompactionWriter(cfs, cfs.getDirectories(), txn, txn.originals(), sstableSize);
1:e5a76bd:         int rows = compact(cfs, txn, writer);
1:ad8cad7:         assertEquals(targetSSTableCount, cfs.getLiveSSTables().size());
1:910170c:         int [] levelCounts = new int[5];
1:910170c:         assertEquals(rowCount, rows);
1:ad8cad7:         for (SSTableReader sstable : cfs.getLiveSSTables())
1:910170c:         {
1:910170c:             levelCounts[sstable.getSSTableLevel()]++;
1:910170c:         }
1:910170c:         assertEquals(0, levelCounts[0]);
1:910170c:         assertEquals(10, levelCounts[1]);
1:e87008b:         assertEquals(targetSSTableCount - 10, levelCounts[2]); // note that if we want more levels, fix this
1:910170c:         for (int i = 3; i < levelCounts.length; i++)
1:910170c:             assertEquals(0, levelCounts[i]);
1:910170c:         validateData(cfs, rowCount);
1:910170c:         cfs.truncateBlocking();
1:910170c:     }
1:910170c: 
1:e5a76bd:     private int compact(ColumnFamilyStore cfs, LifecycleTransaction txn, CompactionAwareWriter writer)
1:910170c:     {
1:e5a76bd:         assert txn.originals().size() == 1;
1:910170c:         int rowsWritten = 0;
1:a991b64:         int nowInSec = FBUtilities.nowInSeconds();
1:a991b64:         try (AbstractCompactionStrategy.ScannerList scanners = cfs.getCompactionStrategyManager().getScanners(txn.originals());
1:a991b64:              CompactionController controller = new CompactionController(cfs, txn.originals(), cfs.gcBefore(nowInSec));
1:a991b64:              CompactionIterator ci = new CompactionIterator(OperationType.COMPACTION, scanners.scanners, controller, nowInSec, UUIDGen.getTimeUUID()))
1:910170c:         {
1:a991b64:             while (ci.hasNext())
1:910170c:             {
1:a991b64:                 if (writer.append(ci.next()))
1:910170c:                     rowsWritten++;
1:910170c:             }
1:910170c:         }
1:29687a8:         writer.finish();
1:910170c:         return rowsWritten;
1:910170c:     }
1:910170c: 
1:a991b64:     private void populate(int count) throws Throwable
1:910170c:     {
1:29687a8:         byte [] payload = new byte[5000];
1:29687a8:         new Random(42).nextBytes(payload);
1:e87008b:         ByteBuffer b = ByteBuffer.wrap(payload);
1:a991b64: 
1:910170c:         for (int i = 0; i < count; i++)
1:a991b64:             for (int j = 0; j < ROW_PER_PARTITION; j++)
1:a991b64:                 execute(String.format("INSERT INTO %s.%s(k, t, v) VALUES (?, ?, ?)", KEYSPACE, TABLE), i, j, b);
1:a991b64: 
1:a991b64:         ColumnFamilyStore cfs = getColumnFamilyStore();
1:910170c:         cfs.forceBlockingFlush();
1:ad8cad7:         if (cfs.getLiveSSTables().size() > 1)
1:e87008b:         {
1:e87008b:             // we want just one big sstable to avoid doing actual compaction in compact() above
1:e87008b:             try
1:e87008b:             {
1:e87008b:                 cfs.forceMajorCompaction();
1:e87008b:             }
1:e87008b:             catch (Throwable t)
1:e87008b:             {
1:e87008b:                 throw new RuntimeException(t);
1:e87008b:             }
1:e87008b:         }
1:ad8cad7:         assert cfs.getLiveSSTables().size() == 1 : cfs.getLiveSSTables();
1:910170c:     }
1:910170c: 
1:a991b64:     private void validateData(ColumnFamilyStore cfs, int rowCount) throws Throwable
1:910170c:     {
1:910170c:         for (int i = 0; i < rowCount; i++)
1:910170c:         {
1:a991b64:             Object[][] expected = new Object[ROW_PER_PARTITION][];
1:a991b64:             for (int j = 0; j < ROW_PER_PARTITION; j++)
1:a991b64:                 expected[j] = row(i, j);
1:a991b64: 
1:a991b64:             assertRows(execute(String.format("SELECT k, t FROM %s.%s WHERE k = :i", KEYSPACE, TABLE), i), expected);
1:910170c:         }
1:910170c:     }
1:910170c: }
============================================================================
author:Blake Eggleston
-------------------------------------------------------------------------------
commit:9ed2727
/////////////////////////////////////////////////////////////////////////
1:         CompactionAwareWriter writer = new DefaultCompactionWriter(cfs, cfs.getDirectories(), txn, txn.originals());
/////////////////////////////////////////////////////////////////////////
1:         CompactionAwareWriter writer = new MaxSSTableSizeWriter(cfs, cfs.getDirectories(), txn, txn.originals(), sstableSize, 0);
/////////////////////////////////////////////////////////////////////////
1:         CompactionAwareWriter writer = new SplittingSizeTieredCompactionWriter(cfs, cfs.getDirectories(), txn, txn.originals(), 0);
/////////////////////////////////////////////////////////////////////////
1:         CompactionAwareWriter writer = new MajorLeveledCompactionWriter(cfs, cfs.getDirectories(), txn, txn.originals(), sstableSize);
author:Ariel Weisberg
-------------------------------------------------------------------------------
commit:29687a8
/////////////////////////////////////////////////////////////////////////
/////////////////////////////////////////////////////////////////////////
1:         writer.finish();
1:         byte [] payload = new byte[5000];
1:         new Random(42).nextBytes(payload);
author:Stefania Alborghetti
-------------------------------------------------------------------------------
commit:605bcdc
/////////////////////////////////////////////////////////////////////////
0:         CompactionAwareWriter writer = new DefaultCompactionWriter(cfs, txn, txn.originals());
/////////////////////////////////////////////////////////////////////////
0:         CompactionAwareWriter writer = new MaxSSTableSizeWriter(cfs, txn, txn.originals(), sstableSize, 0);
/////////////////////////////////////////////////////////////////////////
0:         CompactionAwareWriter writer = new MajorLeveledCompactionWriter(cfs, txn, txn.originals(), sstableSize);
commit:b09e60f
/////////////////////////////////////////////////////////////////////////
0:         CompactionAwareWriter writer = new DefaultCompactionWriter(cfs, txn, txn.originals(), false);
/////////////////////////////////////////////////////////////////////////
0:         CompactionAwareWriter writer = new MaxSSTableSizeWriter(cfs, txn, txn.originals(), sstableSize, 0, false);
/////////////////////////////////////////////////////////////////////////
0:         CompactionAwareWriter writer = new SplittingSizeTieredCompactionWriter(cfs, txn, txn.originals(), 0);
/////////////////////////////////////////////////////////////////////////
0:         CompactionAwareWriter writer = new MajorLeveledCompactionWriter(cfs, txn, txn.originals(), sstableSize, false);
author:Benedict Elliott Smith
-------------------------------------------------------------------------------
commit:ad8cad7
/////////////////////////////////////////////////////////////////////////
1:         LifecycleTransaction txn = cfs.getTracker().tryModify(cfs.getLiveSSTables(), OperationType.COMPACTION);
1:         assertEquals(1, cfs.getLiveSSTables().size());
1:         assertEquals(beforeSize, cfs.getLiveSSTables().iterator().next().onDiskLength());
/////////////////////////////////////////////////////////////////////////
1:         LifecycleTransaction txn = cfs.getTracker().tryModify(cfs.getLiveSSTables(), OperationType.COMPACTION);
1:         assertEquals(10, cfs.getLiveSSTables().size());
/////////////////////////////////////////////////////////////////////////
1:         LifecycleTransaction txn = cfs.getTracker().tryModify(cfs.getLiveSSTables(), OperationType.COMPACTION);
1:         List<SSTableReader> sortedSSTables = new ArrayList<>(cfs.getLiveSSTables());
/////////////////////////////////////////////////////////////////////////
1:         LifecycleTransaction txn = cfs.getTracker().tryModify(cfs.getLiveSSTables(), OperationType.COMPACTION);
1:         assertEquals(targetSSTableCount, cfs.getLiveSSTables().size());
1:         for (SSTableReader sstable : cfs.getLiveSSTables())
/////////////////////////////////////////////////////////////////////////
1:         if (cfs.getLiveSSTables().size() > 1)
/////////////////////////////////////////////////////////////////////////
1:         assert cfs.getLiveSSTables().size() == 1 : cfs.getLiveSSTables();
commit:e5a76bd
/////////////////////////////////////////////////////////////////////////
/////////////////////////////////////////////////////////////////////////
1: import org.apache.cassandra.db.lifecycle.LifecycleTransaction;
1: 
/////////////////////////////////////////////////////////////////////////
0:         LifecycleTransaction txn = cfs.getTracker().tryModify(cfs.getSSTables(), OperationType.COMPACTION);
1:         long beforeSize = txn.originals().iterator().next().onDiskLength();
0:         CompactionAwareWriter writer = new DefaultCompactionWriter(cfs, txn, txn.originals(), false, OperationType.COMPACTION);
1:         int rows = compact(cfs, txn, writer);
/////////////////////////////////////////////////////////////////////////
0:         LifecycleTransaction txn = cfs.getTracker().tryModify(cfs.getSSTables(), OperationType.COMPACTION);
1:         long beforeSize = txn.originals().iterator().next().onDiskLength();
0:         CompactionAwareWriter writer = new MaxSSTableSizeWriter(cfs, txn, txn.originals(), sstableSize, 0, false, OperationType.COMPACTION);
1:         int rows = compact(cfs, txn, writer);
/////////////////////////////////////////////////////////////////////////
0:         LifecycleTransaction txn = cfs.getTracker().tryModify(cfs.getSSTables(), OperationType.COMPACTION);
1:         long beforeSize = txn.originals().iterator().next().onDiskLength();
0:         CompactionAwareWriter writer = new SplittingSizeTieredCompactionWriter(cfs, txn, txn.originals(), OperationType.COMPACTION, 0);
1:         int rows = compact(cfs, txn, writer);
/////////////////////////////////////////////////////////////////////////
0:         LifecycleTransaction txn = cfs.getTracker().tryModify(cfs.getSSTables(), OperationType.COMPACTION);
1:         long beforeSize = txn.originals().iterator().next().onDiskLength();
0:         CompactionAwareWriter writer = new MajorLeveledCompactionWriter(cfs, txn, txn.originals(), sstableSize, false, OperationType.COMPACTION);
1:         int rows = compact(cfs, txn, writer);
/////////////////////////////////////////////////////////////////////////
1:     private int compact(ColumnFamilyStore cfs, LifecycleTransaction txn, CompactionAwareWriter writer)
1:         assert txn.originals().size() == 1;
0:         try (AbstractCompactionStrategy.ScannerList scanners = cfs.getCompactionStrategy().getScanners(txn.originals()))
0:             CompactionController controller = new CompactionController(cfs, txn.originals(), cfs.gcBefore(System.currentTimeMillis()));
/////////////////////////////////////////////////////////////////////////
author:Sylvain Lebresne
-------------------------------------------------------------------------------
commit:a991b64
/////////////////////////////////////////////////////////////////////////
1: import org.junit.*;
1: import org.apache.cassandra.cql3.CQLTester;
1: import org.apache.cassandra.cql3.QueryProcessor;
1: import org.apache.cassandra.db.*;
1: import org.apache.cassandra.utils.FBUtilities;
1: import org.apache.cassandra.utils.UUIDGen;
1: public class CompactionAwareWriterTest extends CQLTester
1:     private static final String KEYSPACE = "cawt_keyspace";
1:     private static final String TABLE = "cawt_table";
1: 
1:     private static final int ROW_PER_PARTITION = 10;
1:     public static void beforeClass() throws Throwable
1:         // Disabling durable write since we don't care
1:         schemaChange("CREATE KEYSPACE IF NOT EXISTS " + KEYSPACE + " WITH replication = {'class': 'SimpleStrategy', 'replication_factor': '1'} AND durable_writes=false");
1:         schemaChange(String.format("CREATE TABLE %s.%s (k int, t int, v blob, PRIMARY KEY (k, t))", KEYSPACE, TABLE));
1:     @AfterClass
1:     public static void tearDownClass()
1:         QueryProcessor.executeInternal("DROP KEYSPACE IF EXISTS " + KEYSPACE);
1:     }
1: 
1:     private ColumnFamilyStore getColumnFamilyStore()
1:     {
1:         return Keyspace.open(KEYSPACE).getColumnFamilyStore(TABLE);
1:     public void testDefaultCompactionWriter() throws Throwable
1:         Keyspace ks = Keyspace.open(KEYSPACE);
1:         ColumnFamilyStore cfs = ks.getColumnFamilyStore(TABLE);
1: 
1:         populate(rowCount);
/////////////////////////////////////////////////////////////////////////
1:     public void testMaxSSTableSizeWriter() throws Throwable
1:         ColumnFamilyStore cfs = getColumnFamilyStore();
1:         populate(rowCount);
/////////////////////////////////////////////////////////////////////////
1: 
1:     public void testSplittingSizeTieredCompactionWriter() throws Throwable
1:         ColumnFamilyStore cfs = getColumnFamilyStore();
1:         populate(rowCount);
/////////////////////////////////////////////////////////////////////////
1:     public void testMajorLeveledCompactionWriter() throws Throwable
1:         ColumnFamilyStore cfs = getColumnFamilyStore();
1:         populate(rowCount);
/////////////////////////////////////////////////////////////////////////
1:         int nowInSec = FBUtilities.nowInSeconds();
1:         try (AbstractCompactionStrategy.ScannerList scanners = cfs.getCompactionStrategyManager().getScanners(txn.originals());
1:              CompactionController controller = new CompactionController(cfs, txn.originals(), cfs.gcBefore(nowInSec));
1:              CompactionIterator ci = new CompactionIterator(OperationType.COMPACTION, scanners.scanners, controller, nowInSec, UUIDGen.getTimeUUID()))
1:             while (ci.hasNext())
1:                 if (writer.append(ci.next()))
/////////////////////////////////////////////////////////////////////////
1:     private void populate(int count) throws Throwable
1: 
1:             for (int j = 0; j < ROW_PER_PARTITION; j++)
1:                 execute(String.format("INSERT INTO %s.%s(k, t, v) VALUES (?, ?, ?)", KEYSPACE, TABLE), i, j, b);
1: 
1:         ColumnFamilyStore cfs = getColumnFamilyStore();
/////////////////////////////////////////////////////////////////////////
1: 
1:     private void validateData(ColumnFamilyStore cfs, int rowCount) throws Throwable
1:             Object[][] expected = new Object[ROW_PER_PARTITION][];
1:             for (int j = 0; j < ROW_PER_PARTITION; j++)
1:                 expected[j] = row(i, j);
1: 
1:             assertRows(execute(String.format("SELECT k, t FROM %s.%s WHERE k = :i", KEYSPACE, TABLE), i), expected);
author:Marcus Eriksson
-------------------------------------------------------------------------------
commit:7df3a5c
/////////////////////////////////////////////////////////////////////////
0:         try (AbstractCompactionStrategy.ScannerList scanners = cfs.getCompactionStrategyManager().getScanners(txn.originals()))
commit:e87008b
/////////////////////////////////////////////////////////////////////////
1: import java.nio.ByteBuffer;
1: import java.util.*;
0: import java.util.concurrent.ExecutionException;
0: import org.junit.Before;
/////////////////////////////////////////////////////////////////////////
1: 
0:     @Before
0:     public void clear()
1:     {
0:         // avoid one test affecting the next one
0:         Keyspace ks = Keyspace.open(KEYSPACE1);
0:         ColumnFamilyStore cfs = ks.getColumnFamilyStore(CF);
0:         cfs.clearUnsafe();
1:     }
1: 
/////////////////////////////////////////////////////////////////////////
1:         int sstableSize = (int)beforeSize/10;
0:         CompactionAwareWriter writer = new MaxSSTableSizeWriter(cfs, sstables, sstables, sstableSize, 0, false, OperationType.COMPACTION);
/////////////////////////////////////////////////////////////////////////
1:             // we dont create smaller files than this, everything will be in the last file
1:             if (expectedSize > SplittingSizeTieredCompactionWriter.DEFAULT_SMALLEST_SSTABLE_BYTES)
1:                 assertEquals(expectedSize, sstable.onDiskLength(), expectedSize / 100); // allow 1% diff in estimated vs actual size
/////////////////////////////////////////////////////////////////////////
1:         int rowCount = 20000;
1:         int targetSSTableCount = 50;
1:         int sstableSize = (int)beforeSize/targetSSTableCount;
0:         CompactionAwareWriter writer = new MajorLeveledCompactionWriter(cfs, sstables, sstables, sstableSize, false, OperationType.COMPACTION);
0:         assertEquals(targetSSTableCount, cfs.getSSTables().size());
/////////////////////////////////////////////////////////////////////////
1:         assertEquals(targetSSTableCount - 10, levelCounts[2]); // note that if we want more levels, fix this
/////////////////////////////////////////////////////////////////////////
0:         byte [] payload = new byte[1000];
0:         new Random().nextBytes(payload);
1:         ByteBuffer b = ByteBuffer.wrap(payload);
0:                         b,
0:         if (cfs.getSSTables().size() > 1)
1:         {
1:             // we want just one big sstable to avoid doing actual compaction in compact() above
1:             try
1:             {
1:                 cfs.forceMajorCompaction();
1:             }
1:             catch (Throwable t)
1:             {
1:                 throw new RuntimeException(t);
1:             }
1:         }
commit:910170c
/////////////////////////////////////////////////////////////////////////
1: /*
1:  * Licensed to the Apache Software Foundation (ASF) under one
1:  * or more contributor license agreements.  See the NOTICE file
1:  * distributed with this work for additional information
1:  * regarding copyright ownership.  The ASF licenses this file
1:  * to you under the Apache License, Version 2.0 (the
1:  * "License"); you may not use this file except in compliance
1:  * with the License.  You may obtain a copy of the License at
1:  *
1:  *     http://www.apache.org/licenses/LICENSE-2.0
1:  *
1:  * Unless required by applicable law or agreed to in writing, software
1:  * distributed under the License is distributed on an "AS IS" BASIS,
1:  * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
1:  * See the License for the specific language governing permissions and
1:  * limitations under the License.
1:  */
1: package org.apache.cassandra.db.compaction;
1: 
0: import java.util.ArrayList;
0: import java.util.Arrays;
0: import java.util.Collection;
0: import java.util.Collections;
0: import java.util.Comparator;
0: import java.util.HashSet;
0: import java.util.Iterator;
0: import java.util.List;
0: import java.util.Set;
1: 
1: import com.google.common.primitives.Longs;
0: import org.junit.BeforeClass;
0: import org.junit.Test;
1: 
0: import org.apache.cassandra.SchemaLoader;
0: import org.apache.cassandra.Util;
0: import org.apache.cassandra.config.KSMetaData;
0: import org.apache.cassandra.db.Cell;
0: import org.apache.cassandra.db.ColumnFamily;
0: import org.apache.cassandra.db.ColumnFamilyStore;
0: import org.apache.cassandra.db.DecoratedKey;
0: import org.apache.cassandra.db.Keyspace;
0: import org.apache.cassandra.db.Mutation;
1: import org.apache.cassandra.db.compaction.writers.CompactionAwareWriter;
1: import org.apache.cassandra.db.compaction.writers.DefaultCompactionWriter;
1: import org.apache.cassandra.db.compaction.writers.MajorLeveledCompactionWriter;
1: import org.apache.cassandra.db.compaction.writers.MaxSSTableSizeWriter;
1: import org.apache.cassandra.db.compaction.writers.SplittingSizeTieredCompactionWriter;
0: import org.apache.cassandra.db.filter.QueryFilter;
0: import org.apache.cassandra.exceptions.ConfigurationException;
0: import org.apache.cassandra.io.sstable.ISSTableScanner;
1: import org.apache.cassandra.io.sstable.format.SSTableReader;
0: import org.apache.cassandra.locator.SimpleStrategy;
0: import org.apache.cassandra.utils.ByteBufferUtil;
1: import static org.junit.Assert.assertEquals;
1: 
0: public class CompactionAwareWriterTest
1: {
0:     private static String KEYSPACE1 = "CompactionAwareWriterTest";
0:     private static String CF = "Standard1";
1: 
1:     @BeforeClass
0:     public static void defineSchema() throws ConfigurationException
1:     {
0:         SchemaLoader.prepareServer();
0:         SchemaLoader.createKeyspace(KEYSPACE1,
0:                                     SimpleStrategy.class,
0:                                     KSMetaData.optsWithRF(1),
0:                                     SchemaLoader.standardCFMD(KEYSPACE1, CF));
1: 
1:     }
1:     @Test
0:     public void testDefaultCompactionWriter()
1:     {
0:         Keyspace ks = Keyspace.open(KEYSPACE1);
0:         ColumnFamilyStore cfs = ks.getColumnFamilyStore(CF);
1:         int rowCount = 1000;
1:         cfs.disableAutoCompaction();
0:         populate(cfs, rowCount);
0:         Set<SSTableReader> sstables = new HashSet<>(cfs.getSSTables());
0:         long beforeSize = sstables.iterator().next().onDiskLength();
0:         CompactionAwareWriter writer = new DefaultCompactionWriter(cfs, sstables, sstables, false, OperationType.COMPACTION);
0:         int rows = compact(cfs, sstables, writer);
0:         assertEquals(1, cfs.getSSTables().size());
1:         assertEquals(rowCount, rows);
0:         assertEquals(beforeSize, cfs.getSSTables().iterator().next().onDiskLength());
1:         validateData(cfs, rowCount);
1:         cfs.truncateBlocking();
1:     }
1: 
1:     @Test
0:     public void testMaxSSTableSizeWriter()
1:     {
0:         Keyspace ks = Keyspace.open(KEYSPACE1);
0:         ColumnFamilyStore cfs = ks.getColumnFamilyStore(CF);
1:         cfs.disableAutoCompaction();
1:         int rowCount = 1000;
0:         populate(cfs, rowCount);
0:         Set<SSTableReader> sstables = new HashSet<>(cfs.getSSTables());
0:         long beforeSize = sstables.iterator().next().onDiskLength();
0:         int sstableCount = (int)beforeSize/10;
0:         CompactionAwareWriter writer = new MaxSSTableSizeWriter(cfs, sstables, sstables, sstableCount, 0, false, OperationType.COMPACTION);
0:         int rows = compact(cfs, sstables, writer);
0:         assertEquals(10, cfs.getSSTables().size());
1:         assertEquals(rowCount, rows);
1:         validateData(cfs, rowCount);
1:         cfs.truncateBlocking();
1:     }
1:     @Test
0:     public void testSplittingSizeTieredCompactionWriter()
1:     {
0:         Keyspace ks = Keyspace.open(KEYSPACE1);
0:         ColumnFamilyStore cfs = ks.getColumnFamilyStore(CF);
1:         cfs.disableAutoCompaction();
1:         int rowCount = 10000;
0:         populate(cfs, rowCount);
0:         Set<SSTableReader> sstables = new HashSet<>(cfs.getSSTables());
0:         long beforeSize = sstables.iterator().next().onDiskLength();
0:         CompactionAwareWriter writer = new SplittingSizeTieredCompactionWriter(cfs, sstables, sstables, OperationType.COMPACTION, 0);
0:         int rows = compact(cfs, sstables, writer);
1:         long expectedSize = beforeSize / 2;
0:         List<SSTableReader> sortedSSTables = new ArrayList<>(cfs.getSSTables());
1: 
1:         Collections.sort(sortedSSTables, new Comparator<SSTableReader>()
1:                                 {
1:                                     @Override
1:                                     public int compare(SSTableReader o1, SSTableReader o2)
1:                                     {
1:                                         return Longs.compare(o2.onDiskLength(), o1.onDiskLength());
1:                                     }
1:                                 });
1:         for (SSTableReader sstable : sortedSSTables)
1:         {
0:             assertEquals(expectedSize, sstable.onDiskLength(), 10000);
1:             expectedSize /= 2;
1:         }
1:         assertEquals(rowCount, rows);
1:         validateData(cfs, rowCount);
1:         cfs.truncateBlocking();
1:     }
1: 
1:     @Test
0:     public void testMajorLeveledCompactionWriter()
1:     {
0:         Keyspace ks = Keyspace.open(KEYSPACE1);
0:         ColumnFamilyStore cfs = ks.getColumnFamilyStore(CF);
1:         cfs.disableAutoCompaction();
1:         int rowCount = 10000;
0:         populate(cfs, rowCount);
0:         Set<SSTableReader> sstables = new HashSet<>(cfs.getSSTables());
0:         long beforeSize = sstables.iterator().next().onDiskLength();
0:         int sstableCount = (int)beforeSize/100;
0:         CompactionAwareWriter writer = new MajorLeveledCompactionWriter(cfs, sstables, sstables, sstableCount, false, OperationType.COMPACTION);
0:         int rows = compact(cfs, sstables, writer);
0:         assertEquals(100, cfs.getSSTables().size());
1:         int [] levelCounts = new int[5];
1:         assertEquals(rowCount, rows);
0:         for (SSTableReader sstable : cfs.getSSTables())
1:         {
1:             levelCounts[sstable.getSSTableLevel()]++;
1:         }
1:         assertEquals(0, levelCounts[0]);
1:         assertEquals(10, levelCounts[1]);
0:         assertEquals(90, levelCounts[2]);
1:         for (int i = 3; i < levelCounts.length; i++)
1:             assertEquals(0, levelCounts[i]);
1:         validateData(cfs, rowCount);
1:         cfs.truncateBlocking();
1:     }
1: 
0:     private int compact(ColumnFamilyStore cfs, Set<SSTableReader> sstables, CompactionAwareWriter writer)
1:     {
0:         assert sstables.size() == 1;
1:         int rowsWritten = 0;
0:         try (AbstractCompactionStrategy.ScannerList scanners = cfs.getCompactionStrategy().getScanners(sstables))
1:         {
0:             CompactionController controller = new CompactionController(cfs, sstables, cfs.gcBefore(System.currentTimeMillis()));
0:             ISSTableScanner scanner = scanners.scanners.get(0);
0:             while(scanner.hasNext())
1:             {
0:                 AbstractCompactedRow row = new LazilyCompactedRow(controller, Arrays.asList(scanner.next()));
0:                 if (writer.append(row))
1:                     rowsWritten++;
1:             }
1:         }
0:         Collection<SSTableReader> newSSTables = writer.finish();
0:         cfs.getDataTracker().markCompactedSSTablesReplaced(sstables, newSSTables, OperationType.COMPACTION);
1:         return rowsWritten;
1:     }
1: 
0:     private void populate(ColumnFamilyStore cfs, int count)
1:     {
0:         long timestamp = System.currentTimeMillis();
1:         for (int i = 0; i < count; i++)
1:         {
0:             DecoratedKey key = Util.dk(Integer.toString(i));
0:             Mutation rm = new Mutation(KEYSPACE1, key.getKey());
0:             for (int j = 0; j < 10; j++)
0:                 rm.add(CF,  Util.cellname(Integer.toString(j)),
0:                         ByteBufferUtil.EMPTY_BYTE_BUFFER,
0:                         timestamp);
0:             rm.applyUnsafe();
1:         }
1:         cfs.forceBlockingFlush();
0:         assert cfs.getSSTables().size() == 1 : cfs.getSSTables();
1:     }
0:     private void validateData(ColumnFamilyStore cfs, int rowCount)
1:     {
1:         for (int i = 0; i < rowCount; i++)
1:         {
0:             ColumnFamily cf = cfs.getTopLevelColumns(QueryFilter.getIdentityFilter(Util.dk(Integer.toString(i)), CF, System.currentTimeMillis()), Integer.MAX_VALUE);
0:             Iterator<Cell> iter = cf.iterator();
0:             int cellCount = 0;
0:             while (iter.hasNext())
1:             {
0:                 Cell c = iter.next();
0:                 assertEquals(Util.cellname(Integer.toString(cellCount)), c.name());
0:                 cellCount++;
1:             }
0:             assertEquals(10, cellCount);
1:         }
1:     }
1: }
============================================================================