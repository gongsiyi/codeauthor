1:07cf56f: /*
1:1ecabe6:  * Licensed to the Apache Software Foundation (ASF) under one
1:1ecabe6:  * or more contributor license agreements.  See the NOTICE file
1:1ecabe6:  * distributed with this work for additional information
1:1ecabe6:  * regarding copyright ownership.  The ASF licenses this file
1:1ecabe6:  * to you under the Apache License, Version 2.0 (the
1:1ecabe6:  * "License"); you may not use this file except in compliance
1:1ecabe6:  * with the License.  You may obtain a copy of the License at
2:1ecabe6:  *
1:07cf56f:  *     http://www.apache.org/licenses/LICENSE-2.0
1:1ecabe6:  *
1:07cf56f:  * Unless required by applicable law or agreed to in writing, software
1:07cf56f:  * distributed under the License is distributed on an "AS IS" BASIS,
1:07cf56f:  * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
1:07cf56f:  * See the License for the specific language governing permissions and
1:07cf56f:  * limitations under the License.
1:1ecabe6:  */
1:1ecabe6: package org.apache.cassandra.io.compress;
1:3679b1b: 
1:fccc123: import java.io.BufferedOutputStream;
1:4e95953: import java.io.DataInput;
1:4e95953: import java.io.DataInputStream;
1:4e95953: import java.io.DataOutput;
1:4e95953: import java.io.DataOutputStream;
1:4e95953: import java.io.EOFException;
1:4e95953: import java.io.File;
1:4e95953: import java.io.FileInputStream;
1:4e95953: import java.io.FileNotFoundException;
1:4e95953: import java.io.FileOutputStream;
1:4e95953: import java.io.IOException;
1:4e95953: import java.util.Collection;
1:4e95953: import java.util.Comparator;
1:4e95953: import java.util.HashMap;
1:4e95953: import java.util.Map;
1:4e95953: import java.util.SortedSet;
1:4e95953: import java.util.TreeSet;
1:21aef92: 
1:4549a98: import com.google.common.annotations.VisibleForTesting;
1:8704006: import com.google.common.base.Throwables;
1:21aef92: import com.google.common.primitives.Longs;
1:3679b1b: 
1:21aef92: import org.apache.cassandra.db.TypeSizes;
1:4e95953: import org.apache.cassandra.exceptions.ConfigurationException;
1:debb15e: import org.apache.cassandra.io.FSReadError;
1:debb15e: import org.apache.cassandra.io.FSWriteError;
1:21aef92: import org.apache.cassandra.io.IVersionedSerializer;
1:3494894: import org.apache.cassandra.io.sstable.Component;
1:debb15e: import org.apache.cassandra.io.sstable.CorruptSSTableException;
1:3494894: import org.apache.cassandra.io.sstable.Descriptor;
1:03f72ac: import org.apache.cassandra.io.util.DataInputPlus;
1:75508ec: import org.apache.cassandra.io.util.DataOutputPlus;
1:4549a98: import org.apache.cassandra.io.util.Memory;
1:746c458: import org.apache.cassandra.io.util.SafeMemory;
1:b31845c: import org.apache.cassandra.schema.CompressionParams;
1:5baf28d: import org.apache.cassandra.utils.ChecksumType;
1:21aef92: import org.apache.cassandra.utils.Pair;
1:8704006: import org.apache.cassandra.utils.concurrent.Transactional;
1:a7f4134: import org.apache.cassandra.utils.concurrent.Ref;
19:1ecabe6: 
1:1ecabe6: /**
1:1ecabe6:  * Holds metadata about compressed file
1:1ecabe6:  */
1:1ecabe6: public class CompressionMetadata
1:debb15e: {
1:a1e2978:     // dataLength can represent either the true length of the file
1:a1e2978:     // or some shorter value, in the case we want to impose a shorter limit on readers
1:a1e2978:     // (when early opening, we want to ensure readers cannot read past fully written sections)
1:1ecabe6:     public final long dataLength;
1:1ecabe6:     public final long compressedFileLength;
1:4549a98:     private final Memory chunkOffsets;
1:4e95953:     private final long chunkOffsetsSize;
1:1ecabe6:     public final String indexFilePath;
1:b31845c:     public final CompressionParams parameters;
1:5baf28d:     public final ChecksumType checksumType;
1:4e95953: 
1:1ecabe6:     /**
1:981ee77:      * Create metadata about given compressed file including uncompressed data length, chunk size
1:3494894:      * and list of the chunk offsets of the compressed data.
1:5baf28d:      *
1:981ee77:      * This is an expensive operation! Don't create more than one for each
1:981ee77:      * sstable.
1:981ee77:      *
1:3494894:      * @param dataFilePath Path to the compressed file
1:1ecabe6:      *
1:3494894:      * @return metadata about given compressed file.
2:3494894:      */
1:981ee77:     public static CompressionMetadata create(String dataFilePath)
1:4e95953:     {
1:3494894:         Descriptor desc = Descriptor.fromFilename(dataFilePath);
1:5baf28d:         return new CompressionMetadata(desc.filenameFor(Component.COMPRESSION_INFO), new File(dataFilePath).length(), desc.version.compressedChecksumType());
1:61384c5:     }
1:bc7941c: 
1:4549a98:     @VisibleForTesting
1:ce63ccc:     public CompressionMetadata(String indexFilePath, long compressedLength, ChecksumType checksumType)
1:21aef92:     {
1:1ecabe6:         this.indexFilePath = indexFilePath;
1:5baf28d:         this.checksumType = checksumType;
1:a1e2978: 
1:7aafe05:         try (DataInputStream stream = new DataInputStream(new FileInputStream(indexFilePath)))
1:a1e2978:         {
1:debb15e:             String compressorName = stream.readUTF();
1:debb15e:             int optionCount = stream.readInt();
1:f096eb6:             Map<String, String> options = new HashMap<>(optionCount);
1:debb15e:             for (int i = 0; i < optionCount; ++i)
1:a1e2978:             {
1:debb15e:                 String key = stream.readUTF();
1:debb15e:                 String value = stream.readUTF();
1:debb15e:                 options.put(key, value);
1:a1e2978:             }
1:debb15e:             int chunkLength = stream.readInt();
1:fccc123:             try
1:fccc123:             {
1:b31845c:                 parameters = new CompressionParams(compressorName, chunkLength, options);
1:fccc123:             }
1:debb15e:             catch (ConfigurationException e)
1:fccc123:             {
1:b31845c:                 throw new RuntimeException("Cannot create CompressionParams for stored parameters", e);
1:fccc123:             }
1:7e056fa: 
1:debb15e:             dataLength = stream.readLong();
1:debb15e:             compressedFileLength = compressedLength;
1:debb15e:             chunkOffsets = readChunkOffsets(stream);
1:a1e2978:         }
1:7aafe05:         catch (FileNotFoundException e)
1:0ced7a3:         {
1:7aafe05:             throw new RuntimeException(e);
1:0ced7a3:         }
1:debb15e:         catch (IOException e)
1:0ced7a3:         {
1:debb15e:             throw new CorruptSSTableException(e, indexFilePath);
1:0ced7a3:         }
1:0ced7a3: 
1:4e95953:         this.chunkOffsetsSize = chunkOffsets.size();
1:0ced7a3:     }
1:0ced7a3: 
1:5baf28d:     private CompressionMetadata(String filePath, CompressionParams parameters, SafeMemory offsets, long offsetsSize, long dataLength, long compressedLength, ChecksumType checksumType)
1:0ced7a3:     {
1:4e95953:         this.indexFilePath = filePath;
1:4e95953:         this.parameters = parameters;
1:4e95953:         this.dataLength = dataLength;
1:4e95953:         this.compressedFileLength = compressedLength;
1:4e95953:         this.chunkOffsets = offsets;
1:4e95953:         this.chunkOffsetsSize = offsetsSize;
1:5baf28d:         this.checksumType = checksumType;
1:3679b1b:     }
1:0ced7a3: 
1:c8afd76:     public ICompressor compressor()
1:61384c5:     {
1:056115f:         return parameters.getSstableCompressor();
1:3679b1b:     }
1:3679b1b: 
1:c8afd76:     public int chunkLength()
1:3679b1b:     {
1:3e77792:         return parameters.chunkLength();
1:3679b1b:     }
1:981ee77: 
1:1ecabe6:     /**
1:7a14a77:      * Returns the amount of memory in bytes used off heap.
1:7a14a77:      * @return the amount of memory in bytes used off heap
1:7a14a77:      */
1:7a14a77:     public long offHeapSize()
1:3679b1b:     {
1:7a14a77:         return chunkOffsets.size();
1:871f003:     }
1:7a14a77: 
1:a7f4134:     public void addTo(Ref.IdentityCollection identities)
1:a7f4134:     {
1:a7f4134:         identities.add(chunkOffsets);
1:a7f4134:     }
1:a7f4134: 
1:7a14a77:     /**
1:1ecabe6:      * Read offsets of the individual chunks from the given input.
1:1ecabe6:      *
1:1ecabe6:      * @param input Source of the data.
1:1ecabe6:      *
1:1ecabe6:      * @return collection of the chunk offsets.
1:1ecabe6:      */
1:4549a98:     private Memory readChunkOffsets(DataInput input)
1:3679b1b:     {
1:7aafe05:         final int chunkCount;
1:8704006:         try
1:871f003:         {
1:7aafe05:             chunkCount = input.readInt();
1:905273e:             if (chunkCount <= 0)
1:905273e:                 throw new IOException("Compressed file with 0 chunks encountered: " + input);
1:7a14a77:         }
1:7aafe05:         catch (IOException e)
1:7a14a77:         {
1:7aafe05:             throw new FSReadError(e, indexFilePath);
1:7aafe05:         }
1:905273e: 
1:7aafe05:         @SuppressWarnings("resource")
1:9499f7c:         Memory offsets = Memory.allocate(chunkCount * 8L);
1:7aafe05:         int i = 0;
1:3679b1b:         try
1:7aafe05:         {
1:1ecabe6: 
1:7aafe05:             for (i = 0; i < chunkCount; i++)
1:7aafe05:             {
1:5a09483:                 offsets.setLong(i * 8L, input.readLong());
1:7aafe05:             }
1:1ecabe6: 
1:debb15e:             return offsets;
1:7aafe05:         }
3:debb15e:         catch (IOException e)
1:7aafe05:         {
1:a1e2978:             if (offsets != null)
1:a1e2978:                 offsets.close();
1:7aafe05: 
1:7aafe05:             if (e instanceof EOFException)
1:7aafe05:             {
1:7aafe05:                 String msg = String.format("Corrupted Index File %s: read %d but expected %d chunks.",
1:7aafe05:                                            indexFilePath, i, chunkCount);
1:7aafe05:                 throw new CorruptSSTableException(new IOException(msg, e), indexFilePath);
1:7aafe05:             }
1:debb15e:             throw new FSReadError(e, indexFilePath);
1:4e95953:         }
1:4e95953:     }
1:debb15e: 
1:1ecabe6:     /**
1:1ecabe6:      * Get a chunk of compressed data (offset, length) corresponding to given position
1:1ecabe6:      *
1:1ecabe6:      * @param position Position in the file.
1:1ecabe6:      * @return pair of chunk offset and length.
1:1ecabe6:      */
1:debb15e:     public Chunk chunkFor(long position)
1:4e95953:     {
1:1ecabe6:         // position of the chunk
1:4549a98:         int idx = 8 * (int) (position / parameters.chunkLength());
1:1ecabe6: 
1:4e95953:         if (idx >= chunkOffsetsSize)
1:debb15e:             throw new CorruptSSTableException(new EOFException(), indexFilePath);
1:1ecabe6: 
1:4549a98:         long chunkOffset = chunkOffsets.getLong(idx);
1:871f003:         long nextChunkOffset = (idx + 8 == chunkOffsetsSize)
1:de6260d:                                 ? compressedFileLength
1:4549a98:                                 : chunkOffsets.getLong(idx + 8);
1:1ecabe6: 
1:ba1821f:         return new Chunk(chunkOffset, (int) (nextChunkOffset - chunkOffset - 4)); // "4" bytes reserved for checksum
1:debb15e:     }
1:1ecabe6: 
1:21aef92:     /**
1:8385bb6:      * @param sections Collection of sections in uncompressed file. Should not contain sections that overlap each other.
1:8385bb6:      * @return Total chunk size in bytes for given sections including checksum.
1:8385bb6:      */
1:8385bb6:     public long getTotalSizeForSections(Collection<Pair<Long, Long>> sections)
1:8385bb6:     {
1:8385bb6:         long size = 0;
1:8385bb6:         long lastOffset = -1;
1:8385bb6:         for (Pair<Long, Long> section : sections)
1:8385bb6:         {
1:8385bb6:             int startIndex = (int) (section.left / parameters.chunkLength());
1:8385bb6:             int endIndex = (int) (section.right / parameters.chunkLength());
1:8385bb6:             endIndex = section.right % parameters.chunkLength() == 0 ? endIndex - 1 : endIndex;
1:8385bb6:             for (int i = startIndex; i <= endIndex; i++)
1:8385bb6:             {
1:9499f7c:                 long offset = i * 8L;
1:8385bb6:                 long chunkOffset = chunkOffsets.getLong(offset);
1:8385bb6:                 if (chunkOffset > lastOffset)
1:8385bb6:                 {
1:8385bb6:                     lastOffset = chunkOffset;
1:8385bb6:                     long nextChunkOffset = offset + 8 == chunkOffsetsSize
1:8385bb6:                                                    ? compressedFileLength
1:8385bb6:                                                    : chunkOffsets.getLong(offset + 8);
1:8385bb6:                     size += (nextChunkOffset - chunkOffset);
1:8385bb6:                 }
1:8385bb6:             }
1:8385bb6:         }
1:8385bb6:         return size;
1:8385bb6:     }
1:8385bb6: 
1:8385bb6:     /**
1:21aef92:      * @param sections Collection of sections in uncompressed file
1:21aef92:      * @return Array of chunks which corresponds to given sections of uncompressed file, sorted by chunk offset
1:21aef92:      */
1:21aef92:     public Chunk[] getChunksForSections(Collection<Pair<Long, Long>> sections)
1:21aef92:     {
1:21aef92:         // use SortedSet to eliminate duplicates and sort by chunk offset
1:21aef92:         SortedSet<Chunk> offsets = new TreeSet<Chunk>(new Comparator<Chunk>()
1:21aef92:         {
1:21aef92:             public int compare(Chunk o1, Chunk o2)
1:21aef92:             {
1:21aef92:                 return Longs.compare(o1.offset, o2.offset);
1:21aef92:             }
1:21aef92:         });
1:21aef92:         for (Pair<Long, Long> section : sections)
1:21aef92:         {
1:21aef92:             int startIndex = (int) (section.left / parameters.chunkLength());
1:21aef92:             int endIndex = (int) (section.right / parameters.chunkLength());
1:ee0be06:             endIndex = section.right % parameters.chunkLength() == 0 ? endIndex - 1 : endIndex;
1:21aef92:             for (int i = startIndex; i <= endIndex; i++)
1:debb15e:             {
1:8385bb6:                 long offset = i * 8L;
1:4549a98:                 long chunkOffset = chunkOffsets.getLong(offset);
1:4e95953:                 long nextChunkOffset = offset + 8 == chunkOffsetsSize
1:21aef92:                                      ? compressedFileLength
1:de6260d:                                      : chunkOffsets.getLong(offset + 8);
1:21aef92:                 offsets.add(new Chunk(chunkOffset, (int) (nextChunkOffset - chunkOffset - 4))); // "4" bytes reserved for checksum
1:21aef92:             }
1:21aef92:         }
1:21aef92:         return offsets.toArray(new Chunk[offsets.size()]);
1:21aef92:     }
1:21aef92: 
1:4549a98:     public void close()
1:4549a98:     {
1:746c458:         chunkOffsets.close();
1:4549a98:     }
1:4549a98: 
1:8704006:     public static class Writer extends Transactional.AbstractTransactional implements Transactional
1:debb15e:     {
1:debb15e:         // path to the file
1:b31845c:         private final CompressionParams parameters;
1:debb15e:         private final String filePath;
1:4e95953:         private int maxCount = 100;
1:9499f7c:         private SafeMemory offsets = new SafeMemory(maxCount * 8L);
1:4e95953:         private int count = 0;
1:0368e97: 
1:8704006:         // provided by user when setDescriptor
1:8704006:         private long dataLength, chunkCount;
1:1ecabe6: 
1:b31845c:         private Writer(CompressionParams parameters, String path)
1:debb15e:         {
1:4e95953:             this.parameters = parameters;
1:debb15e:             filePath = path;
1:debb15e:         }
1:debb15e: 
1:b31845c:         public static Writer open(CompressionParams parameters, String path)
1:debb15e:         {
1:4e95953:             return new Writer(parameters, path);
1:debb15e:         }
1:debb15e: 
1:4e95953:         public void addOffset(long offset)
1:4e95953:         {
1:4e95953:             if (count == maxCount)
1:4e95953:             {
1:5a09483:                 SafeMemory newOffsets = offsets.copy((maxCount *= 2L) * 8L);
1:746c458:                 offsets.close();
1:4e95953:                 offsets = newOffsets;
1:4e95953:             }
1:9499f7c:             offsets.setLong(8L * count++, offset);
1:4e95953:         }
1:4e95953: 
1:4e95953:         private void writeHeader(DataOutput out, long dataLength, int chunks)
1:debb15e:         {
1:7aafe05:             try
1:debb15e:             {
1:056115f:                 out.writeUTF(parameters.getSstableCompressor().getClass().getSimpleName());
1:056115f:                 out.writeInt(parameters.getOtherOptions().size());
1:056115f:                 for (Map.Entry<String, String> entry : parameters.getOtherOptions().entrySet())
1:debb15e:                 {
1:4e95953:                     out.writeUTF(entry.getKey());
1:4e95953:                     out.writeUTF(entry.getValue());
1:debb15e:                 }
1:1ecabe6: 
1:debb15e:                 // store the length of the chunk
1:4e95953:                 out.writeInt(parameters.chunkLength());
1:debb15e:                 // store position and reserve a place for uncompressed data length and chunks count
1:4e95953:                 out.writeLong(dataLength);
1:4e95953:                 out.writeInt(chunks);
1:debb15e:             }
1:debb15e:             catch (IOException e)
1:debb15e:             {
2:debb15e:                 throw new FSWriteError(e, filePath);
1:ecbf0fd:             }
1:debb15e:         }
1:ecbf0fd: 
1:8704006:         // we've written everything; wire up some final metadata state
1:8704006:         public Writer finalizeLength(long dataLength, int chunkCount)
1:ecbf0fd:         {
1:8704006:             this.dataLength = dataLength;
1:8704006:             this.chunkCount = chunkCount;
1:8704006:             return this;
1:debb15e:         }
1:debb15e: 
1:8704006:         public void doPrepare()
1:debb15e:         {
1:8704006:             assert chunkCount == count;
1:8704006: 
1:8704006:             // finalize the size of memory used if it won't now change;
1:8704006:             // unnecessary if already correct size
1:8704006:             if (offsets.size() != count * 8L)
1:8704006:             {
1:8704006:                 SafeMemory tmp = offsets;
1:8704006:                 offsets = offsets.copy(count * 8L);
1:8704006:                 tmp.free();
1:8704006:             }
1:8704006: 
1:8704006:             // flush the data to disk
1:73a730f:             try (FileOutputStream fos = new FileOutputStream(filePath);
1:73a730f:                  DataOutputStream out = new DataOutputStream(new BufferedOutputStream(fos)))
1:8704006:             {
1:8704006:                 writeHeader(out, dataLength, count);
1:73a730f:                 for (int i = 0; i < count; i++)
1:9499f7c:                     out.writeLong(offsets.getLong(i * 8L));
1:73a730f: 
1:73a730f:                 out.flush();
1:73a730f:                 fos.getFD().sync();
1:8704006:             }
1:8704006:             catch (IOException e)
1:8704006:             {
1:8704006:                 throw Throwables.propagate(e);
1:8704006:             }
1:8704006:         }
1:8704006: 
1:7aafe05:         @SuppressWarnings("resource")
1:8704006:         public CompressionMetadata open(long dataLength, long compressedLength)
1:8704006:         {
1:8704006:             SafeMemory offsets = this.offsets.sharedCopy();
1:8704006: 
1:8704006:             // calculate how many entries we need, if our dataLength is truncated
1:8704006:             int count = (int) (dataLength / parameters.chunkLength());
1:8704006:             if (dataLength % parameters.chunkLength() != 0)
1:8704006:                 count++;
1:8704006: 
1:8704006:             assert count > 0;
1:8704006:             // grab our actual compressed length from the next offset from our the position we're opened to
1:8704006:             if (count < this.count)
1:9499f7c:                 compressedLength = offsets.getLong(count * 8L);
1:8704006: 
1:5baf28d:             return new CompressionMetadata(filePath, parameters, offsets, count * 8L, dataLength, compressedLength, ChecksumType.CRC32);
1:debb15e:         }
1:0ced7a3: 
1:1ecabe6:         /**
1:1ecabe6:          * Get a chunk offset by it's index.
1:1ecabe6:          *
1:1ecabe6:          * @param chunkIndex Index of the chunk.
1:1ecabe6:          *
1:1ecabe6:          * @return offset of the chunk in the compressed file.
1:1ecabe6:          */
1:debb15e:         public long chunkOffsetBy(int chunkIndex)
3:debb15e:         {
1:fccc123:             return offsets.getLong(chunkIndex * 8L);
1:debb15e:         }
1:debb15e: 
1:1ecabe6:         /**
1:1ecabe6:          * Reset the writer so that the next chunk offset written will be the
1:1ecabe6:          * one of {@code chunkIndex}.
1:f44110c:          *
1:f44110c:          * @param chunkIndex the next index to write
1:1ecabe6:          */
1:debb15e:         public void resetAndTruncate(int chunkIndex)
1:debb15e:         {
1:4e95953:             count = chunkIndex;
1:debb15e:         }
1:debb15e: 
1:c163d0b:         protected Throwable doPostCleanup(Throwable failed)
1:debb15e:         {
1:8704006:             return offsets.close(failed);
1:debb15e:         }
1:1ecabe6: 
1:8704006:         protected Throwable doCommit(Throwable accumulate)
1:debb15e:         {
1:8704006:             return accumulate;
1:8704006:         }
1:8704006: 
1:8704006:         protected Throwable doAbort(Throwable accumulate)
1:8704006:         {
1:c163d0b:             return accumulate;
4:debb15e:         }
1:debb15e:     }
1:1ecabe6: 
1:1ecabe6:     /**
1:1ecabe6:      * Holds offset and length of the file chunk
1:1ecabe6:      */
1:6ea00a3:     public static class Chunk
1:debb15e:     {
1:21aef92:         public static final IVersionedSerializer<Chunk> serializer = new ChunkSerializer();
1:21aef92: 
1:1ecabe6:         public final long offset;
1:1ecabe6:         public final int length;
1:1ecabe6: 
1:1ecabe6:         public Chunk(long offset, int length)
1:debb15e:         {
1:905273e:             assert(length > 0);
1:905273e: 
1:1ecabe6:             this.offset = offset;
1:1ecabe6:             this.length = length;
1:debb15e:         }
1:1ecabe6: 
1:21aef92:         public boolean equals(Object o)
1:21aef92:         {
1:21aef92:             if (this == o) return true;
1:21aef92:             if (o == null || getClass() != o.getClass()) return false;
1:21aef92: 
1:21aef92:             Chunk chunk = (Chunk) o;
1:21aef92:             return length == chunk.length && offset == chunk.offset;
1:21aef92:         }
1:21aef92: 
1:21aef92:         public int hashCode()
1:21aef92:         {
1:21aef92:             int result = (int) (offset ^ (offset >>> 32));
1:21aef92:             result = 31 * result + length;
1:21aef92:             return result;
1:21aef92:         }
1:21aef92: 
1:ba1821f:         public String toString()
1:debb15e:         {
1:ba1821f:             return String.format("Chunk<offset: %d, length: %d>", offset, length);
1:debb15e:         }
1:debb15e:     }
1:21aef92: 
1:21aef92:     static class ChunkSerializer implements IVersionedSerializer<Chunk>
1:21aef92:     {
1:75508ec:         public void serialize(Chunk chunk, DataOutputPlus out, int version) throws IOException
1:21aef92:         {
1:21aef92:             out.writeLong(chunk.offset);
1:21aef92:             out.writeInt(chunk.length);
1:21aef92:         }
1:21aef92: 
1:03f72ac:         public Chunk deserialize(DataInputPlus in, int version) throws IOException
1:21aef92:         {
1:21aef92:             return new Chunk(in.readLong(), in.readInt());
1:21aef92:         }
1:21aef92: 
1:21aef92:         public long serializedSize(Chunk chunk, int version)
1:21aef92:         {
1:03f72ac:             long size = TypeSizes.sizeof(chunk.offset);
1:03f72ac:             size += TypeSizes.sizeof(chunk.length);
1:21aef92:             return size;
1:21aef92:         }
1:21aef92:     }
1:debb15e: }
============================================================================
author:Dave Brosius
-------------------------------------------------------------------------------
commit:087264f
/////////////////////////////////////////////////////////////////////////
commit:5a09483
/////////////////////////////////////////////////////////////////////////
1:                     offsets.setLong(i * 8L, input.readLong());
/////////////////////////////////////////////////////////////////////////
1:                 SafeMemory newOffsets = offsets.copy((maxCount *= 2L) * 8L);
commit:dd825a5
commit:9499f7c
/////////////////////////////////////////////////////////////////////////
/////////////////////////////////////////////////////////////////////////
/////////////////////////////////////////////////////////////////////////
1:             Memory offsets = Memory.allocate(chunkCount * 8L);
/////////////////////////////////////////////////////////////////////////
1:                 long offset = i * 8L;
/////////////////////////////////////////////////////////////////////////
1:         private SafeMemory offsets = new SafeMemory(maxCount * 8L);
/////////////////////////////////////////////////////////////////////////
0:                 SafeMemory newOffsets = offsets.copy((maxCount *= 2L) * 8);
1:             offsets.setLong(8L * count++, offset);
/////////////////////////////////////////////////////////////////////////
1:                         compressedLength = offsets.getLong(count * 8L);
/////////////////////////////////////////////////////////////////////////
1:                     out.writeLong(offsets.getLong(i * 8L));
commit:f096eb6
/////////////////////////////////////////////////////////////////////////
1:             Map<String, String> options = new HashMap<>(optionCount);
commit:f44110c
/////////////////////////////////////////////////////////////////////////
1:          * 
1:          * @param chunkIndex the next index to write
commit:1def02f
/////////////////////////////////////////////////////////////////////////
commit:fccc123
/////////////////////////////////////////////////////////////////////////
1: import java.io.BufferedOutputStream;
/////////////////////////////////////////////////////////////////////////
0: import org.apache.cassandra.utils.FBUtilities;
/////////////////////////////////////////////////////////////////////////
1:             return offsets.getLong(chunkIndex * 8L);
/////////////////////////////////////////////////////////////////////////
0:             DataOutputStream out = null;
1:             try
1:             {
0:             	out = new DataOutputStream(new BufferedOutputStream(new FileOutputStream(filePath)));
0: 	            assert chunks == count;
0: 	            writeHeader(out, dataLength, chunks);
0: 	            for (int i = 0 ; i < count ; i++)
0: 	                out.writeLong(offsets.getLong(i * 8));
1:             }
0:             finally
1:             {
0:                 FileUtils.closeQuietly(out);
1:             }
author:Yuki Morishita
-------------------------------------------------------------------------------
commit:5cc02dd
commit:ae64cc0
commit:8385bb6
/////////////////////////////////////////////////////////////////////////
1:      * @param sections Collection of sections in uncompressed file. Should not contain sections that overlap each other.
1:      * @return Total chunk size in bytes for given sections including checksum.
1:      */
1:     public long getTotalSizeForSections(Collection<Pair<Long, Long>> sections)
1:     {
1:         long size = 0;
1:         long lastOffset = -1;
1:         for (Pair<Long, Long> section : sections)
1:         {
1:             int startIndex = (int) (section.left / parameters.chunkLength());
1:             int endIndex = (int) (section.right / parameters.chunkLength());
1:             endIndex = section.right % parameters.chunkLength() == 0 ? endIndex - 1 : endIndex;
1:             for (int i = startIndex; i <= endIndex; i++)
1:             {
1:                 long offset = i * 8L;
1:                 long chunkOffset = chunkOffsets.getLong(offset);
1:                 if (chunkOffset > lastOffset)
1:                 {
1:                     lastOffset = chunkOffset;
1:                     long nextChunkOffset = offset + 8 == chunkOffsetsSize
1:                                                    ? compressedFileLength
1:                                                    : chunkOffsets.getLong(offset + 8);
1:                     size += (nextChunkOffset - chunkOffset);
1:                 }
1:             }
1:         }
1:         return size;
1:     }
1: 
1:     /**
commit:35648e8
/////////////////////////////////////////////////////////////////////////
commit:ee0be06
/////////////////////////////////////////////////////////////////////////
1:             endIndex = section.right % parameters.chunkLength() == 0 ? endIndex - 1 : endIndex;
commit:21aef92
/////////////////////////////////////////////////////////////////////////
0: import java.util.*;
1: 
1: import com.google.common.primitives.Longs;
1: import org.apache.cassandra.db.TypeSizes;
1: import org.apache.cassandra.io.IVersionedSerializer;
1: import org.apache.cassandra.utils.Pair;
/////////////////////////////////////////////////////////////////////////
1:     /**
1:      * @param sections Collection of sections in uncompressed file
1:      * @return Array of chunks which corresponds to given sections of uncompressed file, sorted by chunk offset
1:      */
1:     public Chunk[] getChunksForSections(Collection<Pair<Long, Long>> sections)
1:     {
1:         // use SortedSet to eliminate duplicates and sort by chunk offset
1:         SortedSet<Chunk> offsets = new TreeSet<Chunk>(new Comparator<Chunk>()
1:         {
1:             public int compare(Chunk o1, Chunk o2)
1:             {
1:                 return Longs.compare(o1.offset, o2.offset);
1:             }
1:         });
1:         for (Pair<Long, Long> section : sections)
1:         {
1:             int startIndex = (int) (section.left / parameters.chunkLength());
1:             int endIndex = (int) (section.right / parameters.chunkLength());
1:             for (int i = startIndex; i <= endIndex; i++)
1:             {
0:                 long chunkOffset = chunkOffsets.get(i);
0:                 long nextChunkOffset = (i + 1 == chunkOffsets.size)
1:                                                ? compressedFileLength
0:                                                : chunkOffsets.get(i + 1);
1:                 offsets.add(new Chunk(chunkOffset, (int) (nextChunkOffset - chunkOffset - 4))); // "4" bytes reserved for checksum
1:             }
1:         }
1:         return offsets.toArray(new Chunk[offsets.size()]);
1:     }
1: 
/////////////////////////////////////////////////////////////////////////
1:         public static final IVersionedSerializer<Chunk> serializer = new ChunkSerializer();
1: 
/////////////////////////////////////////////////////////////////////////
1:         public boolean equals(Object o)
1:         {
1:             if (this == o) return true;
1:             if (o == null || getClass() != o.getClass()) return false;
1: 
1:             Chunk chunk = (Chunk) o;
1:             return length == chunk.length && offset == chunk.offset;
1:         }
1: 
1:         public int hashCode()
1:         {
1:             int result = (int) (offset ^ (offset >>> 32));
1:             result = 31 * result + length;
1:             return result;
1:         }
1: 
1: 
1:     static class ChunkSerializer implements IVersionedSerializer<Chunk>
1:     {
0:         public void serialize(Chunk chunk, DataOutput out, int version) throws IOException
1:         {
1:             out.writeLong(chunk.offset);
1:             out.writeInt(chunk.length);
1:         }
1: 
0:         public Chunk deserialize(DataInput in, int version) throws IOException
1:         {
1:             return new Chunk(in.readLong(), in.readInt());
1:         }
1: 
1:         public long serializedSize(Chunk chunk, int version)
1:         {
0:             long size = TypeSizes.NATIVE.sizeof(chunk.offset);
0:             size += TypeSizes.NATIVE.sizeof(chunk.length);
1:             return size;
1:         }
1:     }
author:Joshua McKenzie
-------------------------------------------------------------------------------
commit:cb102da
commit:bc7941c
/////////////////////////////////////////////////////////////////////////
1: 
commit:ac27409
commit:905273e
/////////////////////////////////////////////////////////////////////////
1:             if (chunkCount <= 0)
1:                 throw new IOException("Compressed file with 0 chunks encountered: " + input);
1: 
/////////////////////////////////////////////////////////////////////////
1:             assert(length > 0);
1: 
author:Stefania Alborghetti
-------------------------------------------------------------------------------
commit:73a730f
/////////////////////////////////////////////////////////////////////////
1:             try (FileOutputStream fos = new FileOutputStream(filePath);
1:                  DataOutputStream out = new DataOutputStream(new BufferedOutputStream(fos)))
1:                 for (int i = 0; i < count; i++)
1: 
1:                 out.flush();
1:                 fos.getFD().sync();
commit:7e056fa
/////////////////////////////////////////////////////////////////////////
0:             FileOutputStream fos = null;
0:                 fos = new FileOutputStream(filePath);
0:                 out = new DataOutputStream(new BufferedOutputStream(fos));
0:                 assert chunks == count;
0:                 writeHeader(out, dataLength, chunks);
1: 
0:                 out.flush();
0:                 fos.getFD().sync();
commit:c163d0b
/////////////////////////////////////////////////////////////////////////
1:         protected Throwable doPostCleanup(Throwable failed)
/////////////////////////////////////////////////////////////////////////
1:             return accumulate;
commit:ce63ccc
/////////////////////////////////////////////////////////////////////////
1:     public CompressionMetadata(String indexFilePath, long compressedLength, ChecksumType checksumType)
author:Ariel Weisberg
-------------------------------------------------------------------------------
commit:5baf28d
/////////////////////////////////////////////////////////////////////////
1: import org.apache.cassandra.utils.ChecksumType;
/////////////////////////////////////////////////////////////////////////
1:     public final ChecksumType checksumType;
/////////////////////////////////////////////////////////////////////////
1:         return new CompressionMetadata(desc.filenameFor(Component.COMPRESSION_INFO), new File(dataFilePath).length(), desc.version.compressedChecksumType());
0:     CompressionMetadata(String indexFilePath, long compressedLength, ChecksumType checksumType)
1:         this.checksumType = checksumType;
/////////////////////////////////////////////////////////////////////////
1:     private CompressionMetadata(String filePath, CompressionParams parameters, SafeMemory offsets, long offsetsSize, long dataLength, long compressedLength, ChecksumType checksumType)
/////////////////////////////////////////////////////////////////////////
1:         this.checksumType = checksumType;
/////////////////////////////////////////////////////////////////////////
1:             return new CompressionMetadata(filePath, parameters, offsets, count * 8L, dataLength, compressedLength, ChecksumType.CRC32);
/////////////////////////////////////////////////////////////////////////
1:          *
commit:03f72ac
/////////////////////////////////////////////////////////////////////////
1: import org.apache.cassandra.io.util.DataInputPlus;
/////////////////////////////////////////////////////////////////////////
1:         public Chunk deserialize(DataInputPlus in, int version) throws IOException
1:             long size = TypeSizes.sizeof(chunk.offset);
1:             size += TypeSizes.sizeof(chunk.length);
author:Aleksey Yeschenko
-------------------------------------------------------------------------------
commit:b31845c
/////////////////////////////////////////////////////////////////////////
1: import org.apache.cassandra.schema.CompressionParams;
/////////////////////////////////////////////////////////////////////////
1:     public final CompressionParams parameters;
/////////////////////////////////////////////////////////////////////////
1:                 parameters = new CompressionParams(compressorName, chunkLength, options);
1:                 throw new RuntimeException("Cannot create CompressionParams for stored parameters", e);
/////////////////////////////////////////////////////////////////////////
0:     private CompressionMetadata(String filePath, CompressionParams parameters, SafeMemory offsets, long offsetsSize, long dataLength, long compressedLength)
/////////////////////////////////////////////////////////////////////////
1:         private final CompressionParams parameters;
/////////////////////////////////////////////////////////////////////////
1:         private Writer(CompressionParams parameters, String path)
1:         public static Writer open(CompressionParams parameters, String path)
commit:371ad9e
commit:9aaea24
author:Benedict Elliott Smith
-------------------------------------------------------------------------------
commit:a7f4134
/////////////////////////////////////////////////////////////////////////
1: import org.apache.cassandra.utils.concurrent.Ref;
/////////////////////////////////////////////////////////////////////////
1:     public void addTo(Ref.IdentityCollection identities)
1:     {
1:         identities.add(chunkOffsets);
1:     }
1: 
commit:e5a76bd
/////////////////////////////////////////////////////////////////////////
0:         protected Throwable doPreCleanup(Throwable failed)
commit:8704006
/////////////////////////////////////////////////////////////////////////
1: import com.google.common.base.Throwables;
/////////////////////////////////////////////////////////////////////////
1: import org.apache.cassandra.utils.concurrent.Transactional;
/////////////////////////////////////////////////////////////////////////
1:     public static class Writer extends Transactional.AbstractTransactional implements Transactional
/////////////////////////////////////////////////////////////////////////
1:         // provided by user when setDescriptor
1:         private long dataLength, chunkCount;
/////////////////////////////////////////////////////////////////////////
1:         // we've written everything; wire up some final metadata state
1:         public Writer finalizeLength(long dataLength, int chunkCount)
1:             this.dataLength = dataLength;
1:             this.chunkCount = chunkCount;
1:             return this;
1:         public void doPrepare()
1:             assert chunkCount == count;
1: 
1:             // finalize the size of memory used if it won't now change;
1:             // unnecessary if already correct size
1:             if (offsets.size() != count * 8L)
1:                 SafeMemory tmp = offsets;
1:                 offsets = offsets.copy(count * 8L);
1:                 tmp.free();
1: 
1:             // flush the data to disk
0:             DataOutputStream out = null;
1:             try
1:             {
0:                 out = new DataOutputStream(new BufferedOutputStream(new FileOutputStream(filePath)));
1:                 writeHeader(out, dataLength, count);
0:                 for (int i = 0 ; i < count ; i++)
0:                     out.writeLong(offsets.getLong(i * 8L));
1:             }
1:             catch (IOException e)
1:             {
1:                 throw Throwables.propagate(e);
1:             }
0:             finally
1:             {
0:                 FileUtils.closeQuietly(out);
1:             }
1:         }
1: 
1:         public CompressionMetadata open(long dataLength, long compressedLength)
1:         {
1:             SafeMemory offsets = this.offsets.sharedCopy();
1: 
1:             // calculate how many entries we need, if our dataLength is truncated
1:             int count = (int) (dataLength / parameters.chunkLength());
1:             if (dataLength % parameters.chunkLength() != 0)
1:                 count++;
1: 
1:             assert count > 0;
1:             // grab our actual compressed length from the next offset from our the position we're opened to
1:             if (count < this.count)
0:                 compressedLength = offsets.getLong(count * 8L);
1: 
/////////////////////////////////////////////////////////////////////////
0:         protected Throwable doCleanup(Throwable failed)
1:             return offsets.close(failed);
1:         protected Throwable doCommit(Throwable accumulate)
1:             return accumulate;
1:         }
1: 
1:         protected Throwable doAbort(Throwable accumulate)
1:         {
0:             return FileUtils.deleteWithConfirm(filePath, false, accumulate);
commit:36bd31d
commit:a1e2978
/////////////////////////////////////////////////////////////////////////
1:     // dataLength can represent either the true length of the file
1:     // or some shorter value, in the case we want to impose a shorter limit on readers
1:     // (when early opening, we want to ensure readers cannot read past fully written sections)
/////////////////////////////////////////////////////////////////////////
0:             SafeMemory offsets;
0:                         // finalize the size of memory used if it won't now change;
0:                         // unnecessary if already correct size
0:                         SafeMemory tmp = this.offsets.copy(count * 8L);
0:                         this.offsets = tmp;
1: 
1:                     {
0:                         offsets = this.offsets.sharedCopy();
1:                     }
0:                     else
1:                     {
0:                         offsets = this.offsets;
0:                         // null out our reference to the original shared data to catch accidental reuse
0:                         // note that since noone is writing to this Writer while we open it, null:ing out this.offsets is safe
0:                         this.offsets = null;
1:                     }
0:                     offsets = this.offsets.sharedCopy();
0:                     if (dataLength % parameters.chunkLength() != 0)
0:                         count++;
/////////////////////////////////////////////////////////////////////////
1:             if (offsets != null)
1:                 offsets.close();
commit:a4d0758
commit:746c458
/////////////////////////////////////////////////////////////////////////
1: import org.apache.cassandra.io.util.SafeMemory;
/////////////////////////////////////////////////////////////////////////
0:     private CompressionMetadata(String filePath, CompressionParameters parameters, SafeMemory offsets, long offsetsSize, long dataLength, long compressedLength, boolean hasPostCompressionAdlerChecksums)
/////////////////////////////////////////////////////////////////////////
/////////////////////////////////////////////////////////////////////////
1:         chunkOffsets.close();
/////////////////////////////////////////////////////////////////////////
0:         private SafeMemory offsets = new SafeMemory(maxCount * 8);
/////////////////////////////////////////////////////////////////////////
0:                 SafeMemory newOffsets = offsets.copy((maxCount *= 2) * 8);
1:                 offsets.close();
/////////////////////////////////////////////////////////////////////////
0:             SafeMemory offsets = this.offsets;
/////////////////////////////////////////////////////////////////////////
0:                         this.offsets.free();
0:                         this.offsets = offsets.sharedCopy();
/////////////////////////////////////////////////////////////////////////
0:             offsets.close();
commit:b2123db
commit:107a794
/////////////////////////////////////////////////////////////////////////
0:                     // note that since noone is writing to this Writer while we open it, null:ing out this.offsets is safe
commit:36729b9
commit:0ced7a3
/////////////////////////////////////////////////////////////////////////
0:         static enum OpenType
0:             // i.e. FinishType == EARLY; we will use the RefCountedMemory in possibly multiple instances
0:             SHARED,
0:             // i.e. FinishType == EARLY, but the sstable has been completely written, so we can
0:             // finalise the contents and size of the memory, but must retain a reference to it
0:             SHARED_FINAL,
0:             // i.e. FinishType == NORMAL or FINISH_EARLY, i.e. we have actually finished writing the table
0:             // and will never need to open the metadata again, so we can release any references to it here
0:             FINAL
1:         }
1: 
0:         public CompressionMetadata open(long dataLength, long compressedLength, OpenType type)
1:         {
0:             RefCountedMemory offsets = this.offsets;
0:             int count = this.count;
0:             switch (type)
0:                 case FINAL: case SHARED_FINAL:
0:                     // maybe resize the data
0:                     if (this.offsets.size() != count * 8L)
1:                     {
0:                         offsets = this.offsets.copy(count * 8L);
0:                         // release our reference to the original shared data;
0:                         // we don't do this if not resizing since we must pass out existing
0:                         // reference onto our caller
0:                         this.offsets.unreference();
1:                     }
0:                     // null out our reference to the original shared data to catch accidental reuse
0:                     this.offsets = null;
0:                     if (type == OpenType.SHARED_FINAL)
1:                     {
0:                         // we will use the data again, so stash our resized data back, and take an extra reference to it
0:                         this.offsets = offsets;
0:                         this.offsets.reference();
1:                     }
0:                     break;
1: 
0:                 case SHARED:
1: 
0:                     // we should only be opened on a compression data boundary; truncate our size to this boundary
0:                     assert dataLength % parameters.chunkLength() == 0;
0:                     count = (int) (dataLength / parameters.chunkLength());
0:                     // grab our actual compressed length from the next offset from our the position we're opened to
0:                     if (count < this.count)
0:                         compressedLength = offsets.getLong(count * 8);
0:                     break;
1: 
0:                 default:
0:                     throw new AssertionError();
commit:02c3489
commit:61384c5
/////////////////////////////////////////////////////////////////////////
0:             if (finishType.isFinal)
0:                 // we now know how many offsets we have and can resize the offsets properly
0:                 offsets = this.offsets.copy(count * 8L);
0:                 this.offsets.unreference();
1:             }
0:             else
1:             {
0:                 offsets = this.offsets;
commit:729ebe0
commit:55750e0
commit:3679b1b
/////////////////////////////////////////////////////////////////////////
0: import org.slf4j.Logger;
0: import org.slf4j.LoggerFactory;
1: 
/////////////////////////////////////////////////////////////////////////
0:     private static final Logger logger = LoggerFactory.getLogger(CompressionMetadata.class);
1: 
/////////////////////////////////////////////////////////////////////////
1: 
0:         public void abort()
1:         {
1:             try
1:             {
0:                 super.close();
1:             }
0:             catch (Throwable t)
1:             {
0:                 logger.warn("Suppressed exception while closing CompressionMetadata.Writer for {}", filePath, t);
1:             }
1:         }
commit:9a51c3c
commit:871f003
/////////////////////////////////////////////////////////////////////////
0: import org.apache.cassandra.io.sstable.SSTableWriter;
/////////////////////////////////////////////////////////////////////////
1:         long nextChunkOffset = (idx + 8 == chunkOffsetsSize)
/////////////////////////////////////////////////////////////////////////
0:         public CompressionMetadata open(long dataLength, long compressedLength, SSTableWriter.FinishType finishType)
0:             RefCountedMemory offsets;
0:             switch (finishType)
1:             {
0:                 case EARLY:
0:                     offsets = this.offsets;
0:                     break;
0:                 case NORMAL:
0:                 case FINISH_EARLY:
0:                     offsets = this.offsets.copy(count * 8L);
0:                     this.offsets.unreference();
0:                     break;
0:                 default:
0:                     throw new AssertionError();
1:             }
/////////////////////////////////////////////////////////////////////////
0:                 for (int i = 0 ; i < count ; i++)
0:                     out.writeLong(offsets.getLong(i * 8));
author:blerer
-------------------------------------------------------------------------------
commit:056115f
/////////////////////////////////////////////////////////////////////////
1:         return parameters.getSstableCompressor();
/////////////////////////////////////////////////////////////////////////
1:                 out.writeUTF(parameters.getSstableCompressor().getClass().getSimpleName());
1:                 out.writeInt(parameters.getOtherOptions().size());
1:                 for (Map.Entry<String, String> entry : parameters.getOtherOptions().entrySet())
commit:7a14a77
/////////////////////////////////////////////////////////////////////////
1:      * Returns the amount of memory in bytes used off heap.
1:      * @return the amount of memory in bytes used off heap
1:      */
1:     public long offHeapSize()
1:     {
1:         return chunkOffsets.size();
1:     }
1: 
1:     /**
author:T Jake Luciani
-------------------------------------------------------------------------------
commit:7aafe05
/////////////////////////////////////////////////////////////////////////
1:         try (DataInputStream stream = new DataInputStream(new FileInputStream(indexFilePath)))
/////////////////////////////////////////////////////////////////////////
1:         catch (FileNotFoundException e)
1:         {
1:             throw new RuntimeException(e);
1:         }
/////////////////////////////////////////////////////////////////////////
1:         final int chunkCount;
1:             chunkCount = input.readInt();
1:         }
1:         catch (IOException e)
1:         {
1:             throw new FSReadError(e, indexFilePath);
1:         }
1:         @SuppressWarnings("resource")
0:         Memory offsets = Memory.allocate(chunkCount * 8L);
1:         int i = 0;
1:         try
1:         {
1:             for (i = 0; i < chunkCount; i++)
0:                 offsets.setLong(i * 8L, input.readLong());
0:             if (offsets != null)
0:                 offsets.close();
1: 
1:             if (e instanceof EOFException)
1:             {
1:                 String msg = String.format("Corrupted Index File %s: read %d but expected %d chunks.",
1:                                            indexFilePath, i, chunkCount);
1:                 throw new CorruptSSTableException(new IOException(msg, e), indexFilePath);
1:             }
/////////////////////////////////////////////////////////////////////////
0:             try (DataOutputStream out = new DataOutputStream(new BufferedOutputStream(new FileOutputStream(filePath))))
/////////////////////////////////////////////////////////////////////////
1:         @SuppressWarnings("resource")
commit:895ec3e
/////////////////////////////////////////////////////////////////////////
/////////////////////////////////////////////////////////////////////////
0:         return new CompressionMetadata(desc.filenameFor(Component.COMPRESSION_INFO), new File(dataFilePath).length());
0:     CompressionMetadata(String indexFilePath, long compressedLength)
/////////////////////////////////////////////////////////////////////////
0:     private CompressionMetadata(String filePath, CompressionParameters parameters, RefCountedMemory offsets, long offsetsSize, long dataLength, long compressedLength)
/////////////////////////////////////////////////////////////////////////
0:             return new CompressionMetadata(filePath, parameters, offsets, count * 8L, dataLength, compressedLength);
author:Jake Luciani
-------------------------------------------------------------------------------
commit:0368e97
/////////////////////////////////////////////////////////////////////////
0: import org.apache.cassandra.config.DatabaseDescriptor;
/////////////////////////////////////////////////////////////////////////
0: import org.apache.cassandra.io.sstable.format.Version;
0: import org.hsqldb.Database;
/////////////////////////////////////////////////////////////////////////
0:         return new CompressionMetadata(desc.filenameFor(Component.COMPRESSION_INFO), new File(dataFilePath).length(), desc.version.hasPostCompressionAdlerChecksums());
/////////////////////////////////////////////////////////////////////////
0:         private Version latestVersion =  DatabaseDescriptor.getSSTableFormat().info.getLatestVersion();
1: 
/////////////////////////////////////////////////////////////////////////
0:             return new CompressionMetadata(filePath, parameters, offsets, count * 8L, dataLength, compressedLength, latestVersion.hasPostCompressionAdlerChecksums());
0:             return new CompressionMetadata(filePath, parameters, newOffsets, count * 8L, dataLength, compressedLength, latestVersion.hasPostCompressionAdlerChecksums());
commit:815b238
/////////////////////////////////////////////////////////////////////////
0:     public final boolean hasPostCompressionAdlerChecksums;
/////////////////////////////////////////////////////////////////////////
0:         return new CompressionMetadata(desc.filenameFor(Component.COMPRESSION_INFO), new File(dataFilePath).length(), desc.version.hasPostCompressionAdlerChecksums);
0:     CompressionMetadata(String indexFilePath, long compressedLength, boolean hasPostCompressionAdlerChecksums)
0:         this.hasPostCompressionAdlerChecksums = hasPostCompressionAdlerChecksums;
author:belliottsmith
-------------------------------------------------------------------------------
commit:4e95953
/////////////////////////////////////////////////////////////////////////
1: import java.io.DataInput;
1: import java.io.DataInputStream;
1: import java.io.DataOutput;
1: import java.io.DataOutputStream;
1: import java.io.EOFException;
1: import java.io.File;
1: import java.io.FileInputStream;
1: import java.io.FileNotFoundException;
1: import java.io.FileOutputStream;
1: import java.io.IOException;
1: import java.util.Collection;
1: import java.util.Comparator;
1: import java.util.HashMap;
1: import java.util.Map;
1: import java.util.SortedSet;
1: import java.util.TreeSet;
0: import org.apache.cassandra.cache.RefCountedMemory;
1: import org.apache.cassandra.exceptions.ConfigurationException;
/////////////////////////////////////////////////////////////////////////
1:     private final long chunkOffsetsSize;
/////////////////////////////////////////////////////////////////////////
0:             Map<String, String> options = new HashMap<>();
/////////////////////////////////////////////////////////////////////////
1:         this.chunkOffsetsSize = chunkOffsets.size();
1:     }
1: 
0:     private CompressionMetadata(String filePath, CompressionParameters parameters, RefCountedMemory offsets, long offsetsSize, long dataLength, long compressedLength, boolean hasPostCompressionAdlerChecksums)
1:     {
1:         this.indexFilePath = filePath;
1:         this.parameters = parameters;
1:         this.dataLength = dataLength;
1:         this.compressedFileLength = compressedLength;
0:         this.hasPostCompressionAdlerChecksums = hasPostCompressionAdlerChecksums;
1:         this.chunkOffsets = offsets;
0:         offsets.reference();
1:         this.chunkOffsetsSize = offsetsSize;
/////////////////////////////////////////////////////////////////////////
1:         if (idx >= chunkOffsetsSize)
/////////////////////////////////////////////////////////////////////////
1:                 long nextChunkOffset = offset + 8 == chunkOffsetsSize
/////////////////////////////////////////////////////////////////////////
0:         if (chunkOffsets instanceof RefCountedMemory)
0:             ((RefCountedMemory) chunkOffsets).unreference();
0:         else
0:             chunkOffsets.free();
0:     public static class Writer
0:         private final CompressionParameters parameters;
1:         private int maxCount = 100;
0:         private RefCountedMemory offsets = new RefCountedMemory(maxCount * 8);
1:         private int count = 0;
0:         private Writer(CompressionParameters parameters, String path)
1:             this.parameters = parameters;
0:         public static Writer open(CompressionParameters parameters, String path)
1:             return new Writer(parameters, path);
1:         public void addOffset(long offset)
1:         {
1:             if (count == maxCount)
1:             {
0:                 RefCountedMemory newOffsets = offsets.copy((maxCount *= 2) * 8);
0:                 offsets.unreference();
1:                 offsets = newOffsets;
1:             }
0:             offsets.setLong(8 * count++, offset);
1:         }
1: 
1:         private void writeHeader(DataOutput out, long dataLength, int chunks)
0:                 out.writeUTF(parameters.sstableCompressor.getClass().getSimpleName());
0:                 out.writeInt(parameters.otherOptions.size());
1:                     out.writeUTF(entry.getKey());
1:                     out.writeUTF(entry.getValue());
1:                 out.writeInt(parameters.chunkLength());
1:                 out.writeLong(dataLength);
1:                 out.writeInt(chunks);
/////////////////////////////////////////////////////////////////////////
0:         public CompressionMetadata openEarly(long dataLength, long compressedLength)
0:             return new CompressionMetadata(filePath, parameters, offsets, count * 8L, dataLength, compressedLength, Descriptor.Version.CURRENT.hasPostCompressionAdlerChecksums);
1:         }
0:         public CompressionMetadata openAfterClose(long dataLength, long compressedLength)
1:         {
0:             RefCountedMemory newOffsets = offsets.copy(count * 8L);
0:             offsets.unreference();
0:             return new CompressionMetadata(filePath, parameters, newOffsets, count * 8L, dataLength, compressedLength, Descriptor.Version.CURRENT.hasPostCompressionAdlerChecksums);
/////////////////////////////////////////////////////////////////////////
0:             return offsets.getLong(chunkIndex * 8);
/////////////////////////////////////////////////////////////////////////
1:             count = chunkIndex;
0:         public void close(long dataLength, int chunks) throws IOException
0:             final DataOutputStream out = new DataOutputStream(new FileOutputStream(filePath));
0:             assert chunks == count;
0:             writeHeader(out, dataLength, chunks);
0:             for (int i = 0 ; i < count ; i++)
0:                 out.writeLong(offsets.getLong(i * 8));
0:             out.close();
commit:75508ec
/////////////////////////////////////////////////////////////////////////
1: import org.apache.cassandra.io.util.DataOutputPlus;
/////////////////////////////////////////////////////////////////////////
1:         public void serialize(Chunk chunk, DataOutputPlus out, int version) throws IOException
author:Jonathan Ellis
-------------------------------------------------------------------------------
commit:bbc0891
commit:3eef540
/////////////////////////////////////////////////////////////////////////
0:             if (getChannel().isOpen()) // if RAF.closed were public we could just use that, but it's not
0:                 getChannel().force(true);
commit:6a211f4
commit:ecbf0fd
/////////////////////////////////////////////////////////////////////////
/////////////////////////////////////////////////////////////////////////
1: 
0:         public void close() throws IOException
1:         {
0:             getFD().sync();
0:             super.close();
1:         }
commit:de6260d
/////////////////////////////////////////////////////////////////////////
0:                 long nextChunkOffset = offset + 8 == chunkOffsets.size()
1:                                      ? compressedFileLength
1:                                      : chunkOffsets.getLong(offset + 8);
commit:4549a98
/////////////////////////////////////////////////////////////////////////
1: import com.google.common.annotations.VisibleForTesting;
/////////////////////////////////////////////////////////////////////////
1: import org.apache.cassandra.io.util.Memory;
/////////////////////////////////////////////////////////////////////////
1:     private final Memory chunkOffsets;
/////////////////////////////////////////////////////////////////////////
1:     @VisibleForTesting
/////////////////////////////////////////////////////////////////////////
1:     private Memory readChunkOffsets(DataInput input)
0:             Memory offsets = Memory.allocate(chunkCount * 8);
0:                     offsets.setLong(i * 8, input.readLong());
/////////////////////////////////////////////////////////////////////////
1:         int idx = 8 * (int) (position / parameters.chunkLength());
0:         if (idx >= chunkOffsets.size())
1:         long chunkOffset = chunkOffsets.getLong(idx);
0:         long nextChunkOffset = (idx + 8 == chunkOffsets.size())
1:                                 : chunkOffsets.getLong(idx + 8);
/////////////////////////////////////////////////////////////////////////
0:                 long offset = i * 8;
1:                 long chunkOffset = chunkOffsets.getLong(offset);
0:                 long nextChunkOffset = (i + 8 == chunkOffsets.size())
0:                                                : chunkOffsets.getLong(offset + 8);
1:     public void close()
1:     {
0:         chunkOffsets.free();
1:     }
1: 
commit:debb15e
/////////////////////////////////////////////////////////////////////////
1: import org.apache.cassandra.io.FSReadError;
1: import org.apache.cassandra.io.FSWriteError;
1: import org.apache.cassandra.io.sstable.CorruptSSTableException;
/////////////////////////////////////////////////////////////////////////
0:         return new CompressionMetadata(desc.filenameFor(Component.COMPRESSION_INFO), new File(dataFilePath).length());
0:     CompressionMetadata(String indexFilePath, long compressedLength)
0:         DataInputStream stream;
0:             stream = new DataInputStream(new FileInputStream(indexFilePath));
0:         catch (FileNotFoundException e)
0:             throw new RuntimeException(e);
0:         try
1:         {
1:             String compressorName = stream.readUTF();
1:             int optionCount = stream.readInt();
0:             Map<String, String> options = new HashMap<String, String>();
1:             for (int i = 0; i < optionCount; ++i)
1:             {
1:                 String key = stream.readUTF();
1:                 String value = stream.readUTF();
1:                 options.put(key, value);
1:             }
1:             int chunkLength = stream.readInt();
0:             try
1:             {
0:                 parameters = new CompressionParameters(compressorName, chunkLength, options);
1:             }
1:             catch (ConfigurationException e)
1:             {
0:                 throw new RuntimeException("Cannot create CompressionParameters for stored parameters", e);
1:             }
1:             dataLength = stream.readLong();
1:             compressedFileLength = compressedLength;
1:             chunkOffsets = readChunkOffsets(stream);
1:         }
1:         catch (IOException e)
1:         {
1:             throw new CorruptSSTableException(e, indexFilePath);
1:         }
0:         finally
1:         {
0:             FileUtils.closeQuietly(stream);
1:         }
/////////////////////////////////////////////////////////////////////////
0:     private BigLongArray readChunkOffsets(DataInput input)
0:         try
0:             int chunkCount = input.readInt();
0:             BigLongArray offsets = new BigLongArray(chunkCount);
0:             for (int i = 0; i < chunkCount; i++)
1:             {
0:                 try
1:                 {
0:                     offsets.set(i, input.readLong());
1:                 }
0:                 catch (EOFException e)
1:                 {
0:                     String msg = String.format("Corrupted Index File %s: read %d but expected %d chunks.",
0:                                                indexFilePath, i, chunkCount);
0:                     throw new CorruptSSTableException(new IOException(msg, e), indexFilePath);
1:                 }
1:             }
1: 
1:             return offsets;
1:         }
1:         catch (IOException e)
1:         {
1:             throw new FSReadError(e, indexFilePath);
1:         }
/////////////////////////////////////////////////////////////////////////
1:     public Chunk chunkFor(long position)
1:             throw new CorruptSSTableException(new EOFException(), indexFilePath);
/////////////////////////////////////////////////////////////////////////
1:         // path to the file
1:         private final String filePath;
0:         private Writer(String path) throws FileNotFoundException
1:             filePath = path;
0:         public static Writer open(String path)
0:             try
0:                 return new Writer(path);
0:             catch (FileNotFoundException e)
1:             {
0:                 throw new RuntimeException(e);
1:             }
0:         public void writeHeader(CompressionParameters parameters)
1:         {
0:             try
1:             {
0:                 writeUTF(parameters.sstableCompressor.getClass().getSimpleName());
0:                 writeInt(parameters.otherOptions.size());
0:                 for (Map.Entry<String, String> entry : parameters.otherOptions.entrySet())
1:                 {
0:                     writeUTF(entry.getKey());
0:                     writeUTF(entry.getValue());
1:                 }
1: 
1:                 // store the length of the chunk
0:                 writeInt(parameters.chunkLength());
1:                 // store position and reserve a place for uncompressed data length and chunks count
0:                 dataLengthOffset = getFilePointer();
0:                 writeLong(-1);
0:                 writeInt(-1);
1:             }
1:             catch (IOException e)
1:             {
1:                 throw new FSWriteError(e, filePath);
1:             }
1:         }
1: 
0:         public void finalizeHeader(long dataLength, int chunks)
0:             long currentPosition;
0:             try
1:             {
0:                 currentPosition = getFilePointer();
1:             }
1:             catch (IOException e)
1:             {
0:                 throw new FSReadError(e, filePath);
1:             }
0:             try
1:             {
0:                 // seek back to the data length position
0:                 seek(dataLengthOffset);
0:                 // write uncompressed data length and chunks count
0:                 writeLong(dataLength);
0:                 writeInt(chunks);
1: 
0:                 // seek forward to the previous position
0:                 seek(currentPosition);
1:             }
1:             catch (IOException e)
1:             {
1:                 throw new FSWriteError(e, filePath);
1:             }
/////////////////////////////////////////////////////////////////////////
1:         public long chunkOffsetBy(int chunkIndex)
0:                 long position = getFilePointer();
1: 
0:                 // seek to the position of the given chunk
0:                 seek(dataLengthOffset
0:                      + 8 // size reserved for uncompressed data length
0:                      + 4 // size reserved for chunk count
0:                      + (chunkIndex * 8L));
1: 
0:                 try
0:                 {
0:                     return readLong();
0:                 }
0:                 finally
0:                 {
0:                     // back to the original position
0:                     seek(position);
0:                 }
0:             catch (IOException e)
0:                 throw new FSReadError(e, filePath);
/////////////////////////////////////////////////////////////////////////
1:         public void resetAndTruncate(int chunkIndex)
0:             try
0:             {
0:                 seek(dataLengthOffset
0:                      + 8 // size reserved for uncompressed data length
0:                      + 4 // size reserved for chunk count
0:                      + (chunkIndex * 8L));
0:                 getChannel().truncate(getFilePointer());
0:             }
0:             catch (IOException e)
0:             {
0:                 throw new FSWriteError(e, filePath);
0:             }
author:Sylvain Lebresne
-------------------------------------------------------------------------------
commit:3a2faf9
/////////////////////////////////////////////////////////////////////////
0: import org.apache.cassandra.exceptions.ConfigurationException;
commit:07cf56f
/////////////////////////////////////////////////////////////////////////
1: /*
/////////////////////////////////////////////////////////////////////////
1:  *     http://www.apache.org/licenses/LICENSE-2.0
1:  * Unless required by applicable law or agreed to in writing, software
1:  * distributed under the License is distributed on an "AS IS" BASIS,
1:  * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
1:  * See the License for the specific language governing permissions and
1:  * limitations under the License.
commit:6ea00a3
/////////////////////////////////////////////////////////////////////////
1:     public static class Chunk
commit:981ee77
/////////////////////////////////////////////////////////////////////////
/////////////////////////////////////////////////////////////////////////
1:      * Create metadata about given compressed file including uncompressed data length, chunk size
1:      * This is an expensive operation! Don't create more than one for each
1:      * sstable.
1:      *
1:     public static CompressionMetadata create(String dataFilePath)
1: 
0:             return new CompressionMetadata(desc.filenameFor(Component.COMPRESSION_INFO), new File(dataFilePath).length());
/////////////////////////////////////////////////////////////////////////
0:     // This is package protected because of the tests.
commit:3e77792
/////////////////////////////////////////////////////////////////////////
0:             parameters = new CompressionParameters(compressorName, chunkLength, options);
/////////////////////////////////////////////////////////////////////////
0:         return parameters.sstableCompressor;
1:         return parameters.chunkLength();
/////////////////////////////////////////////////////////////////////////
0:         int idx = (int) (position / parameters.chunkLength());
/////////////////////////////////////////////////////////////////////////
0:             writeUTF(parameters.sstableCompressor.getClass().getSimpleName());
0:             writeInt(parameters.otherOptions.size());
0:             for (Map.Entry<String, String> entry : parameters.otherOptions.entrySet())
0:             writeInt(parameters.chunkLength());
commit:1ecabe6
/////////////////////////////////////////////////////////////////////////
1: /**
1:  * Licensed to the Apache Software Foundation (ASF) under one
1:  * or more contributor license agreements.  See the NOTICE file
1:  * distributed with this work for additional information
1:  * regarding copyright ownership.  The ASF licenses this file
1:  * to you under the Apache License, Version 2.0 (the
1:  * "License"); you may not use this file except in compliance
1:  * with the License.  You may obtain a copy of the License at
1:  *
0:  *   http://www.apache.org/licenses/LICENSE-2.0
1:  *
0:  * Unless required by applicable law or agreed to in writing,
0:  * software distributed under the License is distributed on an
0:  * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
0:  * KIND, either express or implied.  See the License for the
0:  * specific language governing permissions and limitations
0:  * under the License.
1:  */
1: package org.apache.cassandra.io.compress;
1: 
0: import java.io.*;
1: 
0: import org.apache.cassandra.io.util.FileUtils;
1: 
1: /**
1:  * Holds metadata about compressed file
1:  */
1: public class CompressionMetadata
0: {
1: 
0:     public final int chunkLength;
1:     public final long dataLength;
1:     public final long compressedFileLength;
0:     public final long[] chunkOffsets;
1:     public final String indexFilePath;
0:     public final String algorithm;
1: 
0:     public CompressionMetadata(String indexFilePath, long compressedLength) throws IOException
0:     {
1:         this.indexFilePath = indexFilePath;
1: 
0:         DataInputStream stream = new DataInputStream(new FileInputStream(indexFilePath));
1: 
0:         algorithm = stream.readUTF();
0:         chunkLength = stream.readInt();
0:         dataLength = stream.readLong();
0:         compressedFileLength = compressedLength;
0:         chunkOffsets = readChunkOffsets(stream);
1: 
0:         FileUtils.closeQuietly(stream);
0:     }
1: 
1:     /**
1:      * Read offsets of the individual chunks from the given input.
1:      *
1:      * @param input Source of the data.
1:      *
1:      * @return collection of the chunk offsets.
1:      *
0:      * @throws java.io.IOException on any I/O error (except EOF).
1:      */
0:     private long[] readChunkOffsets(DataInput input) throws IOException
0:     {
0:         int chunkCount = input.readInt();
0:         long[] offsets = new long[chunkCount];
1: 
0:         for (int i = 0; i < offsets.length; i++)
0:         {
0:             try
0:             {
0:                 offsets[i] = input.readLong();
0:             }
0:             catch (EOFException e)
0:             {
0:                 throw new EOFException(String.format("Corrupted Index File %s: read %d but expected %d chunks.",
0:                                                      indexFilePath,
0:                                                      i,
0:                                                      chunkCount));
0:             }
0:         }
1: 
0:         return offsets;
0:     }
1: 
1:     /**
1:      * Get a chunk of compressed data (offset, length) corresponding to given position
1:      *
1:      * @param position Position in the file.
1:      * @return pair of chunk offset and length.
0:      * @throws java.io.IOException on any I/O error.
1:      */
0:     public Chunk chunkFor(long position) throws IOException
0:     {
1:         // position of the chunk
0:         int idx = (int) (position / chunkLength);
1: 
0:         if (idx >= chunkOffsets.length)
0:             throw new EOFException();
1: 
0:         long chunkOffset = chunkOffsets[idx];
0:         long nextChunkOffset = (idx + 1 == chunkOffsets.length)
0:                                 ? compressedFileLength
0:                                 : chunkOffsets[idx + 1];
1: 
0:         return new Chunk(chunkOffset, (int) (nextChunkOffset - chunkOffset));
0:     }
1: 
0:     public static class Writer extends RandomAccessFile
0:     {
0:         // place for uncompressed data length in the index file
0:         private long dataLengthOffset = -1;
1: 
0:         public Writer(String path) throws IOException
0:         {
0:             super(path, "rw");
0:         }
1: 
0:         public void writeHeader(String algorithm, int chunkLength) throws IOException
0:         {
0:             // algorithm
0:             writeUTF(algorithm);
1: 
0:             // store the length of the chunk
0:             writeInt(chunkLength);
0:             // store position and reserve a place for uncompressed data length and chunks count
0:             dataLengthOffset = getFilePointer();
0:             writeLong(-1);
0:             writeInt(-1);
0:         }
1: 
0:         public void finalizeHeader(long dataLength, int chunks) throws IOException
0:         {
0:             assert dataLengthOffset != -1 : "writeHeader wasn't called";
1: 
0:             long currentPosition = getFilePointer();
1: 
0:             // seek back to the data length position
0:             seek(dataLengthOffset);
1: 
0:             // write uncompressed data length and chunks count
0:             writeLong(dataLength);
0:             writeInt(chunks);
1: 
0:             // seek forward to the previous position
0:             seek(currentPosition);
0:         }
1: 
1:         /**
1:          * Get a chunk offset by it's index.
1:          *
1:          * @param chunkIndex Index of the chunk.
1:          *
1:          * @return offset of the chunk in the compressed file.
1:          *
0:          * @throws IOException any I/O error.
1:          */
0:         public long chunkOffsetBy(int chunkIndex) throws IOException
0:         {
0:             if (dataLengthOffset == -1)
0:                 throw new IllegalStateException("writeHeader wasn't called");
1: 
0:             long position = getFilePointer();
1: 
0:             // seek to the position of the given chunk
0:             seek(dataLengthOffset
0:                  + 8 // size reserved for uncompressed data length
0:                  + 4 // size reserved for chunk count
0:                  + (chunkIndex * 8));
1: 
0:             try
0:             {
0:                 return readLong();
0:             }
0:             finally
0:             {
0:                 // back to the original position
0:                 seek(position);
0:             }
0:         }
1: 
1:         /**
1:          * Reset the writer so that the next chunk offset written will be the
1:          * one of {@code chunkIndex}.
1:          */
0:         public void resetAndTruncate(int chunkIndex) throws IOException
0:         {
0:             seek(dataLengthOffset
0:                  + 8 // size reserved for uncompressed data length
0:                  + 4 // size reserved for chunk count
0:                  + (chunkIndex * 8));
0:             getChannel().truncate(getFilePointer());
0:         }
0:     }
1: 
1:     /**
1:      * Holds offset and length of the file chunk
1:      */
0:     public class Chunk
0:     {
1:         public final long offset;
1:         public final int length;
1: 
1:         public Chunk(long offset, int length)
0:         {
1:             this.offset = offset;
1:             this.length = length;
0:         }
0:     }
0: }
author:Pavel Yaskevich
-------------------------------------------------------------------------------
commit:eba903a
/////////////////////////////////////////////////////////////////////////
0: import org.apache.cassandra.utils.BigLongArray;
/////////////////////////////////////////////////////////////////////////
0:     private final BigLongArray chunkOffsets;
/////////////////////////////////////////////////////////////////////////
0:     private BigLongArray readChunkOffsets(DataInput input) throws IOException
0:         BigLongArray offsets = new BigLongArray(chunkCount);
0:         for (int i = 0; i < chunkCount; i++)
0:                 offsets.set(i, input.readLong());
/////////////////////////////////////////////////////////////////////////
0:         if (idx >= chunkOffsets.size)
0:         long chunkOffset = chunkOffsets.get(idx);
0:         long nextChunkOffset = (idx + 1 == chunkOffsets.size)
0:                                 : chunkOffsets.get(idx + 1);
commit:3494894
/////////////////////////////////////////////////////////////////////////
0: import java.util.concurrent.ConcurrentHashMap;
1: import org.apache.cassandra.io.sstable.Component;
1: import org.apache.cassandra.io.sstable.Descriptor;
/////////////////////////////////////////////////////////////////////////
0:     /**
0:      * Caches instances of CompressionMetadata.
0:      * Each metada holds the chunk offsets index, which is reasonably big for
0:      * enough data, so it's an expensive structure. We thus only want one
0:      * CompressionMetadata created for each sstable.
0:      * Note that we could have a compressionMetadata field in SSTableReader,
0:      * but CompressedSegmentFile.Builder needs it before the reader is
0:      * created, so it's easier that way.
1:      */
0:     private final static Map<String, CompressionMetadata> cache = new ConcurrentHashMap<String, CompressionMetadata>();
0: 
0:     /**
0:      * Get metadata about given compressed file including uncompressed data length, chunk size
1:      * and list of the chunk offsets of the compressed data.
0:      *
1:      * @param dataFilePath Path to the compressed file
0:      *
1:      * @return metadata about given compressed file.
1:      */
0:     public static CompressionMetadata get(String dataFilePath)
0:     {
0:         CompressionMetadata metadata = cache.get(dataFilePath);
0:         if (metadata != null)
0:             return metadata;
0: 
0:         // We want this to be relatively fast, because it's called often (for each
0:         // range query). On the side, we don't care too much if the initial
0:         // creation is no thread-safe, because we'll call this when the
0:         // SSTableReader is loaded/created, so we're pretty sure there won't
0:         // be any contention. Besides, if we really do create two
0:         // CompressionMetadata, it's not the end of the world, so we don't
0:         // bother with synchronization
1:         Descriptor desc = Descriptor.fromFilename(dataFilePath);
0:         try
0:         {
0:             metadata = new CompressionMetadata(desc.filenameFor(Component.COMPRESSION_INFO), new File(dataFilePath).length());
0:             cache.put(dataFilePath, metadata);
0:             return metadata;
0:         }
0:         catch (IOException e)
0:         {
0:             throw new IOError(e);
0:         }
0:     }
0: 
0:     // This is package protected because of the tests. Don't use, use get() instead.
0:     CompressionMetadata(String indexFilePath, long compressedLength) throws IOException
commit:c8afd76
/////////////////////////////////////////////////////////////////////////
0: import java.util.HashMap;
0: import java.util.Map;
0: import org.apache.cassandra.config.ConfigurationException;
/////////////////////////////////////////////////////////////////////////
0:     public final CompressionParameters parameters;
/////////////////////////////////////////////////////////////////////////
0:         String compressorName = stream.readUTF();
0:         int optionCount = stream.readInt();
0:         Map<String, String> options = new HashMap<String, String>();
0:         for (int i = 0; i < optionCount; ++i)
0:         {
0:             String key = stream.readUTF();
0:             String value = stream.readUTF();
0:             options.put(key, value);
0:         }
0:         int chunkLength = stream.readInt();
0:         try
0:         {
0:             parameters = new CompressionParameters(compressorName, options, chunkLength);
0:         }
0:         catch (ConfigurationException e)
0:         {
0:             throw new RuntimeException("Cannot create CompressionParameters for stored parameters", e);
0:         }
0: 
/////////////////////////////////////////////////////////////////////////
1:     public ICompressor compressor()
0:     {
0:         return parameters.compressor;
0:     }
0: 
1:     public int chunkLength()
0:     {
0:         return parameters.chunkLength;
0:     }
0: 
/////////////////////////////////////////////////////////////////////////
0:         int idx = (int) (position / parameters.chunkLength);
/////////////////////////////////////////////////////////////////////////
0:         public void writeHeader(CompressionParameters parameters) throws IOException
0:             writeUTF(parameters.compressorClass.getSimpleName());
0:             writeInt(parameters.compressionOptions.size());
0:             for (Map.Entry<String, String> entry : parameters.compressionOptions.entrySet())
0:             {
0:                 writeUTF(entry.getKey());
0:                 writeUTF(entry.getValue());
0:             }
0:             writeInt(parameters.chunkLength);
commit:ba1821f
/////////////////////////////////////////////////////////////////////////
1:         return new Chunk(chunkOffset, (int) (nextChunkOffset - chunkOffset - 4)); // "4" bytes reserved for checksum
/////////////////////////////////////////////////////////////////////////
0: 
1:         public String toString()
0:         {
1:             return String.format("Chunk<offset: %d, length: %d>", offset, length);
0:         }
author:Brandon Williams
-------------------------------------------------------------------------------
commit:8aeb2fe
/////////////////////////////////////////////////////////////////////////
0:                  + (chunkIndex * 8L));
/////////////////////////////////////////////////////////////////////////
0:                  + (chunkIndex * 8L));
============================================================================