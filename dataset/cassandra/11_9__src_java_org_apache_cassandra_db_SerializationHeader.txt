1:a991b64: /*
1:a991b64:  * Licensed to the Apache Software Foundation (ASF) under one
1:a991b64:  * or more contributor license agreements.  See the NOTICE file
1:a991b64:  * distributed with this work for additional information
1:a991b64:  * regarding copyright ownership.  The ASF licenses this file
1:a991b64:  * to you under the Apache License, Version 2.0 (the
1:a991b64:  * "License"); you may not use this file except in compliance
1:a991b64:  * with the License.  You may obtain a copy of the License at
1:a991b64:  *
1:a991b64:  *     http://www.apache.org/licenses/LICENSE-2.0
1:a991b64:  *
1:a991b64:  * Unless required by applicable law or agreed to in writing, software
1:a991b64:  * distributed under the License is distributed on an "AS IS" BASIS,
1:a991b64:  * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
1:a991b64:  * See the License for the specific language governing permissions and
1:a991b64:  * limitations under the License.
1:a991b64:  */
1:a991b64: package org.apache.cassandra.db;
11:a991b64: 
1:a991b64: import java.io.IOException;
1:a991b64: import java.nio.ByteBuffer;
1:a991b64: import java.util.*;
1:a991b64: 
1:a991b64: import org.apache.cassandra.config.CFMetaData;
1:a991b64: import org.apache.cassandra.config.ColumnDefinition;
1:fe388d4: import org.apache.cassandra.db.filter.ColumnFilter;
1:a991b64: import org.apache.cassandra.db.rows.*;
1:a991b64: import org.apache.cassandra.db.marshal.AbstractType;
1:a991b64: import org.apache.cassandra.db.marshal.UTF8Type;
1:a991b64: import org.apache.cassandra.db.marshal.TypeParser;
1:a991b64: import org.apache.cassandra.io.sstable.format.SSTableReader;
1:a991b64: import org.apache.cassandra.io.sstable.format.Version;
1:a991b64: import org.apache.cassandra.io.sstable.metadata.MetadataType;
1:a991b64: import org.apache.cassandra.io.sstable.metadata.MetadataComponent;
1:a991b64: import org.apache.cassandra.io.sstable.metadata.IMetadataComponentSerializer;
1:03f72ac: import org.apache.cassandra.io.util.DataInputPlus;
1:a991b64: import org.apache.cassandra.io.util.DataOutputPlus;
1:a991b64: import org.apache.cassandra.utils.ByteBufferUtil;
1:a991b64: 
1:a991b64: public class SerializationHeader
1:a991b64: {
1:a991b64:     public static final Serializer serializer = new Serializer();
1:a991b64: 
1:6584331:     private final boolean isForSSTable;
1:6584331: 
1:a991b64:     private final AbstractType<?> keyType;
1:a991b64:     private final List<AbstractType<?>> clusteringTypes;
1:a991b64: 
1:a991b64:     private final PartitionColumns columns;
1:c055ab9:     private final EncodingStats stats;
1:a991b64: 
1:a991b64:     private final Map<ByteBuffer, AbstractType<?>> typeMap;
1:a991b64: 
1:6584331:     private SerializationHeader(boolean isForSSTable,
1:6584331:                                 AbstractType<?> keyType,
1:a991b64:                                 List<AbstractType<?>> clusteringTypes,
1:a991b64:                                 PartitionColumns columns,
1:c055ab9:                                 EncodingStats stats,
1:a991b64:                                 Map<ByteBuffer, AbstractType<?>> typeMap)
1:a991b64:     {
1:6584331:         this.isForSSTable = isForSSTable;
1:a991b64:         this.keyType = keyType;
1:a991b64:         this.clusteringTypes = clusteringTypes;
1:a991b64:         this.columns = columns;
1:a991b64:         this.stats = stats;
1:a991b64:         this.typeMap = typeMap;
1:a991b64:     }
1:a991b64: 
1:e8651b6:     public static SerializationHeader makeWithoutStats(CFMetaData metadata)
1:e8651b6:     {
1:e8651b6:         return new SerializationHeader(true, metadata, metadata.partitionColumns(), EncodingStats.NO_STATS);
1:e8651b6:     }
1:e8651b6: 
1:a991b64:     public static SerializationHeader make(CFMetaData metadata, Collection<SSTableReader> sstables)
1:a991b64:     {
1:a991b64:         // The serialization header has to be computed before the start of compaction (since it's used to write)
1:a991b64:         // the result. This means that when compacting multiple sources, we won't have perfectly accurate stats
1:c055ab9:         // (for EncodingStats) since compaction may delete, purge and generally merge rows in unknown ways. This is
1:a991b64:         // kind of ok because those stats are only used for optimizing the underlying storage format and so we
1:a991b64:         // just have to strive for as good as possible. Currently, we stick to a relatively naive merge of existing
1:a991b64:         // global stats because it's simple and probably good enough in most situation but we could probably
1:a991b64:         // improve our marging of inaccuracy through the use of more fine-grained stats in the future.
1:a991b64:         // Note however that to avoid seeing our accuracy degrade through successive compactions, we don't base
1:a991b64:         // our stats merging on the compacted files headers, which as we just said can be somewhat inaccurate,
1:a991b64:         // but rather on their stats stored in StatsMetadata that are fully accurate.
1:c055ab9:         EncodingStats.Collector stats = new EncodingStats.Collector();
1:a991b64:         PartitionColumns.Builder columns = PartitionColumns.builder();
1:a991b64:         for (SSTableReader sstable : sstables)
1:a991b64:         {
1:a991b64:             stats.updateTimestamp(sstable.getMinTimestamp());
1:a991b64:             stats.updateLocalDeletionTime(sstable.getMinLocalDeletionTime());
1:a991b64:             stats.updateTTL(sstable.getMinTTL());
1:a991b64:             if (sstable.header == null)
1:a991b64:                 columns.addAll(metadata.partitionColumns());
1:a991b64:             else
1:a991b64:                 columns.addAll(sstable.header.columns());
1:a991b64:         }
1:6584331:         return new SerializationHeader(true, metadata, columns.build(), stats.get());
1:a991b64:     }
1:a991b64: 
1:6584331:     public SerializationHeader(boolean isForSSTable,
1:6584331:                                CFMetaData metadata,
1:a991b64:                                PartitionColumns columns,
1:c055ab9:                                EncodingStats stats)
1:a991b64:     {
1:6584331:         this(isForSSTable,
1:6584331:              metadata.getKeyValidator(),
1:dc9ed46:              metadata.comparator.subtypes(),
1:a991b64:              columns,
1:a991b64:              stats,
1:a991b64:              null);
1:a991b64:     }
1:a991b64: 
1:a991b64:     public PartitionColumns columns()
1:a991b64:     {
1:a991b64:         return columns;
1:a991b64:     }
1:a991b64: 
1:a991b64:     public boolean hasStatic()
1:a991b64:     {
1:a991b64:         return !columns.statics.isEmpty();
1:a991b64:     }
1:a991b64: 
1:6584331:     public boolean isForSSTable()
1:6584331:     {
1:6584331:         return isForSSTable;
1:6584331:     }
1:6584331: 
1:c055ab9:     public EncodingStats stats()
1:a991b64:     {
1:a991b64:         return stats;
1:a991b64:     }
1:a991b64: 
1:a991b64:     public AbstractType<?> keyType()
1:a991b64:     {
1:a991b64:         return keyType;
1:a991b64:     }
1:a991b64: 
1:a991b64:     public List<AbstractType<?>> clusteringTypes()
1:a991b64:     {
1:a991b64:         return clusteringTypes;
1:a991b64:     }
1:a991b64: 
1:a991b64:     public Columns columns(boolean isStatic)
1:a991b64:     {
1:a991b64:         return isStatic ? columns.statics : columns.regulars;
1:a991b64:     }
1:a991b64: 
1:a991b64:     public AbstractType<?> getType(ColumnDefinition column)
1:a991b64:     {
1:a991b64:         return typeMap == null ? column.type : typeMap.get(column.name.bytes);
1:a991b64:     }
1:a991b64: 
1:c055ab9:     public void writeTimestamp(long timestamp, DataOutputPlus out) throws IOException
1:a991b64:     {
1:649a106:         out.writeUnsignedVInt(timestamp - stats.minTimestamp);
1:a991b64:     }
1:a991b64: 
1:c055ab9:     public void writeLocalDeletionTime(int localDeletionTime, DataOutputPlus out) throws IOException
1:a991b64:     {
1:649a106:         out.writeUnsignedVInt(localDeletionTime - stats.minLocalDeletionTime);
1:a991b64:     }
1:a991b64: 
1:c055ab9:     public void writeTTL(int ttl, DataOutputPlus out) throws IOException
1:a991b64:     {
1:649a106:         out.writeUnsignedVInt(ttl - stats.minTTL);
1:a991b64:     }
1:a991b64: 
1:c055ab9:     public void writeDeletionTime(DeletionTime dt, DataOutputPlus out) throws IOException
1:a991b64:     {
1:c055ab9:         writeTimestamp(dt.markedForDeleteAt(), out);
1:c055ab9:         writeLocalDeletionTime(dt.localDeletionTime(), out);
1:a991b64:     }
1:a991b64: 
1:c055ab9:     public long readTimestamp(DataInputPlus in) throws IOException
1:a991b64:     {
1:649a106:         return in.readUnsignedVInt() + stats.minTimestamp;
1:a991b64:     }
1:a991b64: 
1:c055ab9:     public int readLocalDeletionTime(DataInputPlus in) throws IOException
1:a991b64:     {
1:649a106:         return (int)in.readUnsignedVInt() + stats.minLocalDeletionTime;
1:c055ab9:     }
1:c055ab9: 
1:c055ab9:     public int readTTL(DataInputPlus in) throws IOException
1:c055ab9:     {
1:649a106:         return (int)in.readUnsignedVInt() + stats.minTTL;
1:c055ab9:     }
1:c055ab9: 
1:c055ab9:     public DeletionTime readDeletionTime(DataInputPlus in) throws IOException
1:c055ab9:     {
1:c055ab9:         long markedAt = readTimestamp(in);
1:c055ab9:         int localDeletionTime = readLocalDeletionTime(in);
1:c055ab9:         return new DeletionTime(markedAt, localDeletionTime);
1:c055ab9:     }
1:c055ab9: 
1:c055ab9:     public long timestampSerializedSize(long timestamp)
1:c055ab9:     {
1:649a106:         return TypeSizes.sizeofUnsignedVInt(timestamp - stats.minTimestamp);
1:c055ab9:     }
1:c055ab9: 
1:c055ab9:     public long localDeletionTimeSerializedSize(int localDeletionTime)
1:c055ab9:     {
1:649a106:         return TypeSizes.sizeofUnsignedVInt(localDeletionTime - stats.minLocalDeletionTime);
1:c055ab9:     }
1:c055ab9: 
1:c055ab9:     public long ttlSerializedSize(int ttl)
1:c055ab9:     {
1:649a106:         return TypeSizes.sizeofUnsignedVInt(ttl - stats.minTTL);
1:c055ab9:     }
1:c055ab9: 
1:c055ab9:     public long deletionTimeSerializedSize(DeletionTime dt)
1:c055ab9:     {
1:c055ab9:         return timestampSerializedSize(dt.markedForDeleteAt())
1:c055ab9:              + localDeletionTimeSerializedSize(dt.localDeletionTime());
1:c055ab9:     }
1:c055ab9: 
1:c055ab9:     public void skipTimestamp(DataInputPlus in) throws IOException
1:c055ab9:     {
1:649a106:         in.readUnsignedVInt();
1:c055ab9:     }
1:c055ab9: 
1:c055ab9:     public void skipLocalDeletionTime(DataInputPlus in) throws IOException
1:c055ab9:     {
1:649a106:         in.readUnsignedVInt();
1:c055ab9:     }
1:c055ab9: 
1:c055ab9:     public void skipTTL(DataInputPlus in) throws IOException
1:c055ab9:     {
1:649a106:         in.readUnsignedVInt();
1:c055ab9:     }
1:c055ab9: 
1:c055ab9:     public void skipDeletionTime(DataInputPlus in) throws IOException
1:c055ab9:     {
1:c055ab9:         skipTimestamp(in);
1:c055ab9:         skipLocalDeletionTime(in);
1:a991b64:     }
1:a991b64: 
1:a991b64:     public Component toComponent()
1:a991b64:     {
1:a991b64:         Map<ByteBuffer, AbstractType<?>> staticColumns = new LinkedHashMap<>();
1:a991b64:         Map<ByteBuffer, AbstractType<?>> regularColumns = new LinkedHashMap<>();
1:a991b64:         for (ColumnDefinition column : columns.statics)
1:a991b64:             staticColumns.put(column.name.bytes, column.type);
1:a991b64:         for (ColumnDefinition column : columns.regulars)
1:a991b64:             regularColumns.put(column.name.bytes, column.type);
1:a991b64:         return new Component(keyType, clusteringTypes, staticColumns, regularColumns, stats);
1:a991b64:     }
1:a991b64: 
1:a991b64:     @Override
1:a991b64:     public String toString()
1:a991b64:     {
1:c055ab9:         return String.format("SerializationHeader[key=%s, cks=%s, columns=%s, stats=%s, typeMap=%s]", keyType, clusteringTypes, columns, stats, typeMap);
1:a991b64:     }
1:a991b64: 
1:a991b64:     /**
1:a991b64:      * We need the CFMetadata to properly deserialize a SerializationHeader but it's clunky to pass that to
1:a991b64:      * a SSTable component, so we use this temporary object to delay the actual need for the metadata.
1:a991b64:      */
1:a991b64:     public static class Component extends MetadataComponent
1:a991b64:     {
1:a991b64:         private final AbstractType<?> keyType;
1:a991b64:         private final List<AbstractType<?>> clusteringTypes;
1:a991b64:         private final Map<ByteBuffer, AbstractType<?>> staticColumns;
1:a991b64:         private final Map<ByteBuffer, AbstractType<?>> regularColumns;
1:c055ab9:         private final EncodingStats stats;
1:a991b64: 
1:a991b64:         private Component(AbstractType<?> keyType,
1:a991b64:                           List<AbstractType<?>> clusteringTypes,
1:a991b64:                           Map<ByteBuffer, AbstractType<?>> staticColumns,
1:a991b64:                           Map<ByteBuffer, AbstractType<?>> regularColumns,
1:c055ab9:                           EncodingStats stats)
1:a991b64:         {
1:a991b64:             this.keyType = keyType;
1:a991b64:             this.clusteringTypes = clusteringTypes;
1:a991b64:             this.staticColumns = staticColumns;
1:a991b64:             this.regularColumns = regularColumns;
1:a991b64:             this.stats = stats;
1:a991b64:         }
1:a991b64: 
1:a991b64:         public MetadataType getType()
1:a991b64:         {
1:a991b64:             return MetadataType.HEADER;
1:a991b64:         }
1:a991b64: 
1:a991b64:         public SerializationHeader toHeader(CFMetaData metadata)
1:a991b64:         {
1:a991b64:             Map<ByteBuffer, AbstractType<?>> typeMap = new HashMap<>(staticColumns.size() + regularColumns.size());
1:a991b64:             typeMap.putAll(staticColumns);
1:a991b64:             typeMap.putAll(regularColumns);
1:a991b64: 
1:a991b64:             PartitionColumns.Builder builder = PartitionColumns.builder();
1:a991b64:             for (ByteBuffer name : typeMap.keySet())
1:a991b64:             {
1:a991b64:                 ColumnDefinition column = metadata.getColumnDefinition(name);
1:a991b64:                 if (column == null)
1:a991b64:                 {
1:a991b64:                     // TODO: this imply we don't read data for a column we don't yet know about, which imply this is theoretically
1:a991b64:                     // racy with column addition. Currently, it is up to the user to not write data before the schema has propagated
1:a991b64:                     // and this is far from being the only place that has such problem in practice. This doesn't mean we shouldn't
1:a991b64:                     // improve this.
1:a991b64: 
1:a991b64:                     // If we don't find the definition, it could be we have data for a dropped column, and we shouldn't
1:a991b64:                     // fail deserialization because of that. So we grab a "fake" ColumnDefinition that ensure proper
1:a991b64:                     // deserialization. The column will be ignore later on anyway.
1:2f0e365:                     boolean isStatic = staticColumns.containsKey(name);
1:2f0e365:                     column = metadata.getDroppedColumnDefinition(name, isStatic);
1:a991b64:                     if (column == null)
1:a991b64:                         throw new RuntimeException("Unknown column " + UTF8Type.instance.getString(name) + " during deserialization");
1:a991b64:                 }
1:a991b64:                 builder.add(column);
1:a991b64:             }
1:6584331:             return new SerializationHeader(true, keyType, clusteringTypes, builder.build(), stats, typeMap);
1:a991b64:         }
1:a991b64: 
1:a991b64:         @Override
1:a991b64:         public boolean equals(Object o)
1:a991b64:         {
1:a991b64:             if(!(o instanceof Component))
1:a991b64:                 return false;
1:a991b64: 
1:a991b64:             Component that = (Component)o;
1:a991b64:             return Objects.equals(this.keyType, that.keyType)
1:a991b64:                 && Objects.equals(this.clusteringTypes, that.clusteringTypes)
1:a991b64:                 && Objects.equals(this.staticColumns, that.staticColumns)
1:a991b64:                 && Objects.equals(this.regularColumns, that.regularColumns)
1:a991b64:                 && Objects.equals(this.stats, that.stats);
1:a991b64:         }
1:a991b64: 
1:a991b64:         @Override
1:a991b64:         public int hashCode()
1:a991b64:         {
1:a991b64:             return Objects.hash(keyType, clusteringTypes, staticColumns, regularColumns, stats);
1:a991b64:         }
1:a991b64: 
1:a991b64:         @Override
1:a991b64:         public String toString()
1:a991b64:         {
1:a991b64:             return String.format("SerializationHeader.Component[key=%s, cks=%s, statics=%s, regulars=%s, stats=%s]",
1:a991b64:                                  keyType, clusteringTypes, staticColumns, regularColumns, stats);
1:71b1c4a:         }
1:71b1c4a: 
1:0a5e220:         public AbstractType<?> getKeyType()
1:71b1c4a:         {
1:71b1c4a:             return keyType;
1:a991b64:         }
1:71b1c4a: 
1:71b1c4a:         public List<AbstractType<?>> getClusteringTypes()
1:71b1c4a:         {
1:71b1c4a:             return clusteringTypes;
1:71b1c4a:         }
1:71b1c4a: 
1:71b1c4a:         public Map<ByteBuffer, AbstractType<?>> getStaticColumns()
1:71b1c4a:         {
1:71b1c4a:             return staticColumns;
1:71b1c4a:         }
1:71b1c4a: 
1:71b1c4a:         public Map<ByteBuffer, AbstractType<?>> getRegularColumns()
1:71b1c4a:         {
1:71b1c4a:             return regularColumns;
1:71b1c4a:         }
1:71b1c4a: 
1:71b1c4a:         public EncodingStats getEncodingStats()
1:71b1c4a:         {
1:71b1c4a:             return stats;
1:71b1c4a:         }
1:a991b64:     }
1:a991b64: 
1:a991b64:     public static class Serializer implements IMetadataComponentSerializer<Component>
1:a991b64:     {
1:fe388d4:         public void serializeForMessaging(SerializationHeader header, ColumnFilter selection, DataOutputPlus out, boolean hasStatic) throws IOException
1:a991b64:         {
1:c055ab9:             EncodingStats.serializer.serialize(header.stats, out);
1:a991b64: 
1:fe388d4:             if (selection == null)
1:a991b64:             {
1:a991b64:                 if (hasStatic)
1:a991b64:                     Columns.serializer.serialize(header.columns.statics, out);
1:a991b64:                 Columns.serializer.serialize(header.columns.regulars, out);
1:a991b64:             }
1:fe388d4:             else
1:a991b64:             {
3:fe388d4:                 if (hasStatic)
1:fe388d4:                     Columns.serializer.serializeSubset(header.columns.statics, selection.fetchedColumns().statics, out);
1:fe388d4:                 Columns.serializer.serializeSubset(header.columns.regulars, selection.fetchedColumns().regulars, out);
1:a991b64:             }
1:a991b64:         }
1:a991b64: 
1:fe388d4:         public SerializationHeader deserializeForMessaging(DataInputPlus in, CFMetaData metadata, ColumnFilter selection, boolean hasStatic) throws IOException
1:a991b64:         {
1:c055ab9:             EncodingStats stats = EncodingStats.serializer.deserialize(in);
1:a991b64: 
1:a991b64:             AbstractType<?> keyType = metadata.getKeyValidator();
1:dc9ed46:             List<AbstractType<?>> clusteringTypes = metadata.comparator.subtypes();
1:a991b64: 
1:fe388d4:             Columns statics, regulars;
1:fe388d4:             if (selection == null)
1:a991b64:             {
1:fe388d4:                 statics = hasStatic ? Columns.serializer.deserialize(in, metadata) : Columns.NONE;
1:fe388d4:                 regulars = Columns.serializer.deserialize(in, metadata);
1:a991b64:             }
1:fe388d4:             else
1:a991b64:             {
1:fe388d4:                 statics = hasStatic ? Columns.serializer.deserializeSubset(selection.fetchedColumns().statics, in) : Columns.NONE;
1:fe388d4:                 regulars = Columns.serializer.deserializeSubset(selection.fetchedColumns().regulars, in);
1:a991b64:             }
1:a991b64: 
1:6584331:             return new SerializationHeader(false, keyType, clusteringTypes, new PartitionColumns(statics, regulars), stats, null);
1:a991b64:         }
1:a991b64: 
1:fe388d4:         public long serializedSizeForMessaging(SerializationHeader header, ColumnFilter selection, boolean hasStatic)
1:a991b64:         {
1:c055ab9:             long size = EncodingStats.serializer.serializedSize(header.stats);
1:a991b64: 
1:fe388d4:             if (selection == null)
1:a991b64:             {
1:a991b64:                 if (hasStatic)
1:fe388d4:                     size += Columns.serializer.serializedSize(header.columns.statics);
1:fe388d4:                 size += Columns.serializer.serializedSize(header.columns.regulars);
1:a991b64:             }
1:fe388d4:             else
1:a991b64:             {
1:fe388d4:                 if (hasStatic)
1:fe388d4:                     size += Columns.serializer.serializedSubsetSize(header.columns.statics, selection.fetchedColumns().statics);
1:fe388d4:                 size += Columns.serializer.serializedSubsetSize(header.columns.regulars, selection.fetchedColumns().regulars);
1:a991b64:             }
1:a991b64:             return size;
1:a991b64:         }
1:a991b64: 
1:a991b64:         // For SSTables
1:0600d7d:         public void serialize(Version version, Component header, DataOutputPlus out) throws IOException
1:a991b64:         {
1:c055ab9:             EncodingStats.serializer.serialize(header.stats, out);
1:a991b64: 
1:a991b64:             writeType(header.keyType, out);
1:649a106:             out.writeUnsignedVInt(header.clusteringTypes.size());
1:a991b64:             for (AbstractType<?> type : header.clusteringTypes)
1:a991b64:                 writeType(type, out);
1:a991b64: 
1:a991b64:             writeColumnsWithTypes(header.staticColumns, out);
1:a991b64:             writeColumnsWithTypes(header.regularColumns, out);
1:a991b64:         }
1:a991b64: 
1:a991b64:         // For SSTables
1:03f72ac:         public Component deserialize(Version version, DataInputPlus in) throws IOException
1:a991b64:         {
1:c055ab9:             EncodingStats stats = EncodingStats.serializer.deserialize(in);
1:a991b64: 
1:a991b64:             AbstractType<?> keyType = readType(in);
1:649a106:             int size = (int)in.readUnsignedVInt();
2:a991b64:             List<AbstractType<?>> clusteringTypes = new ArrayList<>(size);
2:a991b64:             for (int i = 0; i < size; i++)
1:a991b64:                 clusteringTypes.add(readType(in));
1:a991b64: 
1:a991b64:             Map<ByteBuffer, AbstractType<?>> staticColumns = new LinkedHashMap<>();
1:a991b64:             Map<ByteBuffer, AbstractType<?>> regularColumns = new LinkedHashMap<>();
1:a991b64: 
1:a991b64:             readColumnsWithType(in, staticColumns);
1:a991b64:             readColumnsWithType(in, regularColumns);
1:a991b64: 
1:a991b64:             return new Component(keyType, clusteringTypes, staticColumns, regularColumns, stats);
1:a991b64:         }
1:a991b64: 
1:a991b64:         // For SSTables
1:0600d7d:         public int serializedSize(Version version, Component header)
1:a991b64:         {
1:c055ab9:             int size = EncodingStats.serializer.serializedSize(header.stats);
1:a991b64: 
1:03f72ac:             size += sizeofType(header.keyType);
1:649a106:             size += TypeSizes.sizeofUnsignedVInt(header.clusteringTypes.size());
1:a991b64:             for (AbstractType<?> type : header.clusteringTypes)
1:03f72ac:                 size += sizeofType(type);
1:a991b64: 
1:03f72ac:             size += sizeofColumnsWithTypes(header.staticColumns);
1:03f72ac:             size += sizeofColumnsWithTypes(header.regularColumns);
1:a991b64:             return size;
1:a991b64:         }
1:a991b64: 
1:a991b64:         private void writeColumnsWithTypes(Map<ByteBuffer, AbstractType<?>> columns, DataOutputPlus out) throws IOException
1:a991b64:         {
1:649a106:             out.writeUnsignedVInt(columns.size());
1:a991b64:             for (Map.Entry<ByteBuffer, AbstractType<?>> entry : columns.entrySet())
1:a991b64:             {
1:a59be26:                 ByteBufferUtil.writeWithVIntLength(entry.getKey(), out);
1:a991b64:                 writeType(entry.getValue(), out);
1:a991b64:             }
1:a991b64:         }
1:a991b64: 
1:03f72ac:         private long sizeofColumnsWithTypes(Map<ByteBuffer, AbstractType<?>> columns)
1:a991b64:         {
1:649a106:             long size = TypeSizes.sizeofUnsignedVInt(columns.size());
1:a991b64:             for (Map.Entry<ByteBuffer, AbstractType<?>> entry : columns.entrySet())
1:a991b64:             {
1:a59be26:                 size += ByteBufferUtil.serializedSizeWithVIntLength(entry.getKey());
1:03f72ac:                 size += sizeofType(entry.getValue());
1:a991b64:             }
1:a991b64:             return size;
1:a991b64:         }
1:a991b64: 
1:2457599:         private void readColumnsWithType(DataInputPlus in, Map<ByteBuffer, AbstractType<?>> typeMap) throws IOException
1:a991b64:         {
1:649a106:             int length = (int)in.readUnsignedVInt();
1:a991b64:             for (int i = 0; i < length; i++)
1:a991b64:             {
1:a59be26:                 ByteBuffer name = ByteBufferUtil.readWithVIntLength(in);
1:a991b64:                 typeMap.put(name, readType(in));
1:a991b64:             }
1:a991b64:         }
1:a991b64: 
1:a991b64:         private void writeType(AbstractType<?> type, DataOutputPlus out) throws IOException
1:a991b64:         {
1:a991b64:             // TODO: we should have a terser serializaion format. Not a big deal though
1:a59be26:             ByteBufferUtil.writeWithVIntLength(UTF8Type.instance.decompose(type.toString()), out);
1:a991b64:         }
1:a991b64: 
1:2457599:         private AbstractType<?> readType(DataInputPlus in) throws IOException
1:a991b64:         {
1:a59be26:             ByteBuffer raw = ByteBufferUtil.readWithVIntLength(in);
1:a991b64:             return TypeParser.parse(UTF8Type.instance.compose(raw));
1:a991b64:         }
1:a991b64: 
1:03f72ac:         private int sizeofType(AbstractType<?> type)
1:a991b64:         {
1:a59be26:             return ByteBufferUtil.serializedSizeWithVIntLength(UTF8Type.instance.decompose(type.toString()));
1:a991b64:         }
1:a991b64:     }
6:fe388d4: }
============================================================================
author:Stefania Alborghetti
-------------------------------------------------------------------------------
commit:37551e9
commit:2f0e365
/////////////////////////////////////////////////////////////////////////
1:                     boolean isStatic = staticColumns.containsKey(name);
1:                     column = metadata.getDroppedColumnDefinition(name, isStatic);
author:T Jake Luciani
-------------------------------------------------------------------------------
commit:dc9ed46
/////////////////////////////////////////////////////////////////////////
/////////////////////////////////////////////////////////////////////////
1:              metadata.comparator.subtypes(),
/////////////////////////////////////////////////////////////////////////
1:             List<AbstractType<?>> clusteringTypes = metadata.comparator.subtypes();
author:Robert Stupp
-------------------------------------------------------------------------------
commit:ef5bbed
/////////////////////////////////////////////////////////////////////////
/////////////////////////////////////////////////////////////////////////
commit:0600d7d
/////////////////////////////////////////////////////////////////////////
1:         public void serialize(Version version, Component header, DataOutputPlus out) throws IOException
/////////////////////////////////////////////////////////////////////////
1:         public int serializedSize(Version version, Component header)
author:Paulo Motta
-------------------------------------------------------------------------------
commit:e8651b6
/////////////////////////////////////////////////////////////////////////
1:     public static SerializationHeader makeWithoutStats(CFMetaData metadata)
1:     {
1:         return new SerializationHeader(true, metadata, metadata.partitionColumns(), EncodingStats.NO_STATS);
1:     }
1: 
author:Sam Tunnicliffe
-------------------------------------------------------------------------------
commit:0a5e220
/////////////////////////////////////////////////////////////////////////
1:         public AbstractType<?> getKeyType()
author:Chris Lohfink
-------------------------------------------------------------------------------
commit:71b1c4a
/////////////////////////////////////////////////////////////////////////
1: 
0:         public AbstractType<?> getKetType()
1:         {
1:             return keyType;
1:         }
1: 
1:         public List<AbstractType<?>> getClusteringTypes()
1:         {
1:             return clusteringTypes;
1:         }
1: 
1:         public Map<ByteBuffer, AbstractType<?>> getStaticColumns()
1:         {
1:             return staticColumns;
1:         }
1: 
1:         public Map<ByteBuffer, AbstractType<?>> getRegularColumns()
1:         {
1:             return regularColumns;
1:         }
1: 
1:         public EncodingStats getEncodingStats()
1:         {
1:             return stats;
1:         }
author:Sylvain Lebresne
-------------------------------------------------------------------------------
commit:6584331
/////////////////////////////////////////////////////////////////////////
1:     private final boolean isForSSTable;
1: 
/////////////////////////////////////////////////////////////////////////
1:     private SerializationHeader(boolean isForSSTable,
1:                                 AbstractType<?> keyType,
1:         this.isForSSTable = isForSSTable;
/////////////////////////////////////////////////////////////////////////
0:         return new SerializationHeader(false,
0:                                        BytesType.instance,
/////////////////////////////////////////////////////////////////////////
1:         return new SerializationHeader(true, metadata, columns.build(), stats.get());
1:     public SerializationHeader(boolean isForSSTable,
1:                                CFMetaData metadata,
1:         this(isForSSTable,
1:              metadata.getKeyValidator(),
/////////////////////////////////////////////////////////////////////////
1:     public boolean isForSSTable()
1:     {
1:         return isForSSTable;
1:     }
1: 
/////////////////////////////////////////////////////////////////////////
1:             return new SerializationHeader(true, keyType, clusteringTypes, builder.build(), stats, typeMap);
/////////////////////////////////////////////////////////////////////////
1:             return new SerializationHeader(false, keyType, clusteringTypes, new PartitionColumns(statics, regulars), stats, null);
commit:a59be26
/////////////////////////////////////////////////////////////////////////
0:             out.writeVInt(header.clusteringTypes.size());
/////////////////////////////////////////////////////////////////////////
0:             int size = (int)in.readVInt();
/////////////////////////////////////////////////////////////////////////
0:             size += TypeSizes.sizeofVInt(header.clusteringTypes.size());
/////////////////////////////////////////////////////////////////////////
0:             out.writeVInt(columns.size());
1:                 ByteBufferUtil.writeWithVIntLength(entry.getKey(), out);
0:             long size = TypeSizes.sizeofVInt(columns.size());
1:                 size += ByteBufferUtil.serializedSizeWithVIntLength(entry.getKey());
/////////////////////////////////////////////////////////////////////////
0:             int length = (int)in.readVInt();
1:                 ByteBuffer name = ByteBufferUtil.readWithVIntLength(in);
/////////////////////////////////////////////////////////////////////////
1:             ByteBufferUtil.writeWithVIntLength(UTF8Type.instance.decompose(type.toString()), out);
1:             ByteBuffer raw = ByteBufferUtil.readWithVIntLength(in);
1:             return ByteBufferUtil.serializedSizeWithVIntLength(UTF8Type.instance.decompose(type.toString()));
commit:c055ab9
/////////////////////////////////////////////////////////////////////////
1:     private final EncodingStats stats;
1:                                 EncodingStats stats,
/////////////////////////////////////////////////////////////////////////
/////////////////////////////////////////////////////////////////////////
0:                                        EncodingStats.NO_STATS,
/////////////////////////////////////////////////////////////////////////
1:         // (for EncodingStats) since compaction may delete, purge and generally merge rows in unknown ways. This is
/////////////////////////////////////////////////////////////////////////
1:         EncodingStats.Collector stats = new EncodingStats.Collector();
/////////////////////////////////////////////////////////////////////////
1:                                EncodingStats stats)
/////////////////////////////////////////////////////////////////////////
1:     public EncodingStats stats()
/////////////////////////////////////////////////////////////////////////
1:     public void writeTimestamp(long timestamp, DataOutputPlus out) throws IOException
0:         out.writeVInt(timestamp - stats.minTimestamp);
1:     public void writeLocalDeletionTime(int localDeletionTime, DataOutputPlus out) throws IOException
0:         out.writeVInt(localDeletionTime - stats.minLocalDeletionTime);
1:     public void writeTTL(int ttl, DataOutputPlus out) throws IOException
0:         out.writeVInt(ttl - stats.minTTL);
1:     public void writeDeletionTime(DeletionTime dt, DataOutputPlus out) throws IOException
1:         writeTimestamp(dt.markedForDeleteAt(), out);
1:         writeLocalDeletionTime(dt.localDeletionTime(), out);
1:     public long readTimestamp(DataInputPlus in) throws IOException
0:         return in.readVInt() + stats.minTimestamp;
1:     public int readLocalDeletionTime(DataInputPlus in) throws IOException
0:         return (int)in.readVInt() + stats.minLocalDeletionTime;
1:     }
1: 
1:     public int readTTL(DataInputPlus in) throws IOException
1:     {
0:         return (int)in.readVInt() + stats.minTTL;
1:     }
1: 
1:     public DeletionTime readDeletionTime(DataInputPlus in) throws IOException
1:     {
1:         long markedAt = readTimestamp(in);
1:         int localDeletionTime = readLocalDeletionTime(in);
1:         return new DeletionTime(markedAt, localDeletionTime);
1:     }
1: 
1:     public long timestampSerializedSize(long timestamp)
1:     {
0:         return TypeSizes.sizeofVInt(timestamp - stats.minTimestamp);
1:     }
1: 
1:     public long localDeletionTimeSerializedSize(int localDeletionTime)
1:     {
0:         return TypeSizes.sizeofVInt(localDeletionTime - stats.minLocalDeletionTime);
1:     }
1: 
1:     public long ttlSerializedSize(int ttl)
1:     {
0:         return TypeSizes.sizeofVInt(ttl - stats.minTTL);
1:     }
1: 
1:     public long deletionTimeSerializedSize(DeletionTime dt)
1:     {
1:         return timestampSerializedSize(dt.markedForDeleteAt())
1:              + localDeletionTimeSerializedSize(dt.localDeletionTime());
1:     }
1: 
1:     public void skipTimestamp(DataInputPlus in) throws IOException
1:     {
0:         in.readVInt();
1:     }
1: 
1:     public void skipLocalDeletionTime(DataInputPlus in) throws IOException
1:     {
0:         in.readVInt();
1:     }
1: 
1:     public void skipTTL(DataInputPlus in) throws IOException
1:     {
0:         in.readVInt();
1:     }
1: 
1:     public void skipDeletionTime(DataInputPlus in) throws IOException
1:     {
1:         skipTimestamp(in);
1:         skipLocalDeletionTime(in);
/////////////////////////////////////////////////////////////////////////
1:         return String.format("SerializationHeader[key=%s, cks=%s, columns=%s, stats=%s, typeMap=%s]", keyType, clusteringTypes, columns, stats, typeMap);
/////////////////////////////////////////////////////////////////////////
1:         private final EncodingStats stats;
1:                           EncodingStats stats)
/////////////////////////////////////////////////////////////////////////
1:             EncodingStats.serializer.serialize(header.stats, out);
/////////////////////////////////////////////////////////////////////////
1:             EncodingStats stats = EncodingStats.serializer.deserialize(in);
/////////////////////////////////////////////////////////////////////////
1:             long size = EncodingStats.serializer.serializedSize(header.stats);
/////////////////////////////////////////////////////////////////////////
1:             EncodingStats.serializer.serialize(header.stats, out);
/////////////////////////////////////////////////////////////////////////
1:             EncodingStats stats = EncodingStats.serializer.deserialize(in);
/////////////////////////////////////////////////////////////////////////
1:             int size = EncodingStats.serializer.serializedSize(header.stats);
commit:2457599
/////////////////////////////////////////////////////////////////////////
/////////////////////////////////////////////////////////////////////////
0:         return !isStatic && useSparseColumnLayout;
/////////////////////////////////////////////////////////////////////////
0:         return ImmutableList.copyOf(Lists.transform(columns, column -> column.type));
/////////////////////////////////////////////////////////////////////////
0:         public SerializationHeader deserializeForMessaging(DataInputPlus in, CFMetaData metadata, boolean hasStatic) throws IOException
/////////////////////////////////////////////////////////////////////////
1:         private void readColumnsWithType(DataInputPlus in, Map<ByteBuffer, AbstractType<?>> typeMap) throws IOException
/////////////////////////////////////////////////////////////////////////
1:         private AbstractType<?> readType(DataInputPlus in) throws IOException
commit:a991b64
/////////////////////////////////////////////////////////////////////////
1: /*
1:  * Licensed to the Apache Software Foundation (ASF) under one
1:  * or more contributor license agreements.  See the NOTICE file
1:  * distributed with this work for additional information
1:  * regarding copyright ownership.  The ASF licenses this file
1:  * to you under the Apache License, Version 2.0 (the
1:  * "License"); you may not use this file except in compliance
1:  * with the License.  You may obtain a copy of the License at
1:  *
1:  *     http://www.apache.org/licenses/LICENSE-2.0
1:  *
1:  * Unless required by applicable law or agreed to in writing, software
1:  * distributed under the License is distributed on an "AS IS" BASIS,
1:  * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
1:  * See the License for the specific language governing permissions and
1:  * limitations under the License.
1:  */
1: package org.apache.cassandra.db;
1: 
0: import java.io.DataInput;
1: import java.io.IOException;
1: import java.nio.ByteBuffer;
1: import java.util.*;
1: 
0: import com.google.common.collect.ImmutableList;
0: import com.google.common.collect.Lists;
0: import com.google.common.base.Function;
1: 
1: import org.apache.cassandra.config.CFMetaData;
1: import org.apache.cassandra.config.ColumnDefinition;
1: import org.apache.cassandra.db.rows.*;
1: import org.apache.cassandra.db.marshal.AbstractType;
0: import org.apache.cassandra.db.marshal.BytesType;
1: import org.apache.cassandra.db.marshal.UTF8Type;
1: import org.apache.cassandra.db.marshal.TypeParser;
1: import org.apache.cassandra.io.sstable.format.SSTableReader;
1: import org.apache.cassandra.io.sstable.format.Version;
1: import org.apache.cassandra.io.sstable.metadata.MetadataType;
1: import org.apache.cassandra.io.sstable.metadata.MetadataComponent;
1: import org.apache.cassandra.io.sstable.metadata.IMetadataComponentSerializer;
1: import org.apache.cassandra.io.util.DataOutputPlus;
1: import org.apache.cassandra.utils.ByteBufferUtil;
1: 
1: public class SerializationHeader
1: {
0:     private static final int DEFAULT_BASE_DELETION = computeDefaultBaseDeletion();
1: 
1:     public static final Serializer serializer = new Serializer();
1: 
1:     private final AbstractType<?> keyType;
1:     private final List<AbstractType<?>> clusteringTypes;
1: 
1:     private final PartitionColumns columns;
0:     private final RowStats stats;
1: 
1:     private final Map<ByteBuffer, AbstractType<?>> typeMap;
1: 
0:     private final long baseTimestamp;
0:     public final int baseDeletionTime;
0:     private final int baseTTL;
1: 
0:     // Whether or not to store cell in a sparse or dense way. See UnfilteredSerializer for details.
0:     private final boolean useSparseColumnLayout;
1: 
0:     private SerializationHeader(AbstractType<?> keyType,
1:                                 List<AbstractType<?>> clusteringTypes,
1:                                 PartitionColumns columns,
0:                                 RowStats stats,
1:                                 Map<ByteBuffer, AbstractType<?>> typeMap)
1:     {
1:         this.keyType = keyType;
1:         this.clusteringTypes = clusteringTypes;
1:         this.columns = columns;
1:         this.stats = stats;
1:         this.typeMap = typeMap;
1: 
0:         // Not that if a given stats is unset, it means that either it's unused (there is
0:         // no tombstone whatsoever for instance) or that we have no information on it. In
0:         // that former case, it doesn't matter which base we use but in the former, we use
0:         // bases that are more likely to provide small encoded values than the default
0:         // "unset" value.
0:         this.baseTimestamp = stats.hasMinTimestamp() ? stats.minTimestamp : 0;
0:         this.baseDeletionTime = stats.hasMinLocalDeletionTime() ? stats.minLocalDeletionTime : DEFAULT_BASE_DELETION;
0:         this.baseTTL = stats.minTTL;
1: 
0:         // For the dense layout, we have a 1 byte overhead for absent columns. For the sparse layout, it's a 1
0:         // overhead for present columns (in fact we use a 2 byte id, but assuming vint encoding, we'll pay 2 bytes
0:         // only for the columns after the 128th one and for simplicity we assume that once you have that many column,
0:         // you'll tend towards a clearly dense or clearly sparse case so that the heurstic above shouldn't still be
0:         // too inapropriate). So if on average more than half of our columns are set per row, we better go for sparse.
0:         this.useSparseColumnLayout = stats.avgColumnSetPerRow <= (columns.regulars.columnCount()/ 2);
1:     }
1: 
0:     public boolean useSparseColumnLayout(boolean isStatic)
1:     {
0:         // We always use a dense layout for the static row. Having very many static columns with  only a few set at
0:         // any given time doesn't feel very common at all (and we already optimize the case where no static at all
0:         // are provided).
0:         return isStatic ? false : useSparseColumnLayout;
1:     }
1: 
0:     public static SerializationHeader forKeyCache(CFMetaData metadata)
1:     {
0:         // We don't save type information in the key cache (we could change
0:         // that but it's easier right now), so instead we simply use BytesType
0:         // for both serialization and deserialization. Note that we also only
0:         // serializer clustering prefixes in the key cache, so only the clusteringTypes
0:         // really matter.
0:         int size = metadata.clusteringColumns().size();
1:         List<AbstractType<?>> clusteringTypes = new ArrayList<>(size);
1:         for (int i = 0; i < size; i++)
0:             clusteringTypes.add(BytesType.instance);
0:         return new SerializationHeader(BytesType.instance,
0:                                        clusteringTypes,
0:                                        PartitionColumns.NONE,
0:                                        RowStats.NO_STATS,
0:                                        Collections.<ByteBuffer, AbstractType<?>>emptyMap());
1:     }
1: 
1:     public static SerializationHeader make(CFMetaData metadata, Collection<SSTableReader> sstables)
1:     {
1:         // The serialization header has to be computed before the start of compaction (since it's used to write)
1:         // the result. This means that when compacting multiple sources, we won't have perfectly accurate stats
0:         // (for RowStats) since compaction may delete, purge and generally merge rows in unknown ways. This is
1:         // kind of ok because those stats are only used for optimizing the underlying storage format and so we
1:         // just have to strive for as good as possible. Currently, we stick to a relatively naive merge of existing
1:         // global stats because it's simple and probably good enough in most situation but we could probably
1:         // improve our marging of inaccuracy through the use of more fine-grained stats in the future.
1:         // Note however that to avoid seeing our accuracy degrade through successive compactions, we don't base
1:         // our stats merging on the compacted files headers, which as we just said can be somewhat inaccurate,
1:         // but rather on their stats stored in StatsMetadata that are fully accurate.
0:         RowStats.Collector stats = new RowStats.Collector();
1:         PartitionColumns.Builder columns = PartitionColumns.builder();
1:         for (SSTableReader sstable : sstables)
1:         {
1:             stats.updateTimestamp(sstable.getMinTimestamp());
1:             stats.updateLocalDeletionTime(sstable.getMinLocalDeletionTime());
1:             stats.updateTTL(sstable.getMinTTL());
0:             stats.updateColumnSetPerRow(sstable.getTotalColumnsSet(), sstable.getTotalRows());
1:             if (sstable.header == null)
1:                 columns.addAll(metadata.partitionColumns());
1:             else
1:                 columns.addAll(sstable.header.columns());
1:         }
0:         return new SerializationHeader(metadata, columns.build(), stats.get());
1:     }
1: 
0:     public SerializationHeader(CFMetaData metadata,
1:                                PartitionColumns columns,
0:                                RowStats stats)
1:     {
0:         this(metadata.getKeyValidator(),
0:              typesOf(metadata.clusteringColumns()),
1:              columns,
1:              stats,
1:              null);
1:     }
1: 
0:     private static List<AbstractType<?>> typesOf(List<ColumnDefinition> columns)
1:     {
0:         return ImmutableList.copyOf(Lists.transform(columns, new Function<ColumnDefinition, AbstractType<?>>()
1:         {
0:             public AbstractType<?> apply(ColumnDefinition column)
1:             {
0:                 return column.type;
1:             }
0:         }));
1:     }
1: 
1:     public PartitionColumns columns()
1:     {
1:         return columns;
1:     }
1: 
1:     public boolean hasStatic()
1:     {
1:         return !columns.statics.isEmpty();
1:     }
1: 
0:     private static int computeDefaultBaseDeletion()
1:     {
0:         // We need a fixed default, but one that is likely to provide small values (close to 0) when
0:         // substracted to deletion times. Since deletion times are 'the current time in seconds', we
0:         // use as base Jan 1, 2015 (in seconds).
0:         Calendar c = Calendar.getInstance(TimeZone.getTimeZone("GMT-0"), Locale.US);
0:         c.set(Calendar.YEAR, 2015);
0:         c.set(Calendar.MONTH, Calendar.JANUARY);
0:         c.set(Calendar.DAY_OF_MONTH, 1);
0:         c.set(Calendar.HOUR_OF_DAY, 0);
0:         c.set(Calendar.MINUTE, 0);
0:         c.set(Calendar.SECOND, 0);
0:         c.set(Calendar.MILLISECOND, 0);
0:         return (int)(c.getTimeInMillis() / 1000);
1:     }
1: 
0:     public RowStats stats()
1:     {
1:         return stats;
1:     }
1: 
1:     public AbstractType<?> keyType()
1:     {
1:         return keyType;
1:     }
1: 
1:     public List<AbstractType<?>> clusteringTypes()
1:     {
1:         return clusteringTypes;
1:     }
1: 
1:     public Columns columns(boolean isStatic)
1:     {
1:         return isStatic ? columns.statics : columns.regulars;
1:     }
1: 
1:     public AbstractType<?> getType(ColumnDefinition column)
1:     {
1:         return typeMap == null ? column.type : typeMap.get(column.name.bytes);
1:     }
1: 
0:     public long encodeTimestamp(long timestamp)
1:     {
0:         return timestamp - baseTimestamp;
1:     }
1: 
0:     public long decodeTimestamp(long timestamp)
1:     {
0:         return baseTimestamp + timestamp;
1:     }
1: 
0:     public int encodeDeletionTime(int deletionTime)
1:     {
0:         return deletionTime - baseDeletionTime;
1:     }
1: 
0:     public int decodeDeletionTime(int deletionTime)
1:     {
0:         return baseDeletionTime + deletionTime;
1:     }
1: 
0:     public int encodeTTL(int ttl)
1:     {
0:         return ttl - baseTTL;
1:     }
1: 
0:     public int decodeTTL(int ttl)
1:     {
0:         return baseTTL + ttl;
1:     }
1: 
1:     public Component toComponent()
1:     {
1:         Map<ByteBuffer, AbstractType<?>> staticColumns = new LinkedHashMap<>();
1:         Map<ByteBuffer, AbstractType<?>> regularColumns = new LinkedHashMap<>();
1:         for (ColumnDefinition column : columns.statics)
1:             staticColumns.put(column.name.bytes, column.type);
1:         for (ColumnDefinition column : columns.regulars)
1:             regularColumns.put(column.name.bytes, column.type);
1:         return new Component(keyType, clusteringTypes, staticColumns, regularColumns, stats);
1:     }
1: 
1:     @Override
1:     public String toString()
1:     {
0:         return String.format("SerializationHeader[key=%s, cks=%s, columns=%s, stats=%s, typeMap=%s, baseTs=%d, baseDt=%s, baseTTL=%s]",
0:                              keyType, clusteringTypes, columns, stats, typeMap, baseTimestamp, baseDeletionTime, baseTTL);
1:     }
1: 
1:     /**
1:      * We need the CFMetadata to properly deserialize a SerializationHeader but it's clunky to pass that to
1:      * a SSTable component, so we use this temporary object to delay the actual need for the metadata.
1:      */
1:     public static class Component extends MetadataComponent
1:     {
1:         private final AbstractType<?> keyType;
1:         private final List<AbstractType<?>> clusteringTypes;
1:         private final Map<ByteBuffer, AbstractType<?>> staticColumns;
1:         private final Map<ByteBuffer, AbstractType<?>> regularColumns;
0:         private final RowStats stats;
1: 
1:         private Component(AbstractType<?> keyType,
1:                           List<AbstractType<?>> clusteringTypes,
1:                           Map<ByteBuffer, AbstractType<?>> staticColumns,
1:                           Map<ByteBuffer, AbstractType<?>> regularColumns,
0:                           RowStats stats)
1:         {
1:             this.keyType = keyType;
1:             this.clusteringTypes = clusteringTypes;
1:             this.staticColumns = staticColumns;
1:             this.regularColumns = regularColumns;
1:             this.stats = stats;
1:         }
1: 
1:         public MetadataType getType()
1:         {
1:             return MetadataType.HEADER;
1:         }
1: 
1:         public SerializationHeader toHeader(CFMetaData metadata)
1:         {
1:             Map<ByteBuffer, AbstractType<?>> typeMap = new HashMap<>(staticColumns.size() + regularColumns.size());
1:             typeMap.putAll(staticColumns);
1:             typeMap.putAll(regularColumns);
1: 
1:             PartitionColumns.Builder builder = PartitionColumns.builder();
1:             for (ByteBuffer name : typeMap.keySet())
1:             {
1:                 ColumnDefinition column = metadata.getColumnDefinition(name);
1:                 if (column == null)
1:                 {
1:                     // TODO: this imply we don't read data for a column we don't yet know about, which imply this is theoretically
1:                     // racy with column addition. Currently, it is up to the user to not write data before the schema has propagated
1:                     // and this is far from being the only place that has such problem in practice. This doesn't mean we shouldn't
1:                     // improve this.
1: 
1:                     // If we don't find the definition, it could be we have data for a dropped column, and we shouldn't
1:                     // fail deserialization because of that. So we grab a "fake" ColumnDefinition that ensure proper
1:                     // deserialization. The column will be ignore later on anyway.
0:                     column = metadata.getDroppedColumnDefinition(name);
1:                     if (column == null)
1:                         throw new RuntimeException("Unknown column " + UTF8Type.instance.getString(name) + " during deserialization");
1:                 }
1:                 builder.add(column);
1:             }
0:             return new SerializationHeader(keyType, clusteringTypes, builder.build(), stats, typeMap);
1:         }
1: 
1:         @Override
1:         public boolean equals(Object o)
1:         {
1:             if(!(o instanceof Component))
1:                 return false;
1: 
1:             Component that = (Component)o;
1:             return Objects.equals(this.keyType, that.keyType)
1:                 && Objects.equals(this.clusteringTypes, that.clusteringTypes)
1:                 && Objects.equals(this.staticColumns, that.staticColumns)
1:                 && Objects.equals(this.regularColumns, that.regularColumns)
1:                 && Objects.equals(this.stats, that.stats);
1:         }
1: 
1:         @Override
1:         public int hashCode()
1:         {
1:             return Objects.hash(keyType, clusteringTypes, staticColumns, regularColumns, stats);
1:         }
1: 
1:         @Override
1:         public String toString()
1:         {
1:             return String.format("SerializationHeader.Component[key=%s, cks=%s, statics=%s, regulars=%s, stats=%s]",
1:                                  keyType, clusteringTypes, staticColumns, regularColumns, stats);
1:         }
1:     }
1: 
1:     public static class Serializer implements IMetadataComponentSerializer<Component>
1:     {
0:         public void serializeForMessaging(SerializationHeader header, DataOutputPlus out, boolean hasStatic) throws IOException
1:         {
0:             RowStats.serializer.serialize(header.stats, out);
1: 
1:             if (hasStatic)
1:                 Columns.serializer.serialize(header.columns.statics, out);
1:             Columns.serializer.serialize(header.columns.regulars, out);
1:         }
1: 
0:         public SerializationHeader deserializeForMessaging(DataInput in, CFMetaData metadata, boolean hasStatic) throws IOException
1:         {
0:             RowStats stats = RowStats.serializer.deserialize(in);
1: 
1:             AbstractType<?> keyType = metadata.getKeyValidator();
0:             List<AbstractType<?>> clusteringTypes = typesOf(metadata.clusteringColumns());
1: 
0:             Columns statics = hasStatic ? Columns.serializer.deserialize(in, metadata) : Columns.NONE;
0:             Columns regulars = Columns.serializer.deserialize(in, metadata);
1: 
0:             return new SerializationHeader(keyType, clusteringTypes, new PartitionColumns(statics, regulars), stats, null);
1:         }
1: 
0:         public long serializedSizeForMessaging(SerializationHeader header, TypeSizes sizes, boolean hasStatic)
1:         {
0:             long size = RowStats.serializer.serializedSize(header.stats, sizes);
1: 
1:             if (hasStatic)
0:                 size += Columns.serializer.serializedSize(header.columns.statics, sizes);
0:             size += Columns.serializer.serializedSize(header.columns.regulars, sizes);
1:             return size;
1:         }
1: 
1:         // For SSTables
0:         public void serialize(Component header, DataOutputPlus out) throws IOException
1:         {
0:             RowStats.serializer.serialize(header.stats, out);
1: 
1:             writeType(header.keyType, out);
0:             out.writeShort(header.clusteringTypes.size());
1:             for (AbstractType<?> type : header.clusteringTypes)
1:                 writeType(type, out);
1: 
1:             writeColumnsWithTypes(header.staticColumns, out);
1:             writeColumnsWithTypes(header.regularColumns, out);
1:         }
1: 
1:         // For SSTables
0:         public Component deserialize(Version version, DataInput in) throws IOException
1:         {
0:             RowStats stats = RowStats.serializer.deserialize(in);
1: 
1:             AbstractType<?> keyType = readType(in);
0:             int size = in.readUnsignedShort();
1:             List<AbstractType<?>> clusteringTypes = new ArrayList<>(size);
1:             for (int i = 0; i < size; i++)
1:                 clusteringTypes.add(readType(in));
1: 
1:             Map<ByteBuffer, AbstractType<?>> staticColumns = new LinkedHashMap<>();
1:             Map<ByteBuffer, AbstractType<?>> regularColumns = new LinkedHashMap<>();
1: 
1:             readColumnsWithType(in, staticColumns);
1:             readColumnsWithType(in, regularColumns);
1: 
1:             return new Component(keyType, clusteringTypes, staticColumns, regularColumns, stats);
1:         }
1: 
1:         // For SSTables
0:         public int serializedSize(Component header)
1:         {
0:             TypeSizes sizes = TypeSizes.NATIVE;
0:             int size = RowStats.serializer.serializedSize(header.stats, sizes);
1: 
0:             size += sizeofType(header.keyType, sizes);
0:             size += sizes.sizeof((short)header.clusteringTypes.size());
1:             for (AbstractType<?> type : header.clusteringTypes)
0:                 size += sizeofType(type, sizes);
1: 
0:             size += sizeofColumnsWithTypes(header.staticColumns, sizes);
0:             size += sizeofColumnsWithTypes(header.regularColumns, sizes);
1:             return size;
1:         }
1: 
1:         private void writeColumnsWithTypes(Map<ByteBuffer, AbstractType<?>> columns, DataOutputPlus out) throws IOException
1:         {
0:             out.writeShort(columns.size());
1:             for (Map.Entry<ByteBuffer, AbstractType<?>> entry : columns.entrySet())
1:             {
0:                 ByteBufferUtil.writeWithShortLength(entry.getKey(), out);
1:                 writeType(entry.getValue(), out);
1:             }
1:         }
1: 
0:         private long sizeofColumnsWithTypes(Map<ByteBuffer, AbstractType<?>> columns, TypeSizes sizes)
1:         {
0:             long size = sizes.sizeof((short)columns.size());
1:             for (Map.Entry<ByteBuffer, AbstractType<?>> entry : columns.entrySet())
1:             {
0:                 size += sizes.sizeofWithShortLength(entry.getKey());
0:                 size += sizeofType(entry.getValue(), sizes);
1:             }
1:             return size;
1:         }
1: 
0:         private void readColumnsWithType(DataInput in, Map<ByteBuffer, AbstractType<?>> typeMap) throws IOException
1:         {
0:             int length = in.readUnsignedShort();
1:             for (int i = 0; i < length; i++)
1:             {
0:                 ByteBuffer name = ByteBufferUtil.readWithShortLength(in);
1:                 typeMap.put(name, readType(in));
1:             }
1:         }
1: 
1:         private void writeType(AbstractType<?> type, DataOutputPlus out) throws IOException
1:         {
1:             // TODO: we should have a terser serializaion format. Not a big deal though
0:             ByteBufferUtil.writeWithLength(UTF8Type.instance.decompose(type.toString()), out);
1:         }
1: 
0:         private AbstractType<?> readType(DataInput in) throws IOException
1:         {
0:             ByteBuffer raw = ByteBufferUtil.readWithLength(in);
1:             return TypeParser.parse(UTF8Type.instance.compose(raw));
1:         }
1: 
0:         private int sizeofType(AbstractType<?> type, TypeSizes sizes)
1:         {
0:             return sizes.sizeofWithLength(UTF8Type.instance.decompose(type.toString()));
1:         }
1:     }
1: }
author:Benedict Elliott Smith
-------------------------------------------------------------------------------
commit:649a106
/////////////////////////////////////////////////////////////////////////
1:         out.writeUnsignedVInt(timestamp - stats.minTimestamp);
1:         out.writeUnsignedVInt(localDeletionTime - stats.minLocalDeletionTime);
1:         out.writeUnsignedVInt(ttl - stats.minTTL);
/////////////////////////////////////////////////////////////////////////
1:         return in.readUnsignedVInt() + stats.minTimestamp;
1:         return (int)in.readUnsignedVInt() + stats.minLocalDeletionTime;
1:         return (int)in.readUnsignedVInt() + stats.minTTL;
/////////////////////////////////////////////////////////////////////////
1:         return TypeSizes.sizeofUnsignedVInt(timestamp - stats.minTimestamp);
1:         return TypeSizes.sizeofUnsignedVInt(localDeletionTime - stats.minLocalDeletionTime);
1:         return TypeSizes.sizeofUnsignedVInt(ttl - stats.minTTL);
/////////////////////////////////////////////////////////////////////////
1:         in.readUnsignedVInt();
1:         in.readUnsignedVInt();
1:         in.readUnsignedVInt();
/////////////////////////////////////////////////////////////////////////
1:             out.writeUnsignedVInt(header.clusteringTypes.size());
/////////////////////////////////////////////////////////////////////////
1:             int size = (int)in.readUnsignedVInt();
/////////////////////////////////////////////////////////////////////////
1:             size += TypeSizes.sizeofUnsignedVInt(header.clusteringTypes.size());
/////////////////////////////////////////////////////////////////////////
1:             out.writeUnsignedVInt(columns.size());
/////////////////////////////////////////////////////////////////////////
1:             long size = TypeSizes.sizeofUnsignedVInt(columns.size());
/////////////////////////////////////////////////////////////////////////
1:             int length = (int)in.readUnsignedVInt();
commit:aa57626
/////////////////////////////////////////////////////////////////////////
commit:00f0feb
/////////////////////////////////////////////////////////////////////////
commit:0d74c3e
/////////////////////////////////////////////////////////////////////////
/////////////////////////////////////////////////////////////////////////
commit:fe388d4
/////////////////////////////////////////////////////////////////////////
1: import org.apache.cassandra.db.filter.ColumnFilter;
/////////////////////////////////////////////////////////////////////////
1:         public void serializeForMessaging(SerializationHeader header, ColumnFilter selection, DataOutputPlus out, boolean hasStatic) throws IOException
1:             if (selection == null)
0:             {
1:                 if (hasStatic)
0:                     Columns.serializer.serialize(header.columns.statics, out);
0:                 Columns.serializer.serialize(header.columns.regulars, out);
1:             }
1:             else
0:             {
1:                 if (hasStatic)
1:                     Columns.serializer.serializeSubset(header.columns.statics, selection.fetchedColumns().statics, out);
1:                 Columns.serializer.serializeSubset(header.columns.regulars, selection.fetchedColumns().regulars, out);
1:             }
1:         public SerializationHeader deserializeForMessaging(DataInputPlus in, CFMetaData metadata, ColumnFilter selection, boolean hasStatic) throws IOException
1:             Columns statics, regulars;
1:             if (selection == null)
0:             {
1:                 statics = hasStatic ? Columns.serializer.deserialize(in, metadata) : Columns.NONE;
1:                 regulars = Columns.serializer.deserialize(in, metadata);
1:             }
1:             else
0:             {
1:                 statics = hasStatic ? Columns.serializer.deserializeSubset(selection.fetchedColumns().statics, in) : Columns.NONE;
1:                 regulars = Columns.serializer.deserializeSubset(selection.fetchedColumns().regulars, in);
1:             }
1:         public long serializedSizeForMessaging(SerializationHeader header, ColumnFilter selection, boolean hasStatic)
1:             if (selection == null)
0:             {
1:                 if (hasStatic)
1:                     size += Columns.serializer.serializedSize(header.columns.statics);
1:                 size += Columns.serializer.serializedSize(header.columns.regulars);
1:             }
1:             else
0:             {
1:                 if (hasStatic)
1:                     size += Columns.serializer.serializedSubsetSize(header.columns.statics, selection.fetchedColumns().statics);
1:                 size += Columns.serializer.serializedSubsetSize(header.columns.regulars, selection.fetchedColumns().regulars);
1:             }
author:Aleksey Yeschenko
-------------------------------------------------------------------------------
commit:2fea59d
/////////////////////////////////////////////////////////////////////////
0:             out.writeVInt(header.clusteringTypes.size());
/////////////////////////////////////////////////////////////////////////
0:             int size = (int)in.readVInt();
/////////////////////////////////////////////////////////////////////////
0:             size += TypeSizes.sizeofVInt(header.clusteringTypes.size());
/////////////////////////////////////////////////////////////////////////
0:             out.writeVInt(columns.size());
0:                 ByteBufferUtil.writeWithVIntLength(entry.getKey(), out);
0:             long size = TypeSizes.sizeofVInt(columns.size());
0:                 size += ByteBufferUtil.serializedSizeWithVIntLength(entry.getKey());
/////////////////////////////////////////////////////////////////////////
0:             int length = (int)in.readVInt();
0:                 ByteBuffer name = ByteBufferUtil.readWithVIntLength(in);
/////////////////////////////////////////////////////////////////////////
0:             ByteBufferUtil.writeWithVIntLength(UTF8Type.instance.decompose(type.toString()), out);
0:             ByteBuffer raw = ByteBufferUtil.readWithVIntLength(in);
0:             return ByteBufferUtil.serializedSizeWithVIntLength(UTF8Type.instance.decompose(type.toString()));
author:Jonathan Ellis
-------------------------------------------------------------------------------
commit:59a2861
/////////////////////////////////////////////////////////////////////////
0:             out.writeShort(header.clusteringTypes.size());
/////////////////////////////////////////////////////////////////////////
0:             int size = in.readUnsignedShort();
/////////////////////////////////////////////////////////////////////////
0:             size += TypeSizes.sizeof((short)header.clusteringTypes.size());
/////////////////////////////////////////////////////////////////////////
0:             out.writeShort(columns.size());
0:                 ByteBufferUtil.writeWithShortLength(entry.getKey(), out);
0:             long size = TypeSizes.sizeof((short)columns.size());
0:                 size += TypeSizes.sizeofWithShortLength(entry.getKey());
/////////////////////////////////////////////////////////////////////////
0:             int length = in.readUnsignedShort();
0:                 ByteBuffer name = ByteBufferUtil.readWithShortLength(in);
/////////////////////////////////////////////////////////////////////////
0:             ByteBufferUtil.writeWithLength(UTF8Type.instance.decompose(type.toString()), out);
0:             ByteBuffer raw = ByteBufferUtil.readWithLength(in);
0:             return TypeSizes.sizeofWithLength(UTF8Type.instance.decompose(type.toString()));
author:Ariel Weisberg
-------------------------------------------------------------------------------
commit:03f72ac
/////////////////////////////////////////////////////////////////////////
1: import org.apache.cassandra.io.util.DataInputPlus;
/////////////////////////////////////////////////////////////////////////
0:         public long serializedSizeForMessaging(SerializationHeader header, boolean hasStatic)
0:             long size = RowStats.serializer.serializedSize(header.stats);
0:                 size += Columns.serializer.serializedSize(header.columns.statics);
0:             size += Columns.serializer.serializedSize(header.columns.regulars);
/////////////////////////////////////////////////////////////////////////
1:         public Component deserialize(Version version, DataInputPlus in) throws IOException
/////////////////////////////////////////////////////////////////////////
0:             int size = RowStats.serializer.serializedSize(header.stats);
1:             size += sizeofType(header.keyType);
0:             size += TypeSizes.sizeof((short)header.clusteringTypes.size());
1:                 size += sizeofType(type);
1:             size += sizeofColumnsWithTypes(header.staticColumns);
1:             size += sizeofColumnsWithTypes(header.regularColumns);
/////////////////////////////////////////////////////////////////////////
1:         private long sizeofColumnsWithTypes(Map<ByteBuffer, AbstractType<?>> columns)
0:             long size = TypeSizes.sizeof((short)columns.size());
0:                 size += TypeSizes.sizeofWithShortLength(entry.getKey());
1:                 size += sizeofType(entry.getValue());
/////////////////////////////////////////////////////////////////////////
1:         private int sizeofType(AbstractType<?> type)
0:             return TypeSizes.sizeofWithLength(UTF8Type.instance.decompose(type.toString()));
============================================================================