1:4e17ac4: /*
1:4e17ac4:  * Licensed to the Apache Software Foundation (ASF) under one
1:4e17ac4:  * or more contributor license agreements.  See the NOTICE file
1:4e17ac4:  * distributed with this work for additional information
1:4e17ac4:  * regarding copyright ownership.  The ASF licenses this file
1:4e17ac4:  * to you under the Apache License, Version 2.0 (the
1:4e17ac4:  * "License"); you may not use this file except in compliance
1:4e17ac4:  * with the License.  You may obtain a copy of the License at
3:4e17ac4:  *
1:4e17ac4:  *     http://www.apache.org/licenses/LICENSE-2.0
1:4e17ac4:  *
1:4e17ac4:  * Unless required by applicable law or agreed to in writing, software
1:4e17ac4:  * distributed under the License is distributed on an "AS IS" BASIS,
1:4e17ac4:  * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
1:4e17ac4:  * See the License for the specific language governing permissions and
1:4e17ac4:  * limitations under the License.
4:4e17ac4:  */
1:ef5bbed: 
1:4e17ac4: package org.apache.cassandra.db;
1:f81a91d: 
1:4d34917: import java.io.IOException;
1:ef5bbed: import java.nio.ByteBuffer;
1:4d34917: import java.util.*;
4:a991b64: 
1:ef5bbed: import com.google.common.primitives.Ints;
1:a991b64: 
1:4e17ac4: import org.apache.cassandra.config.DatabaseDescriptor;
1:a991b64: import org.apache.cassandra.db.rows.*;
1:ef5bbed: import org.apache.cassandra.io.ISerializer;
1:ef5bbed: import org.apache.cassandra.io.sstable.IndexInfo;
1:f81a91d: import org.apache.cassandra.io.sstable.format.SSTableFlushObserver;
1:a991b64: import org.apache.cassandra.io.sstable.format.Version;
1:ef5bbed: import org.apache.cassandra.io.util.DataOutputBuffer;
1:a991b64: import org.apache.cassandra.io.util.SequentialWriter;
1:a9bd531: import org.apache.cassandra.utils.ByteBufferUtil;
1:a991b64: 
1:ef5bbed: /**
1:ef5bbed:  * Column index builder used by {@link org.apache.cassandra.io.sstable.format.big.BigTableWriter}.
1:ef5bbed:  * For index entries that exceed {@link org.apache.cassandra.config.Config#column_index_cache_size_in_kb},
1:ef5bbed:  * this uses the serialization logic as in {@link RowIndexEntry}.
1:ef5bbed:  */
1:4e17ac4: public class ColumnIndex
1:a991b64: {
1:ef5bbed:     // used, if the row-index-entry reaches config column_index_cache_size_in_kb
1:ef5bbed:     private DataOutputBuffer buffer;
1:ef5bbed:     // used to track the size of the serialized size of row-index-entry (unused for buffer)
1:ef5bbed:     private int indexSamplesSerializedSize;
1:ef5bbed:     // used, until the row-index-entry reaches config column_index_cache_size_in_kb
1:1e92ce4:     private final List<IndexInfo> indexSamples = new ArrayList<>();
1:6584331: 
1:ae4d705:     private DataOutputBuffer reusableBuffer;
1:ae4d705: 
1:ef5bbed:     public int columnIndexCount;
1:ef5bbed:     private int[] indexOffsets;
1:2457599: 
1:ef5bbed:     private final SerializationHeader header;
1:ef5bbed:     private final int version;
1:ef5bbed:     private final SequentialWriter writer;
1:1e92ce4:     private long initialPosition;
1:1e92ce4:     private final  ISerializer<IndexInfo> idxSerializer;
1:1e92ce4:     public long headerLength;
1:1e92ce4:     private long startPosition;
1:60e45c0: 
1:ef5bbed:     private int written;
1:ef5bbed:     private long previousRowStart;
1:a991b64: 
1:ef5bbed:     private ClusteringPrefix firstClustering;
1:ef5bbed:     private ClusteringPrefix lastClustering;
1:a991b64: 
1:ef5bbed:     private DeletionTime openMarker;
1:a991b64: 
1:ef5bbed:     private final Collection<SSTableFlushObserver> observers;
1:f81a91d: 
1:ef5bbed:     public ColumnIndex(SerializationHeader header,
1:1e92ce4:                         SequentialWriter writer,
1:ef5bbed:                         Version version,
1:1e92ce4:                         Collection<SSTableFlushObserver> observers,
1:ef5bbed:                         ISerializer<IndexInfo> indexInfoSerializer)
1:ef5bbed:     {
1:ef5bbed:         this.header = header;
1:ef5bbed:         this.writer = writer;
1:ef5bbed:         this.version = version.correspondingMessagingVersion();
1:ef5bbed:         this.observers = observers;
1:ef5bbed:         this.idxSerializer = indexInfoSerializer;
1:1e92ce4:     }
1:1e92ce4: 
1:1e92ce4:     public void reset()
1:68d2526:     {
1:ef5bbed:         this.initialPosition = writer.position();
1:1e92ce4:         this.headerLength = -1;
1:1e92ce4:         this.startPosition = -1;
1:1e92ce4:         this.previousRowStart = 0;
1:1e92ce4:         this.columnIndexCount = 0;
1:1e92ce4:         this.written = 0;
1:1e92ce4:         this.indexSamplesSerializedSize = 0;
1:1e92ce4:         this.indexSamples.clear();
1:1e92ce4:         this.firstClustering = null;
1:1e92ce4:         this.lastClustering = null;
1:1e92ce4:         this.openMarker = null;
1:ae4d705:         if (this.buffer != null)
1:ae4d705:             this.reusableBuffer = this.buffer;
1:1e92ce4:         this.buffer = null;
1:ef5bbed:     }
1:ef5bbed: 
1:ef5bbed:     public void buildRowIndex(UnfilteredRowIterator iterator) throws IOException
1:ef5bbed:     {
1:ef5bbed:         writePartitionHeader(iterator);
1:ef5bbed:         this.headerLength = writer.position() - initialPosition;
1:ef5bbed: 
1:ef5bbed:         while (iterator.hasNext())
1:ef5bbed:             add(iterator.next());
1:ef5bbed: 
1:1e92ce4:         finish();
1:ef5bbed:     }
1:ef5bbed: 
1:ef5bbed:     private void writePartitionHeader(UnfilteredRowIterator iterator) throws IOException
1:ef5bbed:     {
1:ef5bbed:         ByteBufferUtil.writeWithShortLength(iterator.partitionKey().getKey(), writer);
1:ef5bbed:         DeletionTime.serializer.serialize(iterator.partitionLevelDeletion(), writer);
1:ef5bbed:         if (header.hasStatic())
1:1e92ce4:         {
1:ef5bbed:             Row staticRow = iterator.staticRow();
1:b7d1d44: 
1:7d857b4:             long startPosition = currentPosition();
1:ef5bbed:             UnfilteredSerializer.serializer.serializeStaticRow(staticRow, header, writer, version);
1:b7d1d44:             if (!observers.isEmpty())
1:7d857b4:                 observers.forEach((o) -> o.nextUnfilteredCluster(staticRow, startPosition));
1:ef5bbed:         }
1:b7d1d44:     }
1:f81a91d: 
1:ef5bbed:     private long currentPosition()
1:ef5bbed:     {
1:ef5bbed:         return writer.position() - initialPosition;
1:ef5bbed:     }
1:ef5bbed: 
1:ef5bbed:     public ByteBuffer buffer()
1:ef5bbed:     {
1:ef5bbed:         return buffer != null ? buffer.buffer() : null;
1:ef5bbed:     }
1:ef5bbed: 
1:1e92ce4:     public List<IndexInfo> indexSamples()
1:1e92ce4:     {
1:1e92ce4:         if (indexSamplesSerializedSize + columnIndexCount * TypeSizes.sizeof(0) <= DatabaseDescriptor.getColumnIndexCacheSize())
1:1e92ce4:         {
1:1e92ce4:             return indexSamples;
1:1e92ce4:         }
1:1e92ce4: 
1:1e92ce4:         return null;
1:1e92ce4:     }
1:1e92ce4: 
1:ef5bbed:     public int[] offsets()
1:ef5bbed:     {
1:ef5bbed:         return indexOffsets != null
1:ef5bbed:                ? Arrays.copyOf(indexOffsets, columnIndexCount)
1:ef5bbed:                : null;
1:ef5bbed:     }
1:ef5bbed: 
1:ef5bbed:     private void addIndexBlock() throws IOException
1:ef5bbed:     {
1:ef5bbed:         IndexInfo cIndexInfo = new IndexInfo(firstClustering,
1:ef5bbed:                                              lastClustering,
1:ef5bbed:                                              startPosition,
1:ef5bbed:                                              currentPosition() - startPosition,
1:ef5bbed:                                              openMarker);
1:ef5bbed: 
1:ef5bbed:         // indexOffsets is used for both shallow (ShallowIndexedEntry) and non-shallow IndexedEntry.
1:1e92ce4:         // For shallow ones, we need it to serialize the offsts in finish().
1:ef5bbed:         // For non-shallow ones, the offsts are passed into IndexedEntry, so we don't have to
1:ef5bbed:         // calculate the offsets again.
1:ef5bbed: 
1:ef5bbed:         // indexOffsets contains the offsets of the serialized IndexInfo objects.
1:ef5bbed:         // I.e. indexOffsets[0] is always 0 so we don't have to deal with a special handling
1:ef5bbed:         // for index #0 and always subtracting 1 for the index (which could be error-prone).
1:ef5bbed:         if (indexOffsets == null)
1:ef5bbed:             indexOffsets = new int[10];
1:ef5bbed:         else
1:b7d1d44:         {
1:ef5bbed:             if (columnIndexCount >= indexOffsets.length)
1:ef5bbed:                 indexOffsets = Arrays.copyOf(indexOffsets, indexOffsets.length + 10);
1:1e92ce4: 
1:1e92ce4:             //the 0th element is always 0
1:1e92ce4:             if (columnIndexCount == 0)
1:1e92ce4:             {
1:1e92ce4:                 indexOffsets[columnIndexCount] = 0;
1:1e92ce4:             }
1:1e92ce4:             else
1:1e92ce4:             {
1:ef5bbed:                 indexOffsets[columnIndexCount] =
1:ef5bbed:                 buffer != null
1:ef5bbed:                 ? Ints.checkedCast(buffer.position())
1:ef5bbed:                 : indexSamplesSerializedSize;
1:1e92ce4:             }
1:a991b64:         }
1:ef5bbed:         columnIndexCount++;
1:ef5bbed: 
1:ef5bbed:         // First, we collect the IndexInfo objects until we reach Config.column_index_cache_size_in_kb in an ArrayList.
1:ef5bbed:         // When column_index_cache_size_in_kb is reached, we switch to byte-buffer mode.
1:ef5bbed:         if (buffer == null)
1:ef5bbed:         {
1:ef5bbed:             indexSamplesSerializedSize += idxSerializer.serializedSize(cIndexInfo);
1:ef5bbed:             if (indexSamplesSerializedSize + columnIndexCount * TypeSizes.sizeof(0) > DatabaseDescriptor.getColumnIndexCacheSize())
1:ef5bbed:             {
1:c2106e1:                 buffer = reuseOrAllocateBuffer();
1:ef5bbed:                 for (IndexInfo indexSample : indexSamples)
1:ef5bbed:                 {
1:ef5bbed:                     idxSerializer.serialize(indexSample, buffer);
1:ef5bbed:                 }
1:ef5bbed:             }
1:ef5bbed:             else
1:ef5bbed:             {
1:ef5bbed:                 indexSamples.add(cIndexInfo);
1:ef5bbed:             }
1:ef5bbed:         }
1:ef5bbed:         // don't put an else here...
1:ef5bbed:         if (buffer != null)
1:ef5bbed:         {
1:ef5bbed:             idxSerializer.serialize(cIndexInfo, buffer);
1:ef5bbed:         }
1:ef5bbed: 
1:ef5bbed:         firstClustering = null;
1:ef5bbed:     }
1:ef5bbed: 
1:c2106e1:     private DataOutputBuffer reuseOrAllocateBuffer()
1:ae4d705:     {
1:c2106e1:         // Check whether a reusable DataOutputBuffer already exists for this
1:c2106e1:         // ColumnIndex instance and return it.
1:c2106e1:         if (reusableBuffer != null) {
1:c2106e1:             DataOutputBuffer buffer = reusableBuffer;
1:ae4d705:             buffer.clear();
1:c2106e1:             return buffer;
1:ae4d705:         }
1:ae4d705:         // don't use the standard RECYCLER as that only recycles up to 1MB and requires proper cleanup
1:ae4d705:         return new DataOutputBuffer(DatabaseDescriptor.getColumnIndexCacheSize() * 2);
1:ae4d705:     }
1:ae4d705: 
1:ef5bbed:     private void add(Unfiltered unfiltered) throws IOException
1:ef5bbed:     {
1:7d857b4:         final long origPos = writer.position();
1:ef5bbed:         long pos = currentPosition();
1:ef5bbed: 
1:ef5bbed:         if (firstClustering == null)
1:ef5bbed:         {
1:ef5bbed:             // Beginning of an index block. Remember the start and position
1:ef5bbed:             firstClustering = unfiltered.clustering();
1:ef5bbed:             startPosition = pos;
1:ef5bbed:         }
1:ef5bbed: 
1:ef5bbed:         UnfilteredSerializer.serializer.serialize(unfiltered, header, writer, pos - previousRowStart, version);
1:ef5bbed: 
1:ef5bbed:         // notify observers about each new row
1:ef5bbed:         if (!observers.isEmpty())
1:7d857b4:             observers.forEach((o) -> o.nextUnfilteredCluster(unfiltered, origPos));
1:ef5bbed: 
1:ef5bbed:         lastClustering = unfiltered.clustering();
1:ef5bbed:         previousRowStart = pos;
1:ef5bbed:         ++written;
1:ef5bbed: 
1:ef5bbed:         if (unfiltered.kind() == Unfiltered.Kind.RANGE_TOMBSTONE_MARKER)
1:ef5bbed:         {
1:ef5bbed:             RangeTombstoneMarker marker = (RangeTombstoneMarker) unfiltered;
1:ef5bbed:             openMarker = marker.isOpen(false) ? marker.openDeletionTime(false) : null;
1:ef5bbed:         }
1:ef5bbed: 
1:ef5bbed:         // if we hit the column index size that we have to index after, go ahead and index it.
1:ef5bbed:         if (currentPosition() - startPosition >= DatabaseDescriptor.getColumnIndexSize())
1:ef5bbed:             addIndexBlock();
1:ef5bbed:     }
1:ef5bbed: 
1:1e92ce4:     private void finish() throws IOException
1:ef5bbed:     {
1:ef5bbed:         UnfilteredSerializer.serializer.writeEndOfPartition(writer);
1:ef5bbed: 
1:ef5bbed:         // It's possible we add no rows, just a top level deletion
1:ef5bbed:         if (written == 0)
1:ef5bbed:             return;
1:ef5bbed: 
1:ef5bbed:         // the last column may have fallen on an index boundary already.  if not, index it explicitly.
1:ef5bbed:         if (firstClustering != null)
1:ef5bbed:             addIndexBlock();
1:ef5bbed: 
1:ef5bbed:         // If we serialize the IndexInfo objects directly in the code above into 'buffer',
1:ef5bbed:         // we have to write the offsts to these here. The offsets have already been are collected
1:ef5bbed:         // in indexOffsets[]. buffer is != null, if it exceeds Config.column_index_cache_size_in_kb.
1:ef5bbed:         // In the other case, when buffer==null, the offsets are serialized in RowIndexEntry.IndexedEntry.serialize().
1:ef5bbed:         if (buffer != null)
1:ef5bbed:             RowIndexEntry.Serializer.serializeOffsets(buffer, indexOffsets, columnIndexCount);
1:ef5bbed: 
1:ef5bbed:         // we should always have at least one computed index block, but we only write it out if there is more than that.
1:ef5bbed:         assert columnIndexCount > 0 && headerLength >= 0;
1:ef5bbed:     }
1:ef5bbed: 
1:ef5bbed:     public int indexInfoSerializedSize()
1:ef5bbed:     {
1:ef5bbed:         return buffer != null
1:ef5bbed:                ? buffer.buffer().limit()
1:ef5bbed:                : indexSamplesSerializedSize + columnIndexCount * TypeSizes.sizeof(0);
1:a991b64:     }
2:4d34917: }
============================================================================
author:Robert Stupp
-------------------------------------------------------------------------------
commit:c2106e1
/////////////////////////////////////////////////////////////////////////
1:                 buffer = reuseOrAllocateBuffer();
/////////////////////////////////////////////////////////////////////////
1:     private DataOutputBuffer reuseOrAllocateBuffer()
1:         // Check whether a reusable DataOutputBuffer already exists for this
1:         // ColumnIndex instance and return it.
1:         if (reusableBuffer != null) {
1:             DataOutputBuffer buffer = reusableBuffer;
1:             return buffer;
commit:ae4d705
/////////////////////////////////////////////////////////////////////////
1:     private DataOutputBuffer reusableBuffer;
1: 
/////////////////////////////////////////////////////////////////////////
1:         if (this.buffer != null)
1:             this.reusableBuffer = this.buffer;
/////////////////////////////////////////////////////////////////////////
0:                 buffer = useBuffer();
/////////////////////////////////////////////////////////////////////////
0:     private DataOutputBuffer useBuffer()
1:     {
0:         if (reusableBuffer != null) {
0:             buffer = reusableBuffer;
1:             buffer.clear();
1:         }
1:         // don't use the standard RECYCLER as that only recycles up to 1MB and requires proper cleanup
1:         return new DataOutputBuffer(DatabaseDescriptor.getColumnIndexCacheSize() * 2);
1:     }
1: 
commit:ef5bbed
/////////////////////////////////////////////////////////////////////////
1: 
1: import java.nio.ByteBuffer;
1: import com.google.common.primitives.Ints;
1: import org.apache.cassandra.io.ISerializer;
1: import org.apache.cassandra.io.sstable.IndexInfo;
1: import org.apache.cassandra.io.util.DataOutputBuffer;
1: /**
1:  * Column index builder used by {@link org.apache.cassandra.io.sstable.format.big.BigTableWriter}.
1:  * For index entries that exceed {@link org.apache.cassandra.config.Config#column_index_cache_size_in_kb},
1:  * this uses the serialization logic as in {@link RowIndexEntry}.
1:  */
1:     // used, if the row-index-entry reaches config column_index_cache_size_in_kb
1:     private DataOutputBuffer buffer;
1:     // used to track the size of the serialized size of row-index-entry (unused for buffer)
1:     private int indexSamplesSerializedSize;
1:     // used, until the row-index-entry reaches config column_index_cache_size_in_kb
0:     public List<IndexInfo> indexSamples = new ArrayList<>();
1:     public int columnIndexCount;
1:     private int[] indexOffsets;
1:     private final SerializationHeader header;
1:     private final int version;
1:     private final SequentialWriter writer;
0:     private final long initialPosition;
0:     private final ISerializer<IndexInfo> idxSerializer;
0:     public long headerLength = -1;
0:     private long startPosition = -1;
1:     private int written;
1:     private long previousRowStart;
1:     private ClusteringPrefix firstClustering;
1:     private ClusteringPrefix lastClustering;
1:     private DeletionTime openMarker;
1:     private final Collection<SSTableFlushObserver> observers;
1:     public ColumnIndex(SerializationHeader header,
1:                        Version version,
1:                        ISerializer<IndexInfo> indexInfoSerializer)
1:     {
1:         this.header = header;
1:         this.idxSerializer = indexInfoSerializer;
1:         this.writer = writer;
1:         this.version = version.correspondingMessagingVersion();
1:         this.observers = observers;
1:         this.initialPosition = writer.position();
1:     }
1: 
1:     public void buildRowIndex(UnfilteredRowIterator iterator) throws IOException
1:     {
1:         writePartitionHeader(iterator);
1:         this.headerLength = writer.position() - initialPosition;
1: 
1:         while (iterator.hasNext())
1:             add(iterator.next());
1: 
0:         close();
1:     }
1: 
1:     private void writePartitionHeader(UnfilteredRowIterator iterator) throws IOException
1:     {
1:         ByteBufferUtil.writeWithShortLength(iterator.partitionKey().getKey(), writer);
1:         DeletionTime.serializer.serialize(iterator.partitionLevelDeletion(), writer);
1:         if (header.hasStatic())
1:             Row staticRow = iterator.staticRow();
1:             UnfilteredSerializer.serializer.serializeStaticRow(staticRow, header, writer, version);
0:                 observers.forEach((o) -> o.nextUnfilteredCluster(staticRow));
1:     }
1:     private long currentPosition()
1:     {
1:         return writer.position() - initialPosition;
1:     }
1: 
1:     public ByteBuffer buffer()
1:     {
1:         return buffer != null ? buffer.buffer() : null;
1:     }
1: 
1:     public int[] offsets()
1:     {
1:         return indexOffsets != null
1:                ? Arrays.copyOf(indexOffsets, columnIndexCount)
1:                : null;
1:     }
1: 
1:     private void addIndexBlock() throws IOException
1:     {
1:         IndexInfo cIndexInfo = new IndexInfo(firstClustering,
1:                                              lastClustering,
1:                                              startPosition,
1:                                              currentPosition() - startPosition,
1:                                              openMarker);
1: 
1:         // indexOffsets is used for both shallow (ShallowIndexedEntry) and non-shallow IndexedEntry.
0:         // For shallow ones, we need it to serialize the offsts in close().
1:         // For non-shallow ones, the offsts are passed into IndexedEntry, so we don't have to
1:         // calculate the offsets again.
1: 
1:         // indexOffsets contains the offsets of the serialized IndexInfo objects.
1:         // I.e. indexOffsets[0] is always 0 so we don't have to deal with a special handling
1:         // for index #0 and always subtracting 1 for the index (which could be error-prone).
1:         if (indexOffsets == null)
1:             indexOffsets = new int[10];
1:         else
1:             if (columnIndexCount >= indexOffsets.length)
1:                 indexOffsets = Arrays.copyOf(indexOffsets, indexOffsets.length + 10);
1:             indexOffsets[columnIndexCount] =
1:                 buffer != null
1:                     ? Ints.checkedCast(buffer.position())
1:                     : indexSamplesSerializedSize;
1:         columnIndexCount++;
1: 
1:         // First, we collect the IndexInfo objects until we reach Config.column_index_cache_size_in_kb in an ArrayList.
1:         // When column_index_cache_size_in_kb is reached, we switch to byte-buffer mode.
1:         if (buffer == null)
1:         {
1:             indexSamplesSerializedSize += idxSerializer.serializedSize(cIndexInfo);
1:             if (indexSamplesSerializedSize + columnIndexCount * TypeSizes.sizeof(0) > DatabaseDescriptor.getColumnIndexCacheSize())
1:             {
0:                 buffer = new DataOutputBuffer(DatabaseDescriptor.getColumnIndexCacheSize() * 2);
1:                 for (IndexInfo indexSample : indexSamples)
1:                 {
1:                     idxSerializer.serialize(indexSample, buffer);
1:                 }
0:                 indexSamples = null;
1:             }
1:             else
1:             {
1:                 indexSamples.add(cIndexInfo);
1:             }
1:         }
1:         // don't put an else here...
1:         if (buffer != null)
1:         {
1:             idxSerializer.serialize(cIndexInfo, buffer);
1:         }
1: 
1:         firstClustering = null;
1:     }
1: 
1:     private void add(Unfiltered unfiltered) throws IOException
1:     {
1:         long pos = currentPosition();
1: 
1:         if (firstClustering == null)
1:         {
1:             // Beginning of an index block. Remember the start and position
1:             firstClustering = unfiltered.clustering();
1:             startPosition = pos;
1:         }
1: 
1:         UnfilteredSerializer.serializer.serialize(unfiltered, header, writer, pos - previousRowStart, version);
1: 
1:         // notify observers about each new row
1:         if (!observers.isEmpty())
0:             observers.forEach((o) -> o.nextUnfilteredCluster(unfiltered));
1: 
1:         lastClustering = unfiltered.clustering();
1:         previousRowStart = pos;
1:         ++written;
1: 
1:         if (unfiltered.kind() == Unfiltered.Kind.RANGE_TOMBSTONE_MARKER)
1:         {
1:             RangeTombstoneMarker marker = (RangeTombstoneMarker) unfiltered;
1:             openMarker = marker.isOpen(false) ? marker.openDeletionTime(false) : null;
1:         }
1: 
1:         // if we hit the column index size that we have to index after, go ahead and index it.
1:         if (currentPosition() - startPosition >= DatabaseDescriptor.getColumnIndexSize())
1:             addIndexBlock();
1:     }
1: 
0:     private void close() throws IOException
1:     {
1:         UnfilteredSerializer.serializer.writeEndOfPartition(writer);
1: 
1:         // It's possible we add no rows, just a top level deletion
1:         if (written == 0)
1:             return;
1: 
1:         // the last column may have fallen on an index boundary already.  if not, index it explicitly.
1:         if (firstClustering != null)
1:             addIndexBlock();
1: 
1:         // If we serialize the IndexInfo objects directly in the code above into 'buffer',
1:         // we have to write the offsts to these here. The offsets have already been are collected
1:         // in indexOffsets[]. buffer is != null, if it exceeds Config.column_index_cache_size_in_kb.
1:         // In the other case, when buffer==null, the offsets are serialized in RowIndexEntry.IndexedEntry.serialize().
1:         if (buffer != null)
1:             RowIndexEntry.Serializer.serializeOffsets(buffer, indexOffsets, columnIndexCount);
1: 
1:         // we should always have at least one computed index block, but we only write it out if there is more than that.
1:         assert columnIndexCount > 0 && headerLength >= 0;
1:     }
1: 
1:     public int indexInfoSerializedSize()
1:     {
1:         return buffer != null
1:                ? buffer.buffer().limit()
1:                : indexSamplesSerializedSize + columnIndexCount * TypeSizes.sizeof(0);
commit:753a943
/////////////////////////////////////////////////////////////////////////
0:             this.initialPosition = writer.position();
/////////////////////////////////////////////////////////////////////////
0:             this.headerLength = writer.position() - initialPosition;
/////////////////////////////////////////////////////////////////////////
0:             return writer.position() - initialPosition;
commit:51b1a1c
/////////////////////////////////////////////////////////////////////////
0:                                                                          startPosition,
author:Alex Petrov
-------------------------------------------------------------------------------
commit:7d857b4
/////////////////////////////////////////////////////////////////////////
1:             long startPosition = currentPosition();
1:                 observers.forEach((o) -> o.nextUnfilteredCluster(staticRow, startPosition));
/////////////////////////////////////////////////////////////////////////
1:         final long origPos = writer.position();
/////////////////////////////////////////////////////////////////////////
1:             observers.forEach((o) -> o.nextUnfilteredCluster(unfiltered, origPos));
author:Dave Brosius
-------------------------------------------------------------------------------
commit:68d2526
/////////////////////////////////////////////////////////////////////////
0:         if (reusableBuffer != null) 
1:         {
author:T Jake Luciani
-------------------------------------------------------------------------------
commit:1e92ce4
/////////////////////////////////////////////////////////////////////////
1:     private final List<IndexInfo> indexSamples = new ArrayList<>();
/////////////////////////////////////////////////////////////////////////
1:     private long initialPosition;
1:     private final  ISerializer<IndexInfo> idxSerializer;
1:     public long headerLength;
1:     private long startPosition;
/////////////////////////////////////////////////////////////////////////
1:                         SequentialWriter writer,
0:                         Version version,
1:                         Collection<SSTableFlushObserver> observers,
0:                         ISerializer<IndexInfo> indexInfoSerializer)
0:         this.idxSerializer = indexInfoSerializer;
1:     }
1: 
1:     public void reset()
1:     {
1:         this.headerLength = -1;
1:         this.startPosition = -1;
1:         this.previousRowStart = 0;
1:         this.columnIndexCount = 0;
1:         this.written = 0;
1:         this.indexSamplesSerializedSize = 0;
1:         this.indexSamples.clear();
1:         this.firstClustering = null;
1:         this.lastClustering = null;
1:         this.openMarker = null;
1:         this.buffer = null;
/////////////////////////////////////////////////////////////////////////
1:         finish();
/////////////////////////////////////////////////////////////////////////
1:     public List<IndexInfo> indexSamples()
1:     {
1:         if (indexSamplesSerializedSize + columnIndexCount * TypeSizes.sizeof(0) <= DatabaseDescriptor.getColumnIndexCacheSize())
1:         {
1:             return indexSamples;
1:         }
1: 
1:         return null;
1:     }
1: 
/////////////////////////////////////////////////////////////////////////
1:         // For shallow ones, we need it to serialize the offsts in finish().
/////////////////////////////////////////////////////////////////////////
1: 
1:             //the 0th element is always 0
1:             if (columnIndexCount == 0)
1:             {
1:                 indexOffsets[columnIndexCount] = 0;
1:             }
1:             else
1:             {
0:                 indexOffsets[columnIndexCount] =
0:                 ? Ints.checkedCast(buffer.position())
0:                 : indexSamplesSerializedSize;
1:             }
/////////////////////////////////////////////////////////////////////////
/////////////////////////////////////////////////////////////////////////
1:     private void finish() throws IOException
author:Pavel Yaskevich
-------------------------------------------------------------------------------
commit:b7d1d44
/////////////////////////////////////////////////////////////////////////
1:             {
0:                 Row staticRow = iterator.staticRow();
1: 
0:                 UnfilteredSerializer.serializer.serializeStaticRow(staticRow, header, writer, version);
1:                 if (!observers.isEmpty())
0:                     observers.forEach((o) -> o.nextUnfilteredCluster(staticRow));
1:             }
commit:72790dc
/////////////////////////////////////////////////////////////////////////
0:             // notify observers about each new row
0:             if (!observers.isEmpty())
0:                 observers.forEach((o) -> o.nextUnfilteredCluster(unfiltered));
commit:f81a91d
/////////////////////////////////////////////////////////////////////////
1: import org.apache.cassandra.io.sstable.format.SSTableFlushObserver;
/////////////////////////////////////////////////////////////////////////
0:     public static ColumnIndex writeAndBuildIndex(UnfilteredRowIterator iterator,
0:                                                  SequentialWriter output,
0:                                                  SerializationHeader header,
0:                                                  Collection<SSTableFlushObserver> observers,
0:                                                  Version version) throws IOException
0:         Builder builder = new Builder(iterator, output, header, observers, version.correspondingMessagingVersion());
/////////////////////////////////////////////////////////////////////////
0:         private final Collection<SSTableFlushObserver> observers;
1: 
0:                        Collection<SSTableFlushObserver> observers,
0:             this.observers = observers == null ? Collections.emptyList() : observers;
/////////////////////////////////////////////////////////////////////////
1: 
0:             // notify observers about each new cell added to the row
0:             if (!observers.isEmpty() && unfiltered.isRow())
0:                 ((Row) unfiltered).stream().forEach(cell -> observers.forEach((o) -> o.nextCell(cell)));
1: 
commit:d765b24
/////////////////////////////////////////////////////////////////////////
/////////////////////////////////////////////////////////////////////////
0: import org.apache.cassandra.utils.Filter;
0: import org.apache.cassandra.utils.FilterFactory;
0:     public final Filter bloomFilter;
0:     private static final ColumnIndex EMPTY = new ColumnIndex(Collections.<IndexHelper.IndexInfo>emptyList(), FilterFactory.emptyFilter());
0:         this(new ArrayList<IndexHelper.IndexInfo>(), FilterFactory.getFilter(estimatedColumnCount, 4));
0:     private ColumnIndex(List<IndexHelper.IndexInfo> columnsIndex, Filter bloomFilter)
author:Sylvain Lebresne
-------------------------------------------------------------------------------
commit:6584331
/////////////////////////////////////////////////////////////////////////
0:         private long previousRowStart;
/////////////////////////////////////////////////////////////////////////
0:                 UnfilteredSerializer.serializer.serializeStaticRow(iterator.staticRow(), header, writer, version);
/////////////////////////////////////////////////////////////////////////
0:             long pos = currentPosition();
1: 
0:                 startPosition = pos;
0:             UnfilteredSerializer.serializer.serialize(unfiltered, header, writer, pos - previousRowStart, version);
0:             previousRowStart = pos;
commit:60e45c0
/////////////////////////////////////////////////////////////////////////
0:     public final long partitionHeaderLength;
0:     private static final ColumnIndex EMPTY = new ColumnIndex(-1, Collections.<IndexHelper.IndexInfo>emptyList());
0:     private ColumnIndex(long partitionHeaderLength, List<IndexHelper.IndexInfo> columnsIndex)
0:         this.partitionHeaderLength = partitionHeaderLength;
/////////////////////////////////////////////////////////////////////////
0:         private final List<IndexHelper.IndexInfo> columnsIndex = new ArrayList<>();
0:         private long headerLength = -1;
1: 
/////////////////////////////////////////////////////////////////////////
/////////////////////////////////////////////////////////////////////////
0:             this.headerLength = writer.getFilePointer() - initialPosition;
/////////////////////////////////////////////////////////////////////////
0:             columnsIndex.add(cIndexInfo);
/////////////////////////////////////////////////////////////////////////
0:             assert columnsIndex.size() > 0 && headerLength >= 0;
0:             return new ColumnIndex(headerLength, columnsIndex);
commit:2457599
/////////////////////////////////////////////////////////////////////////
0:         private ClusteringPrefix lastClustering;
/////////////////////////////////////////////////////////////////////////
/////////////////////////////////////////////////////////////////////////
0:                                                                          lastClustering,
/////////////////////////////////////////////////////////////////////////
0:                 firstClustering = unfiltered.clustering();
0:             lastClustering = unfiltered.clustering();
0:             if (unfiltered.kind() == Unfiltered.Kind.RANGE_TOMBSTONE_MARKER)
0:                 RangeTombstoneMarker marker = (RangeTombstoneMarker)unfiltered;
1: 
commit:a991b64
/////////////////////////////////////////////////////////////////////////
1: import org.apache.cassandra.db.rows.*;
1: import org.apache.cassandra.io.sstable.format.Version;
1: import org.apache.cassandra.io.util.SequentialWriter;
/////////////////////////////////////////////////////////////////////////
0:     public static ColumnIndex writeAndBuildIndex(UnfilteredRowIterator iterator, SequentialWriter output, SerializationHeader header, Version version) throws IOException
1:     {
0:         assert !iterator.isEmpty() && version.storeRows();
1: 
0:         Builder builder = new Builder(iterator, output, header, version.correspondingMessagingVersion());
0:         return builder.build();
1:     }
1: 
/////////////////////////////////////////////////////////////////////////
0:     private static class Builder
0:         private final UnfilteredRowIterator iterator;
0:         private final SequentialWriter writer;
0:         private final SerializationHeader header;
0:         private final int version;
1: 
0:         private final long initialPosition;
0:         private int written;
0:         private ClusteringPrefix firstClustering;
0:         private final ReusableClusteringPrefix lastClustering;
1: 
0:         private DeletionTime openMarker;
1: 
0:         public Builder(UnfilteredRowIterator iterator,
0:                        SequentialWriter writer,
0:                        SerializationHeader header,
0:                        int version)
0:             this.iterator = iterator;
0:             this.writer = writer;
0:             this.header = header;
0:             this.version = version;
0:             this.initialPosition = writer.getFilePointer();
0:             this.lastClustering = new ReusableClusteringPrefix(iterator.metadata().clusteringColumns().size());
0:         private void writePartitionHeader(UnfilteredRowIterator iterator) throws IOException
0:             ByteBufferUtil.writeWithShortLength(iterator.partitionKey().getKey(), writer.stream);
0:             DeletionTime.serializer.serialize(iterator.partitionLevelDeletion(), writer.stream);
0:             if (header.hasStatic())
0:                 UnfilteredSerializer.serializer.serialize(iterator.staticRow(), header, writer.stream, version);
0:         public ColumnIndex build() throws IOException
0:             writePartitionHeader(iterator);
1: 
0:             while (iterator.hasNext())
0:                 add(iterator.next());
1: 
0:             return close();
0:         private long currentPosition()
0:             return writer.getFilePointer() - initialPosition;
0:         private void addIndexBlock()
0:             IndexHelper.IndexInfo cIndexInfo = new IndexHelper.IndexInfo(firstClustering,
0:                                                                          lastClustering.get().takeAlias(),
0:                                                                          startPosition,
0:                                                                          currentPosition() - startPosition,
0:                                                                          openMarker);
0:             result.columnsIndex.add(cIndexInfo);
0:             firstClustering = null;
1:         }
0:         private void add(Unfiltered unfiltered) throws IOException
0:         {
0:             lastClustering.copy(unfiltered.clustering());
0:             boolean isMarker = unfiltered.kind() == Unfiltered.Kind.RANGE_TOMBSTONE_MARKER;
1: 
0:             if (firstClustering == null)
0:                 // Beginning of an index block. Remember the start and position
0:                 firstClustering = lastClustering.get().takeAlias();
0:                 startPosition = currentPosition();
0:             UnfilteredSerializer.serializer.serialize(unfiltered, header, writer.stream, version);
0:             ++written;
1: 
0:             if (isMarker)
0:                 RangeTombstoneMarker marker = (RangeTombstoneMarker) unfiltered;
0:                 openMarker = marker.isOpen(false) ? marker.openDeletionTime(false) : null;
0:             if (currentPosition() - startPosition >= DatabaseDescriptor.getColumnIndexSize())
0:                 addIndexBlock();
0:         private ColumnIndex close() throws IOException
0:             UnfilteredSerializer.serializer.writeEndOfPartition(writer.stream);
0:             // It's possible we add no rows, just a top level deletion
0:             if (written == 0)
0:             if (firstClustering != null)
0:                 addIndexBlock();
commit:e50d6af
/////////////////////////////////////////////////////////////////////////
0:             for (Cell c : cf)
commit:362cc05
/////////////////////////////////////////////////////////////////////////
0: import org.apache.cassandra.db.composites.Composite;
/////////////////////////////////////////////////////////////////////////
/////////////////////////////////////////////////////////////////////////
0:         private final OnDiskAtom.Serializer atomSerializer;
0: 
/////////////////////////////////////////////////////////////////////////
0:             this.atomSerializer = cf.getComparator().onDiskAtomSerializer();
/////////////////////////////////////////////////////////////////////////
0:             Comparator<Composite> comparator = cf.getComparator();
/////////////////////////////////////////////////////////////////////////
0:             long size = atomSerializer.serializedSizeForSSTable(column);
commit:88f65a1
/////////////////////////////////////////////////////////////////////////
0:                 DeletionTime.serializer.serialize(deletionInfo.getTopLevelDeletion(), output);
/////////////////////////////////////////////////////////////////////////
0:             if (!deletionInfo.isLive())
commit:82b920b
/////////////////////////////////////////////////////////////////////////
0:                        DataOutput output)
/////////////////////////////////////////////////////////////////////////
0:             this.tombstoneTracker = new RangeTombstone.Tracker(cf.getComparator());
/////////////////////////////////////////////////////////////////////////
0:             return atomCount + tombstoneTracker.writtenAtom();
/////////////////////////////////////////////////////////////////////////
0:                 endPosition += tombstoneTracker.writeOpenedMarker(firstColumn, output, atomSerializer);
/////////////////////////////////////////////////////////////////////////
0:             tombstoneTracker.update(column);
commit:3a005df
/////////////////////////////////////////////////////////////////////////
0:         private static final OnDiskAtom.Serializer atomSerializer = Column.onDiskSerializer();
0: 
/////////////////////////////////////////////////////////////////////////
/////////////////////////////////////////////////////////////////////////
/////////////////////////////////////////////////////////////////////////
0:             for (Column c : cf)
/////////////////////////////////////////////////////////////////////////
0:             if (column instanceof Column)
commit:4d34917
/////////////////////////////////////////////////////////////////////////
0: import java.io.DataOutput;
1: import java.io.IOException;
1: import java.util.*;
/////////////////////////////////////////////////////////////////////////
0:      * Help to create an index for a column family based on size of columns,
0:      * and write said columns to disk.
0:         private long blockSize;
0:         private OnDiskAtom firstColumn;
0:         private OnDiskAtom lastColumn;
0:         private OnDiskAtom lastBlockClosing;
0:         private final DataOutput output;
0:         private final RangeTombstone.Tracker tombstoneTracker;
0:         private final OnDiskAtom.Serializer atomSerializer;
0:         private int atomCount;
0:         public Builder(ColumnFamily cf,
0:                        ByteBuffer key,
0:                        int estimatedColumnCount,
0:                        DataOutput output)
0:             this.indexOffset = rowHeaderSize(key, cf.deletionInfo());
0:             this.output = output;
0:             this.atomSerializer = cf.getOnDiskSerializer();
0:             this.tombstoneTracker = new RangeTombstone.Tracker(cf.getComparator());
0:         private static long rowHeaderSize(ByteBuffer key, DeletionInfo delInfo)
0:             return typeSizes.sizeof((short) keysize) + keysize          // Row key
0:                  + typeSizes.sizeof(0L)                                 // Row data size
0:                  + DeletionTime.serializer.serializedSize(delInfo.getTopLevelDeletion(), typeSizes)
0:                  + typeSizes.sizeof(0);                                 // Column count
1:         }
0: 
0:         public RangeTombstone.Tracker tombstoneTracker()
0:         {
0:             return tombstoneTracker;
1:         }
0: 
0:         public int writtenAtomCount()
0:         {
0:             return atomCount + tombstoneTracker.writtenAtom();
/////////////////////////////////////////////////////////////////////////
0:         public ColumnIndex build(ColumnFamily cf) throws IOException
0:             Iterator<RangeTombstone> rangeIter = cf.deletionInfo().rangeIterator();
0:             RangeTombstone tombstone = rangeIter.hasNext() ? rangeIter.next() : null;
0:             Comparator<ByteBuffer> comparator = cf.getComparator();
0:             for (IColumn c : cf)
0:             {
0:                 while (tombstone != null && comparator.compare(c.name(), tombstone.min) >= 0)
0:                 {
0:                     add(tombstone);
0:                     tombstone = rangeIter.hasNext() ? rangeIter.next() : null;
0:                 }
0:                 add(c);
0:             }
0:             while (tombstone != null)
0:             {
0:                 add(tombstone);
0:                 tombstone = rangeIter.hasNext() ? rangeIter.next() : null;
0:             }
0:             return build();
0:         }
0: 
0:         public ColumnIndex build(Iterable<OnDiskAtom> columns) throws IOException
0:         {
0:             for (OnDiskAtom c : columns)
0:         public void add(OnDiskAtom column) throws IOException
0:             atomCount++;
0: 
0:             if (column instanceof IColumn)
0:                 result.bloomFilter.add(column.name());
0:                 // TODO: have that use the firstColumn as min + make sure we
0:                 // optimize that on read
0:                 endPosition += tombstoneTracker.writeOpenedMarker(firstColumn, output, atomSerializer);
0:                 blockSize = 0; // We don't count repeated tombstone marker in the block size, to avoid a situation
0:                                // where we wouldn't make any problem because a block is filled by said marker
0:             long size = column.serializedSizeForSSTable();
0:             endPosition += size;
0:             blockSize += size;
0:             if (blockSize >= DatabaseDescriptor.getColumnIndexSize())
0:                 lastBlockClosing = column;
0:             if (output != null)
0:                 atomSerializer.serializeForSSTable(column, output);
0: 
0:             // TODO: Should deal with removing unneeded tombstones
0:             tombstoneTracker.update(column);
0: 
/////////////////////////////////////////////////////////////////////////
0:             if (result.columnsIndex.isEmpty() || lastBlockClosing != lastColumn)
commit:4e17ac4
/////////////////////////////////////////////////////////////////////////
1: /*
1:  * Licensed to the Apache Software Foundation (ASF) under one
1:  * or more contributor license agreements.  See the NOTICE file
1:  * distributed with this work for additional information
1:  * regarding copyright ownership.  The ASF licenses this file
1:  * to you under the Apache License, Version 2.0 (the
1:  * "License"); you may not use this file except in compliance
1:  * with the License.  You may obtain a copy of the License at
1:  *
1:  *     http://www.apache.org/licenses/LICENSE-2.0
1:  *
1:  * Unless required by applicable law or agreed to in writing, software
1:  * distributed under the License is distributed on an "AS IS" BASIS,
1:  * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
1:  * See the License for the specific language governing permissions and
1:  * limitations under the License.
1:  */
1: package org.apache.cassandra.db;
0: 
0: import java.io.DataOutput;
0: import java.io.IOError;
0: import java.io.IOException;
0: import java.nio.ByteBuffer;
0: import java.util.ArrayList;
0: import java.util.Collections;
0: import java.util.Comparator;
0: import java.util.List;
0: 
1: import org.apache.cassandra.config.DatabaseDescriptor;
0: import org.apache.cassandra.io.sstable.IndexHelper;
0: import org.apache.cassandra.io.util.DataOutputBuffer;
0: import org.apache.cassandra.io.util.IIterableColumns;
0: import org.apache.cassandra.utils.BloomFilter;
0: 
1: public class ColumnIndex
0: {
0:     public final List<IndexHelper.IndexInfo> columnsIndex;
0:     public final BloomFilter bloomFilter;
0: 
0:     private static final ColumnIndex EMPTY = new ColumnIndex(Collections.<IndexHelper.IndexInfo>emptyList(), BloomFilter.emptyFilter());
0: 
0:     private ColumnIndex(int estimatedColumnCount)
0:     {
0:         this(new ArrayList<IndexHelper.IndexInfo>(), BloomFilter.getFilter(estimatedColumnCount, 4));
0:     }
0: 
0:     private ColumnIndex(List<IndexHelper.IndexInfo> columnsIndex, BloomFilter bloomFilter)
0:     {
0:         this.columnsIndex = columnsIndex;
0:         this.bloomFilter = bloomFilter;
0:     }
0: 
0:     /**
0:      * Help to create an index for a column family based on size of columns
1:      */
0:     public static class Builder
0:     {
0:         private final ColumnIndex result;
0:         private final Comparator<ByteBuffer> comparator;
0:         private final long indexOffset;
0:         private long startPosition = -1;
0:         private long endPosition = 0;
0:         private IColumn firstColumn = null;
0:         private IColumn lastColumn = null;
0: 
0:         public Builder(Comparator<ByteBuffer> comparator, ByteBuffer key, int estimatedColumnCount)
0:         {
0:             this.comparator = comparator;
0:             this.indexOffset = rowHeaderSize(key);
0:             this.result = new ColumnIndex(estimatedColumnCount);
0:         }
0: 
0:         /**
0:          * Returns the number of bytes between the beginning of the row and the
0:          * first serialized column.
1:          */
0:         private static long rowHeaderSize(ByteBuffer key)
0:         {
0:             return DBConstants.SHORT_SIZE + key.remaining()     // Row key
0:                  + DBConstants.LONG_SIZE                        // Row data size
0:                  + DBConstants.INT_SIZE + DBConstants.LONG_SIZE // Deletion info
0:                  + DBConstants.INT_SIZE;                        // Column count
0:         }
0: 
0:         /**
0:          * Serializes the index into in-memory structure with all required components
0:          * such as Bloom Filter, index block size, IndexInfo list
1:          *
0:          * @param columns Column family to create index for
1:          *
0:          * @return information about index - it's Bloom Filter, block size and IndexInfo list
1:          */
0:         public ColumnIndex build(IIterableColumns columns)
0:         {
0:             int columnCount = columns.getEstimatedColumnCount();
0: 
0:             if (columnCount == 0)
0:                 return ColumnIndex.EMPTY;
0: 
0:             for (IColumn c : columns)
0:                 add(c);
0: 
0:             return build();
0:         }
0: 
0:         public void add(IColumn column)
0:         {
0:             result.bloomFilter.add(column.name());
0: 
0:             if (firstColumn == null)
0:             {
0:                 firstColumn = column;
0:                 startPosition = endPosition;
0:             }
0: 
0:             endPosition += column.serializedSize();
0: 
0:             // if we hit the column index size that we have to index after, go ahead and index it.
0:             if (endPosition - startPosition >= DatabaseDescriptor.getColumnIndexSize())
0:             {
0:                 IndexHelper.IndexInfo cIndexInfo = new IndexHelper.IndexInfo(firstColumn.name(), column.name(), indexOffset + startPosition, endPosition - startPosition);
0:                 result.columnsIndex.add(cIndexInfo);
0:                 firstColumn = null;
0:             }
0: 
0:             lastColumn = column;
0:         }
0: 
0:         public ColumnIndex build()
0:         {
0:             // all columns were GC'd after all
0:             if (lastColumn == null)
0:                 return ColumnIndex.EMPTY;
0: 
0:             // the last column may have fallen on an index boundary already.  if not, index it explicitly.
0:             if (result.columnsIndex.isEmpty() || comparator.compare(result.columnsIndex.get(result.columnsIndex.size() - 1).lastName, lastColumn.name()) != 0)
0:             {
0:                 IndexHelper.IndexInfo cIndexInfo = new IndexHelper.IndexInfo(firstColumn.name(), lastColumn.name(), indexOffset + startPosition, endPosition - startPosition);
0:                 result.columnsIndex.add(cIndexInfo);
0:             }
0: 
0:             // we should always have at least one computed index block, but we only write it out if there is more than that.
0:             assert result.columnsIndex.size() > 0;
0:             return result;
0:         }
0:     }
0: }
author:Ariel Weisberg
-------------------------------------------------------------------------------
commit:29687a8
/////////////////////////////////////////////////////////////////////////
0:             ByteBufferUtil.writeWithShortLength(iterator.partitionKey().getKey(), writer);
0:             DeletionTime.serializer.serialize(iterator.partitionLevelDeletion(), writer);
0:                 UnfilteredSerializer.serializer.serialize(iterator.staticRow(), header, writer, version);
/////////////////////////////////////////////////////////////////////////
0:             UnfilteredSerializer.serializer.serialize(unfiltered, header, writer, version);
/////////////////////////////////////////////////////////////////////////
0:             UnfilteredSerializer.serializer.writeEndOfPartition(writer);
author:Marcus Eriksson
-------------------------------------------------------------------------------
commit:049762b
commit:29befa1
commit:7f9e9a8
/////////////////////////////////////////////////////////////////////////
0:                 tombstoneTracker.update(column, false);
author:Yuki Morishita
-------------------------------------------------------------------------------
commit:192596a
/////////////////////////////////////////////////////////////////////////
0:         private long openedMarkerSize = 0;
/////////////////////////////////////////////////////////////////////////
0:                 {
0:                     long tombstoneSize = tombstoneTracker.writeOpenedMarker(firstColumn, output, atomSerializer);
0:                     endPosition += tombstoneSize;
0:                     openedMarkerSize += tombstoneSize;
0:                 }
/////////////////////////////////////////////////////////////////////////
0: 
0:         public long getOpenedMarkerSize()
0:         {
0:             return openedMarkerSize;
0:         }
commit:eba27a6
/////////////////////////////////////////////////////////////////////////
commit:0aaf67a
commit:0f1fb43
/////////////////////////////////////////////////////////////////////////
0:                        DataOutput output,
0:                        boolean fromStream)
0:             this.tombstoneTracker = fromStream ? null : new RangeTombstone.Tracker(cf.getComparator());
0:         }
0: 
0:         public Builder(ColumnFamily cf,
0:                        ByteBuffer key,
0:                        int estimatedColumnCount,
0:                        DataOutput output)
0:         {
0:             this(cf, key, estimatedColumnCount, output, false);
/////////////////////////////////////////////////////////////////////////
0:             return tombstoneTracker == null ? atomCount : atomCount + tombstoneTracker.writtenAtom();
/////////////////////////////////////////////////////////////////////////
0:                 // TODO: have that use the firstColumn as min + make sure we optimize that on read
0:                 if (tombstoneTracker != null)
0:                     endPosition += tombstoneTracker.writeOpenedMarker(firstColumn, output, atomSerializer);
0:                                // where we wouldn't make any progress because a block is filled by said marker
/////////////////////////////////////////////////////////////////////////
0:             if (tombstoneTracker != null)
0:                 tombstoneTracker.update(column);
author:Aleksey Yeschenko
-------------------------------------------------------------------------------
commit:6858dd3
/////////////////////////////////////////////////////////////////////////
0:                 // We can skip any cell if it's shadowed by a tombstone already. This is a more
author:belliottsmith
-------------------------------------------------------------------------------
commit:75508ec
/////////////////////////////////////////////////////////////////////////
/////////////////////////////////////////////////////////////////////////
0: import org.apache.cassandra.io.util.DataOutputPlus;
/////////////////////////////////////////////////////////////////////////
0:         private final DataOutputPlus output;
/////////////////////////////////////////////////////////////////////////
0:                        DataOutputPlus output)
author:Jonathan Ellis
-------------------------------------------------------------------------------
commit:d53c838
commit:3edb62b
/////////////////////////////////////////////////////////////////////////
0:         private final DeletionInfo deletionInfo; // only used for serializing and calculating row header size
commit:58bcdd4
/////////////////////////////////////////////////////////////////////////
0:             Comparator<ByteBuffer> comparator = cf.getComparator();
0:             DeletionInfo.InOrderTester tester = cf.deletionInfo().inOrderTester();
0:                     // skip range tombstones that are shadowed by partition tombstones
0:                     if (!cf.deletionInfo().getTopLevelDeletion().isDeleted(tombstone))
0:                         add(tombstone);
0: 
0:                 // We can skip any cell if it's shadowed by a tombstone already.  This is a more
0:                 // general case than was handled by CASSANDRA-2589.
0:                 if (!tester.isDeleted(c))
0:                     add(c);
/////////////////////////////////////////////////////////////////////////
0:             return build();
commit:36cdf34
/////////////////////////////////////////////////////////////////////////
0:             maybeWriteEmptyRowHeader();
0:         /**
0:          * The important distinction wrt build() is that we may be building for a row that ends up
0:          * being compacted away entirely, i.e., the input consists only of expired tombstones (or
0:          * columns shadowed by expired tombstone).  Thus, it is the caller's responsibility
0:          * to decide whether to write the header for an empty row.
0:          */
0:         public ColumnIndex buildForCompaction(Iterator<OnDiskAtom> columns) throws IOException
0:             while (columns.hasNext())
0:             {
0:                 OnDiskAtom c =  columns.next();
0:             }
/////////////////////////////////////////////////////////////////////////
0:         public void maybeWriteEmptyRowHeader() throws IOException
commit:d4b5b0d
/////////////////////////////////////////////////////////////////////////
0:             maybeWriteEmptyRowHeader();
0:         /**
0:          * The important distinction wrt build() is that we may be building for a row that ends up
0:          * being compacted away entirely, i.e., the input consists only of expired tombstones (or
0:          * columns shadowed by expired tombstone).  Thus, it is the caller's responsibility
0:          * to decide whether to write the header for an empty row.
0:          */
0:         public ColumnIndex buildForCompaction(Iterator<OnDiskAtom> columns) throws IOException
0:             while (columns.hasNext())
0:                 add(columns.next());
0:             return build();
/////////////////////////////////////////////////////////////////////////
0:         public void maybeWriteEmptyRowHeader() throws IOException
commit:a9bd531
/////////////////////////////////////////////////////////////////////////
1: import org.apache.cassandra.utils.ByteBufferUtil;
/////////////////////////////////////////////////////////////////////////
0:         private final ByteBuffer key;
0:         private final DeletionInfo deletionInfo;
0:             assert cf != null;
0:             assert key != null;
0:             assert output != null;
0: 
0:             this.key = key;
0:             deletionInfo = cf.deletionInfo();
0:             this.indexOffset = rowHeaderSize(key, deletionInfo);
/////////////////////////////////////////////////////////////////////////
0:                  + DeletionTime.serializer.serializedSize(delInfo.getTopLevelDeletion(), typeSizes);
/////////////////////////////////////////////////////////////////////////
0:             // cf has disentangled the columns and range tombstones, we need to re-interleave them in comparator order
/////////////////////////////////////////////////////////////////////////
0:             ColumnIndex index = build();
0: 
0:             finish();
0: 
0:             return index;
0:             ColumnIndex index = build();
0:             finish();
0: 
0:             return index;
/////////////////////////////////////////////////////////////////////////
0:             maybeWriteRowHeader();
0:             atomSerializer.serializeForSSTable(column, output);
/////////////////////////////////////////////////////////////////////////
0:         private void maybeWriteRowHeader() throws IOException
0:         {
0:             if (lastColumn == null)
0:             {
0:                 ByteBufferUtil.writeWithShortLength(key, output);
0:                 DeletionInfo.serializer().serializeForSSTable(deletionInfo, output);
0:             }
0:         }
0: 
/////////////////////////////////////////////////////////////////////////
0: 
0:         public void finish() throws IOException
0:         {
0:             if (!deletionInfo.equals(DeletionInfo.LIVE))
0:                 maybeWriteRowHeader();
0:         }
commit:c48acd9
commit:302267e
/////////////////////////////////////////////////////////////////////////
0: import com.google.common.annotations.VisibleForTesting;
0: 
0:     private static final ColumnIndex EMPTY = new ColumnIndex(Collections.<IndexHelper.IndexInfo>emptyList());
0:     private ColumnIndex(List<IndexHelper.IndexInfo> columnsIndex)
0:     }
0: 
0:     @VisibleForTesting
0:     public static ColumnIndex nothing()
0:     {
0:         return EMPTY;
/////////////////////////////////////////////////////////////////////////
0:             this.result = new ColumnIndex(new ArrayList<IndexHelper.IndexInfo>());
/////////////////////////////////////////////////////////////////////////
0:             this(cf, key, output, false);
/////////////////////////////////////////////////////////////////////////
commit:98b5195
/////////////////////////////////////////////////////////////////////////
0:         assert columnsIndex != null;
0: 
commit:7746225
/////////////////////////////////////////////////////////////////////////
/////////////////////////////////////////////////////////////////////////
0:     private static final ColumnIndex EMPTY = new ColumnIndex(Collections.<IndexHelper.IndexInfo>emptyList(), new AlwaysPresentFilter());
/////////////////////////////////////////////////////////////////////////
commit:811f82c
/////////////////////////////////////////////////////////////////////////
0: import com.google.common.annotations.VisibleForTesting;
0: 
/////////////////////////////////////////////////////////////////////////
0:     @VisibleForTesting
0:     public static ColumnIndex nothing()
0:     {
0:         return new ColumnIndex(0);
0:     }
0: 
commit:d7a0982
/////////////////////////////////////////////////////////////////////////
0:     private static final ColumnIndex EMPTY = new ColumnIndex(Collections.<IndexHelper.IndexInfo>emptyList(), AlwaysPresentFilter.instance);
/////////////////////////////////////////////////////////////////////////
0:         assert columnsIndex != null;
0:         assert bloomFilter != null;
0: 
commit:17034c0
/////////////////////////////////////////////////////////////////////////
0:     private static final ColumnIndex EMPTY = new ColumnIndex(Collections.<IndexHelper.IndexInfo>emptyList(), new AlwaysPresentFilter());
/////////////////////////////////////////////////////////////////////////
commit:798470e
/////////////////////////////////////////////////////////////////////////
0:     private static final ColumnIndex EMPTY = new ColumnIndex(Collections.<IndexHelper.IndexInfo>emptyList(), AlwaysPresentFilter.instance);
/////////////////////////////////////////////////////////////////////////
0:         assert columnsIndex != null;
0:         assert bloomFilter != null;
0: 
commit:dd4fd2c
/////////////////////////////////////////////////////////////////////////
0:          * @param cf Column family to create index for
commit:37cf942
/////////////////////////////////////////////////////////////////////////
0: import org.apache.cassandra.utils.AlwaysPresentFilter;
/////////////////////////////////////////////////////////////////////////
0:     private static final ColumnIndex EMPTY = new ColumnIndex(Collections.<IndexHelper.IndexInfo>emptyList(), new AlwaysPresentFilter());
commit:a15500e
/////////////////////////////////////////////////////////////////////////
0: import org.apache.cassandra.utils.IFilter;
0:     public final IFilter bloomFilter;
/////////////////////////////////////////////////////////////////////////
0:     private ColumnIndex(List<IndexHelper.IndexInfo> columnsIndex, IFilter bloomFilter)
commit:dc37dea
/////////////////////////////////////////////////////////////////////////
0:         this(new ArrayList<IndexHelper.IndexInfo>(), FilterFactory.getFilter(estimatedColumnCount, 4, false));
commit:2ae5272
/////////////////////////////////////////////////////////////////////////
0:             TypeSizes typeSizes = TypeSizes.NATIVE;
/////////////////////////////////////////////////////////////////////////
0:             endPosition += column.serializedSize(TypeSizes.NATIVE);
author:Jason Brown
-------------------------------------------------------------------------------
commit:9297e7b
/////////////////////////////////////////////////////////////////////////
0:     private static final ColumnIndex EMPTY = new ColumnIndex(Collections.<IndexHelper.IndexInfo>emptyList());
0:     private ColumnIndex(List<IndexHelper.IndexInfo> columnsIndex)
/////////////////////////////////////////////////////////////////////////
0:             this.result = new ColumnIndex(new ArrayList<IndexHelper.IndexInfo>());
/////////////////////////////////////////////////////////////////////////
author:Vijay Parthasarathy
-------------------------------------------------------------------------------
commit:cb25a8f
/////////////////////////////////////////////////////////////////////////
0:             DBTypeSizes typeSizes = DBTypeSizes.NATIVE;
0:             // TODO fix constantSize when changing the nativeconststs.
0:             int keysize = key.remaining();
0:             return typeSizes.sizeof((short) keysize) + keysize + // Row key
0:                  + typeSizes.sizeof(0L)                        // Row data size
0:                  + typeSizes.sizeof(0) + typeSizes.sizeof(0L) // Deletion info
0:                  + typeSizes.sizeof(0);                        // Column count
/////////////////////////////////////////////////////////////////////////
0:             endPosition += column.serializedSize(DBTypeSizes.NATIVE);
============================================================================