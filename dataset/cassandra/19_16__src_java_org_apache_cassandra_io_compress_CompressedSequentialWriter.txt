1:07cf56f: /*
1:1ecabe6:  * Licensed to the Apache Software Foundation (ASF) under one
1:1ecabe6:  * or more contributor license agreements.  See the NOTICE file
1:1ecabe6:  * distributed with this work for additional information
1:1ecabe6:  * regarding copyright ownership.  The ASF licenses this file
1:1ecabe6:  * to you under the Apache License, Version 2.0 (the
1:1ecabe6:  * "License"); you may not use this file except in compliance
1:1ecabe6:  * with the License.  You may obtain a copy of the License at
2:1ecabe6:  *
1:07cf56f:  *     http://www.apache.org/licenses/LICENSE-2.0
1:1ecabe6:  *
1:07cf56f:  * Unless required by applicable law or agreed to in writing, software
1:07cf56f:  * distributed under the License is distributed on an "AS IS" BASIS,
1:07cf56f:  * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
1:07cf56f:  * See the License for the specific language governing permissions and
1:07cf56f:  * limitations under the License.
1:1ecabe6:  */
1:1ecabe6: package org.apache.cassandra.io.compress;
1:29687a8: 
1:16499ca: import java.io.DataOutputStream;
1:9ecda72: import java.io.EOFException;
1:1ecabe6: import java.io.File;
1:1ecabe6: import java.io.IOException;
1:bc7941c: import java.nio.ByteBuffer;
1:16499ca: import java.nio.channels.Channels;
1:fb22109: import java.util.Optional;
1:5baf28d: import java.util.zip.CRC32;
1:0ced7a3: 
1:9ecda72: import org.apache.cassandra.io.FSReadError;
1:9ecda72: import org.apache.cassandra.io.FSWriteError;
1:9ecda72: import org.apache.cassandra.io.sstable.CorruptSSTableException;
1:74bf5aa: import org.apache.cassandra.io.sstable.metadata.MetadataCollector;
1:fb22109: import org.apache.cassandra.io.util.*;
1:b31845c: import org.apache.cassandra.schema.CompressionParams;
1:41c7424: 
1:fb22109: import static org.apache.cassandra.utils.Throwables.merge;
1:fb22109: 
1:1ecabe6: public class CompressedSequentialWriter extends SequentialWriter
1:9ecda72: {
1:fb22109:     private final ChecksumWriter crcMetadata;
1:07cdfd0: 
1:1ecabe6:     // holds offset in the file where current chunk should be written
1:1ecabe6:     // changed only by flush() method where data buffer gets compressed and stored to the file
1:1ecabe6:     private long chunkOffset = 0;
1:2fd3268: 
1:1ecabe6:     // index file writer (random I/O)
1:1ecabe6:     private final CompressionMetadata.Writer metadataWriter;
1:c8afd76:     private final ICompressor compressor;
17:1ecabe6: 
1:1ecabe6:     // used to store compressed data
1:3adfd15:     private ByteBuffer compressed;
1:1ecabe6: 
1:1ecabe6:     // holds a number of already written chunks
1:1ecabe6:     private int chunkCount = 0;
1:1ecabe6: 
1:bc7941c:     private long uncompressedSize = 0, compressedSize = 0;
1:1ecabe6: 
1:74bf5aa:     private final MetadataCollector sstableMetadataCollector;
1:1ecabe6: 
1:bc7941c:     private final ByteBuffer crcCheckBuffer = ByteBuffer.allocate(4);
1:fb22109:     private final Optional<File> digestFile;
1:bc7941c: 
1:fb22109:     /**
1:fb22109:      * Create CompressedSequentialWriter without digest file.
1:fb22109:      *
1:fb22109:      * @param file File to write
1:fb22109:      * @param offsetsPath File name to write compression metadata
1:fb22109:      * @param digestFile File to write digest
1:fb22109:      * @param option Write option (buffer size and type will be set the same as compression params)
1:fb22109:      * @param parameters Compression mparameters
1:fb22109:      * @param sstableMetadataCollector Metadata collector
1:fb22109:      */
1:debb15e:     public CompressedSequentialWriter(File file,
1:9405ce0:                                       String offsetsPath,
1:fb22109:                                       File digestFile,
1:fb22109:                                       SequentialWriterOption option,
1:b31845c:                                       CompressionParams parameters,
1:74bf5aa:                                       MetadataCollector sstableMetadataCollector)
6:1ecabe6:     {
1:fb22109:         super(file, SequentialWriterOption.newBuilder()
1:fb22109:                             .bufferSize(option.bufferSize())
1:fb22109:                             .bufferType(option.bufferType())
1:fb22109:                             .bufferSize(parameters.chunkLength())
1:fb22109:                             .bufferType(parameters.getSstableCompressor().preferredBufferType())
1:fb22109:                             .finishOnClose(option.finishOnClose())
1:fb22109:                             .build());
1:056115f:         this.compressor = parameters.getSstableCompressor();
1:fb22109:         this.digestFile = Optional.ofNullable(digestFile);
1:1ecabe6: 
1:1ecabe6:         // buffer for compression should be the same size as buffer itself
1:3adfd15:         compressed = compressor.preferredBufferType().allocate(compressor.initialCompressedBufferLength(buffer.capacity()));
1:1ecabe6: 
1:1ecabe6:         /* Index File (-CompressionInfo.db component) and it's header */
1:4e95953:         metadataWriter = CompressionMetadata.Writer.open(parameters, offsetsPath);
1:debb15e: 
1:0b54c99:         this.sstableMetadataCollector = sstableMetadataCollector;
1:fb22109:         crcMetadata = new ChecksumWriter(new DataOutputStream(Channels.newOutputStream(channel)));
1:9ecda72:     }
1:3679b1b: 
1:9405ce0:     @Override
1:9ecda72:     public long getOnDiskFilePointer()
1:3679b1b:     {
1:9ecda72:         try
1:9405ce0:         {
1:29687a8:             return fchannel.position();
1:3679b1b:         }
1:9ecda72:         catch (IOException e)
1:9ecda72:         {
1:9ecda72:             throw new FSReadError(e, getPath());
1:9405ce0:         }
1:9ecda72:     }
1:9405ce0: 
1:0f1e838:     /**
1:0f1e838:      * Get a quick estimation on how many bytes have been written to disk
1:0f1e838:      *
1:0f1e838:      * It should for the most part be exactly the same as getOnDiskFilePointer()
1:0f1e838:      */
1:0f1e838:     @Override
1:0f1e838:     public long getEstimatedOnDiskBytesWritten()
1:0f1e838:     {
1:0f1e838:         return chunkOffset;
1:0f1e838:     }
1:0f1e838: 
1:80d7d43:     @Override
1:9ecda72:     public void flush()
1:9ecda72:     {
2:1ecabe6:         throw new UnsupportedOperationException();
1:9ecda72:     }
1:80d7d43: 
1:2ce8274:     @Override
1:9ecda72:     protected void flushData()
1:9ecda72:     {
1:4e95953:         seekToChunkStart(); // why is this necessary? seems like it should always be at chunk start in normal operation
1:9ecda72: 
1:9ecda72:         try
1:9ecda72:         {
1:9ecda72:             // compressing data with buffer re-use
1:bc7941c:             buffer.flip();
1:3adfd15:             compressed.clear();
1:3adfd15:             compressor.compress(buffer, compressed);
1:9ecda72:         }
1:9ecda72:         catch (IOException e)
1:9ecda72:         {
1:9ecda72:             throw new RuntimeException("Compression exception", e); // shouldn't happen
1:9ecda72:         }
1:bc7941c: 
1:3adfd15:         int compressedLength = compressed.position();
1:bc7941c:         uncompressedSize += buffer.position();
1:0b54c99:         compressedSize += compressedLength;
1:2ce8274: 
1:9ecda72:         try
1:9ecda72:         {
1:9ecda72:             // write an offset of the newly written chunk to the index file
1:4e95953:             metadataWriter.addOffset(chunkOffset);
1:9ecda72:             chunkCount++;
1:07cdfd0: 
1:bc7941c:             // write out the compressed data
1:3adfd15:             compressed.flip();
1:3adfd15:             channel.write(compressed);
1:bc7941c: 
1:9ecda72:             // write corresponding checksum
1:3adfd15:             compressed.rewind();
1:3adfd15:             crcMetadata.appendDirect(compressed, true);
1:4e95953:             lastFlushOffset += compressedLength + 4;
1:9ecda72:         }
1:9ecda72:         catch (IOException e)
1:9ecda72:         {
1:9ecda72:             throw new FSWriteError(e, getPath());
1:9ecda72:         }
1:bc7941c: 
1:ba1821f:         // next chunk should be written right after current + length of the checksum (int)
1:ba1821f:         chunkOffset += compressedLength + 4;
1:4eb9fa7:         if (runPostFlush != null)
1:4eb9fa7:             runPostFlush.run();
1:4e95953:     }
1:4e95953: 
1:8704006:     public CompressionMetadata open(long overrideLength)
1:4e95953:     {
1:0ced7a3:         if (overrideLength <= 0)
1:8704006:             overrideLength = uncompressedSize;
1:8704006:         return metadataWriter.open(overrideLength, chunkOffset);
1:4e95953:     }
1:4e95953: 
4:1ecabe6:     @Override
1:e8651b6:     public DataPosition mark()
1:4e95953:     {
1:29687a8:         if (!buffer.hasRemaining())
1:f7aaea0:             doFlush(0);
1:bc7941c:         return new CompressedFileWriterMark(chunkOffset, current(), buffer.position(), chunkCount + 1);
1:9ecda72:     }
1:2fd3268: 
1:1ecabe6:     @Override
1:e8651b6:     public synchronized void resetAndTruncate(DataPosition mark)
1:9ecda72:     {
1:1ecabe6:         assert mark instanceof CompressedFileWriterMark;
1:1ecabe6: 
1:9ecda72:         CompressedFileWriterMark realMark = (CompressedFileWriterMark) mark;
1:1ecabe6: 
1:1ecabe6:         // reset position
1:bc7941c:         long truncateTarget = realMark.uncDataOffset;
1:1ecabe6: 
1:bc7941c:         if (realMark.chunkOffset == chunkOffset)
1:9ecda72:         {
1:bc7941c:             // simply drop bytes to the right of our mark
1:bc7941c:             buffer.position(realMark.validBufferBytes);
1:bc7941c:             return;
1:9ecda72:         }
1:bc7941c: 
1:bc7941c:         // synchronize current buffer with disk - we don't want any data loss
1:1ecabe6:         syncInternal();
1:1ecabe6: 
1:1ecabe6:         chunkOffset = realMark.chunkOffset;
1:1ecabe6: 
1:ba1821f:         // compressed chunk size (- 4 bytes reserved for checksum)
1:ba1821f:         int chunkSize = (int) (metadataWriter.chunkOffsetBy(realMark.nextChunkIndex) - chunkOffset - 4);
1:3adfd15:         if (compressed.capacity() < chunkSize)
1:3adfd15:             compressed = compressor.preferredBufferType().allocate(chunkSize);
1:1ecabe6: 
1:9ecda72:         try
1:9ecda72:         {
1:3adfd15:             compressed.clear();
1:3adfd15:             compressed.limit(chunkSize);
1:29687a8:             fchannel.position(chunkOffset);
1:29687a8:             fchannel.read(compressed);
1:1ecabe6: 
1:9ecda72:             try
1:80d7d43:             {
1:bc7941c:                 // Repopulate buffer from compressed data
1:bc7941c:                 buffer.clear();
1:3adfd15:                 compressed.flip();
1:3adfd15:                 compressor.uncompress(compressed, buffer);
1:9ecda72:             }
1:9ecda72:             catch (IOException e)
1:2ce8274:             {
1:c39a9b0:                 throw new CorruptBlockException(getPath(), chunkOffset, chunkSize, e);
1:80d7d43:             }
1:bc7941c: 
1:5baf28d:             CRC32 checksum = new CRC32();
1:3adfd15:             compressed.rewind();
1:8c22b4a:             checksum.update(compressed);
1:1ecabe6: 
1:bc7941c:             crcCheckBuffer.clear();
1:29687a8:             fchannel.read(crcCheckBuffer);
1:bc7941c:             crcCheckBuffer.flip();
1:bc7941c:             if (crcCheckBuffer.getInt() != (int) checksum.getValue())
2:9ecda72:                 throw new CorruptBlockException(getPath(), chunkOffset, chunkSize);
1:9ecda72:         }
1:9ecda72:         catch (CorruptBlockException e)
1:9ecda72:         {
1:9ecda72:             throw new CorruptSSTableException(e, getPath());
1:9ecda72:         }
1:9ecda72:         catch (EOFException e)
1:9ecda72:         {
1:9ecda72:             throw new CorruptSSTableException(new CorruptBlockException(getPath(), chunkOffset, chunkSize), getPath());
1:9ecda72:         }
1:9ecda72:         catch (IOException e)
1:9ecda72:         {
1:9ecda72:             throw new FSReadError(e, getPath());
1:9ecda72:         }
1:1ecabe6: 
1:bc7941c:         // Mark as dirty so we can guarantee the newly buffered bytes won't be lost on a rebuffer
1:bc7941c:         buffer.position(realMark.validBufferBytes);
1:bc7941c: 
1:bc7941c:         bufferOffset = truncateTarget - buffer.position();
1:1ecabe6:         chunkCount = realMark.nextChunkIndex - 1;
1:9ff8270: 
1:1ecabe6:         // truncate data and index file
1:1ecabe6:         truncate(chunkOffset);
1:41d8a5f:         metadataWriter.resetAndTruncate(realMark.nextChunkIndex - 1);
1:2ce8274:     }
1:1ecabe6: 
2:1ecabe6:     /**
1:1ecabe6:      * Seek to the offset where next compressed data chunk should be stored.
1:1ecabe6:      */
1:9ecda72:     private void seekToChunkStart()
1:9ecda72:     {
1:9ecda72:         if (getOnDiskFilePointer() != chunkOffset)
1:9ecda72:         {
1:9ecda72:             try
1:9ecda72:             {
1:29687a8:                 fchannel.position(chunkOffset);
1:9ecda72:             }
1:9ecda72:             catch (IOException e)
1:1ecabe6:             {
1:9ecda72:                 throw new FSReadError(e, getPath());
1:9ecda72:             }
1:9ecda72:         }
6:1ecabe6:     }
1:1ecabe6: 
1:8704006:     protected class TransactionalProxy extends SequentialWriter.TransactionalProxy
1:1ecabe6:     {
1:8704006:         @Override
1:8704006:         protected Throwable doCommit(Throwable accumulate)
1:1ecabe6:         {
1:c163d0b:             return super.doCommit(metadataWriter.commit(accumulate));
1:1ecabe6:         }
1:8704006: 
1:8704006:         @Override
1:8704006:         protected Throwable doAbort(Throwable accumulate)
1:1ecabe6:         {
1:8704006:             return super.doAbort(metadataWriter.abort(accumulate));
1:8704006:         }
1:8704006: 
1:8704006:         @Override
1:8704006:         protected void doPrepare()
1:8704006:         {
1:8704006:             syncInternal();
1:fb22109:             digestFile.ifPresent(crcMetadata::writeFullChecksum);
1:8704006:             sstableMetadataCollector.addCompressionRatio(compressedSize, uncompressedSize);
1:8704006:             metadataWriter.finalizeLength(current(), chunkCount).prepareToCommit();
1:1ecabe6:         }
1:29687a8: 
1:29687a8:         @Override
1:29687a8:         protected Throwable doPreCleanup(Throwable accumulate)
1:29687a8:         {
1:29687a8:             accumulate = super.doPreCleanup(accumulate);
1:29687a8:             if (compressed != null)
1:29687a8:             {
1:29687a8:                 try { FileUtils.clean(compressed); }
1:29687a8:                 catch (Throwable t) { accumulate = merge(accumulate, t); }
1:29687a8:                 compressed = null;
1:29687a8:             }
1:29687a8: 
1:29687a8:             return accumulate;
1:29687a8:         }
1:1ecabe6:     }
1:1ecabe6: 
1:1ecabe6:     @Override
1:8704006:     protected SequentialWriter.TransactionalProxy txnProxy()
1:1ecabe6:     {
1:8704006:         return new TransactionalProxy();
1:1ecabe6:     }
1:1ecabe6: 
1:1ecabe6:     /**
1:1ecabe6:      * Class to hold a mark to the position of the file
1:1ecabe6:      */
1:e8651b6:     protected static class CompressedFileWriterMark implements DataPosition
1:1ecabe6:     {
1:1ecabe6:         // chunk offset in the compressed file
1:5a6e2b0:         final long chunkOffset;
1:1ecabe6:         // uncompressed data offset (real data offset)
1:5a6e2b0:         final long uncDataOffset;
1:1ecabe6: 
1:bc7941c:         final int validBufferBytes;
1:5a6e2b0:         final int nextChunkIndex;
1:1ecabe6: 
1:bc7941c:         public CompressedFileWriterMark(long chunkOffset, long uncDataOffset, int validBufferBytes, int nextChunkIndex)
1:1ecabe6:         {
1:1ecabe6:             this.chunkOffset = chunkOffset;
1:1ecabe6:             this.uncDataOffset = uncDataOffset;
1:bc7941c:             this.validBufferBytes = validBufferBytes;
1:1ecabe6:             this.nextChunkIndex = nextChunkIndex;
1:1ecabe6:         }
1:1ecabe6:     }
1:1ecabe6: }
============================================================================
author:Yuki Morishita
-------------------------------------------------------------------------------
commit:0dc8440
commit:c39a9b0
/////////////////////////////////////////////////////////////////////////
1:                 throw new CorruptBlockException(getPath(), chunkOffset, chunkSize, e);
commit:fb22109
/////////////////////////////////////////////////////////////////////////
1: import java.util.Optional;
1: import org.apache.cassandra.io.util.*;
1: import static org.apache.cassandra.utils.Throwables.merge;
1: 
1:     private final ChecksumWriter crcMetadata;
/////////////////////////////////////////////////////////////////////////
1:     private final Optional<File> digestFile;
1:     /**
1:      * Create CompressedSequentialWriter without digest file.
1:      *
1:      * @param file File to write
1:      * @param offsetsPath File name to write compression metadata
1:      * @param digestFile File to write digest
1:      * @param option Write option (buffer size and type will be set the same as compression params)
1:      * @param parameters Compression mparameters
1:      * @param sstableMetadataCollector Metadata collector
1:      */
1:                                       File digestFile,
1:                                       SequentialWriterOption option,
1:         super(file, SequentialWriterOption.newBuilder()
1:                             .bufferSize(option.bufferSize())
1:                             .bufferType(option.bufferType())
1:                             .bufferSize(parameters.chunkLength())
1:                             .bufferType(parameters.getSstableCompressor().preferredBufferType())
1:                             .finishOnClose(option.finishOnClose())
1:                             .build());
1:         this.digestFile = Optional.ofNullable(digestFile);
/////////////////////////////////////////////////////////////////////////
1:         crcMetadata = new ChecksumWriter(new DataOutputStream(Channels.newOutputStream(channel)));
/////////////////////////////////////////////////////////////////////////
1:             digestFile.ifPresent(crcMetadata::writeFullChecksum);
commit:74bf5aa
/////////////////////////////////////////////////////////////////////////
1: import org.apache.cassandra.io.sstable.metadata.MetadataCollector;
/////////////////////////////////////////////////////////////////////////
1:                                         MetadataCollector sstableMetadataCollector)
/////////////////////////////////////////////////////////////////////////
1:     private final MetadataCollector sstableMetadataCollector;
0:                                       MetadataCollector sstableMetadataCollector)
author:Tom Petracca
-------------------------------------------------------------------------------
commit:0f1e838
/////////////////////////////////////////////////////////////////////////
1:     /**
1:      * Get a quick estimation on how many bytes have been written to disk
1:      *
1:      * It should for the most part be exactly the same as getOnDiskFilePointer()
1:      */
1:     @Override
1:     public long getEstimatedOnDiskBytesWritten()
1:     {
1:         return chunkOffset;
1:     }
1: 
author:Paulo Motta
-------------------------------------------------------------------------------
commit:e8651b6
/////////////////////////////////////////////////////////////////////////
0: import org.apache.cassandra.io.util.DataPosition;
/////////////////////////////////////////////////////////////////////////
1:     public DataPosition mark()
/////////////////////////////////////////////////////////////////////////
1:     public synchronized void resetAndTruncate(DataPosition mark)
/////////////////////////////////////////////////////////////////////////
1:     protected static class CompressedFileWriterMark implements DataPosition
author:Ariel Weisberg
-------------------------------------------------------------------------------
commit:f7aaea0
/////////////////////////////////////////////////////////////////////////
1:             doFlush(0);
commit:29687a8
/////////////////////////////////////////////////////////////////////////
0: import static org.apache.cassandra.utils.Throwables.merge;
1: 
/////////////////////////////////////////////////////////////////////////
0: import org.apache.cassandra.io.util.FileUtils;
/////////////////////////////////////////////////////////////////////////
1:             return fchannel.position();
/////////////////////////////////////////////////////////////////////////
/////////////////////////////////////////////////////////////////////////
1:         if (!buffer.hasRemaining())
0:             doFlush();
/////////////////////////////////////////////////////////////////////////
1:             fchannel.position(chunkOffset);
1:             fchannel.read(compressed);
/////////////////////////////////////////////////////////////////////////
1:             fchannel.read(crcCheckBuffer);
/////////////////////////////////////////////////////////////////////////
/////////////////////////////////////////////////////////////////////////
1:                 fchannel.position(chunkOffset);
/////////////////////////////////////////////////////////////////////////
1: 
1:         @Override
1:         protected Throwable doPreCleanup(Throwable accumulate)
1:         {
1:             accumulate = super.doPreCleanup(accumulate);
1:             if (compressed != null)
1:             {
1:                 try { FileUtils.clean(compressed); }
1:                 catch (Throwable t) { accumulate = merge(accumulate, t); }
1:                 compressed = null;
1:             }
1: 
1:             return accumulate;
1:         }
commit:5baf28d
/////////////////////////////////////////////////////////////////////////
1: import java.util.zip.CRC32;
/////////////////////////////////////////////////////////////////////////
1:             CRC32 checksum = new CRC32();
commit:16499ca
/////////////////////////////////////////////////////////////////////////
1: import java.io.DataOutputStream;
1: import java.nio.channels.Channels;
/////////////////////////////////////////////////////////////////////////
/////////////////////////////////////////////////////////////////////////
0:         crcMetadata = new DataIntegrityMetadata.ChecksumWriter(new DataOutputStream(Channels.newOutputStream(channel)));
author:Stefania Alborghetti
-------------------------------------------------------------------------------
commit:c163d0b
/////////////////////////////////////////////////////////////////////////
1:             return super.doCommit(metadataWriter.commit(accumulate));
/////////////////////////////////////////////////////////////////////////
author:Aleksey Yeschenko
-------------------------------------------------------------------------------
commit:b31845c
/////////////////////////////////////////////////////////////////////////
1: import org.apache.cassandra.schema.CompressionParams;
/////////////////////////////////////////////////////////////////////////
1:                                       CompressionParams parameters,
author:blerer
-------------------------------------------------------------------------------
commit:056115f
/////////////////////////////////////////////////////////////////////////
0:         super(file, parameters.chunkLength(), parameters.getSstableCompressor().preferredBufferType());
1:         this.compressor = parameters.getSstableCompressor();
author:T Jake Luciani
-------------------------------------------------------------------------------
commit:8c22b4a
/////////////////////////////////////////////////////////////////////////
1:             checksum.update(compressed);
commit:2d7909d
/////////////////////////////////////////////////////////////////////////
0:             crcMetadata.appendDirect(compressed.buffer, true);
author:Branimir Lambov
-------------------------------------------------------------------------------
commit:3adfd15
/////////////////////////////////////////////////////////////////////////
1:     private ByteBuffer compressed;
/////////////////////////////////////////////////////////////////////////
0:         super(file, parameters.chunkLength(), parameters.sstableCompressor.preferredBufferType());
1:         compressed = compressor.preferredBufferType().allocate(compressor.initialCompressedBufferLength(buffer.capacity()));
/////////////////////////////////////////////////////////////////////////
1:             compressed.clear();
1:             compressor.compress(buffer, compressed);
1:         int compressedLength = compressed.position();
/////////////////////////////////////////////////////////////////////////
1:             compressed.flip();
1:             channel.write(compressed);
1:             compressed.rewind();
1:             crcMetadata.appendDirect(compressed, true);
/////////////////////////////////////////////////////////////////////////
1:         if (compressed.capacity() < chunkSize)
1:             compressed = compressor.preferredBufferType().allocate(chunkSize);
1:             compressed.clear();
1:             compressed.limit(chunkSize);
0:             channel.read(compressed);
1:                 compressed.flip();
1:                 compressor.uncompress(compressed, buffer);
/////////////////////////////////////////////////////////////////////////
1:             compressed.rewind();
0:             FBUtilities.directCheckSum(checksum, compressed);
author:Benedict Elliott Smith
-------------------------------------------------------------------------------
commit:8704006
/////////////////////////////////////////////////////////////////////////
/////////////////////////////////////////////////////////////////////////
/////////////////////////////////////////////////////////////////////////
1:     public CompressionMetadata open(long overrideLength)
1:             overrideLength = uncompressedSize;
1:         return metadataWriter.open(overrideLength, chunkOffset);
/////////////////////////////////////////////////////////////////////////
1:     protected class TransactionalProxy extends SequentialWriter.TransactionalProxy
1:         @Override
1:         protected Throwable doCommit(Throwable accumulate)
0:             return metadataWriter.commit(accumulate);
1: 
1:         @Override
1:         protected Throwable doAbort(Throwable accumulate)
1:             return super.doAbort(metadataWriter.abort(accumulate));
1:         }
1: 
1:         @Override
1:         protected void doPrepare()
1:         {
1:             syncInternal();
0:             if (descriptor != null)
0:                 crcMetadata.writeFullChecksum(descriptor);
0:             releaseFileHandle();
1:             sstableMetadataCollector.addCompressionRatio(compressedSize, uncompressedSize);
1:             metadataWriter.finalizeLength(current(), chunkCount).prepareToCommit();
1:     protected SequentialWriter.TransactionalProxy txnProxy()
1:         return new TransactionalProxy();
commit:3fededc
commit:b2123db
commit:107a794
/////////////////////////////////////////////////////////////////////////
0:         // we are early opening the file, make sure we open metadata with the correct size
0:         assert !isFinal;
commit:36729b9
commit:0ced7a3
/////////////////////////////////////////////////////////////////////////
0: import static org.apache.cassandra.io.compress.CompressionMetadata.Writer.OpenType.FINAL;
0: import static org.apache.cassandra.io.compress.CompressionMetadata.Writer.OpenType.SHARED;
0: import static org.apache.cassandra.io.compress.CompressionMetadata.Writer.OpenType.SHARED_FINAL;
1: 
/////////////////////////////////////////////////////////////////////////
0:     public CompressionMetadata open(long overrideLength, boolean isFinal)
1:         if (overrideLength <= 0)
0:             return metadataWriter.open(originalSize, chunkOffset, isFinal ? FINAL : SHARED_FINAL);
0:         return metadataWriter.open(overrideLength, chunkOffset, SHARED);
commit:ce76e1e
commit:4eb9fa7
/////////////////////////////////////////////////////////////////////////
1:         if (runPostFlush != null)
1:             runPostFlush.run();
commit:729ebe0
commit:55750e0
commit:3679b1b
/////////////////////////////////////////////////////////////////////////
0:     public void abort()
1:     {
0:         super.abort();
0:         metadataWriter.abort();
1:     }
1: 
commit:9a51c3c
commit:871f003
/////////////////////////////////////////////////////////////////////////
0: import org.apache.cassandra.io.sstable.SSTableWriter;
/////////////////////////////////////////////////////////////////////////
0:     public CompressionMetadata open(SSTableWriter.FinishType finishType)
0:         assert finishType != SSTableWriter.FinishType.NORMAL || current == originalSize;
0:         return metadataWriter.open(originalSize, chunkOffset, finishType);
author:Joshua McKenzie
-------------------------------------------------------------------------------
commit:507a363
/////////////////////////////////////////////////////////////////////////
commit:7d62a73
commit:9ff8270
/////////////////////////////////////////////////////////////////////////
0:         // mark as dirty so we don't lose the bytes on subsequent reBuffer calls
0:         isDirty = true;
1: 
commit:bc7941c
/////////////////////////////////////////////////////////////////////////
1: import java.nio.ByteBuffer;
0: import org.apache.cassandra.io.util.DataOutputStreamAndChannel;
0: import org.apache.cassandra.utils.FBUtilities;
/////////////////////////////////////////////////////////////////////////
0:     private final ICompressor.WrappedByteBuffer compressed;
1:     private long uncompressedSize = 0, compressedSize = 0;
1:     private final ByteBuffer crcCheckBuffer = ByteBuffer.allocate(4);
1: 
0:         super(file, parameters.chunkLength(), parameters.sstableCompressor.useDirectOutputByteBuffers());
0:         compressed = compressor.useDirectOutputByteBuffers()
0:             ? new ICompressor.WrappedByteBuffer(ByteBuffer.allocateDirect(compressor.initialCompressedBufferLength(buffer.capacity())))
0:             : new ICompressor.WrappedByteBuffer(ByteBuffer.allocate(compressor.initialCompressedBufferLength(buffer.capacity())));
0:         crcMetadata = new DataIntegrityMetadata.ChecksumWriter(new DataOutputStreamAndChannel(channel));
/////////////////////////////////////////////////////////////////////////
0:             return channel.position();
/////////////////////////////////////////////////////////////////////////
1:             buffer.flip();
0:             compressed.buffer.clear();
0:             compressedLength = compressor.compress(buffer, compressed);
1: 
0:             // Compressors don't modify sentinels in our BB - we rely on buffer.position() for bufferOffset adjustment
0:             buffer.position(buffer.limit());
1:         uncompressedSize += buffer.position();
/////////////////////////////////////////////////////////////////////////
0:             assert compressedLength <= compressed.buffer.capacity();
1:             // write out the compressed data
0:             compressed.buffer.flip();
0:             channel.write(compressed.buffer);
1: 
0:             compressed.buffer.rewind();
0:             crcMetadata.appendDirect(compressed.buffer);
1: 
0:             // adjust our bufferOffset to account for the new uncompressed data we've now written out
0:             resetBuffer();
/////////////////////////////////////////////////////////////////////////
0:             return metadataWriter.open(uncompressedSize, chunkOffset, isFinal ? FINAL : SHARED_FINAL);
/////////////////////////////////////////////////////////////////////////
1:         return new CompressedFileWriterMark(chunkOffset, current(), buffer.position(), chunkCount + 1);
/////////////////////////////////////////////////////////////////////////
1:         long truncateTarget = realMark.uncDataOffset;
1:         if (realMark.chunkOffset == chunkOffset)
1:             // simply drop bytes to the right of our mark
1:             buffer.position(realMark.validBufferBytes);
1:         // synchronize current buffer with disk - we don't want any data loss
0:         if (compressed.buffer.capacity() < chunkSize)
0:             compressed.buffer = compressor.useDirectOutputByteBuffers()
0:                     ? ByteBuffer.allocateDirect(chunkSize)
0:                     : ByteBuffer.allocate(chunkSize);
0:             compressed.buffer.clear();
0:             compressed.buffer.limit(chunkSize);
0:             channel.position(chunkOffset);
0:             channel.read(compressed.buffer);
1:                 // Repopulate buffer from compressed data
1:                 buffer.clear();
0:                 compressed.buffer.flip();
0:                 compressor.uncompress(compressed.buffer, buffer);
0:             Adler32 checksum = new Adler32();
0:             FBUtilities.directCheckSum(checksum, compressed.buffer);
1: 
1:             crcCheckBuffer.clear();
0:             channel.read(crcCheckBuffer);
1:             crcCheckBuffer.flip();
1:             if (crcCheckBuffer.getInt() != (int) checksum.getValue())
/////////////////////////////////////////////////////////////////////////
1:         // Mark as dirty so we can guarantee the newly buffered bytes won't be lost on a rebuffer
1:         buffer.position(realMark.validBufferBytes);
0:         isDirty = true;
1: 
1:         bufferOffset = truncateTarget - buffer.position();
/////////////////////////////////////////////////////////////////////////
0:                 channel.position(chunkOffset);
/////////////////////////////////////////////////////////////////////////
1:             return;
1: 
0:         long finalPosition = current();
0:         sstableMetadataCollector.addCompressionRatio(compressedSize, uncompressedSize);
0:             metadataWriter.close(finalPosition, chunkCount);
/////////////////////////////////////////////////////////////////////////
1:         final int validBufferBytes;
1:         public CompressedFileWriterMark(long chunkOffset, long uncDataOffset, int validBufferBytes, int nextChunkIndex)
1:             this.validBufferBytes = validBufferBytes;
author:Jeff Jirsa
-------------------------------------------------------------------------------
commit:0e62131
/////////////////////////////////////////////////////////////////////////
0:             crcMetadata.append(compressed.buffer, 0, compressedLength, true);
author:belliottsmith
-------------------------------------------------------------------------------
commit:4e95953
/////////////////////////////////////////////////////////////////////////
0:         super(file, parameters.chunkLength());
1:         metadataWriter = CompressionMetadata.Writer.open(parameters, offsetsPath);
/////////////////////////////////////////////////////////////////////////
1:         seekToChunkStart(); // why is this necessary? seems like it should always be at chunk start in normal operation
/////////////////////////////////////////////////////////////////////////
1:             metadataWriter.addOffset(chunkOffset);
/////////////////////////////////////////////////////////////////////////
1:             lastFlushOffset += compressedLength + 4;
/////////////////////////////////////////////////////////////////////////
0:     public CompressionMetadata openEarly()
1:     {
0:         return metadataWriter.openEarly(originalSize, chunkOffset);
1:     }
1: 
0:     public CompressionMetadata openAfterClose()
1:     {
0:         assert current == originalSize;
0:         return metadataWriter.openAfterClose(current, chunkOffset);
1:     }
1: 
/////////////////////////////////////////////////////////////////////////
0:             metadataWriter.close(current, chunkCount);
author:Jonathan Ellis
-------------------------------------------------------------------------------
commit:9405ce0
/////////////////////////////////////////////////////////////////////////
0: import org.apache.cassandra.io.sstable.Descriptor;
0: import org.apache.cassandra.io.util.DataIntegrityMetadata;
0:     private final DataIntegrityMetadata.ChecksumWriter crcMetadata;
/////////////////////////////////////////////////////////////////////////
1:                                       String offsetsPath,
/////////////////////////////////////////////////////////////////////////
0:         metadataWriter = CompressionMetadata.Writer.open(offsetsPath);
0:         crcMetadata = new DataIntegrityMetadata.ChecksumWriter(out);
/////////////////////////////////////////////////////////////////////////
/////////////////////////////////////////////////////////////////////////
0:             crcMetadata.append(compressed.buffer, 0, compressedLength);
/////////////////////////////////////////////////////////////////////////
0:             Checksum checksum = new Adler32();
/////////////////////////////////////////////////////////////////////////
/////////////////////////////////////////////////////////////////////////
1:     @Override
0:     public void writeFullChecksum(Descriptor descriptor)
1:     {
0:         crcMetadata.writeFullChecksum(descriptor);
1:     }
1: 
commit:9ecda72
/////////////////////////////////////////////////////////////////////////
1: import java.io.EOFException;
1: import org.apache.cassandra.io.FSReadError;
1: import org.apache.cassandra.io.FSWriteError;
1: import org.apache.cassandra.io.sstable.CorruptSSTableException;
/////////////////////////////////////////////////////////////////////////
1:     public long getOnDiskFilePointer()
1:         try
1:         {
0:             return out.getFilePointer();
1:         }
1:         catch (IOException e)
1:         {
1:             throw new FSReadError(e, getPath());
1:         }
0:     public void sync()
1:     public void flush()
1:     protected void flushData()
1: 
0:         int compressedLength;
1:         try
1:         {
1:             // compressing data with buffer re-use
0:             compressedLength = compressor.compress(buffer, 0, validBufferBytes, compressed, 0);
1:         }
1:         catch (IOException e)
1:         {
1:             throw new RuntimeException("Compression exception", e); // shouldn't happen
1:         }
/////////////////////////////////////////////////////////////////////////
1:         try
1:         {
1:             // write an offset of the newly written chunk to the index file
0:             metadataWriter.writeLong(chunkOffset);
1:             chunkCount++;
0:             assert compressedLength <= compressed.buffer.length;
0:             // write data itself
0:             out.write(compressed.buffer, 0, compressedLength);
1:             // write corresponding checksum
0:             out.writeInt((int) checksum.getValue());
1:         }
1:         catch (IOException e)
1:         {
1:             throw new FSWriteError(e, getPath());
1:         }
/////////////////////////////////////////////////////////////////////////
0:     public synchronized void resetAndTruncate(FileMark mark)
1:         CompressedFileWriterMark realMark = (CompressedFileWriterMark) mark;
/////////////////////////////////////////////////////////////////////////
1:         try
1:         {
0:             out.seek(chunkOffset);
0:             out.readFully(compressed.buffer, 0, chunkSize);
0:             int validBytes;
1:             try
1:             {
0:                 // decompress data chunk and store its length
0:                 validBytes = compressor.uncompress(compressed.buffer, 0, chunkSize, buffer, 0);
1:             }
1:             catch (IOException e)
1:             {
1:                 throw new CorruptBlockException(getPath(), chunkOffset, chunkSize);
1:             }
0:             checksum.update(buffer, 0, validBytes);
0:             if (out.readInt() != (int) checksum.getValue())
1:                 throw new CorruptBlockException(getPath(), chunkOffset, chunkSize);
1:         }
1:         catch (CorruptBlockException e)
1:         {
1:             throw new CorruptSSTableException(e, getPath());
1:         }
1:         catch (EOFException e)
1:         {
1:             throw new CorruptSSTableException(new CorruptBlockException(getPath(), chunkOffset, chunkSize), getPath());
1:         }
1:         catch (IOException e)
1:         {
1:             throw new FSReadError(e, getPath());
1:         }
/////////////////////////////////////////////////////////////////////////
1:     private void seekToChunkStart()
1:         if (getOnDiskFilePointer() != chunkOffset)
1:         {
1:             try
1:             {
0:                 out.seek(chunkOffset);
1:             }
1:             catch (IOException e)
1:             {
1:                 throw new FSReadError(e, getPath());
1:             }
1:         }
0:     public void close()
/////////////////////////////////////////////////////////////////////////
0:         try
1:         {
0:             metadataWriter.close();
1:         }
0:         catch (IOException e)
1:         {
0:             throw new FSWriteError(e, getPath());
1:         }
commit:debb15e
/////////////////////////////////////////////////////////////////////////
0:     public static SequentialWriter open(String dataFilePath,
0:                                         String indexFilePath,
0:                                         boolean skipIOCache,
0:                                         CompressionParameters parameters,
0:                                         Collector sstableMetadataCollector)
/////////////////////////////////////////////////////////////////////////
1:     public CompressedSequentialWriter(File file,
0:                                       String indexFilePath,
0:                                       boolean skipIOCache,
0:                                       CompressionParameters parameters,
0:                                       Collector sstableMetadataCollector)
/////////////////////////////////////////////////////////////////////////
0:         metadataWriter = CompressionMetadata.Writer.open(indexFilePath);
1: 
/////////////////////////////////////////////////////////////////////////
0:             throw new CorruptBlockException(getPath(), chunkOffset, chunkSize);
commit:80d7d43
/////////////////////////////////////////////////////////////////////////
0:     public long getOnDiskFilePointer() throws IOException
1:     {
0:         return out.getFilePointer();
1:     }
1: 
1:     @Override
commit:2ce8274
/////////////////////////////////////////////////////////////////////////
0:     public long getOnDiskFilePointer() throws IOException
1:     {
0:         return out.getFilePointer();
1:     }
1: 
1:     @Override
commit:41c7424
/////////////////////////////////////////////////////////////////////////
0:     public static SequentialWriter open(String dataFilePath, String indexFilePath, boolean skipIOCache, CompressionParameters parameters) throws IOException
0:         return new CompressedSequentialWriter(new File(dataFilePath), indexFilePath, skipIOCache, parameters);
/////////////////////////////////////////////////////////////////////////
0:     public CompressedSequentialWriter(File file, String indexFilePath, boolean skipIOCache, CompressionParameters parameters) throws IOException
/////////////////////////////////////////////////////////////////////////
/////////////////////////////////////////////////////////////////////////
/////////////////////////////////////////////////////////////////////////
1: 
commit:5ab428a
/////////////////////////////////////////////////////////////////////////
0:         out.readFully(compressed, 0, chunkSize);
author:Sylvain Lebresne
-------------------------------------------------------------------------------
commit:724eabe
commit:07cdfd0
/////////////////////////////////////////////////////////////////////////
1: 
/////////////////////////////////////////////////////////////////////////
1: 
commit:2fd3268
/////////////////////////////////////////////////////////////////////////
1: 
/////////////////////////////////////////////////////////////////////////
1: 
commit:5a6e2b0
/////////////////////////////////////////////////////////////////////////
0:     private final Collector sstableMetadataCollector;
/////////////////////////////////////////////////////////////////////////
1:         final long chunkOffset;
1:         final long uncDataOffset;
0:         final int bufferOffset;
1:         final int nextChunkIndex;
commit:07cf56f
/////////////////////////////////////////////////////////////////////////
1: /*
/////////////////////////////////////////////////////////////////////////
1:  *     http://www.apache.org/licenses/LICENSE-2.0
1:  * Unless required by applicable law or agreed to in writing, software
1:  * distributed under the License is distributed on an "AS IS" BASIS,
1:  * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
1:  * See the License for the specific language governing permissions and
1:  * limitations under the License.
commit:3e77792
/////////////////////////////////////////////////////////////////////////
0:         super(file, parameters.chunkLength(), skipIOCache);
0:         this.compressor = parameters.sstableCompressor;
commit:1e36fb1
/////////////////////////////////////////////////////////////////////////
0:         compressed = new byte[Snappy.maxCompressedLength(buffer.length)];
/////////////////////////////////////////////////////////////////////////
0:         assert compressedLength <= compressed.length;
commit:1ecabe6
/////////////////////////////////////////////////////////////////////////
1: /**
1:  * Licensed to the Apache Software Foundation (ASF) under one
1:  * or more contributor license agreements.  See the NOTICE file
1:  * distributed with this work for additional information
1:  * regarding copyright ownership.  The ASF licenses this file
1:  * to you under the Apache License, Version 2.0 (the
1:  * "License"); you may not use this file except in compliance
1:  * with the License.  You may obtain a copy of the License at
1:  *
0:  *   http://www.apache.org/licenses/LICENSE-2.0
1:  *
0:  * Unless required by applicable law or agreed to in writing,
0:  * software distributed under the License is distributed on an
0:  * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
0:  * KIND, either express or implied.  See the License for the
0:  * specific language governing permissions and limitations
0:  * under the License.
1:  */
1: package org.apache.cassandra.io.compress;
1: 
1: import java.io.File;
1: import java.io.IOException;
1: 
0: import org.apache.cassandra.io.util.FileMark;
0: import org.apache.cassandra.io.util.SequentialWriter;
1: 
0: import org.xerial.snappy.Snappy;
1: 
1: public class CompressedSequentialWriter extends SequentialWriter
1: {
0:     public static final int CHUNK_LENGTH = 65536;
1: 
0:     public static SequentialWriter open(String dataFilePath, String indexFilePath, boolean skipIOCache) throws IOException
1:     {
0:         return new CompressedSequentialWriter(new File(dataFilePath), indexFilePath, skipIOCache);
1:     }
1: 
1:     // holds offset in the file where current chunk should be written
1:     // changed only by flush() method where data buffer gets compressed and stored to the file
1:     private long chunkOffset = 0;
1: 
1:     // index file writer (random I/O)
1:     private final CompressionMetadata.Writer metadataWriter;
1: 
1:     // used to store compressed data
0:     private final byte[] compressed;
1: 
1:     // holds a number of already written chunks
1:     private int chunkCount = 0;
1: 
0:     public CompressedSequentialWriter(File file, String indexFilePath, boolean skipIOCache) throws IOException
1:     {
0:         super(file, CHUNK_LENGTH, skipIOCache);
1: 
1:         // buffer for compression should be the same size as buffer itself
0:         compressed = new byte[buffer.length];
1: 
1:         /* Index File (-CompressionInfo.db component) and it's header */
0:         metadataWriter = new CompressionMetadata.Writer(indexFilePath);
0:         metadataWriter.writeHeader(Snappy.class.getSimpleName(), CHUNK_LENGTH);
1:     }
1: 
1:     @Override
0:     public void sync() throws IOException
1:     {
1:         throw new UnsupportedOperationException();
1:     }
1: 
1:     @Override
0:     public void flush() throws IOException
1:     {
1:         throw new UnsupportedOperationException();
1:     }
1: 
1:     @Override
0:     protected void flushData() throws IOException
1:     {
0:         seekToChunkStart();
1: 
0:         // compressing data with buffer re-use
0:         int compressedLength = Snappy.rawCompress(buffer, 0, validBufferBytes, compressed, 0);
1: 
0:         // write an offset of the newly written chunk to the index file
0:         metadataWriter.writeLong(chunkOffset);
0:         chunkCount++;
1: 
0:         // write data itself
0:         out.write(compressed, 0, compressedLength);
1: 
0:         // next chunk should be written right after current
0:         chunkOffset += compressedLength;
1:     }
1: 
1:     @Override
0:     public FileMark mark()
1:     {
0:         return new CompressedFileWriterMark(chunkOffset, current, validBufferBytes, chunkCount + 1);
1:     }
1: 
1:     @Override
0:     public synchronized void resetAndTruncate(FileMark mark) throws IOException
1:     {
1:         assert mark instanceof CompressedFileWriterMark;
1: 
0:         CompressedFileWriterMark realMark = ((CompressedFileWriterMark) mark);
1: 
1:         // reset position
0:         current = realMark.uncDataOffset;
1: 
0:         if (realMark.chunkOffset == chunkOffset) // current buffer
1:         {
0:             // just reset a buffer offset and return
0:             validBufferBytes = realMark.bufferOffset;
0:             return;
1:         }
1: 
0:         // synchronize current buffer with disk
0:         // because we don't want any data loss
1:         syncInternal();
1: 
0:         // setting marker as a current offset
1:         chunkOffset = realMark.chunkOffset;
1: 
0:         // compressed chunk size
0:         int chunkSize = (int) (metadataWriter.chunkOffsetBy(realMark.nextChunkIndex) - chunkOffset);
1: 
0:         out.seek(chunkOffset);
0:         out.read(compressed, 0, chunkSize);
1: 
0:         Snappy.rawUncompress(compressed, 0, chunkSize, buffer, 0);
1: 
0:         // reset buffer
0:         validBufferBytes = realMark.bufferOffset;
0:         bufferOffset = current - validBufferBytes;
1:         chunkCount = realMark.nextChunkIndex - 1;
1: 
1:         // truncate data and index file
1:         truncate(chunkOffset);
0:         metadataWriter.resetAndTruncate(realMark.nextChunkIndex);
1:     }
1: 
1:     /**
1:      * Seek to the offset where next compressed data chunk should be stored.
1:      *
0:      * @throws IOException on any I/O error.
1:      */
0:     private void seekToChunkStart() throws IOException
1:     {
0:         if (out.getFilePointer() != chunkOffset)
0:             out.seek(chunkOffset);
1:     }
1: 
1:     @Override
0:     public void close() throws IOException
1:     {
0:         if (buffer == null)
0:             return; // already closed
1: 
0:         super.close();
1: 
0:         metadataWriter.finalizeHeader(current, chunkCount);
0:         metadataWriter.close();
1:     }
1: 
1:     /**
1:      * Class to hold a mark to the position of the file
1:      */
0:     protected static class CompressedFileWriterMark implements FileMark
1:     {
1:         // chunk offset in the compressed file
0:         long chunkOffset;
1:         // uncompressed data offset (real data offset)
0:         long uncDataOffset;
1: 
0:         int bufferOffset;
0:         int nextChunkIndex;
1: 
0:         public CompressedFileWriterMark(long chunkOffset, long uncDataOffset, int bufferOffset, int nextChunkIndex)
1:         {
1:             this.chunkOffset = chunkOffset;
1:             this.uncDataOffset = uncDataOffset;
0:             this.bufferOffset = bufferOffset;
1:             this.nextChunkIndex = nextChunkIndex;
1:         }
1:     }
1: }
author:Marcus Eriksson
-------------------------------------------------------------------------------
commit:41d8a5f
/////////////////////////////////////////////////////////////////////////
1:         metadataWriter.resetAndTruncate(realMark.nextChunkIndex - 1);
commit:232906d
/////////////////////////////////////////////////////////////////////////
0: import java.util.zip.CRC32;
/////////////////////////////////////////////////////////////////////////
0: 
0:             try
0:             {
0:                 // repopulate buffer
0:                 compressor.uncompress(compressed.buffer, 0, chunkSize, buffer, 0);
0:             }
0:             catch (IOException e)
0:             {
0:                 throw new CorruptBlockException(getPath(), chunkOffset, chunkSize);
0:             }
0: 
author:Dave Brosius
-------------------------------------------------------------------------------
commit:9644f09
/////////////////////////////////////////////////////////////////////////
/////////////////////////////////////////////////////////////////////////
author:Jake Luciani
-------------------------------------------------------------------------------
commit:815b238
/////////////////////////////////////////////////////////////////////////
0: import java.util.zip.Adler32;
/////////////////////////////////////////////////////////////////////////
0:     private final Checksum checksum = new Adler32();
/////////////////////////////////////////////////////////////////////////
0:         checksum.update(compressed.buffer, 0, compressedLength);
/////////////////////////////////////////////////////////////////////////
0:             checksum.update(compressed.buffer, 0, chunkSize);
author:Pavel Yaskevich
-------------------------------------------------------------------------------
commit:0b54c99
/////////////////////////////////////////////////////////////////////////
0: import org.apache.cassandra.io.sstable.SSTableMetadata.Collector;
0:     public static SequentialWriter open(String dataFilePath, String indexFilePath, boolean skipIOCache, CompressionParameters parameters, Collector sstableMetadataCollector) throws IOException
0:         return new CompressedSequentialWriter(new File(dataFilePath), indexFilePath, skipIOCache, parameters, sstableMetadataCollector);
/////////////////////////////////////////////////////////////////////////
0:     private long originalSize = 0, compressedSize = 0;
0: 
0:     private Collector sstableMetadataCollector;
0:     
0:     public CompressedSequentialWriter(File file, String indexFilePath, boolean skipIOCache, CompressionParameters parameters, Collector sstableMetadataCollector) throws IOException
/////////////////////////////////////////////////////////////////////////
1:         this.sstableMetadataCollector = sstableMetadataCollector;
/////////////////////////////////////////////////////////////////////////
0:         originalSize += validBufferBytes;
1:         compressedSize += compressedLength;
0:         
/////////////////////////////////////////////////////////////////////////
0:         sstableMetadataCollector.addCompressionRatio(compressedSize, originalSize);
commit:84a03ab
/////////////////////////////////////////////////////////////////////////
0: import org.apache.cassandra.io.sstable.SSTableMetadata.Collector;
0:     public static SequentialWriter open(String dataFilePath, String indexFilePath, boolean skipIOCache, CompressionParameters parameters, Collector sstableMetadataCollector) throws IOException
0:         return new CompressedSequentialWriter(new File(dataFilePath), indexFilePath, skipIOCache, parameters, sstableMetadataCollector);
/////////////////////////////////////////////////////////////////////////
0:     private long originalSize = 0, compressedSize = 0;
0: 
0:     private Collector sstableMetadataCollector;
0: 
0:     public CompressedSequentialWriter(File file, String indexFilePath, boolean skipIOCache, CompressionParameters parameters, Collector sstableMetadataCollector) throws IOException
/////////////////////////////////////////////////////////////////////////
0:         this.sstableMetadataCollector = sstableMetadataCollector;
/////////////////////////////////////////////////////////////////////////
0:         originalSize += validBufferBytes;
0:         compressedSize += compressedLength;
0: 
/////////////////////////////////////////////////////////////////////////
0:         sstableMetadataCollector.addCompressionRatio(compressedSize, originalSize);
commit:c8afd76
/////////////////////////////////////////////////////////////////////////
0:     public static SequentialWriter open(String dataFilePath, String indexFilePath, boolean skipIOCache, CompressionParameters parameters) throws IOException
0:         return new CompressedSequentialWriter(new File(dataFilePath), indexFilePath, skipIOCache, parameters);
/////////////////////////////////////////////////////////////////////////
1:     private final ICompressor compressor;
0:     private final ICompressor.WrappedArray compressed;
0:     public CompressedSequentialWriter(File file, String indexFilePath, boolean skipIOCache, CompressionParameters parameters) throws IOException
0:         super(file, parameters.chunkLength, skipIOCache);
0:         this.compressor = parameters.compressor;
0:         compressed = new ICompressor.WrappedArray(new byte[compressor.initialCompressedBufferLength(buffer.length)]);
0:         metadataWriter.writeHeader(parameters);
/////////////////////////////////////////////////////////////////////////
0:         int compressedLength = compressor.compress(buffer, 0, validBufferBytes, compressed, 0);
/////////////////////////////////////////////////////////////////////////
0:         assert compressedLength <= compressed.buffer.length;
0:         out.write(compressed.buffer, 0, compressedLength);
/////////////////////////////////////////////////////////////////////////
0:         if (compressed.buffer.length < chunkSize)
0:             compressed.buffer = new byte[chunkSize];
0:         out.readFully(compressed.buffer, 0, chunkSize);
0:         int validBytes = compressor.uncompress(compressed.buffer, 0, chunkSize, buffer, 0);
commit:ba1821f
/////////////////////////////////////////////////////////////////////////
0: import java.util.zip.CRC32;
0: import java.util.zip.Checksum;
/////////////////////////////////////////////////////////////////////////
0:     private final Checksum checksum = new CRC32();
0: 
/////////////////////////////////////////////////////////////////////////
0:         // update checksum
0:         checksum.update(buffer, 0, validBufferBytes);
0: 
0:         // write data itself
0:         out.write(compressed, 0, compressedLength);
0:         // write corresponding checksum
0:         out.writeInt((int) checksum.getValue());
0: 
0:         // reset checksum object to the blank state for re-use
0:         checksum.reset();
0: 
1:         // next chunk should be written right after current + length of the checksum (int)
1:         chunkOffset += compressedLength + 4;
/////////////////////////////////////////////////////////////////////////
1:         // compressed chunk size (- 4 bytes reserved for checksum)
1:         int chunkSize = (int) (metadataWriter.chunkOffsetBy(realMark.nextChunkIndex) - chunkOffset - 4);
0:         // decompress data chunk and store its length
0:         int validBytes = Snappy.rawUncompress(compressed, 0, chunkSize, buffer, 0);
0: 
0:         checksum.update(buffer, 0, validBytes);
0: 
0:         if (out.readInt() != (int) checksum.getValue())
0:             throw new CorruptedBlockException(getPath(), chunkOffset, chunkSize);
0: 
0:         checksum.reset();
============================================================================