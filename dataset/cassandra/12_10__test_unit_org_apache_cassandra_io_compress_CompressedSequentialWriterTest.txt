1:bc7941c: /*
1:bc7941c:  * Licensed to the Apache Software Foundation (ASF) under one
1:bc7941c:  * or more contributor license agreements.  See the NOTICE file
1:bc7941c:  * distributed with this work for additional information
1:bc7941c:  * regarding copyright ownership.  The ASF licenses this file
1:bc7941c:  * to you under the Apache License, Version 2.0 (the
1:bc7941c:  * "License"); you may not use this file except in compliance
1:bc7941c:  * with the License.  You may obtain a copy of the License at
1:bc7941c:  *
1:bc7941c:  *     http://www.apache.org/licenses/LICENSE-2.0
1:bc7941c:  *
1:bc7941c:  * Unless required by applicable law or agreed to in writing, software
1:bc7941c:  * distributed under the License is distributed on an "AS IS" BASIS,
1:bc7941c:  * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
1:bc7941c:  * See the License for the specific language governing permissions and
1:bc7941c:  * limitations under the License.
1:bc7941c:  */
1:bc7941c: package org.apache.cassandra.io.compress;
10:bc7941c: 
1:8704006: import java.io.ByteArrayInputStream;
1:8704006: import java.io.DataInputStream;
1:bc7941c: import java.io.File;
1:bc7941c: import java.io.IOException;
1:bc7941c: import java.nio.ByteBuffer;
1:8704006: import java.util.*;
1:bc7941c: 
1:8704006: import static org.apache.commons.io.FileUtils.readFileToByteArray;
1:bc7941c: import static org.junit.Assert.assertEquals;
1:8704006: 
1:fb22109: import com.google.common.io.Files;
1:8704006: import org.junit.After;
1:9797511: import org.junit.BeforeClass;
1:bc7941c: import org.junit.Test;
1:bc7941c: 
1:8704006: import junit.framework.Assert;
1:5baf28d: 
1:9797511: import org.apache.cassandra.config.DatabaseDescriptor;
1:a991b64: import org.apache.cassandra.db.ClusteringComparator;
1:bc7941c: import org.apache.cassandra.db.marshal.BytesType;
1:8704006: import org.apache.cassandra.db.marshal.UTF8Type;
1:bc7941c: import org.apache.cassandra.io.sstable.metadata.MetadataCollector;
1:fb22109: import org.apache.cassandra.io.util.*;
1:b31845c: import org.apache.cassandra.schema.CompressionParams;
1:5baf28d: import org.apache.cassandra.utils.ChecksumType;
1:bc7941c: 
1:8704006: public class CompressedSequentialWriterTest extends SequentialWriterTest
2:bc7941c: {
1:b31845c:     private CompressionParams compressionParameters;
1:a991b64: 
1:9797511:     @BeforeClass
1:9797511:     public static void setupDD()
1:9797511:     {
1:9797511:         DatabaseDescriptor.daemonInitialization();
1:9797511:     }
1:9797511: 
1:bc7941c:     private void runTests(String testName) throws IOException
1:bc7941c:     {
1:bc7941c:         // Test small < 1 chunk data set
1:bc7941c:         testWrite(File.createTempFile(testName + "_small", "1"), 25);
1:8704006: 
1:bc7941c:         // Test to confirm pipeline w/chunk-aligned data writes works
1:b31845c:         testWrite(File.createTempFile(testName + "_chunkAligned", "1"), CompressionParams.DEFAULT_CHUNK_LENGTH);
1:4e29b7a: 
1:bc7941c:         // Test to confirm pipeline on non-chunk boundaries works
1:b31845c:         testWrite(File.createTempFile(testName + "_large", "1"), CompressionParams.DEFAULT_CHUNK_LENGTH * 3 + 100);
2:bc7941c:     }
1:bc7941c: 
1:bc7941c:     @Test
1:bc7941c:     public void testLZ4Writer() throws IOException
1:bc7941c:     {
1:b31845c:         compressionParameters = CompressionParams.lz4();
1:bc7941c:         runTests("LZ4");
1:bc7941c:     }
1:bc7941c: 
1:bc7941c:     @Test
1:bc7941c:     public void testDeflateWriter() throws IOException
1:bc7941c:     {
1:b31845c:         compressionParameters = CompressionParams.deflate();
1:bc7941c:         runTests("Deflate");
1:bc7941c:     }
1:bc7941c: 
1:bc7941c:     @Test
1:bc7941c:     public void testSnappyWriter() throws IOException
1:bc7941c:     {
1:b31845c:         compressionParameters = CompressionParams.snappy();
1:bc7941c:         runTests("Snappy");
1:bc7941c:     }
1:bc7941c: 
1:bc7941c:     private void testWrite(File f, int bytesToTest) throws IOException
1:bc7941c:     {
1:4e29b7a:         final String filename = f.getAbsolutePath();
1:b4133f3:         MetadataCollector sstableMetadataCollector = new MetadataCollector(new ClusteringComparator(Collections.singletonList(BytesType.instance)));
1:4e29b7a: 
1:b4133f3:         byte[] dataPre = new byte[bytesToTest];
1:b4133f3:         byte[] rawPost = new byte[bytesToTest];
1:b4133f3:         try (CompressedSequentialWriter writer = new CompressedSequentialWriter(f, filename + ".metadata",
1:b4133f3:                 null, SequentialWriterOption.DEFAULT,
1:b4133f3:                 compressionParameters,
1:b4133f3:                 sstableMetadataCollector))
1:bc7941c:         {
1:b4133f3:             Random r = new Random(42);
1:8704006: 
1:b4133f3:             // Test both write with byte[] and ByteBuffer
1:b4133f3:             r.nextBytes(dataPre);
1:b4133f3:             r.nextBytes(rawPost);
1:b4133f3:             ByteBuffer dataPost = makeBB(bytesToTest);
1:b4133f3:             dataPost.put(rawPost);
1:b4133f3:             dataPost.flip();
1:b4133f3: 
1:b4133f3:             writer.write(dataPre);
1:b4133f3:             DataPosition mark = writer.mark();
1:b4133f3: 
1:b4133f3:             // Write enough garbage to transition chunk
1:b4133f3:             for (int i = 0; i < CompressionParams.DEFAULT_CHUNK_LENGTH; i++)
1:8704006:             {
1:b4133f3:                 writer.write((byte)i);
1:8704006:             }
1:b4133f3:             writer.resetAndTruncate(mark);
1:b4133f3:             writer.write(dataPost);
1:b4133f3:             writer.finish();
1:b4133f3:         }
1:8704006: 
1:b4133f3:         assert f.exists();
1:b4133f3:         try (FileHandle.Builder builder = new FileHandle.Builder(filename).withCompressionMetadata(new CompressionMetadata(filename + ".metadata", f.length(), ChecksumType.CRC32));
1:b4133f3:              FileHandle fh = builder.complete();
1:b4133f3:              RandomAccessReader reader = fh.createReader())
1:b4133f3:         {
1:bc7941c:             assertEquals(dataPre.length + rawPost.length, reader.length());
1:bc7941c:             byte[] result = new byte[(int)reader.length()];
1:8704006: 
1:bc7941c:             reader.readFully(result);
1:bc7941c: 
1:bc7941c:             assert(reader.isEOF());
1:bc7941c:             reader.close();
1:bc7941c: 
1:bc7941c:             byte[] fullInput = new byte[bytesToTest * 2];
1:bc7941c:             System.arraycopy(dataPre, 0, fullInput, 0, dataPre.length);
1:bc7941c:             System.arraycopy(rawPost, 0, fullInput, bytesToTest, rawPost.length);
1:bc7941c:             assert Arrays.equals(result, fullInput);
1:bc7941c:         }
1:bc7941c:         finally
1:bc7941c:         {
1:bc7941c:             if (f.exists())
1:bc7941c:                 f.delete();
1:bc7941c:             File metadata = new File(f + ".metadata");
1:bc7941c:             if (metadata.exists())
1:bc7941c:                 metadata.delete();
1:bc7941c:         }
1:bc7941c:     }
1:bc7941c: 
1:bc7941c:     private ByteBuffer makeBB(int size)
1:bc7941c:     {
1:056115f:         return compressionParameters.getSstableCompressor().preferredBufferType().allocate(size);
1:bc7941c:     }
1:bc7941c: 
1:8704006:     private final List<TestableCSW> writers = new ArrayList<>();
1:8704006: 
1:8704006:     @After
1:8704006:     public void cleanup()
1:8704006:     {
1:8704006:         for (TestableCSW sw : writers)
1:8704006:             sw.cleanup();
1:8704006:         writers.clear();
1:8704006:     }
1:8704006: 
1:fb22109:     @Test
1:fb22109:     @Override
1:fb22109:     public void resetAndTruncateTest()
1:fb22109:     {
1:fb22109:         File tempFile = new File(Files.createTempDir(), "reset.txt");
1:fb22109:         File offsetsFile = FileUtils.createTempFile("compressedsequentialwriter.offset", "test");
1:fb22109:         final int bufferSize = 48;
1:fb22109:         final int writeSize = 64;
1:fb22109:         byte[] toWrite = new byte[writeSize];
1:fb22109:         try (SequentialWriter writer = new CompressedSequentialWriter(tempFile, offsetsFile.getPath(),
1:fb22109:                                                                       null, SequentialWriterOption.DEFAULT,
1:fb22109:                                                                       CompressionParams.lz4(bufferSize),
1:fb22109:                                                                       new MetadataCollector(new ClusteringComparator(UTF8Type.instance))))
1:fb22109:         {
1:fb22109:             // write bytes greather than buffer
1:fb22109:             writer.write(toWrite);
1:fb22109:             long flushedOffset = writer.getLastFlushOffset();
1:fb22109:             assertEquals(writeSize, writer.position());
1:fb22109:             // mark thi position
1:fb22109:             DataPosition pos = writer.mark();
1:fb22109:             // write another
1:fb22109:             writer.write(toWrite);
1:fb22109:             // another buffer should be flushed
1:fb22109:             assertEquals(flushedOffset * 2, writer.getLastFlushOffset());
1:fb22109:             assertEquals(writeSize * 2, writer.position());
1:fb22109:             // reset writer
1:fb22109:             writer.resetAndTruncate(pos);
1:fb22109:             // current position and flushed size should be changed
1:fb22109:             assertEquals(writeSize, writer.position());
1:fb22109:             assertEquals(flushedOffset, writer.getLastFlushOffset());
1:fb22109:             // write another byte less than buffer
1:fb22109:             writer.write(new byte[]{0});
1:fb22109:             assertEquals(writeSize + 1, writer.position());
1:fb22109:             // flush off set should not be increase
1:fb22109:             assertEquals(flushedOffset, writer.getLastFlushOffset());
1:fb22109:             writer.finish();
1:fb22109:         }
1:fb22109:         catch (IOException e)
1:fb22109:         {
1:fb22109:             Assert.fail();
1:fb22109:         }
1:fb22109:     }
1:fb22109: 
1:8704006:     protected TestableTransaction newTest() throws IOException
1:8704006:     {
1:8704006:         TestableCSW sw = new TestableCSW();
1:8704006:         writers.add(sw);
1:8704006:         return sw;
1:8704006:     }
1:8704006: 
1:8704006:     private static class TestableCSW extends TestableSW
1:8704006:     {
1:8704006:         final File offsetsFile;
1:8704006: 
1:8704006:         private TestableCSW() throws IOException
1:8704006:         {
1:8704006:             this(tempFile("compressedsequentialwriter"),
1:8704006:                  tempFile("compressedsequentialwriter.offsets"));
1:8704006:         }
1:8704006: 
1:8704006:         private TestableCSW(File file, File offsetsFile) throws IOException
1:8704006:         {
1:fb22109:             this(file, offsetsFile, new CompressedSequentialWriter(file, offsetsFile.getPath(),
1:fb22109:                                                                    null, SequentialWriterOption.DEFAULT,
1:b31845c:                                                                    CompressionParams.lz4(BUFFER_SIZE),
1:a991b64:                                                                    new MetadataCollector(new ClusteringComparator(UTF8Type.instance))));
1:8704006: 
1:8704006:         }
1:e06d411: 
1:8704006:         private TestableCSW(File file, File offsetsFile, CompressedSequentialWriter sw) throws IOException
1:8704006:         {
1:8704006:             super(file, sw);
1:8704006:             this.offsetsFile = offsetsFile;
1:8704006:         }
1:8704006: 
1:8704006:         protected void assertInProgress() throws Exception
1:8704006:         {
1:8704006:             Assert.assertTrue(file.exists());
2:8704006:             Assert.assertFalse(offsetsFile.exists());
1:8704006:             byte[] compressed = readFileToByteArray(file);
1:8704006:             byte[] uncompressed = new byte[partialContents.length];
1:e06d411:             LZ4Compressor.create(Collections.<String, String>emptyMap()).uncompress(compressed, 0, compressed.length - 4, uncompressed, 0);
1:8704006:             Assert.assertTrue(Arrays.equals(partialContents, uncompressed));
1:8704006:         }
1:8704006: 
1:8704006:         protected void assertPrepared() throws Exception
1:8704006:         {
1:8704006:             Assert.assertTrue(file.exists());
1:8704006:             Assert.assertTrue(offsetsFile.exists());
1:8704006:             DataInputStream offsets = new DataInputStream(new ByteArrayInputStream(readFileToByteArray(offsetsFile)));
1:8704006:             Assert.assertTrue(offsets.readUTF().endsWith("LZ4Compressor"));
1:8704006:             Assert.assertEquals(0, offsets.readInt());
1:8704006:             Assert.assertEquals(BUFFER_SIZE, offsets.readInt());
1:8704006:             Assert.assertEquals(fullContents.length, offsets.readLong());
1:8704006:             Assert.assertEquals(2, offsets.readInt());
1:8704006:             Assert.assertEquals(0, offsets.readLong());
1:8704006:             int offset = (int) offsets.readLong();
1:8704006:             byte[] compressed = readFileToByteArray(file);
1:8704006:             byte[] uncompressed = new byte[fullContents.length];
1:e06d411:             LZ4Compressor.create(Collections.<String, String>emptyMap()).uncompress(compressed, 0, offset - 4, uncompressed, 0);
1:e06d411:             LZ4Compressor.create(Collections.<String, String>emptyMap()).uncompress(compressed, offset, compressed.length - (4 + offset), uncompressed, partialContents.length);
1:8704006:             Assert.assertTrue(Arrays.equals(fullContents, uncompressed));
1:8704006:         }
1:8704006: 
1:8704006:         protected void assertAborted() throws Exception
1:8704006:         {
1:8704006:             super.assertAborted();
1:8704006:         }
1:8704006: 
1:8704006:         void cleanup()
1:8704006:         {
1:8704006:             file.delete();
1:8704006:             offsetsFile.delete();
1:8704006:         }
1:8704006:     }
1:8704006: 
1:bc7941c: }
============================================================================
author:Robert Stupp
-------------------------------------------------------------------------------
commit:9797511
/////////////////////////////////////////////////////////////////////////
1: import org.junit.BeforeClass;
1: import org.apache.cassandra.config.DatabaseDescriptor;
/////////////////////////////////////////////////////////////////////////
1:     @BeforeClass
1:     public static void setupDD()
1:     {
1:         DatabaseDescriptor.daemonInitialization();
1:     }
1: 
author:Yuki Morishita
-------------------------------------------------------------------------------
commit:b4133f3
/////////////////////////////////////////////////////////////////////////
/////////////////////////////////////////////////////////////////////////
1:         MetadataCollector sstableMetadataCollector = new MetadataCollector(new ClusteringComparator(Collections.singletonList(BytesType.instance)));
1:         byte[] dataPre = new byte[bytesToTest];
1:         byte[] rawPost = new byte[bytesToTest];
1:         try (CompressedSequentialWriter writer = new CompressedSequentialWriter(f, filename + ".metadata",
1:                 null, SequentialWriterOption.DEFAULT,
1:                 compressionParameters,
1:                 sstableMetadataCollector))
1:             Random r = new Random(42);
1:             // Test both write with byte[] and ByteBuffer
1:             r.nextBytes(dataPre);
1:             r.nextBytes(rawPost);
1:             ByteBuffer dataPost = makeBB(bytesToTest);
1:             dataPost.put(rawPost);
1:             dataPost.flip();
1: 
1:             writer.write(dataPre);
1:             DataPosition mark = writer.mark();
1: 
1:             // Write enough garbage to transition chunk
1:             for (int i = 0; i < CompressionParams.DEFAULT_CHUNK_LENGTH; i++)
1:                 writer.write((byte)i);
1:             writer.resetAndTruncate(mark);
1:             writer.write(dataPost);
1:             writer.finish();
1:         }
1:         assert f.exists();
1:         try (FileHandle.Builder builder = new FileHandle.Builder(filename).withCompressionMetadata(new CompressionMetadata(filename + ".metadata", f.length(), ChecksumType.CRC32));
1:              FileHandle fh = builder.complete();
1:              RandomAccessReader reader = fh.createReader())
1:         {
/////////////////////////////////////////////////////////////////////////
commit:fb22109
/////////////////////////////////////////////////////////////////////////
1: import com.google.common.io.Files;
/////////////////////////////////////////////////////////////////////////
1: import org.apache.cassandra.io.util.*;
/////////////////////////////////////////////////////////////////////////
0:             try (CompressedSequentialWriter writer = new CompressedSequentialWriter(f, filename + ".metadata",
1:                                                                                     null, SequentialWriterOption.DEFAULT,
0:                                                                                     compressionParameters,
0:                                                                                     sstableMetadataCollector))
/////////////////////////////////////////////////////////////////////////
1:     @Test
1:     @Override
1:     public void resetAndTruncateTest()
1:     {
1:         File tempFile = new File(Files.createTempDir(), "reset.txt");
1:         File offsetsFile = FileUtils.createTempFile("compressedsequentialwriter.offset", "test");
1:         final int bufferSize = 48;
1:         final int writeSize = 64;
1:         byte[] toWrite = new byte[writeSize];
1:         try (SequentialWriter writer = new CompressedSequentialWriter(tempFile, offsetsFile.getPath(),
1:                                                                       null, SequentialWriterOption.DEFAULT,
1:                                                                       CompressionParams.lz4(bufferSize),
1:                                                                       new MetadataCollector(new ClusteringComparator(UTF8Type.instance))))
1:         {
1:             // write bytes greather than buffer
1:             writer.write(toWrite);
1:             long flushedOffset = writer.getLastFlushOffset();
1:             assertEquals(writeSize, writer.position());
1:             // mark thi position
1:             DataPosition pos = writer.mark();
1:             // write another
1:             writer.write(toWrite);
1:             // another buffer should be flushed
1:             assertEquals(flushedOffset * 2, writer.getLastFlushOffset());
1:             assertEquals(writeSize * 2, writer.position());
1:             // reset writer
1:             writer.resetAndTruncate(pos);
1:             // current position and flushed size should be changed
1:             assertEquals(writeSize, writer.position());
1:             assertEquals(flushedOffset, writer.getLastFlushOffset());
1:             // write another byte less than buffer
1:             writer.write(new byte[]{0});
1:             assertEquals(writeSize + 1, writer.position());
1:             // flush off set should not be increase
1:             assertEquals(flushedOffset, writer.getLastFlushOffset());
1:             writer.finish();
1:         }
1:         catch (IOException e)
1:         {
1:             Assert.fail();
1:         }
1:     }
1: 
/////////////////////////////////////////////////////////////////////////
1:             this(file, offsetsFile, new CompressedSequentialWriter(file, offsetsFile.getPath(),
0:                                                                    null, SequentialWriterOption.DEFAULT,
author:Sylvain Lebresne
-------------------------------------------------------------------------------
commit:c9ac050
commit:78a3d2b
commit:a991b64
/////////////////////////////////////////////////////////////////////////
1: import org.apache.cassandra.db.ClusteringComparator;
0: import org.apache.cassandra.db.marshal.AbstractType;
/////////////////////////////////////////////////////////////////////////
0:             MetadataCollector sstableMetadataCollector = new MetadataCollector(new ClusteringComparator(Arrays.<AbstractType<?>>asList(BytesType.instance))).replayPosition(null);
1: 
/////////////////////////////////////////////////////////////////////////
0:             this(file, offsetsFile, new CompressedSequentialWriter(file,
0:                                                                    offsetsFile.getPath(),
0:                                                                    new CompressionParameters(LZ4Compressor.instance, BUFFER_SIZE, new HashMap<String, String>()),
1:                                                                    new MetadataCollector(new ClusteringComparator(UTF8Type.instance))));
author:Benedict Elliott Smith
-------------------------------------------------------------------------------
commit:849a438
/////////////////////////////////////////////////////////////////////////
0:             MetadataCollector sstableMetadataCollector = new MetadataCollector(new SimpleDenseCellNameType(BytesType.instance));
commit:8704006
/////////////////////////////////////////////////////////////////////////
1: import java.io.ByteArrayInputStream;
1: import java.io.DataInputStream;
1: import java.util.*;
1: import static org.apache.commons.io.FileUtils.readFileToByteArray;
1: 
1: import org.junit.After;
1: import junit.framework.Assert;
0: import org.apache.cassandra.db.composites.CellNames;
1: import org.apache.cassandra.db.marshal.UTF8Type;
0: import org.apache.cassandra.io.util.SequentialWriterTest;
1: public class CompressedSequentialWriterTest extends SequentialWriterTest
/////////////////////////////////////////////////////////////////////////
0:             try (CompressedSequentialWriter writer = new CompressedSequentialWriter(f, filename + ".metadata", new CompressionParameters(compressor), sstableMetadataCollector);)
0:                 Random r = new Random();
1: 
0:                 // Test both write with byte[] and ByteBuffer
0:                 r.nextBytes(dataPre);
0:                 r.nextBytes(rawPost);
0:                 ByteBuffer dataPost = makeBB(bytesToTest);
0:                 dataPost.put(rawPost);
0:                 dataPost.flip();
1: 
0:                 writer.write(dataPre);
0:                 FileMark mark = writer.mark();
1: 
0:                 // Write enough garbage to transition chunk
0:                 for (int i = 0; i < CompressionParameters.DEFAULT_CHUNK_LENGTH; i++)
1:                 {
0:                     writer.write((byte)i);
1:                 }
0:                 writer.resetAndTruncate(mark);
0:                 writer.write(dataPost);
0:                 writer.finish();
/////////////////////////////////////////////////////////////////////////
1: 
1:     private final List<TestableCSW> writers = new ArrayList<>();
1: 
1:     @After
1:     public void cleanup()
1:     {
1:         for (TestableCSW sw : writers)
1:             sw.cleanup();
1:         writers.clear();
1:     }
1: 
1:     protected TestableTransaction newTest() throws IOException
1:     {
1:         TestableCSW sw = new TestableCSW();
1:         writers.add(sw);
1:         return sw;
1:     }
1: 
1:     private static class TestableCSW extends TestableSW
1:     {
1:         final File offsetsFile;
1: 
1:         private TestableCSW() throws IOException
1:         {
1:             this(tempFile("compressedsequentialwriter"),
1:                  tempFile("compressedsequentialwriter.offsets"));
1:         }
1: 
1:         private TestableCSW(File file, File offsetsFile) throws IOException
1:         {
0:             this(file, offsetsFile, new CompressedSequentialWriter(file, offsetsFile.getPath(), new CompressionParameters(LZ4Compressor.instance, BUFFER_SIZE, new HashMap<String, String>()), new MetadataCollector(CellNames.fromAbstractType(UTF8Type.instance, false))));
1:         }
1: 
1:         private TestableCSW(File file, File offsetsFile, CompressedSequentialWriter sw) throws IOException
1:         {
1:             super(file, sw);
1:             this.offsetsFile = offsetsFile;
1:         }
1: 
1:         protected void assertInProgress() throws Exception
1:         {
1:             Assert.assertTrue(file.exists());
1:             Assert.assertFalse(offsetsFile.exists());
1:             byte[] compressed = readFileToByteArray(file);
1:             byte[] uncompressed = new byte[partialContents.length];
0:             LZ4Compressor.instance.uncompress(compressed, 0, compressed.length - 4, uncompressed, 0);
1:             Assert.assertTrue(Arrays.equals(partialContents, uncompressed));
1:         }
1: 
1:         protected void assertPrepared() throws Exception
1:         {
1:             Assert.assertTrue(file.exists());
1:             Assert.assertTrue(offsetsFile.exists());
1:             DataInputStream offsets = new DataInputStream(new ByteArrayInputStream(readFileToByteArray(offsetsFile)));
1:             Assert.assertTrue(offsets.readUTF().endsWith("LZ4Compressor"));
1:             Assert.assertEquals(0, offsets.readInt());
1:             Assert.assertEquals(BUFFER_SIZE, offsets.readInt());
1:             Assert.assertEquals(fullContents.length, offsets.readLong());
1:             Assert.assertEquals(2, offsets.readInt());
1:             Assert.assertEquals(0, offsets.readLong());
1:             int offset = (int) offsets.readLong();
1:             byte[] compressed = readFileToByteArray(file);
1:             byte[] uncompressed = new byte[fullContents.length];
0:             LZ4Compressor.instance.uncompress(compressed, 0, offset - 4, uncompressed, 0);
0:             LZ4Compressor.instance.uncompress(compressed, offset, compressed.length - (4 + offset), uncompressed, partialContents.length);
1:             Assert.assertTrue(Arrays.equals(fullContents, uncompressed));
1:         }
1: 
1:         protected void assertAborted() throws Exception
1:         {
1:             super.assertAborted();
1:             Assert.assertFalse(offsetsFile.exists());
1:         }
1: 
1:         void cleanup()
1:         {
1:             file.delete();
1:             offsetsFile.delete();
1:         }
1:     }
1: 
author:Branimir Lambov
-------------------------------------------------------------------------------
commit:30bb255
/////////////////////////////////////////////////////////////////////////
0:             RandomAccessReader reader = RandomAccessReader.builder(channel).compression(new CompressionMetadata(filename + ".metadata", f.length(), ChecksumType.CRC32)).build();
commit:3adfd15
/////////////////////////////////////////////////////////////////////////
0:         return compressor.preferredBufferType().allocate(size);
author:Michael Kjellman
-------------------------------------------------------------------------------
commit:e06d411
/////////////////////////////////////////////////////////////////////////
1: 
/////////////////////////////////////////////////////////////////////////
1:             LZ4Compressor.create(Collections.<String, String>emptyMap()).uncompress(compressed, 0, compressed.length - 4, uncompressed, 0);
/////////////////////////////////////////////////////////////////////////
1:             LZ4Compressor.create(Collections.<String, String>emptyMap()).uncompress(compressed, 0, offset - 4, uncompressed, 0);
1:             LZ4Compressor.create(Collections.<String, String>emptyMap()).uncompress(compressed, offset, compressed.length - (4 + offset), uncompressed, partialContents.length);
author:Paulo Motta
-------------------------------------------------------------------------------
commit:e8651b6
/////////////////////////////////////////////////////////////////////////
0: import org.apache.cassandra.io.util.DataPosition;
/////////////////////////////////////////////////////////////////////////
0:                 DataPosition mark = writer.mark();
author:Stefania Alborghetti
-------------------------------------------------------------------------------
commit:c163d0b
/////////////////////////////////////////////////////////////////////////
commit:ce63ccc
/////////////////////////////////////////////////////////////////////////
0:             RandomAccessReader reader = new CompressedRandomAccessReader.Builder(channel, new CompressionMetadata(filename + ".metadata", f.length(), ChecksumType.CRC32)).build();
commit:4e29b7a
/////////////////////////////////////////////////////////////////////////
0: import org.apache.cassandra.io.util.ChannelProxy;
/////////////////////////////////////////////////////////////////////////
1:         final String filename = f.getAbsolutePath();
0:         final ChannelProxy channel = new ChannelProxy(f);
1: 
/////////////////////////////////////////////////////////////////////////
0:             RandomAccessReader reader = CompressedRandomAccessReader.open(channel, new CompressionMetadata(filename + ".metadata", f.length()));
/////////////////////////////////////////////////////////////////////////
0:             channel.close();
1: 
author:Ariel Weisberg
-------------------------------------------------------------------------------
commit:29687a8
/////////////////////////////////////////////////////////////////////////
0:                 Random r = new Random(42);
commit:5baf28d
/////////////////////////////////////////////////////////////////////////
1: 
/////////////////////////////////////////////////////////////////////////
1: import org.apache.cassandra.utils.ChecksumType;
/////////////////////////////////////////////////////////////////////////
0:             RandomAccessReader reader = CompressedRandomAccessReader.open(channel, new CompressionMetadata(filename + ".metadata", f.length(), ChecksumType.CRC32));
author:Aleksey Yeschenko
-------------------------------------------------------------------------------
commit:b31845c
/////////////////////////////////////////////////////////////////////////
1: import org.apache.cassandra.schema.CompressionParams;
1:     private CompressionParams compressionParameters;
/////////////////////////////////////////////////////////////////////////
1:         testWrite(File.createTempFile(testName + "_chunkAligned", "1"), CompressionParams.DEFAULT_CHUNK_LENGTH);
1:         testWrite(File.createTempFile(testName + "_large", "1"), CompressionParams.DEFAULT_CHUNK_LENGTH * 3 + 100);
1:         compressionParameters = CompressionParams.lz4();
1:         compressionParameters = CompressionParams.deflate();
1:         compressionParameters = CompressionParams.snappy();
/////////////////////////////////////////////////////////////////////////
0:                 for (int i = 0; i < CompressionParams.DEFAULT_CHUNK_LENGTH; i++)
/////////////////////////////////////////////////////////////////////////
1:                                                                    CompressionParams.lz4(BUFFER_SIZE),
author:blerer
-------------------------------------------------------------------------------
commit:056115f
/////////////////////////////////////////////////////////////////////////
0:     private CompressionParameters compressionParameters;
/////////////////////////////////////////////////////////////////////////
0:         compressionParameters = CompressionParameters.lz4();
0:         compressionParameters = CompressionParameters.deflate();
0:         compressionParameters = CompressionParameters.snappy();
/////////////////////////////////////////////////////////////////////////
0:             try (CompressedSequentialWriter writer = new CompressedSequentialWriter(f, filename + ".metadata", compressionParameters, sstableMetadataCollector);)
/////////////////////////////////////////////////////////////////////////
1:         return compressionParameters.getSstableCompressor().preferredBufferType().allocate(size);
/////////////////////////////////////////////////////////////////////////
0:                                                                    CompressionParameters.lz4(BUFFER_SIZE),
author:Joshua McKenzie
-------------------------------------------------------------------------------
commit:bc7941c
/////////////////////////////////////////////////////////////////////////
1: /*
1:  * Licensed to the Apache Software Foundation (ASF) under one
1:  * or more contributor license agreements.  See the NOTICE file
1:  * distributed with this work for additional information
1:  * regarding copyright ownership.  The ASF licenses this file
1:  * to you under the Apache License, Version 2.0 (the
1:  * "License"); you may not use this file except in compliance
1:  * with the License.  You may obtain a copy of the License at
1:  *
1:  *     http://www.apache.org/licenses/LICENSE-2.0
1:  *
1:  * Unless required by applicable law or agreed to in writing, software
1:  * distributed under the License is distributed on an "AS IS" BASIS,
1:  * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
1:  * See the License for the specific language governing permissions and
1:  * limitations under the License.
1:  */
1: package org.apache.cassandra.io.compress;
1: 
1: import java.io.File;
1: import java.io.IOException;
1: import java.nio.ByteBuffer;
0: import java.util.Arrays;
0: import java.util.Random;
1: 
1: import static org.junit.Assert.assertEquals;
1: import org.junit.Test;
1: 
0: import org.apache.cassandra.db.composites.SimpleDenseCellNameType;
1: import org.apache.cassandra.db.marshal.BytesType;
1: import org.apache.cassandra.io.sstable.metadata.MetadataCollector;
0: import org.apache.cassandra.io.util.FileMark;
0: import org.apache.cassandra.io.util.RandomAccessReader;
1: 
0: public class CompressedSequentialWriterTest
1: {
0:     private ICompressor compressor;
1: 
1:     private void runTests(String testName) throws IOException
1:     {
1:         // Test small < 1 chunk data set
1:         testWrite(File.createTempFile(testName + "_small", "1"), 25);
1: 
1:         // Test to confirm pipeline w/chunk-aligned data writes works
0:         testWrite(File.createTempFile(testName + "_chunkAligned", "1"), CompressionParameters.DEFAULT_CHUNK_LENGTH);
1: 
1:         // Test to confirm pipeline on non-chunk boundaries works
0:         testWrite(File.createTempFile(testName + "_large", "1"), CompressionParameters.DEFAULT_CHUNK_LENGTH * 3 + 100);
1:     }
1: 
1:     @Test
1:     public void testLZ4Writer() throws IOException
1:     {
0:         compressor = LZ4Compressor.instance;
1:         runTests("LZ4");
1:     }
1: 
1:     @Test
1:     public void testDeflateWriter() throws IOException
1:     {
0:         compressor = DeflateCompressor.instance;
1:         runTests("Deflate");
1:     }
1: 
1:     @Test
1:     public void testSnappyWriter() throws IOException
1:     {
0:         compressor = SnappyCompressor.instance;
1:         runTests("Snappy");
1:     }
1: 
1:     private void testWrite(File f, int bytesToTest) throws IOException
1:     {
0:         try
1:         {
0:             final String filename = f.getAbsolutePath();
1: 
0:             MetadataCollector sstableMetadataCollector = new MetadataCollector(new SimpleDenseCellNameType(BytesType.instance)).replayPosition(null);
0:             CompressedSequentialWriter writer = new CompressedSequentialWriter(f, filename + ".metadata", new CompressionParameters(compressor), sstableMetadataCollector);
1: 
0:             byte[] dataPre = new byte[bytesToTest];
0:             byte[] rawPost = new byte[bytesToTest];
0:             Random r = new Random();
1: 
0:             // Test both write with byte[] and ByteBuffer
0:             r.nextBytes(dataPre);
0:             r.nextBytes(rawPost);
0:             ByteBuffer dataPost = makeBB(bytesToTest);
0:             dataPost.put(rawPost);
0:             dataPost.flip();
1: 
0:             writer.write(dataPre);
0:             FileMark mark = writer.mark();
1: 
0:             // Write enough garbage to transition chunk
0:             for (int i = 0; i < CompressionParameters.DEFAULT_CHUNK_LENGTH; i++)
1:             {
0:                 writer.write((byte)i);
1:             }
0:             writer.resetAndTruncate(mark);
0:             writer.write(dataPost);
0:             writer.close();
1: 
0:             assert f.exists();
0:             RandomAccessReader reader = CompressedRandomAccessReader.open(filename, new CompressionMetadata(filename + ".metadata", f.length()));
1:             assertEquals(dataPre.length + rawPost.length, reader.length());
1:             byte[] result = new byte[(int)reader.length()];
1: 
1:             reader.readFully(result);
1: 
1:             assert(reader.isEOF());
1:             reader.close();
1: 
1:             byte[] fullInput = new byte[bytesToTest * 2];
1:             System.arraycopy(dataPre, 0, fullInput, 0, dataPre.length);
1:             System.arraycopy(rawPost, 0, fullInput, bytesToTest, rawPost.length);
1:             assert Arrays.equals(result, fullInput);
1:         }
1:         finally
1:         {
0:             // cleanup
1:             if (f.exists())
1:                 f.delete();
1:             File metadata = new File(f + ".metadata");
1:             if (metadata.exists())
1:                 metadata.delete();
1:         }
1:     }
1: 
1:     private ByteBuffer makeBB(int size)
1:     {
0:         return compressor.useDirectOutputByteBuffers()
0:                 ? ByteBuffer.allocateDirect(size)
0:                 : ByteBuffer.allocate(size);
1:     }
1: }
============================================================================