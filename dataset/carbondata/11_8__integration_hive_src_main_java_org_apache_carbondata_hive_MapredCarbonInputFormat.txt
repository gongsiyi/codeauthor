1:a700f83: /*
1:a700f83:  * Licensed to the Apache Software Foundation (ASF) under one or more
1:a700f83:  * contributor license agreements.  See the NOTICE file distributed with
1:a700f83:  * this work for additional information regarding copyright ownership.
1:a700f83:  * The ASF licenses this file to You under the Apache License, Version 2.0
1:a700f83:  * (the "License"); you may not use this file except in compliance with
1:a700f83:  * the License.  You may obtain a copy of the License at
1:cbe1419:  *
1:a700f83:  *    http://www.apache.org/licenses/LICENSE-2.0
2:a700f83:  *
1:a700f83:  * Unless required by applicable law or agreed to in writing, software
1:a700f83:  * distributed under the License is distributed on an "AS IS" BASIS,
1:a700f83:  * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
1:a700f83:  * See the License for the specific language governing permissions and
1:a700f83:  * limitations under the License.
1:cbe1419:  */
1:a700f83: package org.apache.carbondata.hive;
1:0da86b6: 
1:a700f83: import java.io.IOException;
1:cbe1419: import java.util.ArrayList;
1:a700f83: import java.util.List;
9:a700f83: 
1:a324e5d: import org.apache.carbondata.common.logging.LogService;
1:a324e5d: import org.apache.carbondata.common.logging.LogServiceFactory;
1:a324e5d: import org.apache.carbondata.core.exception.InvalidConfigurationException;
1:a700f83: import org.apache.carbondata.core.metadata.AbsoluteTableIdentifier;
1:df5d7a9: import org.apache.carbondata.core.metadata.schema.SchemaReader;
1:a700f83: import org.apache.carbondata.core.metadata.schema.table.CarbonTable;
1:cbe1419: import org.apache.carbondata.core.metadata.schema.table.column.CarbonColumn;
1:a700f83: import org.apache.carbondata.core.scan.model.QueryModel;
1:3ff574d: import org.apache.carbondata.core.scan.model.QueryModelBuilder;
1:09f7cdd: import org.apache.carbondata.core.util.DataTypeConverterImpl;
1:2018048: import org.apache.carbondata.core.util.ObjectSerializationUtil;
1:a700f83: import org.apache.carbondata.hadoop.CarbonInputSplit;
1:a324e5d: import org.apache.carbondata.hadoop.api.CarbonTableInputFormat;
1:a700f83: import org.apache.carbondata.hadoop.readsupport.CarbonReadSupport;
1:a700f83: 
1:a700f83: import org.apache.hadoop.conf.Configuration;
1:cbe1419: import org.apache.hadoop.fs.InvalidPathException;
1:a700f83: import org.apache.hadoop.fs.Path;
1:a700f83: import org.apache.hadoop.hive.ql.io.CombineHiveInputFormat;
1:a700f83: import org.apache.hadoop.io.ArrayWritable;
1:a700f83: import org.apache.hadoop.mapred.InputFormat;
1:a700f83: import org.apache.hadoop.mapred.InputSplit;
1:a700f83: import org.apache.hadoop.mapred.JobConf;
1:a700f83: import org.apache.hadoop.mapred.RecordReader;
1:a700f83: import org.apache.hadoop.mapred.Reporter;
1:a700f83: import org.apache.hadoop.mapreduce.Job;
1:cbe1419: import org.apache.hadoop.util.StringUtils;
1:a700f83: 
1:a324e5d: public class MapredCarbonInputFormat extends CarbonTableInputFormat<ArrayWritable>
1:a700f83:     implements InputFormat<Void, ArrayWritable>, CombineHiveInputFormat.AvoidSplitCombination {
1:cbe1419:   private static final String CARBON_TABLE = "mapreduce.input.carboninputformat.table";
1:a700f83: 
1:a324e5d:   private LogService LOGGER = LogServiceFactory.getLogService(this.getClass().getCanonicalName());
1:a324e5d: 
1:27b178d:   /**
1:27b178d:    * this method will read the schema from the physical file and populate into CARBON_TABLE
1:27b178d:    *
1:27b178d:    * @param configuration
1:27b178d:    * @throws IOException
1:27b178d:    */
1:27b178d:   private static void populateCarbonTable(Configuration configuration, String paths)
1:a324e5d:       throws IOException, InvalidConfigurationException {
1:27b178d:     String dirs = configuration.get(INPUT_DIR, "");
1:27b178d:     String[] inputPaths = StringUtils.split(dirs);
1:27b178d:     String validInputPath = null;
1:27b178d:     if (inputPaths.length == 0) {
1:27b178d:       throw new InvalidPathException("No input paths specified in job");
1:27b178d:     } else {
1:27b178d:       if (paths != null) {
1:27b178d:         for (String inputPath : inputPaths) {
1:27b178d:           if (paths.startsWith(inputPath.replace("file:", ""))) {
1:27b178d:             validInputPath = inputPath;
1:27b178d:             break;
1:27b178d:           }
1:27b178d:         }
1:27b178d:       }
1:27b178d:     }
1:7ef9164:     if (null != validInputPath) {
1:7ef9164:       AbsoluteTableIdentifier absoluteTableIdentifier = AbsoluteTableIdentifier
1:7ef9164:           .from(validInputPath, getDatabaseName(configuration), getTableName(configuration));
1:7ef9164:       // read the schema file to get the absoluteTableIdentifier having the correct table id
1:7ef9164:       // persisted in the schema
1:7ef9164:       CarbonTable carbonTable = SchemaReader.readCarbonTableFromStore(absoluteTableIdentifier);
1:7ef9164:       configuration.set(CARBON_TABLE, ObjectSerializationUtil.convertObjectToString(carbonTable));
1:7ef9164:       setTableInfo(configuration, carbonTable.getTableInfo());
1:7ef9164:     } else {
1:7ef9164:       throw new InvalidPathException("No input paths specified in job");
1:7ef9164:     }
1:27b178d:   }
1:27b178d: 
1:27b178d:   private static CarbonTable getCarbonTable(Configuration configuration, String path)
1:a324e5d:       throws IOException, InvalidConfigurationException {
1:27b178d:     populateCarbonTable(configuration, path);
1:27b178d:     // read it from schema file in the store
1:27b178d:     String carbonTableStr = configuration.get(CARBON_TABLE);
1:27b178d:     return (CarbonTable) ObjectSerializationUtil.convertStringToObject(carbonTableStr);
1:27b178d:   }
1:27b178d: 
1:0da86b6:   @Override public InputSplit[] getSplits(JobConf jobConf, int numSplits) throws IOException {
1:a700f83:     org.apache.hadoop.mapreduce.JobContext jobContext = Job.getInstance(jobConf);
1:a700f83:     List<org.apache.hadoop.mapreduce.InputSplit> splitList = super.getSplits(jobContext);
1:a700f83:     InputSplit[] splits = new InputSplit[splitList.size()];
1:d408a8d:     CarbonInputSplit split;
1:a700f83:     for (int i = 0; i < splitList.size(); i++) {
1:a700f83:       split = (CarbonInputSplit) splitList.get(i);
1:0da86b6:       splits[i] = new CarbonHiveInputSplit(split.getSegmentId(), split.getPath(), split.getStart(),
1:0da86b6:           split.getLength(), split.getLocations(), split.getNumberOfBlocklets(), split.getVersion(),
1:0da86b6:           split.getBlockStorageIdMap());
1:cbe1419:     }
1:a700f83:     return splits;
1:cbe1419:   }
1:a700f83: 
3:a700f83:   @Override
1:a700f83:   public RecordReader<Void, ArrayWritable> getRecordReader(InputSplit inputSplit, JobConf jobConf,
1:0da86b6:       Reporter reporter) throws IOException {
1:cbe1419:     String path = null;
1:cbe1419:     if (inputSplit instanceof CarbonHiveInputSplit) {
1:cbe1419:       path = ((CarbonHiveInputSplit) inputSplit).getPath().toString();
1:cbe1419:     }
1:a324e5d:     QueryModel queryModel = null;
1:a324e5d:     try {
1:a324e5d:       queryModel = getQueryModel(jobConf, path);
1:a324e5d:     } catch (InvalidConfigurationException e) {
1:a324e5d:       LOGGER.error("Failed to create record reader: " + e.getMessage());
1:a324e5d:       return null;
1:a324e5d:     }
1:cbe1419:     CarbonReadSupport<ArrayWritable> readSupport = new CarbonDictionaryDecodeReadSupport<>();
1:a700f83:     return new CarbonHiveRecordReader(queryModel, readSupport, inputSplit, jobConf);
1:cbe1419:   }
1:cbe1419: 
1:a324e5d:   private QueryModel getQueryModel(Configuration configuration, String path)
1:a324e5d:       throws IOException, InvalidConfigurationException {
1:cbe1419:     CarbonTable carbonTable = getCarbonTable(configuration, path);
1:a700f83:     AbsoluteTableIdentifier identifier = carbonTable.getAbsoluteTableIdentifier();
1:daa6465:     String projectionString = getProjection(configuration, carbonTable,
1:cbe1419:         identifier.getCarbonTableIdentifier().getTableName());
1:daa6465:     String[] projectionColumns = projectionString.split(",");
1:3ff574d:     QueryModel queryModel =
1:3ff574d:         new QueryModelBuilder(carbonTable)
1:3ff574d:             .projectColumns(projectionColumns)
1:3ff574d:             .filterExpression(getFilterPredicates(configuration))
1:3ff574d:             .dataConverter(new DataTypeConverterImpl())
1:3ff574d:             .build();
1:cbe1419: 
1:a700f83:     return queryModel;
1:cbe1419:   }
1:cbe1419: 
1:cbe1419:   /**
1:cbe1419:    * Return the Projection for the CarbonQuery.
1:cbe1419:    *
1:cbe1419:    * @param configuration
1:cbe1419:    * @param carbonTable
1:cbe1419:    * @param tableName
1:cbe1419:    * @return
1:cbe1419:    */
1:cbe1419:   private String getProjection(Configuration configuration, CarbonTable carbonTable,
1:cbe1419:       String tableName) {
1:cbe1419:     // query plan includes projection column
1:cbe1419:     String projection = getColumnProjection(configuration);
1:cbe1419:     if (projection == null) {
1:cbe1419:       projection = configuration.get("hive.io.file.readcolumn.names");
1:0da86b6:     }
1:cbe1419:     List<CarbonColumn> carbonColumns = carbonTable.getCreateOrderColumn(tableName);
1:cbe1419:     List<String> carbonColumnNames = new ArrayList<>();
1:cbe1419:     StringBuilder allColumns = new StringBuilder();
1:cbe1419:     StringBuilder projectionColumns = new StringBuilder();
1:cbe1419:     for (CarbonColumn column : carbonColumns) {
1:c15a11d:       carbonColumnNames.add(column.getColName().toLowerCase());
1:cbe1419:       allColumns.append(column.getColName() + ",");
1:0da86b6:     }
1:0da86b6: 
1:cbe1419:     if (!projection.equals("")) {
1:cbe1419:       String[] columnNames = projection.split(",");
1:cbe1419:       //verify that the columns parsed by Hive exist in the table
1:cbe1419:       for (String col : columnNames) {
1:cbe1419:         //show columns command will return these data
1:c15a11d:         if (carbonColumnNames.contains(col.toLowerCase())) {
1:cbe1419:           projectionColumns.append(col + ",");
1:0da86b6:         }
1:0da86b6:       }
1:cbe1419:       return projectionColumns.substring(0, projectionColumns.lastIndexOf(","));
1:0da86b6:     } else {
1:cbe1419:       return allColumns.substring(0, allColumns.lastIndexOf(","));
1:0da86b6:     }
1:0da86b6:   }
1:cbe1419: 
1:0da86b6:   @Override public boolean shouldSkipCombine(Path path, Configuration conf) throws IOException {
1:a700f83:     return true;
1:cbe1419:   }
1:cbe1419: }
============================================================================
author:Raghunandan S
-------------------------------------------------------------------------------
commit:7ef9164
/////////////////////////////////////////////////////////////////////////
1:     if (null != validInputPath) {
1:       AbsoluteTableIdentifier absoluteTableIdentifier = AbsoluteTableIdentifier
1:           .from(validInputPath, getDatabaseName(configuration), getTableName(configuration));
1:       // read the schema file to get the absoluteTableIdentifier having the correct table id
1:       // persisted in the schema
1:       CarbonTable carbonTable = SchemaReader.readCarbonTableFromStore(absoluteTableIdentifier);
1:       configuration.set(CARBON_TABLE, ObjectSerializationUtil.convertObjectToString(carbonTable));
1:       setTableInfo(configuration, carbonTable.getTableInfo());
1:     } else {
1:       throw new InvalidPathException("No input paths specified in job");
1:     }
commit:2d24e18
/////////////////////////////////////////////////////////////////////////
commit:06ddd82
/////////////////////////////////////////////////////////////////////////
0:     setTableInfo(configuration, carbonTable.getTableInfo());
author:akashrn5
-------------------------------------------------------------------------------
commit:2018048
/////////////////////////////////////////////////////////////////////////
1: import org.apache.carbondata.core.util.ObjectSerializationUtil;
author:Jacky Li
-------------------------------------------------------------------------------
commit:3ff574d
/////////////////////////////////////////////////////////////////////////
1: import org.apache.carbondata.core.scan.model.QueryModelBuilder;
/////////////////////////////////////////////////////////////////////////
1:     QueryModel queryModel =
1:         new QueryModelBuilder(carbonTable)
1:             .projectColumns(projectionColumns)
1:             .filterExpression(getFilterPredicates(configuration))
1:             .dataConverter(new DataTypeConverterImpl())
1:             .build();
commit:df5d7a9
/////////////////////////////////////////////////////////////////////////
1: import org.apache.carbondata.core.metadata.schema.SchemaReader;
/////////////////////////////////////////////////////////////////////////
/////////////////////////////////////////////////////////////////////////
0:     carbonTable.processFilterExpression(filter, null, null);
0:     FilterResolverIntf filterIntf = carbonTable.resolveFilter(filter, tableProvider);
commit:daa6465
/////////////////////////////////////////////////////////////////////////
/////////////////////////////////////////////////////////////////////////
1:     String projectionString = getProjection(configuration, carbonTable,
1:     String[] projectionColumns = projectionString.split(",");
0:     QueryModel queryModel = carbonTable.createQueryWithProjection(
0:         projectionColumns, new DataTypeConverterImpl());
commit:a324e5d
/////////////////////////////////////////////////////////////////////////
1: import org.apache.carbondata.common.logging.LogService;
1: import org.apache.carbondata.common.logging.LogServiceFactory;
1: import org.apache.carbondata.core.exception.InvalidConfigurationException;
/////////////////////////////////////////////////////////////////////////
1: import org.apache.carbondata.hadoop.api.CarbonTableInputFormat;
/////////////////////////////////////////////////////////////////////////
1: public class MapredCarbonInputFormat extends CarbonTableInputFormat<ArrayWritable>
1:   private LogService LOGGER = LogServiceFactory.getLogService(this.getClass().getCanonicalName());
1: 
/////////////////////////////////////////////////////////////////////////
1:       throws IOException, InvalidConfigurationException {
/////////////////////////////////////////////////////////////////////////
1:       throws IOException, InvalidConfigurationException {
/////////////////////////////////////////////////////////////////////////
1:     QueryModel queryModel = null;
1:     try {
1:       queryModel = getQueryModel(jobConf, path);
1:     } catch (InvalidConfigurationException e) {
1:       LOGGER.error("Failed to create record reader: " + e.getMessage());
1:       return null;
1:     }
1:   private QueryModel getQueryModel(Configuration configuration, String path)
1:       throws IOException, InvalidConfigurationException {
author:mohammadshahidkhan
-------------------------------------------------------------------------------
commit:1155d4d
/////////////////////////////////////////////////////////////////////////
0:     AbsoluteTableIdentifier absoluteTableIdentifier = AbsoluteTableIdentifier
0:         .from(validInputPath, getDatabaseName(configuration), getTableName(configuration));
author:QiangCai
-------------------------------------------------------------------------------
commit:d7393da
/////////////////////////////////////////////////////////////////////////
0:     CarbonInputFormatUtil.processFilterExpression(filter, carbonTable, null, null);
author:anubhav100
-------------------------------------------------------------------------------
commit:c15a11d
/////////////////////////////////////////////////////////////////////////
1:       carbonColumnNames.add(column.getColName().toLowerCase());
/////////////////////////////////////////////////////////////////////////
1:         if (carbonColumnNames.contains(col.toLowerCase())) {
commit:0da86b6
/////////////////////////////////////////////////////////////////////////
0: import org.apache.carbondata.core.metadata.schema.table.column.CarbonDimension;
0: import org.apache.carbondata.core.metadata.schema.table.column.CarbonMeasure;
/////////////////////////////////////////////////////////////////////////
1:   @Override public InputSplit[] getSplits(JobConf jobConf, int numSplits) throws IOException {
1:       splits[i] = new CarbonHiveInputSplit(split.getSegmentId(), split.getPath(), split.getStart(),
1:           split.getLength(), split.getLocations(), split.getNumberOfBlocklets(), split.getVersion(),
1:           split.getBlockStorageIdMap());
1:       Reporter reporter) throws IOException {
/////////////////////////////////////////////////////////////////////////
1: 
0:     String colNames = "";
0:     if (projection.equals("")) {
0:       List<CarbonDimension> carbonDimensionList = carbonTable.getAllDimensions();
0:       List<CarbonMeasure> carbonMeasureList = carbonTable.getAllMeasures();
1: 
0:       for (CarbonDimension aCarbonDimensionList : carbonDimensionList) {
0:         colNames = (colNames + (aCarbonDimensionList.getColName())) + ",";
1:       }
0:       if (carbonMeasureList.size() < 1) {
0:         colNames = colNames.substring(0, colNames.lastIndexOf(","));
1:       }
0:       for (int index = 0; index < carbonMeasureList.size(); index++) {
0:         if (!carbonMeasureList.get(index).getColName().equals("default_dummy_measure")) {
0:           if (index == carbonMeasureList.size() - 1) {
0:             colNames = (colNames + (carbonMeasureList.get(index).getColName()));
1:           } else {
0:             colNames = (colNames + (carbonMeasureList.get(index).getColName())) + ",";
1:           }
1:         }
1:       }
0:       projection = colNames.trim();
0:       configuration.set("hive.io.file.readcolumn.names", colNames);
1:     }
/////////////////////////////////////////////////////////////////////////
1:   @Override public boolean shouldSkipCombine(Path path, Configuration conf) throws IOException {
author:dhatchayani
-------------------------------------------------------------------------------
commit:d3a09e2
/////////////////////////////////////////////////////////////////////////
0: import org.apache.carbondata.core.scan.filter.SingleTableProvider;
0: import org.apache.carbondata.core.scan.filter.TableProvider;
/////////////////////////////////////////////////////////////////////////
0:     TableProvider tableProvider = new SingleTableProvider(carbonTable);
/////////////////////////////////////////////////////////////////////////
0:     FilterResolverIntf filterIntf =
0:         CarbonInputFormatUtil.resolveFilter(filter, identifier, tableProvider);
author:shivangi
-------------------------------------------------------------------------------
commit:27b178d
/////////////////////////////////////////////////////////////////////////
1:   /**
1:    * this method will read the schema from the physical file and populate into CARBON_TABLE
1:    *
1:    * @param configuration
1:    * @throws IOException
1:    */
1:   private static void populateCarbonTable(Configuration configuration, String paths)
0:       throws IOException {
1:     String dirs = configuration.get(INPUT_DIR, "");
1:     String[] inputPaths = StringUtils.split(dirs);
1:     String validInputPath = null;
1:     if (inputPaths.length == 0) {
1:       throw new InvalidPathException("No input paths specified in job");
1:     } else {
1:       if (paths != null) {
1:         for (String inputPath : inputPaths) {
1:           if (paths.startsWith(inputPath.replace("file:", ""))) {
1:             validInputPath = inputPath;
1:             break;
1:           }
1:         }
1:       }
1:     }
0:     AbsoluteTableIdentifier absoluteTableIdentifier =
0:         AbsoluteTableIdentifier.fromTablePath(validInputPath);
0:     // read the schema file to get the absoluteTableIdentifier having the correct table id
0:     // persisted in the schema
0:     CarbonTable carbonTable = SchemaReader.readCarbonTableFromStore(absoluteTableIdentifier);
0:     configuration.set(CARBON_TABLE, ObjectSerializationUtil.convertObjectToString(carbonTable));
0:     setTableInfo(configuration, carbonTable.getTableInfo());
1:   }
1: 
1:   private static CarbonTable getCarbonTable(Configuration configuration, String path)
0:       throws IOException {
1:     populateCarbonTable(configuration, path);
1:     // read it from schema file in the store
1:     String carbonTableStr = configuration.get(CARBON_TABLE);
1:     return (CarbonTable) ObjectSerializationUtil.convertStringToObject(carbonTableStr);
1:   }
1: 
/////////////////////////////////////////////////////////////////////////
/////////////////////////////////////////////////////////////////////////
0:     QueryModel queryModel =
0:         QueryModel.createModel(identifier, queryPlan, carbonTable, new DataTypeConverterImpl());
author:chenliang613
-------------------------------------------------------------------------------
commit:09f7cdd
/////////////////////////////////////////////////////////////////////////
1: import org.apache.carbondata.core.util.DataTypeConverterImpl;
/////////////////////////////////////////////////////////////////////////
0:     QueryModel queryModel = QueryModel.createModel(identifier, queryPlan, carbonTable,
0:         new DataTypeConverterImpl());
commit:d408a8d
/////////////////////////////////////////////////////////////////////////
1:     CarbonInputSplit split;
/////////////////////////////////////////////////////////////////////////
0:   private QueryModel getQueryModel(Configuration configuration) throws IOException {
0:     StringBuilder colNames = new StringBuilder();
/////////////////////////////////////////////////////////////////////////
0:         colNames = new StringBuilder((colNames + (aCarbonDimensionList.getColName())) + ",");
0:         colNames = new StringBuilder(colNames.substring(0, colNames.lastIndexOf(",")));
0:             colNames.append(carbonMeasureList.get(index).getColName());
0:             colNames =
0:                 new StringBuilder((colNames + (carbonMeasureList.get(index).getColName())) + ",");
0:       projection = colNames.toString().trim();
0:       configuration.set("hive.io.file.readcolumn.names", colNames.toString());
author:Bhavya
-------------------------------------------------------------------------------
commit:cbe1419
/////////////////////////////////////////////////////////////////////////
1: import java.util.ArrayList;
1: import org.apache.carbondata.core.metadata.schema.table.column.CarbonColumn;
/////////////////////////////////////////////////////////////////////////
0: import org.apache.carbondata.hadoop.util.ObjectSerializationUtil;
0: import org.apache.carbondata.hadoop.util.SchemaReader;
1: import org.apache.hadoop.fs.InvalidPathException;
/////////////////////////////////////////////////////////////////////////
1: import org.apache.hadoop.util.StringUtils;
1:   private static final String CARBON_TABLE = "mapreduce.input.carboninputformat.table";
/////////////////////////////////////////////////////////////////////////
1:     String path = null;
1:     if (inputSplit instanceof CarbonHiveInputSplit) {
1:       path = ((CarbonHiveInputSplit) inputSplit).getPath().toString();
1:     }
0:     QueryModel queryModel = getQueryModel(jobConf, path);
1:     CarbonReadSupport<ArrayWritable> readSupport = new CarbonDictionaryDecodeReadSupport<>();
1:   /**
0:    * this method will read the schema from the physical file and populate into CARBON_TABLE
1:    *
1:    * @param configuration
0:    * @throws IOException
1:    */
0:   private static void populateCarbonTable(Configuration configuration, String paths)
0:       throws IOException {
0:     String dirs = configuration.get(INPUT_DIR, "");
0:     String[] inputPaths = StringUtils.split(dirs);
0:     String validInputPath = null;
0:     if (inputPaths.length == 0) {
0:       throw new InvalidPathException("No input paths specified in job");
0:     } else {
0:       if (paths != null) {
0:         for (String inputPath : inputPaths) {
0:           if (paths.startsWith(inputPath)) {
0:             validInputPath = inputPath;
0:             break;
1:           }
1:         }
1:       }
1:     }
0:     AbsoluteTableIdentifier absoluteTableIdentifier =
0:         AbsoluteTableIdentifier.fromTablePath(validInputPath);
0:     // read the schema file to get the absoluteTableIdentifier having the correct table id
0:     // persisted in the schema
0:     CarbonTable carbonTable = SchemaReader.readCarbonTableFromStore(absoluteTableIdentifier);
0:     setCarbonTable(configuration, carbonTable);
1:   }
1: 
0:   private static CarbonTable getCarbonTable(Configuration configuration, String path)
0:       throws IOException {
0:     populateCarbonTable(configuration, path);
0:     // read it from schema file in the store
0:     String carbonTableStr = configuration.get(CARBON_TABLE);
0:     return (CarbonTable) ObjectSerializationUtil.convertStringToObject(carbonTableStr);
1:   }
1: 
0:   private QueryModel getQueryModel(Configuration configuration, String path) throws IOException {
1:     CarbonTable carbonTable = getCarbonTable(configuration, path);
0:     String projection = getProjection(configuration, carbonTable,
1:         identifier.getCarbonTableIdentifier().getTableName());
/////////////////////////////////////////////////////////////////////////
0:   /**
1:    * Return the Projection for the CarbonQuery.
1:    *
0:    * @param configuration
1:    * @param carbonTable
1:    * @param tableName
1:    * @return
1:    */
1:   private String getProjection(Configuration configuration, CarbonTable carbonTable,
1:       String tableName) {
1:     // query plan includes projection column
1:     String projection = getColumnProjection(configuration);
1:     if (projection == null) {
1:       projection = configuration.get("hive.io.file.readcolumn.names");
0:     }
1:     List<CarbonColumn> carbonColumns = carbonTable.getCreateOrderColumn(tableName);
1:     List<String> carbonColumnNames = new ArrayList<>();
1:     StringBuilder allColumns = new StringBuilder();
1:     StringBuilder projectionColumns = new StringBuilder();
1:     for (CarbonColumn column : carbonColumns) {
0:       carbonColumnNames.add(column.getColName());
1:       allColumns.append(column.getColName() + ",");
0:     }
1: 
1:     if (!projection.equals("")) {
1:       String[] columnNames = projection.split(",");
1:       //verify that the columns parsed by Hive exist in the table
1:       for (String col : columnNames) {
1:         //show columns command will return these data
0:         if (carbonColumnNames.contains(col)) {
1:           projectionColumns.append(col + ",");
0:         }
0:       }
1:       return projectionColumns.substring(0, projectionColumns.lastIndexOf(","));
0:     } else {
1:       return allColumns.substring(0, allColumns.lastIndexOf(","));
0:     }
0:   }
1: 
author:cenyuhai
-------------------------------------------------------------------------------
commit:a700f83
/////////////////////////////////////////////////////////////////////////
1: /*
1:  * Licensed to the Apache Software Foundation (ASF) under one or more
1:  * contributor license agreements.  See the NOTICE file distributed with
1:  * this work for additional information regarding copyright ownership.
1:  * The ASF licenses this file to You under the Apache License, Version 2.0
1:  * (the "License"); you may not use this file except in compliance with
1:  * the License.  You may obtain a copy of the License at
1:  *
1:  *    http://www.apache.org/licenses/LICENSE-2.0
1:  *
1:  * Unless required by applicable law or agreed to in writing, software
1:  * distributed under the License is distributed on an "AS IS" BASIS,
1:  * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
1:  * See the License for the specific language governing permissions and
1:  * limitations under the License.
0:  */
1: package org.apache.carbondata.hive;
1: 
1: import java.io.IOException;
1: import java.util.List;
1: 
1: import org.apache.carbondata.core.metadata.AbsoluteTableIdentifier;
1: import org.apache.carbondata.core.metadata.schema.table.CarbonTable;
0: import org.apache.carbondata.core.scan.expression.Expression;
0: import org.apache.carbondata.core.scan.filter.resolver.FilterResolverIntf;
0: import org.apache.carbondata.core.scan.model.CarbonQueryPlan;
1: import org.apache.carbondata.core.scan.model.QueryModel;
0: import org.apache.carbondata.hadoop.CarbonInputFormat;
1: import org.apache.carbondata.hadoop.CarbonInputSplit;
1: import org.apache.carbondata.hadoop.readsupport.CarbonReadSupport;
0: import org.apache.carbondata.hadoop.util.CarbonInputFormatUtil;
1: 
1: import org.apache.hadoop.conf.Configuration;
1: import org.apache.hadoop.fs.Path;
1: import org.apache.hadoop.hive.ql.io.CombineHiveInputFormat;
1: import org.apache.hadoop.io.ArrayWritable;
1: import org.apache.hadoop.mapred.InputFormat;
1: import org.apache.hadoop.mapred.InputSplit;
1: import org.apache.hadoop.mapred.JobConf;
1: import org.apache.hadoop.mapred.RecordReader;
1: import org.apache.hadoop.mapred.Reporter;
1: import org.apache.hadoop.mapreduce.Job;
1: 
1: 
0: public class MapredCarbonInputFormat extends CarbonInputFormat<ArrayWritable>
1:     implements InputFormat<Void, ArrayWritable>, CombineHiveInputFormat.AvoidSplitCombination {
1: 
1:   @Override
0:   public InputSplit[] getSplits(JobConf jobConf, int numSplits) throws IOException {
1:     org.apache.hadoop.mapreduce.JobContext jobContext = Job.getInstance(jobConf);
1:     List<org.apache.hadoop.mapreduce.InputSplit> splitList = super.getSplits(jobContext);
1:     InputSplit[] splits = new InputSplit[splitList.size()];
0:     CarbonInputSplit split = null;
1:     for (int i = 0; i < splitList.size(); i++) {
1:       split = (CarbonInputSplit) splitList.get(i);
0:       splits[i] = new CarbonHiveInputSplit(split.getSegmentId(), split.getPath(),
0:           split.getStart(), split.getLength(), split.getLocations(),
0:           split.getNumberOfBlocklets(), split.getVersion(), split.getBlockStorageIdMap());
0:     }
1:     return splits;
0:   }
1: 
1:   @Override
1:   public RecordReader<Void, ArrayWritable> getRecordReader(InputSplit inputSplit, JobConf jobConf,
0:                                                            Reporter reporter) throws IOException {
0:     QueryModel queryModel = getQueryModel(jobConf);
0:     CarbonReadSupport<ArrayWritable> readSupport = getReadSupportClass(jobConf);
1:     return new CarbonHiveRecordReader(queryModel, readSupport, inputSplit, jobConf);
0:   }
1: 
0:   public QueryModel getQueryModel(Configuration configuration) throws IOException {
0:     CarbonTable carbonTable = getCarbonTable(configuration);
0:     // getting the table absoluteTableIdentifier from the carbonTable
0:     // to avoid unnecessary deserialization
1:     AbsoluteTableIdentifier identifier = carbonTable.getAbsoluteTableIdentifier();
1: 
0:     // query plan includes projection column
1: 
0:     String projection = getColumnProjection(configuration);
0:     if (projection == null) {
0:       projection = configuration.get("hive.io.file.readcolumn.names");
0:     }
0:     CarbonQueryPlan queryPlan = CarbonInputFormatUtil.createQueryPlan(carbonTable, projection);
0:     QueryModel queryModel = QueryModel.createModel(identifier, queryPlan, carbonTable);
1: 
0:     // set the filter to the query model in order to filter blocklet before scan
0:     Expression filter = getFilterPredicates(configuration);
0:     CarbonInputFormatUtil.processFilterExpression(filter, carbonTable);
0:     FilterResolverIntf filterIntf = CarbonInputFormatUtil.resolveFilter(filter, identifier);
0:     queryModel.setFilterExpressionResolverTree(filterIntf);
1: 
1:     return queryModel;
0:   }
1: 
1:   @Override
0:   public boolean shouldSkipCombine(Path path, Configuration conf) throws IOException {
1:     return true;
0:   }
0: }
============================================================================