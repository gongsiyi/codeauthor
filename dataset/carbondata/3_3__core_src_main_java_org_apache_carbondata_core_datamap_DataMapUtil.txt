1:2018048: /*
1:2018048:  * Licensed to the Apache Software Foundation (ASF) under one or more
1:2018048:  * contributor license agreements.  See the NOTICE file distributed with
1:2018048:  * this work for additional information regarding copyright ownership.
1:2018048:  * The ASF licenses this file to You under the Apache License, Version 2.0
1:2018048:  * (the "License"); you may not use this file except in compliance with
1:2018048:  * the License.  You may obtain a copy of the License at
1:2018048:  *
1:2018048:  *    http://www.apache.org/licenses/LICENSE-2.0
1:2018048:  *
1:2018048:  * Unless required by applicable law or agreed to in writing, software
1:2018048:  * distributed under the License is distributed on an "AS IS" BASIS,
1:2018048:  * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
1:2018048:  * See the License for the specific language governing permissions and
1:2018048:  * limitations under the License.
1:2018048:  */
1:2018048: 
1:2018048: package org.apache.carbondata.core.datamap;
1:2018048: 
1:2018048: import java.io.IOException;
1:2018048: import java.lang.reflect.Constructor;
1:2018048: import java.util.List;
1:2018048: 
1:2018048: import org.apache.carbondata.common.logging.LogService;
1:2018048: import org.apache.carbondata.common.logging.LogServiceFactory;
1:2018048: import org.apache.carbondata.core.datamap.dev.expr.DataMapExprWrapper;
1:8f1a029: import org.apache.carbondata.core.datastore.impl.FileFactory;
1:2018048: import org.apache.carbondata.core.indexstore.ExtendedBlocklet;
1:2018048: import org.apache.carbondata.core.indexstore.PartitionSpec;
1:2018048: import org.apache.carbondata.core.metadata.schema.table.CarbonTable;
1:2018048: import org.apache.carbondata.core.scan.filter.resolver.FilterResolverIntf;
1:2018048: import org.apache.carbondata.core.statusmanager.SegmentStatusManager;
1:2018048: import org.apache.carbondata.core.util.ObjectSerializationUtil;
1:2018048: 
1:2018048: import org.apache.hadoop.conf.Configuration;
1:2018048: 
1:2018048: public class DataMapUtil {
1:2018048: 
1:2018048:   private static final String DATA_MAP_DSTR = "mapreduce.input.carboninputformat.datamapdstr";
1:2018048: 
1:2018048:   private static final LogService LOGGER =
1:2018048:       LogServiceFactory.getLogService(DataMapUtil.class.getName());
1:2018048: 
1:2018048:   /**
1:2018048:    * Creates instance for the DataMap Job class
1:2018048:    *
1:2018048:    * @param className
1:2018048:    * @return
1:2018048:    */
1:2018048:   public static Object createDataMapJob(String className) {
1:2018048:     try {
1:2018048:       return Class.forName(className).getDeclaredConstructors()[0].newInstance();
1:2018048:     } catch (Exception e) {
1:2018048:       LOGGER.error(e);
1:2018048:       return null;
1:2018048:     }
1:2018048:   }
1:2018048: 
1:2018048:   /**
1:2018048:    * This method sets the datamapJob in the configuration
1:2018048:    * @param configuration
1:2018048:    * @param dataMapJob
1:2018048:    * @throws IOException
1:2018048:    */
1:2018048:   public static void setDataMapJob(Configuration configuration, Object dataMapJob)
1:2018048:       throws IOException {
1:2018048:     if (dataMapJob != null) {
1:2018048:       String toString = ObjectSerializationUtil.convertObjectToString(dataMapJob);
1:2018048:       configuration.set(DATA_MAP_DSTR, toString);
1:2018048:     }
1:2018048:   }
1:2018048: 
1:2018048:   /**
1:2018048:    * get datamap job from the configuration
1:2018048:    * @param configuration job configuration
1:2018048:    * @return DataMap Job
1:2018048:    * @throws IOException
1:2018048:    */
1:2018048:   public static DataMapJob getDataMapJob(Configuration configuration) throws IOException {
1:2018048:     String jobString = configuration.get(DATA_MAP_DSTR);
1:2018048:     if (jobString != null) {
1:2018048:       return (DataMapJob) ObjectSerializationUtil.convertStringToObject(jobString);
1:2018048:     }
1:2018048:     return null;
1:2018048:   }
1:2018048: 
1:2018048:   /**
1:2018048:    * This method gets the datamapJob and call execute , this job will be launched before clearing
1:2018048:    * datamaps from driver side during drop table and drop datamap and clears the datamap in executor
1:2018048:    * side
1:2018048:    * @param carbonTable
1:2018048:    * @throws IOException
1:2018048:    */
1:2018048:   public static void executeDataMapJobForClearingDataMaps(CarbonTable carbonTable)
1:2018048:       throws IOException {
1:2018048:     String dataMapJobClassName = "org.apache.carbondata.spark.rdd.SparkDataMapJob";
1:2018048:     DataMapJob dataMapJob = (DataMapJob) createDataMapJob(dataMapJobClassName);
1:347b8e1:     if (dataMapJob == null) {
1:347b8e1:       return;
1:347b8e1:     }
1:2018048:     String className = "org.apache.carbondata.core.datamap.DistributableDataMapFormat";
1:2018048:     SegmentStatusManager.ValidAndInvalidSegmentsInfo validAndInvalidSegmentsInfo =
1:8f1a029:         getValidAndInvalidSegments(carbonTable, FileFactory.getConfiguration());
1:2018048:     List<Segment> validSegments = validAndInvalidSegmentsInfo.getValidSegments();
1:2018048:     List<Segment> invalidSegments = validAndInvalidSegmentsInfo.getInvalidSegments();
1:2018048:     DataMapExprWrapper dataMapExprWrapper = null;
1:2018048:     if (DataMapStoreManager.getInstance().getAllDataMap(carbonTable).size() > 0) {
1:2018048:       DataMapChooser dataMapChooser = new DataMapChooser(carbonTable);
1:2018048:       dataMapExprWrapper = dataMapChooser.getAllDataMapsForClear(carbonTable);
1:2018048:     } else {
1:2018048:       return;
1:2018048:     }
1:2018048:     DistributableDataMapFormat dataMapFormat =
1:2018048:         createDataMapJob(carbonTable, dataMapExprWrapper, validSegments, invalidSegments, null,
1:2018048:             className, true);
1:2018048:     dataMapJob.execute(dataMapFormat, null);
1:2018048:   }
1:2018048: 
1:2018048:   private static DistributableDataMapFormat createDataMapJob(CarbonTable carbonTable,
1:2018048:       DataMapExprWrapper dataMapExprWrapper, List<Segment> validsegments,
1:2018048:       List<Segment> invalidSegments, List<PartitionSpec> partitionsToPrune, String clsName,
1:2018048:       boolean isJobToClearDataMaps) {
1:2018048:     try {
1:2018048:       Constructor<?> cons = Class.forName(clsName).getDeclaredConstructors()[0];
1:2018048:       return (DistributableDataMapFormat) cons
1:2018048:           .newInstance(carbonTable, dataMapExprWrapper, validsegments, invalidSegments,
1:2018048:               partitionsToPrune, isJobToClearDataMaps);
1:2018048:     } catch (Exception e) {
1:2018048:       throw new RuntimeException(e);
1:2018048:     }
1:2018048:   }
1:2018048: 
1:2018048:   /**
1:2018048:    * this method gets the datamapJob and call execute of that job, this will be launched for
1:2018048:    * distributed CG or FG
1:2018048:    * @return list of Extended blocklets after pruning
1:2018048:    */
1:2018048:   public static List<ExtendedBlocklet> executeDataMapJob(CarbonTable carbonTable,
1:2018048:       FilterResolverIntf resolver, List<Segment> validSegments,
1:2018048:       DataMapExprWrapper dataMapExprWrapper, DataMapJob dataMapJob,
1:2018048:       List<PartitionSpec> partitionsToPrune) throws IOException {
1:2018048:     String className = "org.apache.carbondata.core.datamap.DistributableDataMapFormat";
1:2018048:     SegmentStatusManager.ValidAndInvalidSegmentsInfo validAndInvalidSegmentsInfo =
1:8f1a029:         getValidAndInvalidSegments(carbonTable, validSegments.get(0).getConfiguration());
1:2018048:     List<Segment> invalidSegments = validAndInvalidSegmentsInfo.getInvalidSegments();
1:2018048:     DistributableDataMapFormat dataMapFormat =
1:2018048:         createDataMapJob(carbonTable, dataMapExprWrapper, validSegments, invalidSegments,
1:2018048:             partitionsToPrune, className, false);
1:2018048:     List<ExtendedBlocklet> prunedBlocklets = dataMapJob.execute(dataMapFormat, resolver);
1:2018048:     // Apply expression on the blocklets.
1:2018048:     prunedBlocklets = dataMapExprWrapper.pruneBlocklets(prunedBlocklets);
1:2018048:     return prunedBlocklets;
1:2018048:   }
1:2018048: 
1:2018048:   private static SegmentStatusManager.ValidAndInvalidSegmentsInfo getValidAndInvalidSegments(
1:8f1a029:       CarbonTable carbonTable, Configuration configuration) throws IOException {
1:8f1a029:     SegmentStatusManager ssm =
1:8f1a029:         new SegmentStatusManager(carbonTable.getAbsoluteTableIdentifier(), configuration);
1:2018048:     return ssm.getValidAndInvalidSegments();
1:2018048:   }
1:2018048: 
1:2018048: }
============================================================================
author:kunal642
-------------------------------------------------------------------------------
commit:8f1a029
/////////////////////////////////////////////////////////////////////////
1: import org.apache.carbondata.core.datastore.impl.FileFactory;
/////////////////////////////////////////////////////////////////////////
1:         getValidAndInvalidSegments(carbonTable, FileFactory.getConfiguration());
/////////////////////////////////////////////////////////////////////////
1:         getValidAndInvalidSegments(carbonTable, validSegments.get(0).getConfiguration());
/////////////////////////////////////////////////////////////////////////
1:       CarbonTable carbonTable, Configuration configuration) throws IOException {
1:     SegmentStatusManager ssm =
1:         new SegmentStatusManager(carbonTable.getAbsoluteTableIdentifier(), configuration);
author:ravipesala
-------------------------------------------------------------------------------
commit:347b8e1
/////////////////////////////////////////////////////////////////////////
1:     if (dataMapJob == null) {
1:       return;
1:     }
author:akashrn5
-------------------------------------------------------------------------------
commit:2018048
/////////////////////////////////////////////////////////////////////////
1: /*
1:  * Licensed to the Apache Software Foundation (ASF) under one or more
1:  * contributor license agreements.  See the NOTICE file distributed with
1:  * this work for additional information regarding copyright ownership.
1:  * The ASF licenses this file to You under the Apache License, Version 2.0
1:  * (the "License"); you may not use this file except in compliance with
1:  * the License.  You may obtain a copy of the License at
1:  *
1:  *    http://www.apache.org/licenses/LICENSE-2.0
1:  *
1:  * Unless required by applicable law or agreed to in writing, software
1:  * distributed under the License is distributed on an "AS IS" BASIS,
1:  * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
1:  * See the License for the specific language governing permissions and
1:  * limitations under the License.
1:  */
1: 
1: package org.apache.carbondata.core.datamap;
1: 
1: import java.io.IOException;
1: import java.lang.reflect.Constructor;
1: import java.util.List;
1: 
1: import org.apache.carbondata.common.logging.LogService;
1: import org.apache.carbondata.common.logging.LogServiceFactory;
1: import org.apache.carbondata.core.datamap.dev.expr.DataMapExprWrapper;
1: import org.apache.carbondata.core.indexstore.ExtendedBlocklet;
1: import org.apache.carbondata.core.indexstore.PartitionSpec;
1: import org.apache.carbondata.core.metadata.schema.table.CarbonTable;
1: import org.apache.carbondata.core.scan.filter.resolver.FilterResolverIntf;
1: import org.apache.carbondata.core.statusmanager.SegmentStatusManager;
1: import org.apache.carbondata.core.util.ObjectSerializationUtil;
1: 
1: import org.apache.hadoop.conf.Configuration;
1: 
1: public class DataMapUtil {
1: 
1:   private static final String DATA_MAP_DSTR = "mapreduce.input.carboninputformat.datamapdstr";
1: 
1:   private static final LogService LOGGER =
1:       LogServiceFactory.getLogService(DataMapUtil.class.getName());
1: 
1:   /**
1:    * Creates instance for the DataMap Job class
1:    *
1:    * @param className
1:    * @return
1:    */
1:   public static Object createDataMapJob(String className) {
1:     try {
1:       return Class.forName(className).getDeclaredConstructors()[0].newInstance();
1:     } catch (Exception e) {
1:       LOGGER.error(e);
1:       return null;
1:     }
1:   }
1: 
1:   /**
1:    * This method sets the datamapJob in the configuration
1:    * @param configuration
1:    * @param dataMapJob
1:    * @throws IOException
1:    */
1:   public static void setDataMapJob(Configuration configuration, Object dataMapJob)
1:       throws IOException {
1:     if (dataMapJob != null) {
1:       String toString = ObjectSerializationUtil.convertObjectToString(dataMapJob);
1:       configuration.set(DATA_MAP_DSTR, toString);
1:     }
1:   }
1: 
1:   /**
1:    * get datamap job from the configuration
1:    * @param configuration job configuration
1:    * @return DataMap Job
1:    * @throws IOException
1:    */
1:   public static DataMapJob getDataMapJob(Configuration configuration) throws IOException {
1:     String jobString = configuration.get(DATA_MAP_DSTR);
1:     if (jobString != null) {
1:       return (DataMapJob) ObjectSerializationUtil.convertStringToObject(jobString);
1:     }
1:     return null;
1:   }
1: 
1:   /**
1:    * This method gets the datamapJob and call execute , this job will be launched before clearing
1:    * datamaps from driver side during drop table and drop datamap and clears the datamap in executor
1:    * side
1:    * @param carbonTable
1:    * @throws IOException
1:    */
1:   public static void executeDataMapJobForClearingDataMaps(CarbonTable carbonTable)
1:       throws IOException {
1:     String dataMapJobClassName = "org.apache.carbondata.spark.rdd.SparkDataMapJob";
1:     DataMapJob dataMapJob = (DataMapJob) createDataMapJob(dataMapJobClassName);
1:     String className = "org.apache.carbondata.core.datamap.DistributableDataMapFormat";
1:     SegmentStatusManager.ValidAndInvalidSegmentsInfo validAndInvalidSegmentsInfo =
0:         getValidAndInvalidSegments(carbonTable);
1:     List<Segment> validSegments = validAndInvalidSegmentsInfo.getValidSegments();
1:     List<Segment> invalidSegments = validAndInvalidSegmentsInfo.getInvalidSegments();
1:     DataMapExprWrapper dataMapExprWrapper = null;
1:     if (DataMapStoreManager.getInstance().getAllDataMap(carbonTable).size() > 0) {
1:       DataMapChooser dataMapChooser = new DataMapChooser(carbonTable);
1:       dataMapExprWrapper = dataMapChooser.getAllDataMapsForClear(carbonTable);
1:     } else {
1:       return;
1:     }
1:     DistributableDataMapFormat dataMapFormat =
1:         createDataMapJob(carbonTable, dataMapExprWrapper, validSegments, invalidSegments, null,
1:             className, true);
1:     dataMapJob.execute(dataMapFormat, null);
1:   }
1: 
1:   private static DistributableDataMapFormat createDataMapJob(CarbonTable carbonTable,
1:       DataMapExprWrapper dataMapExprWrapper, List<Segment> validsegments,
1:       List<Segment> invalidSegments, List<PartitionSpec> partitionsToPrune, String clsName,
1:       boolean isJobToClearDataMaps) {
1:     try {
1:       Constructor<?> cons = Class.forName(clsName).getDeclaredConstructors()[0];
1:       return (DistributableDataMapFormat) cons
1:           .newInstance(carbonTable, dataMapExprWrapper, validsegments, invalidSegments,
1:               partitionsToPrune, isJobToClearDataMaps);
1:     } catch (Exception e) {
1:       throw new RuntimeException(e);
1:     }
1:   }
1: 
1:   /**
1:    * this method gets the datamapJob and call execute of that job, this will be launched for
1:    * distributed CG or FG
1:    * @return list of Extended blocklets after pruning
1:    */
1:   public static List<ExtendedBlocklet> executeDataMapJob(CarbonTable carbonTable,
1:       FilterResolverIntf resolver, List<Segment> validSegments,
1:       DataMapExprWrapper dataMapExprWrapper, DataMapJob dataMapJob,
1:       List<PartitionSpec> partitionsToPrune) throws IOException {
1:     String className = "org.apache.carbondata.core.datamap.DistributableDataMapFormat";
1:     SegmentStatusManager.ValidAndInvalidSegmentsInfo validAndInvalidSegmentsInfo =
0:         getValidAndInvalidSegments(carbonTable);
1:     List<Segment> invalidSegments = validAndInvalidSegmentsInfo.getInvalidSegments();
1:     DistributableDataMapFormat dataMapFormat =
1:         createDataMapJob(carbonTable, dataMapExprWrapper, validSegments, invalidSegments,
1:             partitionsToPrune, className, false);
1:     List<ExtendedBlocklet> prunedBlocklets = dataMapJob.execute(dataMapFormat, resolver);
1:     // Apply expression on the blocklets.
1:     prunedBlocklets = dataMapExprWrapper.pruneBlocklets(prunedBlocklets);
1:     return prunedBlocklets;
1:   }
1: 
1:   private static SegmentStatusManager.ValidAndInvalidSegmentsInfo getValidAndInvalidSegments(
0:       CarbonTable carbonTable) throws IOException {
0:     SegmentStatusManager ssm = new SegmentStatusManager(carbonTable.getAbsoluteTableIdentifier());
1:     return ssm.getValidAndInvalidSegments();
1:   }
1: 
1: }
============================================================================