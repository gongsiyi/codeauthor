1:cd6a4ff: /*
1:41347d8:  * Licensed to the Apache Software Foundation (ASF) under one or more
1:41347d8:  * contributor license agreements.  See the NOTICE file distributed with
1:41347d8:  * this work for additional information regarding copyright ownership.
1:41347d8:  * The ASF licenses this file to You under the Apache License, Version 2.0
1:41347d8:  * (the "License"); you may not use this file except in compliance with
1:41347d8:  * the License.  You may obtain a copy of the License at
13:cd6a4ff:  *
1:cd6a4ff:  *    http://www.apache.org/licenses/LICENSE-2.0
1:cd6a4ff:  *
1:41347d8:  * Unless required by applicable law or agreed to in writing, software
1:41347d8:  * distributed under the License is distributed on an "AS IS" BASIS,
1:41347d8:  * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
1:41347d8:  * See the License for the specific language governing permissions and
1:41347d8:  * limitations under the License.
13:cd6a4ff:  */
1:cd6a4ff: package org.apache.carbondata.core.util;
16:cd6a4ff: 
1:cd6a4ff: import java.io.IOException;
1:cd6a4ff: import java.util.ArrayList;
1:cd6a4ff: import java.util.Iterator;
1:cd6a4ff: import java.util.List;
1:cd6a4ff: 
1:daa6465: import org.apache.carbondata.core.datastore.FileReader;
1:ce09aaa: import org.apache.carbondata.core.datastore.block.TableBlockInfo;
1:ce09aaa: import org.apache.carbondata.core.datastore.impl.FileFactory;
1:ce09aaa: import org.apache.carbondata.core.metadata.ColumnarFormatVersion;
1:ce09aaa: import org.apache.carbondata.core.metadata.blocklet.BlockletInfo;
1:ce09aaa: import org.apache.carbondata.core.metadata.blocklet.DataFileFooter;
1:ce09aaa: import org.apache.carbondata.core.metadata.blocklet.datachunk.DataChunk;
1:ce09aaa: import org.apache.carbondata.core.metadata.blocklet.index.BlockletIndex;
1:ce09aaa: import org.apache.carbondata.core.metadata.schema.table.column.ColumnSchema;
1:cd6a4ff: import org.apache.carbondata.core.reader.CarbonFooterReader;
1:cd6a4ff: import org.apache.carbondata.format.FileFooter;
1:cd6a4ff: 
1:8f1a029: import org.apache.hadoop.conf.Configuration;
1:8f1a029: 
13:cd6a4ff: /**
1:cd6a4ff:  * Below class will be used to convert the thrift object of data file
1:cd6a4ff:  * meta data to wrapper object
1:cd6a4ff:  */
1:d54dc64: public class DataFileFooterConverter extends AbstractDataFileFooterConverter {
1:cd6a4ff: 
1:8f1a029:   public DataFileFooterConverter(Configuration configuration) {
1:8f1a029:     super(configuration);
1:8f1a029:   }
1:8f1a029: 
1:8f1a029:   public DataFileFooterConverter() {
1:8f1a029:     super(FileFactory.getConfiguration());
1:8f1a029:   }
1:8f1a029: 
1:cd6a4ff:   /**
1:cd6a4ff:    * Below method will be used to convert thrift file meta to wrapper file meta
1:cd6a4ff:    */
1:d54dc64:   @Override public DataFileFooter readDataFileFooter(TableBlockInfo tableBlockInfo)
2:cd6a4ff:       throws IOException {
1:cd6a4ff:     DataFileFooter dataFileFooter = new DataFileFooter();
1:daa6465:     FileReader fileReader = null;
3:cd6a4ff:     try {
1:d54dc64:       long completeBlockLength = tableBlockInfo.getBlockLength();
1:cd6a4ff:       long footerPointer = completeBlockLength - 8;
1:d54dc64:       fileReader = FileFactory.getFileHolder(FileFactory.getFileType(tableBlockInfo.getFilePath()));
1:d54dc64:       long actualFooterOffset = fileReader.readLong(tableBlockInfo.getFilePath(), footerPointer);
1:d54dc64:       CarbonFooterReader reader =
1:d54dc64:           new CarbonFooterReader(tableBlockInfo.getFilePath(), actualFooterOffset);
1:cd6a4ff:       FileFooter footer = reader.readFooter();
1:0ef3fb8:       dataFileFooter.setVersionId(ColumnarFormatVersion.valueOf((short) footer.getVersion()));
1:cd6a4ff:       dataFileFooter.setNumberOfRows(footer.getNum_rows());
1:cd6a4ff:       dataFileFooter.setSegmentInfo(getSegmentInfo(footer.getSegment_info()));
2:cd6a4ff:       List<ColumnSchema> columnSchemaList = new ArrayList<ColumnSchema>();
1:cd6a4ff:       List<org.apache.carbondata.format.ColumnSchema> table_columns = footer.getTable_columns();
2:cd6a4ff:       for (int i = 0; i < table_columns.size(); i++) {
1:8896a63:         columnSchemaList.add(thriftColumnSchemaToWrapperColumnSchema(table_columns.get(i)));
32:cd6a4ff:       }
2:cd6a4ff:       dataFileFooter.setColumnInTable(columnSchemaList);
1:cd6a4ff: 
1:cd6a4ff:       List<org.apache.carbondata.format.BlockletIndex> leaf_node_indices_Thrift =
1:cd6a4ff:           footer.getBlocklet_index_list();
1:cd6a4ff:       List<BlockletIndex> blockletIndexList = new ArrayList<BlockletIndex>();
1:cd6a4ff:       for (int i = 0; i < leaf_node_indices_Thrift.size(); i++) {
1:cd6a4ff:         BlockletIndex blockletIndex = getBlockletIndex(leaf_node_indices_Thrift.get(i));
1:cd6a4ff:         blockletIndexList.add(blockletIndex);
1:cd6a4ff:       }
1:cd6a4ff: 
1:cd6a4ff:       List<org.apache.carbondata.format.BlockletInfo> leaf_node_infos_Thrift =
1:cd6a4ff:           footer.getBlocklet_info_list();
1:cd6a4ff:       List<BlockletInfo> blockletInfoList = new ArrayList<BlockletInfo>();
1:cd6a4ff:       for (int i = 0; i < leaf_node_infos_Thrift.size(); i++) {
1:cd6a4ff:         BlockletInfo blockletInfo = getBlockletInfo(leaf_node_infos_Thrift.get(i));
1:cd6a4ff:         blockletInfo.setBlockletIndex(blockletIndexList.get(i));
1:cd6a4ff:         blockletInfoList.add(blockletInfo);
1:cd6a4ff:       }
1:cd6a4ff:       dataFileFooter.setBlockletList(blockletInfoList);
1:cd6a4ff:       dataFileFooter.setBlockletIndex(getBlockletIndexForDataFileFooter(blockletIndexList));
2:cd6a4ff:     } finally {
1:cd6a4ff:       if (null != fileReader) {
1:cd6a4ff:         fileReader.finish();
1:cd6a4ff:       }
1:cd6a4ff:     }
1:cd6a4ff:     return dataFileFooter;
1:cd6a4ff:   }
1:cd6a4ff: 
1:cd6a4ff:   /**
1:cd6a4ff:    * Below method is to convert the blocklet info of the thrift to wrapper
1:cd6a4ff:    * blocklet info
1:cd6a4ff:    *
1:cd6a4ff:    * @param blockletInfoThrift blocklet info of the thrift
1:cd6a4ff:    * @return blocklet info wrapper
1:cd6a4ff:    */
1:cd6a4ff:   private BlockletInfo getBlockletInfo(
1:cd6a4ff:       org.apache.carbondata.format.BlockletInfo blockletInfoThrift) {
1:cd6a4ff:     BlockletInfo blockletInfo = new BlockletInfo();
1:cd6a4ff:     List<DataChunk> dimensionColumnChunk = new ArrayList<DataChunk>();
1:cd6a4ff:     List<DataChunk> measureChunk = new ArrayList<DataChunk>();
1:cd6a4ff:     Iterator<org.apache.carbondata.format.DataChunk> column_data_chunksIterator =
1:cd6a4ff:         blockletInfoThrift.getColumn_data_chunksIterator();
1:cd6a4ff:     if (null != column_data_chunksIterator) {
1:cd6a4ff:       while (column_data_chunksIterator.hasNext()) {
1:cd6a4ff:         org.apache.carbondata.format.DataChunk next = column_data_chunksIterator.next();
1:cd6a4ff:         if (next.isRowMajor()) {
1:cd6a4ff:           dimensionColumnChunk.add(getDataChunk(next, false));
1:cd6a4ff:         } else if (next.getEncoders().contains(org.apache.carbondata.format.Encoding.DELTA)) {
1:cd6a4ff:           measureChunk.add(getDataChunk(next, true));
2:cd6a4ff:         } else {
1:cd6a4ff:           dimensionColumnChunk.add(getDataChunk(next, false));
1:cd6a4ff:         }
1:cd6a4ff:       }
1:cd6a4ff:     }
1:cd6a4ff:     blockletInfo.setDimensionColumnChunk(dimensionColumnChunk);
1:cd6a4ff:     blockletInfo.setMeasureColumnChunk(measureChunk);
1:cd6a4ff:     blockletInfo.setNumberOfRows(blockletInfoThrift.getNum_rows());
1:cd6a4ff:     return blockletInfo;
1:cd6a4ff:   }
1:b681244: 
1:b681244:   @Override public List<ColumnSchema> getSchema(TableBlockInfo tableBlockInfo) throws IOException {
1:daa6465:     FileReader fileReader = null;
1:133b303:     List<ColumnSchema> columnSchemaList = new ArrayList<ColumnSchema>();
1:133b303:     try {
1:133b303:       long completeBlockLength = tableBlockInfo.getBlockLength();
1:133b303:       long footerPointer = completeBlockLength - 8;
1:133b303:       fileReader = FileFactory.getFileHolder(FileFactory.getFileType(tableBlockInfo.getFilePath()));
1:133b303:       long actualFooterOffset = fileReader.readLong(tableBlockInfo.getFilePath(), footerPointer);
1:133b303:       CarbonFooterReader reader =
1:133b303:           new CarbonFooterReader(tableBlockInfo.getFilePath(), actualFooterOffset);
1:133b303:       FileFooter footer = reader.readFooter();
1:133b303:       List<org.apache.carbondata.format.ColumnSchema> table_columns = footer.getTable_columns();
1:133b303:       for (int i = 0; i < table_columns.size(); i++) {
1:8896a63:         columnSchemaList.add(thriftColumnSchemaToWrapperColumnSchema(table_columns.get(i)));
1:133b303:       }
1:133b303:     } finally {
1:133b303:       if (null != fileReader) {
1:133b303:         fileReader.finish();
1:133b303:       }
1:133b303:     }
1:133b303:     return columnSchemaList;
1:b681244:   }
1:cd6a4ff: }
============================================================================
author:kunal642
-------------------------------------------------------------------------------
commit:8f1a029
/////////////////////////////////////////////////////////////////////////
1: import org.apache.hadoop.conf.Configuration;
1: 
1:   public DataFileFooterConverter(Configuration configuration) {
1:     super(configuration);
1:   }
1: 
1:   public DataFileFooterConverter() {
1:     super(FileFactory.getConfiguration());
1:   }
1: 
author:xubo245
-------------------------------------------------------------------------------
commit:8896a63
/////////////////////////////////////////////////////////////////////////
1:         columnSchemaList.add(thriftColumnSchemaToWrapperColumnSchema(table_columns.get(i)));
/////////////////////////////////////////////////////////////////////////
1:         columnSchemaList.add(thriftColumnSchemaToWrapperColumnSchema(table_columns.get(i)));
author:Jacky Li
-------------------------------------------------------------------------------
commit:daa6465
/////////////////////////////////////////////////////////////////////////
1: import org.apache.carbondata.core.datastore.FileReader;
/////////////////////////////////////////////////////////////////////////
1:     FileReader fileReader = null;
/////////////////////////////////////////////////////////////////////////
1:     FileReader fileReader = null;
author:Ravindra Pesala
-------------------------------------------------------------------------------
commit:133b303
/////////////////////////////////////////////////////////////////////////
0:     FileHolder fileReader = null;
1:     List<ColumnSchema> columnSchemaList = new ArrayList<ColumnSchema>();
1:     try {
1:       long completeBlockLength = tableBlockInfo.getBlockLength();
1:       long footerPointer = completeBlockLength - 8;
1:       fileReader = FileFactory.getFileHolder(FileFactory.getFileType(tableBlockInfo.getFilePath()));
1:       long actualFooterOffset = fileReader.readLong(tableBlockInfo.getFilePath(), footerPointer);
1:       CarbonFooterReader reader =
1:           new CarbonFooterReader(tableBlockInfo.getFilePath(), actualFooterOffset);
1:       FileFooter footer = reader.readFooter();
1:       List<org.apache.carbondata.format.ColumnSchema> table_columns = footer.getTable_columns();
1:       for (int i = 0; i < table_columns.size(); i++) {
0:         columnSchemaList.add(thriftColumnSchmeaToWrapperColumnSchema(table_columns.get(i)));
1:       }
1:     } finally {
1:       if (null != fileReader) {
1:         fileReader.finish();
1:       }
1:     }
1:     return columnSchemaList;
author:ravipesala
-------------------------------------------------------------------------------
commit:b681244
/////////////////////////////////////////////////////////////////////////
1: 
1:   @Override public List<ColumnSchema> getSchema(TableBlockInfo tableBlockInfo) throws IOException {
0:     return null;
1:   }
commit:cd6a4ff
/////////////////////////////////////////////////////////////////////////
1: /*
0:  * Licensed to the Apache Software Foundation (ASF) under one
0:  * or more contributor license agreements.  See the NOTICE file
0:  * distributed with this work for additional information
0:  * regarding copyright ownership.  The ASF licenses this file
0:  * to you under the Apache License, Version 2.0 (the
0:  * "License"); you may not use this file except in compliance
0:  * with the License.  You may obtain a copy of the License at
1:  *
1:  *    http://www.apache.org/licenses/LICENSE-2.0
1:  *
0:  * Unless required by applicable law or agreed to in writing,
0:  * software distributed under the License is distributed on an
0:  * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
0:  * KIND, either express or implied.  See the License for the
0:  * specific language governing permissions and limitations
0:  * under the License.
1:  */
1: package org.apache.carbondata.core.util;
1: 
0: import java.io.ByteArrayInputStream;
1: import java.io.IOException;
0: import java.io.ObjectInputStream;
0: import java.nio.ByteBuffer;
1: import java.util.ArrayList;
0: import java.util.BitSet;
1: import java.util.Iterator;
1: import java.util.List;
1: 
0: import org.apache.carbondata.common.logging.LogService;
0: import org.apache.carbondata.common.logging.LogServiceFactory;
0: import org.apache.carbondata.core.carbon.datastore.block.TableBlockInfo;
0: import org.apache.carbondata.core.carbon.metadata.blocklet.BlockletInfo;
0: import org.apache.carbondata.core.carbon.metadata.blocklet.DataFileFooter;
0: import org.apache.carbondata.core.carbon.metadata.blocklet.SegmentInfo;
0: import org.apache.carbondata.core.carbon.metadata.blocklet.compressor.ChunkCompressorMeta;
0: import org.apache.carbondata.core.carbon.metadata.blocklet.compressor.CompressionCodec;
0: import org.apache.carbondata.core.carbon.metadata.blocklet.datachunk.DataChunk;
0: import org.apache.carbondata.core.carbon.metadata.blocklet.datachunk.PresenceMeta;
0: import org.apache.carbondata.core.carbon.metadata.blocklet.index.BlockletBTreeIndex;
0: import org.apache.carbondata.core.carbon.metadata.blocklet.index.BlockletIndex;
0: import org.apache.carbondata.core.carbon.metadata.blocklet.index.BlockletMinMaxIndex;
0: import org.apache.carbondata.core.carbon.metadata.blocklet.sort.SortState;
0: import org.apache.carbondata.core.carbon.metadata.datatype.DataType;
0: import org.apache.carbondata.core.carbon.metadata.encoder.Encoding;
0: import org.apache.carbondata.core.carbon.metadata.schema.table.column.ColumnSchema;
0: import org.apache.carbondata.core.datastorage.store.FileHolder;
0: import org.apache.carbondata.core.datastorage.store.impl.FileFactory;
0: import org.apache.carbondata.core.metadata.ValueEncoderMeta;
1: import org.apache.carbondata.core.reader.CarbonFooterReader;
0: import org.apache.carbondata.core.reader.CarbonIndexFileReader;
0: import org.apache.carbondata.format.BlockIndex;
1: import org.apache.carbondata.format.FileFooter;
1: 
1: /**
1:  * Below class will be used to convert the thrift object of data file
1:  * meta data to wrapper object
1:  */
0: public class DataFileFooterConverter {
1: 
0:   private static final LogService LOGGER =
0:       LogServiceFactory.getLogService(DataFileFooterConverter.class.getName());
1: 
1:   /**
0:    * Below method will be used to get the index info from index file
1:    *
0:    * @param filePath           file path of the index file
0:    * @param tableBlockInfoList table block index
0:    * @return list of index info
0:    * @throws IOException problem while reading the index file
1:    */
0:   public List<DataFileFooter> getIndexInfo(String filePath, List<TableBlockInfo> tableBlockInfoList)
1:       throws IOException {
0:     CarbonIndexFileReader indexReader = new CarbonIndexFileReader();
0:     List<DataFileFooter> dataFileFooters = new ArrayList<DataFileFooter>();
1:     try {
0:       // open the reader
0:       indexReader.openThriftReader(filePath);
0:       // get the index header
0:       org.apache.carbondata.format.IndexHeader readIndexHeader = indexReader.readIndexHeader();
1:       List<ColumnSchema> columnSchemaList = new ArrayList<ColumnSchema>();
0:       List<org.apache.carbondata.format.ColumnSchema> table_columns =
0:           readIndexHeader.getTable_columns();
1:       for (int i = 0; i < table_columns.size(); i++) {
0:         columnSchemaList.add(thriftColumnSchmeaToWrapperColumnSchema(table_columns.get(i)));
1:       }
0:       // get the segment info
0:       SegmentInfo segmentInfo = getSegmentInfo(readIndexHeader.getSegment_info());
0:       BlockletIndex blockletIndex = null;
0:       int counter = 0;
0:       DataFileFooter dataFileFooter = null;
0:       // read the block info from file
0:       while (indexReader.hasNext()) {
0:         BlockIndex readBlockIndexInfo = indexReader.readBlockIndexInfo();
0:         blockletIndex = getBlockletIndex(readBlockIndexInfo.getBlock_index());
0:         dataFileFooter = new DataFileFooter();
0:         dataFileFooter.setBlockletIndex(blockletIndex);
1:         dataFileFooter.setColumnInTable(columnSchemaList);
0:         dataFileFooter.setNumberOfRows(readBlockIndexInfo.getNum_rows());
0:         dataFileFooter.setTableBlockInfo(tableBlockInfoList.get(counter++));
0:         dataFileFooter.setSegmentInfo(segmentInfo);
0:         dataFileFooters.add(dataFileFooter);
1:       }
1:     } finally {
0:       indexReader.closeThriftReader();
1:     }
0:     return dataFileFooters;
1:   }
1: 
1:   /**
1:    * Below method will be used to convert thrift file meta to wrapper file meta
1:    */
0:   public DataFileFooter readDataFileFooter(String filePath, long blockOffset, long blockLength)
1:       throws IOException {
1:     DataFileFooter dataFileFooter = new DataFileFooter();
0:     FileHolder fileReader = null;
1:     try {
0:       long completeBlockLength = blockOffset + blockLength;
1:       long footerPointer = completeBlockLength - 8;
0:       fileReader = FileFactory.getFileHolder(FileFactory.getFileType(filePath));
0:       long actualFooterOffset = fileReader.readLong(filePath, footerPointer);
0:       CarbonFooterReader reader = new CarbonFooterReader(filePath, actualFooterOffset);
1:       FileFooter footer = reader.readFooter();
0:       dataFileFooter.setVersionId(footer.getVersion());
1:       dataFileFooter.setNumberOfRows(footer.getNum_rows());
1:       dataFileFooter.setSegmentInfo(getSegmentInfo(footer.getSegment_info()));
1:       List<ColumnSchema> columnSchemaList = new ArrayList<ColumnSchema>();
1:       List<org.apache.carbondata.format.ColumnSchema> table_columns = footer.getTable_columns();
1:       for (int i = 0; i < table_columns.size(); i++) {
0:         columnSchemaList.add(thriftColumnSchmeaToWrapperColumnSchema(table_columns.get(i)));
1:       }
1:       dataFileFooter.setColumnInTable(columnSchemaList);
1: 
1:       List<org.apache.carbondata.format.BlockletIndex> leaf_node_indices_Thrift =
1:           footer.getBlocklet_index_list();
1:       List<BlockletIndex> blockletIndexList = new ArrayList<BlockletIndex>();
1:       for (int i = 0; i < leaf_node_indices_Thrift.size(); i++) {
1:         BlockletIndex blockletIndex = getBlockletIndex(leaf_node_indices_Thrift.get(i));
1:         blockletIndexList.add(blockletIndex);
1:       }
1: 
1:       List<org.apache.carbondata.format.BlockletInfo> leaf_node_infos_Thrift =
1:           footer.getBlocklet_info_list();
1:       List<BlockletInfo> blockletInfoList = new ArrayList<BlockletInfo>();
1:       for (int i = 0; i < leaf_node_infos_Thrift.size(); i++) {
1:         BlockletInfo blockletInfo = getBlockletInfo(leaf_node_infos_Thrift.get(i));
1:         blockletInfo.setBlockletIndex(blockletIndexList.get(i));
1:         blockletInfoList.add(blockletInfo);
1:       }
1:       dataFileFooter.setBlockletList(blockletInfoList);
1:       dataFileFooter.setBlockletIndex(getBlockletIndexForDataFileFooter(blockletIndexList));
1:     } finally {
1:       if (null != fileReader) {
1:         fileReader.finish();
1:       }
1:     }
1:     return dataFileFooter;
1:   }
1: 
1:   /**
0:    * Below method will be used to get blocklet index for data file meta
1:    *
0:    * @param blockletIndexList
0:    * @return blocklet index
1:    */
0:   private BlockletIndex getBlockletIndexForDataFileFooter(List<BlockletIndex> blockletIndexList) {
0:     BlockletIndex blockletIndex = new BlockletIndex();
0:     BlockletBTreeIndex blockletBTreeIndex = new BlockletBTreeIndex();
0:     blockletBTreeIndex.setStartKey(blockletIndexList.get(0).getBtreeIndex().getStartKey());
0:     blockletBTreeIndex
0:         .setEndKey(blockletIndexList.get(blockletIndexList.size() - 1).getBtreeIndex().getEndKey());
0:     blockletIndex.setBtreeIndex(blockletBTreeIndex);
0:     byte[][] currentMinValue = blockletIndexList.get(0).getMinMaxIndex().getMinValues().clone();
0:     byte[][] currentMaxValue = blockletIndexList.get(0).getMinMaxIndex().getMaxValues().clone();
0:     byte[][] minValue = null;
0:     byte[][] maxValue = null;
0:     for (int i = 1; i < blockletIndexList.size(); i++) {
0:       minValue = blockletIndexList.get(i).getMinMaxIndex().getMinValues();
0:       maxValue = blockletIndexList.get(i).getMinMaxIndex().getMaxValues();
0:       for (int j = 0; j < maxValue.length; j++) {
0:         if (ByteUtil.UnsafeComparer.INSTANCE.compareTo(currentMinValue[j], minValue[j]) > 0) {
0:           currentMinValue[j] = minValue[j].clone();
1:         }
0:         if (ByteUtil.UnsafeComparer.INSTANCE.compareTo(currentMaxValue[j], maxValue[j]) < 0) {
0:           currentMaxValue[j] = maxValue[j].clone();
1:         }
1:       }
1:     }
1: 
0:     BlockletMinMaxIndex minMax = new BlockletMinMaxIndex();
0:     minMax.setMaxValues(currentMaxValue);
0:     minMax.setMinValues(currentMinValue);
0:     blockletIndex.setMinMaxIndex(minMax);
0:     return blockletIndex;
1:   }
1: 
0:   private ColumnSchema thriftColumnSchmeaToWrapperColumnSchema(
0:       org.apache.carbondata.format.ColumnSchema externalColumnSchema) {
0:     ColumnSchema wrapperColumnSchema = new ColumnSchema();
0:     wrapperColumnSchema.setColumnUniqueId(externalColumnSchema.getColumn_id());
0:     wrapperColumnSchema.setColumnName(externalColumnSchema.getColumn_name());
0:     wrapperColumnSchema.setColumnar(externalColumnSchema.isColumnar());
0:     wrapperColumnSchema
0:         .setDataType(thriftDataTyopeToWrapperDataType(externalColumnSchema.data_type));
0:     wrapperColumnSchema.setDimensionColumn(externalColumnSchema.isDimension());
0:     List<Encoding> encoders = new ArrayList<Encoding>();
0:     for (org.apache.carbondata.format.Encoding encoder : externalColumnSchema.getEncoders()) {
0:       encoders.add(fromExternalToWrapperEncoding(encoder));
1:     }
0:     wrapperColumnSchema.setEncodingList(encoders);
0:     wrapperColumnSchema.setNumberOfChild(externalColumnSchema.getNum_child());
0:     wrapperColumnSchema.setPrecision(externalColumnSchema.getPrecision());
0:     wrapperColumnSchema.setColumnGroup(externalColumnSchema.getColumn_group_id());
0:     wrapperColumnSchema.setScale(externalColumnSchema.getScale());
0:     wrapperColumnSchema.setDefaultValue(externalColumnSchema.getDefault_value());
0:     wrapperColumnSchema.setAggregateFunction(externalColumnSchema.getAggregate_function());
0:     return wrapperColumnSchema;
1:   }
1: 
1:   /**
1:    * Below method is to convert the blocklet info of the thrift to wrapper
1:    * blocklet info
1:    *
1:    * @param blockletInfoThrift blocklet info of the thrift
1:    * @return blocklet info wrapper
1:    */
1:   private BlockletInfo getBlockletInfo(
1:       org.apache.carbondata.format.BlockletInfo blockletInfoThrift) {
1:     BlockletInfo blockletInfo = new BlockletInfo();
1:     List<DataChunk> dimensionColumnChunk = new ArrayList<DataChunk>();
1:     List<DataChunk> measureChunk = new ArrayList<DataChunk>();
1:     Iterator<org.apache.carbondata.format.DataChunk> column_data_chunksIterator =
1:         blockletInfoThrift.getColumn_data_chunksIterator();
1:     if (null != column_data_chunksIterator) {
1:       while (column_data_chunksIterator.hasNext()) {
1:         org.apache.carbondata.format.DataChunk next = column_data_chunksIterator.next();
1:         if (next.isRowMajor()) {
1:           dimensionColumnChunk.add(getDataChunk(next, false));
1:         } else if (next.getEncoders().contains(org.apache.carbondata.format.Encoding.DELTA)) {
1:           measureChunk.add(getDataChunk(next, true));
1:         } else {
1:           dimensionColumnChunk.add(getDataChunk(next, false));
1:         }
1:       }
1:     }
1:     blockletInfo.setDimensionColumnChunk(dimensionColumnChunk);
1:     blockletInfo.setMeasureColumnChunk(measureChunk);
1:     blockletInfo.setNumberOfRows(blockletInfoThrift.getNum_rows());
1:     return blockletInfo;
1:   }
1: 
1:   /**
0:    * Below method is convert the thrift encoding to wrapper encoding
1:    *
0:    * @param encoderThrift thrift encoding
0:    * @return wrapper encoding
1:    */
0:   private Encoding fromExternalToWrapperEncoding(
0:       org.apache.carbondata.format.Encoding encoderThrift) {
0:     switch (encoderThrift) {
0:       case DICTIONARY:
0:         return Encoding.DICTIONARY;
0:       case DELTA:
0:         return Encoding.DELTA;
0:       case RLE:
0:         return Encoding.RLE;
0:       case INVERTED_INDEX:
0:         return Encoding.INVERTED_INDEX;
0:       case BIT_PACKED:
0:         return Encoding.BIT_PACKED;
0:       case DIRECT_DICTIONARY:
0:         return Encoding.DIRECT_DICTIONARY;
0:       default:
0:         return Encoding.DICTIONARY;
1:     }
1:   }
1: 
1:   /**
0:    * Below method will be used to convert the thrift compression to wrapper
0:    * compression codec
1:    *
0:    * @param compressionCodecThrift
0:    * @return wrapper compression codec
1:    */
0:   private CompressionCodec getCompressionCodec(
0:       org.apache.carbondata.format.CompressionCodec compressionCodecThrift) {
0:     switch (compressionCodecThrift) {
0:       case SNAPPY:
0:         return CompressionCodec.SNAPPY;
0:       default:
0:         return CompressionCodec.SNAPPY;
1:     }
1:   }
1: 
1:   /**
0:    * Below method will be used to convert thrift segment object to wrapper
0:    * segment object
1:    *
0:    * @param segmentInfo thrift segment info object
0:    * @return wrapper segment info object
1:    */
0:   private SegmentInfo getSegmentInfo(org.apache.carbondata.format.SegmentInfo segmentInfo) {
0:     SegmentInfo info = new SegmentInfo();
0:     int[] cardinality = new int[segmentInfo.getColumn_cardinalities().size()];
0:     for (int i = 0; i < cardinality.length; i++) {
0:       cardinality[i] = segmentInfo.getColumn_cardinalities().get(i);
1:     }
0:     info.setColumnCardinality(cardinality);
0:     info.setNumberOfColumns(segmentInfo.getNum_cols());
0:     return info;
1:   }
1: 
1:   /**
0:    * Below method will be used to convert the blocklet index of thrift to
0:    * wrapper
1:    *
0:    * @param blockletIndexThrift
0:    * @return blocklet index wrapper
1:    */
0:   private BlockletIndex getBlockletIndex(
0:       org.apache.carbondata.format.BlockletIndex blockletIndexThrift) {
0:     org.apache.carbondata.format.BlockletBTreeIndex btreeIndex =
0:         blockletIndexThrift.getB_tree_index();
0:     org.apache.carbondata.format.BlockletMinMaxIndex minMaxIndex =
0:         blockletIndexThrift.getMin_max_index();
0:     return new BlockletIndex(
0:         new BlockletBTreeIndex(btreeIndex.getStart_key(), btreeIndex.getEnd_key()),
0:         new BlockletMinMaxIndex(minMaxIndex.getMin_values(), minMaxIndex.getMax_values()));
1:   }
1: 
1:   /**
0:    * Below method will be used to convert the thrift compression meta to
0:    * wrapper chunk compression meta
1:    *
0:    * @param chunkCompressionMetaThrift
0:    * @return chunkCompressionMetaWrapper
1:    */
0:   private ChunkCompressorMeta getChunkCompressionMeta(
0:       org.apache.carbondata.format.ChunkCompressionMeta chunkCompressionMetaThrift) {
0:     ChunkCompressorMeta compressorMeta = new ChunkCompressorMeta();
0:     compressorMeta
0:         .setCompressor(getCompressionCodec(chunkCompressionMetaThrift.getCompression_codec()));
0:     compressorMeta.setCompressedSize(chunkCompressionMetaThrift.getTotal_compressed_size());
0:     compressorMeta.setUncompressedSize(chunkCompressionMetaThrift.getTotal_uncompressed_size());
0:     return compressorMeta;
1:   }
1: 
1:   /**
0:    * Below method will be used to convert the thrift data type to wrapper data
0:    * type
1:    *
0:    * @param dataTypeThrift
0:    * @return dataType wrapper
1:    */
0:   private DataType thriftDataTyopeToWrapperDataType(
0:       org.apache.carbondata.format.DataType dataTypeThrift) {
0:     switch (dataTypeThrift) {
0:       case STRING:
0:         return DataType.STRING;
0:       case SHORT:
0:         return DataType.SHORT;
0:       case INT:
0:         return DataType.INT;
0:       case LONG:
0:         return DataType.LONG;
0:       case DOUBLE:
0:         return DataType.DOUBLE;
0:       case DECIMAL:
0:         return DataType.DECIMAL;
0:       case TIMESTAMP:
0:         return DataType.TIMESTAMP;
0:       case ARRAY:
0:         return DataType.ARRAY;
0:       case STRUCT:
0:         return DataType.STRUCT;
0:       default:
0:         return DataType.STRING;
1:     }
1:   }
1: 
1:   /**
0:    * Below method will be used to convert the thrift presence meta to wrapper
0:    * presence meta
1:    *
0:    * @param presentMetadataThrift
0:    * @return wrapper presence meta
1:    */
0:   private PresenceMeta getPresenceMeta(
0:       org.apache.carbondata.format.PresenceMeta presentMetadataThrift) {
0:     PresenceMeta presenceMeta = new PresenceMeta();
0:     presenceMeta.setRepresentNullValues(presentMetadataThrift.isRepresents_presence());
0:     presenceMeta.setBitSet(BitSet.valueOf(presentMetadataThrift.getPresent_bit_stream()));
0:     return presenceMeta;
1:   }
1: 
1:   /**
0:    * Below method will be used to convert the thrift object to wrapper object
1:    *
0:    * @param sortStateThrift
0:    * @return wrapper sort state object
1:    */
0:   private SortState getSortState(org.apache.carbondata.format.SortState sortStateThrift) {
0:     if (sortStateThrift == org.apache.carbondata.format.SortState.SORT_EXPLICIT) {
0:       return SortState.SORT_EXPLICT;
0:     } else if (sortStateThrift == org.apache.carbondata.format.SortState.SORT_NATIVE) {
0:       return SortState.SORT_NATIVE;
1:     } else {
0:       return SortState.SORT_NONE;
1:     }
1:   }
1: 
1:   /**
0:    * Below method will be used to convert the thrift data chunk to wrapper
0:    * data chunk
1:    *
0:    * @param datachunkThrift
0:    * @return wrapper data chunk
1:    */
0:   private DataChunk getDataChunk(org.apache.carbondata.format.DataChunk datachunkThrift,
0:       boolean isPresenceMetaPresent) {
0:     DataChunk dataChunk = new DataChunk();
0:     dataChunk.setColumnUniqueIdList(datachunkThrift.getColumn_ids());
0:     dataChunk.setDataPageLength(datachunkThrift.getData_page_length());
0:     dataChunk.setDataPageOffset(datachunkThrift.getData_page_offset());
0:     if (isPresenceMetaPresent) {
0:       dataChunk.setNullValueIndexForColumn(getPresenceMeta(datachunkThrift.getPresence()));
1:     }
0:     dataChunk.setRlePageLength(datachunkThrift.getRle_page_length());
0:     dataChunk.setRlePageOffset(datachunkThrift.getRle_page_offset());
0:     dataChunk.setRowMajor(datachunkThrift.isRowMajor());
0:     dataChunk.setRowIdPageLength(datachunkThrift.getRowid_page_length());
0:     dataChunk.setRowIdPageOffset(datachunkThrift.getRowid_page_offset());
0:     dataChunk.setSortState(getSortState(datachunkThrift.getSort_state()));
0:     dataChunk.setChunkCompressionMeta(getChunkCompressionMeta(datachunkThrift.getChunk_meta()));
0:     List<Encoding> encodingList = new ArrayList<Encoding>(datachunkThrift.getEncoders().size());
0:     for (int i = 0; i < datachunkThrift.getEncoders().size(); i++) {
0:       encodingList.add(fromExternalToWrapperEncoding(datachunkThrift.getEncoders().get(i)));
1:     }
0:     dataChunk.setEncoderList(encodingList);
0:     if (encodingList.contains(Encoding.DELTA)) {
0:       List<ByteBuffer> thriftEncoderMeta = datachunkThrift.getEncoder_meta();
0:       List<ValueEncoderMeta> encodeMetaList =
0:           new ArrayList<ValueEncoderMeta>(thriftEncoderMeta.size());
0:       for (int i = 0; i < thriftEncoderMeta.size(); i++) {
0:         encodeMetaList.add(deserializeEncoderMeta(thriftEncoderMeta.get(i).array()));
1:       }
0:       dataChunk.setValueEncoderMeta(encodeMetaList);
1:     }
0:     return dataChunk;
1:   }
1: 
1:   /**
0:    * Below method will be used to convert the encode metadata to
0:    * ValueEncoderMeta object
1:    *
0:    * @param encoderMeta
0:    * @return ValueEncoderMeta object
1:    */
0:   private ValueEncoderMeta deserializeEncoderMeta(byte[] encoderMeta) {
0:     // TODO : should remove the unnecessary fields.
0:     ByteArrayInputStream aos = null;
0:     ObjectInputStream objStream = null;
0:     ValueEncoderMeta meta = null;
1:     try {
0:       aos = new ByteArrayInputStream(encoderMeta);
0:       objStream = new ObjectInputStream(aos);
0:       meta = (ValueEncoderMeta) objStream.readObject();
0:     } catch (ClassNotFoundException e) {
0:       LOGGER.error(e);
0:     } catch (IOException e) {
0:       CarbonUtil.closeStreams(objStream);
1:     }
0:     return meta;
1:   }
1: }
author:QiangCai
-------------------------------------------------------------------------------
commit:41347d8
/////////////////////////////////////////////////////////////////////////
1:  * Licensed to the Apache Software Foundation (ASF) under one or more
1:  * contributor license agreements.  See the NOTICE file distributed with
1:  * this work for additional information regarding copyright ownership.
1:  * The ASF licenses this file to You under the Apache License, Version 2.0
1:  * (the "License"); you may not use this file except in compliance with
1:  * the License.  You may obtain a copy of the License at
1:  * Unless required by applicable law or agreed to in writing, software
1:  * distributed under the License is distributed on an "AS IS" BASIS,
1:  * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
1:  * See the License for the specific language governing permissions and
1:  * limitations under the License.
author:jackylk
-------------------------------------------------------------------------------
commit:ce09aaa
/////////////////////////////////////////////////////////////////////////
0: import org.apache.carbondata.core.datastore.FileHolder;
1: import org.apache.carbondata.core.datastore.block.TableBlockInfo;
1: import org.apache.carbondata.core.datastore.impl.FileFactory;
1: import org.apache.carbondata.core.metadata.ColumnarFormatVersion;
1: import org.apache.carbondata.core.metadata.blocklet.BlockletInfo;
1: import org.apache.carbondata.core.metadata.blocklet.DataFileFooter;
1: import org.apache.carbondata.core.metadata.blocklet.datachunk.DataChunk;
1: import org.apache.carbondata.core.metadata.blocklet.index.BlockletIndex;
1: import org.apache.carbondata.core.metadata.schema.table.column.ColumnSchema;
commit:0ef3fb8
/////////////////////////////////////////////////////////////////////////
0: import org.apache.carbondata.core.carbon.ColumnarFormatVersion;
/////////////////////////////////////////////////////////////////////////
1:       dataFileFooter.setVersionId(ColumnarFormatVersion.valueOf((short) footer.getVersion()));
author:kumarvishal
-------------------------------------------------------------------------------
commit:d54dc64
/////////////////////////////////////////////////////////////////////////
1: public class DataFileFooterConverter extends AbstractDataFileFooterConverter {
1:   @Override public DataFileFooter readDataFileFooter(TableBlockInfo tableBlockInfo)
1:       long completeBlockLength = tableBlockInfo.getBlockLength();
1:       fileReader = FileFactory.getFileHolder(FileFactory.getFileType(tableBlockInfo.getFilePath()));
1:       long actualFooterOffset = fileReader.readLong(tableBlockInfo.getFilePath(), footerPointer);
1:       CarbonFooterReader reader =
1:           new CarbonFooterReader(tableBlockInfo.getFilePath(), actualFooterOffset);
0:       dataFileFooter.setVersionId((short) footer.getVersion());
/////////////////////////////////////////////////////////////////////////
/////////////////////////////////////////////////////////////////////////
commit:b87c743
/////////////////////////////////////////////////////////////////////////
0: import org.apache.carbondata.core.carbon.datastore.block.BlockInfo;
/////////////////////////////////////////////////////////////////////////
0:         dataFileFooter.setBlockInfo(new BlockInfo(tableBlockInfo));
author:mohammadshahidkhan
-------------------------------------------------------------------------------
commit:fe1b0f0
/////////////////////////////////////////////////////////////////////////
0: import org.apache.carbondata.core.constants.CarbonCommonConstants;
/////////////////////////////////////////////////////////////////////////
0:       throws IOException, CarbonUtilException {
/////////////////////////////////////////////////////////////////////////
0:         TableBlockInfo tableBlockInfo = tableBlockInfoList.get(counter++);
0:         int blockletSize = getBlockletSize(readBlockIndexInfo);
0:         tableBlockInfo.getBlockletInfos().setNoOfBlockLets(blockletSize);
0:         dataFileFooter.setTableBlockInfo(tableBlockInfo);
/////////////////////////////////////////////////////////////////////////
0:    * the methods returns the number of blocklets in a block
0:    * @param readBlockIndexInfo
0:    * @return
0:    */
0:   private int getBlockletSize(BlockIndex readBlockIndexInfo) {
0:     long num_rows = readBlockIndexInfo.getNum_rows();
0:     int blockletSize = Integer.parseInt(CarbonProperties.getInstance()
0:         .getProperty(CarbonCommonConstants.BLOCKLET_SIZE,
0:             CarbonCommonConstants.BLOCKLET_SIZE_DEFAULT_VAL));
0:     int remainder = (int) (num_rows % blockletSize);
0:     int noOfBlockLet = (int) (num_rows / blockletSize);
0:     // there could be some blocklets which will not
0:     // contain the total records equal to the blockletSize
0:     if (remainder > 0) {
0:       noOfBlockLet = noOfBlockLet + 1;
0:     }
0:     return noOfBlockLet;
0:   }
0: 
0:   /**
============================================================================