1:ca7e2e3: /*
1:ca7e2e3:  * Licensed to the Apache Software Foundation (ASF) under one or more
1:ca7e2e3:  * contributor license agreements.  See the NOTICE file distributed with
1:ca7e2e3:  * this work for additional information regarding copyright ownership.
1:ca7e2e3:  * The ASF licenses this file to You under the Apache License, Version 2.0
1:ca7e2e3:  * (the "License"); you may not use this file except in compliance with
1:ca7e2e3:  * the License.  You may obtain a copy of the License at
1:ca7e2e3:  *
1:ca7e2e3:  *    http://www.apache.org/licenses/LICENSE-2.0
1:ca7e2e3:  *
1:ca7e2e3:  * Unless required by applicable law or agreed to in writing, software
1:ca7e2e3:  * distributed under the License is distributed on an "AS IS" BASIS,
1:ca7e2e3:  * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
1:ca7e2e3:  * See the License for the specific language governing permissions and
1:ca7e2e3:  * limitations under the License.
1:ca7e2e3:  */
2:ca7e2e3: 
1:ca7e2e3: package org.apache.carbondata.datamap.examples;
1:ca7e2e3: 
1:ca7e2e3: import java.io.BufferedReader;
1:ca7e2e3: import java.io.DataInputStream;
1:ecd6c0c: import java.io.File;
1:ca7e2e3: import java.io.IOException;
1:ca7e2e3: import java.io.InputStreamReader;
1:ca7e2e3: import java.util.ArrayList;
1:ca7e2e3: import java.util.BitSet;
1:ca7e2e3: import java.util.List;
1:ca7e2e3: 
1:ca7e2e3: import org.apache.carbondata.common.logging.LogService;
1:ca7e2e3: import org.apache.carbondata.common.logging.LogServiceFactory;
1:d35fbaf: import org.apache.carbondata.core.datamap.dev.DataMapModel;
1:fc2a7eb: import org.apache.carbondata.core.datamap.dev.cgdatamap.CoarseGrainDataMap;
1:ca7e2e3: import org.apache.carbondata.core.datastore.block.SegmentProperties;
1:ca7e2e3: import org.apache.carbondata.core.datastore.impl.FileFactory;
1:ca7e2e3: import org.apache.carbondata.core.fileoperations.AtomicFileOperations;
1:68e5b52: import org.apache.carbondata.core.fileoperations.AtomicFileOperationFactory;
1:ca7e2e3: import org.apache.carbondata.core.indexstore.Blocklet;
1:d35fbaf: import org.apache.carbondata.core.indexstore.PartitionSpec;
1:ca7e2e3: import org.apache.carbondata.core.memory.MemoryException;
1:ca7e2e3: import org.apache.carbondata.core.scan.filter.FilterUtil;
1:ca7e2e3: import org.apache.carbondata.core.scan.filter.executer.FilterExecuter;
1:ca7e2e3: import org.apache.carbondata.core.scan.filter.resolver.FilterResolverIntf;
1:ca7e2e3: import org.apache.carbondata.core.util.CarbonUtil;
1:ca7e2e3: 
1:ca7e2e3: import com.google.gson.Gson;
1:ecd6c0c: import org.apache.hadoop.fs.FileStatus;
1:ecd6c0c: import org.apache.hadoop.fs.FileSystem;
1:ecd6c0c: import org.apache.hadoop.fs.Path;
1:ecd6c0c: import org.apache.hadoop.fs.PathFilter;
1:ca7e2e3: 
1:ca7e2e3: /**
1:ca7e2e3:  * Datamap implementation for min max blocklet.
1:ca7e2e3:  */
1:fc2a7eb: public class MinMaxIndexDataMap extends CoarseGrainDataMap {
1:ca7e2e3: 
1:ca7e2e3:   private static final LogService LOGGER =
1:89a12af:       LogServiceFactory.getLogService(MinMaxIndexDataMap.class.getName());
1:ca7e2e3: 
1:ecd6c0c:   private String[] indexFilePath;
1:ca7e2e3: 
1:ecd6c0c:   private MinMaxIndexBlockDetails[][] readMinMaxDataMap;
1:ca7e2e3: 
1:d35fbaf:   @Override
1:d35fbaf:   public void init(DataMapModel model) throws MemoryException, IOException {
1:ecd6c0c:     Path indexPath = FileFactory.getPath(model.getFilePath());
1:ca7e2e3: 
1:ecd6c0c:     FileSystem fs = FileFactory.getFileSystem(indexPath);
1:ecd6c0c:     if (!fs.exists(indexPath)) {
1:ecd6c0c:       throw new IOException(
1:ecd6c0c:           String.format("Path %s for MinMax index dataMap does not exist", indexPath));
1:ecd6c0c:     }
1:ecd6c0c:     if (!fs.isDirectory(indexPath)) {
1:ecd6c0c:       throw new IOException(
1:ecd6c0c:           String.format("Path %s for MinMax index dataMap must be a directory", indexPath));
1:ecd6c0c:     }
1:ecd6c0c: 
1:ecd6c0c:     FileStatus[] indexFileStatus = fs.listStatus(indexPath, new PathFilter() {
1:ecd6c0c:       @Override public boolean accept(Path path) {
1:ecd6c0c:         return path.getName().endsWith(".minmaxindex");
3:ca7e2e3:       }
1:ca7e2e3:     });
1:ecd6c0c: 
1:ecd6c0c:     this.indexFilePath = new String[indexFileStatus.length];
1:ecd6c0c:     this.readMinMaxDataMap = new MinMaxIndexBlockDetails[indexFileStatus.length][];
1:ecd6c0c:     for (int i = 0; i < indexFileStatus.length; i++) {
1:ecd6c0c:       this.indexFilePath[i] = indexFileStatus[i].getPath().toString();
1:ecd6c0c:       this.readMinMaxDataMap[i] = readJson(this.indexFilePath[i]);
1:ecd6c0c:     }
1:ca7e2e3:   }
1:ca7e2e3: 
1:d35fbaf:   private MinMaxIndexBlockDetails[] readJson(String filePath) {
1:ca7e2e3:     Gson gsonObjectToRead = new Gson();
1:ca7e2e3:     DataInputStream dataInputStream = null;
1:ca7e2e3:     BufferedReader buffReader = null;
1:ca7e2e3:     InputStreamReader inStream = null;
1:ca7e2e3:     MinMaxIndexBlockDetails[] readMinMax = null;
1:68e5b52:     AtomicFileOperations fileOperation =
1:68e5b52:         AtomicFileOperationFactory.getAtomicFileOperations(filePath);
1:ca7e2e3: 
1:ca7e2e3:     try {
1:ca7e2e3:       if (!FileFactory.isFileExist(filePath, FileFactory.getFileType(filePath))) {
1:ca7e2e3:         return null;
1:ca7e2e3:       }
1:ca7e2e3:       dataInputStream = fileOperation.openForRead();
1:d35fbaf:       inStream = new InputStreamReader(dataInputStream, "UTF-8");
1:ca7e2e3:       buffReader = new BufferedReader(inStream);
1:ca7e2e3:       readMinMax = gsonObjectToRead.fromJson(buffReader, MinMaxIndexBlockDetails[].class);
1:ca7e2e3:     } catch (IOException e) {
1:ca7e2e3:       return null;
1:ca7e2e3:     } finally {
1:ca7e2e3:       CarbonUtil.closeStreams(buffReader, inStream, dataInputStream);
1:ca7e2e3:     }
1:ca7e2e3:     return readMinMax;
1:ca7e2e3:   }
1:ca7e2e3: 
1:ca7e2e3:   /**
1:ca7e2e3:    * Block Prunning logic for Min Max DataMap.
1:ca7e2e3:    *
1:ca7e2e3:    * @param filterExp
1:ca7e2e3:    * @param segmentProperties
1:ca7e2e3:    * @return
1:ca7e2e3:    */
1:d35fbaf:   @Override
1:d35fbaf:   public List<Blocklet> prune(FilterResolverIntf filterExp,
1:d35fbaf:       SegmentProperties segmentProperties, List<PartitionSpec> partitions) {
1:ca7e2e3:     List<Blocklet> blocklets = new ArrayList<>();
1:ca7e2e3: 
1:ca7e2e3:     if (filterExp == null) {
1:ca7e2e3:       for (int i = 0; i < readMinMaxDataMap.length; i++) {
1:ecd6c0c:         for (int j = 0; j < readMinMaxDataMap[i].length; j++) {
1:ecd6c0c:           blocklets.add(new Blocklet(indexFilePath[i],
1:ecd6c0c:               String.valueOf(readMinMaxDataMap[i][j].getBlockletId())));
1:ecd6c0c:         }
1:ca7e2e3:       }
1:ca7e2e3:     } else {
1:ca7e2e3:       FilterExecuter filterExecuter =
1:ca7e2e3:           FilterUtil.getFilterExecuterTree(filterExp, segmentProperties, null);
1:ecd6c0c:       for (int blkIdx = 0; blkIdx < readMinMaxDataMap.length; blkIdx++) {
1:ecd6c0c:         for (int blkltIdx = 0; blkltIdx < readMinMaxDataMap[blkIdx].length; blkltIdx++) {
1:ecd6c0c: 
1:ecd6c0c:           BitSet bitSet = filterExecuter.isScanRequired(
1:ecd6c0c:               readMinMaxDataMap[blkIdx][blkltIdx].getMaxValues(),
1:ecd6c0c:               readMinMaxDataMap[blkIdx][blkltIdx].getMinValues());
1:ecd6c0c:           if (!bitSet.isEmpty()) {
1:ecd6c0c:             String blockFileName = indexFilePath[blkIdx].substring(
1:ecd6c0c:                 indexFilePath[blkIdx].lastIndexOf(File.separatorChar) + 1,
1:ecd6c0c:                 indexFilePath[blkIdx].indexOf(".minmaxindex"));
1:ecd6c0c:             Blocklet blocklet = new Blocklet(blockFileName,
1:ecd6c0c:                 String.valueOf(readMinMaxDataMap[blkIdx][blkltIdx].getBlockletId()));
1:ecd6c0c:             LOGGER.info(String.format("MinMaxDataMap: Need to scan block#%s -> blocklet#%s, %s",
1:ecd6c0c:                 blkIdx, blkltIdx, blocklet));
1:ecd6c0c:             blocklets.add(blocklet);
1:ecd6c0c:           } else {
1:ecd6c0c:             LOGGER.info(String.format("MinMaxDataMap: Skip scan block#%s -> blocklet#%s",
1:ecd6c0c:                 blkIdx, blkltIdx));
1:ecd6c0c:           }
1:ca7e2e3:         }
1:ca7e2e3:       }
1:ca7e2e3:     }
1:ca7e2e3:     return blocklets;
1:ca7e2e3:   }
1:ca7e2e3: 
1:ca7e2e3:   @Override
1:d35fbaf:   public boolean isScanRequired(FilterResolverIntf filterExp) {
1:d35fbaf:     throw new UnsupportedOperationException();
1:d35fbaf:   }
1:d35fbaf: 
1:d35fbaf:   @Override
1:ca7e2e3:   public void clear() {
1:ca7e2e3:     readMinMaxDataMap = null;
1:ca7e2e3:   }
1:07a77fa: 
1:07a77fa:   @Override
1:07a77fa:   public void finish() {
1:07a77fa: 
1:07a77fa:   }
1:ca7e2e3: 
1:ca7e2e3: }
============================================================================
author:Zhang Zhichao
-------------------------------------------------------------------------------
commit:68e5b52
/////////////////////////////////////////////////////////////////////////
1: import org.apache.carbondata.core.fileoperations.AtomicFileOperationFactory;
/////////////////////////////////////////////////////////////////////////
1:     AtomicFileOperations fileOperation =
1:         AtomicFileOperationFactory.getAtomicFileOperations(filePath);
author:kunal642
-------------------------------------------------------------------------------
commit:878bbd8
/////////////////////////////////////////////////////////////////////////
/////////////////////////////////////////////////////////////////////////
0:     AtomicFileOperations fileOperation = AtomicFileOperationsF(filePath, FileFactory.getFileType(filePath));
author:akashrn5
-------------------------------------------------------------------------------
commit:07a77fa
/////////////////////////////////////////////////////////////////////////
1:   @Override
1:   public void finish() {
1: 
1:   }
1: 
author:xuchuanyin
-------------------------------------------------------------------------------
commit:ecd6c0c
/////////////////////////////////////////////////////////////////////////
1: import java.io.File;
/////////////////////////////////////////////////////////////////////////
/////////////////////////////////////////////////////////////////////////
1: import org.apache.hadoop.fs.FileStatus;
1: import org.apache.hadoop.fs.FileSystem;
1: import org.apache.hadoop.fs.Path;
1: import org.apache.hadoop.fs.PathFilter;
/////////////////////////////////////////////////////////////////////////
1:   private String[] indexFilePath;
1:   private MinMaxIndexBlockDetails[][] readMinMaxDataMap;
1:     Path indexPath = FileFactory.getPath(model.getFilePath());
1:     FileSystem fs = FileFactory.getFileSystem(indexPath);
1:     if (!fs.exists(indexPath)) {
1:       throw new IOException(
1:           String.format("Path %s for MinMax index dataMap does not exist", indexPath));
1:     }
1:     if (!fs.isDirectory(indexPath)) {
1:       throw new IOException(
1:           String.format("Path %s for MinMax index dataMap must be a directory", indexPath));
1:     }
1: 
1:     FileStatus[] indexFileStatus = fs.listStatus(indexPath, new PathFilter() {
1:       @Override public boolean accept(Path path) {
1:         return path.getName().endsWith(".minmaxindex");
1: 
1:     this.indexFilePath = new String[indexFileStatus.length];
1:     this.readMinMaxDataMap = new MinMaxIndexBlockDetails[indexFileStatus.length][];
1:     for (int i = 0; i < indexFileStatus.length; i++) {
1:       this.indexFilePath[i] = indexFileStatus[i].getPath().toString();
1:       this.readMinMaxDataMap[i] = readJson(this.indexFilePath[i]);
1:     }
/////////////////////////////////////////////////////////////////////////
1:         for (int j = 0; j < readMinMaxDataMap[i].length; j++) {
1:           blocklets.add(new Blocklet(indexFilePath[i],
1:               String.valueOf(readMinMaxDataMap[i][j].getBlockletId())));
1:         }
1:       for (int blkIdx = 0; blkIdx < readMinMaxDataMap.length; blkIdx++) {
1:         for (int blkltIdx = 0; blkltIdx < readMinMaxDataMap[blkIdx].length; blkltIdx++) {
1: 
1:           BitSet bitSet = filterExecuter.isScanRequired(
1:               readMinMaxDataMap[blkIdx][blkltIdx].getMaxValues(),
1:               readMinMaxDataMap[blkIdx][blkltIdx].getMinValues());
1:           if (!bitSet.isEmpty()) {
1:             String blockFileName = indexFilePath[blkIdx].substring(
1:                 indexFilePath[blkIdx].lastIndexOf(File.separatorChar) + 1,
1:                 indexFilePath[blkIdx].indexOf(".minmaxindex"));
1:             Blocklet blocklet = new Blocklet(blockFileName,
1:                 String.valueOf(readMinMaxDataMap[blkIdx][blkltIdx].getBlockletId()));
1:             LOGGER.info(String.format("MinMaxDataMap: Need to scan block#%s -> blocklet#%s, %s",
1:                 blkIdx, blkltIdx, blocklet));
1:             blocklets.add(blocklet);
1:           } else {
1:             LOGGER.info(String.format("MinMaxDataMap: Skip scan block#%s -> blocklet#%s",
1:                 blkIdx, blkltIdx));
1:           }
author:Jacky Li
-------------------------------------------------------------------------------
commit:fc2a7eb
/////////////////////////////////////////////////////////////////////////
1: import org.apache.carbondata.core.datamap.dev.cgdatamap.CoarseGrainDataMap;
/////////////////////////////////////////////////////////////////////////
1: public class MinMaxIndexDataMap extends CoarseGrainDataMap {
commit:89a12af
/////////////////////////////////////////////////////////////////////////
0: import org.apache.carbondata.core.datamap.dev.cgdatamap.AbstractCoarseGrainIndexDataMap;
/////////////////////////////////////////////////////////////////////////
/////////////////////////////////////////////////////////////////////////
0: public class MinMaxIndexDataMap extends AbstractCoarseGrainIndexDataMap {
1:       LogServiceFactory.getLogService(MinMaxIndexDataMap.class.getName());
author:ravipesala
-------------------------------------------------------------------------------
commit:d35fbaf
/////////////////////////////////////////////////////////////////////////
1: import org.apache.carbondata.core.datamap.dev.DataMapModel;
0: import org.apache.carbondata.core.datamap.dev.cgdatamap.AbstractCoarseGrainDataMap;
/////////////////////////////////////////////////////////////////////////
1: import org.apache.carbondata.core.indexstore.PartitionSpec;
0: import org.apache.carbondata.core.indexstore.row.DataMapRow;
0: import org.apache.carbondata.core.scan.filter.FilterExpressionProcessor;
/////////////////////////////////////////////////////////////////////////
0: public class MinMaxDataMap extends AbstractCoarseGrainDataMap {
/////////////////////////////////////////////////////////////////////////
1:   @Override
1:   public void init(DataMapModel model) throws MemoryException, IOException {
0:     this.filePath = model.getFilePath();
/////////////////////////////////////////////////////////////////////////
1:   private MinMaxIndexBlockDetails[] readJson(String filePath) {
/////////////////////////////////////////////////////////////////////////
1:       inStream = new InputStreamReader(dataInputStream, "UTF-8");
/////////////////////////////////////////////////////////////////////////
1:   @Override
1:   public List<Blocklet> prune(FilterResolverIntf filterExp,
1:       SegmentProperties segmentProperties, List<PartitionSpec> partitions) {
0:         blocklets.add(new Blocklet(filePath, String.valueOf(readMinMaxDataMap[i].getBlockletId())));
/////////////////////////////////////////////////////////////////////////
0:           blocklets.add(new Blocklet(filePath,
/////////////////////////////////////////////////////////////////////////
1:   public boolean isScanRequired(FilterResolverIntf filterExp) {
1:     throw new UnsupportedOperationException();
1:   }
1: 
1:   @Override
author:sounakr
-------------------------------------------------------------------------------
commit:ca7e2e3
/////////////////////////////////////////////////////////////////////////
1: /*
1:  * Licensed to the Apache Software Foundation (ASF) under one or more
1:  * contributor license agreements.  See the NOTICE file distributed with
1:  * this work for additional information regarding copyright ownership.
1:  * The ASF licenses this file to You under the Apache License, Version 2.0
1:  * (the "License"); you may not use this file except in compliance with
1:  * the License.  You may obtain a copy of the License at
1:  *
1:  *    http://www.apache.org/licenses/LICENSE-2.0
1:  *
1:  * Unless required by applicable law or agreed to in writing, software
1:  * distributed under the License is distributed on an "AS IS" BASIS,
1:  * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
1:  * See the License for the specific language governing permissions and
1:  * limitations under the License.
1:  */
1: 
1: package org.apache.carbondata.datamap.examples;
1: 
1: import java.io.BufferedReader;
1: import java.io.DataInputStream;
1: import java.io.IOException;
1: import java.io.InputStreamReader;
1: import java.util.ArrayList;
1: import java.util.BitSet;
1: import java.util.List;
1: 
1: import org.apache.carbondata.common.logging.LogService;
1: import org.apache.carbondata.common.logging.LogServiceFactory;
0: import org.apache.carbondata.core.constants.CarbonCommonConstants;
0: import org.apache.carbondata.core.datamap.dev.DataMap;
1: import org.apache.carbondata.core.datastore.block.SegmentProperties;
0: import org.apache.carbondata.core.datastore.filesystem.CarbonFile;
0: import org.apache.carbondata.core.datastore.filesystem.CarbonFileFilter;
1: import org.apache.carbondata.core.datastore.impl.FileFactory;
1: import org.apache.carbondata.core.fileoperations.AtomicFileOperations;
0: import org.apache.carbondata.core.fileoperations.AtomicFileOperationsImpl;
1: import org.apache.carbondata.core.indexstore.Blocklet;
1: import org.apache.carbondata.core.memory.MemoryException;
1: import org.apache.carbondata.core.scan.filter.FilterUtil;
1: import org.apache.carbondata.core.scan.filter.executer.FilterExecuter;
1: import org.apache.carbondata.core.scan.filter.resolver.FilterResolverIntf;
1: import org.apache.carbondata.core.util.CarbonUtil;
1: 
1: import com.google.gson.Gson;
1: 
1: /**
1:  * Datamap implementation for min max blocklet.
1:  */
0: public class MinMaxDataMap implements DataMap {
1: 
1:   private static final LogService LOGGER =
0:       LogServiceFactory.getLogService(MinMaxDataMap.class.getName());
1: 
0:   public static final String NAME = "clustered.minmax.btree.blocklet";
1: 
0:   private String filePath;
1: 
0:   private MinMaxIndexBlockDetails[] readMinMaxDataMap;
1: 
0:   @Override public void init(String filePath) throws MemoryException, IOException {
0:     this.filePath = filePath;
0:     CarbonFile[] listFiles = getCarbonMinMaxIndexFiles(filePath, "0");
0:     for (int i = 0; i < listFiles.length; i++) {
0:       readMinMaxDataMap = readJson(listFiles[i].getPath());
1:     }
1:   }
1: 
0:   private CarbonFile[] getCarbonMinMaxIndexFiles(String filePath, String segmentId) {
0:     String path = filePath.substring(0, filePath.lastIndexOf("/") + 1);
0:     CarbonFile carbonFile = FileFactory.getCarbonFile(path);
0:     return carbonFile.listFiles(new CarbonFileFilter() {
0:       @Override public boolean accept(CarbonFile file) {
0:         return file.getName().endsWith(".minmaxindex");
1:       }
1:     });
1:   }
1: 
0:   public MinMaxIndexBlockDetails[] readJson(String filePath) throws IOException {
1:     Gson gsonObjectToRead = new Gson();
1:     DataInputStream dataInputStream = null;
1:     BufferedReader buffReader = null;
1:     InputStreamReader inStream = null;
1:     MinMaxIndexBlockDetails[] readMinMax = null;
0:     AtomicFileOperations fileOperation =
0:         new AtomicFileOperationsImpl(filePath, FileFactory.getFileType(filePath));
1: 
1:     try {
1:       if (!FileFactory.isFileExist(filePath, FileFactory.getFileType(filePath))) {
1:         return null;
1:       }
1:       dataInputStream = fileOperation.openForRead();
0:       inStream = new InputStreamReader(dataInputStream,
0:           CarbonCommonConstants.CARBON_DEFAULT_STREAM_ENCODEFORMAT);
1:       buffReader = new BufferedReader(inStream);
1:       readMinMax = gsonObjectToRead.fromJson(buffReader, MinMaxIndexBlockDetails[].class);
1:     } catch (IOException e) {
1:       return null;
1:     } finally {
1:       CarbonUtil.closeStreams(buffReader, inStream, dataInputStream);
1:     }
1:     return readMinMax;
1:   }
1: 
1:   /**
1:    * Block Prunning logic for Min Max DataMap.
1:    *
1:    * @param filterExp
1:    * @param segmentProperties
1:    * @return
1:    */
0:   @Override public List<Blocklet> prune(FilterResolverIntf filterExp,
0:       SegmentProperties segmentProperties) {
1:     List<Blocklet> blocklets = new ArrayList<>();
1: 
1:     if (filterExp == null) {
1:       for (int i = 0; i < readMinMaxDataMap.length; i++) {
0:         blocklets.add(new Blocklet(readMinMaxDataMap[i].getFilePath(),
0:             String.valueOf(readMinMaxDataMap[i].getBlockletId())));
1:       }
1:     } else {
1:       FilterExecuter filterExecuter =
1:           FilterUtil.getFilterExecuterTree(filterExp, segmentProperties, null);
0:       int startIndex = 0;
0:       while (startIndex < readMinMaxDataMap.length) {
0:         BitSet bitSet = filterExecuter.isScanRequired(readMinMaxDataMap[startIndex].getMaxValues(),
0:             readMinMaxDataMap[startIndex].getMinValues());
0:         if (!bitSet.isEmpty()) {
0:           blocklets.add(new Blocklet(readMinMaxDataMap[startIndex].getFilePath(),
0:               String.valueOf(readMinMaxDataMap[startIndex].getBlockletId())));
1:         }
0:         startIndex++;
1:       }
1:     }
1:     return blocklets;
1:   }
1: 
1:   @Override
1:   public void clear() {
1:     readMinMaxDataMap = null;
1:   }
1: 
1: }
============================================================================