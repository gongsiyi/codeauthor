1:cd6a4ff: /*
1:41347d8:  * Licensed to the Apache Software Foundation (ASF) under one or more
1:41347d8:  * contributor license agreements.  See the NOTICE file distributed with
1:41347d8:  * this work for additional information regarding copyright ownership.
1:41347d8:  * The ASF licenses this file to You under the Apache License, Version 2.0
1:41347d8:  * (the "License"); you may not use this file except in compliance with
1:41347d8:  * the License.  You may obtain a copy of the License at
1:cd6a4ff:  *
1:cd6a4ff:  *    http://www.apache.org/licenses/LICENSE-2.0
1:cd6a4ff:  *
1:41347d8:  * Unless required by applicable law or agreed to in writing, software
1:41347d8:  * distributed under the License is distributed on an "AS IS" BASIS,
1:41347d8:  * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
1:41347d8:  * See the License for the specific language governing permissions and
1:41347d8:  * limitations under the License.
1:cd6a4ff:  */
1:cd6a4ff: package org.apache.carbondata.hadoop;
1:cd6a4ff: 
1:cd6a4ff: import java.io.IOException;
1:cd6a4ff: import java.util.ArrayList;
1:cd6a4ff: import java.util.List;
1:cd6a4ff: import java.util.Map;
1:cd6a4ff: 
1:cd6a4ff: import org.apache.carbondata.common.CarbonIterator;
1:cd6a4ff: import org.apache.carbondata.core.cache.dictionary.Dictionary;
1:a7ac656: import org.apache.carbondata.core.datamap.DataMapStoreManager;
1:ce09aaa: import org.apache.carbondata.core.datastore.block.TableBlockInfo;
1:ce09aaa: import org.apache.carbondata.core.scan.executor.QueryExecutor;
1:ce09aaa: import org.apache.carbondata.core.scan.executor.QueryExecutorFactory;
1:ce09aaa: import org.apache.carbondata.core.scan.executor.exception.QueryExecutionException;
1:ce09aaa: import org.apache.carbondata.core.scan.model.QueryModel;
1:ce09aaa: import org.apache.carbondata.core.scan.result.iterator.ChunkRowIterator;
1:cd6a4ff: import org.apache.carbondata.core.util.CarbonUtil;
1:cd6a4ff: import org.apache.carbondata.hadoop.readsupport.CarbonReadSupport;
1:cd6a4ff: 
1:2a9604c: import org.apache.hadoop.conf.Configuration;
1:cd6a4ff: import org.apache.hadoop.mapreduce.InputSplit;
1:cd6a4ff: import org.apache.hadoop.mapreduce.TaskAttemptContext;
1:cd6a4ff: 
1:cd6a4ff: /**
1:cd6a4ff:  * Reads the data from Carbon store.
1:cd6a4ff:  */
1:ec2d742: public class CarbonRecordReader<T> extends AbstractRecordReader<T> {
1:cd6a4ff: 
1:a700f83:   protected QueryModel queryModel;
1:cd6a4ff: 
1:a700f83:   protected CarbonReadSupport<T> readSupport;
1:cd6a4ff: 
1:a700f83:   protected CarbonIterator<Object[]> carbonIterator;
1:cd6a4ff: 
1:a700f83:   protected QueryExecutor queryExecutor;
1:e3f98fa:   private InputMetricsStats inputMetricsStats;
1:e3f98fa: 
1:0112ed0:   /**
1:0112ed0:    * Whether to clear datamap when reader is closed. In some scenarios such as datamap rebuild,
1:0112ed0:    * we will set it to true and will clear the datamap after rebuild
1:0112ed0:    */
1:0112ed0:   private boolean skipClearDataMapAtClose = false;
1:0112ed0: 
1:e3f98fa:   public CarbonRecordReader(QueryModel queryModel, CarbonReadSupport<T> readSupport,
1:2a9604c:       InputMetricsStats inputMetricsStats, Configuration configuration) {
1:2a9604c:     this(queryModel, readSupport, configuration);
1:e3f98fa:     this.inputMetricsStats = inputMetricsStats;
1:e3f98fa:   }
1:cd6a4ff: 
1:2a9604c:   public CarbonRecordReader(QueryModel queryModel, CarbonReadSupport<T> readSupport,
1:2a9604c:       Configuration configuration) {
1:cd6a4ff:     this.queryModel = queryModel;
1:cd6a4ff:     this.readSupport = readSupport;
1:2a9604c:     this.queryExecutor = QueryExecutorFactory.getQueryExecutor(queryModel, configuration);
1:cd6a4ff:   }
1:cd6a4ff: 
1:5f6a56c:   @Override
1:5f6a56c:   public void initialize(InputSplit inputSplit, TaskAttemptContext context)
1:cd6a4ff:       throws IOException, InterruptedException {
1:5f6a56c:     // The input split can contain single HDFS block or multiple blocks, so firstly get all the
1:5f6a56c:     // blocks and then set them in the query model.
1:5f6a56c:     List<CarbonInputSplit> splitList;
1:5f6a56c:     if (inputSplit instanceof CarbonInputSplit) {
1:5f6a56c:       splitList = new ArrayList<>(1);
1:5f6a56c:       splitList.add((CarbonInputSplit) inputSplit);
1:256dbed:     } else if (inputSplit instanceof CarbonMultiBlockSplit) {
1:5f6a56c:       // contains multiple blocks, this is an optimization for concurrent query.
1:5f6a56c:       CarbonMultiBlockSplit multiBlockSplit = (CarbonMultiBlockSplit) inputSplit;
1:5f6a56c:       splitList = multiBlockSplit.getAllSplits();
1:5f6a56c:     } else {
1:5f6a56c:       throw new RuntimeException("unsupported input split type: " + inputSplit);
1:5f6a56c:     }
1:b338459:     // It should use the exists tableBlockInfos if tableBlockInfos of queryModel is not empty
1:b338459:     // otherwise the prune is no use before this method
1:5593d16:     if (!queryModel.isFG()) {
1:b338459:       List<TableBlockInfo> tableBlockInfoList = CarbonInputSplit.createBlocks(splitList);
1:b338459:       queryModel.setTableBlockInfos(tableBlockInfoList);
1:b338459:     }
1:29dc302:     readSupport.initialize(queryModel.getProjectionColumns(), queryModel.getTable());
1:cd6a4ff:     try {
1:5f6a56c:       carbonIterator = new ChunkRowIterator(queryExecutor.execute(queryModel));
1:cd6a4ff:     } catch (QueryExecutionException e) {
1:cd6a4ff:       throw new InterruptedException(e.getMessage());
1:cd6a4ff:     }
1:cd6a4ff:   }
1:cd6a4ff: 
1:cd6a4ff:   @Override public boolean nextKeyValue() {
1:cd6a4ff:     return carbonIterator.hasNext();
1:cd6a4ff:   }
1:cd6a4ff: 
1:cd6a4ff:   @Override public Void getCurrentKey() throws IOException, InterruptedException {
1:cd6a4ff:     return null;
1:cd6a4ff:   }
1:cd6a4ff: 
1:cd6a4ff:   @Override public T getCurrentValue() throws IOException, InterruptedException {
1:ec2d742:     rowCount += 1;
1:e3f98fa:     if (null != inputMetricsStats) {
1:e3f98fa:       inputMetricsStats.incrementRecordRead(1L);
1:e3f98fa:     }
1:cd6a4ff:     return readSupport.readRow(carbonIterator.next());
1:cd6a4ff:   }
1:cd6a4ff: 
1:cd6a4ff:   @Override public float getProgress() throws IOException, InterruptedException {
1:cd6a4ff:     // TODO : Implement it based on total number of rows it is going to retrive.
1:cd6a4ff:     return 0;
1:cd6a4ff:   }
1:cd6a4ff: 
1:cd6a4ff:   @Override public void close() throws IOException {
1:ec2d742:     logStatistics(rowCount, queryModel.getStatisticsRecorder());
1:cd6a4ff:     // clear dictionary cache
1:cd6a4ff:     Map<String, Dictionary> columnToDictionaryMapping = queryModel.getColumnToDictionaryMapping();
1:cd6a4ff:     if (null != columnToDictionaryMapping) {
1:cd6a4ff:       for (Map.Entry<String, Dictionary> entry : columnToDictionaryMapping.entrySet()) {
1:cd6a4ff:         CarbonUtil.clearDictionaryCache(entry.getValue());
1:cd6a4ff:       }
1:cd6a4ff:     }
1:0112ed0:     if (!skipClearDataMapAtClose) {
1:0112ed0:       // Clear the datamap cache
1:0112ed0:       DataMapStoreManager.getInstance().clearDataMaps(
1:0112ed0:           queryModel.getTable().getAbsoluteTableIdentifier());
1:0112ed0:     }
1:cd6a4ff:     // close read support
1:cd6a4ff:     readSupport.close();
1:5a3d5bb:     try {
1:5a3d5bb:       queryExecutor.finish();
1:5a3d5bb:     } catch (QueryExecutionException e) {
1:5a3d5bb:       throw new IOException(e);
1:cd6a4ff:     }
1:cd6a4ff:   }
1:0112ed0: 
1:0112ed0:   public void setSkipClearDataMapAtClose(boolean skipClearDataMapAtClose) {
1:0112ed0:     this.skipClearDataMapAtClose = skipClearDataMapAtClose;
1:0112ed0:   }
1:5a3d5bb: }
============================================================================
author:kunal642
-------------------------------------------------------------------------------
commit:2a9604c
/////////////////////////////////////////////////////////////////////////
1: import org.apache.hadoop.conf.Configuration;
/////////////////////////////////////////////////////////////////////////
1:       InputMetricsStats inputMetricsStats, Configuration configuration) {
1:     this(queryModel, readSupport, configuration);
1:   public CarbonRecordReader(QueryModel queryModel, CarbonReadSupport<T> readSupport,
1:       Configuration configuration) {
1:     this.queryExecutor = QueryExecutorFactory.getQueryExecutor(queryModel, configuration);
author:xuchuanyin
-------------------------------------------------------------------------------
commit:0112ed0
/////////////////////////////////////////////////////////////////////////
1:   /**
1:    * Whether to clear datamap when reader is closed. In some scenarios such as datamap rebuild,
1:    * we will set it to true and will clear the datamap after rebuild
1:    */
1:   private boolean skipClearDataMapAtClose = false;
1: 
/////////////////////////////////////////////////////////////////////////
1:     if (!skipClearDataMapAtClose) {
1:       // Clear the datamap cache
1:       DataMapStoreManager.getInstance().clearDataMaps(
1:           queryModel.getTable().getAbsoluteTableIdentifier());
1:     }
/////////////////////////////////////////////////////////////////////////
1: 
1:   public void setSkipClearDataMapAtClose(boolean skipClearDataMapAtClose) {
1:     this.skipClearDataMapAtClose = skipClearDataMapAtClose;
1:   }
author:xubo245
-------------------------------------------------------------------------------
commit:5593d16
/////////////////////////////////////////////////////////////////////////
1:     if (!queryModel.isFG()) {
commit:b338459
/////////////////////////////////////////////////////////////////////////
1:     // It should use the exists tableBlockInfos if tableBlockInfos of queryModel is not empty
1:     // otherwise the prune is no use before this method
0:     if (queryModel.getTableBlockInfos().isEmpty()) {
1:       List<TableBlockInfo> tableBlockInfoList = CarbonInputSplit.createBlocks(splitList);
1:       queryModel.setTableBlockInfos(tableBlockInfoList);
1:     }
commit:a7faef8
/////////////////////////////////////////////////////////////////////////
commit:a7ac656
/////////////////////////////////////////////////////////////////////////
1: import org.apache.carbondata.core.datamap.DataMapStoreManager;
/////////////////////////////////////////////////////////////////////////
0:     // Clear the datamap cache
0:     DataMapStoreManager.getInstance().getDefaultDataMap(queryModel.getTable()).clear();
author:ajantha-bhat
-------------------------------------------------------------------------------
commit:5f68a79
/////////////////////////////////////////////////////////////////////////
0:     DataMapStoreManager.getInstance()
0:         .clearDataMaps(queryModel.getTable().getAbsoluteTableIdentifier());
author:manishgupta88
-------------------------------------------------------------------------------
commit:29dc302
/////////////////////////////////////////////////////////////////////////
1:     readSupport.initialize(queryModel.getProjectionColumns(), queryModel.getTable());
author:Manohar
-------------------------------------------------------------------------------
commit:e3f98fa
/////////////////////////////////////////////////////////////////////////
1:   private InputMetricsStats inputMetricsStats;
1: 
1:   public CarbonRecordReader(QueryModel queryModel, CarbonReadSupport<T> readSupport,
0:       InputMetricsStats inputMetricsStats) {
0:     this(queryModel, readSupport);
1:     this.inputMetricsStats = inputMetricsStats;
1:   }
/////////////////////////////////////////////////////////////////////////
1:     if (null != inputMetricsStats) {
1:       inputMetricsStats.incrementRecordRead(1L);
1:     }
author:cenyuhai
-------------------------------------------------------------------------------
commit:a700f83
/////////////////////////////////////////////////////////////////////////
1:   protected QueryModel queryModel;
1:   protected CarbonReadSupport<T> readSupport;
1:   protected CarbonIterator<Object[]> carbonIterator;
1:   protected QueryExecutor queryExecutor;
author:nareshpr
-------------------------------------------------------------------------------
commit:ec2d742
/////////////////////////////////////////////////////////////////////////
1: public class CarbonRecordReader<T> extends AbstractRecordReader<T> {
/////////////////////////////////////////////////////////////////////////
1:     rowCount += 1;
/////////////////////////////////////////////////////////////////////////
1:     logStatistics(rowCount, queryModel.getStatisticsRecorder());
author:QiangCai
-------------------------------------------------------------------------------
commit:256dbed
/////////////////////////////////////////////////////////////////////////
1:     } else if (inputSplit instanceof CarbonMultiBlockSplit) {
commit:41347d8
/////////////////////////////////////////////////////////////////////////
1:  * Licensed to the Apache Software Foundation (ASF) under one or more
1:  * contributor license agreements.  See the NOTICE file distributed with
1:  * this work for additional information regarding copyright ownership.
1:  * The ASF licenses this file to You under the Apache License, Version 2.0
1:  * (the "License"); you may not use this file except in compliance with
1:  * the License.  You may obtain a copy of the License at
1:  * Unless required by applicable law or agreed to in writing, software
1:  * distributed under the License is distributed on an "AS IS" BASIS,
1:  * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
1:  * See the License for the specific language governing permissions and
1:  * limitations under the License.
author:jackylk
-------------------------------------------------------------------------------
commit:ce09aaa
/////////////////////////////////////////////////////////////////////////
1: import org.apache.carbondata.core.datastore.block.TableBlockInfo;
1: import org.apache.carbondata.core.scan.executor.QueryExecutor;
1: import org.apache.carbondata.core.scan.executor.QueryExecutorFactory;
1: import org.apache.carbondata.core.scan.executor.exception.QueryExecutionException;
1: import org.apache.carbondata.core.scan.model.QueryModel;
1: import org.apache.carbondata.core.scan.result.iterator.ChunkRowIterator;
commit:5f6a56c
/////////////////////////////////////////////////////////////////////////
/////////////////////////////////////////////////////////////////////////
/////////////////////////////////////////////////////////////////////////
1:   @Override
1:   public void initialize(InputSplit inputSplit, TaskAttemptContext context)
1:     // The input split can contain single HDFS block or multiple blocks, so firstly get all the
1:     // blocks and then set them in the query model.
1:     List<CarbonInputSplit> splitList;
1:     if (inputSplit instanceof CarbonInputSplit) {
1:       splitList = new ArrayList<>(1);
1:       splitList.add((CarbonInputSplit) inputSplit);
0:     } else if (inputSplit instanceof CarbonMultiBlockSplit){
1:       // contains multiple blocks, this is an optimization for concurrent query.
1:       CarbonMultiBlockSplit multiBlockSplit = (CarbonMultiBlockSplit) inputSplit;
1:       splitList = multiBlockSplit.getAllSplits();
1:     } else {
1:       throw new RuntimeException("unsupported input split type: " + inputSplit);
1:     }
0:     List<TableBlockInfo> tableBlockInfoList = CarbonInputSplit.createBlocks(splitList);
0:     readSupport.initialize(queryModel.getProjectionColumns(),
0:         queryModel.getAbsoluteTableIdentifier());
1:       carbonIterator = new ChunkRowIterator(queryExecutor.execute(queryModel));
author:ravipesala
-------------------------------------------------------------------------------
commit:376d69f
/////////////////////////////////////////////////////////////////////////
0:     this.queryExecutor = QueryExecutorFactory.getQueryExecutor(queryModel);
commit:cd6a4ff
/////////////////////////////////////////////////////////////////////////
1: /*
0:  * Licensed to the Apache Software Foundation (ASF) under one
0:  * or more contributor license agreements.  See the NOTICE file
0:  * distributed with this work for additional information
0:  * regarding copyright ownership.  The ASF licenses this file
0:  * to you under the Apache License, Version 2.0 (the
0:  * "License"); you may not use this file except in compliance
0:  * with the License.  You may obtain a copy of the License at
1:  *
1:  *    http://www.apache.org/licenses/LICENSE-2.0
1:  *
0:  * Unless required by applicable law or agreed to in writing,
0:  * software distributed under the License is distributed on an
0:  * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
0:  * KIND, either express or implied.  See the License for the
0:  * specific language governing permissions and limitations
0:  * under the License.
1:  */
1: package org.apache.carbondata.hadoop;
1: 
1: import java.io.IOException;
1: import java.util.ArrayList;
1: import java.util.List;
1: import java.util.Map;
1: 
1: import org.apache.carbondata.common.CarbonIterator;
1: import org.apache.carbondata.core.cache.dictionary.Dictionary;
0: import org.apache.carbondata.core.carbon.datastore.block.TableBlockInfo;
1: import org.apache.carbondata.core.util.CarbonUtil;
1: import org.apache.carbondata.hadoop.readsupport.CarbonReadSupport;
0: import org.apache.carbondata.scan.executor.QueryExecutorFactory;
0: import org.apache.carbondata.scan.executor.exception.QueryExecutionException;
0: import org.apache.carbondata.scan.model.QueryModel;
0: import org.apache.carbondata.scan.result.BatchResult;
0: import org.apache.carbondata.scan.result.iterator.ChunkRowIterator;
1: 
1: import org.apache.hadoop.mapreduce.InputSplit;
0: import org.apache.hadoop.mapreduce.RecordReader;
1: import org.apache.hadoop.mapreduce.TaskAttemptContext;
1: 
1: /**
1:  * Reads the data from Carbon store.
1:  */
0: public class CarbonRecordReader<T> extends RecordReader<Void, T> {
1: 
0:   private QueryModel queryModel;
1: 
0:   private CarbonReadSupport<T> readSupport;
1: 
0:   private CarbonIterator<Object[]> carbonIterator;
1: 
0:   public CarbonRecordReader(QueryModel queryModel, CarbonReadSupport<T> readSupport) {
1:     this.queryModel = queryModel;
1:     this.readSupport = readSupport;
1:   }
1: 
0:   @Override public void initialize(InputSplit split, TaskAttemptContext context)
1:       throws IOException, InterruptedException {
0:     CarbonInputSplit carbonInputSplit = (CarbonInputSplit) split;
0:     List<TableBlockInfo> tableBlockInfoList = new ArrayList<TableBlockInfo>();
0:     tableBlockInfoList.add(
0:         new TableBlockInfo(carbonInputSplit.getPath().toString(), carbonInputSplit.getStart(),
0:             carbonInputSplit.getSegmentId(), carbonInputSplit.getLocations(),
0:             carbonInputSplit.getLength()));
0:     queryModel.setTableBlockInfos(tableBlockInfoList);
0:     readSupport
0:         .intialize(queryModel.getProjectionColumns(), queryModel.getAbsoluteTableIdentifier());
1:     try {
0:       carbonIterator = new ChunkRowIterator(
0:           (CarbonIterator<BatchResult>) QueryExecutorFactory.getQueryExecutor(queryModel)
0:               .execute(queryModel));
1:     } catch (QueryExecutionException e) {
1:       throw new InterruptedException(e.getMessage());
1:     }
1:   }
1: 
1:   @Override public boolean nextKeyValue() {
1:     return carbonIterator.hasNext();
1: 
1:   }
1: 
1:   @Override public Void getCurrentKey() throws IOException, InterruptedException {
1:     return null;
1:   }
1: 
1:   @Override public T getCurrentValue() throws IOException, InterruptedException {
1:     return readSupport.readRow(carbonIterator.next());
1:   }
1: 
1:   @Override public float getProgress() throws IOException, InterruptedException {
1:     // TODO : Implement it based on total number of rows it is going to retrive.
1:     return 0;
1:   }
1: 
1:   @Override public void close() throws IOException {
1:     // clear dictionary cache
1:     Map<String, Dictionary> columnToDictionaryMapping = queryModel.getColumnToDictionaryMapping();
1:     if (null != columnToDictionaryMapping) {
1:       for (Map.Entry<String, Dictionary> entry : columnToDictionaryMapping.entrySet()) {
1:         CarbonUtil.clearDictionaryCache(entry.getValue());
1:       }
1:     }
1:     // close read support
1:     readSupport.close();
1:   }
1: }
author:kumarvishal
-------------------------------------------------------------------------------
commit:5a3d5bb
/////////////////////////////////////////////////////////////////////////
0: import org.apache.carbondata.scan.executor.QueryExecutor;
/////////////////////////////////////////////////////////////////////////
0:   private QueryExecutor queryExecutor;
0: 
0:     this.queryExecutor = QueryExecutorFactory.getQueryExecutor();
/////////////////////////////////////////////////////////////////////////
0:       carbonIterator =
0:           new ChunkRowIterator((CarbonIterator<BatchResult>) queryExecutor.execute(queryModel));
/////////////////////////////////////////////////////////////////////////
1:     try {
1:       queryExecutor.finish();
1:     } catch (QueryExecutionException e) {
1:       throw new IOException(e);
1:     }
author:mohammadshahidkhan
-------------------------------------------------------------------------------
commit:fe1b0f0
/////////////////////////////////////////////////////////////////////////
0: import org.apache.carbondata.core.carbon.datastore.block.BlockletInfos;
/////////////////////////////////////////////////////////////////////////
0:     BlockletInfos blockletInfos = new BlockletInfos(carbonInputSplit.getNumberOfBlocklets(), 0,
0:         carbonInputSplit.getNumberOfBlocklets());
0:             carbonInputSplit.getLength(), blockletInfos));
============================================================================