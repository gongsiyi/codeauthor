1:d509f17: /*
1:d509f17:  * Licensed to the Apache Software Foundation (ASF) under one or more
1:d509f17:  * contributor license agreements.  See the NOTICE file distributed with
1:d509f17:  * this work for additional information regarding copyright ownership.
1:d509f17:  * The ASF licenses this file to You under the Apache License, Version 2.0
1:d509f17:  * (the "License"); you may not use this file except in compliance with
1:d509f17:  * the License.  You may obtain a copy of the License at
1:d509f17:  *
1:d509f17:  *    http://www.apache.org/licenses/LICENSE-2.0
1:d509f17:  *
1:d509f17:  * Unless required by applicable law or agreed to in writing, software
1:d509f17:  * distributed under the License is distributed on an "AS IS" BASIS,
1:d509f17:  * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
1:d509f17:  * See the License for the specific language governing permissions and
1:d509f17:  * limitations under the License.
1:d509f17:  */
1:d509f17: package org.apache.carbondata.core.datastore.chunk.reader.measure.v3;
1:d509f17: 
1:d509f17: import java.io.ByteArrayInputStream;
1:d509f17: import java.io.IOException;
1:d509f17: import java.nio.ByteBuffer;
1:d509f17: 
1:daa6465: import org.apache.carbondata.core.datastore.FileReader;
1:d509f17: import org.apache.carbondata.core.datastore.chunk.impl.MeasureRawColumnChunk;
1:8f08c4a: import org.apache.carbondata.core.datastore.compression.CompressorFactory;
1:d509f17: import org.apache.carbondata.core.datastore.page.ColumnPage;
1:d509f17: import org.apache.carbondata.core.memory.MemoryException;
1:d509f17: import org.apache.carbondata.core.metadata.blocklet.BlockletInfo;
1:438b442: import org.apache.carbondata.core.scan.executor.util.QueryUtil;
1:8f08c4a: import org.apache.carbondata.core.util.CarbonMetadataUtil;
1:d509f17: import org.apache.carbondata.core.util.CarbonUtil;
1:d509f17: import org.apache.carbondata.format.DataChunk2;
1:d509f17: import org.apache.carbondata.format.DataChunk3;
1:d509f17: 
1:d509f17: /**
1:d509f17:  * Measure column V3 Reader class which will be used to read and uncompress
1:d509f17:  * V3 format data
1:d509f17:  * data format
1:d509f17:  * Data Format
1:d509f17:  * <FileHeader>
1:d509f17:  * <Column1 Data ChunkV3><Column1<Page1><Page2><Page3><Page4>>
1:d509f17:  * <Column2 Data ChunkV3><Column2<Page1><Page2><Page3><Page4>>
1:d509f17:  * <Column3 Data ChunkV3><Column3<Page1><Page2><Page3><Page4>>
1:d509f17:  * <Column4 Data ChunkV3><Column4<Page1><Page2><Page3><Page4>>
1:d509f17:  * <File Footer>
1:d509f17:  */
1:d509f17: public class CompressedMsrChunkFileBasedPageLevelReaderV3
1:d509f17:     extends CompressedMeasureChunkFileBasedReaderV3 {
1:d509f17: 
1:d509f17:   /**
1:d509f17:    * end position of last measure in carbon data file
1:d509f17:    */
1:d509f17:   private long measureOffsets;
1:d509f17: 
1:d509f17:   public CompressedMsrChunkFileBasedPageLevelReaderV3(BlockletInfo blockletInfo, String filePath) {
1:d509f17:     super(blockletInfo, filePath);
1:d509f17:     measureOffsets = blockletInfo.getMeasureOffsets();
1:d509f17:   }
1:d509f17: 
1:d509f17:   /**
1:d509f17:    * Below method will be used to read the measure column data form carbon data file
1:d509f17:    * 1. Get the length of the data to be read
1:d509f17:    * 2. Allocate the direct buffer
1:d509f17:    * 3. read the data from file
1:d509f17:    * 4. Get the data chunk object from data read
1:d509f17:    * 5. Create the raw chunk object and fill the details
1:d509f17:    *
1:d509f17:    * @param fileReader          reader for reading the column from carbon data file
1:d509f17:    * @param blockletColumnIndex blocklet index of the column in carbon data file
1:d509f17:    * @return measure raw chunk
1:d509f17:    */
1:daa6465:   @Override public MeasureRawColumnChunk readRawMeasureChunk(FileReader fileReader,
1:d509f17:       int blockletColumnIndex) throws IOException {
1:d509f17:     int dataLength = 0;
1:d509f17:     // to calculate the length of the data to be read
1:d509f17:     // column other than last column we can subtract the offset of current column with
1:d509f17:     // next column and get the total length.
1:d509f17:     // but for last column we need to use lastDimensionOffset which is the end position
1:e8da880:     // of the last dimension, we can subtract current dimension offset from lastDimensionOffset
1:d509f17:     if (measureColumnChunkOffsets.size() - 1 == blockletColumnIndex) {
1:d509f17:       dataLength = (int) (measureOffsets - measureColumnChunkOffsets.get(blockletColumnIndex));
1:d509f17:     } else {
1:d509f17:       dataLength =
1:d509f17:           (int) (measureColumnChunkOffsets.get(blockletColumnIndex + 1) - measureColumnChunkOffsets
1:d509f17:               .get(blockletColumnIndex));
1:d509f17:     }
1:d509f17:     ByteBuffer buffer;
1:d509f17:     // read the data from carbon data file
1:d509f17:     synchronized (fileReader) {
1:d509f17:       buffer = fileReader
1:d509f17:           .readByteBuffer(filePath, measureColumnChunkOffsets.get(blockletColumnIndex),
1:d509f17:               measureColumnChunkLength.get(blockletColumnIndex));
1:d509f17:     }
1:d509f17:     // get the data chunk which will have all the details about the data pages
1:d509f17:     DataChunk3 dataChunk = CarbonUtil.readDataChunk3(new ByteArrayInputStream(buffer.array()));
1:d509f17:     return getMeasureRawColumnChunk(fileReader, blockletColumnIndex,
1:d509f17:         measureColumnChunkOffsets.get(blockletColumnIndex), dataLength, null, dataChunk);
1:d509f17:   }
1:d509f17: 
1:d509f17:   /**
1:d509f17:    * Below method will be used to read the multiple measure column data in group
1:d509f17:    * and divide into measure raw chunk object
1:d509f17:    * Steps for reading
1:d509f17:    * 1. Get the length of the data to be read
1:d509f17:    * 2. Allocate the direct buffer
1:d509f17:    * 3. read the data from file
1:d509f17:    * 4. Get the data chunk object from file for each column
1:d509f17:    * 5. Create the raw chunk object and fill the details for each column
1:d509f17:    * 6. increment the offset of the data
1:d509f17:    *
1:d509f17:    * @param fileReader         reader which will be used to read the measure columns data from file
1:d509f17:    * @param startColumnBlockletIndex blocklet index of the first measure column
1:d509f17:    * @param endColumnBlockletIndex   blocklet index of the last measure column
1:d509f17:    * @return MeasureRawColumnChunk array
1:d509f17:    */
1:daa6465:   protected MeasureRawColumnChunk[] readRawMeasureChunksInGroup(FileReader fileReader,
1:d509f17:       int startColumnBlockletIndex, int endColumnBlockletIndex) throws IOException {
1:d509f17:     // create raw chunk for each measure column
1:d509f17:     MeasureRawColumnChunk[] measureDataChunk =
1:d509f17:         new MeasureRawColumnChunk[endColumnBlockletIndex - startColumnBlockletIndex + 1];
1:d509f17:     int index = 0;
1:d509f17:     for (int i = startColumnBlockletIndex; i <= endColumnBlockletIndex; i++) {
1:d509f17:       measureDataChunk[index] = readRawMeasureChunk(fileReader, i);
1:d509f17:       index++;
1:d509f17:     }
1:d509f17:     return measureDataChunk;
1:d509f17:   }
1:d509f17: 
1:d509f17:   /**
1:d509f17:    * Below method will be used to convert the compressed measure chunk raw data to actual data
1:d509f17:    *
1:d509f17:    * @param rawColumnPage measure raw chunk
1:d509f17:    * @param pageNumber            number
1:d509f17:    * @return DimensionColumnDataChunk
1:d509f17:    */
1:daa6465:   @Override public ColumnPage decodeColumnPage(
1:d509f17:       MeasureRawColumnChunk rawColumnPage, int pageNumber)
1:d509f17:       throws IOException, MemoryException {
1:d509f17:     // data chunk of blocklet column
1:d509f17:     DataChunk3 dataChunk3 = rawColumnPage.getDataChunkV3();
1:d509f17:     // data chunk of page
1:d509f17:     DataChunk2 pageMetadata = dataChunk3.getData_chunk_list().get(pageNumber);
1:8f08c4a:     String compressorName = CarbonMetadataUtil.getCompressorNameFromChunkMeta(
1:8f08c4a:         pageMetadata.getChunk_meta());
1:8f08c4a:     this.compressor = CompressorFactory.getInstance().getCompressor(compressorName);
1:d509f17:     // calculating the start point of data
1:d509f17:     // as buffer can contain multiple column data, start point will be datachunkoffset +
1:d509f17:     // data chunk length + page offset
1:d509f17:     long offset = rawColumnPage.getOffSet() + measureColumnChunkLength
1:d509f17:         .get(rawColumnPage.getColumnIndex()) + dataChunk3.getPage_offset().get(pageNumber);
1:d509f17:     ByteBuffer buffer = rawColumnPage.getFileReader()
1:d509f17:         .readByteBuffer(filePath, offset, pageMetadata.data_page_length);
1:d509f17: 
1:d509f17:     ColumnPage decodedPage = decodeMeasure(pageMetadata, buffer, 0);
1:8f08c4a:     decodedPage.setNullBits(QueryUtil.getNullBitSet(pageMetadata.presence, this.compressor));
1:d509f17:     return decodedPage;
1:d509f17:   }
1:d509f17: 
1:d509f17: }
============================================================================
author:xuchuanyin
-------------------------------------------------------------------------------
commit:8f08c4a
/////////////////////////////////////////////////////////////////////////
1: import org.apache.carbondata.core.datastore.compression.CompressorFactory;
1: import org.apache.carbondata.core.util.CarbonMetadataUtil;
/////////////////////////////////////////////////////////////////////////
1:     String compressorName = CarbonMetadataUtil.getCompressorNameFromChunkMeta(
1:         pageMetadata.getChunk_meta());
1:     this.compressor = CompressorFactory.getInstance().getCompressor(compressorName);
/////////////////////////////////////////////////////////////////////////
1:     decodedPage.setNullBits(QueryUtil.getNullBitSet(pageMetadata.presence, this.compressor));
author:ajantha-bhat
-------------------------------------------------------------------------------
commit:438b442
/////////////////////////////////////////////////////////////////////////
1: import org.apache.carbondata.core.scan.executor.util.QueryUtil;
/////////////////////////////////////////////////////////////////////////
0:     decodedPage.setNullBits(QueryUtil.getNullBitSet(pageMetadata.presence));
author:xubo245
-------------------------------------------------------------------------------
commit:e8da880
/////////////////////////////////////////////////////////////////////////
1:     // of the last dimension, we can subtract current dimension offset from lastDimensionOffset
author:Jacky Li
-------------------------------------------------------------------------------
commit:daa6465
/////////////////////////////////////////////////////////////////////////
1: import org.apache.carbondata.core.datastore.FileReader;
/////////////////////////////////////////////////////////////////////////
1:   @Override public MeasureRawColumnChunk readRawMeasureChunk(FileReader fileReader,
/////////////////////////////////////////////////////////////////////////
1:   protected MeasureRawColumnChunk[] readRawMeasureChunksInGroup(FileReader fileReader,
/////////////////////////////////////////////////////////////////////////
1:   @Override public ColumnPage decodeColumnPage(
author:ravipesala
-------------------------------------------------------------------------------
commit:d509f17
/////////////////////////////////////////////////////////////////////////
1: /*
1:  * Licensed to the Apache Software Foundation (ASF) under one or more
1:  * contributor license agreements.  See the NOTICE file distributed with
1:  * this work for additional information regarding copyright ownership.
1:  * The ASF licenses this file to You under the Apache License, Version 2.0
1:  * (the "License"); you may not use this file except in compliance with
1:  * the License.  You may obtain a copy of the License at
1:  *
1:  *    http://www.apache.org/licenses/LICENSE-2.0
1:  *
1:  * Unless required by applicable law or agreed to in writing, software
1:  * distributed under the License is distributed on an "AS IS" BASIS,
1:  * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
1:  * See the License for the specific language governing permissions and
1:  * limitations under the License.
1:  */
1: package org.apache.carbondata.core.datastore.chunk.reader.measure.v3;
1: 
1: import java.io.ByteArrayInputStream;
1: import java.io.IOException;
1: import java.nio.ByteBuffer;
1: 
0: import org.apache.carbondata.core.datastore.FileHolder;
1: import org.apache.carbondata.core.datastore.chunk.impl.MeasureRawColumnChunk;
1: import org.apache.carbondata.core.datastore.page.ColumnPage;
1: import org.apache.carbondata.core.memory.MemoryException;
1: import org.apache.carbondata.core.metadata.blocklet.BlockletInfo;
1: import org.apache.carbondata.core.util.CarbonUtil;
1: import org.apache.carbondata.format.DataChunk2;
1: import org.apache.carbondata.format.DataChunk3;
1: 
1: /**
1:  * Measure column V3 Reader class which will be used to read and uncompress
1:  * V3 format data
1:  * data format
1:  * Data Format
1:  * <FileHeader>
1:  * <Column1 Data ChunkV3><Column1<Page1><Page2><Page3><Page4>>
1:  * <Column2 Data ChunkV3><Column2<Page1><Page2><Page3><Page4>>
1:  * <Column3 Data ChunkV3><Column3<Page1><Page2><Page3><Page4>>
1:  * <Column4 Data ChunkV3><Column4<Page1><Page2><Page3><Page4>>
1:  * <File Footer>
1:  */
1: public class CompressedMsrChunkFileBasedPageLevelReaderV3
1:     extends CompressedMeasureChunkFileBasedReaderV3 {
1: 
1:   /**
1:    * end position of last measure in carbon data file
1:    */
1:   private long measureOffsets;
1: 
1:   public CompressedMsrChunkFileBasedPageLevelReaderV3(BlockletInfo blockletInfo, String filePath) {
1:     super(blockletInfo, filePath);
1:     measureOffsets = blockletInfo.getMeasureOffsets();
1:   }
1: 
1:   /**
1:    * Below method will be used to read the measure column data form carbon data file
1:    * 1. Get the length of the data to be read
1:    * 2. Allocate the direct buffer
1:    * 3. read the data from file
1:    * 4. Get the data chunk object from data read
1:    * 5. Create the raw chunk object and fill the details
1:    *
1:    * @param fileReader          reader for reading the column from carbon data file
1:    * @param blockletColumnIndex blocklet index of the column in carbon data file
1:    * @return measure raw chunk
1:    */
0:   @Override public MeasureRawColumnChunk readRawMeasureChunk(FileHolder fileReader,
1:       int blockletColumnIndex) throws IOException {
1:     int dataLength = 0;
1:     // to calculate the length of the data to be read
1:     // column other than last column we can subtract the offset of current column with
1:     // next column and get the total length.
1:     // but for last column we need to use lastDimensionOffset which is the end position
0:     // of the last dimension, we can subtract current dimension offset from lastDimesionOffset
1:     if (measureColumnChunkOffsets.size() - 1 == blockletColumnIndex) {
1:       dataLength = (int) (measureOffsets - measureColumnChunkOffsets.get(blockletColumnIndex));
1:     } else {
1:       dataLength =
1:           (int) (measureColumnChunkOffsets.get(blockletColumnIndex + 1) - measureColumnChunkOffsets
1:               .get(blockletColumnIndex));
1:     }
1:     ByteBuffer buffer;
1:     // read the data from carbon data file
1:     synchronized (fileReader) {
1:       buffer = fileReader
1:           .readByteBuffer(filePath, measureColumnChunkOffsets.get(blockletColumnIndex),
1:               measureColumnChunkLength.get(blockletColumnIndex));
1:     }
1:     // get the data chunk which will have all the details about the data pages
1:     DataChunk3 dataChunk = CarbonUtil.readDataChunk3(new ByteArrayInputStream(buffer.array()));
1:     return getMeasureRawColumnChunk(fileReader, blockletColumnIndex,
1:         measureColumnChunkOffsets.get(blockletColumnIndex), dataLength, null, dataChunk);
1:   }
1: 
1:   /**
1:    * Below method will be used to read the multiple measure column data in group
1:    * and divide into measure raw chunk object
1:    * Steps for reading
1:    * 1. Get the length of the data to be read
1:    * 2. Allocate the direct buffer
1:    * 3. read the data from file
1:    * 4. Get the data chunk object from file for each column
1:    * 5. Create the raw chunk object and fill the details for each column
1:    * 6. increment the offset of the data
1:    *
1:    * @param fileReader         reader which will be used to read the measure columns data from file
1:    * @param startColumnBlockletIndex blocklet index of the first measure column
1:    * @param endColumnBlockletIndex   blocklet index of the last measure column
1:    * @return MeasureRawColumnChunk array
1:    */
0:   protected MeasureRawColumnChunk[] readRawMeasureChunksInGroup(FileHolder fileReader,
1:       int startColumnBlockletIndex, int endColumnBlockletIndex) throws IOException {
1:     // create raw chunk for each measure column
1:     MeasureRawColumnChunk[] measureDataChunk =
1:         new MeasureRawColumnChunk[endColumnBlockletIndex - startColumnBlockletIndex + 1];
1:     int index = 0;
1:     for (int i = startColumnBlockletIndex; i <= endColumnBlockletIndex; i++) {
1:       measureDataChunk[index] = readRawMeasureChunk(fileReader, i);
1:       index++;
1:     }
1:     return measureDataChunk;
1:   }
1: 
1:   /**
1:    * Below method will be used to convert the compressed measure chunk raw data to actual data
1:    *
1:    * @param rawColumnPage measure raw chunk
1:    * @param pageNumber            number
1:    * @return DimensionColumnDataChunk
1:    */
0:   @Override public ColumnPage convertToColumnPage(
1:       MeasureRawColumnChunk rawColumnPage, int pageNumber)
1:       throws IOException, MemoryException {
1:     // data chunk of blocklet column
1:     DataChunk3 dataChunk3 = rawColumnPage.getDataChunkV3();
1:     // data chunk of page
1:     DataChunk2 pageMetadata = dataChunk3.getData_chunk_list().get(pageNumber);
1:     // calculating the start point of data
1:     // as buffer can contain multiple column data, start point will be datachunkoffset +
1:     // data chunk length + page offset
1:     long offset = rawColumnPage.getOffSet() + measureColumnChunkLength
1:         .get(rawColumnPage.getColumnIndex()) + dataChunk3.getPage_offset().get(pageNumber);
1:     ByteBuffer buffer = rawColumnPage.getFileReader()
1:         .readByteBuffer(filePath, offset, pageMetadata.data_page_length);
1: 
1:     ColumnPage decodedPage = decodeMeasure(pageMetadata, buffer, 0);
0:     decodedPage.setNullBits(getNullBitSet(pageMetadata.presence));
1:     return decodedPage;
1:   }
1: 
1: }
============================================================================