1:1e21cd1: /*
1:1e21cd1:  * Licensed to the Apache Software Foundation (ASF) under one or more
1:1e21cd1:  * contributor license agreements.  See the NOTICE file distributed with
1:1e21cd1:  * this work for additional information regarding copyright ownership.
1:1e21cd1:  * The ASF licenses this file to You under the Apache License, Version 2.0
1:1e21cd1:  * (the "License"); you may not use this file except in compliance with
1:1e21cd1:  * the License.  You may obtain a copy of the License at
1:1e21cd1:  *
1:1e21cd1:  *    http://www.apache.org/licenses/LICENSE-2.0
1:1e21cd1:  *
1:1e21cd1:  * Unless required by applicable law or agreed to in writing, software
1:1e21cd1:  * distributed under the License is distributed on an "AS IS" BASIS,
1:1e21cd1:  * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
1:1e21cd1:  * See the License for the specific language governing permissions and
1:1e21cd1:  * limitations under the License.
1:1e21cd1:  */
1:2018048: package org.apache.carbondata.core.datamap;
5:1e21cd1: 
1:1e21cd1: import java.io.IOException;
1:1e21cd1: import java.io.Serializable;
1:1e21cd1: import java.util.ArrayList;
1:2018048: import java.util.Collections;
1:1e21cd1: import java.util.Iterator;
1:1e21cd1: import java.util.List;
1:1e21cd1: 
1:07a77fa: import org.apache.carbondata.core.datamap.dev.DataMap;
1:56330ae: import org.apache.carbondata.core.datamap.dev.expr.DataMapDistributableWrapper;
1:56330ae: import org.apache.carbondata.core.datamap.dev.expr.DataMapExprWrapper;
1:bd02656: import org.apache.carbondata.core.datastore.block.SegmentPropertiesAndSchemaHolder;
1:28f78b2: import org.apache.carbondata.core.indexstore.ExtendedBlocklet;
1:8d3c774: import org.apache.carbondata.core.indexstore.PartitionSpec;
1:7e0803f: import org.apache.carbondata.core.metadata.schema.table.CarbonTable;
1:1e21cd1: 
1:1e21cd1: import org.apache.hadoop.mapreduce.InputSplit;
1:1e21cd1: import org.apache.hadoop.mapreduce.JobContext;
1:1e21cd1: import org.apache.hadoop.mapreduce.RecordReader;
1:1e21cd1: import org.apache.hadoop.mapreduce.TaskAttemptContext;
1:1e21cd1: import org.apache.hadoop.mapreduce.lib.input.FileInputFormat;
1:1e21cd1: 
1:1e21cd1: /**
1:1e21cd1:  * Input format for datamaps, it makes the datamap pruning distributable.
1:1e21cd1:  */
1:28f78b2: public class DistributableDataMapFormat extends FileInputFormat<Void, ExtendedBlocklet> implements
1:1e21cd1:     Serializable {
1:1e21cd1: 
1:7e0803f:   private CarbonTable table;
1:1e21cd1: 
1:56330ae:   private DataMapExprWrapper dataMapExprWrapper;
1:1e21cd1: 
1:8d3c774:   private List<Segment> validSegments;
1:1e21cd1: 
1:2018048:   private List<Segment> invalidSegments;
1:1e21cd1: 
1:8d3c774:   private List<PartitionSpec> partitions;
1:1e21cd1: 
1:2018048:   private  DataMapDistributableWrapper distributable;
1:2018048: 
1:2018048:   private boolean isJobToClearDataMaps = false;
1:2018048: 
1:2018048:   DistributableDataMapFormat(CarbonTable table, DataMapExprWrapper dataMapExprWrapper,
1:2018048:       List<Segment> validSegments, List<Segment> invalidSegments, List<PartitionSpec> partitions,
1:2018048:       boolean isJobToClearDataMaps) {
1:7e0803f:     this.table = table;
1:56330ae:     this.dataMapExprWrapper = dataMapExprWrapper;
1:1e21cd1:     this.validSegments = validSegments;
1:2018048:     this.invalidSegments = invalidSegments;
1:b8a02f3:     this.partitions = partitions;
1:2018048:     this.isJobToClearDataMaps = isJobToClearDataMaps;
1:2018048:   }
1:2018048: 
2:1e21cd1:   @Override
1:1e21cd1:   public List<InputSplit> getSplits(JobContext job) throws IOException {
1:56330ae:     List<DataMapDistributableWrapper> distributables =
1:56330ae:         dataMapExprWrapper.toDistributable(validSegments);
1:1e21cd1:     List<InputSplit> inputSplits = new ArrayList<>(distributables.size());
1:1e21cd1:     inputSplits.addAll(distributables);
1:1e21cd1:     return inputSplits;
6:1e21cd1:   }
1:1e21cd1: 
1:1e21cd1:   @Override
1:28f78b2:   public RecordReader<Void, ExtendedBlocklet> createRecordReader(InputSplit inputSplit,
1:1e21cd1:       TaskAttemptContext taskAttemptContext) throws IOException, InterruptedException {
1:28f78b2:     return new RecordReader<Void, ExtendedBlocklet>() {
1:28f78b2:       private Iterator<ExtendedBlocklet> blockletIterator;
1:28f78b2:       private ExtendedBlocklet currBlocklet;
1:07a77fa:       private List<DataMap> dataMaps;
1:1e21cd1: 
1:280a400:       @Override public void initialize(InputSplit inputSplit, TaskAttemptContext taskAttemptContext)
1:1e21cd1:           throws IOException, InterruptedException {
1:2018048:         distributable = (DataMapDistributableWrapper) inputSplit;
1:2018048:         // clear the segmentMap and from cache in executor when there are invalid segments
1:2018048:         if (invalidSegments.size() > 0) {
1:2018048:           DataMapStoreManager.getInstance().clearInvalidSegments(table, invalidSegments);
1:2018048:         }
1:2018048:         TableDataMap tableDataMap = DataMapStoreManager.getInstance()
1:7e0803f:             .getDataMap(table, distributable.getDistributable().getDataMapSchema());
1:2018048:         if (isJobToClearDataMaps) {
1:2018048:           // if job is to clear datamaps just clear datamaps from cache and return
1:2018048:           DataMapStoreManager.getInstance()
1:2018048:               .clearDataMaps(table.getCarbonTableIdentifier().getTableUniqueName());
1:bd02656:           // clear the segment properties cache from executor
1:bd02656:           SegmentPropertiesAndSchemaHolder.getInstance()
1:bd02656:               .invalidate(table.getAbsoluteTableIdentifier());
1:2018048:           blockletIterator = Collections.emptyIterator();
1:2018048:           return;
1:2018048:         }
1:07a77fa:         dataMaps = tableDataMap.getTableDataMaps(distributable.getDistributable());
1:07a77fa:         List<ExtendedBlocklet> blocklets = tableDataMap
1:07a77fa:             .prune(dataMaps,
1:07a77fa:                 distributable.getDistributable(),
1:07a77fa:                 dataMapExprWrapper.getFilterResolverIntf(distributable.getUniqueId()), partitions);
1:280a400:         for (ExtendedBlocklet blocklet : blocklets) {
1:56330ae:           blocklet.setDataMapUniqueId(distributable.getUniqueId());
1:56330ae:         }
1:56330ae:         blockletIterator = blocklets.iterator();
1:1e21cd1:       }
1:1e21cd1: 
1:1e21cd1:       @Override
1:1e21cd1:       public boolean nextKeyValue() throws IOException, InterruptedException {
1:1e21cd1:         boolean hasNext = blockletIterator.hasNext();
1:1e21cd1:         if (hasNext) {
1:1e21cd1:           currBlocklet = blockletIterator.next();
1:07a77fa:         } else {
1:07a77fa:           // close all resources when all the results are returned
1:07a77fa:           close();
1:1e21cd1:         }
1:1e21cd1:         return hasNext;
1:1e21cd1:       }
1:1e21cd1: 
1:1e21cd1:       @Override
1:1e21cd1:       public Void getCurrentKey() throws IOException, InterruptedException {
2:1e21cd1:         return null;
1:1e21cd1:       }
1:1e21cd1: 
1:1e21cd1:       @Override
1:28f78b2:       public ExtendedBlocklet getCurrentValue() throws IOException, InterruptedException {
1:1e21cd1:         return currBlocklet;
1:1e21cd1:       }
1:1e21cd1: 
1:1e21cd1:       @Override
1:1e21cd1:       public float getProgress() throws IOException, InterruptedException {
1:1e21cd1:         return 0;
1:1e21cd1:       }
1:1e21cd1: 
1:1e21cd1:       @Override
1:1e21cd1:       public void close() throws IOException {
1:07a77fa:         if (null != dataMaps) {
1:07a77fa:           for (DataMap dataMap : dataMaps) {
1:07a77fa:             dataMap.finish();
1:07a77fa:           }
1:07a77fa:         }
1:1e21cd1:       }
1:1e21cd1:     };
1:1e21cd1:   }
1:1e21cd1: 
1:1e21cd1: }
============================================================================
author:manishgupta88
-------------------------------------------------------------------------------
commit:bd02656
/////////////////////////////////////////////////////////////////////////
1: import org.apache.carbondata.core.datastore.block.SegmentPropertiesAndSchemaHolder;
/////////////////////////////////////////////////////////////////////////
1:           // clear the segment properties cache from executor
1:           SegmentPropertiesAndSchemaHolder.getInstance()
1:               .invalidate(table.getAbsoluteTableIdentifier());
author:sraghunandan
-------------------------------------------------------------------------------
commit:f911403
/////////////////////////////////////////////////////////////////////////
/////////////////////////////////////////////////////////////////////////
/////////////////////////////////////////////////////////////////////////
author:akashrn5
-------------------------------------------------------------------------------
commit:07a77fa
/////////////////////////////////////////////////////////////////////////
1: import org.apache.carbondata.core.datamap.dev.DataMap;
/////////////////////////////////////////////////////////////////////////
1:       private List<DataMap> dataMaps;
/////////////////////////////////////////////////////////////////////////
1:         dataMaps = tableDataMap.getTableDataMaps(distributable.getDistributable());
1:         List<ExtendedBlocklet> blocklets = tableDataMap
1:             .prune(dataMaps,
1:                 distributable.getDistributable(),
1:                 dataMapExprWrapper.getFilterResolverIntf(distributable.getUniqueId()), partitions);
/////////////////////////////////////////////////////////////////////////
1:         } else {
1:           // close all resources when all the results are returned
1:           close();
/////////////////////////////////////////////////////////////////////////
1:         if (null != dataMaps) {
1:           for (DataMap dataMap : dataMaps) {
1:             dataMap.finish();
1:           }
1:         }
commit:2018048
/////////////////////////////////////////////////////////////////////////
1: package org.apache.carbondata.core.datamap;
1: import java.util.Collections;
0: import org.apache.carbondata.core.util.ObjectSerializationUtil;
/////////////////////////////////////////////////////////////////////////
1:   private List<Segment> invalidSegments;
1:   private  DataMapDistributableWrapper distributable;
1: 
1:   private boolean isJobToClearDataMaps = false;
1: 
1:   DistributableDataMapFormat(CarbonTable table, DataMapExprWrapper dataMapExprWrapper,
1:       List<Segment> validSegments, List<Segment> invalidSegments, List<PartitionSpec> partitions,
1:       boolean isJobToClearDataMaps) {
1:     this.invalidSegments = invalidSegments;
1:     this.isJobToClearDataMaps = isJobToClearDataMaps;
1:   }
1: 
0:   public boolean isJobToClearDataMaps() {
0:     return isJobToClearDataMaps;
/////////////////////////////////////////////////////////////////////////
1:         distributable = (DataMapDistributableWrapper) inputSplit;
1:         // clear the segmentMap and from cache in executor when there are invalid segments
1:         if (invalidSegments.size() > 0) {
1:           DataMapStoreManager.getInstance().clearInvalidSegments(table, invalidSegments);
1:         }
1:         TableDataMap tableDataMap = DataMapStoreManager.getInstance()
1:         if (isJobToClearDataMaps) {
1:           // if job is to clear datamaps just clear datamaps from cache and return
1:           DataMapStoreManager.getInstance()
1:               .clearDataMaps(table.getCarbonTableIdentifier().getTableUniqueName());
1:           blockletIterator = Collections.emptyIterator();
1:           return;
1:         }
0:         List<ExtendedBlocklet> blocklets = tableDataMap.prune(distributable.getDistributable(),
author:sounakr
-------------------------------------------------------------------------------
commit:c58eb43
/////////////////////////////////////////////////////////////////////////
/////////////////////////////////////////////////////////////////////////
0:             dataMapExprWrapper.getFilterResolverIntf(distributable.getUniqueId()), partitions);
author:ajantha-bhat
-------------------------------------------------------------------------------
commit:280a400
/////////////////////////////////////////////////////////////////////////
0: import org.apache.carbondata.core.readcommitter.ReadCommittedScope;
0: import org.apache.carbondata.core.readcommitter.TableStatusReadCommittedScope;
/////////////////////////////////////////////////////////////////////////
1:       @Override public void initialize(InputSplit inputSplit, TaskAttemptContext taskAttemptContext)
0:         ReadCommittedScope readCommittedScope =
0:             new TableStatusReadCommittedScope(table.getAbsoluteTableIdentifier());
0:         List<ExtendedBlocklet> blocklets = dataMap.prune(distributable.getDistributable(),
0:             dataMapExprWrapper.getFilterResolverIntf(distributable.getUniqueId()), partitions,
0:             readCommittedScope);
1:         for (ExtendedBlocklet blocklet : blocklets) {
author:Jacky Li
-------------------------------------------------------------------------------
commit:7e0803f
/////////////////////////////////////////////////////////////////////////
1: import org.apache.carbondata.core.metadata.schema.table.CarbonTable;
/////////////////////////////////////////////////////////////////////////
1:   private CarbonTable table;
/////////////////////////////////////////////////////////////////////////
0:   DistributableDataMapFormat(CarbonTable table,
1:     this.table = table;
/////////////////////////////////////////////////////////////////////////
1:             .getDataMap(table, distributable.getDistributable().getDataMapSchema());
author:Ravindra Pesala
-------------------------------------------------------------------------------
commit:56330ae
/////////////////////////////////////////////////////////////////////////
1: import org.apache.carbondata.core.datamap.dev.expr.DataMapDistributableWrapper;
1: import org.apache.carbondata.core.datamap.dev.expr.DataMapExprWrapper;
/////////////////////////////////////////////////////////////////////////
1:   private DataMapExprWrapper dataMapExprWrapper;
/////////////////////////////////////////////////////////////////////////
0:   DistributableDataMapFormat(AbsoluteTableIdentifier identifier,
0:       DataMapExprWrapper dataMapExprWrapper, List<Segment> validSegments,
0:       List<PartitionSpec> partitions, String className) {
1:     this.dataMapExprWrapper = dataMapExprWrapper;
/////////////////////////////////////////////////////////////////////////
1:     List<DataMapDistributableWrapper> distributables =
1:         dataMapExprWrapper.toDistributable(validSegments);
/////////////////////////////////////////////////////////////////////////
0:         DataMapDistributableWrapper distributable = (DataMapDistributableWrapper) inputSplit;
0:             .getDataMap(identifier, distributable.getDistributable().getDataMapSchema());
0:         List<ExtendedBlocklet> blocklets = dataMap.prune(
0:             distributable.getDistributable(),
0:             dataMapExprWrapper.getFilterResolverIntf(distributable.getUniqueId()), partitions);
0:         for (ExtendedBlocklet blocklet: blocklets) {
1:           blocklet.setDataMapUniqueId(distributable.getUniqueId());
1:         }
1:         blockletIterator = blocklets.iterator();
commit:28f78b2
/////////////////////////////////////////////////////////////////////////
1: import org.apache.carbondata.core.indexstore.ExtendedBlocklet;
/////////////////////////////////////////////////////////////////////////
1: public class DistributableDataMapFormat extends FileInputFormat<Void, ExtendedBlocklet> implements
/////////////////////////////////////////////////////////////////////////
1:   public RecordReader<Void, ExtendedBlocklet> createRecordReader(InputSplit inputSplit,
1:     return new RecordReader<Void, ExtendedBlocklet>() {
1:       private Iterator<ExtendedBlocklet> blockletIterator;
1:       private ExtendedBlocklet currBlocklet;
/////////////////////////////////////////////////////////////////////////
1:       public ExtendedBlocklet getCurrentValue() throws IOException, InterruptedException {
commit:1e21cd1
/////////////////////////////////////////////////////////////////////////
1: /*
1:  * Licensed to the Apache Software Foundation (ASF) under one or more
1:  * contributor license agreements.  See the NOTICE file distributed with
1:  * this work for additional information regarding copyright ownership.
1:  * The ASF licenses this file to You under the Apache License, Version 2.0
1:  * (the "License"); you may not use this file except in compliance with
1:  * the License.  You may obtain a copy of the License at
1:  *
1:  *    http://www.apache.org/licenses/LICENSE-2.0
1:  *
1:  * Unless required by applicable law or agreed to in writing, software
1:  * distributed under the License is distributed on an "AS IS" BASIS,
1:  * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
1:  * See the License for the specific language governing permissions and
1:  * limitations under the License.
1:  */
0: package org.apache.carbondata.hadoop.api;
1: 
1: import java.io.IOException;
1: import java.io.Serializable;
1: import java.util.ArrayList;
1: import java.util.Iterator;
1: import java.util.List;
1: 
0: import org.apache.carbondata.core.datamap.DataMapDistributable;
0: import org.apache.carbondata.core.datamap.DataMapStoreManager;
0: import org.apache.carbondata.core.datamap.TableDataMap;
0: import org.apache.carbondata.core.indexstore.Blocklet;
0: import org.apache.carbondata.core.metadata.AbsoluteTableIdentifier;
0: import org.apache.carbondata.core.scan.filter.resolver.FilterResolverIntf;
0: import org.apache.carbondata.hadoop.util.ObjectSerializationUtil;
1: 
0: import org.apache.hadoop.conf.Configuration;
1: import org.apache.hadoop.mapreduce.InputSplit;
1: import org.apache.hadoop.mapreduce.JobContext;
1: import org.apache.hadoop.mapreduce.RecordReader;
1: import org.apache.hadoop.mapreduce.TaskAttemptContext;
1: import org.apache.hadoop.mapreduce.lib.input.FileInputFormat;
1: 
1: /**
1:  * Input format for datamaps, it makes the datamap pruning distributable.
1:  */
0: public class DistributableDataMapFormat extends FileInputFormat<Void, Blocklet> implements
1:     Serializable {
1: 
0:   private static final String FILTER_EXP = "mapreduce.input.distributed.datamap.filter";
1: 
0:   private AbsoluteTableIdentifier identifier;
1: 
0:   private String dataMapName;
1: 
0:   private List<String> validSegments;
1: 
0:   private String className;
1: 
0:   public DistributableDataMapFormat(AbsoluteTableIdentifier identifier,
0:       String dataMapName, List<String> validSegments, String className) {
0:     this.identifier = identifier;
0:     this.dataMapName = dataMapName;
1:     this.validSegments = validSegments;
0:     this.className = className;
1:   }
1: 
0:   public static void setFilterExp(Configuration configuration, FilterResolverIntf filterExp)
0:       throws IOException {
0:     if (filterExp != null) {
0:       String string = ObjectSerializationUtil.convertObjectToString(filterExp);
0:       configuration.set(FILTER_EXP, string);
1:     }
1:   }
1: 
0:   private static FilterResolverIntf getFilterExp(Configuration configuration) throws IOException {
0:     String filterString = configuration.get(FILTER_EXP);
0:     if (filterString != null) {
0:       Object toObject = ObjectSerializationUtil.convertStringToObject(filterString);
0:       return (FilterResolverIntf) toObject;
1:     }
1:     return null;
1:   }
1: 
1:   @Override
1:   public List<InputSplit> getSplits(JobContext job) throws IOException {
0:     TableDataMap dataMap =
0:         DataMapStoreManager.getInstance().getDataMap(identifier, dataMapName, className);
0:     List<DataMapDistributable> distributables = dataMap.toDistributable(validSegments);
1:     List<InputSplit> inputSplits = new ArrayList<>(distributables.size());
1:     inputSplits.addAll(distributables);
1:     return inputSplits;
1:   }
1: 
1:   @Override
0:   public RecordReader<Void, Blocklet> createRecordReader(InputSplit inputSplit,
1:       TaskAttemptContext taskAttemptContext) throws IOException, InterruptedException {
0:     return new RecordReader<Void, Blocklet>() {
0:       private Iterator<Blocklet> blockletIterator;
0:       private Blocklet currBlocklet;
1: 
1:       @Override
0:       public void initialize(InputSplit inputSplit, TaskAttemptContext taskAttemptContext)
1:           throws IOException, InterruptedException {
0:         DataMapDistributable distributable = (DataMapDistributable)inputSplit;
0:         AbsoluteTableIdentifier identifier =
0:             AbsoluteTableIdentifier.fromTablePath(distributable.getTablePath());
0:         TableDataMap dataMap = DataMapStoreManager.getInstance()
0:             .getDataMap(identifier, distributable.getDataMapName(),
0:                 distributable.getDataMapFactoryClass());
0:         blockletIterator =
0:             dataMap.prune(distributable, getFilterExp(taskAttemptContext.getConfiguration()))
0:                 .iterator();
1:       }
1: 
1:       @Override
1:       public boolean nextKeyValue() throws IOException, InterruptedException {
1:         boolean hasNext = blockletIterator.hasNext();
1:         if (hasNext) {
1:           currBlocklet = blockletIterator.next();
1:         }
1:         return hasNext;
1:       }
1: 
1:       @Override
1:       public Void getCurrentKey() throws IOException, InterruptedException {
1:         return null;
1:       }
1: 
1:       @Override
0:       public Blocklet getCurrentValue() throws IOException, InterruptedException {
1:         return currBlocklet;
1:       }
1: 
1:       @Override
1:       public float getProgress() throws IOException, InterruptedException {
1:         return 0;
1:       }
1: 
1:       @Override
1:       public void close() throws IOException {
1: 
1:       }
1:     };
1:   }
1: 
1: }
author:ravipesala
-------------------------------------------------------------------------------
commit:8d3c774
/////////////////////////////////////////////////////////////////////////
0: import org.apache.carbondata.core.datamap.Segment;
1: import org.apache.carbondata.core.indexstore.PartitionSpec;
/////////////////////////////////////////////////////////////////////////
1:   private List<Segment> validSegments;
1:   private List<PartitionSpec> partitions;
0:   public DistributableDataMapFormat(AbsoluteTableIdentifier identifier, String dataMapName,
0:       List<Segment> validSegments, List<PartitionSpec> partitions, String className) {
commit:b8a02f3
/////////////////////////////////////////////////////////////////////////
0:   private List<String> partitions;
0: 
0:       String dataMapName, List<String> validSegments, List<String> partitions, String className) {
1:     this.partitions = partitions;
/////////////////////////////////////////////////////////////////////////
0:         blockletIterator = dataMap.prune(
0:             distributable, getFilterExp(taskAttemptContext.getConfiguration()), partitions)
0:             .iterator();
author:mohammadshahidkhan
-------------------------------------------------------------------------------
commit:1155d4d
/////////////////////////////////////////////////////////////////////////
============================================================================