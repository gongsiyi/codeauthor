1:68cafe5: /*
1:68cafe5:  * Licensed to the Apache Software Foundation (ASF) under one or more
1:68cafe5:  * contributor license agreements.  See the NOTICE file distributed with
1:68cafe5:  * this work for additional information regarding copyright ownership.
1:68cafe5:  * The ASF licenses this file to You under the Apache License, Version 2.0
1:68cafe5:  * (the "License"); you may not use this file except in compliance with
1:68cafe5:  * the License.  You may obtain a copy of the License at
1:68cafe5:  *
1:68cafe5:  *    http://www.apache.org/licenses/LICENSE-2.0
1:68cafe5:  *
1:68cafe5:  * Unless required by applicable law or agreed to in writing, software
1:68cafe5:  * distributed under the License is distributed on an "AS IS" BASIS,
1:68cafe5:  * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
1:68cafe5:  * See the License for the specific language governing permissions and
1:68cafe5:  * limitations under the License.
1:68cafe5:  */
1:68cafe5: 
1:68cafe5: package org.apache.carbondata.hive.server;
1:68cafe5: 
1:68cafe5: import java.io.File;
1:68cafe5: import java.lang.reflect.Field;
1:2d24e18: import java.security.SecureRandom;
1:68cafe5: import java.util.HashMap;
1:68cafe5: import java.util.Map;
1:68cafe5: import java.util.Properties;
1:68cafe5: import java.util.Random;
1:68cafe5: import java.util.concurrent.TimeUnit;
1:68cafe5: import java.util.concurrent.TimeoutException;
1:68cafe5: 
1:68cafe5: import org.apache.commons.logging.Log;
1:68cafe5: import org.apache.commons.logging.LogFactory;
1:68cafe5: import org.apache.hadoop.conf.Configuration;
1:68cafe5: import org.apache.hadoop.hive.conf.HiveConf;
1:68cafe5: import org.apache.hadoop.hive.conf.HiveConf.ConfVars;
1:68cafe5: import org.apache.hadoop.hive.metastore.MetaStoreUtils;
1:68cafe5: import org.apache.hadoop.hive.ql.metadata.Hive;
1:68cafe5: import org.apache.hadoop.hive.ql.session.SessionState;
1:68cafe5: import org.apache.hive.service.Service;
1:68cafe5: import org.apache.hive.service.cli.CLIService;
1:68cafe5: import org.apache.hive.service.cli.SessionHandle;
1:68cafe5: import org.apache.hive.service.server.HiveServer2;
1:68cafe5: 
1:68cafe5: /**
1:68cafe5:  * Utility starting a local/embedded Hive org.apache.carbondata.hive.server for testing purposes.
1:68cafe5:  * Uses sensible defaults to properly clean between reruns.
1:53267c8:  * Additionally it wrangles the Hive internals so it rather executes the jobs locally not within
1:53267c8:  * a child JVM (which Hive calls local) or external.
1:68cafe5:  */
1:68cafe5: public class HiveEmbeddedServer2 {
1:68cafe5:   private static final String SCRATCH_DIR = "/tmp/hive";
1:d408a8d:   private static final Log log = LogFactory.getLog(Hive.class);
1:68cafe5:   private HiveServer2 hiveServer;
1:68cafe5:   private HiveConf config;
1:68cafe5:   private int port;
1:2d24e18:   private static Random secureRandom = new SecureRandom();
1:68cafe5: 
1:68cafe5:   public void start() throws Exception {
1:68cafe5:     log.info("Starting Hive Local/Embedded Server...");
1:68cafe5:     if (hiveServer == null) {
1:68cafe5:       config = configure();
1:68cafe5:       hiveServer = new HiveServer2();
1:68cafe5:       port = MetaStoreUtils.findFreePort();
1:68cafe5:       config.setIntVar(ConfVars.HIVE_SERVER2_THRIFT_PORT, port);
1:68cafe5:       hiveServer.init(config);
1:68cafe5:       hiveServer.start();
1:68cafe5:       waitForStartup();
1:68cafe5:     }
1:68cafe5:   }
1:68cafe5: 
1:68cafe5:   public int getFreePort() {
1:68cafe5:     log.info("Free Port Available is " + port);
1:68cafe5:     return port;
1:68cafe5:   }
1:68cafe5: 
1:68cafe5:   private void waitForStartup() throws Exception {
1:68cafe5:     long timeout = TimeUnit.MINUTES.toMillis(1);
1:68cafe5:     long unitOfWait = TimeUnit.SECONDS.toMillis(1);
1:68cafe5: 
1:68cafe5:     CLIService hs2Client = getServiceClientInternal();
1:68cafe5:     SessionHandle sessionHandle = null;
1:68cafe5:     for (int interval = 0; interval < timeout / unitOfWait; interval++) {
1:68cafe5:       Thread.sleep(unitOfWait);
1:68cafe5:       try {
1:68cafe5:         Map<String, String> sessionConf = new HashMap<String, String>();
1:68cafe5:         sessionHandle = hs2Client.openSession("foo", "bar", sessionConf);
1:68cafe5:         return;
1:68cafe5:       } catch (Exception e) {
1:68cafe5:         // service not started yet
1:68cafe5:       } finally {
1:68cafe5:         hs2Client.closeSession(sessionHandle);
1:68cafe5:       }
1:68cafe5:     }
1:68cafe5:     throw new TimeoutException("Couldn't get a hold of HiveServer2...");
1:68cafe5:   }
1:68cafe5: 
1:68cafe5:   private CLIService getServiceClientInternal() {
1:68cafe5:     for (Service service : hiveServer.getServices()) {
1:68cafe5:       if (service instanceof CLIService) {
1:68cafe5:         return (CLIService) service;
1:68cafe5:       }
1:68cafe5:     }
1:68cafe5:     throw new IllegalStateException("Cannot find CLIService");
1:68cafe5:   }
1:68cafe5: 
1:68cafe5:   private HiveConf configure() throws Exception {
1:68cafe5:     log.info("Setting The Hive Conf Variables");
1:68cafe5:     String scratchDir = SCRATCH_DIR;
1:68cafe5: 
1:68cafe5:     File scratchDirFile = new File(scratchDir);
1:68cafe5:     //TestUtils.delete(scratchDirFile);
1:68cafe5: 
1:68cafe5:     Configuration cfg = new Configuration();
1:68cafe5:     HiveConf conf = new HiveConf(cfg, HiveConf.class);
1:68cafe5:     conf.addToRestrictList("columns.comments");
1:68cafe5:     conf.set("hive.scratch.dir.permission", "777");
1:68cafe5:     conf.setVar(ConfVars.SCRATCHDIRPERMISSION, "777");
1:2d24e18:     if (!scratchDirFile.exists()) {
1:2d24e18:       if (!scratchDirFile.mkdirs()) {
1:2d24e18:         throw new IllegalArgumentException("could not create the directory:" + scratchDir);
1:2d24e18:       }
1:2d24e18:       // also set the permissions manually since Hive doesn't do it...
1:2d24e18:       if (!scratchDirFile.setWritable(true, false)) {
1:2d24e18:         throw new IllegalArgumentException("could not set write permissions for the directory:" +
1:2d24e18:             scratchDir);
1:2d24e18:       }
1:2d24e18:     }
1:68cafe5: 
1:2d24e18:     int random = secureRandom.nextInt();
1:68cafe5: 
1:68cafe5:     conf.set("hive.metastore.warehouse.dir", scratchDir + "/warehouse" + random);
1:68cafe5:     conf.set("hive.metastore.metadb.dir", scratchDir + "/metastore_db" + random);
1:68cafe5:     conf.set("hive.exec.scratchdir", scratchDir);
1:68cafe5:     conf.set("fs.permissions.umask-mode", "022");
1:68cafe5:     conf.set("javax.jdo.option.ConnectionURL",
1:68cafe5:         "jdbc:derby:;databaseName=" + scratchDir + "/metastore_db" + random + ";create=true");
1:68cafe5:     conf.set("hive.metastore.local", "true");
1:68cafe5:     conf.set("hive.aux.jars.path", "");
1:68cafe5:     conf.set("hive.added.jars.path", "");
1:68cafe5:     conf.set("hive.added.files.path", "");
1:68cafe5:     conf.set("hive.added.archives.path", "");
1:68cafe5:     conf.set("fs.default.name", "file:///");
1:cbe1419:     conf.set(HiveConf.ConfVars.SUBMITLOCALTASKVIACHILD.varname, "false");
1:68cafe5: 
1:53267c8:     // clear mapred.job.tracker - Hadoop defaults to 'local' if not defined. Hive however expects
1:53267c8:     // this to be set to 'local' - if it's not, it does a remote execution (i.e. no child JVM)
1:68cafe5:     Field field = Configuration.class.getDeclaredField("properties");
1:68cafe5:     field.setAccessible(true);
1:68cafe5:     Properties props = (Properties) field.get(conf);
1:68cafe5:     props.remove("mapred.job.tracker");
1:68cafe5:     props.remove("mapreduce.framework.name");
1:68cafe5:     props.setProperty("fs.default.name", "file:///");
1:68cafe5: 
1:68cafe5:     // intercept SessionState to clean the threadlocal
1:68cafe5:     Field tss = SessionState.class.getDeclaredField("tss");
1:68cafe5:     tss.setAccessible(true);
1:68cafe5:     return new HiveConf(conf);
1:68cafe5:   }
1:68cafe5: 
1:68cafe5:   public void stop() {
1:68cafe5:     if (hiveServer != null) {
1:68cafe5:       log.info("Stopping Hive Local/Embedded Server...");
1:68cafe5:       hiveServer.stop();
1:68cafe5:       hiveServer = null;
1:68cafe5:       config = null;
1:68cafe5:       log.info("Hive Local/Embedded Server Stopped SucessFully...");
1:68cafe5: 
1:68cafe5:     }
1:68cafe5:   }
1:68cafe5: 
1:68cafe5: }
============================================================================
author:Raghunandan S
-------------------------------------------------------------------------------
commit:2d24e18
/////////////////////////////////////////////////////////////////////////
1: import java.security.SecureRandom;
/////////////////////////////////////////////////////////////////////////
1:   private static Random secureRandom = new SecureRandom();
/////////////////////////////////////////////////////////////////////////
1:     if (!scratchDirFile.exists()) {
1:       if (!scratchDirFile.mkdirs()) {
1:         throw new IllegalArgumentException("could not create the directory:" + scratchDir);
1:       }
1:       // also set the permissions manually since Hive doesn't do it...
1:       if (!scratchDirFile.setWritable(true, false)) {
1:         throw new IllegalArgumentException("could not set write permissions for the directory:" +
1:             scratchDir);
1:       }
1:     }
1:     int random = secureRandom.nextInt();
author:Bhavya
-------------------------------------------------------------------------------
commit:cbe1419
/////////////////////////////////////////////////////////////////////////
1:     conf.set(HiveConf.ConfVars.SUBMITLOCALTASKVIACHILD.varname, "false");
author:chenliang613
-------------------------------------------------------------------------------
commit:d408a8d
/////////////////////////////////////////////////////////////////////////
1:   private static final Log log = LogFactory.getLog(Hive.class);
/////////////////////////////////////////////////////////////////////////
commit:53267c8
/////////////////////////////////////////////////////////////////////////
1:  * Additionally it wrangles the Hive internals so it rather executes the jobs locally not within
1:  * a child JVM (which Hive calls local) or external.
/////////////////////////////////////////////////////////////////////////
1:     // clear mapred.job.tracker - Hadoop defaults to 'local' if not defined. Hive however expects
1:     // this to be set to 'local' - if it's not, it does a remote execution (i.e. no child JVM)
author:anubhav100
-------------------------------------------------------------------------------
commit:68cafe5
/////////////////////////////////////////////////////////////////////////
1: /*
1:  * Licensed to the Apache Software Foundation (ASF) under one or more
1:  * contributor license agreements.  See the NOTICE file distributed with
1:  * this work for additional information regarding copyright ownership.
1:  * The ASF licenses this file to You under the Apache License, Version 2.0
1:  * (the "License"); you may not use this file except in compliance with
1:  * the License.  You may obtain a copy of the License at
1:  *
1:  *    http://www.apache.org/licenses/LICENSE-2.0
1:  *
1:  * Unless required by applicable law or agreed to in writing, software
1:  * distributed under the License is distributed on an "AS IS" BASIS,
1:  * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
1:  * See the License for the specific language governing permissions and
1:  * limitations under the License.
1:  */
1: 
1: package org.apache.carbondata.hive.server;
1: 
1: import java.io.File;
1: import java.lang.reflect.Field;
1: import java.util.HashMap;
1: import java.util.Map;
1: import java.util.Properties;
1: import java.util.Random;
1: import java.util.concurrent.TimeUnit;
1: import java.util.concurrent.TimeoutException;
1: 
1: import org.apache.commons.logging.Log;
1: import org.apache.commons.logging.LogFactory;
1: import org.apache.hadoop.conf.Configuration;
1: import org.apache.hadoop.hive.conf.HiveConf;
1: import org.apache.hadoop.hive.conf.HiveConf.ConfVars;
1: import org.apache.hadoop.hive.metastore.MetaStoreUtils;
1: import org.apache.hadoop.hive.ql.metadata.Hive;
1: import org.apache.hadoop.hive.ql.session.SessionState;
1: import org.apache.hive.service.Service;
1: import org.apache.hive.service.cli.CLIService;
1: import org.apache.hive.service.cli.SessionHandle;
1: import org.apache.hive.service.server.HiveServer2;
1: 
1: /**
1:  * Utility starting a local/embedded Hive org.apache.carbondata.hive.server for testing purposes.
1:  * Uses sensible defaults to properly clean between reruns.
0:  * Additionally it wrangles the Hive internals so it rather executes the jobs locally not within a child JVM (which Hive calls local) or external.
1:  */
1: public class HiveEmbeddedServer2 {
1:   private static final String SCRATCH_DIR = "/tmp/hive";
0:   private static Log log = LogFactory.getLog(Hive.class);
1:   private HiveServer2 hiveServer;
1:   private HiveConf config;
1:   private int port;
1: 
1:   public void start() throws Exception {
1:     log.info("Starting Hive Local/Embedded Server...");
1:     if (hiveServer == null) {
1:       config = configure();
1:       hiveServer = new HiveServer2();
1:       port = MetaStoreUtils.findFreePort();
1:       config.setIntVar(ConfVars.HIVE_SERVER2_THRIFT_PORT, port);
1:       hiveServer.init(config);
1:       hiveServer.start();
1:       waitForStartup();
1:     }
1:   }
1: 
1:   public int getFreePort() {
1:     log.info("Free Port Available is " + port);
1:     return port;
1:   }
1: 
1:   private void waitForStartup() throws Exception {
1:     long timeout = TimeUnit.MINUTES.toMillis(1);
1:     long unitOfWait = TimeUnit.SECONDS.toMillis(1);
1: 
1:     CLIService hs2Client = getServiceClientInternal();
1:     SessionHandle sessionHandle = null;
1:     for (int interval = 0; interval < timeout / unitOfWait; interval++) {
1:       Thread.sleep(unitOfWait);
1:       try {
1:         Map<String, String> sessionConf = new HashMap<String, String>();
1:         sessionHandle = hs2Client.openSession("foo", "bar", sessionConf);
1:         return;
1:       } catch (Exception e) {
1:         // service not started yet
0:         continue;
1:       } finally {
1:         hs2Client.closeSession(sessionHandle);
1:       }
1:     }
1:     throw new TimeoutException("Couldn't get a hold of HiveServer2...");
1:   }
1: 
1:   private CLIService getServiceClientInternal() {
1:     for (Service service : hiveServer.getServices()) {
1:       if (service instanceof CLIService) {
1:         return (CLIService) service;
1:       }
1:     }
1:     throw new IllegalStateException("Cannot find CLIService");
1:   }
1: 
1:   private HiveConf configure() throws Exception {
1:     log.info("Setting The Hive Conf Variables");
1:     String scratchDir = SCRATCH_DIR;
1: 
1:     File scratchDirFile = new File(scratchDir);
1:     //TestUtils.delete(scratchDirFile);
1: 
1:     Configuration cfg = new Configuration();
1:     HiveConf conf = new HiveConf(cfg, HiveConf.class);
1:     conf.addToRestrictList("columns.comments");
1:     conf.set("hive.scratch.dir.permission", "777");
1:     conf.setVar(ConfVars.SCRATCHDIRPERMISSION, "777");
0:     scratchDirFile.mkdirs();
0:     // also set the permissions manually since Hive doesn't do it...
0:     scratchDirFile.setWritable(true, false);
1: 
0:     int random = new Random().nextInt();
1: 
1:     conf.set("hive.metastore.warehouse.dir", scratchDir + "/warehouse" + random);
1:     conf.set("hive.metastore.metadb.dir", scratchDir + "/metastore_db" + random);
1:     conf.set("hive.exec.scratchdir", scratchDir);
1:     conf.set("fs.permissions.umask-mode", "022");
1:     conf.set("javax.jdo.option.ConnectionURL",
1:         "jdbc:derby:;databaseName=" + scratchDir + "/metastore_db" + random + ";create=true");
1:     conf.set("hive.metastore.local", "true");
1:     conf.set("hive.aux.jars.path", "");
1:     conf.set("hive.added.jars.path", "");
1:     conf.set("hive.added.files.path", "");
1:     conf.set("hive.added.archives.path", "");
1:     conf.set("fs.default.name", "file:///");
1: 
0:     // clear mapred.job.tracker - Hadoop defaults to 'local' if not defined. Hive however expects this to be set to 'local' - if it's not, it does a remote execution (i.e. no child JVM)
1:     Field field = Configuration.class.getDeclaredField("properties");
1:     field.setAccessible(true);
1:     Properties props = (Properties) field.get(conf);
1:     props.remove("mapred.job.tracker");
1:     props.remove("mapreduce.framework.name");
1:     props.setProperty("fs.default.name", "file:///");
1: 
1:     // intercept SessionState to clean the threadlocal
1:     Field tss = SessionState.class.getDeclaredField("tss");
1:     tss.setAccessible(true);
1:     return new HiveConf(conf);
1:   }
1: 
1:   public void stop() {
1:     if (hiveServer != null) {
1:       log.info("Stopping Hive Local/Embedded Server...");
1:       hiveServer.stop();
1:       hiveServer = null;
1:       config = null;
1:       log.info("Hive Local/Embedded Server Stopped SucessFully...");
1: 
1:     }
1:   }
1: 
1: }
============================================================================