1:531ecdf: /*
1:531ecdf:  * Licensed to the Apache Software Foundation (ASF) under one or more
1:531ecdf:  * contributor license agreements.  See the NOTICE file distributed with
1:531ecdf:  * this work for additional information regarding copyright ownership.
1:531ecdf:  * The ASF licenses this file to You under the Apache License, Version 2.0
1:531ecdf:  * (the "License"); you may not use this file except in compliance with
1:531ecdf:  * the License.  You may obtain a copy of the License at
2:531ecdf:  *
1:531ecdf:  *    http://www.apache.org/licenses/LICENSE-2.0
1:531ecdf:  *
1:531ecdf:  * Unless required by applicable law or agreed to in writing, software
1:531ecdf:  * distributed under the License is distributed on an "AS IS" BASIS,
1:531ecdf:  * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
1:531ecdf:  * See the License for the specific language governing permissions and
1:531ecdf:  * limitations under the License.
1:531ecdf:  */
1:531ecdf: 
1:531ecdf: package org.apache.carbondata.core.util;
1:531ecdf: 
1:f4a58c5: import java.io.ByteArrayInputStream;
1:f4a58c5: import java.io.ByteArrayOutputStream;
1:f4a58c5: import java.io.DataInput;
1:f4a58c5: import java.io.DataInputStream;
1:f4a58c5: import java.io.DataOutput;
1:f4a58c5: import java.io.DataOutputStream;
1:531ecdf: import java.io.IOException;
1:6118711: import java.math.BigDecimal;
1:6118711: import java.nio.ByteBuffer;
1:531ecdf: import java.util.ArrayList;
1:531ecdf: import java.util.HashMap;
1:531ecdf: import java.util.HashSet;
1:531ecdf: import java.util.List;
1:531ecdf: import java.util.Map;
1:531ecdf: import java.util.Set;
1:531ecdf: import java.util.TreeMap;
1:531ecdf: 
1:531ecdf: import org.apache.carbondata.core.constants.CarbonCommonConstants;
1:531ecdf: import org.apache.carbondata.core.datamap.Segment;
1:6118711: import org.apache.carbondata.core.datastore.block.SegmentProperties;
1:f4a58c5: import org.apache.carbondata.core.datastore.compression.CompressorFactory;
1:531ecdf: import org.apache.carbondata.core.datastore.filesystem.AbstractDFSCarbonFile;
1:531ecdf: import org.apache.carbondata.core.datastore.filesystem.CarbonFile;
1:531ecdf: import org.apache.carbondata.core.datastore.impl.FileFactory;
1:531ecdf: import org.apache.carbondata.core.indexstore.BlockMetaInfo;
1:531ecdf: import org.apache.carbondata.core.indexstore.TableBlockIndexUniqueIdentifier;
1:5f68a79: import org.apache.carbondata.core.indexstore.TableBlockIndexUniqueIdentifierWrapper;
1:531ecdf: import org.apache.carbondata.core.indexstore.blockletindex.BlockletDataMapDistributable;
1:3cbabcd: import org.apache.carbondata.core.indexstore.blockletindex.BlockletDataMapFactory;
1:531ecdf: import org.apache.carbondata.core.indexstore.blockletindex.SegmentIndexFileStore;
1:531ecdf: import org.apache.carbondata.core.metadata.blocklet.DataFileFooter;
1:6118711: import org.apache.carbondata.core.metadata.datatype.DataType;
1:6118711: import org.apache.carbondata.core.metadata.datatype.DataTypes;
1:5f68a79: import org.apache.carbondata.core.metadata.schema.table.CarbonTable;
1:dc29319: import org.apache.carbondata.core.metadata.schema.table.column.CarbonColumn;
1:4df335f: import org.apache.carbondata.core.metadata.schema.table.column.CarbonDimension;
1:6118711: import org.apache.carbondata.core.metadata.schema.table.column.CarbonMeasure;
1:5f68a79: import org.apache.carbondata.core.metadata.schema.table.column.ColumnSchema;
1:4df335f: import org.apache.carbondata.core.scan.executor.util.QueryUtil;
1:4df335f: import org.apache.carbondata.core.scan.filter.resolver.FilterResolverIntf;
1:531ecdf: import org.apache.carbondata.core.util.path.CarbonTablePath;
1:531ecdf: 
1:5f68a79: import org.apache.commons.logging.Log;
1:5f68a79: import org.apache.commons.logging.LogFactory;
1:8f1a029: import org.apache.hadoop.conf.Configuration;
1:531ecdf: import org.apache.hadoop.fs.Path;
1:531ecdf: import org.apache.hadoop.fs.PathFilter;
1:531ecdf: 
1:531ecdf: public class BlockletDataMapUtil {
1:531ecdf: 
1:5f68a79:   private static final Log LOG = LogFactory.getLog(BlockletDataMapUtil.class);
1:5f68a79: 
1:531ecdf:   public static Map<String, BlockMetaInfo> getBlockMetaInfoMap(
1:5f68a79:       TableBlockIndexUniqueIdentifierWrapper identifierWrapper,
1:5f68a79:       SegmentIndexFileStore indexFileStore, Set<String> filesRead,
1:5f68a79:       Map<String, BlockMetaInfo> fileNameToMetaInfoMapping) throws IOException {
1:5f68a79:     boolean isTransactionalTable = true;
1:5f68a79:     TableBlockIndexUniqueIdentifier identifier =
1:5f68a79:         identifierWrapper.getTableBlockIndexUniqueIdentifier();
1:5f68a79:     List<ColumnSchema> tableColumnList = null;
1:531ecdf:     if (identifier.getMergeIndexFileName() != null
1:531ecdf:         && indexFileStore.getFileData(identifier.getIndexFileName()) == null) {
1:531ecdf:       CarbonFile indexMergeFile = FileFactory.getCarbonFile(
2:531ecdf:           identifier.getIndexFilePath() + CarbonCommonConstants.FILE_SEPARATOR + identifier
1:8f1a029:               .getMergeIndexFileName(), identifierWrapper.getConfiguration());
1:531ecdf:       if (indexMergeFile.exists() && !filesRead.contains(indexMergeFile.getPath())) {
1:531ecdf:         indexFileStore.readAllIIndexOfSegment(new CarbonFile[] { indexMergeFile });
1:531ecdf:         filesRead.add(indexMergeFile.getPath());
1:531ecdf:       }
1:531ecdf:     }
1:531ecdf:     if (indexFileStore.getFileData(identifier.getIndexFileName()) == null) {
1:531ecdf:       indexFileStore.readAllIIndexOfSegment(new CarbonFile[] { FileFactory.getCarbonFile(
1:531ecdf:           identifier.getIndexFilePath() + CarbonCommonConstants.FILE_SEPARATOR + identifier
1:8f1a029:               .getIndexFileName(), identifierWrapper.getConfiguration()) });
1:531ecdf:     }
1:531ecdf:     Map<String, BlockMetaInfo> blockMetaInfoMap = new HashMap<>();
1:5f68a79:     CarbonTable carbonTable = identifierWrapper.getCarbonTable();
1:5f68a79:     if (carbonTable != null) {
1:5f68a79:       isTransactionalTable = carbonTable.getTableInfo().isTransactionalTable();
1:5f68a79:       tableColumnList =
1:5f68a79:           carbonTable.getTableInfo().getFactTable().getListOfColumns();
1:5f68a79:     }
1:8f1a029:     DataFileFooterConverter fileFooterConverter =
1:8f1a029:         new DataFileFooterConverter(identifierWrapper.getConfiguration());
1:3894e1d:     List<DataFileFooter> indexInfo = fileFooterConverter.getIndexInfo(
1:3894e1d:         identifier.getIndexFilePath() + CarbonCommonConstants.FILE_SEPARATOR + identifier
1:3894e1d:             .getIndexFileName(), indexFileStore.getFileData(identifier.getIndexFileName()),
1:3894e1d:         isTransactionalTable);
1:531ecdf:     for (DataFileFooter footer : indexInfo) {
1:5f68a79:       if ((!isTransactionalTable) && (tableColumnList.size() != 0) &&
1:5f68a79:           !isSameColumnSchemaList(footer.getColumnInTable(), tableColumnList)) {
1:5f68a79:         LOG.error("Schema of " + identifier.getIndexFileName()
1:5f68a79:             + " doesn't match with the table's schema");
1:5f68a79:         throw new IOException("All the files doesn't have same schema. "
1:5f68a79:             + "Unsupported operation on nonTransactional table. Check logs.");
1:5f68a79:       }
1:5f68a79:       if ((tableColumnList != null) && (tableColumnList.size() == 0)) {
1:5f68a79:         // Carbon reader have used dummy columnSchema. Update it with inferred schema now
1:5f68a79:         carbonTable.getTableInfo().getFactTable().setListOfColumns(footer.getColumnInTable());
1:5f68a79:         CarbonTable.updateTableByTableInfo(carbonTable, carbonTable.getTableInfo());
1:5f68a79:       }
1:531ecdf:       String blockPath = footer.getBlockInfo().getTableBlockInfo().getFilePath();
1:7158d52:       if (null == blockMetaInfoMap.get(blockPath)) {
1:fd747a3:         BlockMetaInfo blockMetaInfo = createBlockMetaInfo(fileNameToMetaInfoMapping, blockPath);
1:fd747a3:         // if blockMetaInfo is null that means the file has been deleted from the file system.
1:fd747a3:         // This can happen in case IUD scenarios where after deleting or updating the data the
1:fd747a3:         // complete block is deleted but the entry still exists in index or merge index file
1:fd747a3:         if (null != blockMetaInfo) {
1:fd747a3:           blockMetaInfoMap.put(blockPath, blockMetaInfo);
1:fd747a3:         }
1:531ecdf:       }
1:531ecdf:     }
1:531ecdf:     return blockMetaInfoMap;
1:531ecdf:   }
1:531ecdf: 
1:531ecdf:   /**
1:531ecdf:    * This method will create file name to block Meta Info Mapping. This method will reduce the
1:531ecdf:    * number of namenode calls and using this method one namenode will fetch 1000 entries
1:531ecdf:    *
1:531ecdf:    * @param segmentFilePath
2:531ecdf:    * @return
1:531ecdf:    * @throws IOException
1:531ecdf:    */
1:531ecdf:   public static Map<String, BlockMetaInfo> createCarbonDataFileBlockMetaInfoMapping(
1:8f1a029:       String segmentFilePath, Configuration configuration) throws IOException {
1:531ecdf:     Map<String, BlockMetaInfo> fileNameToMetaInfoMapping = new TreeMap();
1:8f1a029:     CarbonFile carbonFile = FileFactory.getCarbonFile(segmentFilePath, configuration);
1:531ecdf:     if (carbonFile instanceof AbstractDFSCarbonFile) {
1:531ecdf:       PathFilter pathFilter = new PathFilter() {
1:531ecdf:         @Override public boolean accept(Path path) {
1:531ecdf:           return CarbonTablePath.isCarbonDataFile(path.getName());
1:531ecdf:         }
1:531ecdf:       };
1:531ecdf:       CarbonFile[] carbonFiles = carbonFile.locationAwareListFiles(pathFilter);
1:531ecdf:       for (CarbonFile file : carbonFiles) {
1:531ecdf:         String[] location = file.getLocations();
1:531ecdf:         long len = file.getSize();
1:531ecdf:         BlockMetaInfo blockMetaInfo = new BlockMetaInfo(location, len);
1:e580d64:         fileNameToMetaInfoMapping.put(file.getPath(), blockMetaInfo);
1:531ecdf:       }
1:531ecdf:     }
1:531ecdf:     return fileNameToMetaInfoMapping;
1:531ecdf:   }
1:531ecdf: 
1:531ecdf:   private static BlockMetaInfo createBlockMetaInfo(
1:fd747a3:       Map<String, BlockMetaInfo> fileNameToMetaInfoMapping, String carbonDataFile)
1:fd747a3:       throws IOException {
1:531ecdf:     FileFactory.FileType fileType = FileFactory.getFileType(carbonDataFile);
1:531ecdf:     switch (fileType) {
1:531ecdf:       case LOCAL:
1:fd747a3:         if (!FileFactory.isFileExist(carbonDataFile)) {
1:fd747a3:           return null;
1:fd747a3:         }
1:531ecdf:         CarbonFile carbonFile = FileFactory.getCarbonFile(carbonDataFile, fileType);
1:531ecdf:         return new BlockMetaInfo(new String[] { "localhost" }, carbonFile.getSize());
1:531ecdf:       default:
1:531ecdf:         return fileNameToMetaInfoMapping.get(carbonDataFile);
1:531ecdf:     }
1:531ecdf:   }
1:531ecdf: 
1:531ecdf:   public static Set<TableBlockIndexUniqueIdentifier> getTableBlockUniqueIdentifiers(Segment segment)
2:531ecdf:       throws IOException {
1:531ecdf:     Set<TableBlockIndexUniqueIdentifier> tableBlockIndexUniqueIdentifiers = new HashSet<>();
1:531ecdf:     Map<String, String> indexFiles = segment.getCommittedIndexFile();
1:531ecdf:     for (Map.Entry<String, String> indexFileEntry : indexFiles.entrySet()) {
1:531ecdf:       Path indexFile = new Path(indexFileEntry.getKey());
1:531ecdf:       tableBlockIndexUniqueIdentifiers.add(
1:531ecdf:           new TableBlockIndexUniqueIdentifier(indexFile.getParent().toString(), indexFile.getName(),
1:531ecdf:               indexFileEntry.getValue(), segment.getSegmentNo()));
1:531ecdf:     }
1:531ecdf:     return tableBlockIndexUniqueIdentifiers;
1:531ecdf:   }
1:531ecdf: 
1:6118711:   /**
1:531ecdf:    * This method will filter out the TableBlockIndexUniqueIdentifier belongs to that distributable
1:6118711:    *
1:531ecdf:    * @param tableBlockIndexUniqueIdentifiers
1:531ecdf:    * @param distributable
1:6118711:    * @return
1:531ecdf:    */
1:531ecdf:   public static TableBlockIndexUniqueIdentifier filterIdentifiersBasedOnDistributable(
1:531ecdf:       Set<TableBlockIndexUniqueIdentifier> tableBlockIndexUniqueIdentifiers,
1:531ecdf:       BlockletDataMapDistributable distributable) {
1:531ecdf:     TableBlockIndexUniqueIdentifier validIdentifier = null;
1:531ecdf:     String fileName = CarbonTablePath.DataFileUtil.getFileName(distributable.getFilePath());
1:531ecdf:     for (TableBlockIndexUniqueIdentifier tableBlockIndexUniqueIdentifier :
1:531ecdf:         tableBlockIndexUniqueIdentifiers) {
1:531ecdf:       if (fileName.equals(tableBlockIndexUniqueIdentifier.getIndexFileName())) {
1:531ecdf:         validIdentifier = tableBlockIndexUniqueIdentifier;
1:531ecdf:         break;
1:531ecdf:       }
1:531ecdf:     }
1:531ecdf:     return validIdentifier;
1:531ecdf:   }
1:531ecdf: 
1:531ecdf:   /**
1:531ecdf:    * This method will the index files tableBlockIndexUniqueIdentifiers of a merge index file
1:531ecdf:    *
1:531ecdf:    * @param identifier
1:5f68a79:    * @param segmentIndexFileStore
1:531ecdf:    * @return
1:531ecdf:    * @throws IOException
1:531ecdf:    */
1:531ecdf:   public static List<TableBlockIndexUniqueIdentifier> getIndexFileIdentifiersFromMergeFile(
1:531ecdf:       TableBlockIndexUniqueIdentifier identifier, SegmentIndexFileStore segmentIndexFileStore)
1:531ecdf:       throws IOException {
1:531ecdf:     List<TableBlockIndexUniqueIdentifier> tableBlockIndexUniqueIdentifiers = new ArrayList<>();
1:531ecdf:     String mergeFilePath =
1:531ecdf:         identifier.getIndexFilePath() + CarbonCommonConstants.FILE_SEPARATOR + identifier
1:531ecdf:             .getIndexFileName();
1:531ecdf:     segmentIndexFileStore.readMergeFile(mergeFilePath);
1:531ecdf:     List<String> indexFiles =
1:531ecdf:         segmentIndexFileStore.getCarbonMergeFileToIndexFilesMap().get(mergeFilePath);
1:531ecdf:     for (String indexFile : indexFiles) {
1:531ecdf:       tableBlockIndexUniqueIdentifiers.add(
1:531ecdf:           new TableBlockIndexUniqueIdentifier(identifier.getIndexFilePath(), indexFile,
1:531ecdf:               identifier.getIndexFileName(), identifier.getSegmentId()));
1:531ecdf:     }
1:531ecdf:     return tableBlockIndexUniqueIdentifiers;
1:531ecdf:   }
1:531ecdf: 
1:531ecdf:   /**
1:6118711:    * Method to check if CACHE_LEVEL is set to BLOCK or BLOCKLET
1:6118711:    */
1:3cbabcd:   public static boolean isCacheLevelBlock(CarbonTable carbonTable) {
1:6118711:     String cacheLevel = carbonTable.getTableInfo().getFactTable().getTableProperties()
1:6118711:         .get(CarbonCommonConstants.CACHE_LEVEL);
1:3cbabcd:     if (BlockletDataMapFactory.CACHE_LEVEL_BLOCKLET.equals(cacheLevel)) {
1:3cbabcd:       return false;
1:6118711:     }
1:3cbabcd:     return true;
1:6118711:   }
1:6118711: 
1:3894e1d:   public static boolean isSameColumnSchemaList(List<ColumnSchema> indexFileColumnList,
1:5f68a79:       List<ColumnSchema> tableColumnList) {
1:5f68a79:     if (indexFileColumnList.size() != tableColumnList.size()) {
1:5f68a79:       LOG.error("Index file's column size is " + indexFileColumnList.size()
1:5f68a79:           + " but table's column size is " + tableColumnList.size());
1:6118711:       return false;
1:531ecdf:     }
1:5f68a79:     for (int i = 0; i < tableColumnList.size(); i++) {
1:3894e1d:       if (!tableColumnList.contains(indexFileColumnList.get(i))) {
2:5f68a79:         return false;
1:5f68a79:       }
1:5f68a79:     }
1:6118711:     return true;
1:5f68a79:   }
1:6118711: 
1:6118711:   /**
1:6118711:    * Fill the measures min values with minimum , this is needed for backward version compatability
1:6118711:    * as older versions don't store min values for measures
1:6118711:    */
1:6118711:   public static byte[][] updateMinValues(SegmentProperties segmentProperties, byte[][] minValues) {
1:6118711:     byte[][] updatedValues = minValues;
1:6118711:     int[] minMaxLen = segmentProperties.getColumnsValueSize();
1:6118711:     if (minValues.length < minMaxLen.length) {
1:6118711:       updatedValues = new byte[minMaxLen.length][];
1:6118711:       System.arraycopy(minValues, 0, updatedValues, 0, minValues.length);
1:6118711:       List<CarbonMeasure> measures = segmentProperties.getMeasures();
1:6118711:       ByteBuffer buffer = ByteBuffer.allocate(8);
1:6118711:       for (int i = 0; i < measures.size(); i++) {
1:6118711:         buffer.rewind();
1:6118711:         DataType dataType = measures.get(i).getDataType();
1:6118711:         if (dataType == DataTypes.BYTE) {
1:6118711:           buffer.putLong(Byte.MIN_VALUE);
1:6118711:           updatedValues[minValues.length + i] = buffer.array().clone();
1:6118711:         } else if (dataType == DataTypes.SHORT) {
1:6118711:           buffer.putLong(Short.MIN_VALUE);
1:6118711:           updatedValues[minValues.length + i] = buffer.array().clone();
1:6118711:         } else if (dataType == DataTypes.INT) {
1:6118711:           buffer.putLong(Integer.MIN_VALUE);
1:6118711:           updatedValues[minValues.length + i] = buffer.array().clone();
1:6118711:         } else if (dataType == DataTypes.LONG) {
1:6118711:           buffer.putLong(Long.MIN_VALUE);
1:6118711:           updatedValues[minValues.length + i] = buffer.array().clone();
1:6118711:         } else if (DataTypes.isDecimal(dataType)) {
1:6118711:           updatedValues[minValues.length + i] =
1:6118711:               DataTypeUtil.bigDecimalToByte(BigDecimal.valueOf(Long.MIN_VALUE));
1:e580d64:         } else {
1:6118711:           buffer.putDouble(Double.MIN_VALUE);
1:6118711:           updatedValues[minValues.length + i] = buffer.array().clone();
1:6118711:         }
1:6118711:       }
1:6118711:     }
1:6118711:     return updatedValues;
1:6118711:   }
1:f4a58c5: 
1:f4a58c5:   /**
1:6118711:    * Fill the measures max values with maximum , this is needed for backward version compatability
1:6118711:    * as older versions don't store max values for measures
1:6118711:    */
1:6118711:   public static byte[][] updateMaxValues(SegmentProperties segmentProperties, byte[][] maxValues) {
1:6118711:     byte[][] updatedValues = maxValues;
1:6118711:     int[] minMaxLen = segmentProperties.getColumnsValueSize();
1:6118711:     if (maxValues.length < minMaxLen.length) {
1:6118711:       updatedValues = new byte[minMaxLen.length][];
1:6118711:       System.arraycopy(maxValues, 0, updatedValues, 0, maxValues.length);
1:6118711:       List<CarbonMeasure> measures = segmentProperties.getMeasures();
1:6118711:       ByteBuffer buffer = ByteBuffer.allocate(8);
1:6118711:       for (int i = 0; i < measures.size(); i++) {
1:6118711:         buffer.rewind();
1:6118711:         DataType dataType = measures.get(i).getDataType();
1:6118711:         if (dataType == DataTypes.BYTE) {
1:6118711:           buffer.putLong(Byte.MAX_VALUE);
1:6118711:           updatedValues[maxValues.length + i] = buffer.array().clone();
1:6118711:         } else if (dataType == DataTypes.SHORT) {
1:6118711:           buffer.putLong(Short.MAX_VALUE);
1:6118711:           updatedValues[maxValues.length + i] = buffer.array().clone();
1:6118711:         } else if (dataType == DataTypes.INT) {
1:6118711:           buffer.putLong(Integer.MAX_VALUE);
1:6118711:           updatedValues[maxValues.length + i] = buffer.array().clone();
1:6118711:         } else if (dataType == DataTypes.LONG) {
1:6118711:           buffer.putLong(Long.MAX_VALUE);
1:6118711:           updatedValues[maxValues.length + i] = buffer.array().clone();
1:6118711:         } else if (DataTypes.isDecimal(dataType)) {
1:6118711:           updatedValues[maxValues.length + i] =
1:6118711:               DataTypeUtil.bigDecimalToByte(BigDecimal.valueOf(Long.MAX_VALUE));
2:6118711:         } else {
1:6118711:           buffer.putDouble(Double.MAX_VALUE);
1:6118711:           updatedValues[maxValues.length + i] = buffer.array().clone();
1:6118711:         }
1:6118711:       }
1:6118711:     }
1:6118711:     return updatedValues;
1:6118711:   }
1:6118711: 
1:6118711:   /**
1:f4a58c5:    * Convert schema to binary
1:f4a58c5:    */
1:f4a58c5:   public static byte[] convertSchemaToBinary(List<ColumnSchema> columnSchemas) throws IOException {
1:f4a58c5:     ByteArrayOutputStream stream = new ByteArrayOutputStream();
1:f4a58c5:     DataOutput dataOutput = new DataOutputStream(stream);
1:f4a58c5:     dataOutput.writeShort(columnSchemas.size());
1:f4a58c5:     for (ColumnSchema columnSchema : columnSchemas) {
1:f4a58c5:       if (columnSchema.getColumnReferenceId() == null) {
1:f4a58c5:         columnSchema.setColumnReferenceId(columnSchema.getColumnUniqueId());
1:f4a58c5:       }
1:f4a58c5:       columnSchema.write(dataOutput);
1:f4a58c5:     }
1:f4a58c5:     byte[] byteArray = stream.toByteArray();
1:8f08c4a:     // Compress to reduce the size of schema
1:8f08c4a:     return CompressorFactory.SupportedCompressor.SNAPPY.getCompressor().compressByte(byteArray);
1:f4a58c5:   }
1:f4a58c5: 
1:f4a58c5:   /**
1:f4a58c5:    * Read column schema from binary
1:f4a58c5:    *
1:f4a58c5:    * @param schemaArray
1:f4a58c5:    * @throws IOException
1:f4a58c5:    */
1:f4a58c5:   public static List<ColumnSchema> readColumnSchema(byte[] schemaArray) throws IOException {
1:f4a58c5:     // uncompress it.
1:8f08c4a:     schemaArray = CompressorFactory.SupportedCompressor.SNAPPY.getCompressor().unCompressByte(
1:8f08c4a:         schemaArray);
1:f4a58c5:     ByteArrayInputStream schemaStream = new ByteArrayInputStream(schemaArray);
1:f4a58c5:     DataInput schemaInput = new DataInputStream(schemaStream);
1:f4a58c5:     List<ColumnSchema> columnSchemas = new ArrayList<>();
1:f4a58c5:     int size = schemaInput.readShort();
1:f4a58c5:     for (int i = 0; i < size; i++) {
1:f4a58c5:       ColumnSchema columnSchema = new ColumnSchema();
1:f4a58c5:       columnSchema.readFields(schemaInput);
1:f4a58c5:       columnSchemas.add(columnSchema);
1:f4a58c5:     }
1:f4a58c5:     return columnSchemas;
1:f4a58c5:   }
1:dc29319: 
1:dc29319:   /**
1:dc29319:    * Method to get the min/max values for columns to be cached
1:dc29319:    *
1:dc29319:    * @param segmentProperties
1:dc29319:    * @param minMaxCacheColumns
1:dc29319:    * @param minMaxValuesForAllColumns
1:dc29319:    * @return
1:dc29319:    */
1:dc29319:   public static byte[][] getMinMaxForColumnsToBeCached(SegmentProperties segmentProperties,
1:dc29319:       List<CarbonColumn> minMaxCacheColumns, byte[][] minMaxValuesForAllColumns) {
1:dc29319:     byte[][] minMaxValuesForColumnsToBeCached = minMaxValuesForAllColumns;
1:dc29319:     if (null != minMaxCacheColumns) {
1:dc29319:       minMaxValuesForColumnsToBeCached = new byte[minMaxCacheColumns.size()][];
1:dc29319:       int counter = 0;
1:dc29319:       for (CarbonColumn column : minMaxCacheColumns) {
1:dc29319:         minMaxValuesForColumnsToBeCached[counter++] =
1:dc29319:             minMaxValuesForAllColumns[getColumnOrdinal(segmentProperties, column)];
1:dc29319:       }
1:dc29319:     }
1:dc29319:     return minMaxValuesForColumnsToBeCached;
1:dc29319:   }
1:dc29319: 
1:dc29319:   /**
1:dc29319:    * compute the column ordinal as per data is stored
1:dc29319:    *
1:dc29319:    * @param segmentProperties
1:dc29319:    * @param column
1:dc29319:    * @return
1:dc29319:    */
1:dc29319:   public static int getColumnOrdinal(SegmentProperties segmentProperties, CarbonColumn column) {
1:dc29319:     if (column.isMeasure()) {
1:dc29319:       // as measures are stored at the end after all dimensions and complex dimensions hence add
1:dc29319:       // the last dimension ordinal to measure ordinal. Segment properties will store min max
1:dc29319:       // length in one array on the order normal dimension, complex dimension and then measure
1:dc29319:       return segmentProperties.getLastDimensionColOrdinal() + column.getOrdinal();
1:dc29319:     } else {
1:dc29319:       return column.getOrdinal();
1:dc29319:     }
1:dc29319:   }
1:4df335f: 
1:4df335f:   /**
1:4df335f:    * Method to check whether to serialize min/max values to executor. Returns true if
1:4df335f:    * filter column min/max is not cached in driver
1:4df335f:    *
1:4df335f:    * @param filterResolverTree
1:4df335f:    * @param minMaxCacheColumns
1:4df335f:    * @return
1:4df335f:    */
1:4df335f:   public static boolean useMinMaxForBlockletPruning(FilterResolverIntf filterResolverTree,
1:4df335f:       List<CarbonColumn> minMaxCacheColumns) {
1:4df335f:     boolean serializeMinMax = false;
1:4df335f:     if (null != minMaxCacheColumns) {
1:4df335f:       Set<CarbonDimension> filterDimensions = new HashSet<>();
1:4df335f:       Set<CarbonMeasure> filterMeasures = new HashSet<>();
1:4df335f:       QueryUtil
1:4df335f:           .getAllFilterDimensionsAndMeasures(filterResolverTree, filterDimensions, filterMeasures);
1:4df335f:       // set flag to true if columns cached size is lesser than filter columns
1:4df335f:       if (minMaxCacheColumns.size() < (filterDimensions.size() + filterMeasures.size())) {
1:4df335f:         serializeMinMax = true;
1:4df335f:       } else {
1:4df335f:         // check if all the filter dimensions are cached
1:4df335f:         for (CarbonDimension filterDimension : filterDimensions) {
1:4df335f:           // complex dimensions are not allwed to be specified in COLUMN_META_CACHE property, so
1:4df335f:           // cannot validate for complex columns
1:4df335f:           if (filterDimension.isComplex()) {
1:4df335f:             continue;
1:4df335f:           }
1:4df335f:           if (!filterColumnExistsInMinMaxColumnList(minMaxCacheColumns, filterDimension)) {
1:4df335f:             serializeMinMax = true;
1:4df335f:             break;
1:4df335f:           }
1:4df335f:         }
1:4df335f:         // check if all the filter measures are cached only if all filter dimensions are cached
1:4df335f:         if (!serializeMinMax) {
1:4df335f:           for (CarbonMeasure filterMeasure : filterMeasures) {
1:4df335f:             if (!filterColumnExistsInMinMaxColumnList(minMaxCacheColumns, filterMeasure)) {
1:4df335f:               serializeMinMax = true;
1:4df335f:               break;
1:4df335f:             }
1:4df335f:           }
1:4df335f:         }
1:4df335f:       }
1:4df335f:     }
1:4df335f:     return serializeMinMax;
1:4df335f:   }
1:4df335f: 
1:4df335f:   /**
1:4df335f:    * Method to check for filter column in min/max cache columns list
1:4df335f:    *
1:4df335f:    * @param minMaxCacheColumns
1:4df335f:    * @param filterColumn
1:4df335f:    * @return
1:4df335f:    */
1:4df335f:   private static boolean filterColumnExistsInMinMaxColumnList(List<CarbonColumn> minMaxCacheColumns,
1:4df335f:       CarbonColumn filterColumn) {
1:4df335f:     for (CarbonColumn column : minMaxCacheColumns) {
1:4c692d1:       if (filterColumn.getColumnId().equalsIgnoreCase(column.getColumnId())) {
1:4df335f:         return true;
1:4df335f:       }
1:4df335f:     }
1:4df335f:     return false;
1:4df335f:   }
1:5f68a79: }
============================================================================
author:ravipesala
-------------------------------------------------------------------------------
commit:4c692d1
/////////////////////////////////////////////////////////////////////////
1:       if (filterColumn.getColumnId().equalsIgnoreCase(column.getColumnId())) {
commit:3894e1d
/////////////////////////////////////////////////////////////////////////
0:     DataFileFooterConverter fileFooterConverter = new DataFileFooterConverter();
1:     List<DataFileFooter> indexInfo = fileFooterConverter.getIndexInfo(
1:         identifier.getIndexFilePath() + CarbonCommonConstants.FILE_SEPARATOR + identifier
1:             .getIndexFileName(), indexFileStore.getFileData(identifier.getIndexFileName()),
1:         isTransactionalTable);
/////////////////////////////////////////////////////////////////////////
1:   public static boolean isSameColumnSchemaList(List<ColumnSchema> indexFileColumnList,
/////////////////////////////////////////////////////////////////////////
1:       if (!tableColumnList.contains(indexFileColumnList.get(i))) {
author:xuchuanyin
-------------------------------------------------------------------------------
commit:8f08c4a
/////////////////////////////////////////////////////////////////////////
1:     // Compress to reduce the size of schema
1:     return CompressorFactory.SupportedCompressor.SNAPPY.getCompressor().compressByte(byteArray);
/////////////////////////////////////////////////////////////////////////
1:     schemaArray = CompressorFactory.SupportedCompressor.SNAPPY.getCompressor().unCompressByte(
1:         schemaArray);
author:kunal642
-------------------------------------------------------------------------------
commit:8f1a029
/////////////////////////////////////////////////////////////////////////
1: import org.apache.hadoop.conf.Configuration;
/////////////////////////////////////////////////////////////////////////
1:               .getMergeIndexFileName(), identifierWrapper.getConfiguration());
/////////////////////////////////////////////////////////////////////////
1:               .getIndexFileName(), identifierWrapper.getConfiguration()) });
/////////////////////////////////////////////////////////////////////////
1:     DataFileFooterConverter fileFooterConverter =
1:         new DataFileFooterConverter(identifierWrapper.getConfiguration());
/////////////////////////////////////////////////////////////////////////
1:       String segmentFilePath, Configuration configuration) throws IOException {
1:     CarbonFile carbonFile = FileFactory.getCarbonFile(segmentFilePath, configuration);
author:Manhua
-------------------------------------------------------------------------------
commit:3cbabcd
/////////////////////////////////////////////////////////////////////////
1: import org.apache.carbondata.core.indexstore.blockletindex.BlockletDataMapFactory;
/////////////////////////////////////////////////////////////////////////
1:   public static boolean isCacheLevelBlock(CarbonTable carbonTable) {
1:     if (BlockletDataMapFactory.CACHE_LEVEL_BLOCKLET.equals(cacheLevel)) {
1:       return false;
1:     return true;
author:BJangir
-------------------------------------------------------------------------------
commit:7158d52
/////////////////////////////////////////////////////////////////////////
1:       if (null == blockMetaInfoMap.get(blockPath)) {
/////////////////////////////////////////////////////////////////////////
author:rahul
-------------------------------------------------------------------------------
commit:e580d64
/////////////////////////////////////////////////////////////////////////
0:       if (null != fileNameToMetaInfoMapping && null == blockMetaInfoMap.get(blockPath)) {
/////////////////////////////////////////////////////////////////////////
1:       } else {
0:         blockMetaInfoMap.put(blockPath, new BlockMetaInfo(new String[] {},0));
/////////////////////////////////////////////////////////////////////////
1:         fileNameToMetaInfoMapping.put(file.getPath(), blockMetaInfo);
author:manishgupta88
-------------------------------------------------------------------------------
commit:fd747a3
/////////////////////////////////////////////////////////////////////////
0:       if (null == blockMetaInfoMap.get(blockPath)) {
1:         BlockMetaInfo blockMetaInfo = createBlockMetaInfo(fileNameToMetaInfoMapping, blockPath);
1:         // if blockMetaInfo is null that means the file has been deleted from the file system.
1:         // This can happen in case IUD scenarios where after deleting or updating the data the
1:         // complete block is deleted but the entry still exists in index or merge index file
1:         if (null != blockMetaInfo) {
1:           blockMetaInfoMap.put(blockPath, blockMetaInfo);
1:         }
/////////////////////////////////////////////////////////////////////////
1:       Map<String, BlockMetaInfo> fileNameToMetaInfoMapping, String carbonDataFile)
1:       throws IOException {
1:         if (!FileFactory.isFileExist(carbonDataFile)) {
1:           return null;
1:         }
commit:4df335f
/////////////////////////////////////////////////////////////////////////
1: import org.apache.carbondata.core.metadata.schema.table.column.CarbonDimension;
1: import org.apache.carbondata.core.scan.executor.util.QueryUtil;
1: import org.apache.carbondata.core.scan.filter.resolver.FilterResolverIntf;
/////////////////////////////////////////////////////////////////////////
1: 
1:   /**
1:    * Method to check whether to serialize min/max values to executor. Returns true if
1:    * filter column min/max is not cached in driver
1:    *
1:    * @param filterResolverTree
1:    * @param minMaxCacheColumns
1:    * @return
1:    */
1:   public static boolean useMinMaxForBlockletPruning(FilterResolverIntf filterResolverTree,
1:       List<CarbonColumn> minMaxCacheColumns) {
1:     boolean serializeMinMax = false;
1:     if (null != minMaxCacheColumns) {
1:       Set<CarbonDimension> filterDimensions = new HashSet<>();
1:       Set<CarbonMeasure> filterMeasures = new HashSet<>();
1:       QueryUtil
1:           .getAllFilterDimensionsAndMeasures(filterResolverTree, filterDimensions, filterMeasures);
1:       // set flag to true if columns cached size is lesser than filter columns
1:       if (minMaxCacheColumns.size() < (filterDimensions.size() + filterMeasures.size())) {
1:         serializeMinMax = true;
1:       } else {
1:         // check if all the filter dimensions are cached
1:         for (CarbonDimension filterDimension : filterDimensions) {
1:           // complex dimensions are not allwed to be specified in COLUMN_META_CACHE property, so
1:           // cannot validate for complex columns
1:           if (filterDimension.isComplex()) {
1:             continue;
1:           }
1:           if (!filterColumnExistsInMinMaxColumnList(minMaxCacheColumns, filterDimension)) {
1:             serializeMinMax = true;
1:             break;
1:           }
1:         }
1:         // check if all the filter measures are cached only if all filter dimensions are cached
1:         if (!serializeMinMax) {
1:           for (CarbonMeasure filterMeasure : filterMeasures) {
1:             if (!filterColumnExistsInMinMaxColumnList(minMaxCacheColumns, filterMeasure)) {
1:               serializeMinMax = true;
1:               break;
1:             }
1:           }
1:         }
1:       }
1:     }
1:     return serializeMinMax;
1:   }
1: 
1:   /**
1:    * Method to check for filter column in min/max cache columns list
1:    *
1:    * @param minMaxCacheColumns
1:    * @param filterColumn
1:    * @return
1:    */
1:   private static boolean filterColumnExistsInMinMaxColumnList(List<CarbonColumn> minMaxCacheColumns,
1:       CarbonColumn filterColumn) {
1:     for (CarbonColumn column : minMaxCacheColumns) {
0:       if (filterColumn.getColumnId().equals(column.getColumnId())) {
1:         return true;
1:       }
1:     }
1:     return false;
1:   }
commit:dc29319
/////////////////////////////////////////////////////////////////////////
1: import org.apache.carbondata.core.metadata.schema.table.column.CarbonColumn;
/////////////////////////////////////////////////////////////////////////
1: 
1:   /**
1:    * Method to get the min/max values for columns to be cached
1:    *
1:    * @param segmentProperties
1:    * @param minMaxCacheColumns
1:    * @param minMaxValuesForAllColumns
1:    * @return
1:    */
1:   public static byte[][] getMinMaxForColumnsToBeCached(SegmentProperties segmentProperties,
1:       List<CarbonColumn> minMaxCacheColumns, byte[][] minMaxValuesForAllColumns) {
1:     byte[][] minMaxValuesForColumnsToBeCached = minMaxValuesForAllColumns;
1:     if (null != minMaxCacheColumns) {
1:       minMaxValuesForColumnsToBeCached = new byte[minMaxCacheColumns.size()][];
1:       int counter = 0;
1:       for (CarbonColumn column : minMaxCacheColumns) {
1:         minMaxValuesForColumnsToBeCached[counter++] =
1:             minMaxValuesForAllColumns[getColumnOrdinal(segmentProperties, column)];
1:       }
1:     }
1:     return minMaxValuesForColumnsToBeCached;
1:   }
1: 
1:   /**
1:    * compute the column ordinal as per data is stored
1:    *
1:    * @param segmentProperties
1:    * @param column
1:    * @return
1:    */
1:   public static int getColumnOrdinal(SegmentProperties segmentProperties, CarbonColumn column) {
1:     if (column.isMeasure()) {
1:       // as measures are stored at the end after all dimensions and complex dimensions hence add
1:       // the last dimension ordinal to measure ordinal. Segment properties will store min max
1:       // length in one array on the order normal dimension, complex dimension and then measure
1:       return segmentProperties.getLastDimensionColOrdinal() + column.getOrdinal();
1:     } else {
1:       return column.getOrdinal();
1:     }
1:   }
commit:f4a58c5
/////////////////////////////////////////////////////////////////////////
1: import java.io.ByteArrayInputStream;
1: import java.io.ByteArrayOutputStream;
1: import java.io.DataInput;
1: import java.io.DataInputStream;
1: import java.io.DataOutput;
1: import java.io.DataOutputStream;
/////////////////////////////////////////////////////////////////////////
1: import org.apache.carbondata.core.datastore.compression.CompressorFactory;
/////////////////////////////////////////////////////////////////////////
1: 
1:   /**
1:    * Convert schema to binary
1:    */
1:   public static byte[] convertSchemaToBinary(List<ColumnSchema> columnSchemas) throws IOException {
1:     ByteArrayOutputStream stream = new ByteArrayOutputStream();
1:     DataOutput dataOutput = new DataOutputStream(stream);
1:     dataOutput.writeShort(columnSchemas.size());
1:     for (ColumnSchema columnSchema : columnSchemas) {
1:       if (columnSchema.getColumnReferenceId() == null) {
1:         columnSchema.setColumnReferenceId(columnSchema.getColumnUniqueId());
1:       }
1:       columnSchema.write(dataOutput);
1:     }
1:     byte[] byteArray = stream.toByteArray();
0:     // Compress with snappy to reduce the size of schema
0:     return CompressorFactory.getInstance().getCompressor().compressByte(byteArray);
1:   }
1: 
1:   /**
1:    * Read column schema from binary
1:    *
1:    * @param schemaArray
1:    * @throws IOException
1:    */
1:   public static List<ColumnSchema> readColumnSchema(byte[] schemaArray) throws IOException {
1:     // uncompress it.
0:     schemaArray = CompressorFactory.getInstance().getCompressor().unCompressByte(schemaArray);
1:     ByteArrayInputStream schemaStream = new ByteArrayInputStream(schemaArray);
1:     DataInput schemaInput = new DataInputStream(schemaStream);
1:     List<ColumnSchema> columnSchemas = new ArrayList<>();
1:     int size = schemaInput.readShort();
1:     for (int i = 0; i < size; i++) {
1:       ColumnSchema columnSchema = new ColumnSchema();
1:       columnSchema.readFields(schemaInput);
1:       columnSchemas.add(columnSchema);
1:     }
1:     return columnSchemas;
1:   }
commit:6118711
/////////////////////////////////////////////////////////////////////////
1: import java.math.BigDecimal;
1: import java.nio.ByteBuffer;
/////////////////////////////////////////////////////////////////////////
1: import org.apache.carbondata.core.datastore.block.SegmentProperties;
/////////////////////////////////////////////////////////////////////////
1: import org.apache.carbondata.core.metadata.datatype.DataType;
1: import org.apache.carbondata.core.metadata.datatype.DataTypes;
1: import org.apache.carbondata.core.metadata.schema.table.column.CarbonMeasure;
/////////////////////////////////////////////////////////////////////////
1:   /**
1:    * Method to check if CACHE_LEVEL is set to BLOCK or BLOCKLET
1:    *
0:    * @param carbonTable
0:    * @param cacheLevelBlocklet
1:    * @return
1:    */
0:   public static boolean isCacheLevelBlock(CarbonTable carbonTable, String cacheLevelBlocklet) {
1:     String cacheLevel = carbonTable.getTableInfo().getFactTable().getTableProperties()
1:         .get(CarbonCommonConstants.CACHE_LEVEL);
0:     if (!cacheLevelBlocklet.equals(cacheLevel)) {
1:       return true;
1:     }
1:     return false;
1:   }
1: 
/////////////////////////////////////////////////////////////////////////
1: 
1:   /**
1:    * Fill the measures min values with minimum , this is needed for backward version compatability
1:    * as older versions don't store min values for measures
1:    */
1:   public static byte[][] updateMinValues(SegmentProperties segmentProperties, byte[][] minValues) {
1:     byte[][] updatedValues = minValues;
1:     int[] minMaxLen = segmentProperties.getColumnsValueSize();
1:     if (minValues.length < minMaxLen.length) {
1:       updatedValues = new byte[minMaxLen.length][];
1:       System.arraycopy(minValues, 0, updatedValues, 0, minValues.length);
1:       List<CarbonMeasure> measures = segmentProperties.getMeasures();
1:       ByteBuffer buffer = ByteBuffer.allocate(8);
1:       for (int i = 0; i < measures.size(); i++) {
1:         buffer.rewind();
1:         DataType dataType = measures.get(i).getDataType();
1:         if (dataType == DataTypes.BYTE) {
1:           buffer.putLong(Byte.MIN_VALUE);
1:           updatedValues[minValues.length + i] = buffer.array().clone();
1:         } else if (dataType == DataTypes.SHORT) {
1:           buffer.putLong(Short.MIN_VALUE);
1:           updatedValues[minValues.length + i] = buffer.array().clone();
1:         } else if (dataType == DataTypes.INT) {
1:           buffer.putLong(Integer.MIN_VALUE);
1:           updatedValues[minValues.length + i] = buffer.array().clone();
1:         } else if (dataType == DataTypes.LONG) {
1:           buffer.putLong(Long.MIN_VALUE);
1:           updatedValues[minValues.length + i] = buffer.array().clone();
1:         } else if (DataTypes.isDecimal(dataType)) {
1:           updatedValues[minValues.length + i] =
1:               DataTypeUtil.bigDecimalToByte(BigDecimal.valueOf(Long.MIN_VALUE));
1:         } else {
1:           buffer.putDouble(Double.MIN_VALUE);
1:           updatedValues[minValues.length + i] = buffer.array().clone();
1:         }
1:       }
1:     }
1:     return updatedValues;
1:   }
1: 
1:   /**
1:    * Fill the measures max values with maximum , this is needed for backward version compatability
1:    * as older versions don't store max values for measures
1:    */
1:   public static byte[][] updateMaxValues(SegmentProperties segmentProperties, byte[][] maxValues) {
1:     byte[][] updatedValues = maxValues;
1:     int[] minMaxLen = segmentProperties.getColumnsValueSize();
1:     if (maxValues.length < minMaxLen.length) {
1:       updatedValues = new byte[minMaxLen.length][];
1:       System.arraycopy(maxValues, 0, updatedValues, 0, maxValues.length);
1:       List<CarbonMeasure> measures = segmentProperties.getMeasures();
1:       ByteBuffer buffer = ByteBuffer.allocate(8);
1:       for (int i = 0; i < measures.size(); i++) {
1:         buffer.rewind();
1:         DataType dataType = measures.get(i).getDataType();
1:         if (dataType == DataTypes.BYTE) {
1:           buffer.putLong(Byte.MAX_VALUE);
1:           updatedValues[maxValues.length + i] = buffer.array().clone();
1:         } else if (dataType == DataTypes.SHORT) {
1:           buffer.putLong(Short.MAX_VALUE);
1:           updatedValues[maxValues.length + i] = buffer.array().clone();
1:         } else if (dataType == DataTypes.INT) {
1:           buffer.putLong(Integer.MAX_VALUE);
1:           updatedValues[maxValues.length + i] = buffer.array().clone();
1:         } else if (dataType == DataTypes.LONG) {
1:           buffer.putLong(Long.MAX_VALUE);
1:           updatedValues[maxValues.length + i] = buffer.array().clone();
1:         } else if (DataTypes.isDecimal(dataType)) {
1:           updatedValues[maxValues.length + i] =
1:               DataTypeUtil.bigDecimalToByte(BigDecimal.valueOf(Long.MAX_VALUE));
1:         } else {
1:           buffer.putDouble(Double.MAX_VALUE);
1:           updatedValues[maxValues.length + i] = buffer.array().clone();
1:         }
1:       }
1:     }
1:     return updatedValues;
1:   }
author:dhatchayani
-------------------------------------------------------------------------------
commit:7341907
/////////////////////////////////////////////////////////////////////////
0:       if (null == blockMetaInfoMap.get(blockPath) && FileFactory.isFileExist(blockPath)) {
commit:531ecdf
/////////////////////////////////////////////////////////////////////////
1: /*
1:  * Licensed to the Apache Software Foundation (ASF) under one or more
1:  * contributor license agreements.  See the NOTICE file distributed with
1:  * this work for additional information regarding copyright ownership.
1:  * The ASF licenses this file to You under the Apache License, Version 2.0
1:  * (the "License"); you may not use this file except in compliance with
1:  * the License.  You may obtain a copy of the License at
1:  *
1:  *    http://www.apache.org/licenses/LICENSE-2.0
1:  *
1:  * Unless required by applicable law or agreed to in writing, software
1:  * distributed under the License is distributed on an "AS IS" BASIS,
1:  * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
1:  * See the License for the specific language governing permissions and
1:  * limitations under the License.
1:  */
1: 
1: package org.apache.carbondata.core.util;
1: 
1: import java.io.IOException;
1: import java.util.ArrayList;
1: import java.util.HashMap;
1: import java.util.HashSet;
1: import java.util.List;
1: import java.util.Map;
1: import java.util.Set;
1: import java.util.TreeMap;
1: 
1: import org.apache.carbondata.core.constants.CarbonCommonConstants;
1: import org.apache.carbondata.core.datamap.Segment;
1: import org.apache.carbondata.core.datastore.filesystem.AbstractDFSCarbonFile;
1: import org.apache.carbondata.core.datastore.filesystem.CarbonFile;
1: import org.apache.carbondata.core.datastore.impl.FileFactory;
1: import org.apache.carbondata.core.indexstore.BlockMetaInfo;
1: import org.apache.carbondata.core.indexstore.TableBlockIndexUniqueIdentifier;
1: import org.apache.carbondata.core.indexstore.blockletindex.BlockletDataMapDistributable;
1: import org.apache.carbondata.core.indexstore.blockletindex.SegmentIndexFileStore;
1: import org.apache.carbondata.core.metadata.blocklet.DataFileFooter;
1: import org.apache.carbondata.core.util.path.CarbonTablePath;
1: 
1: import org.apache.hadoop.fs.Path;
1: import org.apache.hadoop.fs.PathFilter;
1: 
1: public class BlockletDataMapUtil {
1: 
1:   public static Map<String, BlockMetaInfo> getBlockMetaInfoMap(
0:       TableBlockIndexUniqueIdentifier identifier, SegmentIndexFileStore indexFileStore,
0:       Set<String> filesRead, Map<String, BlockMetaInfo> fileNameToMetaInfoMapping)
1:       throws IOException {
1:     if (identifier.getMergeIndexFileName() != null
1:         && indexFileStore.getFileData(identifier.getIndexFileName()) == null) {
1:       CarbonFile indexMergeFile = FileFactory.getCarbonFile(
1:           identifier.getIndexFilePath() + CarbonCommonConstants.FILE_SEPARATOR + identifier
0:               .getMergeIndexFileName());
1:       if (indexMergeFile.exists() && !filesRead.contains(indexMergeFile.getPath())) {
1:         indexFileStore.readAllIIndexOfSegment(new CarbonFile[] { indexMergeFile });
1:         filesRead.add(indexMergeFile.getPath());
1:       }
1:     }
1:     if (indexFileStore.getFileData(identifier.getIndexFileName()) == null) {
1:       indexFileStore.readAllIIndexOfSegment(new CarbonFile[] { FileFactory.getCarbonFile(
1:           identifier.getIndexFilePath() + CarbonCommonConstants.FILE_SEPARATOR + identifier
0:               .getIndexFileName()) });
1:     }
0:     DataFileFooterConverter fileFooterConverter = new DataFileFooterConverter();
1:     Map<String, BlockMetaInfo> blockMetaInfoMap = new HashMap<>();
0:     List<DataFileFooter> indexInfo = fileFooterConverter.getIndexInfo(
1:         identifier.getIndexFilePath() + CarbonCommonConstants.FILE_SEPARATOR + identifier
0:             .getIndexFileName(), indexFileStore.getFileData(identifier.getIndexFileName()));
1:     for (DataFileFooter footer : indexInfo) {
1:       String blockPath = footer.getBlockInfo().getTableBlockInfo().getFilePath();
0:       if (null == blockMetaInfoMap.get(blockPath)) {
0:         blockMetaInfoMap.put(blockPath, createBlockMetaInfo(fileNameToMetaInfoMapping, blockPath));
1:       }
1:     }
1:     return blockMetaInfoMap;
1:   }
1: 
1:   /**
1:    * This method will create file name to block Meta Info Mapping. This method will reduce the
1:    * number of namenode calls and using this method one namenode will fetch 1000 entries
1:    *
1:    * @param segmentFilePath
1:    * @return
1:    * @throws IOException
1:    */
1:   public static Map<String, BlockMetaInfo> createCarbonDataFileBlockMetaInfoMapping(
0:       String segmentFilePath) throws IOException {
1:     Map<String, BlockMetaInfo> fileNameToMetaInfoMapping = new TreeMap();
0:     CarbonFile carbonFile = FileFactory.getCarbonFile(segmentFilePath);
1:     if (carbonFile instanceof AbstractDFSCarbonFile) {
1:       PathFilter pathFilter = new PathFilter() {
1:         @Override public boolean accept(Path path) {
1:           return CarbonTablePath.isCarbonDataFile(path.getName());
1:         }
1:       };
1:       CarbonFile[] carbonFiles = carbonFile.locationAwareListFiles(pathFilter);
1:       for (CarbonFile file : carbonFiles) {
1:         String[] location = file.getLocations();
1:         long len = file.getSize();
1:         BlockMetaInfo blockMetaInfo = new BlockMetaInfo(location, len);
0:         fileNameToMetaInfoMapping.put(file.getPath().toString(), blockMetaInfo);
1:       }
1:     }
1:     return fileNameToMetaInfoMapping;
1:   }
1: 
1:   private static BlockMetaInfo createBlockMetaInfo(
0:       Map<String, BlockMetaInfo> fileNameToMetaInfoMapping, String carbonDataFile) {
1:     FileFactory.FileType fileType = FileFactory.getFileType(carbonDataFile);
1:     switch (fileType) {
1:       case LOCAL:
1:         CarbonFile carbonFile = FileFactory.getCarbonFile(carbonDataFile, fileType);
1:         return new BlockMetaInfo(new String[] { "localhost" }, carbonFile.getSize());
1:       default:
1:         return fileNameToMetaInfoMapping.get(carbonDataFile);
1:     }
1:   }
1: 
1:   public static Set<TableBlockIndexUniqueIdentifier> getTableBlockUniqueIdentifiers(Segment segment)
1:       throws IOException {
1:     Set<TableBlockIndexUniqueIdentifier> tableBlockIndexUniqueIdentifiers = new HashSet<>();
1:     Map<String, String> indexFiles = segment.getCommittedIndexFile();
1:     for (Map.Entry<String, String> indexFileEntry : indexFiles.entrySet()) {
1:       Path indexFile = new Path(indexFileEntry.getKey());
1:       tableBlockIndexUniqueIdentifiers.add(
1:           new TableBlockIndexUniqueIdentifier(indexFile.getParent().toString(), indexFile.getName(),
1:               indexFileEntry.getValue(), segment.getSegmentNo()));
1:     }
1:     return tableBlockIndexUniqueIdentifiers;
1:   }
1: 
1:   /**
1:    * This method will filter out the TableBlockIndexUniqueIdentifier belongs to that distributable
1:    *
1:    * @param tableBlockIndexUniqueIdentifiers
1:    * @param distributable
1:    * @return
1:    */
1:   public static TableBlockIndexUniqueIdentifier filterIdentifiersBasedOnDistributable(
1:       Set<TableBlockIndexUniqueIdentifier> tableBlockIndexUniqueIdentifiers,
1:       BlockletDataMapDistributable distributable) {
1:     TableBlockIndexUniqueIdentifier validIdentifier = null;
1:     String fileName = CarbonTablePath.DataFileUtil.getFileName(distributable.getFilePath());
1:     for (TableBlockIndexUniqueIdentifier tableBlockIndexUniqueIdentifier :
1:         tableBlockIndexUniqueIdentifiers) {
1:       if (fileName.equals(tableBlockIndexUniqueIdentifier.getIndexFileName())) {
1:         validIdentifier = tableBlockIndexUniqueIdentifier;
1:         break;
1:       }
1:     }
1:     return validIdentifier;
1:   }
1: 
1:   /**
1:    * This method will the index files tableBlockIndexUniqueIdentifiers of a merge index file
1:    *
1:    * @param identifier
1:    * @return
1:    * @throws IOException
1:    */
1:   public static List<TableBlockIndexUniqueIdentifier> getIndexFileIdentifiersFromMergeFile(
1:       TableBlockIndexUniqueIdentifier identifier, SegmentIndexFileStore segmentIndexFileStore)
1:       throws IOException {
1:     List<TableBlockIndexUniqueIdentifier> tableBlockIndexUniqueIdentifiers = new ArrayList<>();
1:     String mergeFilePath =
1:         identifier.getIndexFilePath() + CarbonCommonConstants.FILE_SEPARATOR + identifier
1:             .getIndexFileName();
1:     segmentIndexFileStore.readMergeFile(mergeFilePath);
1:     List<String> indexFiles =
1:         segmentIndexFileStore.getCarbonMergeFileToIndexFilesMap().get(mergeFilePath);
1:     for (String indexFile : indexFiles) {
1:       tableBlockIndexUniqueIdentifiers.add(
1:           new TableBlockIndexUniqueIdentifier(identifier.getIndexFilePath(), indexFile,
1:               identifier.getIndexFileName(), identifier.getSegmentId()));
1:     }
1:     return tableBlockIndexUniqueIdentifiers;
1:   }
1: 
1: }
author:ajantha-bhat
-------------------------------------------------------------------------------
commit:5f68a79
/////////////////////////////////////////////////////////////////////////
1: import org.apache.carbondata.core.indexstore.TableBlockIndexUniqueIdentifierWrapper;
1: import org.apache.carbondata.core.metadata.schema.table.CarbonTable;
1: import org.apache.carbondata.core.metadata.schema.table.column.ColumnSchema;
1: import org.apache.commons.logging.Log;
1: import org.apache.commons.logging.LogFactory;
1:   private static final Log LOG = LogFactory.getLog(BlockletDataMapUtil.class);
1: 
1:       TableBlockIndexUniqueIdentifierWrapper identifierWrapper,
1:       SegmentIndexFileStore indexFileStore, Set<String> filesRead,
1:       Map<String, BlockMetaInfo> fileNameToMetaInfoMapping) throws IOException {
1:     boolean isTransactionalTable = true;
1:     TableBlockIndexUniqueIdentifier identifier =
1:         identifierWrapper.getTableBlockIndexUniqueIdentifier();
1:     List<ColumnSchema> tableColumnList = null;
/////////////////////////////////////////////////////////////////////////
1:     CarbonTable carbonTable = identifierWrapper.getCarbonTable();
1:     if (carbonTable != null) {
1:       isTransactionalTable = carbonTable.getTableInfo().isTransactionalTable();
1:       tableColumnList =
1:           carbonTable.getTableInfo().getFactTable().getListOfColumns();
1:     }
1:       if ((!isTransactionalTable) && (tableColumnList.size() != 0) &&
1:           !isSameColumnSchemaList(footer.getColumnInTable(), tableColumnList)) {
1:         LOG.error("Schema of " + identifier.getIndexFileName()
1:             + " doesn't match with the table's schema");
1:         throw new IOException("All the files doesn't have same schema. "
1:             + "Unsupported operation on nonTransactional table. Check logs.");
1:       }
1:       if ((tableColumnList != null) && (tableColumnList.size() == 0)) {
1:         // Carbon reader have used dummy columnSchema. Update it with inferred schema now
1:         carbonTable.getTableInfo().getFactTable().setListOfColumns(footer.getColumnInTable());
1:         CarbonTable.updateTableByTableInfo(carbonTable, carbonTable.getTableInfo());
1:       }
/////////////////////////////////////////////////////////////////////////
1:    * @param segmentIndexFileStore
/////////////////////////////////////////////////////////////////////////
0:   private static boolean isSameColumnSchemaList(List<ColumnSchema> indexFileColumnList,
1:       List<ColumnSchema> tableColumnList) {
1:     if (indexFileColumnList.size() != tableColumnList.size()) {
1:       LOG.error("Index file's column size is " + indexFileColumnList.size()
1:           + " but table's column size is " + tableColumnList.size());
1:       return false;
1:     }
1:     for (int i = 0; i < tableColumnList.size(); i++) {
0:       if (!indexFileColumnList.get(i).equalsWithStrictCheck(tableColumnList.get(i))) {
1:         return false;
1:       }
1:     }
0:     return true;
1:   }
============================================================================