1:2cf1104: /*
1:2cf1104:  * Licensed to the Apache Software Foundation (ASF) under one or more
1:2cf1104:  * contributor license agreements.  See the NOTICE file distributed with
1:2cf1104:  * this work for additional information regarding copyright ownership.
1:2cf1104:  * The ASF licenses this file to You under the Apache License, Version 2.0
1:2cf1104:  * (the "License"); you may not use this file except in compliance with
1:2cf1104:  * the License.  You may obtain a copy of the License at
2:2cf1104:  *
1:2cf1104:  *    http://www.apache.org/licenses/LICENSE-2.0
1:2cf1104:  *
1:2cf1104:  * Unless required by applicable law or agreed to in writing, software
1:2cf1104:  * distributed under the License is distributed on an "AS IS" BASIS,
1:2cf1104:  * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
1:2cf1104:  * See the License for the specific language governing permissions and
1:2cf1104:  * limitations under the License.
2:2cf1104:  */
1:2cf1104: package org.apache.carbondata.core.datastore.chunk.reader.dimension;
2:2cf1104: 
1:2cf1104: import java.io.IOException;
1:2cf1104: import java.util.List;
1:2cf1104: 
1:daa6465: import org.apache.carbondata.core.datastore.FileReader;
1:2cf1104: import org.apache.carbondata.core.datastore.chunk.impl.DimensionRawColumnChunk;
1:2cf1104: import org.apache.carbondata.core.metadata.blocklet.BlockletInfo;
1:2cf1104: 
2:2cf1104: /**
1:2cf1104:  * Abstract class for V2, V3 format dimension column reader
1:2cf1104:  */
1:2cf1104: public abstract class AbstractChunkReaderV2V3Format extends AbstractChunkReader {
1:2cf1104: 
1:2cf1104:   /**
1:2cf1104:    * dimension chunks offset
1:2cf1104:    */
1:2cf1104:   protected List<Long> dimensionChunksOffset;
1:2cf1104: 
1:2cf1104:   /**
1:2cf1104:    * dimension chunks length
1:2cf1104:    */
1:2cf1104:   protected List<Integer> dimensionChunksLength;
1:2cf1104: 
1:2cf1104:   public AbstractChunkReaderV2V3Format(final BlockletInfo blockletInfo,
1:2cf1104:       final int[] eachColumnValueSize, final String filePath) {
1:2cf1104:     super(eachColumnValueSize, filePath, blockletInfo.getNumberOfRows());
1:2cf1104:     dimensionChunksOffset = blockletInfo.getDimensionChunkOffsets();
1:2cf1104:     dimensionChunksLength = blockletInfo.getDimensionChunksLength();
2:2cf1104:   }
1:2cf1104: 
1:2cf1104:   /**
1:2cf1104:    * Below method will be used to read the chunk based on block indexes
1:2cf1104:    * Reading logic of below method is:
1:2cf1104:    * Except last column all the column chunk can be read in group
1:2cf1104:    * if not last column then read data of all the column present in block index
1:2cf1104:    * together then process it.
1:2cf1104:    * For last column read is separately and process
1:2cf1104:    *
1:2cf1104:    * @param fileReader      file reader to read the blocks from file
1:daa6465:    * @param columnIndexRange column index range to be read
1:2cf1104:    * @return dimension column chunks
1:2cf1104:    */
1:daa6465:   @Override public DimensionRawColumnChunk[] readRawDimensionChunks(final FileReader fileReader,
1:daa6465:       final int[][] columnIndexRange) throws IOException {
1:2cf1104:     // read the column chunk based on block index and add
1:2cf1104:     DimensionRawColumnChunk[] dataChunks =
1:2cf1104:         new DimensionRawColumnChunk[dimensionChunksOffset.size()];
1:2cf1104:     // if blocklet index is empty then return empry data chunk
1:daa6465:     if (columnIndexRange.length == 0) {
1:2cf1104:       return dataChunks;
1:2cf1104:     }
1:2cf1104:     DimensionRawColumnChunk[] groupChunk = null;
1:2cf1104:     int index = 0;
1:2cf1104:     // iterate till block indexes -1 as block index will be in sorted order, so to avoid
1:2cf1104:     // the last column reading in group
1:daa6465:     for (int i = 0; i < columnIndexRange.length - 1; i++) {
1:2cf1104:       index = 0;
2:2cf1104:       groupChunk =
1:daa6465:           readRawDimensionChunksInGroup(fileReader, columnIndexRange[i][0], columnIndexRange[i][1]);
1:daa6465:       for (int j = columnIndexRange[i][0]; j <= columnIndexRange[i][1]; j++) {
1:2cf1104:         dataChunks[j] = groupChunk[index++];
1:2cf1104:       }
1:2cf1104:     }
1:2cf1104:     // check last index is present in block index, if it is present then read separately
1:daa6465:     if (columnIndexRange[columnIndexRange.length - 1][0] == dimensionChunksOffset.size() - 1) {
1:daa6465:       dataChunks[columnIndexRange[columnIndexRange.length - 1][0]] =
1:daa6465:           readRawDimensionChunk(fileReader, columnIndexRange[columnIndexRange.length - 1][0]);
1:2cf1104:     }
1:2cf1104:     // otherwise read the data in group
1:2cf1104:     else {
1:daa6465:       groupChunk = readRawDimensionChunksInGroup(
1:daa6465:           fileReader, columnIndexRange[columnIndexRange.length - 1][0],
1:daa6465:           columnIndexRange[columnIndexRange.length - 1][1]);
1:2cf1104:       index = 0;
1:daa6465:       for (int j = columnIndexRange[columnIndexRange.length - 1][0];
1:daa6465:            j <= columnIndexRange[columnIndexRange.length - 1][1]; j++) {
1:2cf1104:         dataChunks[j] = groupChunk[index++];
1:2cf1104:       }
1:2cf1104:     }
1:2cf1104:     return dataChunks;
1:2cf1104:   }
1:2cf1104: 
1:2cf1104:   /**
1:2cf1104:    * Below method will be used to read measure chunk data in group.
1:2cf1104:    * This method will be useful to avoid multiple IO while reading the
1:2cf1104:    * data from
1:2cf1104:    *
1:2cf1104:    * @param fileReader               file reader to read the data
1:2cf1104:    * @param startColumnBlockletIndex first column blocklet index to be read
1:2cf1104:    * @param endColumnBlockletIndex   end column blocklet index to be read
1:2cf1104:    * @return measure raw chunkArray
1:2cf1104:    * @throws IOException
1:2cf1104:    */
1:daa6465:   protected abstract DimensionRawColumnChunk[] readRawDimensionChunksInGroup(FileReader fileReader,
1:2cf1104:       int startColumnBlockletIndex, int endColumnBlockletIndex) throws IOException;
1:2cf1104: 
1:2cf1104: }
============================================================================
author:akashrn5
-------------------------------------------------------------------------------
commit:2ccdbb7
/////////////////////////////////////////////////////////////////////////
/////////////////////////////////////////////////////////////////////////
author:Jacky Li
-------------------------------------------------------------------------------
commit:daa6465
/////////////////////////////////////////////////////////////////////////
1: import org.apache.carbondata.core.datastore.FileReader;
/////////////////////////////////////////////////////////////////////////
1:    * @param columnIndexRange column index range to be read
1:   @Override public DimensionRawColumnChunk[] readRawDimensionChunks(final FileReader fileReader,
1:       final int[][] columnIndexRange) throws IOException {
1:     if (columnIndexRange.length == 0) {
1:     for (int i = 0; i < columnIndexRange.length - 1; i++) {
1:           readRawDimensionChunksInGroup(fileReader, columnIndexRange[i][0], columnIndexRange[i][1]);
1:       for (int j = columnIndexRange[i][0]; j <= columnIndexRange[i][1]; j++) {
1:     if (columnIndexRange[columnIndexRange.length - 1][0] == dimensionChunksOffset.size() - 1) {
1:       dataChunks[columnIndexRange[columnIndexRange.length - 1][0]] =
1:           readRawDimensionChunk(fileReader, columnIndexRange[columnIndexRange.length - 1][0]);
1:       groupChunk = readRawDimensionChunksInGroup(
1:           fileReader, columnIndexRange[columnIndexRange.length - 1][0],
1:           columnIndexRange[columnIndexRange.length - 1][1]);
1:       for (int j = columnIndexRange[columnIndexRange.length - 1][0];
1:            j <= columnIndexRange[columnIndexRange.length - 1][1]; j++) {
/////////////////////////////////////////////////////////////////////////
1:   protected abstract DimensionRawColumnChunk[] readRawDimensionChunksInGroup(FileReader fileReader,
author:kumarvishal
-------------------------------------------------------------------------------
commit:2cf1104
/////////////////////////////////////////////////////////////////////////
1: /*
1:  * Licensed to the Apache Software Foundation (ASF) under one or more
1:  * contributor license agreements.  See the NOTICE file distributed with
1:  * this work for additional information regarding copyright ownership.
1:  * The ASF licenses this file to You under the Apache License, Version 2.0
1:  * (the "License"); you may not use this file except in compliance with
1:  * the License.  You may obtain a copy of the License at
1:  *
1:  *    http://www.apache.org/licenses/LICENSE-2.0
1:  *
1:  * Unless required by applicable law or agreed to in writing, software
1:  * distributed under the License is distributed on an "AS IS" BASIS,
1:  * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
1:  * See the License for the specific language governing permissions and
1:  * limitations under the License.
1:  */
1: package org.apache.carbondata.core.datastore.chunk.reader.dimension;
1: 
1: import java.io.IOException;
1: import java.util.List;
1: 
0: import org.apache.carbondata.core.datastore.FileHolder;
1: import org.apache.carbondata.core.datastore.chunk.impl.DimensionRawColumnChunk;
1: import org.apache.carbondata.core.metadata.blocklet.BlockletInfo;
0: import org.apache.carbondata.format.Encoding;
1: 
1: /**
1:  * Abstract class for V2, V3 format dimension column reader
1:  */
1: public abstract class AbstractChunkReaderV2V3Format extends AbstractChunkReader {
1: 
1:   /**
1:    * dimension chunks offset
1:    */
1:   protected List<Long> dimensionChunksOffset;
1: 
1:   /**
1:    * dimension chunks length
1:    */
1:   protected List<Integer> dimensionChunksLength;
1: 
1:   public AbstractChunkReaderV2V3Format(final BlockletInfo blockletInfo,
1:       final int[] eachColumnValueSize, final String filePath) {
1:     super(eachColumnValueSize, filePath, blockletInfo.getNumberOfRows());
1:     dimensionChunksOffset = blockletInfo.getDimensionChunkOffsets();
1:     dimensionChunksLength = blockletInfo.getDimensionChunksLength();
1:   }
1: 
1:   /**
1:    * Below method will be used to read the chunk based on block indexes
1:    * Reading logic of below method is:
1:    * Except last column all the column chunk can be read in group
1:    * if not last column then read data of all the column present in block index
1:    * together then process it.
1:    * For last column read is separately and process
1:    *
1:    * @param fileReader      file reader to read the blocks from file
0:    * @param blockletIndexes blocks range to be read
1:    * @return dimension column chunks
1:    */
0:   @Override public DimensionRawColumnChunk[] readRawDimensionChunks(final FileHolder fileReader,
0:       final int[][] blockletIndexes) throws IOException {
1:     // read the column chunk based on block index and add
1:     DimensionRawColumnChunk[] dataChunks =
1:         new DimensionRawColumnChunk[dimensionChunksOffset.size()];
1:     // if blocklet index is empty then return empry data chunk
0:     if (blockletIndexes.length == 0) {
1:       return dataChunks;
1:     }
1:     DimensionRawColumnChunk[] groupChunk = null;
1:     int index = 0;
1:     // iterate till block indexes -1 as block index will be in sorted order, so to avoid
1:     // the last column reading in group
0:     for (int i = 0; i < blockletIndexes.length - 1; i++) {
1:       index = 0;
1:       groupChunk =
0:           readRawDimensionChunksInGroup(fileReader, blockletIndexes[i][0], blockletIndexes[i][1]);
0:       for (int j = blockletIndexes[i][0]; j <= blockletIndexes[i][1]; j++) {
1:         dataChunks[j] = groupChunk[index++];
1:       }
1:     }
1:     // check last index is present in block index, if it is present then read separately
0:     if (blockletIndexes[blockletIndexes.length - 1][0] == dimensionChunksOffset.size() - 1) {
0:       dataChunks[blockletIndexes[blockletIndexes.length - 1][0]] =
0:           readRawDimensionChunk(fileReader, blockletIndexes[blockletIndexes.length - 1][0]);
1:     }
1:     // otherwise read the data in group
1:     else {
1:       groupChunk =
0:           readRawDimensionChunksInGroup(fileReader, blockletIndexes[blockletIndexes.length - 1][0],
0:               blockletIndexes[blockletIndexes.length - 1][1]);
1:       index = 0;
0:       for (int j = blockletIndexes[blockletIndexes.length - 1][0];
0:            j <= blockletIndexes[blockletIndexes.length - 1][1]; j++) {
1:         dataChunks[j] = groupChunk[index++];
1:       }
1:     }
1:     return dataChunks;
1:   }
1: 
1:   /**
1:    * Below method will be used to read measure chunk data in group.
1:    * This method will be useful to avoid multiple IO while reading the
1:    * data from
1:    *
1:    * @param fileReader               file reader to read the data
1:    * @param startColumnBlockletIndex first column blocklet index to be read
1:    * @param endColumnBlockletIndex   end column blocklet index to be read
1:    * @return measure raw chunkArray
1:    * @throws IOException
1:    */
0:   protected abstract DimensionRawColumnChunk[] readRawDimensionChunksInGroup(FileHolder fileReader,
1:       int startColumnBlockletIndex, int endColumnBlockletIndex) throws IOException;
1: 
1:   /**
0:    * Below method will be used to check whether particular encoding is present
0:    * in the dimension or not
1:    *
0:    * @param encoding encoding to search
0:    * @return if encoding is present in dimension
1:    */
0:   protected boolean hasEncoding(List<Encoding> encodings, Encoding encoding) {
0:     return encodings.contains(encoding);
1:   }
1: 
1: }
============================================================================