1:e39b0a1: /*
1:e39b0a1:  * Licensed to the Apache Software Foundation (ASF) under one or more
1:e39b0a1:  * contributor license agreements.  See the NOTICE file distributed with
1:e39b0a1:  * this work for additional information regarding copyright ownership.
1:e39b0a1:  * The ASF licenses this file to You under the Apache License, Version 2.0
1:e39b0a1:  * (the "License"); you may not use this file except in compliance with
1:e39b0a1:  * the License.  You may obtain a copy of the License at
1:e39b0a1:  *
1:e39b0a1:  *    http://www.apache.org/licenses/LICENSE-2.0
1:e39b0a1:  *
1:e39b0a1:  * Unless required by applicable law or agreed to in writing, software
1:e39b0a1:  * distributed under the License is distributed on an "AS IS" BASIS,
1:e39b0a1:  * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
1:e39b0a1:  * See the License for the specific language governing permissions and
1:e39b0a1:  * limitations under the License.
1:e39b0a1:  */
1:3ea2a1d: 
1:e39b0a1: package org.apache.carbondata.sdk.file;
4:e39b0a1: 
1:e39b0a1: import java.io.IOException;
1:b588cb6: import java.math.BigDecimal;
1:9ebab57: import java.math.BigInteger;
1:b588cb6: import java.nio.ByteBuffer;
1:b1c85fa: import java.util.ArrayList;
1:fb6dffe: import java.util.HashMap;
1:fb6dffe: import java.util.Iterator;
1:e39b0a1: import java.util.List;
1:fb6dffe: import java.util.Map;
1:e39b0a1: import java.util.Random;
1:e39b0a1: import java.util.UUID;
1:e39b0a1: 
1:e39b0a1: import org.apache.carbondata.common.annotations.InterfaceAudience;
1:2f23486: import org.apache.carbondata.common.logging.LogService;
1:2f23486: import org.apache.carbondata.common.logging.LogServiceFactory;
1:2f23486: import org.apache.carbondata.core.keygenerator.directdictionary.timestamp.DateDirectDictionaryGenerator;
1:cf3e919: import org.apache.carbondata.core.metadata.datatype.DataType;
1:cf3e919: import org.apache.carbondata.core.metadata.datatype.DataTypes;
1:fb6dffe: import org.apache.carbondata.core.metadata.datatype.MapType;
1:cf3e919: import org.apache.carbondata.core.metadata.datatype.StructField;
1:fb6dffe: import org.apache.carbondata.core.metadata.datatype.StructType;
1:2f23486: import org.apache.carbondata.core.metadata.schema.table.CarbonTable;
1:e39b0a1: import org.apache.carbondata.hadoop.api.CarbonTableOutputFormat;
1:e39b0a1: import org.apache.carbondata.hadoop.internal.ObjectArrayWritable;
1:ec33c11: import org.apache.carbondata.processing.loading.complexobjects.ArrayObject;
1:ec33c11: import org.apache.carbondata.processing.loading.complexobjects.StructObject;
1:9ebab57: import org.apache.carbondata.processing.loading.exception.CarbonDataLoadingException;
1:e39b0a1: import org.apache.carbondata.processing.loading.model.CarbonLoadModel;
1:e39b0a1: 
1:2f23486: import org.apache.avro.LogicalType;
1:2f23486: import org.apache.avro.LogicalTypes;
1:e39b0a1: import org.apache.avro.Schema;
1:e39b0a1: import org.apache.avro.generic.GenericData;
1:b588cb6: import org.apache.avro.util.Utf8;
1:e39b0a1: import org.apache.hadoop.conf.Configuration;
1:e39b0a1: import org.apache.hadoop.io.NullWritable;
1:e39b0a1: import org.apache.hadoop.mapreduce.JobID;
1:e39b0a1: import org.apache.hadoop.mapreduce.RecordWriter;
1:e39b0a1: import org.apache.hadoop.mapreduce.TaskAttemptContext;
1:e39b0a1: import org.apache.hadoop.mapreduce.TaskAttemptID;
1:e39b0a1: import org.apache.hadoop.mapreduce.TaskID;
1:e39b0a1: import org.apache.hadoop.mapreduce.TaskType;
1:e39b0a1: import org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl;
1:e39b0a1: 
1:e39b0a1: /**
1:e39b0a1:  * Writer Implementation to write Avro Record to carbondata file.
1:e39b0a1:  */
1:e39b0a1: @InterfaceAudience.Internal
1:cf3e919: public class AvroCarbonWriter extends CarbonWriter {
1:e39b0a1: 
1:e39b0a1:   private RecordWriter<NullWritable, ObjectArrayWritable> recordWriter;
1:e39b0a1:   private TaskAttemptContext context;
1:e39b0a1:   private ObjectArrayWritable writable;
1:e39b0a1:   private Schema avroSchema;
1:2f23486:   private static final LogService LOGGER =
1:2f23486:       LogServiceFactory.getLogService(CarbonTable.class.getName());
1:e39b0a1: 
1:8f1a029:   AvroCarbonWriter(CarbonLoadModel loadModel, Configuration hadoopConf) throws IOException {
1:e39b0a1:     CarbonTableOutputFormat.setLoadModel(hadoopConf, loadModel);
1:e39b0a1:     CarbonTableOutputFormat format = new CarbonTableOutputFormat();
1:e39b0a1:     JobID jobId = new JobID(UUID.randomUUID().toString(), 0);
1:e39b0a1:     Random random = new Random();
1:e39b0a1:     TaskID task = new TaskID(jobId, TaskType.MAP, random.nextInt());
1:e39b0a1:     TaskAttemptID attemptID = new TaskAttemptID(task, random.nextInt());
1:e39b0a1:     TaskAttemptContextImpl context = new TaskAttemptContextImpl(hadoopConf, attemptID);
1:e39b0a1:     this.recordWriter = format.getRecordWriter(context);
1:e39b0a1:     this.context = context;
1:e39b0a1:     this.writable = new ObjectArrayWritable();
1:3202cf5:   }
1:e39b0a1: 
1:3202cf5:   private Object[] avroToCsv(GenericData.Record avroRecord) {
1:e39b0a1:     if (avroSchema == null) {
1:e39b0a1:       avroSchema = avroRecord.getSchema();
1:cf3e919:     }
1:e39b0a1:     List<Schema.Field> fields = avroSchema.getFields();
1:3d8b085:     List<Object> csvFields = new ArrayList<>();
1:e39b0a1:     for (int i = 0; i < fields.size(); i++) {
1:3d8b085:       Object field = avroFieldToObject(fields.get(i), avroRecord.get(i));
1:3d8b085:       if (field != null) {
1:3d8b085:         csvFields.add(field);
1:3d8b085:       }
1:3202cf5:     }
1:3d8b085:     return csvFields.toArray();
11:e39b0a1:   }
1:e39b0a1: 
1:ec33c11:   private Object avroFieldToObject(Schema.Field avroField, Object fieldValue) {
1:9ebab57:     Object out = null;
1:b1c85fa:     Schema.Type type = avroField.schema().getType();
1:2f23486:     LogicalType logicalType = avroField.schema().getLogicalType();
1:e39b0a1:     switch (type) {
1:fb6dffe:       case MAP:
1:fb6dffe:         // Note: Avro object takes care of removing the duplicates so we should not handle it again
1:fb6dffe:         // Map will be internally stored as Array<Struct<Key,Value>>
1:fb6dffe:         Map mapEntries = (HashMap) fieldValue;
1:fb6dffe:         Object[] arrayMapChildObjects = new Object[mapEntries.size()];
1:fb6dffe:         if (!mapEntries.isEmpty()) {
1:fb6dffe:           Iterator iterator = mapEntries.entrySet().iterator();
1:fb6dffe:           int counter = 0;
1:fb6dffe:           while (iterator.hasNext()) {
1:fb6dffe:             // size is 2 because map will have key and value
1:fb6dffe:             Object[] mapChildObjects = new Object[2];
1:9ebab57:             Map.Entry mapEntry = (Map.Entry) iterator.next();
1:fb6dffe:             // evaluate key
1:fb6dffe:             Object keyObject = avroFieldToObject(
1:fb6dffe:                 new Schema.Field(avroField.name(), Schema.create(Schema.Type.STRING),
1:fb6dffe:                     avroField.doc(), avroField.defaultVal()), mapEntry.getKey());
1:fb6dffe:             // evaluate value
1:fb6dffe:             Object valueObject = avroFieldToObject(
1:fb6dffe:                 new Schema.Field(avroField.name(), avroField.schema().getValueType(),
1:fb6dffe:                     avroField.doc(), avroField.defaultVal()), mapEntry.getValue());
1:fb6dffe:             if (keyObject != null) {
1:fb6dffe:               mapChildObjects[0] = keyObject;
1:b588cb6:             }
1:fb6dffe:             if (valueObject != null) {
1:fb6dffe:               mapChildObjects[1] = valueObject;
1:fb6dffe:             }
1:fb6dffe:             StructObject keyValueObject = new StructObject(mapChildObjects);
1:fb6dffe:             arrayMapChildObjects[counter++] = keyValueObject;
1:fb6dffe:           }
1:fb6dffe:         }
1:fb6dffe:         out = new ArrayObject(arrayMapChildObjects);
1:b588cb6:         break;
1:3202cf5:       case RECORD:
1:b1c85fa:         List<Schema.Field> fields = avroField.schema().getFields();
1:b1c85fa: 
1:ec33c11:         Object[] structChildObjects = new Object[fields.size()];
1:b1c85fa:         for (int i = 0; i < fields.size(); i++) {
1:3d8b085:           Object childObject =
1:ec33c11:               avroFieldToObject(fields.get(i), ((GenericData.Record) fieldValue).get(i));
1:3d8b085:           if (childObject != null) {
1:3d8b085:             structChildObjects[i] = childObject;
1:3d8b085:           }
1:b588cb6:         }
1:ec33c11:         StructObject structObject = new StructObject(structChildObjects);
1:ec33c11:         out = structObject;
1:b588cb6:         break;
1:b1c85fa:       case ARRAY:
1:3d8b085:         Object[] arrayChildObjects;
1:3d8b085:         if (fieldValue instanceof GenericData.Array) {
1:3d8b085:           int size = ((GenericData.Array) fieldValue).size();
1:3d8b085:           arrayChildObjects = new Object[size];
1:3d8b085:           for (int i = 0; i < size; i++) {
1:3d8b085:             Object childObject = avroFieldToObject(
1:cf1b50b:                 new Schema.Field(avroField.name(), avroField.schema().getElementType(),
1:cf1b50b:                     avroField.doc(), avroField.defaultVal()),
1:3d8b085:                 ((GenericData.Array) fieldValue).get(i));
1:3d8b085:             if (childObject != null) {
1:3d8b085:               arrayChildObjects[i] = childObject;
1:3d8b085:             }
1:3d8b085:           }
1:3d8b085:         } else {
1:3d8b085:           int size = ((ArrayList) fieldValue).size();
1:3d8b085:           arrayChildObjects = new Object[size];
1:3d8b085:           for (int i = 0; i < size; i++) {
1:3d8b085:             Object childObject = avroFieldToObject(
1:cf1b50b:                 new Schema.Field(avroField.name(), avroField.schema().getElementType(),
1:cf1b50b:                     avroField.doc(), avroField.defaultVal()), ((ArrayList) fieldValue).get(i));
1:3d8b085:             if (childObject != null) {
1:3d8b085:               arrayChildObjects[i] = childObject;
1:3d8b085:             }
1:3d8b085:           }
1:b588cb6:         }
1:3d8b085:         out = new ArrayObject(arrayChildObjects);
1:b588cb6:         break;
1:b588cb6:       case UNION:
1:b588cb6:         // Union type will be internally stored as Struct<col:type>
1:b588cb6:         // Fill data object only if fieldvalue is instance of datatype
1:b588cb6:         // For other field datatypes, fill value as Null
1:b588cb6:         List<Schema> unionFields = avroField.schema().getTypes();
1:b588cb6:         int notNullUnionFieldsCount = 0;
1:b588cb6:         for (Schema unionField : unionFields) {
1:b588cb6:           if (!unionField.getType().equals(Schema.Type.NULL)) {
1:b588cb6:             notNullUnionFieldsCount++;
1:b588cb6:           }
1:b588cb6:         }
1:b588cb6:         Object[] values = new Object[notNullUnionFieldsCount];
1:b588cb6:         int j = 0;
1:b588cb6:         for (Schema unionField : unionFields) {
1:b588cb6:           if (unionField.getType().equals(Schema.Type.NULL)) {
1:b588cb6:             continue;
1:b588cb6:           }
1:9ebab57:           // Union may not contain more than one schema with the same type,
1:9ebab57:           // except for the named types record,fixed and enum
1:9ebab57:           // hence check for schema also in case of union of multiple record or enum or fixed type
1:9ebab57:           if (validateUnionFieldValue(unionField.getType(), fieldValue, unionField)) {
1:b588cb6:             values[j] = avroFieldToObjectForUnionType(unionField, fieldValue, avroField);
1:b588cb6:             break;
1:b588cb6:           }
1:b588cb6:           j++;
1:b588cb6:         }
1:b588cb6:         out = new StructObject(values);
1:b588cb6:         break;
1:9ebab57:       case BYTES:
1:9ebab57:         // DECIMAL type is defined in Avro as a BYTE type with the logicalType property
1:9ebab57:         // set to "decimal" and a specified precision and scale
1:9ebab57:         if (logicalType instanceof LogicalTypes.Decimal) {
1:9ebab57:           out = extractDecimalValue(fieldValue,
1:9ebab57:               ((LogicalTypes.Decimal) avroField.schema().getLogicalType()).getScale(),
1:9ebab57:               ((LogicalTypes.Decimal) avroField.schema().getLogicalType()).getPrecision());
1:9ebab57:         }
1:9ebab57:         break;
1:b588cb6:       default:
1:b588cb6:         out = avroPrimitiveFieldToObject(type, logicalType, fieldValue);
1:b588cb6:     }
1:b588cb6:     return out;
1:b588cb6:   }
1:b588cb6: 
1:b588cb6:   /**
1:b588cb6:    * For Union type, fill data if Schema.Type is instance of fieldValue
1:b588cb6:    * and return result
1:b588cb6:    *
1:b588cb6:    * @param type
1:b588cb6:    * @param fieldValue
1:9ebab57:    * @param unionField
1:b588cb6:    * @return
1:b588cb6:    */
1:9ebab57:   private boolean validateUnionFieldValue(Schema.Type type, Object fieldValue, Schema unionField) {
1:b588cb6:     switch (type) {
1:b588cb6:       case INT:
1:b588cb6:         return (fieldValue instanceof Integer);
1:b588cb6:       case BOOLEAN:
1:b588cb6:         return (fieldValue instanceof Boolean);
1:b588cb6:       case LONG:
1:b588cb6:         return (fieldValue instanceof Long);
1:b588cb6:       case DOUBLE:
1:b588cb6:         return (fieldValue instanceof Double);
1:b588cb6:       case STRING:
1:b588cb6:         return (fieldValue instanceof Utf8 || fieldValue instanceof String);
1:b588cb6:       case FLOAT:
1:b588cb6:         return (fieldValue instanceof Float);
1:b588cb6:       case RECORD:
1:9ebab57:         return (fieldValue instanceof GenericData.Record && unionField
1:9ebab57:             .equals(((GenericData.Record) fieldValue).getSchema()));
1:b588cb6:       case ARRAY:
1:b588cb6:         return (fieldValue instanceof GenericData.Array || fieldValue instanceof ArrayList);
1:b588cb6:       case BYTES:
1:b588cb6:         return (fieldValue instanceof ByteBuffer);
1:b588cb6:       case MAP:
1:b588cb6:         return (fieldValue instanceof HashMap);
1:b588cb6:       case ENUM:
1:9ebab57:         return (fieldValue instanceof GenericData.EnumSymbol && unionField
1:9ebab57:             .equals(((GenericData.EnumSymbol) fieldValue).getSchema()));
1:b588cb6:       default:
1:b588cb6:         return false;
1:b588cb6:     }
1:b588cb6:   }
1:b588cb6: 
1:b588cb6:   private Object avroPrimitiveFieldToObject(Schema.Type type, LogicalType logicalType,
1:b588cb6:       Object fieldValue) {
1:3d8b085:     Object out;
1:b588cb6:     switch (type) {
1:b588cb6:       case INT:
1:2f23486:         if (logicalType != null) {
1:2f23486:           if (logicalType instanceof LogicalTypes.Date) {
1:2f23486:             int dateIntValue = (int) fieldValue;
1:2f23486:             out = dateIntValue * DateDirectDictionaryGenerator.MILLIS_PER_DAY;
1:2f23486:           } else {
1:2f23486:             LOGGER.warn("Actual type: INT, Logical Type: " + logicalType.getName());
1:2f23486:             out = fieldValue;
1:2f23486:           }
1:2f23486:         } else {
1:2f23486:           out = fieldValue;
1:2f23486:         }
1:2f23486:         break;
1:2f23486:       case BOOLEAN:
1:b588cb6:       case LONG:
1:2f23486:         if (logicalType != null && !(logicalType instanceof LogicalTypes.TimestampMillis)) {
1:2f23486:           if (logicalType instanceof LogicalTypes.TimestampMicros) {
1:2f23486:             long dateIntValue = (long) fieldValue;
1:2f23486:             out = dateIntValue / 1000L;
1:2f23486:           } else {
1:2f23486:             LOGGER.warn("Actual type: INT, Logical Type: " + logicalType.getName());
1:2f23486:             out = fieldValue;
1:2f23486:           }
1:2f23486:         } else {
1:2f23486:           out = fieldValue;
1:2f23486:         }
1:2f23486:         break;
1:b588cb6:       case DOUBLE:
1:b588cb6:       case STRING:
1:b588cb6:       case ENUM:
5:b588cb6:         out = fieldValue;
1:b588cb6:         break;
1:b588cb6:       case FLOAT:
1:b588cb6:         // direct conversion will change precision. So parse from string.
1:b588cb6:         // also carbon internally needs float as double
1:b588cb6:         out = Double.parseDouble(fieldValue.toString());
1:b588cb6:         break;
1:3d8b085:       case NULL:
1:3d8b085:         out = null;
1:3d8b085:         break;
1:e39b0a1:       default:
1:cf3e919:         throw new UnsupportedOperationException(
1:cf3e919:             "carbon not support " + type.toString() + " avro type yet");
1:b588cb6:     }
1:ec33c11:     return out;
1:b588cb6:   }
1:ec33c11: 
1:e39b0a1:   /**
1:b588cb6:    * fill fieldvalue for union type
1:b588cb6:    *
1:b588cb6:    * @param avroField
1:b588cb6:    * @param fieldValue
1:b588cb6:    * @param avroFields
1:b588cb6:    * @return
1:b588cb6:    */
1:b588cb6:   private Object avroFieldToObjectForUnionType(Schema avroField, Object fieldValue,
1:b588cb6:       Schema.Field avroFields) {
1:9ebab57:     Object out = null;
1:b588cb6:     Schema.Type type = avroField.getType();
1:b588cb6:     LogicalType logicalType = avroField.getLogicalType();
1:b588cb6:     switch (type) {
1:b588cb6:       case RECORD:
1:b588cb6:         if (fieldValue instanceof GenericData.Record) {
1:b588cb6:           List<Schema.Field> fields = avroField.getFields();
1:b588cb6: 
1:b588cb6:           Object[] structChildObjects = new Object[fields.size()];
1:b588cb6:           for (int i = 0; i < fields.size(); i++) {
1:b588cb6:             Object childObject =
1:b588cb6:                 avroFieldToObject(fields.get(i), ((GenericData.Record) fieldValue).get(i));
1:b588cb6:             if (childObject != null) {
1:b588cb6:               structChildObjects[i] = childObject;
1:b588cb6:             }
1:b588cb6:           }
1:b588cb6:           out = new StructObject(structChildObjects);
1:b588cb6:         } else {
1:b588cb6:           out = null;
1:b588cb6:         }
1:b588cb6:         break;
1:b588cb6:       case ARRAY:
1:b588cb6:         if (fieldValue instanceof GenericData.Array || fieldValue instanceof ArrayList) {
1:b588cb6:           Object[] arrayChildObjects;
1:b588cb6:           if (fieldValue instanceof GenericData.Array) {
1:b588cb6:             int size = ((GenericData.Array) fieldValue).size();
1:b588cb6:             arrayChildObjects = new Object[size];
1:b588cb6:             for (int i = 0; i < size; i++) {
1:b588cb6:               Object childObject = avroFieldToObject(
1:b588cb6:                   new Schema.Field(avroFields.name(), avroField.getElementType(), avroFields.doc(),
1:b588cb6:                       avroFields.defaultVal()), ((GenericData.Array) fieldValue).get(i));
1:b588cb6:               if (childObject != null) {
1:b588cb6:                 arrayChildObjects[i] = childObject;
1:b588cb6:               }
1:b588cb6:             }
1:b588cb6:           } else {
1:b588cb6:             int size = ((ArrayList) fieldValue).size();
1:b588cb6:             arrayChildObjects = new Object[size];
1:b588cb6:             for (int i = 0; i < size; i++) {
1:b588cb6:               Object childObject = avroFieldToObject(
1:b588cb6:                   new Schema.Field(avroFields.name(), avroField.getElementType(), avroFields.doc(),
1:b588cb6:                       avroFields.defaultVal()), ((ArrayList) fieldValue).get(i));
1:b588cb6:               if (childObject != null) {
1:b588cb6:                 arrayChildObjects[i] = childObject;
1:b588cb6:               }
1:b588cb6:             }
1:b588cb6:           }
1:b588cb6:           out = new ArrayObject(arrayChildObjects);
1:b588cb6:         } else {
1:b588cb6:           out = null;
1:b588cb6:         }
1:b588cb6:         break;
1:b588cb6:       case MAP:
1:b588cb6:         // Note: Avro object takes care of removing the duplicates so we should not handle it again
1:b588cb6:         // Map will be internally stored as Array<Struct<Key,Value>>
1:b588cb6:         if (fieldValue instanceof HashMap) {
1:b588cb6:           Map mapEntries = (HashMap) fieldValue;
1:b588cb6:           Object[] arrayMapChildObjects = new Object[mapEntries.size()];
1:b588cb6:           if (!mapEntries.isEmpty()) {
1:b588cb6:             Iterator iterator = mapEntries.entrySet().iterator();
1:b588cb6:             int counter = 0;
1:b588cb6:             while (iterator.hasNext()) {
1:b588cb6:               // size is 2 because map will have key and value
1:b588cb6:               Object[] mapChildObjects = new Object[2];
1:9ebab57:               Map.Entry mapEntry = (Map.Entry) iterator.next();
1:b588cb6:               // evaluate key
1:b588cb6:               Object keyObject = avroFieldToObject(
1:b588cb6:                   new Schema.Field(avroFields.name(), Schema.create(Schema.Type.STRING),
1:b588cb6:                       avroFields.doc(), avroFields.defaultVal()), mapEntry.getKey());
1:b588cb6:               // evaluate value
1:b588cb6:               Object valueObject = avroFieldToObject(
1:b588cb6:                   new Schema.Field(avroFields.name(), avroField.getValueType(), avroFields.doc(),
1:b588cb6:                       avroFields.defaultVal()), mapEntry.getValue());
1:b588cb6:               if (keyObject != null) {
1:b588cb6:                 mapChildObjects[0] = keyObject;
1:b588cb6:               }
1:b588cb6:               if (valueObject != null) {
1:b588cb6:                 mapChildObjects[1] = valueObject;
1:b588cb6:               }
1:b588cb6:               StructObject keyValueObject = new StructObject(mapChildObjects);
1:b588cb6:               arrayMapChildObjects[counter++] = keyValueObject;
1:b588cb6:             }
1:b588cb6:           }
1:b588cb6:           out = new ArrayObject(arrayMapChildObjects);
1:b588cb6:         } else {
1:b588cb6:           out = null;
1:b588cb6:         }
1:b588cb6:         break;
1:9ebab57:       case BYTES:
1:9ebab57:         // DECIMAL type is defined in Avro as a BYTE type with the logicalType property
1:9ebab57:         // set to "decimal" and a specified precision and scale
1:9ebab57:         if (logicalType instanceof LogicalTypes.Decimal) {
1:9ebab57:           out = extractDecimalValue(fieldValue,
1:9ebab57:               ((LogicalTypes.Decimal) avroField.getLogicalType()).getScale(),
1:9ebab57:               ((LogicalTypes.Decimal) avroField.getLogicalType()).getPrecision());
1:9ebab57:         }
1:9ebab57:         break;
1:b588cb6:       default:
1:b588cb6:         out = avroPrimitiveFieldToObject(type, logicalType, fieldValue);
1:b588cb6:     }
1:b588cb6:     return out;
1:b588cb6:   }
1:b588cb6: 
1:9ebab57:   private Object extractDecimalValue(Object fieldValue, int scale, int precision) {
1:9ebab57:     BigDecimal dataValue = new BigDecimal(new BigInteger(((ByteBuffer) fieldValue).array()), scale);
1:9ebab57:     if (!(dataValue.precision() > precision)) {
1:9ebab57:       return dataValue;
1:9ebab57:     } else {
1:9ebab57:       throw new CarbonDataLoadingException(
1:9ebab57:           "Data Loading failed as value Precision " + dataValue.precision()
1:9ebab57:               + " is greater than specified Precision " + precision + " in Avro Schema");
1:9ebab57:     }
1:9ebab57:   }
1:9ebab57: 
1:b588cb6:   /**
1:cf3e919:    * converts avro schema to carbon schema required by carbonWriter
1:cf3e919:    *
1:26eb2d0:    * @param avroSchema avro schema
1:cf3e919:    * @return carbon sdk schema
1:cf3e919:    */
1:cf3e919:   public static org.apache.carbondata.sdk.file.Schema getCarbonSchemaFromAvroSchema(
1:26eb2d0:       Schema avroSchema) {
1:cf3e919:     Field[] carbonField = new Field[avroSchema.getFields().size()];
1:cf3e919:     int i = 0;
1:cf3e919:     for (Schema.Field avroField : avroSchema.getFields()) {
1:3d8b085:       Field field = prepareFields(avroField);
1:3d8b085:       if (field != null) {
1:3d8b085:         carbonField[i] = field;
1:3d8b085:       }
1:cf3e919:       i++;
1:fb6dffe:     }
1:cf3e919:     return new org.apache.carbondata.sdk.file.Schema(carbonField);
1:cf3e919:   }
1:cf3e919: 
1:cf3e919:   private static Field prepareFields(Schema.Field avroField) {
1:fb6dffe:     String fieldName = avroField.name();
1:cf3e919:     Schema childSchema = avroField.schema();
1:cf3e919:     Schema.Type type = childSchema.getType();
1:2f23486:     LogicalType logicalType = childSchema.getLogicalType();
1:cf3e919:     switch (type) {
1:b588cb6:       case BOOLEAN:
1:fb6dffe:         return new Field(fieldName, DataTypes.BOOLEAN);
1:cf3e919:       case INT:
1:2f23486:         if (logicalType instanceof LogicalTypes.Date) {
1:fb6dffe:           return new Field(fieldName, DataTypes.DATE);
1:2f23486:         } else {
1:b6bd90d:           // Avro supports logical types time-millis as type INT,
1:b6bd90d:           // which will be mapped to carbon as INT data type
1:fb6dffe:           return new Field(fieldName, DataTypes.INT);
1:2f23486:         }
1:cf3e919:       case LONG:
1:2f23486:         if (logicalType instanceof LogicalTypes.TimestampMillis
1:2f23486:             || logicalType instanceof LogicalTypes.TimestampMicros) {
1:fb6dffe:           return new Field(fieldName, DataTypes.TIMESTAMP);
1:2f23486:         } else {
1:b6bd90d:           // Avro supports logical types time-micros as type LONG,
1:b6bd90d:           // which will be mapped to carbon as LONG data type
1:fb6dffe:           return new Field(fieldName, DataTypes.LONG);
1:2f23486:         }
1:cf3e919:       case DOUBLE:
1:fb6dffe:         return new Field(fieldName, DataTypes.DOUBLE);
1:b588cb6:       case ENUM:
1:cf3e919:       case STRING:
1:fb6dffe:         return new Field(fieldName, DataTypes.STRING);
1:cf3e919:       case FLOAT:
1:fb6dffe:         return new Field(fieldName, DataTypes.DOUBLE);
1:fb6dffe:       case MAP:
1:fb6dffe:         // recursively get the sub fields
1:fb6dffe:         ArrayList<StructField> mapSubFields = new ArrayList<>();
1:68b359e:         StructField mapField = prepareSubFields(fieldName, childSchema);
1:fb6dffe:         if (null != mapField) {
1:fb6dffe:           // key value field will be wrapped inside a map struct field
1:fb6dffe:           StructField keyValueField = mapField.getChildren().get(0);
1:fb6dffe:           // value dataType will be at position 1 in the fields
1:fb6dffe:           DataType valueType =
1:fb6dffe:               ((StructType) keyValueField.getDataType()).getFields().get(1).getDataType();
1:fb6dffe:           MapType mapType = DataTypes.createMapType(DataTypes.STRING, valueType);
1:fb6dffe:           mapSubFields.add(keyValueField);
1:fb6dffe:           return new Field(fieldName, mapType, mapSubFields);
1:2f23486:         }
1:b588cb6:         return null;
1:cf3e919:       case RECORD:
1:cf3e919:         // recursively get the sub fields
1:cf3e919:         ArrayList<StructField> structSubFields = new ArrayList<>();
1:cf3e919:         for (Schema.Field avroSubField : childSchema.getFields()) {
1:3d8b085:           StructField structField = prepareSubFields(avroSubField.name(), avroSubField.schema());
1:3d8b085:           if (structField != null) {
1:3d8b085:             structSubFields.add(structField);
1:2f23486:           }
1:3d8b085:         }
1:fb6dffe:         return new Field(fieldName, "struct", structSubFields);
1:cf3e919:       case ARRAY:
1:cf3e919:         // recursively get the sub fields
1:cf3e919:         ArrayList<StructField> arraySubField = new ArrayList<>();
1:cf3e919:         // array will have only one sub field.
1:3d8b085:         StructField structField = prepareSubFields("val", childSchema.getElementType());
1:3d8b085:         if (structField != null) {
1:3d8b085:           arraySubField.add(structField);
1:fb6dffe:           return new Field(fieldName, "array", arraySubField);
1:2f23486:         } else {
1:3d8b085:           return null;
1:3d8b085:         }
1:b588cb6:       case UNION:
1:b588cb6:         int i = 0;
1:b588cb6:         // Get union types and store as Struct<type>
1:b588cb6:         ArrayList<StructField> unionFields = new ArrayList<>();
1:b588cb6:         for (Schema avroSubField : avroField.schema().getTypes()) {
1:b588cb6:           if (!avroSubField.getType().equals(Schema.Type.NULL)) {
1:b588cb6:             StructField unionField = prepareSubFields(avroField.name() + i++, avroSubField);
1:b588cb6:             if (unionField != null) {
1:b588cb6:               unionFields.add(unionField);
1:3d8b085:             }
1:3d8b085:           }
1:b588cb6:         }
1:b588cb6:         if (unionFields.isEmpty()) {
1:b588cb6:           throw new UnsupportedOperationException(
1:b588cb6:               "Carbon do not support Avro UNION with only null type");
1:b588cb6:         }
1:b588cb6:         return new Field(fieldName, "struct", unionFields);
1:b588cb6:       case BYTES:
1:b588cb6:         // DECIMAL type is defined in Avro as a BYTE type with the logicalType property
1:b588cb6:         // set to "decimal" and a specified precision and scale
1:b588cb6:         if (logicalType instanceof LogicalTypes.Decimal) {
1:b588cb6:           int precision = ((LogicalTypes.Decimal) childSchema.getLogicalType()).getPrecision();
1:b588cb6:           int scale = ((LogicalTypes.Decimal) childSchema.getLogicalType()).getScale();
1:b588cb6:           return new Field(fieldName, DataTypes.createDecimalType(precision, scale));
1:2f23486:         } else {
1:9ebab57:           throw new UnsupportedOperationException(
1:9ebab57:               "carbon not support " + type.toString() + " avro type yet");
1:b588cb6:         }
1:3d8b085:       case NULL:
1:3d8b085:         return null;
1:cf3e919:       default:
1:cf3e919:         throw new UnsupportedOperationException(
1:cf3e919:             "carbon not support " + type.toString() + " avro type yet");
1:b588cb6:     }
1:b588cb6:   }
1:cf3e919: 
1:fb6dffe:   private static StructField prepareSubFields(String fieldName, Schema childSchema) {
1:cf3e919:     Schema.Type type = childSchema.getType();
1:2f23486:     LogicalType logicalType = childSchema.getLogicalType();
1:cf3e919:     switch (type) {
2:cf3e919:       case BOOLEAN:
1:fb6dffe:         return new StructField(fieldName, DataTypes.BOOLEAN);
1:cf3e919:       case INT:
1:2f23486:         if (logicalType instanceof LogicalTypes.Date) {
1:fb6dffe:           return new StructField(fieldName, DataTypes.DATE);
1:3d8b085:         } else {
1:b6bd90d:           // Avro supports logical types time-millis as type INT,
1:b6bd90d:           // which will be mapped to carbon as INT data type
1:fb6dffe:           return new StructField(fieldName, DataTypes.INT);
1:cf3e919:         }
1:cf3e919:       case LONG:
1:2f23486:         if (logicalType instanceof LogicalTypes.TimestampMillis
1:2f23486:             || logicalType instanceof LogicalTypes.TimestampMicros) {
1:fb6dffe:           return new StructField(fieldName, DataTypes.TIMESTAMP);
1:9ebab57:         } else {
1:b6bd90d:           // Avro supports logical types time-micros as type LONG,
1:b6bd90d:           // which will be mapped to carbon as LONG data type
1:fb6dffe:           return new StructField(fieldName, DataTypes.LONG);
1:cf3e919:         }
1:cf3e919:       case DOUBLE:
1:fb6dffe:         return new StructField(fieldName, DataTypes.DOUBLE);
1:b588cb6:       case ENUM:
1:cf3e919:       case STRING:
1:fb6dffe:         return new StructField(fieldName, DataTypes.STRING);
1:cf3e919:       case FLOAT:
1:fb6dffe:         return new StructField(fieldName, DataTypes.DOUBLE);
1:fb6dffe:       case MAP:
1:fb6dffe:         // recursively get the sub fields
1:fb6dffe:         ArrayList<StructField> keyValueFields = new ArrayList<>();
1:fb6dffe:         // for Avro key dataType is always fixed as String
1:fb6dffe:         StructField keyField = new StructField(fieldName + ".key", DataTypes.STRING);
1:fb6dffe:         StructField valueField = prepareSubFields(fieldName + ".value", childSchema.getValueType());
1:fb6dffe:         if (null != valueField) {
1:fb6dffe:           keyValueFields.add(keyField);
1:fb6dffe:           keyValueFields.add(valueField);
1:fb6dffe:           StructField mapKeyValueField =
1:68b359e:               new StructField(fieldName + ".val", DataTypes.createStructType(keyValueFields));
1:fb6dffe:           // value dataType will be at position 1 in the fields
1:fb6dffe:           MapType mapType =
1:fb6dffe:               DataTypes.createMapType(DataTypes.STRING, mapKeyValueField.getDataType());
1:fb6dffe:           List<StructField> mapStructFields = new ArrayList<>();
1:fb6dffe:           mapStructFields.add(mapKeyValueField);
1:fb6dffe:           return new StructField(fieldName, mapType, mapStructFields);
1:fb6dffe:         }
1:b588cb6:         return null;
1:cf3e919:       case RECORD:
1:cf3e919:         // recursively get the sub fields
1:cf3e919:         ArrayList<StructField> structSubFields = new ArrayList<>();
1:cf3e919:         for (Schema.Field avroSubField : childSchema.getFields()) {
1:3d8b085:           StructField structField = prepareSubFields(avroSubField.name(), avroSubField.schema());
1:3d8b085:           if (structField != null) {
1:3d8b085:             structSubFields.add(structField);
1:cf3e919:           }
1:cf3e919:         }
1:fb6dffe:         return (new StructField(fieldName, DataTypes.createStructType(structSubFields)));
1:cf3e919:       case ARRAY:
1:cf3e919:         // recursively get the sub fields
1:cf3e919:         // array will have only one sub field.
1:fb6dffe:         DataType subType = getMappingDataTypeForCollectionRecord(childSchema.getElementType());
1:3d8b085:         if (subType != null) {
1:fb6dffe:           return (new StructField(fieldName, DataTypes.createArrayType(subType)));
1:3d8b085:         } else {
1:3d8b085:           return null;
1:3d8b085:         }
1:b588cb6:       case UNION:
1:b588cb6:         // recursively get the union types
1:b588cb6:         int i = 0;
1:b588cb6:         ArrayList<StructField> structSubTypes = new ArrayList<>();
1:b588cb6:         for (Schema avroSubField : childSchema.getTypes()) {
1:b588cb6:           StructField structField = prepareSubFields(fieldName + i++, avroSubField);
1:3d8b085:           if (structField != null) {
1:b588cb6:             structSubTypes.add(structField);
1:b588cb6:           }
1:b588cb6:         }
1:b588cb6:         return (new StructField(fieldName, DataTypes.createStructType(structSubTypes)));
1:b588cb6:       case BYTES:
1:b588cb6:         // DECIMAL type is defined in Avro as a BYTE type with the logicalType property
1:b588cb6:         // set to "decimal" and a specified precision and scale
1:b588cb6:         if (logicalType instanceof LogicalTypes.Decimal) {
1:b588cb6:           int precision = ((LogicalTypes.Decimal) childSchema.getLogicalType()).getPrecision();
1:b588cb6:           int scale = ((LogicalTypes.Decimal) childSchema.getLogicalType()).getScale();
1:b588cb6:           return new StructField(fieldName, DataTypes.createDecimalType(precision, scale));
1:9ebab57:         } else {
1:9ebab57:           throw new UnsupportedOperationException(
1:9ebab57:               "carbon not support " + type.toString() + " avro type yet");
1:b588cb6:         }
1:3d8b085:       case NULL:
1:3d8b085:         return null;
1:cf3e919:       default:
1:cf3e919:         throw new UnsupportedOperationException(
1:cf3e919:             "carbon not support " + type.toString() + " avro type yet");
1:cf3e919:     }
1:cf3e919:   }
1:cf3e919: 
1:fb6dffe:   private static DataType getMappingDataTypeForCollectionRecord(Schema childSchema) {
1:041603d:     LogicalType logicalType = childSchema.getLogicalType();
1:cf3e919:     switch (childSchema.getType()) {
1:cf3e919:       case BOOLEAN:
1:cf3e919:         return DataTypes.BOOLEAN;
1:cf3e919:       case INT:
1:b588cb6:         if (logicalType != null) {
1:b588cb6:           if (logicalType instanceof LogicalTypes.Date) {
1:041603d:             return DataTypes.DATE;
1:b588cb6:           } else {
1:b6bd90d:             // Avro supports logical types time-millis as type INT,
1:b6bd90d:             // which will be mapped to carbon as INT data type
1:041603d:             return DataTypes.INT;
1:041603d:           }
1:b588cb6:         } else {
1:041603d:           return DataTypes.INT;
1:041603d:         }
1:cf3e919:       case LONG:
2:041603d:         if (logicalType != null) {
1:041603d:           if (logicalType instanceof LogicalTypes.TimestampMillis
1:041603d:               || logicalType instanceof LogicalTypes.TimestampMicros) {
1:041603d:             return DataTypes.TIMESTAMP;
1:b588cb6:           } else {
1:b6bd90d:             // Avro supports logical types time-micros as type LONG,
1:b6bd90d:             // which will be mapped to carbon as LONG data type
1:041603d:             return DataTypes.LONG;
1:041603d:           }
1:b588cb6:         } else {
1:041603d:           return DataTypes.LONG;
1:041603d:         }
1:cf3e919:       case DOUBLE:
1:cf3e919:         return DataTypes.DOUBLE;
1:b588cb6:       case ENUM:
1:cf3e919:       case STRING:
1:cf3e919:         return DataTypes.STRING;
1:cf3e919:       case FLOAT:
1:cf3e919:         return DataTypes.DOUBLE;
1:fb6dffe:       case MAP:
1:fb6dffe:         // recursively get the sub fields
1:fb6dffe:         StructField mapField = prepareSubFields("val", childSchema);
1:fb6dffe:         if (mapField != null) {
1:fb6dffe:           DataType ketType = ((StructType) mapField.getDataType()).getFields().get(0).getDataType();
1:fb6dffe:           DataType valueType =
1:fb6dffe:               ((StructType) mapField.getDataType()).getFields().get(1).getDataType();
1:fb6dffe:           return DataTypes.createMapType(ketType, valueType);
1:fb6dffe:         }
1:3d8b085:         return null;
1:cf3e919:       case RECORD:
1:cf3e919:         // recursively get the sub fields
1:cf3e919:         ArrayList<StructField> structSubFields = new ArrayList<>();
1:cf3e919:         for (Schema.Field avroSubField : childSchema.getFields()) {
1:3d8b085:           StructField structField = prepareSubFields(avroSubField.name(), avroSubField.schema());
1:b588cb6:           if (structField != null) {
1:3d8b085:             structSubFields.add(structField);
1:fb6dffe:           }
1:cf3e919:         }
1:cf3e919:         return DataTypes.createStructType(structSubFields);
1:cf3e919:       case ARRAY:
1:cf3e919:         // array will have only one sub field.
1:fb6dffe:         DataType subType = getMappingDataTypeForCollectionRecord(childSchema.getElementType());
1:3d8b085:         if (subType != null) {
1:3d8b085:           return DataTypes.createArrayType(subType);
1:3d8b085:         } else {
1:3d8b085:           return null;
1:3d8b085:         }
1:b588cb6:       case UNION:
1:b588cb6:         int i = 0;
1:b588cb6:         // recursively get the union types and create struct type
1:b588cb6:         ArrayList<StructField> unionFields = new ArrayList<>();
1:b588cb6:         for (Schema avroSubField : childSchema.getTypes()) {
1:b588cb6:           StructField unionField = prepareSubFields(avroSubField.getName() + i++, avroSubField);
1:b588cb6:           if (unionField != null) {
1:b588cb6:             unionFields.add(unionField);
1:b588cb6:           }
1:b588cb6:         }
1:b588cb6:         return DataTypes.createStructType(unionFields);
1:b588cb6:       case BYTES:
1:b588cb6:         // DECIMAL type is defined in Avro as a BYTE type with the logicalType property
1:b588cb6:         // set to "decimal" and a specified precision and scale
1:b588cb6:         if (logicalType instanceof LogicalTypes.Decimal) {
1:b588cb6:           int precision = ((LogicalTypes.Decimal) childSchema.getLogicalType()).getPrecision();
1:b588cb6:           int scale = ((LogicalTypes.Decimal) childSchema.getLogicalType()).getScale();
1:b588cb6:           return DataTypes.createDecimalType(precision, scale);
1:9ebab57:         } else {
1:9ebab57:           throw new UnsupportedOperationException(
1:9ebab57:               "carbon not support " + childSchema.getType().toString() + " avro type yet");
1:b588cb6:         }
1:3d8b085:       case NULL:
1:b588cb6:         return null;
1:cf3e919:       default:
1:cf3e919:         throw new UnsupportedOperationException(
1:cf3e919:             "carbon not support " + childSchema.getType().toString() + " avro type yet");
1:cf3e919:     }
1:cf3e919:   }
1:cf3e919: 
1:cf3e919:   /**
1:e39b0a1:    * Write single row data, input row is Avro Record
1:e39b0a1:    */
2:e39b0a1:   @Override
1:e39b0a1:   public void write(Object object) throws IOException {
1:e39b0a1:     try {
1:3ea2a1d:       GenericData.Record record = (GenericData.Record) object;
1:b1c85fa: 
1:3ea2a1d:       // convert Avro record to CSV String[]
1:3202cf5:       Object[] csvRecord = avroToCsv(record);
1:3ea2a1d:       writable.set(csvRecord);
1:e39b0a1:       recordWriter.write(NullWritable.get(), writable);
1:3ea2a1d:     } catch (Exception e) {
1:3ea2a1d:       close();
1:e39b0a1:       throw new IOException(e);
1:cf3e919:     }
1:b1c85fa:   }
1:e39b0a1: 
1:e39b0a1:   /**
1:e39b0a1:    * Flush and close the writer
1:e39b0a1:    */
1:3202cf5:   @Override public void close() throws IOException {
1:e39b0a1:     try {
1:e39b0a1:       recordWriter.close(context);
2:e39b0a1:     } catch (InterruptedException e) {
1:e39b0a1:       throw new IOException(e);
1:b1c85fa:     }
1:b1c85fa:   }
1:3202cf5: }
============================================================================
author:kunal642
-------------------------------------------------------------------------------
commit:8f1a029
/////////////////////////////////////////////////////////////////////////
1:   AvroCarbonWriter(CarbonLoadModel loadModel, Configuration hadoopConf) throws IOException {
commit:2f23486
/////////////////////////////////////////////////////////////////////////
1: import org.apache.carbondata.common.logging.LogService;
1: import org.apache.carbondata.common.logging.LogServiceFactory;
1: import org.apache.carbondata.core.keygenerator.directdictionary.timestamp.DateDirectDictionaryGenerator;
1: import org.apache.carbondata.core.metadata.schema.table.CarbonTable;
1: import org.apache.avro.LogicalType;
1: import org.apache.avro.LogicalTypes;
/////////////////////////////////////////////////////////////////////////
1:   private static final LogService LOGGER =
1:       LogServiceFactory.getLogService(CarbonTable.class.getName());
/////////////////////////////////////////////////////////////////////////
1:     LogicalType logicalType = avroField.schema().getLogicalType();
1:         if (logicalType != null) {
1:           if (logicalType instanceof LogicalTypes.Date) {
1:             int dateIntValue = (int) fieldValue;
1:             out = dateIntValue * DateDirectDictionaryGenerator.MILLIS_PER_DAY;
1:           } else {
1:             LOGGER.warn("Actual type: INT, Logical Type: " + logicalType.getName());
1:             out = fieldValue;
1:           }
1:         } else {
1:           out = fieldValue;
1:         }
1:         break;
1:       case BOOLEAN:
1:         if (logicalType != null && !(logicalType instanceof LogicalTypes.TimestampMillis)) {
1:           if (logicalType instanceof LogicalTypes.TimestampMicros) {
1:             long dateIntValue = (long) fieldValue;
1:             out = dateIntValue / 1000L;
1:           } else {
1:             LOGGER.warn("Actual type: INT, Logical Type: " + logicalType.getName());
1:             out = fieldValue;
1:           }
1:         } else {
1:           out = fieldValue;
1:         }
1:         break;
/////////////////////////////////////////////////////////////////////////
1:     LogicalType logicalType = childSchema.getLogicalType();
1:         if (logicalType instanceof LogicalTypes.Date) {
0:           return new Field(FieldName, DataTypes.DATE);
1:         } else {
0:           LOGGER.warn("Unsupported logical type. Considering Data Type as INT for " + childSchema
0:               .getName());
0:           return new Field(FieldName, DataTypes.INT);
1:         }
1:         if (logicalType instanceof LogicalTypes.TimestampMillis
1:             || logicalType instanceof LogicalTypes.TimestampMicros) {
0:           return new Field(FieldName, DataTypes.TIMESTAMP);
1:         } else {
0:           LOGGER.warn("Unsupported logical type. Considering Data Type as LONG for " + childSchema
0:               .getName());
0:           return new Field(FieldName, DataTypes.LONG);
1:         }
/////////////////////////////////////////////////////////////////////////
1:     LogicalType logicalType = childSchema.getLogicalType();
1:         if (logicalType instanceof LogicalTypes.Date) {
0:           return new StructField(FieldName, DataTypes.DATE);
1:         } else {
0:           LOGGER.warn("Unsupported logical type. Considering Data Type as INT for " + childSchema
0:               .getName());
0:           return new StructField(FieldName, DataTypes.INT);
1:         }
1:         if (logicalType instanceof LogicalTypes.TimestampMillis
1:             || logicalType instanceof LogicalTypes.TimestampMicros) {
0:           return new StructField(FieldName, DataTypes.TIMESTAMP);
1:         } else {
0:           LOGGER.warn("Unsupported logical type. Considering Data Type as LONG for " + childSchema
0:               .getName());
0:           return new StructField(FieldName, DataTypes.LONG);
1:         }
commit:26eb2d0
/////////////////////////////////////////////////////////////////////////
1:    * @param avroSchema avro schema
1:       Schema avroSchema) {
commit:3d8b085
/////////////////////////////////////////////////////////////////////////
1:     List<Object> csvFields = new ArrayList<>();
1:       Object field = avroFieldToObject(fields.get(i), avroRecord.get(i));
1:       if (field != null) {
1:         csvFields.add(field);
1:       }
1:     return csvFields.toArray();
1:     Object out;
/////////////////////////////////////////////////////////////////////////
1:           Object childObject =
1:           if (childObject != null) {
1:             structChildObjects[i] = childObject;
1:           }
1:         Object[] arrayChildObjects;
1:         if (fieldValue instanceof GenericData.Array) {
1:           int size = ((GenericData.Array) fieldValue).size();
1:           arrayChildObjects = new Object[size];
1:           for (int i = 0; i < size; i++) {
1:             Object childObject = avroFieldToObject(
0:                 new Schema.Field(avroField.name(), avroField.schema().getElementType(), null, true),
1:                 ((GenericData.Array) fieldValue).get(i));
1:             if (childObject != null) {
1:               arrayChildObjects[i] = childObject;
1:             }
1:           }
1:         } else {
1:           int size = ((ArrayList) fieldValue).size();
1:           arrayChildObjects = new Object[size];
1:           for (int i = 0; i < size; i++) {
1:             Object childObject = avroFieldToObject(
0:                 new Schema.Field(avroField.name(), avroField.schema().getElementType(), null, true),
0:                 ((ArrayList) fieldValue).get(i));
1:             if (childObject != null) {
1:               arrayChildObjects[i] = childObject;
1:             }
1:           }
1:         out = new ArrayObject(arrayChildObjects);
1:       case NULL:
1:         out = null;
1:         break;
/////////////////////////////////////////////////////////////////////////
1:       Field field = prepareFields(avroField);
1:       if (field != null) {
1:         carbonField[i] = field;
1:       }
/////////////////////////////////////////////////////////////////////////
1:           StructField structField = prepareSubFields(avroSubField.name(), avroSubField.schema());
1:           if (structField != null) {
1:             structSubFields.add(structField);
1:           }
1:         StructField structField = prepareSubFields("val", childSchema.getElementType());
1:         if (structField != null) {
1:           arraySubField.add(structField);
0:           return new Field(FieldName, "array", arraySubField);
1:         } else {
1:           return null;
1:         }
1:       case NULL:
1:         return null;
/////////////////////////////////////////////////////////////////////////
1:           StructField structField = prepareSubFields(avroSubField.name(), avroSubField.schema());
1:           if (structField != null) {
1:             structSubFields.add(structField);
1:           }
0:         DataType subType = getMappingDataTypeForArrayRecord(childSchema.getElementType());
1:         if (subType != null) {
0:           return (new StructField(FieldName, DataTypes.createArrayType(subType)));
1:         } else {
1:           return null;
1:         }
1:       case NULL:
1:         return null;
/////////////////////////////////////////////////////////////////////////
1:           StructField structField = prepareSubFields(avroSubField.name(), avroSubField.schema());
1:           if (structField != null) {
1:             structSubFields.add(structField);
1:           }
0:         DataType subType = getMappingDataTypeForArrayRecord(childSchema.getElementType());
1:         if (subType != null) {
1:           return DataTypes.createArrayType(subType);
1:         } else {
1:           return null;
1:         }
1:       case NULL:
1:         return null;
author:Indhumathi27
-------------------------------------------------------------------------------
commit:9ebab57
/////////////////////////////////////////////////////////////////////////
1: import java.math.BigInteger;
/////////////////////////////////////////////////////////////////////////
/////////////////////////////////////////////////////////////////////////
1: import org.apache.carbondata.processing.loading.exception.CarbonDataLoadingException;
/////////////////////////////////////////////////////////////////////////
1:     Object out = null;
/////////////////////////////////////////////////////////////////////////
1:             Map.Entry mapEntry = (Map.Entry) iterator.next();
/////////////////////////////////////////////////////////////////////////
1:           // Union may not contain more than one schema with the same type,
1:           // except for the named types record,fixed and enum
1:           // hence check for schema also in case of union of multiple record or enum or fixed type
1:           if (validateUnionFieldValue(unionField.getType(), fieldValue, unionField)) {
/////////////////////////////////////////////////////////////////////////
1:       case BYTES:
1:         // DECIMAL type is defined in Avro as a BYTE type with the logicalType property
1:         // set to "decimal" and a specified precision and scale
1:         if (logicalType instanceof LogicalTypes.Decimal) {
1:           out = extractDecimalValue(fieldValue,
1:               ((LogicalTypes.Decimal) avroField.schema().getLogicalType()).getScale(),
1:               ((LogicalTypes.Decimal) avroField.schema().getLogicalType()).getPrecision());
1:         }
1:         break;
/////////////////////////////////////////////////////////////////////////
1:    * @param unionField
1:   private boolean validateUnionFieldValue(Schema.Type type, Object fieldValue, Schema unionField) {
/////////////////////////////////////////////////////////////////////////
1:         return (fieldValue instanceof GenericData.Record && unionField
1:             .equals(((GenericData.Record) fieldValue).getSchema()));
/////////////////////////////////////////////////////////////////////////
1:         return (fieldValue instanceof GenericData.EnumSymbol && unionField
1:             .equals(((GenericData.EnumSymbol) fieldValue).getSchema()));
/////////////////////////////////////////////////////////////////////////
0:     Object out;
/////////////////////////////////////////////////////////////////////////
/////////////////////////////////////////////////////////////////////////
1:     Object out = null;
/////////////////////////////////////////////////////////////////////////
1:               Map.Entry mapEntry = (Map.Entry) iterator.next();
/////////////////////////////////////////////////////////////////////////
1:       case BYTES:
1:         // DECIMAL type is defined in Avro as a BYTE type with the logicalType property
1:         // set to "decimal" and a specified precision and scale
1:         if (logicalType instanceof LogicalTypes.Decimal) {
1:           out = extractDecimalValue(fieldValue,
1:               ((LogicalTypes.Decimal) avroField.getLogicalType()).getScale(),
1:               ((LogicalTypes.Decimal) avroField.getLogicalType()).getPrecision());
1:         }
1:         break;
1:   private Object extractDecimalValue(Object fieldValue, int scale, int precision) {
1:     BigDecimal dataValue = new BigDecimal(new BigInteger(((ByteBuffer) fieldValue).array()), scale);
1:     if (!(dataValue.precision() > precision)) {
1:       return dataValue;
1:     } else {
1:       throw new CarbonDataLoadingException(
1:           "Data Loading failed as value Precision " + dataValue.precision()
1:               + " is greater than specified Precision " + precision + " in Avro Schema");
1:     }
1:   }
1: 
/////////////////////////////////////////////////////////////////////////
1:         } else {
1:           throw new UnsupportedOperationException(
1:               "carbon not support " + type.toString() + " avro type yet");
/////////////////////////////////////////////////////////////////////////
1:         } else {
1:           throw new UnsupportedOperationException(
1:               "carbon not support " + type.toString() + " avro type yet");
/////////////////////////////////////////////////////////////////////////
1:         } else {
1:           throw new UnsupportedOperationException(
1:               "carbon not support " + childSchema.getType().toString() + " avro type yet");
commit:b6bd90d
/////////////////////////////////////////////////////////////////////////
1:           // Avro supports logical types time-millis as type INT,
1:           // which will be mapped to carbon as INT data type
/////////////////////////////////////////////////////////////////////////
1:           // Avro supports logical types time-micros as type LONG,
1:           // which will be mapped to carbon as LONG data type
/////////////////////////////////////////////////////////////////////////
1:           // Avro supports logical types time-millis as type INT,
1:           // which will be mapped to carbon as INT data type
/////////////////////////////////////////////////////////////////////////
1:           // Avro supports logical types time-micros as type LONG,
1:           // which will be mapped to carbon as LONG data type
/////////////////////////////////////////////////////////////////////////
1:             // Avro supports logical types time-millis as type INT,
1:             // which will be mapped to carbon as INT data type
/////////////////////////////////////////////////////////////////////////
1:             // Avro supports logical types time-micros as type LONG,
1:             // which will be mapped to carbon as LONG data type
commit:b588cb6
/////////////////////////////////////////////////////////////////////////
1: import java.math.BigDecimal;
1: import java.nio.ByteBuffer;
/////////////////////////////////////////////////////////////////////////
0: import org.apache.carbondata.core.constants.CarbonCommonConstants;
/////////////////////////////////////////////////////////////////////////
1: import org.apache.avro.util.Utf8;
/////////////////////////////////////////////////////////////////////////
/////////////////////////////////////////////////////////////////////////
1:       case UNION:
1:         // Union type will be internally stored as Struct<col:type>
1:         // Fill data object only if fieldvalue is instance of datatype
1:         // For other field datatypes, fill value as Null
1:         List<Schema> unionFields = avroField.schema().getTypes();
1:         int notNullUnionFieldsCount = 0;
1:         for (Schema unionField : unionFields) {
1:           if (!unionField.getType().equals(Schema.Type.NULL)) {
1:             notNullUnionFieldsCount++;
1:           }
1:         }
1:         Object[] values = new Object[notNullUnionFieldsCount];
1:         int j = 0;
1:         for (Schema unionField : unionFields) {
1:           if (unionField.getType().equals(Schema.Type.NULL)) {
1:             continue;
1:           }
0:           if (checkFieldValueType(unionField.getType(), fieldValue)) {
1:             values[j] = avroFieldToObjectForUnionType(unionField, fieldValue, avroField);
1:             break;
1:           }
1:           j++;
1:         }
1:         out = new StructObject(values);
1:         break;
1:       default:
1:         out = avroPrimitiveFieldToObject(type, logicalType, fieldValue);
1:     }
1:     return out;
1:   }
1: 
1:   /**
1:    * For Union type, fill data if Schema.Type is instance of fieldValue
1:    * and return result
1:    *
1:    * @param type
1:    * @param fieldValue
1:    * @return
1:    */
0:   private boolean checkFieldValueType(Schema.Type type, Object fieldValue) {
1:     switch (type) {
1:       case INT:
1:         return (fieldValue instanceof Integer);
1:       case BOOLEAN:
1:         return (fieldValue instanceof Boolean);
1:       case LONG:
1:         return (fieldValue instanceof Long);
1:       case DOUBLE:
1:         return (fieldValue instanceof Double);
1:       case STRING:
1:         return (fieldValue instanceof Utf8 || fieldValue instanceof String);
1:       case FLOAT:
1:         return (fieldValue instanceof Float);
1:       case RECORD:
0:         return (fieldValue instanceof GenericData.Record);
1:       case ARRAY:
1:         return (fieldValue instanceof GenericData.Array || fieldValue instanceof ArrayList);
1:       case BYTES:
1:         return (fieldValue instanceof ByteBuffer);
1:       case MAP:
1:         return (fieldValue instanceof HashMap);
1:       case ENUM:
0:         return (fieldValue instanceof GenericData.EnumSymbol);
1:       default:
1:         return false;
1:     }
1:   }
1: 
1:   private Object avroPrimitiveFieldToObject(Schema.Type type, LogicalType logicalType,
1:       Object fieldValue) {
0:     Object out = null;
1:     switch (type) {
1:       case INT:
1:         if (logicalType != null) {
1:           if (logicalType instanceof LogicalTypes.Date) {
0:             int dateIntValue = (int) fieldValue;
0:             out = dateIntValue * DateDirectDictionaryGenerator.MILLIS_PER_DAY;
1:           } else {
0:             LOGGER.warn("Actual type: INT, Logical Type: " + logicalType.getName());
1:             out = fieldValue;
1:           }
1:         } else {
1:           out = fieldValue;
1:         }
1:         break;
1:       case BOOLEAN:
1:       case LONG:
0:         if (logicalType != null && !(logicalType instanceof LogicalTypes.TimestampMillis)) {
0:           if (logicalType instanceof LogicalTypes.TimestampMicros) {
0:             long dateIntValue = (long) fieldValue;
0:             out = dateIntValue / 1000L;
1:           } else {
0:             LOGGER.warn("Actual type: INT, Logical Type: " + logicalType.getName());
1:             out = fieldValue;
1:           }
1:         } else {
1:           out = fieldValue;
1:         }
1:         break;
1:       case DOUBLE:
1:       case STRING:
1:       case ENUM:
1:         out = fieldValue;
1:         break;
1:       case FLOAT:
1:         // direct conversion will change precision. So parse from string.
1:         // also carbon internally needs float as double
1:         out = Double.parseDouble(fieldValue.toString());
1:         break;
1:       case BYTES:
1:         // DECIMAL type is defined in Avro as a BYTE type with the logicalType property
1:         // set to "decimal" and a specified precision and scale
0:         // As binary type is not supported yet,value will be null
1:         if (logicalType instanceof LogicalTypes.Decimal) {
0:           out = new BigDecimal(new String(((ByteBuffer) fieldValue).array(),
0:               CarbonCommonConstants.DEFAULT_CHARSET_CLASS));
1:         }
1:         break;
/////////////////////////////////////////////////////////////////////////
1:    * fill fieldvalue for union type
1:    *
1:    * @param avroField
1:    * @param fieldValue
1:    * @param avroFields
1:    * @return
1:    */
1:   private Object avroFieldToObjectForUnionType(Schema avroField, Object fieldValue,
1:       Schema.Field avroFields) {
0:     Object out;
1:     Schema.Type type = avroField.getType();
1:     LogicalType logicalType = avroField.getLogicalType();
1:     switch (type) {
1:       case RECORD:
1:         if (fieldValue instanceof GenericData.Record) {
1:           List<Schema.Field> fields = avroField.getFields();
1: 
1:           Object[] structChildObjects = new Object[fields.size()];
1:           for (int i = 0; i < fields.size(); i++) {
1:             Object childObject =
1:                 avroFieldToObject(fields.get(i), ((GenericData.Record) fieldValue).get(i));
1:             if (childObject != null) {
1:               structChildObjects[i] = childObject;
1:             }
1:           }
1:           out = new StructObject(structChildObjects);
1:         } else {
1:           out = null;
1:         }
1:         break;
1:       case ARRAY:
1:         if (fieldValue instanceof GenericData.Array || fieldValue instanceof ArrayList) {
1:           Object[] arrayChildObjects;
1:           if (fieldValue instanceof GenericData.Array) {
1:             int size = ((GenericData.Array) fieldValue).size();
1:             arrayChildObjects = new Object[size];
1:             for (int i = 0; i < size; i++) {
1:               Object childObject = avroFieldToObject(
1:                   new Schema.Field(avroFields.name(), avroField.getElementType(), avroFields.doc(),
1:                       avroFields.defaultVal()), ((GenericData.Array) fieldValue).get(i));
1:               if (childObject != null) {
1:                 arrayChildObjects[i] = childObject;
1:               }
1:             }
1:           } else {
1:             int size = ((ArrayList) fieldValue).size();
1:             arrayChildObjects = new Object[size];
1:             for (int i = 0; i < size; i++) {
1:               Object childObject = avroFieldToObject(
1:                   new Schema.Field(avroFields.name(), avroField.getElementType(), avroFields.doc(),
1:                       avroFields.defaultVal()), ((ArrayList) fieldValue).get(i));
1:               if (childObject != null) {
1:                 arrayChildObjects[i] = childObject;
1:               }
1:             }
1:           }
1:           out = new ArrayObject(arrayChildObjects);
1:         } else {
1:           out = null;
1:         }
1:         break;
1:       case MAP:
1:         // Note: Avro object takes care of removing the duplicates so we should not handle it again
1:         // Map will be internally stored as Array<Struct<Key,Value>>
1:         if (fieldValue instanceof HashMap) {
1:           Map mapEntries = (HashMap) fieldValue;
1:           Object[] arrayMapChildObjects = new Object[mapEntries.size()];
1:           if (!mapEntries.isEmpty()) {
1:             Iterator iterator = mapEntries.entrySet().iterator();
1:             int counter = 0;
1:             while (iterator.hasNext()) {
1:               // size is 2 because map will have key and value
1:               Object[] mapChildObjects = new Object[2];
0:               Map.Entry mapEntry = (HashMap.Entry) iterator.next();
1:               // evaluate key
1:               Object keyObject = avroFieldToObject(
1:                   new Schema.Field(avroFields.name(), Schema.create(Schema.Type.STRING),
1:                       avroFields.doc(), avroFields.defaultVal()), mapEntry.getKey());
1:               // evaluate value
1:               Object valueObject = avroFieldToObject(
1:                   new Schema.Field(avroFields.name(), avroField.getValueType(), avroFields.doc(),
1:                       avroFields.defaultVal()), mapEntry.getValue());
1:               if (keyObject != null) {
1:                 mapChildObjects[0] = keyObject;
1:               }
1:               if (valueObject != null) {
1:                 mapChildObjects[1] = valueObject;
1:               }
1:               StructObject keyValueObject = new StructObject(mapChildObjects);
1:               arrayMapChildObjects[counter++] = keyValueObject;
1:             }
1:           }
1:           out = new ArrayObject(arrayMapChildObjects);
1:         } else {
1:           out = null;
1:         }
1:         break;
1:       default:
1:         out = avroPrimitiveFieldToObject(type, logicalType, fieldValue);
1:     }
1:     return out;
1:   }
1: 
1:   /**
/////////////////////////////////////////////////////////////////////////
1:       case ENUM:
/////////////////////////////////////////////////////////////////////////
1:       case UNION:
1:         int i = 0;
1:         // Get union types and store as Struct<type>
1:         ArrayList<StructField> unionFields = new ArrayList<>();
1:         for (Schema avroSubField : avroField.schema().getTypes()) {
1:           if (!avroSubField.getType().equals(Schema.Type.NULL)) {
1:             StructField unionField = prepareSubFields(avroField.name() + i++, avroSubField);
1:             if (unionField != null) {
1:               unionFields.add(unionField);
1:             }
1:           }
1:         }
1:         if (unionFields.isEmpty()) {
1:           throw new UnsupportedOperationException(
1:               "Carbon do not support Avro UNION with only null type");
1:         }
1:         return new Field(fieldName, "struct", unionFields);
1:       case BYTES:
1:         // DECIMAL type is defined in Avro as a BYTE type with the logicalType property
1:         // set to "decimal" and a specified precision and scale
1:         if (logicalType instanceof LogicalTypes.Decimal) {
1:           int precision = ((LogicalTypes.Decimal) childSchema.getLogicalType()).getPrecision();
1:           int scale = ((LogicalTypes.Decimal) childSchema.getLogicalType()).getScale();
1:           return new Field(fieldName, DataTypes.createDecimalType(precision, scale));
1:         }
1:         return null;
/////////////////////////////////////////////////////////////////////////
1:       case ENUM:
/////////////////////////////////////////////////////////////////////////
1:       case UNION:
1:         // recursively get the union types
1:         int i = 0;
1:         ArrayList<StructField> structSubTypes = new ArrayList<>();
1:         for (Schema avroSubField : childSchema.getTypes()) {
1:           StructField structField = prepareSubFields(fieldName + i++, avroSubField);
1:           if (structField != null) {
1:             structSubTypes.add(structField);
1:           }
1:         }
1:         return (new StructField(fieldName, DataTypes.createStructType(structSubTypes)));
1:       case BYTES:
1:         // DECIMAL type is defined in Avro as a BYTE type with the logicalType property
1:         // set to "decimal" and a specified precision and scale
1:         if (logicalType instanceof LogicalTypes.Decimal) {
1:           int precision = ((LogicalTypes.Decimal) childSchema.getLogicalType()).getPrecision();
1:           int scale = ((LogicalTypes.Decimal) childSchema.getLogicalType()).getScale();
1:           return new StructField(fieldName, DataTypes.createDecimalType(precision, scale));
1:         }
1:         return null;
/////////////////////////////////////////////////////////////////////////
1:       case ENUM:
/////////////////////////////////////////////////////////////////////////
1:       case UNION:
1:         int i = 0;
1:         // recursively get the union types and create struct type
1:         ArrayList<StructField> unionFields = new ArrayList<>();
1:         for (Schema avroSubField : childSchema.getTypes()) {
1:           StructField unionField = prepareSubFields(avroSubField.getName() + i++, avroSubField);
1:           if (unionField != null) {
1:             unionFields.add(unionField);
1:           }
1:         }
1:         return DataTypes.createStructType(unionFields);
0:       case BYTES:
0:         // DECIMAL type is defined in Avro as a BYTE type with the logicalType property
0:         // set to "decimal" and a specified precision and scale
0:         if (logicalType instanceof LogicalTypes.Decimal) {
1:           int precision = ((LogicalTypes.Decimal) childSchema.getLogicalType()).getPrecision();
1:           int scale = ((LogicalTypes.Decimal) childSchema.getLogicalType()).getScale();
1:           return DataTypes.createDecimalType(precision, scale);
1:         }
1:         return null;
author:manishgupta88
-------------------------------------------------------------------------------
commit:68b359e
/////////////////////////////////////////////////////////////////////////
1:         StructField mapField = prepareSubFields(fieldName, childSchema);
/////////////////////////////////////////////////////////////////////////
1:               new StructField(fieldName + ".val", DataTypes.createStructType(keyValueFields));
commit:fb6dffe
/////////////////////////////////////////////////////////////////////////
1: import java.util.HashMap;
1: import java.util.Iterator;
1: import java.util.Map;
/////////////////////////////////////////////////////////////////////////
1: import org.apache.carbondata.core.metadata.datatype.MapType;
1: import org.apache.carbondata.core.metadata.datatype.StructType;
/////////////////////////////////////////////////////////////////////////
1:       case MAP:
1:         // Note: Avro object takes care of removing the duplicates so we should not handle it again
1:         // Map will be internally stored as Array<Struct<Key,Value>>
1:         Map mapEntries = (HashMap) fieldValue;
1:         Object[] arrayMapChildObjects = new Object[mapEntries.size()];
1:         if (!mapEntries.isEmpty()) {
1:           Iterator iterator = mapEntries.entrySet().iterator();
1:           int counter = 0;
1:           while (iterator.hasNext()) {
1:             // size is 2 because map will have key and value
1:             Object[] mapChildObjects = new Object[2];
0:             Map.Entry mapEntry = (HashMap.Entry) iterator.next();
1:             // evaluate key
1:             Object keyObject = avroFieldToObject(
1:                 new Schema.Field(avroField.name(), Schema.create(Schema.Type.STRING),
1:                     avroField.doc(), avroField.defaultVal()), mapEntry.getKey());
1:             // evaluate value
1:             Object valueObject = avroFieldToObject(
1:                 new Schema.Field(avroField.name(), avroField.schema().getValueType(),
1:                     avroField.doc(), avroField.defaultVal()), mapEntry.getValue());
1:             if (keyObject != null) {
1:               mapChildObjects[0] = keyObject;
1:             }
1:             if (valueObject != null) {
1:               mapChildObjects[1] = valueObject;
1:             }
1:             StructObject keyValueObject = new StructObject(mapChildObjects);
1:             arrayMapChildObjects[counter++] = keyValueObject;
1:           }
1:         }
1:         out = new ArrayObject(arrayMapChildObjects);
0:         break;
/////////////////////////////////////////////////////////////////////////
1:     String fieldName = avroField.name();
1:         return new Field(fieldName, DataTypes.BOOLEAN);
1:           return new Field(fieldName, DataTypes.DATE);
1:           return new Field(fieldName, DataTypes.INT);
1:           return new Field(fieldName, DataTypes.TIMESTAMP);
1:           return new Field(fieldName, DataTypes.LONG);
1:         return new Field(fieldName, DataTypes.DOUBLE);
1:         return new Field(fieldName, DataTypes.STRING);
1:         return new Field(fieldName, DataTypes.DOUBLE);
1:       case MAP:
1:         // recursively get the sub fields
1:         ArrayList<StructField> mapSubFields = new ArrayList<>();
1:         StructField mapField = prepareSubFields("val", childSchema);
1:         if (null != mapField) {
1:           // key value field will be wrapped inside a map struct field
1:           StructField keyValueField = mapField.getChildren().get(0);
1:           // value dataType will be at position 1 in the fields
1:           DataType valueType =
1:               ((StructType) keyValueField.getDataType()).getFields().get(1).getDataType();
1:           MapType mapType = DataTypes.createMapType(DataTypes.STRING, valueType);
1:           mapSubFields.add(keyValueField);
1:           return new Field(fieldName, mapType, mapSubFields);
1:         }
0:         return null;
/////////////////////////////////////////////////////////////////////////
1:         return new Field(fieldName, "struct", structSubFields);
/////////////////////////////////////////////////////////////////////////
1:           return new Field(fieldName, "array", arraySubField);
/////////////////////////////////////////////////////////////////////////
1:   private static StructField prepareSubFields(String fieldName, Schema childSchema) {
1:         return new StructField(fieldName, DataTypes.BOOLEAN);
1:           return new StructField(fieldName, DataTypes.DATE);
1:           return new StructField(fieldName, DataTypes.INT);
1:           return new StructField(fieldName, DataTypes.TIMESTAMP);
1:           return new StructField(fieldName, DataTypes.LONG);
1:         return new StructField(fieldName, DataTypes.DOUBLE);
1:         return new StructField(fieldName, DataTypes.STRING);
1:         return new StructField(fieldName, DataTypes.DOUBLE);
1:       case MAP:
1:         // recursively get the sub fields
1:         ArrayList<StructField> keyValueFields = new ArrayList<>();
1:         // for Avro key dataType is always fixed as String
1:         StructField keyField = new StructField(fieldName + ".key", DataTypes.STRING);
1:         StructField valueField = prepareSubFields(fieldName + ".value", childSchema.getValueType());
1:         if (null != valueField) {
1:           keyValueFields.add(keyField);
1:           keyValueFields.add(valueField);
1:           StructField mapKeyValueField =
0:               new StructField(fieldName, DataTypes.createStructType(keyValueFields));
1:           // value dataType will be at position 1 in the fields
1:           MapType mapType =
1:               DataTypes.createMapType(DataTypes.STRING, mapKeyValueField.getDataType());
1:           List<StructField> mapStructFields = new ArrayList<>();
1:           mapStructFields.add(mapKeyValueField);
1:           return new StructField(fieldName, mapType, mapStructFields);
1:         }
0:         return null;
/////////////////////////////////////////////////////////////////////////
1:         return (new StructField(fieldName, DataTypes.createStructType(structSubFields)));
1:         DataType subType = getMappingDataTypeForCollectionRecord(childSchema.getElementType());
1:           return (new StructField(fieldName, DataTypes.createArrayType(subType)));
/////////////////////////////////////////////////////////////////////////
1:   private static DataType getMappingDataTypeForCollectionRecord(Schema childSchema) {
/////////////////////////////////////////////////////////////////////////
1:       case MAP:
1:         // recursively get the sub fields
0:         StructField mapField = prepareSubFields("val", childSchema);
1:         if (mapField != null) {
1:           DataType ketType = ((StructType) mapField.getDataType()).getFields().get(0).getDataType();
1:           DataType valueType =
1:               ((StructType) mapField.getDataType()).getFields().get(1).getDataType();
1:           return DataTypes.createMapType(ketType, valueType);
1:         }
0:         return null;
/////////////////////////////////////////////////////////////////////////
1:         DataType subType = getMappingDataTypeForCollectionRecord(childSchema.getElementType());
author:ajantha-bhat
-------------------------------------------------------------------------------
commit:041603d
/////////////////////////////////////////////////////////////////////////
1:     LogicalType logicalType = childSchema.getLogicalType();
1:         if (logicalType != null) {
0:           if (logicalType instanceof LogicalTypes.Date) {
1:             return DataTypes.DATE;
0:           } else {
0:             LOGGER.warn("Unsupported logical type. Considering Data Type as INT for " + childSchema
0:                 .getName());
1:             return DataTypes.INT;
1:           }
0:         } else {
1:           return DataTypes.INT;
1:         }
1:         if (logicalType != null) {
1:           if (logicalType instanceof LogicalTypes.TimestampMillis
1:               || logicalType instanceof LogicalTypes.TimestampMicros) {
1:             return DataTypes.TIMESTAMP;
0:           } else {
0:             LOGGER.warn("Unsupported logical type. Considering Data Type as LONG for " + childSchema
0:                 .getName());
1:             return DataTypes.LONG;
1:           }
0:         } else {
1:           return DataTypes.LONG;
1:         }
commit:cf1b50b
/////////////////////////////////////////////////////////////////////////
0:         // direct conversion will change precision. So parse from string.
0:         // also carbon internally needs float as double
0:         out = Double.parseDouble(fieldValue.toString());
/////////////////////////////////////////////////////////////////////////
1:                 new Schema.Field(avroField.name(), avroField.schema().getElementType(),
1:                     avroField.doc(), avroField.defaultVal()),
/////////////////////////////////////////////////////////////////////////
1:                 new Schema.Field(avroField.name(), avroField.schema().getElementType(),
1:                     avroField.doc(), avroField.defaultVal()), ((ArrayList) fieldValue).get(i));
commit:cf3e919
/////////////////////////////////////////////////////////////////////////
1: import org.apache.carbondata.core.metadata.datatype.DataType;
1: import org.apache.carbondata.core.metadata.datatype.DataTypes;
1: import org.apache.carbondata.core.metadata.datatype.StructField;
/////////////////////////////////////////////////////////////////////////
1: public class AvroCarbonWriter extends CarbonWriter {
/////////////////////////////////////////////////////////////////////////
1:         throw new UnsupportedOperationException(
1:             "carbon not support " + type.toString() + " avro type yet");
1:    * converts avro schema to carbon schema required by carbonWriter
1:    *
0:    * @param avroSchemaString json formatted avro schema as string
1:    * @return carbon sdk schema
1:    */
1:   public static org.apache.carbondata.sdk.file.Schema getCarbonSchemaFromAvroSchema(
0:       String avroSchemaString) {
0:     if (avroSchemaString == null) {
0:       throw new UnsupportedOperationException("avro schema string cannot be null");
1:     }
0:     Schema avroSchema = new Schema.Parser().parse(avroSchemaString);
1:     Field[] carbonField = new Field[avroSchema.getFields().size()];
1:     int i = 0;
1:     for (Schema.Field avroField : avroSchema.getFields()) {
0:       carbonField[i] = prepareFields(avroField);
1:       i++;
1:     }
1:     return new org.apache.carbondata.sdk.file.Schema(carbonField);
1:   }
1: 
1:   private static Field prepareFields(Schema.Field avroField) {
0:     String FieldName = avroField.name();
1:     Schema childSchema = avroField.schema();
1:     Schema.Type type = childSchema.getType();
1:     switch (type) {
1:       case BOOLEAN:
0:         return new Field(FieldName, DataTypes.BOOLEAN);
1:       case INT:
0:         return new Field(FieldName, DataTypes.INT);
1:       case LONG:
0:         return new Field(FieldName, DataTypes.LONG);
1:       case DOUBLE:
0:         return new Field(FieldName, DataTypes.DOUBLE);
1:       case STRING:
0:         return new Field(FieldName, DataTypes.STRING);
1:       case FLOAT:
0:         return new Field(FieldName, DataTypes.DOUBLE);
1:       case RECORD:
1:         // recursively get the sub fields
1:         ArrayList<StructField> structSubFields = new ArrayList<>();
1:         for (Schema.Field avroSubField : childSchema.getFields()) {
0:           structSubFields.add(prepareSubFields(avroSubField.name(), avroSubField.schema()));
1:         }
0:         return new Field(FieldName, "struct", structSubFields);
1:       case ARRAY:
1:         // recursively get the sub fields
1:         ArrayList<StructField> arraySubField = new ArrayList<>();
1:         // array will have only one sub field.
0:         arraySubField.add(prepareSubFields("val", childSchema.getElementType()));
0:         return new Field(FieldName, "array", arraySubField);
1:       default:
1:         throw new UnsupportedOperationException(
1:             "carbon not support " + type.toString() + " avro type yet");
1:     }
1:   }
1: 
0:   private static StructField prepareSubFields(String FieldName, Schema childSchema) {
1:     Schema.Type type = childSchema.getType();
1:     switch (type) {
1:       case BOOLEAN:
0:         return new StructField(FieldName, DataTypes.BOOLEAN);
1:       case INT:
0:         return new StructField(FieldName, DataTypes.INT);
1:       case LONG:
0:         return new StructField(FieldName, DataTypes.LONG);
1:       case DOUBLE:
0:         return new StructField(FieldName, DataTypes.DOUBLE);
1:       case STRING:
0:         return new StructField(FieldName, DataTypes.STRING);
1:       case FLOAT:
0:         return new StructField(FieldName, DataTypes.DOUBLE);
1:       case RECORD:
1:         // recursively get the sub fields
1:         ArrayList<StructField> structSubFields = new ArrayList<>();
1:         for (Schema.Field avroSubField : childSchema.getFields()) {
0:           structSubFields.add(prepareSubFields(avroSubField.name(), avroSubField.schema()));
1:         }
0:         return (new StructField(FieldName, DataTypes.createStructType(structSubFields)));
1:       case ARRAY:
1:         // recursively get the sub fields
1:         // array will have only one sub field.
0:         return (new StructField(FieldName, DataTypes.createArrayType(
0:             getMappingDataTypeForArrayRecord(childSchema.getElementType()))));
1:       default:
1:         throw new UnsupportedOperationException(
1:             "carbon not support " + type.toString() + " avro type yet");
1:     }
1:   }
1: 
0:   private static DataType getMappingDataTypeForArrayRecord(Schema childSchema) {
1:     switch (childSchema.getType()) {
1:       case BOOLEAN:
1:         return DataTypes.BOOLEAN;
1:       case INT:
0:         return DataTypes.INT;
1:       case LONG:
0:         return DataTypes.LONG;
1:       case DOUBLE:
1:         return DataTypes.DOUBLE;
1:       case STRING:
1:         return DataTypes.STRING;
1:       case FLOAT:
1:         return DataTypes.DOUBLE;
1:       case RECORD:
1:         // recursively get the sub fields
1:         ArrayList<StructField> structSubFields = new ArrayList<>();
1:         for (Schema.Field avroSubField : childSchema.getFields()) {
0:           structSubFields.add(prepareSubFields(avroSubField.name(), avroSubField.schema()));
1:         }
1:         return DataTypes.createStructType(structSubFields);
1:       case ARRAY:
1:         // array will have only one sub field.
0:         return DataTypes.createArrayType(
0:             getMappingDataTypeForArrayRecord(childSchema.getElementType()));
1:       default:
1:         throw new UnsupportedOperationException(
1:             "carbon not support " + childSchema.getType().toString() + " avro type yet");
1:     }
1:   }
1: 
1:   /**
commit:4b98af2
/////////////////////////////////////////////////////////////////////////
0:       case FLOAT:
author:sounakr
-------------------------------------------------------------------------------
commit:ec33c11
/////////////////////////////////////////////////////////////////////////
1: import org.apache.carbondata.processing.loading.complexobjects.ArrayObject;
1: import org.apache.carbondata.processing.loading.complexobjects.StructObject;
/////////////////////////////////////////////////////////////////////////
0:     Object[] csvField = new Object[fields.size()];
0:       csvField[i] = avroFieldToObject(fields.get(i), avroRecord.get(i));
1:   private Object avroFieldToObject(Schema.Field avroField, Object fieldValue) {
0:     Object out = new Object();
/////////////////////////////////////////////////////////////////////////
0:         out = fieldValue;
0:         break;
0:         Float f = (Float) fieldValue;
0:         out = f.doubleValue();
1: 
1:         Object[] structChildObjects = new Object[fields.size()];
0:           structChildObjects[i] =
1:               avroFieldToObject(fields.get(i), ((GenericData.Record) fieldValue).get(i));
1:         StructObject structObject = new StructObject(structChildObjects);
1:         out = structObject;
0:         Object[] arrayChildObjects = new Object[size];
0:           arrayChildObjects[i] = (avroFieldToObject(
0:               new Schema.Field(avroField.name(), avroField.schema().getElementType(), null, true),
0:               ((ArrayList) fieldValue).get(i)));
0:         ArrayObject arrayObject = new ArrayObject(arrayChildObjects);
0:         out = arrayObject;
1:     return out;
commit:b1c85fa
/////////////////////////////////////////////////////////////////////////
1: import java.util.ArrayList;
/////////////////////////////////////////////////////////////////////////
0:     Object [] csvField = new Object[fields.size()];
0:       csvField[i] = avroFieldToObject(fields.get(i), avroRecord.get(i), 0);
0:   private String avroFieldToObject(Schema.Field avroField, Object fieldValue, int delimiterLevel) {
1:     Schema.Type type = avroField.schema().getType();
/////////////////////////////////////////////////////////////////////////
1:         List<Schema.Field> fields = avroField.schema().getFields();
0:         delimiterLevel ++;
1:         for (int i = 0; i < fields.size(); i++) {
0:           if (delimiterLevel == 1) {
0:           } else if (delimiterLevel > 1) {
0:             out.append(avroFieldToObject(fields.get(i), ((GenericData.Record) fieldValue).get(i),
0:                 delimiterLevel)).append(delimiter);
0:             out.append(avroFieldToObject(fields.get(i), ((GenericData.Record) fieldValue).get(i),
0:                 delimiterLevel));
1:       case ARRAY:
0:         int size = ((ArrayList) fieldValue).size();
0:         String delimiterArray = null;
0:         delimiterLevel ++;
0:         if (delimiterLevel == 1) {
0:           delimiterArray = "$";
0:         } else if (delimiterLevel > 1) {
0:           delimiterArray = ":";
1:         }
1: 
0:         for (int i = 0; i < size; i++) {
0:           if (i != size - 1) {
0:             out.append(avroFieldToObject(
0:                 new Schema.Field(avroField.name(), avroField.schema().getElementType(), null, true),
0:                 ((ArrayList) fieldValue).get(i), delimiterLevel)).append(delimiterArray);
0:           } else {
0:             out.append(avroFieldToObject(
0:                 new Schema.Field(avroField.name(), avroField.schema().getElementType(), null, true),
0:                 ((ArrayList) fieldValue).get(i), delimiterLevel));
1:           }
1:         }
0:         break;
1: 
commit:3202cf5
/////////////////////////////////////////////////////////////////////////
1:   private Object[] avroToCsv(GenericData.Record avroRecord) {
0:     Object [] csvField = new String[fields.size()];
/////////////////////////////////////////////////////////////////////////
1:       case RECORD:
0:         List<Schema.Field> fields = fieldType.schema().getFields();
0:         String delimiter = null;
0:         for (int i = 0; i < fields.size(); i ++) {
0:           if (i == 0) {
0:             delimiter = "$";
0:           } else {
0:             delimiter = ":";
1:           }
0:           if (i != (fields.size() - 1)) {
0:             out.append(avroFieldToString(fields.get(i), ((GenericData.Record) fieldValue).get(i)))
0:                 .append(delimiter);
0:           } else {
0:             out.append(avroFieldToString(fields.get(i), ((GenericData.Record) fieldValue).get(i)));
1:           }
1:         }
0:         break;
/////////////////////////////////////////////////////////////////////////
1:       Object[] csvRecord = avroToCsv(record);
/////////////////////////////////////////////////////////////////////////
1:   @Override public void close() throws IOException {
author:BJangir
-------------------------------------------------------------------------------
commit:3ea2a1d
/////////////////////////////////////////////////////////////////////////
1:       GenericData.Record record = (GenericData.Record) object;
1: 
1:       // convert Avro record to CSV String[]
0:       String[] csvRecord = avroToCsv(record);
1:       writable.set(csvRecord);
1:     } catch (Exception e) {
1:       close();
author:Jacky Li
-------------------------------------------------------------------------------
commit:e39b0a1
/////////////////////////////////////////////////////////////////////////
1: /*
1:  * Licensed to the Apache Software Foundation (ASF) under one or more
1:  * contributor license agreements.  See the NOTICE file distributed with
1:  * this work for additional information regarding copyright ownership.
1:  * The ASF licenses this file to You under the Apache License, Version 2.0
1:  * (the "License"); you may not use this file except in compliance with
1:  * the License.  You may obtain a copy of the License at
1:  *
1:  *    http://www.apache.org/licenses/LICENSE-2.0
1:  *
1:  * Unless required by applicable law or agreed to in writing, software
1:  * distributed under the License is distributed on an "AS IS" BASIS,
1:  * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
1:  * See the License for the specific language governing permissions and
1:  * limitations under the License.
1:  */
1: 
1: package org.apache.carbondata.sdk.file;
1: 
1: import java.io.IOException;
1: import java.util.List;
1: import java.util.Random;
1: import java.util.UUID;
1: 
1: import org.apache.carbondata.common.annotations.InterfaceAudience;
1: import org.apache.carbondata.hadoop.api.CarbonTableOutputFormat;
1: import org.apache.carbondata.hadoop.internal.ObjectArrayWritable;
1: import org.apache.carbondata.processing.loading.model.CarbonLoadModel;
1: 
1: import org.apache.avro.Schema;
1: import org.apache.avro.generic.GenericData;
1: import org.apache.hadoop.conf.Configuration;
1: import org.apache.hadoop.io.NullWritable;
1: import org.apache.hadoop.mapreduce.JobID;
1: import org.apache.hadoop.mapreduce.RecordWriter;
1: import org.apache.hadoop.mapreduce.TaskAttemptContext;
1: import org.apache.hadoop.mapreduce.TaskAttemptID;
1: import org.apache.hadoop.mapreduce.TaskID;
1: import org.apache.hadoop.mapreduce.TaskType;
1: import org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl;
1: 
1: /**
1:  * Writer Implementation to write Avro Record to carbondata file.
1:  */
1: @InterfaceAudience.Internal
0: class AvroCarbonWriter extends CarbonWriter {
1: 
1:   private RecordWriter<NullWritable, ObjectArrayWritable> recordWriter;
1:   private TaskAttemptContext context;
1:   private ObjectArrayWritable writable;
1:   private Schema avroSchema;
1: 
0:   AvroCarbonWriter(CarbonLoadModel loadModel) throws IOException {
0:     Configuration hadoopConf = new Configuration();
1:     CarbonTableOutputFormat.setLoadModel(hadoopConf, loadModel);
1:     CarbonTableOutputFormat format = new CarbonTableOutputFormat();
1:     JobID jobId = new JobID(UUID.randomUUID().toString(), 0);
1:     Random random = new Random();
1:     TaskID task = new TaskID(jobId, TaskType.MAP, random.nextInt());
1:     TaskAttemptID attemptID = new TaskAttemptID(task, random.nextInt());
1:     TaskAttemptContextImpl context = new TaskAttemptContextImpl(hadoopConf, attemptID);
1:     this.recordWriter = format.getRecordWriter(context);
1:     this.context = context;
1:     this.writable = new ObjectArrayWritable();
1:   }
1: 
0:   private String[] avroToCsv(GenericData.Record avroRecord) {
1:     if (avroSchema == null) {
1:       avroSchema = avroRecord.getSchema();
1:     }
1:     List<Schema.Field> fields = avroSchema.getFields();
0:     String[] csvField = new String[fields.size()];
1:     for (int i = 0; i < fields.size(); i++) {
0:       csvField[i] = avroFieldToString(fields.get(i), avroRecord.get(i));
1:     }
0:     return csvField;
1:   }
1: 
0:   private String avroFieldToString(Schema.Field fieldType, Object fieldValue) {
0:     StringBuilder out = new StringBuilder();
0:     Schema.Type type = fieldType.schema().getType();
1:     switch (type) {
0:       case BOOLEAN:
0:       case INT:
0:       case LONG:
0:       case DOUBLE:
0:       case STRING:
0:         out.append(fieldValue.toString());
0:         break;
1:       default:
0:         throw new UnsupportedOperationException();
0:       // TODO: convert complex type
1:     }
0:     return out.toString();
1:   }
1: 
1:   /**
1:    * Write single row data, input row is Avro Record
1:    */
1:   @Override
1:   public void write(Object object) throws IOException {
0:     GenericData.Record record = (GenericData.Record) object;
1: 
0:     // convert Avro record to CSV String[]
0:     String[] csvRecord = avroToCsv(record);
0:     writable.set(csvRecord);
1:     try {
1:       recordWriter.write(NullWritable.get(), writable);
1:     } catch (InterruptedException e) {
1:       throw new IOException(e);
1:     }
1:   }
1: 
1:   /**
1:    * Flush and close the writer
1:    */
1:   @Override
0:   public void close() throws IOException {
1:     try {
1:       recordWriter.close(context);
1:     } catch (InterruptedException e) {
1:       throw new IOException(e);
1:     }
1:   }
1: }
============================================================================