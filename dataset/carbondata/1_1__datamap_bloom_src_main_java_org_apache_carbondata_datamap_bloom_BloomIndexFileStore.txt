1:7b31b91: /*
1:7b31b91:  * Licensed to the Apache Software Foundation (ASF) under one or more
1:7b31b91:  * contributor license agreements.  See the NOTICE file distributed with
1:7b31b91:  * this work for additional information regarding copyright ownership.
1:7b31b91:  * The ASF licenses this file to You under the Apache License, Version 2.0
1:7b31b91:  * (the "License"); you may not use this file except in compliance with
1:7b31b91:  * the License.  You may obtain a copy of the License at
1:7b31b91:  *
1:7b31b91:  *    http://www.apache.org/licenses/LICENSE-2.0
1:7b31b91:  *
1:7b31b91:  * Unless required by applicable law or agreed to in writing, software
1:7b31b91:  * distributed under the License is distributed on an "AS IS" BASIS,
1:7b31b91:  * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
1:7b31b91:  * See the License for the specific language governing permissions and
1:7b31b91:  * limitations under the License.
1:7b31b91:  */
1:7b31b91: package org.apache.carbondata.datamap.bloom;
1:7b31b91: 
1:7b31b91: import java.io.*;
1:7b31b91: import java.nio.charset.Charset;
1:7b31b91: import java.util.ArrayList;
1:7b31b91: import java.util.List;
1:7b31b91: 
1:7b31b91: import org.apache.carbondata.common.annotations.InterfaceAudience;
1:7b31b91: import org.apache.carbondata.common.logging.LogService;
1:7b31b91: import org.apache.carbondata.common.logging.LogServiceFactory;
1:7b31b91: import org.apache.carbondata.core.datastore.filesystem.CarbonFile;
1:7b31b91: import org.apache.carbondata.core.datastore.filesystem.CarbonFileFilter;
1:7b31b91: import org.apache.carbondata.core.datastore.impl.FileFactory;
1:7b31b91: import org.apache.carbondata.core.util.CarbonUtil;
1:7b31b91: 
1:7b31b91: import org.apache.hadoop.fs.Path;
1:7b31b91: import org.apache.hadoop.util.bloom.CarbonBloomFilter;
1:7b31b91: 
1:7b31b91: /**
1:7b31b91:  * This class works for merging and loading bloom index
1:7b31b91:  */
1:7b31b91: @InterfaceAudience.Internal
1:7b31b91: public class BloomIndexFileStore {
1:7b31b91: 
1:7b31b91:   private static final LogService LOGGER =
1:7b31b91:           LogServiceFactory.getLogService(BloomIndexFileStore.class.getName());
1:7b31b91: 
1:7b31b91:   // suffix of original generated file
1:7b31b91:   public static final String BLOOM_INDEX_SUFFIX = ".bloomindex";
1:7b31b91:   // suffix of merged bloom index file
1:7b31b91:   public static final String MERGE_BLOOM_INDEX_SUFFIX = ".bloomindexmerge";
1:7b31b91:   // directory to store merged bloom index files
1:7b31b91:   public static final String MERGE_BLOOM_INDEX_SHARD_NAME = "mergeShard";
1:7b31b91:   /**
1:7b31b91:    * flag file for merging
1:7b31b91:    * if flag file exists, query won't use mergeShard
1:7b31b91:    * if flag file not exists and mergeShard generated, query will use mergeShard
1:7b31b91:    */
1:7b31b91:   public static final String MERGE_INPROGRESS_FILE = "mergeShard.inprogress";
1:7b31b91: 
1:7b31b91: 
1:7b31b91:   public static void mergeBloomIndexFile(String dmSegmentPathString, List<String> indexCols) {
1:7b31b91:     // get all shard paths of old store
1:7b31b91:     CarbonFile segmentPath = FileFactory.getCarbonFile(dmSegmentPathString,
1:7b31b91:             FileFactory.getFileType(dmSegmentPathString));
1:7b31b91:     CarbonFile[] shardPaths = segmentPath.listFiles(new CarbonFileFilter() {
1:7b31b91:       @Override
1:7b31b91:       public boolean accept(CarbonFile file) {
1:7b31b91:         return file.isDirectory() && !file.getName().equals(MERGE_BLOOM_INDEX_SHARD_NAME);
1:7b31b91:       }
1:7b31b91:     });
1:7b31b91: 
1:7b31b91:     String mergeShardPath = dmSegmentPathString + File.separator + MERGE_BLOOM_INDEX_SHARD_NAME;
1:7b31b91:     String mergeInprogressFile = dmSegmentPathString + File.separator + MERGE_INPROGRESS_FILE;
1:7b31b91:     try {
1:7b31b91:       // delete mergeShard folder if exists
1:7b31b91:       if (FileFactory.isFileExist(mergeShardPath)) {
1:7b31b91:         FileFactory.deleteFile(mergeShardPath, FileFactory.getFileType(mergeShardPath));
1:7b31b91:       }
1:7b31b91:       // create flag file before creating mergeShard folder
1:7b31b91:       if (!FileFactory.isFileExist(mergeInprogressFile)) {
1:7b31b91:         FileFactory.createNewFile(
1:7b31b91:             mergeInprogressFile, FileFactory.getFileType(mergeInprogressFile));
1:7b31b91:       }
1:7b31b91:       // create mergeShard output folder
1:7b31b91:       if (!FileFactory.mkdirs(mergeShardPath, FileFactory.getFileType(mergeShardPath))) {
1:7b31b91:         throw new RuntimeException("Failed to create directory " + mergeShardPath);
1:7b31b91:       }
1:7b31b91:     } catch (IOException e) {
1:7b31b91:       LOGGER.error(e, "Error occurs while create directory " + mergeShardPath);
1:7b31b91:       throw new RuntimeException("Error occurs while create directory " + mergeShardPath);
1:7b31b91:     }
1:7b31b91: 
1:7b31b91:     // for each index column, merge the bloomindex files from all shards into one
1:7b31b91:     for (String indexCol: indexCols) {
1:7b31b91:       String mergeIndexFile = getMergeBloomIndexFile(mergeShardPath, indexCol);
1:7b31b91:       DataInputStream dataInputStream = null;
1:7b31b91:       DataOutputStream dataOutputStream = null;
1:7b31b91:       try {
1:7b31b91:         FileFactory.createNewFile(mergeIndexFile, FileFactory.getFileType(mergeIndexFile));
1:7b31b91:         dataOutputStream = FileFactory.getDataOutputStream(
1:7b31b91:             mergeIndexFile, FileFactory.getFileType(mergeIndexFile));
1:7b31b91:         for (CarbonFile shardPath : shardPaths) {
1:7b31b91:           String bloomIndexFile = getBloomIndexFile(shardPath.getCanonicalPath(), indexCol);
1:7b31b91:           dataInputStream = FileFactory.getDataInputStream(
1:7b31b91:               bloomIndexFile, FileFactory.getFileType(bloomIndexFile));
1:7b31b91:           byte[] fileData = new byte[(int) FileFactory.getCarbonFile(bloomIndexFile).getSize()];
1:7b31b91:           dataInputStream.readFully(fileData);
1:7b31b91:           byte[] shardName = shardPath.getName().getBytes(Charset.forName("UTF-8"));
1:7b31b91:           dataOutputStream.writeInt(shardName.length);
1:7b31b91:           dataOutputStream.write(shardName);
1:7b31b91:           dataOutputStream.writeInt(fileData.length);
1:7b31b91:           dataOutputStream.write(fileData);
1:7b31b91:           CarbonUtil.closeStream(dataInputStream);
1:7b31b91:         }
1:7b31b91:       } catch (IOException e) {
1:7b31b91:         LOGGER.error(e, "Error occurs while merge bloom index file of column: " + indexCol);
1:7b31b91:         // delete merge shard of bloom index for this segment when failed
1:7b31b91:         FileFactory.deleteAllCarbonFilesOfDir(FileFactory.getCarbonFile(mergeShardPath));
1:7b31b91:         throw new RuntimeException(
1:7b31b91:             "Error occurs while merge bloom index file of column: " + indexCol);
1:7b31b91:       } finally {
1:7b31b91:         CarbonUtil.closeStreams(dataInputStream, dataOutputStream);
1:7b31b91:       }
1:7b31b91:     }
1:7b31b91:     // delete flag file and mergeShard can be used
1:7b31b91:     try {
1:7b31b91:       FileFactory.deleteFile(mergeInprogressFile, FileFactory.getFileType(mergeInprogressFile));
1:7b31b91:     } catch (IOException e) {
1:7b31b91:       LOGGER.error(e, "Error occurs while deleting file " + mergeInprogressFile);
1:7b31b91:       throw new RuntimeException("Error occurs while deleting file " + mergeInprogressFile);
1:7b31b91:     }
1:7b31b91:     // remove old store
1:7b31b91:     for (CarbonFile shardpath: shardPaths) {
1:7b31b91:       FileFactory.deleteAllCarbonFilesOfDir(shardpath);
1:7b31b91:     }
1:7b31b91:   }
1:7b31b91: 
1:7b31b91:   /**
1:7b31b91:    * load bloom filter from bloom index file
1:7b31b91:    */
1:7b31b91:   public static List<CarbonBloomFilter> loadBloomFilterFromFile(
1:7b31b91:           String shardPath, String colName) {
1:7b31b91:     if (shardPath.endsWith(MERGE_BLOOM_INDEX_SHARD_NAME)) {
1:7b31b91:       return loadMergeBloomIndex(shardPath, colName);
1:7b31b91:     } else {
1:7b31b91:       return loadBloomIndex(shardPath, colName);
1:7b31b91:     }
1:7b31b91:   }
1:7b31b91: 
1:7b31b91:   /**
1:7b31b91:    * load bloom filter of {@code colName} from {@code shardPath}
1:7b31b91:    */
1:7b31b91:   public static List<CarbonBloomFilter> loadBloomIndex(
1:7b31b91:           String shardPath, String colName) {
1:7b31b91:     DataInputStream dataInStream = null;
1:7b31b91:     List<CarbonBloomFilter> bloomFilters = new ArrayList<>();
1:7b31b91:     try {
1:7b31b91:       String indexFile = getBloomIndexFile(shardPath, colName);
1:7b31b91:       dataInStream = FileFactory.getDataInputStream(indexFile, FileFactory.getFileType(indexFile));
1:7b31b91:       while (dataInStream.available() > 0) {
1:7b31b91:         CarbonBloomFilter bloomFilter = new CarbonBloomFilter();
1:7b31b91:         bloomFilter.readFields(dataInStream);
1:7b31b91:         bloomFilter.setShardName(new Path(shardPath).getName());
1:7b31b91:         bloomFilters.add(bloomFilter);
1:7b31b91:       }
1:7b31b91:       LOGGER.info(String.format("Read %d bloom indices from %s", bloomFilters.size(), indexFile));
1:7b31b91: 
1:7b31b91:       return bloomFilters;
1:7b31b91:     } catch (IOException e) {
1:7b31b91:       LOGGER.error(e, "Error occurs while reading bloom index");
1:7b31b91:       throw new RuntimeException("Error occurs while reading bloom index", e);
1:7b31b91:     } finally {
1:7b31b91:       CarbonUtil.closeStreams(dataInStream);
1:7b31b91:     }
1:7b31b91:   }
1:7b31b91: 
1:7b31b91: 
1:7b31b91:   /**
1:7b31b91:    * load bloom filter of {@code colName} from {@code mergeShardPath}
1:7b31b91:    */
1:7b31b91:   public static List<CarbonBloomFilter> loadMergeBloomIndex(
1:7b31b91:           String mergeShardPath, String colName) {
1:7b31b91:     String mergeIndexFile = getMergeBloomIndexFile(mergeShardPath, colName);
1:7b31b91:     DataInputStream mergeIndexInStream = null;
1:7b31b91:     List<CarbonBloomFilter> bloomFilters = new ArrayList<>();
1:7b31b91:     try {
1:7b31b91:       mergeIndexInStream = FileFactory.getDataInputStream(
1:7b31b91:           mergeIndexFile, FileFactory.getFileType(mergeIndexFile));
1:7b31b91:       while (mergeIndexInStream.available() > 0) {
1:7b31b91:         // read shard name
1:7b31b91:         int shardNameByteLength = mergeIndexInStream.readInt();
1:7b31b91:         byte[] shardNameBytes = new byte[shardNameByteLength];
1:7b31b91:         mergeIndexInStream.readFully(shardNameBytes);
1:7b31b91:         String shardName = new String(shardNameBytes, Charset.forName("UTF-8"));
1:7b31b91:         // read bloom index file data
1:7b31b91:         int indexFileByteLength = mergeIndexInStream.readInt();
1:7b31b91:         byte[] indexFileBytes = new byte[indexFileByteLength];
1:7b31b91:         mergeIndexInStream.readFully(indexFileBytes);
1:7b31b91:         // warp byte array as input stream to get bloom filters
1:7b31b91:         ByteArrayInputStream byteArrayInputStream = new ByteArrayInputStream(indexFileBytes);
1:7b31b91:         DataInputStream indexDataInStream = new DataInputStream(byteArrayInputStream);
1:7b31b91:         while (indexDataInStream.available() > 0) {
1:7b31b91:           CarbonBloomFilter bloomFilter = new CarbonBloomFilter();
1:7b31b91:           bloomFilter.readFields(indexDataInStream);
1:7b31b91:           bloomFilter.setShardName(shardName);
1:7b31b91:           bloomFilters.add(bloomFilter);
1:7b31b91:         }
1:7b31b91:       }
1:7b31b91:       LOGGER.info(
1:7b31b91:           String.format("Read %d bloom indices from %s", bloomFilters.size(), mergeIndexFile));
1:7b31b91:       return bloomFilters;
1:7b31b91:     } catch (IOException e) {
1:7b31b91:       LOGGER.error(e, "Error occurs while reading merge bloom index");
1:7b31b91:       throw new RuntimeException("Error occurs while reading merge bloom index", e);
1:7b31b91:     } finally {
1:7b31b91:       CarbonUtil.closeStreams(mergeIndexInStream);
1:7b31b91:     }
1:7b31b91:   }
1:7b31b91: 
1:7b31b91:   /**
1:7b31b91:    * get bloom index file
1:7b31b91:    */
1:7b31b91:   public static String getBloomIndexFile(String shardPath, String colName) {
1:7b31b91:     return shardPath.concat(File.separator).concat(colName).concat(BLOOM_INDEX_SUFFIX);
1:7b31b91:   }
1:7b31b91: 
1:7b31b91:   /**
1:7b31b91:    * get merge bloom index file
1:7b31b91:    */
1:7b31b91:   public static String getMergeBloomIndexFile(String mergeShardPath, String colName) {
1:7b31b91:     return mergeShardPath.concat(File.separator).concat(colName).concat(MERGE_BLOOM_INDEX_SUFFIX);
1:7b31b91:   }
1:7b31b91: }
============================================================================
author:Manhua
-------------------------------------------------------------------------------
commit:7b31b91
/////////////////////////////////////////////////////////////////////////
1: /*
1:  * Licensed to the Apache Software Foundation (ASF) under one or more
1:  * contributor license agreements.  See the NOTICE file distributed with
1:  * this work for additional information regarding copyright ownership.
1:  * The ASF licenses this file to You under the Apache License, Version 2.0
1:  * (the "License"); you may not use this file except in compliance with
1:  * the License.  You may obtain a copy of the License at
1:  *
1:  *    http://www.apache.org/licenses/LICENSE-2.0
1:  *
1:  * Unless required by applicable law or agreed to in writing, software
1:  * distributed under the License is distributed on an "AS IS" BASIS,
1:  * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
1:  * See the License for the specific language governing permissions and
1:  * limitations under the License.
1:  */
1: package org.apache.carbondata.datamap.bloom;
1: 
1: import java.io.*;
1: import java.nio.charset.Charset;
1: import java.util.ArrayList;
1: import java.util.List;
1: 
1: import org.apache.carbondata.common.annotations.InterfaceAudience;
1: import org.apache.carbondata.common.logging.LogService;
1: import org.apache.carbondata.common.logging.LogServiceFactory;
1: import org.apache.carbondata.core.datastore.filesystem.CarbonFile;
1: import org.apache.carbondata.core.datastore.filesystem.CarbonFileFilter;
1: import org.apache.carbondata.core.datastore.impl.FileFactory;
1: import org.apache.carbondata.core.util.CarbonUtil;
1: 
1: import org.apache.hadoop.fs.Path;
1: import org.apache.hadoop.util.bloom.CarbonBloomFilter;
1: 
1: /**
1:  * This class works for merging and loading bloom index
1:  */
1: @InterfaceAudience.Internal
1: public class BloomIndexFileStore {
1: 
1:   private static final LogService LOGGER =
1:           LogServiceFactory.getLogService(BloomIndexFileStore.class.getName());
1: 
1:   // suffix of original generated file
1:   public static final String BLOOM_INDEX_SUFFIX = ".bloomindex";
1:   // suffix of merged bloom index file
1:   public static final String MERGE_BLOOM_INDEX_SUFFIX = ".bloomindexmerge";
1:   // directory to store merged bloom index files
1:   public static final String MERGE_BLOOM_INDEX_SHARD_NAME = "mergeShard";
1:   /**
1:    * flag file for merging
1:    * if flag file exists, query won't use mergeShard
1:    * if flag file not exists and mergeShard generated, query will use mergeShard
1:    */
1:   public static final String MERGE_INPROGRESS_FILE = "mergeShard.inprogress";
1: 
1: 
1:   public static void mergeBloomIndexFile(String dmSegmentPathString, List<String> indexCols) {
1:     // get all shard paths of old store
1:     CarbonFile segmentPath = FileFactory.getCarbonFile(dmSegmentPathString,
1:             FileFactory.getFileType(dmSegmentPathString));
1:     CarbonFile[] shardPaths = segmentPath.listFiles(new CarbonFileFilter() {
1:       @Override
1:       public boolean accept(CarbonFile file) {
1:         return file.isDirectory() && !file.getName().equals(MERGE_BLOOM_INDEX_SHARD_NAME);
1:       }
1:     });
1: 
1:     String mergeShardPath = dmSegmentPathString + File.separator + MERGE_BLOOM_INDEX_SHARD_NAME;
1:     String mergeInprogressFile = dmSegmentPathString + File.separator + MERGE_INPROGRESS_FILE;
1:     try {
1:       // delete mergeShard folder if exists
1:       if (FileFactory.isFileExist(mergeShardPath)) {
1:         FileFactory.deleteFile(mergeShardPath, FileFactory.getFileType(mergeShardPath));
1:       }
1:       // create flag file before creating mergeShard folder
1:       if (!FileFactory.isFileExist(mergeInprogressFile)) {
1:         FileFactory.createNewFile(
1:             mergeInprogressFile, FileFactory.getFileType(mergeInprogressFile));
1:       }
1:       // create mergeShard output folder
1:       if (!FileFactory.mkdirs(mergeShardPath, FileFactory.getFileType(mergeShardPath))) {
1:         throw new RuntimeException("Failed to create directory " + mergeShardPath);
1:       }
1:     } catch (IOException e) {
1:       LOGGER.error(e, "Error occurs while create directory " + mergeShardPath);
1:       throw new RuntimeException("Error occurs while create directory " + mergeShardPath);
1:     }
1: 
1:     // for each index column, merge the bloomindex files from all shards into one
1:     for (String indexCol: indexCols) {
1:       String mergeIndexFile = getMergeBloomIndexFile(mergeShardPath, indexCol);
1:       DataInputStream dataInputStream = null;
1:       DataOutputStream dataOutputStream = null;
1:       try {
1:         FileFactory.createNewFile(mergeIndexFile, FileFactory.getFileType(mergeIndexFile));
1:         dataOutputStream = FileFactory.getDataOutputStream(
1:             mergeIndexFile, FileFactory.getFileType(mergeIndexFile));
1:         for (CarbonFile shardPath : shardPaths) {
1:           String bloomIndexFile = getBloomIndexFile(shardPath.getCanonicalPath(), indexCol);
1:           dataInputStream = FileFactory.getDataInputStream(
1:               bloomIndexFile, FileFactory.getFileType(bloomIndexFile));
1:           byte[] fileData = new byte[(int) FileFactory.getCarbonFile(bloomIndexFile).getSize()];
1:           dataInputStream.readFully(fileData);
1:           byte[] shardName = shardPath.getName().getBytes(Charset.forName("UTF-8"));
1:           dataOutputStream.writeInt(shardName.length);
1:           dataOutputStream.write(shardName);
1:           dataOutputStream.writeInt(fileData.length);
1:           dataOutputStream.write(fileData);
1:           CarbonUtil.closeStream(dataInputStream);
1:         }
1:       } catch (IOException e) {
1:         LOGGER.error(e, "Error occurs while merge bloom index file of column: " + indexCol);
1:         // delete merge shard of bloom index for this segment when failed
1:         FileFactory.deleteAllCarbonFilesOfDir(FileFactory.getCarbonFile(mergeShardPath));
1:         throw new RuntimeException(
1:             "Error occurs while merge bloom index file of column: " + indexCol);
1:       } finally {
1:         CarbonUtil.closeStreams(dataInputStream, dataOutputStream);
1:       }
1:     }
1:     // delete flag file and mergeShard can be used
1:     try {
1:       FileFactory.deleteFile(mergeInprogressFile, FileFactory.getFileType(mergeInprogressFile));
1:     } catch (IOException e) {
1:       LOGGER.error(e, "Error occurs while deleting file " + mergeInprogressFile);
1:       throw new RuntimeException("Error occurs while deleting file " + mergeInprogressFile);
1:     }
1:     // remove old store
1:     for (CarbonFile shardpath: shardPaths) {
1:       FileFactory.deleteAllCarbonFilesOfDir(shardpath);
1:     }
1:   }
1: 
1:   /**
1:    * load bloom filter from bloom index file
1:    */
1:   public static List<CarbonBloomFilter> loadBloomFilterFromFile(
1:           String shardPath, String colName) {
1:     if (shardPath.endsWith(MERGE_BLOOM_INDEX_SHARD_NAME)) {
1:       return loadMergeBloomIndex(shardPath, colName);
1:     } else {
1:       return loadBloomIndex(shardPath, colName);
1:     }
1:   }
1: 
1:   /**
1:    * load bloom filter of {@code colName} from {@code shardPath}
1:    */
1:   public static List<CarbonBloomFilter> loadBloomIndex(
1:           String shardPath, String colName) {
1:     DataInputStream dataInStream = null;
1:     List<CarbonBloomFilter> bloomFilters = new ArrayList<>();
1:     try {
1:       String indexFile = getBloomIndexFile(shardPath, colName);
1:       dataInStream = FileFactory.getDataInputStream(indexFile, FileFactory.getFileType(indexFile));
1:       while (dataInStream.available() > 0) {
1:         CarbonBloomFilter bloomFilter = new CarbonBloomFilter();
1:         bloomFilter.readFields(dataInStream);
1:         bloomFilter.setShardName(new Path(shardPath).getName());
1:         bloomFilters.add(bloomFilter);
1:       }
1:       LOGGER.info(String.format("Read %d bloom indices from %s", bloomFilters.size(), indexFile));
1: 
1:       return bloomFilters;
1:     } catch (IOException e) {
1:       LOGGER.error(e, "Error occurs while reading bloom index");
1:       throw new RuntimeException("Error occurs while reading bloom index", e);
1:     } finally {
1:       CarbonUtil.closeStreams(dataInStream);
1:     }
1:   }
1: 
1: 
1:   /**
1:    * load bloom filter of {@code colName} from {@code mergeShardPath}
1:    */
1:   public static List<CarbonBloomFilter> loadMergeBloomIndex(
1:           String mergeShardPath, String colName) {
1:     String mergeIndexFile = getMergeBloomIndexFile(mergeShardPath, colName);
1:     DataInputStream mergeIndexInStream = null;
1:     List<CarbonBloomFilter> bloomFilters = new ArrayList<>();
1:     try {
1:       mergeIndexInStream = FileFactory.getDataInputStream(
1:           mergeIndexFile, FileFactory.getFileType(mergeIndexFile));
1:       while (mergeIndexInStream.available() > 0) {
1:         // read shard name
1:         int shardNameByteLength = mergeIndexInStream.readInt();
1:         byte[] shardNameBytes = new byte[shardNameByteLength];
1:         mergeIndexInStream.readFully(shardNameBytes);
1:         String shardName = new String(shardNameBytes, Charset.forName("UTF-8"));
1:         // read bloom index file data
1:         int indexFileByteLength = mergeIndexInStream.readInt();
1:         byte[] indexFileBytes = new byte[indexFileByteLength];
1:         mergeIndexInStream.readFully(indexFileBytes);
1:         // warp byte array as input stream to get bloom filters
1:         ByteArrayInputStream byteArrayInputStream = new ByteArrayInputStream(indexFileBytes);
1:         DataInputStream indexDataInStream = new DataInputStream(byteArrayInputStream);
1:         while (indexDataInStream.available() > 0) {
1:           CarbonBloomFilter bloomFilter = new CarbonBloomFilter();
1:           bloomFilter.readFields(indexDataInStream);
1:           bloomFilter.setShardName(shardName);
1:           bloomFilters.add(bloomFilter);
1:         }
1:       }
1:       LOGGER.info(
1:           String.format("Read %d bloom indices from %s", bloomFilters.size(), mergeIndexFile));
1:       return bloomFilters;
1:     } catch (IOException e) {
1:       LOGGER.error(e, "Error occurs while reading merge bloom index");
1:       throw new RuntimeException("Error occurs while reading merge bloom index", e);
1:     } finally {
1:       CarbonUtil.closeStreams(mergeIndexInStream);
1:     }
1:   }
1: 
1:   /**
1:    * get bloom index file
1:    */
1:   public static String getBloomIndexFile(String shardPath, String colName) {
1:     return shardPath.concat(File.separator).concat(colName).concat(BLOOM_INDEX_SUFFIX);
1:   }
1: 
1:   /**
1:    * get merge bloom index file
1:    */
1:   public static String getMergeBloomIndexFile(String mergeShardPath, String colName) {
1:     return mergeShardPath.concat(File.separator).concat(colName).concat(MERGE_BLOOM_INDEX_SUFFIX);
1:   }
1: }
============================================================================