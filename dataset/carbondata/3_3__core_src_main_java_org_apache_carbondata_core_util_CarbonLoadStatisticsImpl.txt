1:cd6a4ff: /*
1:cd6a4ff:  * Licensed to the Apache Software Foundation (ASF) under one or more
1:cd6a4ff:  * contributor license agreements.  See the NOTICE file distributed with
1:cd6a4ff:  * this work for additional information regarding copyright ownership.
1:cd6a4ff:  * The ASF licenses this file to You under the Apache License, Version 2.0
1:cd6a4ff:  * (the "License"); you may not use this file except in compliance with
1:cd6a4ff:  * the License.  You may obtain a copy of the License at
1:cd6a4ff:  *
1:cd6a4ff:  *    http://www.apache.org/licenses/LICENSE-2.0
1:cd6a4ff:  *
1:cd6a4ff:  * Unless required by applicable law or agreed to in writing, software
1:cd6a4ff:  * distributed under the License is distributed on an "AS IS" BASIS,
1:cd6a4ff:  * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
1:cd6a4ff:  * See the License for the specific language governing permissions and
1:cd6a4ff:  * limitations under the License.
1:cd6a4ff:  */
1:cd6a4ff: 
1:cd6a4ff: package org.apache.carbondata.core.util;
1:cd6a4ff: 
1:cd6a4ff: import java.util.concurrent.ConcurrentHashMap;
1:cd6a4ff: 
1:cd6a4ff: import org.apache.carbondata.common.logging.LogService;
1:cd6a4ff: import org.apache.carbondata.common.logging.LogServiceFactory;
1:cd6a4ff: 
1:cd6a4ff: /**
1:cd6a4ff:  * A util which provide methods used to record time information druing data loading.
1:cd6a4ff:  */
1:cd6a4ff: public class CarbonLoadStatisticsImpl implements LoadStatistics {
1:cd6a4ff:   private CarbonLoadStatisticsImpl() {
1:cd6a4ff: 
1:cd6a4ff:   }
1:cd6a4ff: 
1:cd6a4ff:   private static CarbonLoadStatisticsImpl carbonLoadStatisticsImplInstance =
1:cd6a4ff:           new CarbonLoadStatisticsImpl();
1:cd6a4ff: 
1:cd6a4ff:   public static CarbonLoadStatisticsImpl getInstance() {
1:cd6a4ff:     return carbonLoadStatisticsImplInstance;
1:cd6a4ff:   }
1:cd6a4ff: 
1:cd6a4ff:   private final LogService LOGGER =
1:cd6a4ff:           LogServiceFactory.getLogService(CarbonLoadStatisticsImpl.class.getName());
1:cd6a4ff: 
1:cd6a4ff:   /*
1:cd6a4ff:    *We only care about the earliest start time(EST) and the latest end time(LET) of different
1:cd6a4ff:    *threads, who does the same thing, LET - EST is the cost time of doing one thing using
1:cd6a4ff:    *multiple thread.
1:cd6a4ff:  */
1:cd6a4ff:   private long loadCsvfilesToDfStartTime = 0;
1:cd6a4ff:   private long loadCsvfilesToDfCostTime = 0;
1:cd6a4ff:   private long dicShuffleAndWriteFileTotalStartTime = 0;
1:cd6a4ff:   private long dicShuffleAndWriteFileTotalCostTime = 0;
1:cd6a4ff: 
1:cd6a4ff:   //LRU cache load one time
1:cd6a4ff:   private double lruCacheLoadTime = 0;
1:cd6a4ff: 
1:cd6a4ff:   //Generate surrogate keys total time for each partition:
1:cd6a4ff:   private ConcurrentHashMap<String, Long[]> parDictionaryValuesTotalTimeMap =
1:cd6a4ff:           new ConcurrentHashMap<String, Long[]>();
1:cd6a4ff:   private ConcurrentHashMap<String, Long[]> parCsvInputStepTimeMap =
1:cd6a4ff:           new ConcurrentHashMap<String, Long[]>();
1:cd6a4ff:   private ConcurrentHashMap<String, Long[]> parGeneratingDictionaryValuesTimeMap =
1:cd6a4ff:           new ConcurrentHashMap<String, Long[]>();
1:cd6a4ff: 
1:cd6a4ff:   //Sort rows step total time for each partition:
1:cd6a4ff:   private ConcurrentHashMap<String, Long[]> parSortRowsStepTotalTimeMap =
1:cd6a4ff:           new ConcurrentHashMap<String, Long[]>();
1:cd6a4ff: 
1:cd6a4ff:   //MDK generate total time for each partition:
1:cd6a4ff:   private ConcurrentHashMap<String, Long[]> parMdkGenerateTotalTimeMap =
1:cd6a4ff:           new ConcurrentHashMap<String, Long[]>();
1:cd6a4ff:   private ConcurrentHashMap<String, Long[]> parDictionaryValue2MdkAdd2FileTime =
1:cd6a4ff:           new ConcurrentHashMap<String, Long[]>();
1:cd6a4ff: 
1:cd6a4ff:   //Node block process information
1:cd6a4ff:   private ConcurrentHashMap<String, Integer> hostBlockMap =
1:cd6a4ff:           new ConcurrentHashMap<String, Integer>();
1:cd6a4ff: 
1:cd6a4ff:   //Partition block process information
1:cd6a4ff:   private ConcurrentHashMap<String, Integer> partitionBlockMap =
1:cd6a4ff:           new ConcurrentHashMap<String, Integer>();
1:cd6a4ff: 
1:cd6a4ff:   private long totalRecords = 0;
1:cd6a4ff:   private double totalTime = 0;
1:cd6a4ff: 
1:cd6a4ff:   @Override
1:e8da880:   public void initPartitionInfo(String PartitionId) {
1:cd6a4ff:     parDictionaryValuesTotalTimeMap.put(PartitionId, new Long[2]);
1:cd6a4ff:     parCsvInputStepTimeMap.put(PartitionId, new Long[2]);
1:cd6a4ff:     parSortRowsStepTotalTimeMap.put(PartitionId, new Long[2]);
1:cd6a4ff:     parGeneratingDictionaryValuesTimeMap.put(PartitionId, new Long[2]);
1:cd6a4ff:     parMdkGenerateTotalTimeMap.put(PartitionId, new Long[2]);
1:cd6a4ff:     parDictionaryValue2MdkAdd2FileTime.put(PartitionId, new Long[2]);
1:cd6a4ff:   }
1:cd6a4ff: 
1:cd6a4ff:   //Record the time
1:cd6a4ff:   public void recordDicShuffleAndWriteTime() {
1:cd6a4ff:     Long dicShuffleAndWriteTimePoint = System.currentTimeMillis();
1:cd6a4ff:     if (0 == dicShuffleAndWriteFileTotalStartTime) {
1:cd6a4ff:       dicShuffleAndWriteFileTotalStartTime = dicShuffleAndWriteTimePoint;
1:cd6a4ff:     }
1:cd6a4ff:     if (dicShuffleAndWriteTimePoint - dicShuffleAndWriteFileTotalStartTime >
1:cd6a4ff:             dicShuffleAndWriteFileTotalCostTime) {
1:cd6a4ff:       dicShuffleAndWriteFileTotalCostTime =
1:cd6a4ff:           dicShuffleAndWriteTimePoint - dicShuffleAndWriteFileTotalStartTime;
1:cd6a4ff:     }
1:cd6a4ff:   }
1:cd6a4ff: 
1:cd6a4ff:   public void recordLoadCsvfilesToDfTime() {
1:cd6a4ff:     Long loadCsvfilesToDfTimePoint = System.currentTimeMillis();
1:cd6a4ff:     if (0 == loadCsvfilesToDfStartTime) {
1:cd6a4ff:       loadCsvfilesToDfStartTime = loadCsvfilesToDfTimePoint;
1:cd6a4ff:     }
1:cd6a4ff:     if (loadCsvfilesToDfTimePoint - loadCsvfilesToDfStartTime > loadCsvfilesToDfCostTime) {
1:cd6a4ff:       loadCsvfilesToDfCostTime = loadCsvfilesToDfTimePoint - loadCsvfilesToDfStartTime;
1:cd6a4ff:     }
1:cd6a4ff:   }
1:cd6a4ff: 
1:cd6a4ff:   public double getLruCacheLoadTime() {
1:cd6a4ff:     return lruCacheLoadTime;
1:cd6a4ff:   }
1:cd6a4ff: 
1:cd6a4ff:   public void recordDictionaryValuesTotalTime(String partitionID,
1:cd6a4ff:       Long dictionaryValuesTotalTimeTimePoint) {
1:cd6a4ff:     if (null != parDictionaryValuesTotalTimeMap.get(partitionID)) {
1:cd6a4ff:       if (null == parDictionaryValuesTotalTimeMap.get(partitionID)[0]) {
1:cd6a4ff:         parDictionaryValuesTotalTimeMap.get(partitionID)[0] = dictionaryValuesTotalTimeTimePoint;
1:cd6a4ff:       }
1:cd6a4ff:       if (null == parDictionaryValuesTotalTimeMap.get(partitionID)[1] ||
1:cd6a4ff:           dictionaryValuesTotalTimeTimePoint - parDictionaryValuesTotalTimeMap.get(partitionID)[0] >
1:cd6a4ff:               parDictionaryValuesTotalTimeMap.get(partitionID)[1]) {
1:cd6a4ff:         parDictionaryValuesTotalTimeMap.get(partitionID)[1] = dictionaryValuesTotalTimeTimePoint -
1:cd6a4ff:             parDictionaryValuesTotalTimeMap.get(partitionID)[0];
1:cd6a4ff:       }
1:cd6a4ff:     }
1:cd6a4ff:   }
1:cd6a4ff: 
1:cd6a4ff:   public void recordCsvInputStepTime(String partitionID,
1:cd6a4ff:       Long csvInputStepTimePoint) {
1:cd6a4ff:     if (null != parCsvInputStepTimeMap.get(partitionID)) {
1:cd6a4ff:       if (null == parCsvInputStepTimeMap.get(partitionID)[0]) {
1:cd6a4ff:         parCsvInputStepTimeMap.get(partitionID)[0] = csvInputStepTimePoint;
1:cd6a4ff:       }
1:cd6a4ff:       if (null == parCsvInputStepTimeMap.get(partitionID)[1] ||
1:cd6a4ff:               csvInputStepTimePoint - parCsvInputStepTimeMap.get(partitionID)[0] >
1:cd6a4ff:                       parCsvInputStepTimeMap.get(partitionID)[1]) {
1:cd6a4ff:         parCsvInputStepTimeMap.get(partitionID)[1] = csvInputStepTimePoint -
1:cd6a4ff:                 parCsvInputStepTimeMap.get(partitionID)[0];
1:cd6a4ff:       }
1:cd6a4ff:     }
1:cd6a4ff:   }
1:cd6a4ff: 
1:cd6a4ff:   public void recordLruCacheLoadTime(double lruCacheLoadTime) {
1:cd6a4ff:     this.lruCacheLoadTime = lruCacheLoadTime;
1:cd6a4ff:   }
1:cd6a4ff: 
1:cd6a4ff:   public void recordGeneratingDictionaryValuesTime(String partitionID,
1:cd6a4ff:       Long generatingDictionaryValuesTimePoint) {
1:cd6a4ff:     if (null != parGeneratingDictionaryValuesTimeMap.get(partitionID)) {
1:cd6a4ff:       if (null == parGeneratingDictionaryValuesTimeMap.get(partitionID)[0]) {
1:cd6a4ff:         parGeneratingDictionaryValuesTimeMap.get(partitionID)[0] =
1:cd6a4ff:                 generatingDictionaryValuesTimePoint;
1:cd6a4ff:       }
1:cd6a4ff:       if (null == parGeneratingDictionaryValuesTimeMap.get(partitionID)[1] ||
1:cd6a4ff:               generatingDictionaryValuesTimePoint - parGeneratingDictionaryValuesTimeMap
1:cd6a4ff:                       .get(partitionID)[0] > parGeneratingDictionaryValuesTimeMap
1:cd6a4ff:                       .get(partitionID)[1]) {
1:cd6a4ff:         parGeneratingDictionaryValuesTimeMap.get(partitionID)[1] =
1:cd6a4ff:                 generatingDictionaryValuesTimePoint - parGeneratingDictionaryValuesTimeMap
1:cd6a4ff:                         .get(partitionID)[0];
1:cd6a4ff:       }
1:cd6a4ff:     }
1:cd6a4ff:   }
1:cd6a4ff: 
1:cd6a4ff:   public void recordSortRowsStepTotalTime(String partitionID,
1:cd6a4ff:                                           Long sortRowsStepTotalTimePoint) {
1:cd6a4ff:     if (null != parSortRowsStepTotalTimeMap.get(partitionID)) {
1:cd6a4ff:       if (null == parSortRowsStepTotalTimeMap.get(partitionID)[0]) {
1:cd6a4ff:         parSortRowsStepTotalTimeMap.get(partitionID)[0] = sortRowsStepTotalTimePoint;
1:cd6a4ff:       }
1:cd6a4ff:       if (null == parSortRowsStepTotalTimeMap.get(partitionID)[1] ||
1:cd6a4ff:               sortRowsStepTotalTimePoint - parSortRowsStepTotalTimeMap.get(partitionID)[0] >
1:cd6a4ff:                       parSortRowsStepTotalTimeMap.get(partitionID)[1]) {
1:cd6a4ff:         parSortRowsStepTotalTimeMap.get(partitionID)[1] = sortRowsStepTotalTimePoint -
1:cd6a4ff:                 parSortRowsStepTotalTimeMap.get(partitionID)[0];
1:cd6a4ff:       }
1:cd6a4ff:     }
1:cd6a4ff:   }
1:cd6a4ff: 
1:cd6a4ff:   public void recordMdkGenerateTotalTime(String partitionID,
1:cd6a4ff:                                          Long mdkGenerateTotalTimePoint) {
1:cd6a4ff:     if (null != parMdkGenerateTotalTimeMap.get(partitionID)) {
1:cd6a4ff:       if (null == parMdkGenerateTotalTimeMap.get(partitionID)[0]) {
1:cd6a4ff:         parMdkGenerateTotalTimeMap.get(partitionID)[0] = mdkGenerateTotalTimePoint;
1:cd6a4ff:       }
1:cd6a4ff:       if (null == parMdkGenerateTotalTimeMap.get(partitionID)[1] ||
1:cd6a4ff:               mdkGenerateTotalTimePoint - parMdkGenerateTotalTimeMap.get(partitionID)[0] >
1:cd6a4ff:                       parMdkGenerateTotalTimeMap.get(partitionID)[1]) {
1:cd6a4ff:         parMdkGenerateTotalTimeMap.get(partitionID)[1] = mdkGenerateTotalTimePoint -
1:cd6a4ff:                 parMdkGenerateTotalTimeMap.get(partitionID)[0];
1:cd6a4ff:       }
1:cd6a4ff:     }
1:cd6a4ff:   }
1:cd6a4ff: 
1:cd6a4ff:   public void recordDictionaryValue2MdkAdd2FileTime(String partitionID,
1:cd6a4ff:       Long dictionaryValue2MdkAdd2FileTimePoint) {
1:cd6a4ff:     if (null != parDictionaryValue2MdkAdd2FileTime.get(partitionID)) {
1:cd6a4ff:       if (null == parDictionaryValue2MdkAdd2FileTime.get(partitionID)[0]) {
1:cd6a4ff:         parDictionaryValue2MdkAdd2FileTime.get(partitionID)[0] =
1:cd6a4ff:                 dictionaryValue2MdkAdd2FileTimePoint;
1:cd6a4ff:       }
1:cd6a4ff:       if (null == parDictionaryValue2MdkAdd2FileTime.get(partitionID)[1] ||
1:cd6a4ff:               dictionaryValue2MdkAdd2FileTimePoint - parDictionaryValue2MdkAdd2FileTime
1:cd6a4ff:                       .get(partitionID)[0] > parDictionaryValue2MdkAdd2FileTime
1:cd6a4ff:                       .get(partitionID)[1]) {
1:cd6a4ff:         parDictionaryValue2MdkAdd2FileTime.get(partitionID)[1] =
1:cd6a4ff:                 dictionaryValue2MdkAdd2FileTimePoint - parDictionaryValue2MdkAdd2FileTime
1:cd6a4ff:                         .get(partitionID)[0];
1:cd6a4ff:       }
1:cd6a4ff:     }
1:cd6a4ff:   }
1:cd6a4ff: 
1:cd6a4ff:   //Record the node blocks information map
1:cd6a4ff:   public void recordHostBlockMap(String host, Integer numBlocks) {
1:cd6a4ff:     hostBlockMap.put(host, numBlocks);
1:cd6a4ff:   }
1:cd6a4ff: 
1:cd6a4ff:   //Record the partition blocks information map
1:cd6a4ff:   public void recordPartitionBlockMap(String partitionID, Integer numBlocks) {
1:cd6a4ff:     partitionBlockMap.put(partitionID, numBlocks);
1:cd6a4ff:   }
1:cd6a4ff: 
1:cd6a4ff:   public void recordTotalRecords(long totalRecords) {
1:cd6a4ff:     this.totalRecords = totalRecords;
1:cd6a4ff:   }
1:cd6a4ff: 
1:cd6a4ff:   //Get the time
1:cd6a4ff:   private double getDicShuffleAndWriteFileTotalTime() {
1:cd6a4ff:     return dicShuffleAndWriteFileTotalCostTime / 1000.0;
1:cd6a4ff:   }
1:cd6a4ff: 
1:cd6a4ff:   private double getLoadCsvfilesToDfTime() {
1:cd6a4ff:     return loadCsvfilesToDfCostTime / 1000.0;
1:cd6a4ff:   }
1:cd6a4ff: 
1:cd6a4ff:   private double getDictionaryValuesTotalTime(String partitionID) {
1:cd6a4ff:     return parDictionaryValuesTotalTimeMap.get(partitionID)[1] / 1000.0;
1:cd6a4ff:   }
1:cd6a4ff: 
1:cd6a4ff:   private double getCsvInputStepTime(String partitionID) {
1:cd6a4ff:     return parCsvInputStepTimeMap.get(partitionID)[1] / 1000.0;
1:cd6a4ff:   }
1:cd6a4ff: 
1:cd6a4ff:   private double getGeneratingDictionaryValuesTime(String partitionID) {
1:cd6a4ff:     return parGeneratingDictionaryValuesTimeMap.get(partitionID)[1] / 1000.0;
1:cd6a4ff:   }
1:cd6a4ff: 
1:cd6a4ff:   private double getSortRowsStepTotalTime(String partitionID) {
1:cd6a4ff:     return parSortRowsStepTotalTimeMap.get(partitionID)[1] / 1000.0;
1:cd6a4ff:   }
1:cd6a4ff: 
1:cd6a4ff:   private double getDictionaryValue2MdkAdd2FileTime(String partitionID) {
1:cd6a4ff:     return parDictionaryValue2MdkAdd2FileTime.get(partitionID)[1] / 1000.0;
1:cd6a4ff:   }
1:cd6a4ff: 
1:cd6a4ff:   //Get the hostBlockMap
1:cd6a4ff:   private ConcurrentHashMap<String, Integer> getHostBlockMap() {
1:cd6a4ff:     return hostBlockMap;
1:cd6a4ff:   }
1:cd6a4ff: 
1:cd6a4ff:   //Get the partitionBlockMap
1:cd6a4ff:   private ConcurrentHashMap<String, Integer> getPartitionBlockMap() {
1:cd6a4ff:     return partitionBlockMap;
1:cd6a4ff:   }
1:cd6a4ff: 
1:cd6a4ff:   //Speed calculate
1:cd6a4ff:   private long getTotalRecords() {
1:cd6a4ff:     return this.totalRecords;
1:cd6a4ff:   }
1:cd6a4ff: 
1:cd6a4ff:   private int getLoadSpeed() {
1:cd6a4ff:     return (int)(totalRecords / totalTime);
1:cd6a4ff:   }
1:cd6a4ff: 
1:cd6a4ff:   private int getGenDicSpeed() {
1:cd6a4ff:     return (int)(totalRecords / getLoadCsvfilesToDfTime() + getDicShuffleAndWriteFileTotalTime());
1:cd6a4ff:   }
1:cd6a4ff: 
1:cd6a4ff:   private int getReadCSVSpeed(String partitionID) {
1:cd6a4ff:     return (int)(totalRecords / getCsvInputStepTime(partitionID));
1:cd6a4ff:   }
1:cd6a4ff: 
1:cd6a4ff:   private int getGenSurKeySpeed(String partitionID) {
1:cd6a4ff:     return (int)(totalRecords / getGeneratingDictionaryValuesTime(partitionID));
1:cd6a4ff:   }
1:cd6a4ff: 
1:cd6a4ff:   private int getSortKeySpeed(String partitionID) {
1:cd6a4ff:     return (int)(totalRecords / getSortRowsStepTotalTime(partitionID));
1:cd6a4ff:   }
1:cd6a4ff: 
1:cd6a4ff:   private int getMDKSpeed(String partitionID) {
1:cd6a4ff:     return (int)(totalRecords / getDictionaryValue2MdkAdd2FileTime(partitionID));
1:cd6a4ff:   }
1:cd6a4ff: 
1:cd6a4ff:   private double getTotalTime(String partitionID) {
1:cd6a4ff:     this.totalTime = getLoadCsvfilesToDfTime() + getDicShuffleAndWriteFileTotalTime() +
1:cd6a4ff:         getLruCacheLoadTime() + getDictionaryValuesTotalTime(partitionID) +
1:cd6a4ff:         getDictionaryValue2MdkAdd2FileTime(partitionID);
1:cd6a4ff:     return totalTime;
1:cd6a4ff:   }
1:cd6a4ff: 
1:cd6a4ff:   //Print the statistics information
1:cd6a4ff:   private void printDicGenStatisticsInfo() {
1:cd6a4ff:     double loadCsvfilesToDfTime = getLoadCsvfilesToDfTime();
1:873c3de:     LOGGER.info("STAGE 1 ->Load csv to DataFrame and generate" +
1:cd6a4ff:             " block distinct values: " + loadCsvfilesToDfTime + "(s)");
1:cd6a4ff:     double dicShuffleAndWriteFileTotalTime = getDicShuffleAndWriteFileTotalTime();
1:873c3de:     LOGGER.info("STAGE 2 ->Global dict shuffle and write dict file: " +
1:cd6a4ff:             + dicShuffleAndWriteFileTotalTime + "(s)");
1:cd6a4ff:   }
1:cd6a4ff: 
1:cd6a4ff:   private void printLruCacheLoadTimeInfo() {
1:873c3de:     LOGGER.info("STAGE 3 ->LRU cache load: " + getLruCacheLoadTime() + "(s)");
1:cd6a4ff:   }
1:cd6a4ff: 
1:cd6a4ff:   private void printDictionaryValuesGenStatisticsInfo(String partitionID) {
1:cd6a4ff:     double dictionaryValuesTotalTime = getDictionaryValuesTotalTime(partitionID);
1:873c3de:     LOGGER.info("STAGE 4 ->Total cost of gen dictionary values, sort and write to temp files: "
1:cd6a4ff:             + dictionaryValuesTotalTime + "(s)");
1:cd6a4ff:     double csvInputStepTime = getCsvInputStepTime(partitionID);
1:cd6a4ff:     double generatingDictionaryValuesTime = getGeneratingDictionaryValuesTime(partitionID);
1:873c3de:     LOGGER.info("STAGE 4.1 ->  |_read csv file: " + csvInputStepTime + "(s)");
1:873c3de:     LOGGER.info("STAGE 4.2 ->  |_transform to surrogate key: "
1:cd6a4ff:             + generatingDictionaryValuesTime + "(s)");
1:cd6a4ff:   }
1:cd6a4ff: 
1:cd6a4ff:   private void printSortRowsStepStatisticsInfo(String partitionID) {
1:cd6a4ff:     double sortRowsStepTotalTime = getSortRowsStepTotalTime(partitionID);
1:873c3de:     LOGGER.info("STAGE 4.3 ->  |_sort rows and write to temp file: "
1:cd6a4ff:             + sortRowsStepTotalTime + "(s)");
1:cd6a4ff:   }
1:cd6a4ff: 
1:cd6a4ff:   private void printGenMdkStatisticsInfo(String partitionID) {
1:cd6a4ff:     double dictionaryValue2MdkAdd2FileTime = getDictionaryValue2MdkAdd2FileTime(partitionID);
1:873c3de:     LOGGER.info("STAGE 5 ->Transform to MDK, compress and write fact files: "
1:cd6a4ff:             + dictionaryValue2MdkAdd2FileTime + "(s)");
1:cd6a4ff:   }
1:cd6a4ff: 
1:cd6a4ff:   //Print the node blocks information
1:cd6a4ff:   private void printHostBlockMapInfo() {
1:873c3de:     LOGGER.info("========== BLOCK_INFO ==========");
1:cd6a4ff:     if (getHostBlockMap().size() > 0) {
1:cd6a4ff:       for (String host: getHostBlockMap().keySet()) {
1:873c3de:         LOGGER.info("BLOCK_INFO ->Node host: " + host);
1:873c3de:         LOGGER.info("BLOCK_INFO ->The block count in this node: " + getHostBlockMap().get(host));
1:cd6a4ff:       }
1:cd6a4ff:     } else if (getPartitionBlockMap().size() > 0) {
1:cd6a4ff:       for (String parID: getPartitionBlockMap().keySet()) {
1:873c3de:         LOGGER.info("BLOCK_INFO ->Partition ID: " + parID);
1:873c3de:         LOGGER.info("BLOCK_INFO ->The block count in this partition: " +
1:cd6a4ff:                 getPartitionBlockMap().get(parID));
1:cd6a4ff:       }
1:cd6a4ff:     }
1:cd6a4ff:   }
1:cd6a4ff: 
1:cd6a4ff:   //Print the speed information
1:cd6a4ff:   private void printLoadSpeedInfo(String partitionID) {
1:873c3de:     LOGGER.info("===============Load_Speed_Info===============");
1:873c3de:     LOGGER.info("Total Num of Records Processed: " + getTotalRecords());
1:873c3de:     LOGGER.info("Total Time Cost: " + getTotalTime(partitionID) + "(s)");
1:873c3de:     LOGGER.info("Total Load Speed: " + getLoadSpeed() + "records/s");
1:873c3de:     LOGGER.info("Generate Dictionaries Speed: " + getGenDicSpeed() + "records/s");
1:873c3de:     LOGGER.info("Read CSV Speed: " + getReadCSVSpeed(partitionID) + " records/s");
1:873c3de:     LOGGER.info("Generate Surrogate Key Speed: " + getGenSurKeySpeed(partitionID) + " records/s");
1:873c3de:     LOGGER.info("Sort Key/Write Temp Files Speed: " + getSortKeySpeed(partitionID) + " records/s");
1:873c3de:     LOGGER.info("MDK Step Speed: " + getMDKSpeed(partitionID) + " records/s");
1:873c3de:     LOGGER.info("=============================================");
1:cd6a4ff:   }
1:cd6a4ff: 
1:cd6a4ff:   public void printStatisticsInfo(String partitionID) {
1:cd6a4ff:     try {
1:873c3de:       LOGGER.info("========== TIME_STATISTICS PartitionID: " + partitionID + "==========");
1:cd6a4ff:       printDicGenStatisticsInfo();
1:cd6a4ff:       printLruCacheLoadTimeInfo();
1:cd6a4ff:       printDictionaryValuesGenStatisticsInfo(partitionID);
1:cd6a4ff:       printSortRowsStepStatisticsInfo(partitionID);
1:cd6a4ff:       printGenMdkStatisticsInfo(partitionID);
1:cd6a4ff:       printHostBlockMapInfo();
1:cd6a4ff:       printLoadSpeedInfo(partitionID);
1:cd6a4ff:     } catch (Exception e) {
1:873c3de:       LOGGER.error("Can't print Statistics Information");
1:cd6a4ff:     } finally {
1:cd6a4ff:       resetLoadStatistics();
1:cd6a4ff:     }
1:cd6a4ff:   }
1:cd6a4ff: 
1:cd6a4ff:   //Reset the load statistics values
1:cd6a4ff:   private void resetLoadStatistics() {
1:cd6a4ff:     loadCsvfilesToDfStartTime = 0;
1:cd6a4ff:     loadCsvfilesToDfCostTime = 0;
1:cd6a4ff:     dicShuffleAndWriteFileTotalStartTime = 0;
1:cd6a4ff:     dicShuffleAndWriteFileTotalCostTime = 0;
1:cd6a4ff:     lruCacheLoadTime = 0;
1:cd6a4ff:     totalRecords = 0;
1:cd6a4ff:     totalTime = 0;
1:cd6a4ff:     parDictionaryValuesTotalTimeMap.clear();
1:cd6a4ff:     parCsvInputStepTimeMap.clear();
1:cd6a4ff:     parSortRowsStepTotalTimeMap.clear();
1:cd6a4ff:     parGeneratingDictionaryValuesTimeMap.clear();
1:cd6a4ff:     parMdkGenerateTotalTimeMap.clear();
1:cd6a4ff:     parDictionaryValue2MdkAdd2FileTime.clear();
1:cd6a4ff:   }
1:cd6a4ff: 
1:cd6a4ff: }
============================================================================
author:xubo245
-------------------------------------------------------------------------------
commit:e8da880
/////////////////////////////////////////////////////////////////////////
1:   public void initPartitionInfo(String PartitionId) {
author:Jacky Li
-------------------------------------------------------------------------------
commit:873c3de
/////////////////////////////////////////////////////////////////////////
1:     LOGGER.info("STAGE 1 ->Load csv to DataFrame and generate" +
1:     LOGGER.info("STAGE 2 ->Global dict shuffle and write dict file: " +
1:     LOGGER.info("STAGE 3 ->LRU cache load: " + getLruCacheLoadTime() + "(s)");
1:     LOGGER.info("STAGE 4 ->Total cost of gen dictionary values, sort and write to temp files: "
1:     LOGGER.info("STAGE 4.1 ->  |_read csv file: " + csvInputStepTime + "(s)");
1:     LOGGER.info("STAGE 4.2 ->  |_transform to surrogate key: "
1:     LOGGER.info("STAGE 4.3 ->  |_sort rows and write to temp file: "
1:     LOGGER.info("STAGE 5 ->Transform to MDK, compress and write fact files: "
1:     LOGGER.info("========== BLOCK_INFO ==========");
1:         LOGGER.info("BLOCK_INFO ->Node host: " + host);
1:         LOGGER.info("BLOCK_INFO ->The block count in this node: " + getHostBlockMap().get(host));
1:         LOGGER.info("BLOCK_INFO ->Partition ID: " + parID);
1:         LOGGER.info("BLOCK_INFO ->The block count in this partition: " +
/////////////////////////////////////////////////////////////////////////
1:     LOGGER.info("===============Load_Speed_Info===============");
1:     LOGGER.info("Total Num of Records Processed: " + getTotalRecords());
1:     LOGGER.info("Total Time Cost: " + getTotalTime(partitionID) + "(s)");
1:     LOGGER.info("Total Load Speed: " + getLoadSpeed() + "records/s");
1:     LOGGER.info("Generate Dictionaries Speed: " + getGenDicSpeed() + "records/s");
1:     LOGGER.info("Read CSV Speed: " + getReadCSVSpeed(partitionID) + " records/s");
1:     LOGGER.info("Generate Surrogate Key Speed: " + getGenSurKeySpeed(partitionID) + " records/s");
1:     LOGGER.info("Sort Key/Write Temp Files Speed: " + getSortKeySpeed(partitionID) + " records/s");
1:     LOGGER.info("MDK Step Speed: " + getMDKSpeed(partitionID) + " records/s");
1:     LOGGER.info("=============================================");
1:       LOGGER.info("========== TIME_STATISTICS PartitionID: " + partitionID + "==========");
/////////////////////////////////////////////////////////////////////////
1:       LOGGER.error("Can't print Statistics Information");
author:ravipesala
-------------------------------------------------------------------------------
commit:cd6a4ff
/////////////////////////////////////////////////////////////////////////
1: /*
1:  * Licensed to the Apache Software Foundation (ASF) under one or more
1:  * contributor license agreements.  See the NOTICE file distributed with
1:  * this work for additional information regarding copyright ownership.
1:  * The ASF licenses this file to You under the Apache License, Version 2.0
1:  * (the "License"); you may not use this file except in compliance with
1:  * the License.  You may obtain a copy of the License at
1:  *
1:  *    http://www.apache.org/licenses/LICENSE-2.0
1:  *
1:  * Unless required by applicable law or agreed to in writing, software
1:  * distributed under the License is distributed on an "AS IS" BASIS,
1:  * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
1:  * See the License for the specific language governing permissions and
1:  * limitations under the License.
1:  */
1: 
1: package org.apache.carbondata.core.util;
1: 
1: import java.util.concurrent.ConcurrentHashMap;
1: 
1: import org.apache.carbondata.common.logging.LogService;
1: import org.apache.carbondata.common.logging.LogServiceFactory;
1: 
1: /**
1:  * A util which provide methods used to record time information druing data loading.
1:  */
1: public class CarbonLoadStatisticsImpl implements LoadStatistics {
1:   private CarbonLoadStatisticsImpl() {
1: 
1:   }
1: 
1:   private static CarbonLoadStatisticsImpl carbonLoadStatisticsImplInstance =
1:           new CarbonLoadStatisticsImpl();
1: 
1:   public static CarbonLoadStatisticsImpl getInstance() {
1:     return carbonLoadStatisticsImplInstance;
1:   }
1: 
1:   private final LogService LOGGER =
1:           LogServiceFactory.getLogService(CarbonLoadStatisticsImpl.class.getName());
1: 
1:   /*
1:    *We only care about the earliest start time(EST) and the latest end time(LET) of different
1:    *threads, who does the same thing, LET - EST is the cost time of doing one thing using
1:    *multiple thread.
1:  */
1:   private long loadCsvfilesToDfStartTime = 0;
1:   private long loadCsvfilesToDfCostTime = 0;
1:   private long dicShuffleAndWriteFileTotalStartTime = 0;
1:   private long dicShuffleAndWriteFileTotalCostTime = 0;
1: 
1:   //LRU cache load one time
1:   private double lruCacheLoadTime = 0;
1: 
1:   //Generate surrogate keys total time for each partition:
1:   private ConcurrentHashMap<String, Long[]> parDictionaryValuesTotalTimeMap =
1:           new ConcurrentHashMap<String, Long[]>();
1:   private ConcurrentHashMap<String, Long[]> parCsvInputStepTimeMap =
1:           new ConcurrentHashMap<String, Long[]>();
1:   private ConcurrentHashMap<String, Long[]> parGeneratingDictionaryValuesTimeMap =
1:           new ConcurrentHashMap<String, Long[]>();
1: 
1:   //Sort rows step total time for each partition:
1:   private ConcurrentHashMap<String, Long[]> parSortRowsStepTotalTimeMap =
1:           new ConcurrentHashMap<String, Long[]>();
1: 
1:   //MDK generate total time for each partition:
1:   private ConcurrentHashMap<String, Long[]> parMdkGenerateTotalTimeMap =
1:           new ConcurrentHashMap<String, Long[]>();
1:   private ConcurrentHashMap<String, Long[]> parDictionaryValue2MdkAdd2FileTime =
1:           new ConcurrentHashMap<String, Long[]>();
1: 
1:   //Node block process information
1:   private ConcurrentHashMap<String, Integer> hostBlockMap =
1:           new ConcurrentHashMap<String, Integer>();
1: 
1:   //Partition block process information
1:   private ConcurrentHashMap<String, Integer> partitionBlockMap =
1:           new ConcurrentHashMap<String, Integer>();
1: 
1:   private long totalRecords = 0;
1:   private double totalTime = 0;
1: 
1:   @Override
0:   public void initPartitonInfo(String PartitionId) {
1:     parDictionaryValuesTotalTimeMap.put(PartitionId, new Long[2]);
1:     parCsvInputStepTimeMap.put(PartitionId, new Long[2]);
1:     parSortRowsStepTotalTimeMap.put(PartitionId, new Long[2]);
1:     parGeneratingDictionaryValuesTimeMap.put(PartitionId, new Long[2]);
1:     parMdkGenerateTotalTimeMap.put(PartitionId, new Long[2]);
1:     parDictionaryValue2MdkAdd2FileTime.put(PartitionId, new Long[2]);
1:   }
1: 
1:   //Record the time
1:   public void recordDicShuffleAndWriteTime() {
1:     Long dicShuffleAndWriteTimePoint = System.currentTimeMillis();
1:     if (0 == dicShuffleAndWriteFileTotalStartTime) {
1:       dicShuffleAndWriteFileTotalStartTime = dicShuffleAndWriteTimePoint;
1:     }
1:     if (dicShuffleAndWriteTimePoint - dicShuffleAndWriteFileTotalStartTime >
1:             dicShuffleAndWriteFileTotalCostTime) {
1:       dicShuffleAndWriteFileTotalCostTime =
1:           dicShuffleAndWriteTimePoint - dicShuffleAndWriteFileTotalStartTime;
1:     }
1:   }
1: 
1:   public void recordLoadCsvfilesToDfTime() {
1:     Long loadCsvfilesToDfTimePoint = System.currentTimeMillis();
1:     if (0 == loadCsvfilesToDfStartTime) {
1:       loadCsvfilesToDfStartTime = loadCsvfilesToDfTimePoint;
1:     }
1:     if (loadCsvfilesToDfTimePoint - loadCsvfilesToDfStartTime > loadCsvfilesToDfCostTime) {
1:       loadCsvfilesToDfCostTime = loadCsvfilesToDfTimePoint - loadCsvfilesToDfStartTime;
1:     }
1:   }
1: 
1:   public double getLruCacheLoadTime() {
1:     return lruCacheLoadTime;
1:   }
1: 
1:   public void recordDictionaryValuesTotalTime(String partitionID,
1:       Long dictionaryValuesTotalTimeTimePoint) {
1:     if (null != parDictionaryValuesTotalTimeMap.get(partitionID)) {
1:       if (null == parDictionaryValuesTotalTimeMap.get(partitionID)[0]) {
1:         parDictionaryValuesTotalTimeMap.get(partitionID)[0] = dictionaryValuesTotalTimeTimePoint;
1:       }
1:       if (null == parDictionaryValuesTotalTimeMap.get(partitionID)[1] ||
1:           dictionaryValuesTotalTimeTimePoint - parDictionaryValuesTotalTimeMap.get(partitionID)[0] >
1:               parDictionaryValuesTotalTimeMap.get(partitionID)[1]) {
1:         parDictionaryValuesTotalTimeMap.get(partitionID)[1] = dictionaryValuesTotalTimeTimePoint -
1:             parDictionaryValuesTotalTimeMap.get(partitionID)[0];
1:       }
1:     }
1:   }
1: 
1:   public void recordCsvInputStepTime(String partitionID,
1:       Long csvInputStepTimePoint) {
1:     if (null != parCsvInputStepTimeMap.get(partitionID)) {
1:       if (null == parCsvInputStepTimeMap.get(partitionID)[0]) {
1:         parCsvInputStepTimeMap.get(partitionID)[0] = csvInputStepTimePoint;
1:       }
1:       if (null == parCsvInputStepTimeMap.get(partitionID)[1] ||
1:               csvInputStepTimePoint - parCsvInputStepTimeMap.get(partitionID)[0] >
1:                       parCsvInputStepTimeMap.get(partitionID)[1]) {
1:         parCsvInputStepTimeMap.get(partitionID)[1] = csvInputStepTimePoint -
1:                 parCsvInputStepTimeMap.get(partitionID)[0];
1:       }
1:     }
1:   }
1: 
1:   public void recordLruCacheLoadTime(double lruCacheLoadTime) {
1:     this.lruCacheLoadTime = lruCacheLoadTime;
1:   }
1: 
1:   public void recordGeneratingDictionaryValuesTime(String partitionID,
1:       Long generatingDictionaryValuesTimePoint) {
1:     if (null != parGeneratingDictionaryValuesTimeMap.get(partitionID)) {
1:       if (null == parGeneratingDictionaryValuesTimeMap.get(partitionID)[0]) {
1:         parGeneratingDictionaryValuesTimeMap.get(partitionID)[0] =
1:                 generatingDictionaryValuesTimePoint;
1:       }
1:       if (null == parGeneratingDictionaryValuesTimeMap.get(partitionID)[1] ||
1:               generatingDictionaryValuesTimePoint - parGeneratingDictionaryValuesTimeMap
1:                       .get(partitionID)[0] > parGeneratingDictionaryValuesTimeMap
1:                       .get(partitionID)[1]) {
1:         parGeneratingDictionaryValuesTimeMap.get(partitionID)[1] =
1:                 generatingDictionaryValuesTimePoint - parGeneratingDictionaryValuesTimeMap
1:                         .get(partitionID)[0];
1:       }
1:     }
1:   }
1: 
1:   public void recordSortRowsStepTotalTime(String partitionID,
1:                                           Long sortRowsStepTotalTimePoint) {
1:     if (null != parSortRowsStepTotalTimeMap.get(partitionID)) {
1:       if (null == parSortRowsStepTotalTimeMap.get(partitionID)[0]) {
1:         parSortRowsStepTotalTimeMap.get(partitionID)[0] = sortRowsStepTotalTimePoint;
1:       }
1:       if (null == parSortRowsStepTotalTimeMap.get(partitionID)[1] ||
1:               sortRowsStepTotalTimePoint - parSortRowsStepTotalTimeMap.get(partitionID)[0] >
1:                       parSortRowsStepTotalTimeMap.get(partitionID)[1]) {
1:         parSortRowsStepTotalTimeMap.get(partitionID)[1] = sortRowsStepTotalTimePoint -
1:                 parSortRowsStepTotalTimeMap.get(partitionID)[0];
1:       }
1:     }
1:   }
1: 
1:   public void recordMdkGenerateTotalTime(String partitionID,
1:                                          Long mdkGenerateTotalTimePoint) {
1:     if (null != parMdkGenerateTotalTimeMap.get(partitionID)) {
1:       if (null == parMdkGenerateTotalTimeMap.get(partitionID)[0]) {
1:         parMdkGenerateTotalTimeMap.get(partitionID)[0] = mdkGenerateTotalTimePoint;
1:       }
1:       if (null == parMdkGenerateTotalTimeMap.get(partitionID)[1] ||
1:               mdkGenerateTotalTimePoint - parMdkGenerateTotalTimeMap.get(partitionID)[0] >
1:                       parMdkGenerateTotalTimeMap.get(partitionID)[1]) {
1:         parMdkGenerateTotalTimeMap.get(partitionID)[1] = mdkGenerateTotalTimePoint -
1:                 parMdkGenerateTotalTimeMap.get(partitionID)[0];
1:       }
1:     }
1:   }
1: 
1:   public void recordDictionaryValue2MdkAdd2FileTime(String partitionID,
1:       Long dictionaryValue2MdkAdd2FileTimePoint) {
1:     if (null != parDictionaryValue2MdkAdd2FileTime.get(partitionID)) {
1:       if (null == parDictionaryValue2MdkAdd2FileTime.get(partitionID)[0]) {
1:         parDictionaryValue2MdkAdd2FileTime.get(partitionID)[0] =
1:                 dictionaryValue2MdkAdd2FileTimePoint;
1:       }
1:       if (null == parDictionaryValue2MdkAdd2FileTime.get(partitionID)[1] ||
1:               dictionaryValue2MdkAdd2FileTimePoint - parDictionaryValue2MdkAdd2FileTime
1:                       .get(partitionID)[0] > parDictionaryValue2MdkAdd2FileTime
1:                       .get(partitionID)[1]) {
1:         parDictionaryValue2MdkAdd2FileTime.get(partitionID)[1] =
1:                 dictionaryValue2MdkAdd2FileTimePoint - parDictionaryValue2MdkAdd2FileTime
1:                         .get(partitionID)[0];
1:       }
1:     }
1:   }
1: 
1:   //Record the node blocks information map
1:   public void recordHostBlockMap(String host, Integer numBlocks) {
1:     hostBlockMap.put(host, numBlocks);
1:   }
1: 
1:   //Record the partition blocks information map
1:   public void recordPartitionBlockMap(String partitionID, Integer numBlocks) {
1:     partitionBlockMap.put(partitionID, numBlocks);
1:   }
1: 
1:   public void recordTotalRecords(long totalRecords) {
1:     this.totalRecords = totalRecords;
1:   }
1: 
1:   //Get the time
1:   private double getDicShuffleAndWriteFileTotalTime() {
1:     return dicShuffleAndWriteFileTotalCostTime / 1000.0;
1:   }
1: 
1:   private double getLoadCsvfilesToDfTime() {
1:     return loadCsvfilesToDfCostTime / 1000.0;
1:   }
1: 
1:   private double getDictionaryValuesTotalTime(String partitionID) {
1:     return parDictionaryValuesTotalTimeMap.get(partitionID)[1] / 1000.0;
1:   }
1: 
1:   private double getCsvInputStepTime(String partitionID) {
1:     return parCsvInputStepTimeMap.get(partitionID)[1] / 1000.0;
1:   }
1: 
1:   private double getGeneratingDictionaryValuesTime(String partitionID) {
1:     return parGeneratingDictionaryValuesTimeMap.get(partitionID)[1] / 1000.0;
1:   }
1: 
1:   private double getSortRowsStepTotalTime(String partitionID) {
1:     return parSortRowsStepTotalTimeMap.get(partitionID)[1] / 1000.0;
1:   }
1: 
1:   private double getDictionaryValue2MdkAdd2FileTime(String partitionID) {
1:     return parDictionaryValue2MdkAdd2FileTime.get(partitionID)[1] / 1000.0;
1:   }
1: 
1:   //Get the hostBlockMap
1:   private ConcurrentHashMap<String, Integer> getHostBlockMap() {
1:     return hostBlockMap;
1:   }
1: 
1:   //Get the partitionBlockMap
1:   private ConcurrentHashMap<String, Integer> getPartitionBlockMap() {
1:     return partitionBlockMap;
1:   }
1: 
1:   //Speed calculate
1:   private long getTotalRecords() {
1:     return this.totalRecords;
1:   }
1: 
1:   private int getLoadSpeed() {
1:     return (int)(totalRecords / totalTime);
1:   }
1: 
1:   private int getGenDicSpeed() {
1:     return (int)(totalRecords / getLoadCsvfilesToDfTime() + getDicShuffleAndWriteFileTotalTime());
1:   }
1: 
1:   private int getReadCSVSpeed(String partitionID) {
1:     return (int)(totalRecords / getCsvInputStepTime(partitionID));
1:   }
1: 
1:   private int getGenSurKeySpeed(String partitionID) {
1:     return (int)(totalRecords / getGeneratingDictionaryValuesTime(partitionID));
1:   }
1: 
1:   private int getSortKeySpeed(String partitionID) {
1:     return (int)(totalRecords / getSortRowsStepTotalTime(partitionID));
1:   }
1: 
1:   private int getMDKSpeed(String partitionID) {
1:     return (int)(totalRecords / getDictionaryValue2MdkAdd2FileTime(partitionID));
1:   }
1: 
1:   private double getTotalTime(String partitionID) {
1:     this.totalTime = getLoadCsvfilesToDfTime() + getDicShuffleAndWriteFileTotalTime() +
1:         getLruCacheLoadTime() + getDictionaryValuesTotalTime(partitionID) +
1:         getDictionaryValue2MdkAdd2FileTime(partitionID);
1:     return totalTime;
1:   }
1: 
1:   //Print the statistics information
1:   private void printDicGenStatisticsInfo() {
1:     double loadCsvfilesToDfTime = getLoadCsvfilesToDfTime();
0:     LOGGER.audit("STAGE 1 ->Load csv to DataFrame and generate" +
1:             " block distinct values: " + loadCsvfilesToDfTime + "(s)");
1:     double dicShuffleAndWriteFileTotalTime = getDicShuffleAndWriteFileTotalTime();
0:     LOGGER.audit("STAGE 2 ->Global dict shuffle and write dict file: " +
1:             + dicShuffleAndWriteFileTotalTime + "(s)");
1:   }
1: 
1:   private void printLruCacheLoadTimeInfo() {
0:     LOGGER.audit("STAGE 3 ->LRU cache load: " + getLruCacheLoadTime() + "(s)");
1:   }
1: 
1:   private void printDictionaryValuesGenStatisticsInfo(String partitionID) {
1:     double dictionaryValuesTotalTime = getDictionaryValuesTotalTime(partitionID);
0:     LOGGER.audit("STAGE 4 ->Total cost of gen dictionary values, sort and write to temp files: "
1:             + dictionaryValuesTotalTime + "(s)");
1:     double csvInputStepTime = getCsvInputStepTime(partitionID);
1:     double generatingDictionaryValuesTime = getGeneratingDictionaryValuesTime(partitionID);
0:     LOGGER.audit("STAGE 4.1 ->  |_read csv file: " + csvInputStepTime + "(s)");
0:     LOGGER.audit("STAGE 4.2 ->  |_transform to surrogate key: "
1:             + generatingDictionaryValuesTime + "(s)");
1:   }
1: 
1:   private void printSortRowsStepStatisticsInfo(String partitionID) {
1:     double sortRowsStepTotalTime = getSortRowsStepTotalTime(partitionID);
0:     LOGGER.audit("STAGE 4.3 ->  |_sort rows and write to temp file: "
1:             + sortRowsStepTotalTime + "(s)");
1:   }
1: 
1:   private void printGenMdkStatisticsInfo(String partitionID) {
1:     double dictionaryValue2MdkAdd2FileTime = getDictionaryValue2MdkAdd2FileTime(partitionID);
0:     LOGGER.audit("STAGE 5 ->Transform to MDK, compress and write fact files: "
1:             + dictionaryValue2MdkAdd2FileTime + "(s)");
1:   }
1: 
1:   //Print the node blocks information
1:   private void printHostBlockMapInfo() {
0:     LOGGER.audit("========== BLOCK_INFO ==========");
1:     if (getHostBlockMap().size() > 0) {
1:       for (String host: getHostBlockMap().keySet()) {
0:         LOGGER.audit("BLOCK_INFO ->Node host: " + host);
0:         LOGGER.audit("BLOCK_INFO ->The block count in this node: " + getHostBlockMap().get(host));
1:       }
1:     } else if (getPartitionBlockMap().size() > 0) {
1:       for (String parID: getPartitionBlockMap().keySet()) {
0:         LOGGER.audit("BLOCK_INFO ->Partition ID: " + parID);
0:         LOGGER.audit("BLOCK_INFO ->The block count in this partition: " +
1:                 getPartitionBlockMap().get(parID));
1:       }
1:     }
1:   }
1: 
1:   //Print the speed information
1:   private void printLoadSpeedInfo(String partitionID) {
0:     LOGGER.audit("===============Load_Speed_Info===============");
0:     LOGGER.audit("Total Num of Records Processed: " + getTotalRecords());
0:     LOGGER.audit("Total Time Cost: " + getTotalTime(partitionID) + "(s)");
0:     LOGGER.audit("Total Load Speed: " + getLoadSpeed() + "records/s");
0:     LOGGER.audit("Generate Dictionaries Speed: " + getGenDicSpeed() + "records/s");
0:     LOGGER.audit("Read CSV Speed: " + getReadCSVSpeed(partitionID) + " records/s");
0:     LOGGER.audit("Generate Surrogate Key Speed: " + getGenSurKeySpeed(partitionID) + " records/s");
0:     LOGGER.audit("Sort Key/Write Temp Files Speed: " + getSortKeySpeed(partitionID) + " records/s");
0:     LOGGER.audit("MDK Step Speed: " + getMDKSpeed(partitionID) + " records/s");
0:     LOGGER.audit("=============================================");
1:   }
1: 
1:   public void printStatisticsInfo(String partitionID) {
1:     try {
0:       LOGGER.audit("========== TIME_STATISTICS PartitionID: " + partitionID + "==========");
1:       printDicGenStatisticsInfo();
1:       printLruCacheLoadTimeInfo();
1:       printDictionaryValuesGenStatisticsInfo(partitionID);
1:       printSortRowsStepStatisticsInfo(partitionID);
1:       printGenMdkStatisticsInfo(partitionID);
1:       printHostBlockMapInfo();
1:       printLoadSpeedInfo(partitionID);
1:     } catch (Exception e) {
0:       LOGGER.audit("Can't print Statistics Information");
1:     } finally {
1:       resetLoadStatistics();
1:     }
1:   }
1: 
1:   //Reset the load statistics values
1:   private void resetLoadStatistics() {
1:     loadCsvfilesToDfStartTime = 0;
1:     loadCsvfilesToDfCostTime = 0;
1:     dicShuffleAndWriteFileTotalStartTime = 0;
1:     dicShuffleAndWriteFileTotalCostTime = 0;
1:     lruCacheLoadTime = 0;
1:     totalRecords = 0;
1:     totalTime = 0;
1:     parDictionaryValuesTotalTimeMap.clear();
1:     parCsvInputStepTimeMap.clear();
1:     parSortRowsStepTotalTimeMap.clear();
1:     parGeneratingDictionaryValuesTimeMap.clear();
1:     parMdkGenerateTotalTimeMap.clear();
1:     parDictionaryValue2MdkAdd2FileTime.clear();
1:   }
1: 
1: }
============================================================================