1:5804d75: /*
1:5804d75:  * Licensed to the Apache Software Foundation (ASF) under one or more
1:5804d75:  * contributor license agreements.  See the NOTICE file distributed with
1:5804d75:  * this work for additional information regarding copyright ownership.
1:5804d75:  * The ASF licenses this file to You under the Apache License, Version 2.0
1:5804d75:  * (the "License"); you may not use this file except in compliance with
1:5804d75:  * the License.  You may obtain a copy of the License at
1:5804d75:  *
1:5804d75:  *    http://www.apache.org/licenses/LICENSE-2.0
1:5804d75:  *
1:5804d75:  * Unless required by applicable law or agreed to in writing, software
1:5804d75:  * distributed under the License is distributed on an "AS IS" BASIS,
1:5804d75:  * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
1:5804d75:  * See the License for the specific language governing permissions and
1:5804d75:  * limitations under the License.
1:5804d75:  */
1:5804d75: 
1:5804d75: package org.apache.carbondata.sdk.file;
1:5804d75: 
1:5804d75: import java.io.IOException;
1:5804d75: import java.util.Objects;
1:5804d75: import java.util.Random;
1:5804d75: import java.util.UUID;
1:5804d75: 
1:5804d75: import org.apache.carbondata.common.annotations.InterfaceAudience;
1:5804d75: import org.apache.carbondata.hadoop.api.CarbonTableOutputFormat;
1:5804d75: import org.apache.carbondata.hadoop.internal.ObjectArrayWritable;
1:5804d75: import org.apache.carbondata.processing.loading.model.CarbonLoadModel;
1:5804d75: 
1:5804d75: import org.apache.hadoop.conf.Configuration;
1:5804d75: import org.apache.hadoop.io.NullWritable;
1:5804d75: import org.apache.hadoop.mapreduce.JobID;
1:5804d75: import org.apache.hadoop.mapreduce.RecordWriter;
1:5804d75: import org.apache.hadoop.mapreduce.TaskAttemptContext;
1:5804d75: import org.apache.hadoop.mapreduce.TaskAttemptID;
1:5804d75: import org.apache.hadoop.mapreduce.TaskID;
1:5804d75: import org.apache.hadoop.mapreduce.TaskType;
1:5804d75: import org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl;
1:5804d75: 
1:5804d75: /**
1:5804d75:  * Writer Implementation to write Json Record to carbondata file.
1:5804d75:  * json writer requires the path of json file and carbon schema.
1:5804d75:  */
1:5804d75: @InterfaceAudience.User public class JsonCarbonWriter extends CarbonWriter {
1:5804d75:   private RecordWriter<NullWritable, ObjectArrayWritable> recordWriter;
1:5804d75:   private TaskAttemptContext context;
1:5804d75:   private ObjectArrayWritable writable;
1:5804d75: 
1:8f1a029:   JsonCarbonWriter(CarbonLoadModel loadModel, Configuration configuration) throws IOException {
1:8f1a029:     CarbonTableOutputFormat.setLoadModel(configuration, loadModel);
1:5804d75:     CarbonTableOutputFormat outputFormat = new CarbonTableOutputFormat();
1:5804d75:     JobID jobId = new JobID(UUID.randomUUID().toString(), 0);
1:5804d75:     Random random = new Random();
1:5804d75:     TaskID task = new TaskID(jobId, TaskType.MAP, random.nextInt());
1:5804d75:     TaskAttemptID attemptID = new TaskAttemptID(task, random.nextInt());
1:8f1a029:     TaskAttemptContextImpl context = new TaskAttemptContextImpl(configuration, attemptID);
1:5804d75:     this.recordWriter = outputFormat.getRecordWriter(context);
1:5804d75:     this.context = context;
1:5804d75:     this.writable = new ObjectArrayWritable();
1:5804d75:   }
1:5804d75: 
1:5804d75:   /**
1:5804d75:    * Write single row data, accepts one row of data as json string
1:5804d75:    *
1:5804d75:    * @param object (json row as a string)
1:5804d75:    * @throws IOException
1:5804d75:    */
1:5804d75:   @Override public void write(Object object) throws IOException {
1:5804d75:     Objects.requireNonNull(object, "Input cannot be null");
1:5804d75:     try {
1:5804d75:       String[] jsonString = new String[1];
1:5804d75:       jsonString[0] = (String) object;
1:5804d75:       writable.set(jsonString);
1:5804d75:       recordWriter.write(NullWritable.get(), writable);
1:5804d75:     } catch (Exception e) {
1:5804d75:       close();
1:5804d75:       throw new IOException(e);
1:5804d75:     }
1:5804d75:   }
1:5804d75: 
1:5804d75:   /**
1:5804d75:    * Flush and close the writer
1:5804d75:    */
1:5804d75:   @Override public void close() throws IOException {
1:5804d75:     try {
1:5804d75:       recordWriter.close(context);
1:5804d75:     } catch (InterruptedException e) {
1:5804d75:       throw new IOException(e);
1:5804d75:     }
1:5804d75:   }
1:5804d75: }
============================================================================
author:kunal642
-------------------------------------------------------------------------------
commit:8f1a029
/////////////////////////////////////////////////////////////////////////
/////////////////////////////////////////////////////////////////////////
1:   JsonCarbonWriter(CarbonLoadModel loadModel, Configuration configuration) throws IOException {
1:     CarbonTableOutputFormat.setLoadModel(configuration, loadModel);
1:     TaskAttemptContextImpl context = new TaskAttemptContextImpl(configuration, attemptID);
commit:2a9604c
/////////////////////////////////////////////////////////////////////////
0: import org.apache.carbondata.core.datastore.impl.FileFactory;
/////////////////////////////////////////////////////////////////////////
0:     Configuration OutputHadoopConf = FileFactory.getConfiguration();
author:ajantha-bhat
-------------------------------------------------------------------------------
commit:5804d75
/////////////////////////////////////////////////////////////////////////
1: /*
1:  * Licensed to the Apache Software Foundation (ASF) under one or more
1:  * contributor license agreements.  See the NOTICE file distributed with
1:  * this work for additional information regarding copyright ownership.
1:  * The ASF licenses this file to You under the Apache License, Version 2.0
1:  * (the "License"); you may not use this file except in compliance with
1:  * the License.  You may obtain a copy of the License at
1:  *
1:  *    http://www.apache.org/licenses/LICENSE-2.0
1:  *
1:  * Unless required by applicable law or agreed to in writing, software
1:  * distributed under the License is distributed on an "AS IS" BASIS,
1:  * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
1:  * See the License for the specific language governing permissions and
1:  * limitations under the License.
1:  */
1: 
1: package org.apache.carbondata.sdk.file;
1: 
1: import java.io.IOException;
1: import java.util.Objects;
1: import java.util.Random;
1: import java.util.UUID;
1: 
1: import org.apache.carbondata.common.annotations.InterfaceAudience;
1: import org.apache.carbondata.hadoop.api.CarbonTableOutputFormat;
1: import org.apache.carbondata.hadoop.internal.ObjectArrayWritable;
1: import org.apache.carbondata.processing.loading.model.CarbonLoadModel;
1: 
1: import org.apache.hadoop.conf.Configuration;
1: import org.apache.hadoop.io.NullWritable;
1: import org.apache.hadoop.mapreduce.JobID;
1: import org.apache.hadoop.mapreduce.RecordWriter;
1: import org.apache.hadoop.mapreduce.TaskAttemptContext;
1: import org.apache.hadoop.mapreduce.TaskAttemptID;
1: import org.apache.hadoop.mapreduce.TaskID;
1: import org.apache.hadoop.mapreduce.TaskType;
1: import org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl;
1: 
1: /**
1:  * Writer Implementation to write Json Record to carbondata file.
1:  * json writer requires the path of json file and carbon schema.
1:  */
1: @InterfaceAudience.User public class JsonCarbonWriter extends CarbonWriter {
1:   private RecordWriter<NullWritable, ObjectArrayWritable> recordWriter;
1:   private TaskAttemptContext context;
1:   private ObjectArrayWritable writable;
1: 
0:   JsonCarbonWriter(CarbonLoadModel loadModel) throws IOException {
0:     Configuration OutputHadoopConf = new Configuration();
0:     CarbonTableOutputFormat.setLoadModel(OutputHadoopConf, loadModel);
1:     CarbonTableOutputFormat outputFormat = new CarbonTableOutputFormat();
1:     JobID jobId = new JobID(UUID.randomUUID().toString(), 0);
1:     Random random = new Random();
1:     TaskID task = new TaskID(jobId, TaskType.MAP, random.nextInt());
1:     TaskAttemptID attemptID = new TaskAttemptID(task, random.nextInt());
0:     TaskAttemptContextImpl context = new TaskAttemptContextImpl(OutputHadoopConf, attemptID);
1:     this.recordWriter = outputFormat.getRecordWriter(context);
1:     this.context = context;
1:     this.writable = new ObjectArrayWritable();
1:   }
1: 
1:   /**
1:    * Write single row data, accepts one row of data as json string
1:    *
1:    * @param object (json row as a string)
1:    * @throws IOException
1:    */
1:   @Override public void write(Object object) throws IOException {
1:     Objects.requireNonNull(object, "Input cannot be null");
1:     try {
1:       String[] jsonString = new String[1];
1:       jsonString[0] = (String) object;
1:       writable.set(jsonString);
1:       recordWriter.write(NullWritable.get(), writable);
1:     } catch (Exception e) {
1:       close();
1:       throw new IOException(e);
1:     }
1:   }
1: 
1:   /**
1:    * Flush and close the writer
1:    */
1:   @Override public void close() throws IOException {
1:     try {
1:       recordWriter.close(context);
1:     } catch (InterruptedException e) {
1:       throw new IOException(e);
1:     }
1:   }
1: }
============================================================================