1:21c5fb1: /*
1:21c5fb1:  * Licensed to the Apache Software Foundation (ASF) under one or more
1:21c5fb1:  * contributor license agreements.  See the NOTICE file distributed with
1:21c5fb1:  * this work for additional information regarding copyright ownership.
1:21c5fb1:  * The ASF licenses this file to You under the Apache License, Version 2.0
1:21c5fb1:  * (the "License"); you may not use this file except in compliance with
1:21c5fb1:  * the License.  You may obtain a copy of the License at
1:21c5fb1:  *
1:21c5fb1:  *    http://www.apache.org/licenses/LICENSE-2.0
1:21c5fb1:  *
1:21c5fb1:  * Unless required by applicable law or agreed to in writing, software
1:21c5fb1:  * distributed under the License is distributed on an "AS IS" BASIS,
1:21c5fb1:  * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
1:21c5fb1:  * See the License for the specific language governing permissions and
1:21c5fb1:  * limitations under the License.
1:21c5fb1:  */
15:21c5fb1: 
1:21c5fb1: package org.apache.carbondata.datamap.lucene;
1:21c5fb1: 
1:f184de8: import java.io.File;
1:21c5fb1: import java.io.IOException;
1:f184de8: import java.nio.ByteBuffer;
1:f184de8: import java.util.HashMap;
1:9db662a: import java.util.List;
1:f184de8: import java.util.Map;
1:21c5fb1: 
1:21c5fb1: import org.apache.carbondata.common.logging.LogService;
1:21c5fb1: import org.apache.carbondata.common.logging.LogServiceFactory;
1:21c5fb1: import org.apache.carbondata.core.constants.CarbonCommonConstants;
1:9db662a: import org.apache.carbondata.core.datamap.Segment;
1:747be9b: import org.apache.carbondata.core.datamap.dev.DataMapBuilder;
1:21c5fb1: import org.apache.carbondata.core.datastore.impl.FileFactory;
1:9db662a: import org.apache.carbondata.core.metadata.schema.table.column.CarbonColumn;
1:21c5fb1: import org.apache.carbondata.core.util.CarbonProperties;
1:9db662a: import org.apache.carbondata.core.util.path.CarbonTablePath;
1:21c5fb1: 
1:f184de8: import static org.apache.carbondata.datamap.lucene.LuceneDataMapWriter.addData;
1:f184de8: import static org.apache.carbondata.datamap.lucene.LuceneDataMapWriter.addToCache;
1:f184de8: import static org.apache.carbondata.datamap.lucene.LuceneDataMapWriter.flushCache;
1:f184de8: 
1:21c5fb1: import org.apache.hadoop.fs.FileSystem;
1:21c5fb1: import org.apache.hadoop.fs.Path;
1:21c5fb1: import org.apache.lucene.analysis.Analyzer;
1:21c5fb1: import org.apache.lucene.analysis.standard.StandardAnalyzer;
1:21c5fb1: import org.apache.lucene.codecs.lucene50.Lucene50StoredFieldsFormat;
1:21c5fb1: import org.apache.lucene.codecs.lucene62.Lucene62Codec;
1:21c5fb1: import org.apache.lucene.index.IndexWriter;
1:21c5fb1: import org.apache.lucene.index.IndexWriterConfig;
1:21c5fb1: import org.apache.lucene.store.Directory;
1:21c5fb1: import org.apache.solr.store.hdfs.HdfsDirectory;
1:f184de8: import org.roaringbitmap.RoaringBitmap;
1:21c5fb1: 
1:747be9b: public class LuceneDataMapBuilder implements DataMapBuilder {
1:21c5fb1: 
1:21c5fb1:   private static final LogService LOGGER =
1:21c5fb1:       LogServiceFactory.getLogService(LuceneDataMapWriter.class.getName());
1:21c5fb1: 
1:9db662a:   private String dataMapPath;
1:21c5fb1: 
1:9db662a:   private List<CarbonColumn> indexColumns;
1:21c5fb1: 
1:21c5fb1:   private int columnsCount;
1:21c5fb1: 
1:21c5fb1:   private IndexWriter indexWriter = null;
1:21c5fb1: 
1:21c5fb1:   private Analyzer analyzer = null;
1:21c5fb1: 
1:f184de8:   private int writeCacheSize;
1:f184de8: 
1:f184de8:   private Map<LuceneDataMapWriter.LuceneColumnKeys, Map<Integer, RoaringBitmap>> cache =
1:f184de8:       new HashMap<>();
1:f184de8: 
1:f184de8:   private ByteBuffer intBuffer = ByteBuffer.allocate(4);
1:f184de8: 
1:f184de8:   private boolean storeBlockletWise;
1:f184de8: 
1:f184de8:   private int currentBlockletId = -1;
1:f184de8: 
1:f184de8:   LuceneDataMapBuilder(String tablePath, String dataMapName, Segment segment, String shardName,
1:f184de8:       List<CarbonColumn> indexColumns, int writeCacheSize, boolean storeBlockletWise) {
1:f184de8:     this.dataMapPath = CarbonTablePath
1:f184de8:         .getDataMapStorePathOnShardName(tablePath, segment.getSegmentNo(), dataMapName, shardName);
1:21c5fb1:     this.indexColumns = indexColumns;
1:9db662a:     this.columnsCount = indexColumns.size();
1:f184de8:     this.writeCacheSize = writeCacheSize;
1:f184de8:     this.storeBlockletWise = storeBlockletWise;
5:21c5fb1:   }
1:21c5fb1: 
1:5c483f3:   @Override
1:21c5fb1:   public void initialize() throws IOException {
1:f184de8:     if (!storeBlockletWise) {
1:f184de8:       // get index path, put index data into segment's path
1:f184de8:       indexWriter = createIndexWriter(dataMapPath);
1:f184de8:     }
1:f184de8:   }
1:f184de8: 
1:f184de8:   private IndexWriter createIndexWriter(String dataMapPath) throws IOException {
1:9db662a:     Path indexPath = FileFactory.getPath(dataMapPath);
1:21c5fb1:     FileSystem fs = FileFactory.getFileSystem(indexPath);
1:21c5fb1: 
1:9db662a:     // if index path exists, should delete it because we are
1:9db662a:     // rebuilding the whole datamap for all segments
1:9db662a:     if (fs.exists(indexPath)) {
1:9db662a:       fs.delete(indexPath, true);
1:9db662a:     }
1:9db662a:     if (!fs.mkdirs(indexPath)) {
1:9db662a:       LOGGER.error("Failed to create directory " + indexPath);
1:21c5fb1:     }
1:21c5fb1: 
1:21c5fb1:     if (null == analyzer) {
1:21c5fb1:       analyzer = new StandardAnalyzer();
1:21c5fb1:     }
1:21c5fb1: 
1:21c5fb1:     // create a index writer
1:21c5fb1:     Directory indexDir = new HdfsDirectory(indexPath, FileFactory.getConfiguration());
1:21c5fb1: 
1:21c5fb1:     IndexWriterConfig indexWriterConfig = new IndexWriterConfig(analyzer);
1:21c5fb1:     if (CarbonProperties.getInstance()
1:21c5fb1:         .getProperty(CarbonCommonConstants.CARBON_LUCENE_COMPRESSION_MODE,
1:21c5fb1:             CarbonCommonConstants.CARBON_LUCENE_COMPRESSION_MODE_DEFAULT)
1:21c5fb1:         .equalsIgnoreCase(CarbonCommonConstants.CARBON_LUCENE_COMPRESSION_MODE_DEFAULT)) {
1:21c5fb1:       indexWriterConfig.setCodec(new Lucene62Codec(Lucene50StoredFieldsFormat.Mode.BEST_SPEED));
2:21c5fb1:     } else {
1:21c5fb1:       indexWriterConfig
1:21c5fb1:           .setCodec(new Lucene62Codec(Lucene50StoredFieldsFormat.Mode.BEST_COMPRESSION));
1:21c5fb1:     }
1:21c5fb1: 
1:f184de8:     return new IndexWriter(indexDir, new IndexWriterConfig(analyzer));
1:21c5fb1:   }
1:21c5fb1: 
1:9db662a:   @Override
1:f184de8:   public void addRow(int blockletId, int pageId, int rowId, Object[] values)
1:f184de8:       throws IOException {
1:f184de8:     if (storeBlockletWise) {
1:f184de8:       if (currentBlockletId != blockletId) {
1:f184de8:         close();
1:f184de8:         indexWriter = createIndexWriter(dataMapPath + File.separator + blockletId);
1:f184de8:         currentBlockletId = blockletId;
1:21c5fb1:       }
1:21c5fb1:     }
1:21c5fb1:     // add other fields
1:f184de8:     LuceneDataMapWriter.LuceneColumnKeys columns =
1:f184de8:         new LuceneDataMapWriter.LuceneColumnKeys(columnsCount);
1:21c5fb1:     for (int colIdx = 0; colIdx < columnsCount; colIdx++) {
1:f184de8:       columns.getColValues()[colIdx] = values[colIdx];
1:f184de8:     }
1:f184de8:     if (writeCacheSize > 0) {
1:f184de8:       addToCache(columns, rowId, pageId, blockletId, cache, intBuffer, storeBlockletWise);
1:f184de8:       flushCacheIfPossible();
1:f184de8:     } else {
1:f184de8:       addData(columns, rowId, pageId, blockletId, intBuffer, indexWriter, indexColumns,
1:f184de8:           storeBlockletWise);
1:21c5fb1:     }
1:21c5fb1: 
1:21c5fb1:   }
1:21c5fb1: 
1:f184de8:   private void flushCacheIfPossible() throws IOException {
1:f184de8:     if (cache.size() >= writeCacheSize) {
1:f184de8:       flushCache(cache, indexColumns, indexWriter, storeBlockletWise);
1:21c5fb1:     }
1:21c5fb1:   }
1:21c5fb1: 
1:9db662a:   @Override
1:21c5fb1:   public void finish() throws IOException {
1:f184de8:     flushCache(cache, indexColumns, indexWriter, storeBlockletWise);
1:21c5fb1:   }
1:21c5fb1: 
1:9db662a:   @Override
1:21c5fb1:   public void close() throws IOException {
1:21c5fb1:     if (indexWriter != null) {
1:21c5fb1:       indexWriter.close();
1:21c5fb1:     }
1:21c5fb1:   }
1:21c5fb1: 
1:9db662a:   @Override
1:5c483f3:   public boolean isIndexForCarbonRawBytes() {
1:5c483f3:     return false;
1:5c483f3:   }
1:21c5fb1: }
============================================================================
author:xuchuanyin
-------------------------------------------------------------------------------
commit:5c483f3
/////////////////////////////////////////////////////////////////////////
1:   @Override
1:   public boolean isIndexForCarbonRawBytes() {
1:     return false;
1:   }
author:ravipesala
-------------------------------------------------------------------------------
commit:f184de8
/////////////////////////////////////////////////////////////////////////
1: import java.io.File;
1: import java.nio.ByteBuffer;
1: import java.util.HashMap;
1: import java.util.Map;
/////////////////////////////////////////////////////////////////////////
1: import static org.apache.carbondata.datamap.lucene.LuceneDataMapWriter.addData;
1: import static org.apache.carbondata.datamap.lucene.LuceneDataMapWriter.addToCache;
1: import static org.apache.carbondata.datamap.lucene.LuceneDataMapWriter.flushCache;
1: 
1: import org.roaringbitmap.RoaringBitmap;
/////////////////////////////////////////////////////////////////////////
1:   private int writeCacheSize;
1: 
1:   private Map<LuceneDataMapWriter.LuceneColumnKeys, Map<Integer, RoaringBitmap>> cache =
1:       new HashMap<>();
1: 
1:   private ByteBuffer intBuffer = ByteBuffer.allocate(4);
1: 
1:   private boolean storeBlockletWise;
1: 
1:   private int currentBlockletId = -1;
1: 
1:   LuceneDataMapBuilder(String tablePath, String dataMapName, Segment segment, String shardName,
1:       List<CarbonColumn> indexColumns, int writeCacheSize, boolean storeBlockletWise) {
1:     this.dataMapPath = CarbonTablePath
1:         .getDataMapStorePathOnShardName(tablePath, segment.getSegmentNo(), dataMapName, shardName);
1:     this.writeCacheSize = writeCacheSize;
1:     this.storeBlockletWise = storeBlockletWise;
1:     if (!storeBlockletWise) {
1:       // get index path, put index data into segment's path
1:       indexWriter = createIndexWriter(dataMapPath);
1:     }
1:   }
1: 
1:   private IndexWriter createIndexWriter(String dataMapPath) throws IOException {
/////////////////////////////////////////////////////////////////////////
1:     return new IndexWriter(indexDir, new IndexWriterConfig(analyzer));
1:   public void addRow(int blockletId, int pageId, int rowId, Object[] values)
1:       throws IOException {
1:     if (storeBlockletWise) {
1:       if (currentBlockletId != blockletId) {
1:         close();
1:         indexWriter = createIndexWriter(dataMapPath + File.separator + blockletId);
1:         currentBlockletId = blockletId;
1:     LuceneDataMapWriter.LuceneColumnKeys columns =
1:         new LuceneDataMapWriter.LuceneColumnKeys(columnsCount);
1:       columns.getColValues()[colIdx] = values[colIdx];
1:     }
1:     if (writeCacheSize > 0) {
1:       addToCache(columns, rowId, pageId, blockletId, cache, intBuffer, storeBlockletWise);
1:       flushCacheIfPossible();
1:     } else {
1:       addData(columns, rowId, pageId, blockletId, intBuffer, indexWriter, indexColumns,
1:           storeBlockletWise);
1:   private void flushCacheIfPossible() throws IOException {
1:     if (cache.size() >= writeCacheSize) {
1:       flushCache(cache, indexColumns, indexWriter, storeBlockletWise);
1:     flushCache(cache, indexColumns, indexWriter, storeBlockletWise);
author:Jacky Li
-------------------------------------------------------------------------------
commit:747be9b
/////////////////////////////////////////////////////////////////////////
1: import org.apache.carbondata.core.datamap.dev.DataMapBuilder;
/////////////////////////////////////////////////////////////////////////
1: public class LuceneDataMapBuilder implements DataMapBuilder {
/////////////////////////////////////////////////////////////////////////
0:   LuceneDataMapBuilder(String tablePath, String dataMapName,
commit:9db662a
/////////////////////////////////////////////////////////////////////////
1: import java.util.List;
1: import org.apache.carbondata.core.datamap.Segment;
0: import org.apache.carbondata.core.datamap.dev.DataMapRefresher;
1: import org.apache.carbondata.core.metadata.schema.table.column.CarbonColumn;
1: import org.apache.carbondata.core.util.path.CarbonTablePath;
/////////////////////////////////////////////////////////////////////////
0: public class LuceneDataMapRefresher implements DataMapRefresher {
1:   private String dataMapPath;
1:   private List<CarbonColumn> indexColumns;
/////////////////////////////////////////////////////////////////////////
0:   LuceneDataMapRefresher(String tablePath, String dataMapName,
0:       Segment segment, String shardName, List<CarbonColumn> indexColumns) {
0:     this.dataMapPath = CarbonTablePath.getDataMapStorePathOnShardName(
0:         tablePath, segment.getSegmentNo(), dataMapName, shardName);
1:     this.columnsCount = indexColumns.size();
1:   @Override
1:     Path indexPath = FileFactory.getPath(dataMapPath);
1:     // if index path exists, should delete it because we are
1:     // rebuilding the whole datamap for all segments
1:     if (fs.exists(indexPath)) {
1:       fs.delete(indexPath, true);
1:     }
1:     if (!fs.mkdirs(indexPath)) {
1:       LOGGER.error("Failed to create directory " + indexPath);
/////////////////////////////////////////////////////////////////////////
0:     return new IndexWriter(ramDir, new IndexWriterConfig(analyzer));
/////////////////////////////////////////////////////////////////////////
0:     indexWriter.addIndexes(directory);
1:   @Override
0:   public void addRow(int blockletId, int pageId, int rowId, Object[] values) throws IOException {
/////////////////////////////////////////////////////////////////////////
0:     doc.add(new IntPoint(LuceneDataMapWriter.BLOCKLETID_NAME, (int) values[columnsCount]));
0:     doc.add(new IntPoint(LuceneDataMapWriter.PAGEID_NAME, (int) values[columnsCount + 1]));
0:     doc.add(new IntPoint(LuceneDataMapWriter.ROWID_NAME, rowId));
0:       CarbonColumn column = indexColumns.get(colIdx);
0:       addField(doc, column.getColName(), column.getDataType(), values[colIdx]);
0:     if (type == DataTypes.STRING) {
0:       doc.add(new TextField(fieldName, (String) value, Field.Store.NO));
0:     } else if (type == DataTypes.BYTE) {
/////////////////////////////////////////////////////////////////////////
0:       doc.add(new IntPoint(fieldName, (int) value));
0:       doc.add(new LongPoint(fieldName, (long) value));
0:       doc.add(new FloatPoint(fieldName, (float) value));
0:       doc.add(new DoublePoint(fieldName, (double) value));
/////////////////////////////////////////////////////////////////////////
1:   @Override
1:   @Override
author:QiangCai
-------------------------------------------------------------------------------
commit:21c5fb1
/////////////////////////////////////////////////////////////////////////
1: /*
1:  * Licensed to the Apache Software Foundation (ASF) under one or more
1:  * contributor license agreements.  See the NOTICE file distributed with
1:  * this work for additional information regarding copyright ownership.
1:  * The ASF licenses this file to You under the Apache License, Version 2.0
1:  * (the "License"); you may not use this file except in compliance with
1:  * the License.  You may obtain a copy of the License at
1:  *
1:  *    http://www.apache.org/licenses/LICENSE-2.0
1:  *
1:  * Unless required by applicable law or agreed to in writing, software
1:  * distributed under the License is distributed on an "AS IS" BASIS,
1:  * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
1:  * See the License for the specific language governing permissions and
1:  * limitations under the License.
1:  */
1: 
1: package org.apache.carbondata.datamap.lucene;
1: 
1: import java.io.IOException;
1: 
1: import org.apache.carbondata.common.logging.LogService;
1: import org.apache.carbondata.common.logging.LogServiceFactory;
1: import org.apache.carbondata.core.constants.CarbonCommonConstants;
1: import org.apache.carbondata.core.datastore.impl.FileFactory;
0: import org.apache.carbondata.core.metadata.datatype.DataType;
0: import org.apache.carbondata.core.metadata.datatype.DataTypes;
1: import org.apache.carbondata.core.util.CarbonProperties;
1: 
1: import org.apache.hadoop.fs.FileSystem;
1: import org.apache.hadoop.fs.Path;
1: import org.apache.lucene.analysis.Analyzer;
1: import org.apache.lucene.analysis.standard.StandardAnalyzer;
1: import org.apache.lucene.codecs.lucene50.Lucene50StoredFieldsFormat;
1: import org.apache.lucene.codecs.lucene62.Lucene62Codec;
0: import org.apache.lucene.document.Document;
0: import org.apache.lucene.document.DoublePoint;
0: import org.apache.lucene.document.Field;
0: import org.apache.lucene.document.FloatPoint;
0: import org.apache.lucene.document.IntPoint;
0: import org.apache.lucene.document.IntRangeField;
0: import org.apache.lucene.document.LongPoint;
0: import org.apache.lucene.document.StoredField;
0: import org.apache.lucene.document.TextField;
1: import org.apache.lucene.index.IndexWriter;
1: import org.apache.lucene.index.IndexWriterConfig;
1: import org.apache.lucene.store.Directory;
0: import org.apache.lucene.store.RAMDirectory;
1: import org.apache.solr.store.hdfs.HdfsDirectory;
1: 
0: public class LuceneIndexRefreshBuilder {
1: 
1:   private static final LogService LOGGER =
1:       LogServiceFactory.getLogService(LuceneDataMapWriter.class.getName());
1: 
0:   private String strIndexPath;
1: 
0:   private String[] indexColumns;
0:   private DataType[] dataTypes;
1: 
1:   private int columnsCount;
1: 
1:   private IndexWriter indexWriter = null;
1: 
0:   private IndexWriter pageIndexWriter = null;
1: 
1:   private Analyzer analyzer = null;
1: 
0:   public LuceneIndexRefreshBuilder(String strIndexPath, String[] indexColumns,
0:       DataType[] dataTypes) {
0:     this.strIndexPath = strIndexPath;
1:     this.indexColumns = indexColumns;
0:     this.columnsCount = indexColumns.length;
0:     this.dataTypes = dataTypes;
1:   }
1: 
1:   public void initialize() throws IOException {
0:     // get index path, put index data into segment's path
0:     Path indexPath = FileFactory.getPath(strIndexPath);
1:     FileSystem fs = FileFactory.getFileSystem(indexPath);
1: 
0:     // if index path not exists, create it
0:     if (!fs.exists(indexPath)) {
0:       fs.mkdirs(indexPath);
1:     }
1: 
1:     if (null == analyzer) {
1:       analyzer = new StandardAnalyzer();
1:     }
1: 
1:     // create a index writer
1:     Directory indexDir = new HdfsDirectory(indexPath, FileFactory.getConfiguration());
1: 
1:     IndexWriterConfig indexWriterConfig = new IndexWriterConfig(analyzer);
1:     if (CarbonProperties.getInstance()
1:         .getProperty(CarbonCommonConstants.CARBON_LUCENE_COMPRESSION_MODE,
1:             CarbonCommonConstants.CARBON_LUCENE_COMPRESSION_MODE_DEFAULT)
1:         .equalsIgnoreCase(CarbonCommonConstants.CARBON_LUCENE_COMPRESSION_MODE_DEFAULT)) {
1:       indexWriterConfig.setCodec(new Lucene62Codec(Lucene50StoredFieldsFormat.Mode.BEST_SPEED));
1:     } else {
1:       indexWriterConfig
1:           .setCodec(new Lucene62Codec(Lucene50StoredFieldsFormat.Mode.BEST_COMPRESSION));
1:     }
1: 
0:     indexWriter = new IndexWriter(indexDir, new IndexWriterConfig(analyzer));
1:   }
1: 
0:   private IndexWriter createPageIndexWriter() throws IOException {
0:     // save index data into ram, write into disk after one page finished
0:     RAMDirectory ramDir = new RAMDirectory();
0:     IndexWriter ramIndexWriter = new IndexWriter(ramDir, new IndexWriterConfig(analyzer));
1: 
0:     return ramIndexWriter;
1:   }
1: 
0:   private void addPageIndex(IndexWriter pageIndexWriter) throws IOException {
1: 
0:     Directory directory = pageIndexWriter.getDirectory();
1: 
0:     // close ram writer
0:     pageIndexWriter.close();
1: 
0:     // add ram index data into disk
0:     indexWriter.addIndexes(new Directory[] { directory });
1: 
0:     // delete this ram data
0:     directory.close();
1:   }
1: 
0:   public void addDocument(Object[] values) throws IOException {
1: 
0:     if (values.length != indexColumns.length + 3) {
0:       throw new IOException("The column number (" + values.length + ") of the row  is incorrect.");
1:     }
0:     int rowId = (int) values[indexColumns.length + 2];
0:     if (rowId == 0) {
0:       if (pageIndexWriter != null) {
0:         addPageIndex(pageIndexWriter);
1:       }
0:       pageIndexWriter = createPageIndexWriter();
1:     }
1: 
0:     // create a new document
0:     Document doc = new Document();
1: 
0:     // add blocklet Id
0:     doc.add(new IntPoint(LuceneDataMapWriter.BLOCKLETID_NAME,
0:         new int[] { (int) values[columnsCount] }));
0:     doc.add(new StoredField(LuceneDataMapWriter.BLOCKLETID_NAME, (int) values[columnsCount]));
1: 
0:     // add page id
0:     doc.add(new IntPoint(LuceneDataMapWriter.PAGEID_NAME,
0:         new int[] { (int) values[columnsCount + 1] }));
0:     doc.add(new StoredField(LuceneDataMapWriter.PAGEID_NAME, (int) values[columnsCount + 1]));
1: 
0:     // add row id
0:     doc.add(new IntPoint(LuceneDataMapWriter.ROWID_NAME, new int[] { rowId }));
0:     doc.add(new StoredField(LuceneDataMapWriter.ROWID_NAME, rowId));
1: 
1:     // add other fields
1:     for (int colIdx = 0; colIdx < columnsCount; colIdx++) {
0:       addField(doc, indexColumns[colIdx], dataTypes[colIdx], values[colIdx]);
1:     }
1: 
0:     pageIndexWriter.addDocument(doc);
1:   }
1: 
0:   private boolean addField(Document doc, String fieldName, DataType type, Object value) {
0:     if (type == DataTypes.BYTE) {
0:       // byte type , use int range to deal with byte, lucene has no byte type
0:       IntRangeField field =
0:           new IntRangeField(fieldName, new int[] { Byte.MIN_VALUE }, new int[] { Byte.MAX_VALUE });
0:       field.setIntValue((int) value);
0:       doc.add(field);
0:     } else if (type == DataTypes.SHORT) {
0:       // short type , use int range to deal with short type, lucene has no short type
0:       IntRangeField field = new IntRangeField(fieldName, new int[] { Short.MIN_VALUE },
0:           new int[] { Short.MAX_VALUE });
0:       field.setShortValue((short) value);
0:       doc.add(field);
0:     } else if (type == DataTypes.INT) {
0:       // int type , use int point to deal with int type
0:       doc.add(new IntPoint(fieldName, new int[] { (int) value }));
0:     } else if (type == DataTypes.LONG) {
0:       // long type , use long point to deal with long type
0:       doc.add(new LongPoint(fieldName, new long[] { (long) value }));
0:     } else if (type == DataTypes.FLOAT) {
0:       doc.add(new FloatPoint(fieldName, new float[] { (float) value }));
0:     } else if (type == DataTypes.DOUBLE) {
0:       doc.add(new DoublePoint(fieldName, new double[] { (double) value }));
0:     } else if (type == DataTypes.STRING) {
0:       doc.add(new TextField(fieldName, (String) value, Field.Store.NO));
0:     } else if (type == DataTypes.DATE) {
0:       // TODO: how to get data value
0:     } else if (type == DataTypes.TIMESTAMP) {
0:       // TODO: how to get
0:     } else if (type == DataTypes.BOOLEAN) {
0:       IntRangeField field = new IntRangeField(fieldName, new int[] { 0 }, new int[] { 1 });
0:       field.setIntValue((boolean) value ? 1 : 0);
0:       doc.add(field);
1:     } else {
0:       LOGGER.error("unsupport data type " + type);
0:       throw new RuntimeException("unsupported data type " + type);
1:     }
0:     return true;
1:   }
1: 
1:   public void finish() throws IOException {
0:     if (indexWriter != null && pageIndexWriter != null) {
0:       addPageIndex(pageIndexWriter);
1:     }
1:   }
1: 
1:   public void close() throws IOException {
1:     if (indexWriter != null) {
1:       indexWriter.close();
1:     }
1:   }
1: 
1: }
============================================================================