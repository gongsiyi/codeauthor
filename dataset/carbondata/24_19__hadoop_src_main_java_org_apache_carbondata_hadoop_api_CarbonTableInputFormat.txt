1:edb02ab: /*
1:41347d8:  * Licensed to the Apache Software Foundation (ASF) under one or more
1:41347d8:  * contributor license agreements.  See the NOTICE file distributed with
1:41347d8:  * this work for additional information regarding copyright ownership.
1:41347d8:  * The ASF licenses this file to You under the Apache License, Version 2.0
1:41347d8:  * (the "License"); you may not use this file except in compliance with
1:41347d8:  * the License.  You may obtain a copy of the License at
3:b681244:  *
1:edb02ab:  *    http://www.apache.org/licenses/LICENSE-2.0
1:b681244:  *
1:41347d8:  * Unless required by applicable law or agreed to in writing, software
1:41347d8:  * distributed under the License is distributed on an "AS IS" BASIS,
1:41347d8:  * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
1:41347d8:  * See the License for the specific language governing permissions and
1:41347d8:  * limitations under the License.
2:fd0bdf6:  */
11:b681244: 
1:edb02ab: package org.apache.carbondata.hadoop.api;
1:b681244: 
1:edb02ab: import java.io.IOException;
1:b681244: import java.util.ArrayList;
1:b681244: import java.util.Arrays;
1:b681244: import java.util.BitSet;
1:b681244: import java.util.HashMap;
1:fd0bdf6: import java.util.HashSet;
1:28f78b2: import java.util.LinkedList;
1:28f78b2: import java.util.List;
1:b681244: import java.util.Map;
1:b681244: 
1:f089287: import org.apache.carbondata.core.datamap.DataMapStoreManager;
1:8d3c774: import org.apache.carbondata.core.datamap.Segment;
1:f089287: import org.apache.carbondata.core.datamap.TableDataMap;
1:d7393da: import org.apache.carbondata.core.datastore.impl.FileFactory;
1:28f78b2: import org.apache.carbondata.core.indexstore.ExtendedBlocklet;
1:8d3c774: import org.apache.carbondata.core.indexstore.PartitionSpec;
1:b681244: import org.apache.carbondata.core.metadata.AbsoluteTableIdentifier;
1:b681244: import org.apache.carbondata.core.metadata.schema.PartitionInfo;
1:df5d7a9: import org.apache.carbondata.core.metadata.schema.SchemaReader;
1:4430178: import org.apache.carbondata.core.metadata.schema.partition.PartitionType;
1:b681244: import org.apache.carbondata.core.metadata.schema.table.CarbonTable;
1:79feac9: import org.apache.carbondata.core.metadata.schema.table.TableInfo;
1:b681244: import org.apache.carbondata.core.mutate.CarbonUpdateUtil;
1:b681244: import org.apache.carbondata.core.mutate.SegmentUpdateDetails;
1:b681244: import org.apache.carbondata.core.mutate.UpdateVO;
1:b681244: import org.apache.carbondata.core.mutate.data.BlockMappingVO;
1:280a400: import org.apache.carbondata.core.readcommitter.LatestFilesReadCommittedScope;
1:280a400: import org.apache.carbondata.core.readcommitter.ReadCommittedScope;
1:280a400: import org.apache.carbondata.core.readcommitter.TableStatusReadCommittedScope;
1:ce09aaa: import org.apache.carbondata.core.scan.expression.Expression;
1:b681244: import org.apache.carbondata.core.scan.filter.FilterExpressionProcessor;
1:ce09aaa: import org.apache.carbondata.core.scan.filter.resolver.FilterResolverIntf;
1:d7393da: import org.apache.carbondata.core.statusmanager.FileFormat;
1:c125f0c: import org.apache.carbondata.core.statusmanager.LoadMetadataDetails;
1:b681244: import org.apache.carbondata.core.statusmanager.SegmentStatusManager;
1:b681244: import org.apache.carbondata.core.statusmanager.SegmentUpdateStatusManager;
1:21a72bf: import org.apache.carbondata.core.stream.StreamFile;
1:21a72bf: import org.apache.carbondata.core.stream.StreamPruner;
1:b681244: import org.apache.carbondata.core.util.CarbonUtil;
1:b681244: import org.apache.carbondata.core.util.path.CarbonTablePath;
1:874764f: import org.apache.carbondata.hadoop.CarbonInputSplit;
1:b681244: 
1:b681244: import org.apache.commons.logging.Log;
1:b681244: import org.apache.commons.logging.LogFactory;
1:edb02ab: import org.apache.hadoop.conf.Configuration;
1:d7393da: import org.apache.hadoop.fs.BlockLocation;
1:d7393da: import org.apache.hadoop.fs.FileStatus;
1:b681244: import org.apache.hadoop.fs.FileSystem;
1:b681244: import org.apache.hadoop.fs.Path;
1:edb02ab: import org.apache.hadoop.mapreduce.InputSplit;
1:c16d543: import org.apache.hadoop.mapreduce.Job;
1:edb02ab: import org.apache.hadoop.mapreduce.JobContext;
1:b681244: import org.apache.hadoop.mapreduce.lib.input.FileSplit;
1:b681244: 
1:7e93d7b: /**
1:c09ef99:  * InputFormat for reading carbondata files with table level metadata support,
1:c09ef99:  * such as segment and explicit schema metadata.
1:b681244:  *
1:edb02ab:  * @param <T>
1:7e93d7b:  */
1:c09ef99: public class CarbonTableInputFormat<T> extends CarbonInputFormat<T> {
1:b681244: 
1:b681244:   // comma separated list of input segment numbers
1:b681244:   public static final String INPUT_SEGMENT_NUMBERS =
1:b681244:       "mapreduce.input.carboninputformat.segmentnumbers";
1:b681244:   // comma separated list of input files
1:b681244:   public static final String INPUT_FILES = "mapreduce.input.carboninputformat.files";
1:daa6465:   private static final String ALTER_PARTITION_ID = "mapreduce.input.carboninputformat.partitionid";
1:b681244:   private static final Log LOG = LogFactory.getLog(CarbonTableInputFormat.class);
1:b681244:   private static final String CARBON_READ_SUPPORT = "mapreduce.input.carboninputformat.readsupport";
1:09f7cdd:   private static final String CARBON_CONVERTER = "mapreduce.input.carboninputformat.converter";
1:b7b8073:   private static final String CARBON_TRANSACTIONAL_TABLE =
1:b7b8073:       "mapreduce.input.carboninputformat.transactional";
1:1155d4d:   public static final String DATABASE_NAME = "mapreduce.input.carboninputformat.databaseName";
1:1155d4d:   public static final String TABLE_NAME = "mapreduce.input.carboninputformat.tableName";
1:79feac9:   // a cache for carbon table, it will be used in task side
1:79feac9:   private CarbonTable carbonTable;
1:280a400:   private ReadCommittedScope readCommittedScope;
1:b681244: 
1:b681244:   /**
1:79feac9:    * Get the cached CarbonTable or create it by TableInfo in `configuration`
1:b681244:    */
1:531ecdf:   public CarbonTable getOrCreateCarbonTable(Configuration configuration) throws IOException {
1:79feac9:     if (carbonTable == null) {
1:79feac9:       // carbon table should be created either from deserialized table info (schema saved in
1:79feac9:       // hive metastore) or by reading schema in HDFS (schema saved in HDFS)
1:79feac9:       TableInfo tableInfo = getTableInfo(configuration);
1:79feac9:       CarbonTable carbonTable;
1:79feac9:       if (tableInfo != null) {
1:79feac9:         carbonTable = CarbonTable.buildFromTableInfo(tableInfo);
1:b7b8073:       } else {
1:79feac9:         carbonTable = SchemaReader.readCarbonTableFromStore(
1:79feac9:             getAbsoluteTableIdentifier(configuration));
1:45787fb:       }
1:79feac9:       this.carbonTable = carbonTable;
1:79feac9:       return carbonTable;
1:cc0e6f1:     } else {
1:79feac9:       return this.carbonTable;
1:cc0e6f1:     }
1:cc0e6f1:   }
1:cc0e6f1: 
1:b8a02f3:   /**
1:b681244:    * {@inheritDoc}
1:b681244:    * Configurations FileInputFormat.INPUT_DIR
1:b681244:    * are used to get table path to read.
1:b681244:    *
1:b681244:    * @param job
1:b681244:    * @return List<InputSplit> list of CarbonInputSplit
3:b681244:    * @throws IOException
1:b8a02f3:    */
1:daa6465:   @Override
1:daa6465:   public List<InputSplit> getSplits(JobContext job) throws IOException {
1:b681244:     AbsoluteTableIdentifier identifier = getAbsoluteTableIdentifier(job.getConfiguration());
1:cc0e6f1:     CarbonTable carbonTable = getOrCreateCarbonTable(job.getConfiguration());
1:cc0e6f1:     if (null == carbonTable) {
1:cc0e6f1:       throw new IOException("Missing/Corrupt schema file for table.");
1:cc0e6f1:     }
1:280a400:     this.readCommittedScope = getReadCommitted(job, identifier);
1:280a400:     LoadMetadataDetails[] loadMetadataDetails = readCommittedScope.getSegmentList();
1:cc0e6f1: 
1:c125f0c:     SegmentUpdateStatusManager updateStatusManager =
1:7e0803f:         new SegmentUpdateStatusManager(carbonTable, loadMetadataDetails);
1:8d3c774:     List<Segment> invalidSegments = new ArrayList<>();
1:b681244:     List<UpdateVO> invalidTimestampsList = new ArrayList<>();
1:8d3c774:     List<Segment> streamSegments = null;
1:8d3c774:     // get all valid segments and set them into the configuration
1:8f1a029:     SegmentStatusManager segmentStatusManager = new SegmentStatusManager(identifier,
1:8f1a029:         readCommittedScope.getConfiguration());
1:c58eb43:     SegmentStatusManager.ValidAndInvalidSegmentsInfo segments = segmentStatusManager
1:c58eb43:         .getValidAndInvalidSegments(loadMetadataDetails, this.readCommittedScope);
1:60dfdd3: 
1:fb1516c:     // to check whether only streaming segments access is enabled or not,
1:fb1516c:     // if access streaming segment is true then data will be read from streaming segments
1:fb1516c:     boolean accessStreamingSegments = getAccessStreamingSegments(job.getConfiguration());
1:fd0bdf6:     if (getValidateSegmentsToAccess(job.getConfiguration())) {
1:fb1516c:       if (!accessStreamingSegments) {
1:8d3c774:         List<Segment> validSegments = segments.getValidSegments();
1:d7393da:         streamSegments = segments.getStreamSegments();
1:c58eb43:         streamSegments = getFilteredSegment(job, streamSegments, true, readCommittedScope);
2:b681244:         if (validSegments.size() == 0) {
1:21a72bf:           return getSplitsOfStreaming(job, streamSegments, carbonTable);
1:b8a02f3:         }
1:fb1516c:         List<Segment> filteredSegmentToAccess =
1:c58eb43:             getFilteredSegment(job, segments.getValidSegments(), true, readCommittedScope);
1:fd0bdf6:         if (filteredSegmentToAccess.size() == 0) {
1:21a72bf:           return getSplitsOfStreaming(job, streamSegments, carbonTable);
1:fd0bdf6:         } else {
1:fd0bdf6:           setSegmentsToAccess(job.getConfiguration(), filteredSegmentToAccess);
1:b8a02f3:         }
1:fd0bdf6:       } else {
1:fb1516c:         List<Segment> filteredNormalSegments =
1:c58eb43:             getFilteredNormalSegments(job, segments.getValidSegments(),
1:c58eb43:                 getSegmentsToAccess(job, readCommittedScope));
2:fb1516c:         streamSegments = segments.getStreamSegments();
1:fb1516c:         if (filteredNormalSegments.size() == 0) {
1:21a72bf:           return getSplitsOfStreaming(job, streamSegments, carbonTable);
1:b8a02f3:         }
1:fb1516c:         setSegmentsToAccess(job.getConfiguration(),filteredNormalSegments);
1:0586146:       }
1:b681244:       // remove entry in the segment index if there are invalid segments
1:b681244:       invalidSegments.addAll(segments.getInvalidSegments());
1:0111587:       invalidTimestampsList.addAll(updateStatusManager.getInvalidTimestampRange());
1:b681244:       if (invalidSegments.size() > 0) {
1:56330ae:         DataMapStoreManager.getInstance()
1:56330ae:             .clearInvalidSegments(getOrCreateCarbonTable(job.getConfiguration()), invalidSegments);
6:b681244:       }
1:b681244:     }
1:fb1516c:     List<Segment> validAndInProgressSegments = new ArrayList<>(segments.getValidSegments());
1:8d3c774:     // Add in progress segments also to filter it as in case of aggregate table load it loads
1:8d3c774:     // data from in progress table.
1:8d3c774:     validAndInProgressSegments.addAll(segments.getListOfInProgressSegments());
1:34cb551:     // get updated filtered list
1:8d3c774:     List<Segment> filteredSegmentToAccess =
1:c58eb43:         getFilteredSegment(job, new ArrayList<>(validAndInProgressSegments), false,
1:c58eb43:             readCommittedScope);
1:eb771f5:     // Clean the updated segments from memory if the update happens on segments
1:7e93d7b:     refreshSegmentCacheIfRequired(job, carbonTable, updateStatusManager, filteredSegmentToAccess);
1:8d3c774: 
1:874764f:     // process and resolve the expression
1:874764f:     Expression filter = getFilterPredicates(job.getConfiguration());
1:874764f:     // this will be null in case of corrupt schema file.
1:5fc7f06:     PartitionInfo partitionInfo = carbonTable.getPartitionInfo(carbonTable.getTableName());
1:8d3c774: 
1:874764f:     // prune partitions for filter query on partition table
1:874764f:     BitSet matchedPartitions = null;
1:4430178:     if (partitionInfo != null && partitionInfo.getPartitionType() != PartitionType.NATIVE_HIVE) {
1:3894e1d:       carbonTable.processFilterExpression(filter, null, null);
1:cb51b86:       matchedPartitions = setMatchedPartitions(null, filter, partitionInfo, null);
1:874764f:       if (matchedPartitions != null) {
1:874764f:         if (matchedPartitions.cardinality() == 0) {
1:874764f:           return new ArrayList<InputSplit>();
1:874764f:         } else if (matchedPartitions.cardinality() == partitionInfo.getNumPartitions()) {
1:874764f:           matchedPartitions = null;
1:b681244:         }
1:b681244:       }
1:b681244:     }
1:b8a02f3: 
1:874764f:     // do block filtering and get split
1:874764f:     List<InputSplit> splits =
1:3894e1d:         getSplits(job, filter, filteredSegmentToAccess, matchedPartitions, partitionInfo,
1:c125f0c:             null, updateStatusManager);
1:874764f:     // pass the invalid segment to task side in order to remove index entry in task side
1:b681244:     if (invalidSegments.size() > 0) {
1:b681244:       for (InputSplit split : splits) {
1:b681244:         ((org.apache.carbondata.hadoop.CarbonInputSplit) split).setInvalidSegments(invalidSegments);
1:b681244:         ((org.apache.carbondata.hadoop.CarbonInputSplit) split)
1:b681244:             .setInvalidTimestampRange(invalidTimestampsList);
1:b681244:       }
1:b681244:     }
1:d7393da: 
1:d7393da:     // add all splits of streaming
1:3894e1d:     List<InputSplit> splitsOfStreaming = getSplitsOfStreaming(job, streamSegments, carbonTable);
1:d7393da:     if (!splitsOfStreaming.isEmpty()) {
1:d7393da:       splits.addAll(splitsOfStreaming);
1:d7393da:     }
1:874764f:     return splits;
1:b681244:   }
1:d7393da: 
1:b8a02f3:   /**
1:7e93d7b:    * Method to check and refresh segment cache
1:b681244:    *
1:7e93d7b:    * @param job
1:b681244:    * @param carbonTable
1:7e93d7b:    * @param updateStatusManager
1:7e93d7b:    * @param filteredSegmentToAccess
1:b681244:    * @throws IOException
1:b8a02f3:    */
1:7e93d7b:   public void refreshSegmentCacheIfRequired(JobContext job, CarbonTable carbonTable,
1:7e93d7b:       SegmentUpdateStatusManager updateStatusManager, List<Segment> filteredSegmentToAccess)
5:b681244:       throws IOException {
1:8d3c774:     List<Segment> toBeCleanedSegments = new ArrayList<>();
1:7e93d7b:     for (Segment filteredSegment : filteredSegmentToAccess) {
1:7e93d7b:       boolean refreshNeeded =
1:7e93d7b:           DataMapStoreManager.getInstance().getTableSegmentRefresher(carbonTable)
1:7e93d7b:               .isRefreshNeeded(filteredSegment,
1:7e93d7b:                   updateStatusManager.getInvalidTimestampRange(filteredSegment.getSegmentNo()));
1:7e93d7b:       if (refreshNeeded) {
1:7e93d7b:         toBeCleanedSegments.add(filteredSegment);
1:b681244:       }
1:b681244:     }
1:7e93d7b:     // Clean segments if refresh is needed
1:8d3c774:     for (Segment segment : filteredSegmentToAccess) {
1:7e93d7b:       if (DataMapStoreManager.getInstance().getTableSegmentRefresher(carbonTable)
1:8d3c774:           .isRefreshNeeded(segment.getSegmentNo())) {
1:7e93d7b:         toBeCleanedSegments.add(segment);
1:b681244:       }
1:b681244:     }
1:7e93d7b:     if (toBeCleanedSegments.size() > 0) {
1:7e93d7b:       DataMapStoreManager.getInstance()
1:7e93d7b:           .clearInvalidSegments(getOrCreateCarbonTable(job.getConfiguration()),
1:7e93d7b:               toBeCleanedSegments);
1:b681244:     }
1:b681244:   }
1:b681244: 
1:b681244:   /**
1:fb1516c:    * Below method will be used to get the filter segments when query is fired on pre Aggregate
1:fb1516c:    * and main table in case of streaming.
1:fb1516c:    * For Pre Aggregate rules it will set all the valid segments for both streaming and
1:fb1516c:    * and normal for fact table, so if any handoff happened in between it will
1:fb1516c:    * select only new hand off segments segments for fact.
1:fb1516c:    * @param job
1:fb1516c:    * @param validSegments
1:fb1516c:    * @param segmentsToAccess
1:b681244:    * @return
1:b681244:    */
1:fb1516c:   private List<Segment> getFilteredNormalSegments(JobContext job, List<Segment> validSegments,
1:fb1516c:       Segment[] segmentsToAccess) {
1:fb1516c:     List<Segment> segmentToAccessSet = Arrays.asList(segmentsToAccess);
1:fb1516c:     List<Segment> filteredSegment = new ArrayList<>();
1:fb1516c:     for (Segment seg : validSegments) {
1:fb1516c:       if (!segmentToAccessSet.contains(seg)) {
1:fb1516c:         filteredSegment.add(seg);
1:b681244:       }
1:b681244:     }
1:fb1516c:     return filteredSegment;
1:b681244:   }
1:b681244: 
1:b681244:   /**
1:c16d543:    * Return segment list after filtering out valid segments and segments set by user by
1:c16d543:    * `INPUT_SEGMENT_NUMBERS` in job configuration
1:b681244:    */
1:0c200d8:   private List<Segment> getFilteredSegment(JobContext job, List<Segment> validSegments,
1:c58eb43:       boolean validationRequired, ReadCommittedScope readCommittedScope) {
1:c58eb43:     Segment[] segmentsToAccess = getSegmentsToAccess(job, readCommittedScope);
1:8d3c774:     List<Segment> segmentToAccessSet =
1:8d3c774:         new ArrayList<>(new HashSet<>(Arrays.asList(segmentsToAccess)));
1:8d3c774:     List<Segment> filteredSegmentToAccess = new ArrayList<>();
1:8d3c774:     if (segmentsToAccess.length == 0 || segmentsToAccess[0].getSegmentNo().equalsIgnoreCase("*")) {
1:fd0bdf6:       filteredSegmentToAccess.addAll(validSegments);
1:b681244:     } else {
1:8d3c774:       for (Segment validSegment : validSegments) {
1:8d3c774:         int index = segmentToAccessSet.indexOf(validSegment);
1:8d3c774:         if (index > -1) {
1:8d3c774:           // In case of in progress reading segment, segment file name is set to the property itself
1:8d3c774:           if (segmentToAccessSet.get(index).getSegmentFileName() != null
1:8d3c774:               && validSegment.getSegmentFileName() == null) {
1:8d3c774:             filteredSegmentToAccess.add(segmentToAccessSet.get(index));
1:8d3c774:           } else {
1:8d3c774:             filteredSegmentToAccess.add(validSegment);
1:8d3c774:           }
1:b681244:         }
1:b681244:       }
1:0c200d8:       if (filteredSegmentToAccess.size() != segmentToAccessSet.size() && !validationRequired) {
1:0c200d8:         for (Segment segment : segmentToAccessSet) {
1:0c200d8:           if (!filteredSegmentToAccess.contains(segment)) {
1:0c200d8:             filteredSegmentToAccess.add(segment);
1:b681244:           }
1:b681244:         }
1:b681244:       }
1:fd0bdf6:       if (!filteredSegmentToAccess.containsAll(segmentToAccessSet)) {
1:8d3c774:         List<Segment> filteredSegmentToAccessTemp = new ArrayList<>(filteredSegmentToAccess);
1:fd0bdf6:         filteredSegmentToAccessTemp.removeAll(segmentToAccessSet);
1:fd0bdf6:         LOG.info(
1:fd0bdf6:             "Segments ignored are : " + Arrays.toString(filteredSegmentToAccessTemp.toArray()));
1:b681244:       }
1:b681244:     }
1:c16d543:     return filteredSegmentToAccess;
1:b681244:   }
1:21a72bf: 
1:21a72bf:   public List<InputSplit> getSplitsOfStreaming(JobContext job, List<Segment> streamSegments,
1:21a72bf:       CarbonTable carbonTable) throws IOException {
1:21a72bf:     return getSplitsOfStreaming(job, streamSegments, carbonTable, null);
1:21a72bf:   }
1:21a72bf: 
1:b681244:   /**
1:d7393da:    * use file list in .carbonindex file to get the split of streaming.
1:d7393da:    */
1:21a72bf:   public List<InputSplit> getSplitsOfStreaming(JobContext job, List<Segment> streamSegments,
1:21a72bf:       CarbonTable carbonTable, FilterResolverIntf filterResolverIntf) throws IOException {
1:d7393da:     List<InputSplit> splits = new ArrayList<InputSplit>();
1:d7393da:     if (streamSegments != null && !streamSegments.isEmpty()) {
1:d5bec4d:       numStreamSegments = streamSegments.size();
1:d7393da:       long minSize = Math.max(getFormatMinSplitSize(), getMinSplitSize(job));
1:d7393da:       long maxSize = getMaxSplitSize(job);
1:21a72bf:       if (filterResolverIntf == null) {
1:21a72bf:         if (carbonTable != null) {
1:21a72bf:           Expression filter = getFilterPredicates(job.getConfiguration());
1:21a72bf:           if (filter != null) {
1:21a72bf:             carbonTable.processFilterExpression(filter, null, null);
1:3894e1d:             filterResolverIntf =
1:3894e1d:                 CarbonTable.resolveFilter(filter, carbonTable.getAbsoluteTableIdentifier());
1:21a72bf:           }
1:21a72bf:         }
1:21a72bf:       }
1:21a72bf:       StreamPruner streamPruner = new StreamPruner(carbonTable);
1:21a72bf:       streamPruner.init(filterResolverIntf);
1:21a72bf:       List<StreamFile> streamFiles = streamPruner.prune(streamSegments);
1:0528a79:       // record the hit information of the streaming files
1:0528a79:       this.hitedStreamFiles = streamFiles.size();
1:0528a79:       this.numStreamFiles = streamPruner.getTotalFileNums();
1:21a72bf:       for (StreamFile streamFile : streamFiles) {
1:21a72bf:         Path path = new Path(streamFile.getFilePath());
1:21a72bf:         long length = streamFile.getFileSize();
1:21a72bf:         if (length != 0) {
1:21a72bf:           BlockLocation[] blkLocations;
1:21a72bf:           FileSystem fs = FileFactory.getFileSystem(path);
1:21a72bf:           FileStatus file = fs.getFileStatus(path);
1:21a72bf:           blkLocations = fs.getFileBlockLocations(path, 0, length);
1:21a72bf:           long blockSize = file.getBlockSize();
1:21a72bf:           long splitSize = computeSplitSize(blockSize, minSize, maxSize);
1:21a72bf:           long bytesRemaining = length;
1:21a72bf:           // split the stream file to small splits
1:21a72bf:           // there is 10% slop to avoid to generate very small split in the end
1:21a72bf:           while (((double) bytesRemaining) / splitSize > 1.1) {
1:21a72bf:             int blkIndex = getBlockIndex(blkLocations, length - bytesRemaining);
1:21a72bf:             splits.add(
1:21a72bf:                 makeSplit(streamFile.getSegmentNo(), path, length - bytesRemaining,
1:21a72bf:                     splitSize, blkLocations[blkIndex].getHosts(),
1:21a72bf:                     blkLocations[blkIndex].getCachedHosts(), FileFormat.ROW_V1));
1:21a72bf:             bytesRemaining -= splitSize;
1:21a72bf:           }
1:21a72bf:           if (bytesRemaining != 0) {
1:21a72bf:             int blkIndex = getBlockIndex(blkLocations, length - bytesRemaining);
1:21a72bf:             splits.add(makeSplit(streamFile.getSegmentNo(), path, length - bytesRemaining,
1:21a72bf:                 bytesRemaining, blkLocations[blkIndex].getHosts(),
1:21a72bf:                 blkLocations[blkIndex].getCachedHosts(), FileFormat.ROW_V1));
1:d7393da:           }
1:d7393da:         }
1:d7393da:       }
1:d7393da:     }
1:d7393da:     return splits;
1:d7393da:   }
1:d7393da: 
1:d7393da:   protected FileSplit makeSplit(String segmentId, Path file, long start, long length,
1:d7393da:       String[] hosts, FileFormat fileFormat) {
1:d7393da:     return new CarbonInputSplit(segmentId, file, start, length, hosts, fileFormat);
1:d7393da:   }
1:d7393da: 
1:d7393da: 
1:d7393da:   protected FileSplit makeSplit(String segmentId, Path file, long start, long length,
1:d7393da:       String[] hosts, String[] inMemoryHosts, FileFormat fileFormat) {
1:d7393da:     return new CarbonInputSplit(segmentId, file, start, length, hosts, inMemoryHosts, fileFormat);
1:d7393da:   }
1:d7393da: 
1:d7393da:   /**
1:874764f:    * Read data in one segment. For alter table partition statement
1:b681244:    * @param job
1:874764f:    * @param targetSegment
1:874764f:    * @param oldPartitionIdList  get old partitionId before partitionInfo was changed
1:fb1516c:    * @return
1:b681244:    */
1:874764f:   public List<InputSplit> getSplitsOfOneSegment(JobContext job, String targetSegment,
1:7e0803f:       List<Integer> oldPartitionIdList, PartitionInfo partitionInfo) {
1:8d3c774:     List<Segment> invalidSegments = new ArrayList<>();
1:874764f:     List<UpdateVO> invalidTimestampsList = new ArrayList<>();
1:b681244: 
1:b681244: 
1:d7393da:     try {
1:280a400:       carbonTable = getOrCreateCarbonTable(job.getConfiguration());
1:280a400:       ReadCommittedScope readCommittedScope =
1:280a400:           getReadCommitted(job, carbonTable.getAbsoluteTableIdentifier());
1:280a400:       this.readCommittedScope = readCommittedScope;
1:b681244: 
1:8d3c774:       List<Segment> segmentList = new ArrayList<>();
1:60dfdd3:       Segment segment = Segment.getSegment(targetSegment, carbonTable.getTablePath());
1:60dfdd3:       segmentList.add(
1:60dfdd3:           new Segment(segment.getSegmentNo(), segment.getSegmentFileName(), readCommittedScope));
1:c58eb43:       setSegmentsToAccess(job.getConfiguration(), segmentList);
1:b681244: 
1:b681244:       // process and resolve the expression
1:b681244:       Expression filter = getFilterPredicates(job.getConfiguration());
1:874764f:       CarbonTable carbonTable = getOrCreateCarbonTable(job.getConfiguration());
1:b681244:       // this will be null in case of corrupt schema file.
1:b681244:       if (null == carbonTable) {
1:b681244:         throw new IOException("Missing/Corrupt schema file for table.");
1:d7393da:       }
1:df5d7a9:       carbonTable.processFilterExpression(filter, null, null);
1:b681244:       // prune partitions for filter query on partition table
1:874764f:       String partitionIds = job.getConfiguration().get(ALTER_PARTITION_ID);
1:cb51b86:       // matchedPartitions records partitionIndex, not partitionId
1:b681244:       BitSet matchedPartitions = null;
1:874764f:       if (partitionInfo != null) {
1:cb51b86:         matchedPartitions =
1:cb51b86:             setMatchedPartitions(partitionIds, filter, partitionInfo, oldPartitionIdList);
1:874764f:         if (matchedPartitions != null) {
1:b681244:           if (matchedPartitions.cardinality() == 0) {
1:b681244:             return new ArrayList<InputSplit>();
1:874764f:           } else if (matchedPartitions.cardinality() == partitionInfo.getNumPartitions()) {
1:b681244:             matchedPartitions = null;
1:d7393da:           }
1:d7393da:         }
1:d7393da:       }
1:b681244: 
1:b681244:       // do block filtering and get split
1:3894e1d:       List<InputSplit> splits = getSplits(job, filter, segmentList, matchedPartitions,
1:7e0803f:           partitionInfo, oldPartitionIdList, new SegmentUpdateStatusManager(carbonTable));
1:b681244:       // pass the invalid segment to task side in order to remove index entry in task side
1:b681244:       if (invalidSegments.size() > 0) {
1:874764f:         for (InputSplit split : splits) {
1:874764f:           ((CarbonInputSplit) split).setInvalidSegments(invalidSegments);
1:874764f:           ((CarbonInputSplit) split).setInvalidTimestampRange(invalidTimestampsList);
1:d7393da:         }
1:0586146:       }
1:b681244:       return splits;
1:b681244:     } catch (IOException e) {
1:874764f:       throw new RuntimeException("Can't get splits of the target segment ", e);
1:b681244:     }
1:b681244:   }
1:b681244: 
1:b681244:   /**
1:cb51b86:    * set the matched partition indices into a BitSet
1:cb51b86:    * @param partitionIds  from alter table command, for normal query, it's null
1:cb51b86:    * @param filter   from query
1:cb51b86:    * @param partitionInfo
1:cb51b86:    * @param oldPartitionIdList  only used in alter table command
1:cb51b86:    * @return
1:b681244:    */
1:874764f:   private BitSet setMatchedPartitions(String partitionIds, Expression filter,
1:cb51b86:       PartitionInfo partitionInfo, List<Integer> oldPartitionIdList) {
1:874764f:     BitSet matchedPartitions = null;
1:874764f:     if (null != partitionIds) {
1:874764f:       String[] partList = partitionIds.replace("[", "").replace("]", "").split(",");
1:cb51b86:       // partList[0] -> use the first element to initiate BitSet, will auto expand later
1:cb51b86:       matchedPartitions = new BitSet(Integer.parseInt(partList[0].trim()));
1:874764f:       for (String partitionId : partList) {
1:cb51b86:         Integer index = oldPartitionIdList.indexOf(Integer.parseInt(partitionId.trim()));
1:cb51b86:         matchedPartitions.set(index);
1:b681244:       }
1:d7393da:     } else {
1:b681244:       if (null != filter) {
1:874764f:         matchedPartitions =
1:874764f:             new FilterExpressionProcessor().getFilteredPartitions(filter, partitionInfo);
1:b681244:       }
1:b681244:     }
1:874764f:     return matchedPartitions;
1:b681244:   }
1:b681244:   /**
1:b681244:    * {@inheritDoc}
1:a324e5d:    * Configurations FileInputFormat.INPUT_DIR, CarbonTableInputFormat.INPUT_SEGMENT_NUMBERS
1:b681244:    * are used to get table path to read.
1:b681244:    *
1:b681244:    * @return
1:b681244:    * @throws IOException
1:b681244:    */
1:3894e1d:   private List<InputSplit> getSplits(JobContext job, Expression expression,
1:8d3c774:       List<Segment> validSegments, BitSet matchedPartitions, PartitionInfo partitionInfo,
1:c125f0c:       List<Integer> oldPartitionIdList, SegmentUpdateStatusManager updateStatusManager)
1:c125f0c:       throws IOException {
1:b681244: 
1:d5bec4d:     numSegments = validSegments.size();
1:b681244:     List<InputSplit> result = new LinkedList<InputSplit>();
1:b681244:     UpdateVO invalidBlockVOForSegmentId = null;
1:b681244:     Boolean isIUDTable = false;
1:b681244: 
1:b681244:     isIUDTable = (updateStatusManager.getUpdateStatusDetails().length != 0);
1:b681244: 
1:874764f:     // for each segment fetch blocks matching filter in Driver BTree
1:b681244:     List<org.apache.carbondata.hadoop.CarbonInputSplit> dataBlocksOfSegment =
1:3894e1d:         getDataBlocksOfSegment(job, carbonTable, expression, matchedPartitions,
1:c58eb43:             validSegments, partitionInfo, oldPartitionIdList);
1:d5bec4d:     numBlocks = dataBlocksOfSegment.size();
1:b681244:     for (org.apache.carbondata.hadoop.CarbonInputSplit inputSplit : dataBlocksOfSegment) {
1:b681244: 
1:b681244:       // Get the UpdateVO for those tables on which IUD operations being performed.
1:b681244:       if (isIUDTable) {
1:b681244:         invalidBlockVOForSegmentId =
1:b681244:             updateStatusManager.getInvalidTimestampRange(inputSplit.getSegmentId());
1:b681244:       }
1:b681244:       String[] deleteDeltaFilePath = null;
1:b681244:       if (isIUDTable) {
1:b681244:         // In case IUD is not performed in this table avoid searching for
1:b681244:         // invalidated blocks.
1:b681244:         if (CarbonUtil
1:b681244:             .isInvalidTableBlock(inputSplit.getSegmentId(), inputSplit.getPath().toString(),
1:b681244:                 invalidBlockVOForSegmentId, updateStatusManager)) {
1:b681244:           continue;
1:b681244:         }
1:940f4d5:         // When iud is done then only get delete delta files for a block
1:45787fb:         try {
1:8d3c774:           deleteDeltaFilePath = updateStatusManager
1:8d3c774:               .getDeleteDeltaFilePath(inputSplit.getPath().toString(), inputSplit.getSegmentId());
1:45787fb:         } catch (Exception e) {
1:b681244:           throw new IOException(e);
1:45787fb:         }
1:b681244:       }
1:b681244:       inputSplit.setDeleteDeltaFiles(deleteDeltaFilePath);
1:b681244:       result.add(inputSplit);
1:b681244:     }
1:b681244:     return result;
1:b681244:   }
1:b681244: 
1:b681244:   /**
1:b681244:    * return valid segment to access
1:b681244:    */
1:c58eb43:   public Segment[] getSegmentsToAccess(JobContext job, ReadCommittedScope readCommittedScope) {
1:b681244:     String segmentString = job.getConfiguration().get(INPUT_SEGMENT_NUMBERS, "");
1:b681244:     if (segmentString.trim().isEmpty()) {
1:8d3c774:       return new Segment[0];
1:b681244:     }
1:c58eb43:     List<Segment> segments = Segment.toSegmentList(segmentString.split(","), readCommittedScope);
1:8d3c774:     return segments.toArray(new Segment[segments.size()]);
1:b681244:   }
1:b681244: 
1:b681244:   /**
1:b681244:    * Get the row count of the Block and mapping of segment and Block count.
1:b681244:    */
1:7e0803f:   public BlockMappingVO getBlockRowCount(Job job, CarbonTable table,
1:8d3c774:       List<PartitionSpec> partitions) throws IOException {
1:7e0803f:     AbsoluteTableIdentifier identifier = table.getAbsoluteTableIdentifier();
1:7e0803f:     TableDataMap blockletMap = DataMapStoreManager.getInstance().getDefaultDataMap(table);
1:b681244: 
1:280a400:     ReadCommittedScope readCommittedScope = getReadCommitted(job, identifier);
1:280a400:     LoadMetadataDetails[] loadMetadataDetails = readCommittedScope.getSegmentList();
1:b681244: 
1:f9291cd:     SegmentUpdateStatusManager updateStatusManager = new SegmentUpdateStatusManager(
1:7e0803f:         table, loadMetadataDetails);
1:c16d543:     SegmentStatusManager.ValidAndInvalidSegmentsInfo allSegments =
1:8f1a029:         new SegmentStatusManager(identifier, readCommittedScope.getConfiguration())
1:c58eb43:             .getValidAndInvalidSegments(loadMetadataDetails, readCommittedScope);
1:c16d543:     Map<String, Long> blockRowCountMapping = new HashMap<>();
1:c16d543:     Map<String, Long> segmentAndBlockCountMapping = new HashMap<>();
1:b681244: 
1:c16d543:     // TODO: currently only batch segment is supported, add support for streaming table
1:c58eb43:     List<Segment> filteredSegment =
1:c58eb43:         getFilteredSegment(job, allSegments.getValidSegments(), false, readCommittedScope);
1:6d40d3a:     /* In the select * flow, getSplits() method was clearing the segmentMap if,
1:6d40d3a:     segment needs refreshing. same thing need for select count(*) flow also.
1:6d40d3a:     For NonTransactional table, one of the reason for a segment refresh is below scenario.
1:6d40d3a:     SDK is written one set of files with UUID, with same UUID it can write again.
1:6d40d3a:     So, latest files content should reflect the new count by refreshing the segment */
1:7e93d7b:     List<Segment> toBeCleanedSegments = new ArrayList<>();
1:6d40d3a:     for (Segment eachSegment : filteredSegment) {
1:6d40d3a:       boolean refreshNeeded = DataMapStoreManager.getInstance()
1:6d40d3a:           .getTableSegmentRefresher(getOrCreateCarbonTable(job.getConfiguration()))
1:6d40d3a:           .isRefreshNeeded(eachSegment,
1:6d40d3a:               updateStatusManager.getInvalidTimestampRange(eachSegment.getSegmentNo()));
1:6d40d3a:       if (refreshNeeded) {
1:6d40d3a:         toBeCleanedSegments.add(eachSegment);
1:b681244:       }
1:b681244:     }
1:6d40d3a:     if (toBeCleanedSegments.size() > 0) {
1:6d40d3a:       DataMapStoreManager.getInstance()
1:6d40d3a:           .clearInvalidSegments(getOrCreateCarbonTable(job.getConfiguration()),
1:6d40d3a:               toBeCleanedSegments);
1:b681244:     }
1:280a400:     List<ExtendedBlocklet> blocklets =
1:3894e1d:         blockletMap.prune(filteredSegment, (FilterResolverIntf) null, partitions);
1:28f78b2:     for (ExtendedBlocklet blocklet : blocklets) {
1:28f78b2:       String blockName = blocklet.getPath();
1:b681244:       blockName = CarbonTablePath.getCarbonDataFileName(blockName);
1:b681244:       blockName = blockName + CarbonTablePath.getCarbonDataExtension();
1:b681244: 
1:b681244:       long rowCount = blocklet.getDetailInfo().getRowCount();
1:b681244: 
1:60dfdd3:       String segmentId = Segment.toSegment(blocklet.getSegmentId()).getSegmentNo();
1:60dfdd3:       String key = CarbonUpdateUtil.getSegmentBlockNameKey(segmentId, blockName);
1:b681244: 
1:b681244:       // if block is invalid then dont add the count
1:b681244:       SegmentUpdateDetails details = updateStatusManager.getDetailsForABlock(key);
1:b681244: 
1:2f0959a:       if (null == details || !CarbonUpdateUtil.isBlockInvalid(details.getSegmentStatus())) {
1:b681244:         Long blockCount = blockRowCountMapping.get(key);
1:b681244:         if (blockCount == null) {
1:b681244:           blockCount = 0L;
1:60dfdd3:           Long count = segmentAndBlockCountMapping.get(segmentId);
1:b681244:           if (count == null) {
1:b681244:             count = 0L;
1:b681244:           }
1:60dfdd3:           segmentAndBlockCountMapping.put(segmentId, count + 1);
1:b681244:         }
1:b681244:         blockCount += rowCount;
1:b681244:         blockRowCountMapping.put(key, blockCount);
1:b681244:       }
1:b681244:     }
1:b681244: 
1:b681244:     return new BlockMappingVO(blockRowCountMapping, segmentAndBlockCountMapping);
1:b681244:   }
1:b681244: 
1:280a400:   public ReadCommittedScope getReadCommitted(JobContext job, AbsoluteTableIdentifier identifier)
1:8d3c774:       throws IOException {
1:280a400:     if (readCommittedScope == null) {
1:280a400:       ReadCommittedScope readCommittedScope;
1:b7b8073:       if (job.getConfiguration().getBoolean(CARBON_TRANSACTIONAL_TABLE, true)) {
1:8f1a029:         readCommittedScope = new TableStatusReadCommittedScope(identifier, job.getConfiguration());
1:280a400:       } else {
1:347b8e1:         readCommittedScope = getReadCommittedScope(job.getConfiguration());
1:347b8e1:         if (readCommittedScope == null) {
1:8f1a029:           readCommittedScope =
1:8f1a029:               new LatestFilesReadCommittedScope(identifier.getTablePath(), job.getConfiguration());
1:347b8e1:         }
1:b681244:       }
1:280a400:       this.readCommittedScope = readCommittedScope;
1:b681244:     }
1:280a400:     return readCommittedScope;
1:b681244:   }
1:b681244: }
============================================================================
author:kunal642
-------------------------------------------------------------------------------
commit:8f1a029
/////////////////////////////////////////////////////////////////////////
1:     SegmentStatusManager segmentStatusManager = new SegmentStatusManager(identifier,
1:         readCommittedScope.getConfiguration());
/////////////////////////////////////////////////////////////////////////
1:         new SegmentStatusManager(identifier, readCommittedScope.getConfiguration())
/////////////////////////////////////////////////////////////////////////
1:         readCommittedScope = new TableStatusReadCommittedScope(identifier, job.getConfiguration());
1:           readCommittedScope =
1:               new LatestFilesReadCommittedScope(identifier.getTablePath(), job.getConfiguration());
commit:2a9604c
/////////////////////////////////////////////////////////////////////////
commit:0111587
/////////////////////////////////////////////////////////////////////////
1:       invalidTimestampsList.addAll(updateStatusManager.getInvalidTimestampRange());
commit:7b56126
/////////////////////////////////////////////////////////////////////////
commit:cc0e6f1
/////////////////////////////////////////////////////////////////////////
0:   public static void setAggeragateTableSegments(Configuration configuration, String segments) {
0:     configuration.set(CarbonCommonConstants.CARBON_INPUT_SEGMENTS, segments);
1:   }
1: 
0:   private static String getAggeragateTableSegments(Configuration configuration) {
0:     return configuration.get(CarbonCommonConstants.CARBON_INPUT_SEGMENTS);
1:   }
1: 
/////////////////////////////////////////////////////////////////////////
1:     CarbonTable carbonTable = getOrCreateCarbonTable(job.getConfiguration());
1:     if (null == carbonTable) {
1:       throw new IOException("Missing/Corrupt schema file for table.");
1:     }
0:     String aggregateTableSegments = getAggeragateTableSegments(job.getConfiguration());
/////////////////////////////////////////////////////////////////////////
1:     } else {
0:       filteredSegmentToAccess = Arrays.asList(aggregateTableSegments.split(","));
/////////////////////////////////////////////////////////////////////////
author:QiangCai
-------------------------------------------------------------------------------
commit:0528a79
/////////////////////////////////////////////////////////////////////////
1:       // record the hit information of the streaming files
1:       this.hitedStreamFiles = streamFiles.size();
1:       this.numStreamFiles = streamPruner.getTotalFileNums();
commit:21a72bf
/////////////////////////////////////////////////////////////////////////
/////////////////////////////////////////////////////////////////////////
/////////////////////////////////////////////////////////////////////////
/////////////////////////////////////////////////////////////////////////
1: import org.apache.carbondata.core.stream.StreamFile;
1: import org.apache.carbondata.core.stream.StreamPruner;
/////////////////////////////////////////////////////////////////////////
1:           return getSplitsOfStreaming(job, streamSegments, carbonTable);
1:           return getSplitsOfStreaming(job, streamSegments, carbonTable);
/////////////////////////////////////////////////////////////////////////
1:           return getSplitsOfStreaming(job, streamSegments, carbonTable);
/////////////////////////////////////////////////////////////////////////
0:     List<InputSplit> splitsOfStreaming =
0:         getSplitsOfStreaming(job, streamSegments, carbonTable, filterInterface);
/////////////////////////////////////////////////////////////////////////
1:   public List<InputSplit> getSplitsOfStreaming(JobContext job, List<Segment> streamSegments,
1:       CarbonTable carbonTable) throws IOException {
1:     return getSplitsOfStreaming(job, streamSegments, carbonTable, null);
1:   }
1: 
1:   public List<InputSplit> getSplitsOfStreaming(JobContext job, List<Segment> streamSegments,
1:       CarbonTable carbonTable, FilterResolverIntf filterResolverIntf) throws IOException {
1:       if (filterResolverIntf == null) {
1:         if (carbonTable != null) {
1:           Expression filter = getFilterPredicates(job.getConfiguration());
1:           if (filter != null) {
1:             carbonTable.processFilterExpression(filter, null, null);
0:             filterResolverIntf = carbonTable.resolveFilter(filter);
1:           }
1:         }
1:       }
1:       StreamPruner streamPruner = new StreamPruner(carbonTable);
1:       streamPruner.init(filterResolverIntf);
1:       List<StreamFile> streamFiles = streamPruner.prune(streamSegments);
1: 
1:       for (StreamFile streamFile : streamFiles) {
1:         Path path = new Path(streamFile.getFilePath());
1:         long length = streamFile.getFileSize();
1:         if (length != 0) {
1:           BlockLocation[] blkLocations;
1:           FileSystem fs = FileFactory.getFileSystem(path);
1:           FileStatus file = fs.getFileStatus(path);
1:           blkLocations = fs.getFileBlockLocations(path, 0, length);
1:           long blockSize = file.getBlockSize();
1:           long splitSize = computeSplitSize(blockSize, minSize, maxSize);
1:           long bytesRemaining = length;
1:           // split the stream file to small splits
1:           // there is 10% slop to avoid to generate very small split in the end
1:           while (((double) bytesRemaining) / splitSize > 1.1) {
1:             int blkIndex = getBlockIndex(blkLocations, length - bytesRemaining);
1:             splits.add(
1:                 makeSplit(streamFile.getSegmentNo(), path, length - bytesRemaining,
1:                     splitSize, blkLocations[blkIndex].getHosts(),
1:                     blkLocations[blkIndex].getCachedHosts(), FileFormat.ROW_V1));
1:             bytesRemaining -= splitSize;
1:           }
1:           if (bytesRemaining != 0) {
1:             int blkIndex = getBlockIndex(blkLocations, length - bytesRemaining);
1:             splits.add(makeSplit(streamFile.getSegmentNo(), path, length - bytesRemaining,
1:                 bytesRemaining, blkLocations[blkIndex].getHosts(),
1:                 blkLocations[blkIndex].getCachedHosts(), FileFormat.ROW_V1));
commit:d5bec4d
/////////////////////////////////////////////////////////////////////////
1:       numStreamSegments = streamSegments.size();
/////////////////////////////////////////////////////////////////////////
1:     numSegments = validSegments.size();
/////////////////////////////////////////////////////////////////////////
1:     numBlocks = dataBlocksOfSegment.size();
commit:f8e0585
/////////////////////////////////////////////////////////////////////////
0:   public List<InputSplit> getSplitsOfStreaming(JobContext job, AbsoluteTableIdentifier identifier,
commit:d7393da
/////////////////////////////////////////////////////////////////////////
0: import java.io.File;
/////////////////////////////////////////////////////////////////////////
0: import org.apache.carbondata.core.datastore.filesystem.CarbonFile;
1: import org.apache.carbondata.core.datastore.impl.FileFactory;
/////////////////////////////////////////////////////////////////////////
0: import org.apache.carbondata.core.reader.CarbonIndexFileReader;
/////////////////////////////////////////////////////////////////////////
1: import org.apache.carbondata.core.statusmanager.FileFormat;
/////////////////////////////////////////////////////////////////////////
0: import org.apache.carbondata.format.BlockIndex;
/////////////////////////////////////////////////////////////////////////
1: import org.apache.hadoop.fs.BlockLocation;
1: import org.apache.hadoop.fs.FileStatus;
/////////////////////////////////////////////////////////////////////////
0:   public static TableInfo getTableInfo(Configuration configuration) throws IOException {
/////////////////////////////////////////////////////////////////////////
0:     List<String> streamSegments = null;
/////////////////////////////////////////////////////////////////////////
1:       streamSegments = segments.getStreamSegments();
0:         return getSplitsOfStreaming(job, identifier, streamSegments);
/////////////////////////////////////////////////////////////////////////
0:     CarbonInputFormatUtil.processFilterExpression(filter, carbonTable, null, null);
/////////////////////////////////////////////////////////////////////////
1: 
1:     // add all splits of streaming
0:     List<InputSplit> splitsOfStreaming = getSplitsOfStreaming(job, identifier, streamSegments);
1:     if (!splitsOfStreaming.isEmpty()) {
1:       splits.addAll(splitsOfStreaming);
1:     }
1:    * use file list in .carbonindex file to get the split of streaming.
1:    */
0:   private List<InputSplit> getSplitsOfStreaming(JobContext job, AbsoluteTableIdentifier identifier,
0:       List<String> streamSegments) throws IOException {
1:     List<InputSplit> splits = new ArrayList<InputSplit>();
1:     if (streamSegments != null && !streamSegments.isEmpty()) {
1: 
0:       CarbonTablePath tablePath = CarbonStorePath.getCarbonTablePath(identifier);
1:       long minSize = Math.max(getFormatMinSplitSize(), getMinSplitSize(job));
1:       long maxSize = getMaxSplitSize(job);
0:       for (String segmentId : streamSegments) {
0:         String segmentDir = tablePath.getSegmentDir("0", segmentId);
0:         FileFactory.FileType fileType = FileFactory.getFileType(segmentDir);
0:         if (FileFactory.isFileExist(segmentDir, fileType)) {
0:           String indexName = CarbonTablePath.getCarbonStreamIndexFileName();
0:           String indexPath = segmentDir + File.separator + indexName;
0:           CarbonFile index = FileFactory.getCarbonFile(indexPath, fileType);
0:           // index file exists
0:           if (index.exists()) {
0:             // data file exists
0:             CarbonIndexFileReader indexReader = new CarbonIndexFileReader();
1:             try {
0:               // map block index
0:               indexReader.openThriftReader(indexPath);
0:               while (indexReader.hasNext()) {
0:                 BlockIndex blockIndex = indexReader.readBlockIndexInfo();
0:                 String filePath = segmentDir + File.separator + blockIndex.getFile_name();
0:                 Path path = new Path(filePath);
0:                 long length = blockIndex.getFile_size();
0:                 if (length != 0) {
0:                   BlockLocation[] blkLocations;
0:                   FileSystem fs = FileFactory.getFileSystem(path);
0:                   FileStatus file = fs.getFileStatus(path);
0:                   blkLocations = fs.getFileBlockLocations(path, 0, length);
0:                   long blockSize = file.getBlockSize();
0:                   long splitSize = computeSplitSize(blockSize, minSize, maxSize);
0:                   long bytesRemaining = length;
0:                   while (((double) bytesRemaining) / splitSize > 1.1) {
0:                     int blkIndex = getBlockIndex(blkLocations, length - bytesRemaining);
0:                     splits.add(makeSplit(segmentId, path, length - bytesRemaining, splitSize,
0:                         blkLocations[blkIndex].getHosts(),
0:                         blkLocations[blkIndex].getCachedHosts(), FileFormat.rowformat));
0:                     bytesRemaining -= splitSize;
1:                   }
0:                   if (bytesRemaining != 0) {
0:                     int blkIndex = getBlockIndex(blkLocations, length - bytesRemaining);
0:                     splits.add(makeSplit(segmentId, path, length - bytesRemaining, bytesRemaining,
0:                         blkLocations[blkIndex].getHosts(),
0:                         blkLocations[blkIndex].getCachedHosts(), FileFormat.rowformat));
1:                   }
1:                 } else {
0:                   //Create empty hosts array for zero length files
0:                   splits.add(makeSplit(segmentId, path, 0, length, new String[0],
0:                       FileFormat.rowformat));
1:                 }
1:               }
0:             } finally {
0:               indexReader.closeThriftReader();
1:             }
1:           }
1:         }
1:       }
1:     }
1:     return splits;
1:   }
1: 
1:   protected FileSplit makeSplit(String segmentId, Path file, long start, long length,
1:       String[] hosts, FileFormat fileFormat) {
1:     return new CarbonInputSplit(segmentId, file, start, length, hosts, fileFormat);
1:   }
1: 
1: 
1:   protected FileSplit makeSplit(String segmentId, Path file, long start, long length,
1:       String[] hosts, String[] inMemoryHosts, FileFormat fileFormat) {
1:     return new CarbonInputSplit(segmentId, file, start, length, hosts, inMemoryHosts, fileFormat);
1:   }
1: 
1:   /**
/////////////////////////////////////////////////////////////////////////
0:       CarbonInputFormatUtil.processFilterExpression(filter, carbonTable, null, null);
/////////////////////////////////////////////////////////////////////////
0:     boolean[] isFilterDimensions = new boolean[carbonTable.getDimensionOrdinalMax()];
0:     boolean[] isFilterMeasures =
0:         new boolean[carbonTable.getNumberOfMeasures(carbonTable.getFactTableName())];
0:     CarbonInputFormatUtil.processFilterExpression(filter, carbonTable, isFilterDimensions,
0:         isFilterMeasures);
0:     queryModel.setIsFilterDimensions(isFilterDimensions);
0:     queryModel.setIsFilterMeasures(isFilterMeasures);
commit:256dbed
/////////////////////////////////////////////////////////////////////////
0:     assert (filter instanceof Expression);
commit:41347d8
/////////////////////////////////////////////////////////////////////////
1:  * Licensed to the Apache Software Foundation (ASF) under one or more
1:  * contributor license agreements.  See the NOTICE file distributed with
1:  * this work for additional information regarding copyright ownership.
1:  * The ASF licenses this file to You under the Apache License, Version 2.0
1:  * (the "License"); you may not use this file except in compliance with
1:  * the License.  You may obtain a copy of the License at
1:  * Unless required by applicable law or agreed to in writing, software
1:  * distributed under the License is distributed on an "AS IS" BASIS,
1:  * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
1:  * See the License for the specific language governing permissions and
1:  * limitations under the License.
author:ravipesala
-------------------------------------------------------------------------------
commit:3894e1d
/////////////////////////////////////////////////////////////////////////
1:       carbonTable.processFilterExpression(filter, null, null);
/////////////////////////////////////////////////////////////////////////
1:         getSplits(job, filter, filteredSegmentToAccess, matchedPartitions, partitionInfo,
/////////////////////////////////////////////////////////////////////////
1:     List<InputSplit> splitsOfStreaming = getSplitsOfStreaming(job, streamSegments, carbonTable);
/////////////////////////////////////////////////////////////////////////
1:             filterResolverIntf =
1:                 CarbonTable.resolveFilter(filter, carbonTable.getAbsoluteTableIdentifier());
/////////////////////////////////////////////////////////////////////////
/////////////////////////////////////////////////////////////////////////
1:       List<InputSplit> splits = getSplits(job, filter, segmentList, matchedPartitions,
/////////////////////////////////////////////////////////////////////////
1:   private List<InputSplit> getSplits(JobContext job, Expression expression,
/////////////////////////////////////////////////////////////////////////
1:         getDataBlocksOfSegment(job, carbonTable, expression, matchedPartitions,
/////////////////////////////////////////////////////////////////////////
1:         blockletMap.prune(filteredSegment, (FilterResolverIntf) null, partitions);
commit:347b8e1
/////////////////////////////////////////////////////////////////////////
1:         readCommittedScope = getReadCommittedScope(job.getConfiguration());
1:         if (readCommittedScope == null) {
0:           readCommittedScope = new LatestFilesReadCommittedScope(identifier.getTablePath());
1:         }
commit:60dfdd3
/////////////////////////////////////////////////////////////////////////
1: 
/////////////////////////////////////////////////////////////////////////
1:       Segment segment = Segment.getSegment(targetSegment, carbonTable.getTablePath());
1:       segmentList.add(
1:           new Segment(segment.getSegmentNo(), segment.getSegmentFileName(), readCommittedScope));
/////////////////////////////////////////////////////////////////////////
1:       String segmentId = Segment.toSegment(blocklet.getSegmentId()).getSegmentNo();
1:       String key = CarbonUpdateUtil.getSegmentBlockNameKey(segmentId, blockName);
/////////////////////////////////////////////////////////////////////////
1:           Long count = segmentAndBlockCountMapping.get(segmentId);
1:           segmentAndBlockCountMapping.put(segmentId, count + 1);
commit:d35fbaf
/////////////////////////////////////////////////////////////////////////
0: import org.apache.carbondata.core.datamap.DataMapType;
/////////////////////////////////////////////////////////////////////////
0:     boolean distributedCG = Boolean.parseBoolean(CarbonProperties.getInstance()
0:         .getProperty(CarbonCommonConstants.USE_DISTRIBUTED_DATAMAP,
0:             CarbonCommonConstants.USE_DISTRIBUTED_DATAMAP_DEFAULT));
0:     TableDataMap blockletMap =
0:         DataMapStoreManager.getInstance().chooseDataMap(absoluteTableIdentifier);
0:     if (distributedCG || blockletMap.getDataMapFactory().getDataMapType() == DataMapType.FG) {
0:           new DistributableDataMapFormat(absoluteTableIdentifier, blockletMap.getDataMapName(),
/////////////////////////////////////////////////////////////////////////
0:             ColumnarFormatVersion.valueOf((short) blocklet.getDetailInfo().getVersionNumber()),
0:             blocklet.getDataMapWriterPath());
commit:c125f0c
/////////////////////////////////////////////////////////////////////////
1: import org.apache.carbondata.core.statusmanager.LoadMetadataDetails;
/////////////////////////////////////////////////////////////////////////
0:     LoadMetadataDetails[] loadMetadataDetails = SegmentStatusManager
0:         .readTableStatusFile(CarbonTablePath.getTableStatusFilePath(identifier.getTablePath()));
1:     SegmentUpdateStatusManager updateStatusManager =
0:         new SegmentUpdateStatusManager(identifier, loadMetadataDetails);
/////////////////////////////////////////////////////////////////////////
0:         segmentStatusManager.getValidAndInvalidSegments(loadMetadataDetails);
/////////////////////////////////////////////////////////////////////////
1:             null, updateStatusManager);
/////////////////////////////////////////////////////////////////////////
0:           partitionInfo, oldPartitionIdList, new SegmentUpdateStatusManager(identifier));
/////////////////////////////////////////////////////////////////////////
1:       List<Integer> oldPartitionIdList, SegmentUpdateStatusManager updateStatusManager)
1:       throws IOException {
/////////////////////////////////////////////////////////////////////////
/////////////////////////////////////////////////////////////////////////
0:     LoadMetadataDetails[] loadMetadataDetails = SegmentStatusManager
0:         .readTableStatusFile(CarbonTablePath.getTableStatusFilePath(identifier.getTablePath()));
0:     SegmentUpdateStatusManager updateStatusManager =
0:         new SegmentUpdateStatusManager(identifier, loadMetadataDetails);
0:         new SegmentStatusManager(identifier).getValidAndInvalidSegments(loadMetadataDetails);
commit:8d3c774
/////////////////////////////////////////////////////////////////////////
1: import org.apache.carbondata.core.datamap.Segment;
1: import org.apache.carbondata.core.indexstore.PartitionSpec;
/////////////////////////////////////////////////////////////////////////
0:   public static void setSegmentsToAccess(Configuration configuration, List<Segment> validSegments) {
/////////////////////////////////////////////////////////////////////////
0:           .setSegmentsToAccess(conf, Segment.toSegmentList(segmentNumbersFromProperty.split(",")));
/////////////////////////////////////////////////////////////////////////
0:   public static void setPartitionsToPrune(Configuration configuration,
0:       List<PartitionSpec> partitions) {
0:       String partitionString =
0:           ObjectSerializationUtil.convertObjectToString(new ArrayList<>(partitions));
0:       throw new RuntimeException("Error while setting patition information to Job" + partitions, e);
0:   public static List<PartitionSpec> getPartitionsToPrune(Configuration configuration)
1:       throws IOException {
0:       return (List<PartitionSpec>) ObjectSerializationUtil.convertStringToObject(partitionString);
/////////////////////////////////////////////////////////////////////////
1:     List<Segment> invalidSegments = new ArrayList<>();
1:     List<Segment> streamSegments = null;
1:     // get all valid segments and set them into the configuration
0:     SegmentStatusManager segmentStatusManager = new SegmentStatusManager(identifier);
0:     SegmentStatusManager.ValidAndInvalidSegmentsInfo segments =
0:         segmentStatusManager.getValidAndInvalidSegments();
1:       List<Segment> validSegments = segments.getValidSegments();
1: 
1: 
0:       List<Segment> filteredSegmentToAccess = getFilteredSegment(job, segments.getValidSegments());
/////////////////////////////////////////////////////////////////////////
0:       for (Segment invalidSegmentId : invalidSegments) {
0:         invalidTimestampsList
0:             .add(updateStatusManager.getInvalidTimestampRange(invalidSegmentId.getSegmentNo()));
0:     ArrayList<Segment> validAndInProgressSegments = new ArrayList<>(segments.getValidSegments());
1:     // Add in progress segments also to filter it as in case of aggregate table load it loads
1:     // data from in progress table.
1:     validAndInProgressSegments.addAll(segments.getListOfInProgressSegments());
1:     List<Segment> filteredSegmentToAccess =
0:         getFilteredSegment(job, new ArrayList<>(validAndInProgressSegments));
1:     List<Segment> toBeCleanedSegments = new ArrayList<>();
0:         toBeCleanedSegments.add(new Segment(segmentUpdateDetail.getSegmentName(), null));
1:     for (Segment segment : filteredSegmentToAccess) {
1:           .isRefreshNeeded(segment.getSegmentNo())) {
/////////////////////////////////////////////////////////////////////////
0:   private List<Segment> getFilteredSegment(JobContext job, List<Segment> validSegments) {
0:     Segment[] segmentsToAccess = getSegmentsToAccess(job);
1:     List<Segment> segmentToAccessSet =
1:         new ArrayList<>(new HashSet<>(Arrays.asList(segmentsToAccess)));
1:     List<Segment> filteredSegmentToAccess = new ArrayList<>();
1:     if (segmentsToAccess.length == 0 || segmentsToAccess[0].getSegmentNo().equalsIgnoreCase("*")) {
1:       for (Segment validSegment : validSegments) {
1:         int index = segmentToAccessSet.indexOf(validSegment);
1:         if (index > -1) {
1:           // In case of in progress reading segment, segment file name is set to the property itself
1:           if (segmentToAccessSet.get(index).getSegmentFileName() != null
1:               && validSegment.getSegmentFileName() == null) {
1:             filteredSegmentToAccess.add(segmentToAccessSet.get(index));
1:           } else {
1:             filteredSegmentToAccess.add(validSegment);
1:           }
1:         List<Segment> filteredSegmentToAccessTemp = new ArrayList<>(filteredSegmentToAccess);
/////////////////////////////////////////////////////////////////////////
0:       List<Segment> streamSegments) throws IOException {
0:       for (Segment segment : streamSegments) {
0:         String segmentDir = tablePath.getSegmentDir("0", segment.getSegmentNo());
/////////////////////////////////////////////////////////////////////////
0:                     splits.add(makeSplit(segment.getSegmentNo(), path, length - bytesRemaining,
0:                         splitSize, blkLocations[blkIndex].getHosts(),
0:                     splits.add(makeSplit(segment.getSegmentNo(), path, length - bytesRemaining,
0:                         bytesRemaining, blkLocations[blkIndex].getHosts(),
0:                   splits.add(makeSplit(segment.getSegmentNo(), path, 0, length, new String[0],
/////////////////////////////////////////////////////////////////////////
1:     List<Segment> invalidSegments = new ArrayList<>();
1:     List<Segment> segmentList = new ArrayList<>();
0:     segmentList.add(new Segment(targetSegment, null));
/////////////////////////////////////////////////////////////////////////
1:       List<Segment> validSegments, BitSet matchedPartitions, PartitionInfo partitionInfo,
/////////////////////////////////////////////////////////////////////////
1:           deleteDeltaFilePath = updateStatusManager
1:               .getDeleteDeltaFilePath(inputSplit.getPath().toString(), inputSplit.getSegmentId());
/////////////////////////////////////////////////////////////////////////
0:       BitSet matchedPartitions, List<Segment> segmentIds, PartitionInfo partitionInfo,
/////////////////////////////////////////////////////////////////////////
0:     List<PartitionSpec> partitionsToPrune = getPartitionsToPrune(job.getConfiguration());
/////////////////////////////////////////////////////////////////////////
0:   private Segment[] getSegmentsToAccess(JobContext job) {
1:       return new Segment[0];
0:     List<Segment> segments = Segment.toSegmentList(segmentString.split(","));
1:     return segments.toArray(new Segment[segments.size()]);
/////////////////////////////////////////////////////////////////////////
1:       List<PartitionSpec> partitions) throws IOException {
/////////////////////////////////////////////////////////////////////////
0:     List<Segment> filteredSegment = getFilteredSegment(job, allSegments.getValidSegments());
commit:41b0074
/////////////////////////////////////////////////////////////////////////
/////////////////////////////////////////////////////////////////////////
0:   private CarbonInputSplit convertToCarbonInputSplit(ExtendedBlocklet blocklet) throws IOException {
commit:7504a5c
/////////////////////////////////////////////////////////////////////////
0:       configuration.set(TABLE_INFO, CarbonUtil.encodeToString(tableInfo.serialize()));
/////////////////////////////////////////////////////////////////////////
0:               new ByteArrayInputStream(CarbonUtil.decodeStringToBytes(tableInfoStr))));
commit:45787fb
/////////////////////////////////////////////////////////////////////////
0:     if (partitions == null) {
0:       return;
1:     }
1:     try {
0:       String partitionString = ObjectSerializationUtil.convertObjectToString(partitions);
0:       configuration.set(PARTITIONS_TO_PRUNE, partitionString);
1:     } catch (Exception e) {
0:       throw new RuntimeException("Error while setting patition information to Job", e);
1:     }
0:   public static List<String> getPartitionsToPrune(Configuration configuration) throws IOException {
0:     String partitionString = configuration.get(PARTITIONS_TO_PRUNE);
0:     if (partitionString != null) {
0:       return (List<String>) ObjectSerializationUtil.convertStringToObject(partitionString);
0:     return null;
commit:a89587e
/////////////////////////////////////////////////////////////////////////
0:   public static final String UPADTE_T =
0:       "mapreduce.input.carboninputformat.partitions.to.prune";
/////////////////////////////////////////////////////////////////////////
commit:3ff55a2
/////////////////////////////////////////////////////////////////////////
/////////////////////////////////////////////////////////////////////////
0:   public BlockMappingVO getBlockRowCount(Job job, AbsoluteTableIdentifier identifier,
0:       List<String> partitions) throws IOException {
/////////////////////////////////////////////////////////////////////////
0:     List<ExtendedBlocklet> blocklets = blockletMap.prune(filteredSegment, null, partitions);
commit:b8a02f3
/////////////////////////////////////////////////////////////////////////
0:   public static final String PARTITIONS_TO_PRUNE =
0:       "mapreduce.input.carboninputformat.partitions.to.prune";
/////////////////////////////////////////////////////////////////////////
0:     configuration.set(INPUT_SEGMENT_NUMBERS, CarbonUtil.convertToString(validSegments));
/////////////////////////////////////////////////////////////////////////
0:    * set list of partitions to prune
1:    */
0:   public static void setPartitionsToPrune(Configuration configuration, List<String> partitions) {
0:     configuration.set(
0:         CarbonTableInputFormat.PARTITIONS_TO_PRUNE, CarbonUtil.convertToString(partitions));
1:   }
1: 
1:   /**
0:    * get list of partitions to prune
1:    */
0:   public static List<String> getPartitionsToPrune(Configuration configuration) {
0:     String partitionString = configuration.get(PARTITIONS_TO_PRUNE, "");
0:     if (partitionString.trim().isEmpty()) {
0:       return null;
1:     }
0:     return Arrays.asList(partitionString.split(","));
1:   }
1:   /**
0:     configuration.set(INPUT_FILES, CarbonUtil.convertToString(validFiles));
/////////////////////////////////////////////////////////////////////////
0:     List<String> partitionsToPrune = getPartitionsToPrune(job.getConfiguration());
0:               segmentIds, partitionsToPrune,
0:               BlockletDataMapFactory.class.getName());
0:       prunedBlocklets = blockletMap.prune(segmentIds, resolver, partitionsToPrune);
/////////////////////////////////////////////////////////////////////////
0:     List<ExtendedBlocklet> blocklets = blockletMap.prune(filteredSegment, null, null);
commit:4430178
/////////////////////////////////////////////////////////////////////////
1: import org.apache.carbondata.core.metadata.schema.partition.PartitionType;
/////////////////////////////////////////////////////////////////////////
1:     if (partitionInfo != null && partitionInfo.getPartitionType() != PartitionType.NATIVE_HIVE) {
/////////////////////////////////////////////////////////////////////////
0:     if (partitionInfo != null && partitionInfo.getPartitionType() != PartitionType.NATIVE_HIVE) {
0:       long partitionId = CarbonTablePath.DataFileUtil.getTaskIdFromTaskNo(
0:           CarbonTablePath.DataFileUtil.getTaskNo(blocklet.getPath()));
0:       if (partitionInfo != null && partitionInfo.getPartitionType() != PartitionType.NATIVE_HIVE) {
0:           partitionIndex = oldPartitionIdList.indexOf((int)partitionId);
0:           partitionIndex = partitionIdList.indexOf((int)partitionId);
commit:0586146
/////////////////////////////////////////////////////////////////////////
0: import java.io.FileNotFoundException;
/////////////////////////////////////////////////////////////////////////
0:           CarbonInputSplit inputSplit = convertToCarbonInputSplit(blocklet);
0:           if (inputSplit != null) {
0:             resultFilterredBlocks.add(inputSplit);
1:           }
/////////////////////////////////////////////////////////////////////////
0:     try {
0:       blocklet.updateLocations();
0:     } catch (FileNotFoundException e) {
0:       // In case of clean files there is a chance of carbondata file is deleted but index file
0:       // exist inside merged file. So just return null.
0:       return null;
1:     }
commit:b681244
/////////////////////////////////////////////////////////////////////////
0: import java.lang.reflect.Constructor;
1: import java.util.ArrayList;
1: import java.util.Arrays;
1: import java.util.BitSet;
1: import java.util.HashMap;
1: import java.util.Map;
0: import org.apache.carbondata.core.constants.CarbonCommonConstants;
0: import org.apache.carbondata.core.datastore.TableSegmentUniqueIdentifier;
0: import org.apache.carbondata.core.indexstore.Blocklet;
0: import org.apache.carbondata.core.indexstore.DataMapStoreManager;
0: import org.apache.carbondata.core.indexstore.DataMapType;
0: import org.apache.carbondata.core.indexstore.TableDataMap;
0: import org.apache.carbondata.core.keygenerator.KeyGenException;
1: import org.apache.carbondata.core.metadata.AbsoluteTableIdentifier;
0: import org.apache.carbondata.core.metadata.ColumnarFormatVersion;
1: import org.apache.carbondata.core.metadata.schema.PartitionInfo;
1: import org.apache.carbondata.core.metadata.schema.table.CarbonTable;
1: import org.apache.carbondata.core.mutate.CarbonUpdateUtil;
1: import org.apache.carbondata.core.mutate.SegmentUpdateDetails;
1: import org.apache.carbondata.core.mutate.UpdateVO;
1: import org.apache.carbondata.core.mutate.data.BlockMappingVO;
1: import org.apache.carbondata.core.scan.filter.FilterExpressionProcessor;
0: import org.apache.carbondata.core.scan.model.CarbonQueryPlan;
0: import org.apache.carbondata.core.scan.model.QueryModel;
0: import org.apache.carbondata.core.scan.partition.PartitionUtil;
0: import org.apache.carbondata.core.scan.partition.Partitioner;
0: import org.apache.carbondata.core.stats.QueryStatistic;
0: import org.apache.carbondata.core.stats.QueryStatisticsConstants;
0: import org.apache.carbondata.core.stats.QueryStatisticsRecorder;
1: import org.apache.carbondata.core.statusmanager.SegmentStatusManager;
1: import org.apache.carbondata.core.statusmanager.SegmentUpdateStatusManager;
0: import org.apache.carbondata.core.util.CarbonTimeStatisticsFactory;
1: import org.apache.carbondata.core.util.CarbonUtil;
0: import org.apache.carbondata.core.util.path.CarbonStorePath;
1: import org.apache.carbondata.core.util.path.CarbonTablePath;
0: import org.apache.carbondata.hadoop.CarbonMultiBlockSplit;
0: import org.apache.carbondata.hadoop.CarbonRecordReader;
0: import org.apache.carbondata.hadoop.readsupport.CarbonReadSupport;
0: import org.apache.carbondata.hadoop.readsupport.impl.DictionaryDecodeReadSupport;
0: import org.apache.carbondata.hadoop.util.SchemaReader;
1: import org.apache.commons.logging.Log;
1: import org.apache.commons.logging.LogFactory;
1: import org.apache.hadoop.fs.FileSystem;
0: import org.apache.hadoop.fs.InvalidPathException;
0: import org.apache.hadoop.fs.LocalFileSystem;
1: import org.apache.hadoop.fs.Path;
1: import org.apache.hadoop.mapreduce.lib.input.FileSplit;
0: import org.apache.hadoop.mapreduce.security.TokenCache;
0: import org.apache.hadoop.util.StringUtils;
1:  *
1:   // comma separated list of input segment numbers
1:   public static final String INPUT_SEGMENT_NUMBERS =
1:       "mapreduce.input.carboninputformat.segmentnumbers";
1:   // comma separated list of input files
1:   public static final String INPUT_FILES = "mapreduce.input.carboninputformat.files";
1:   private static final Log LOG = LogFactory.getLog(CarbonTableInputFormat.class);
0:   private static final String COLUMN_PROJECTION = "mapreduce.input.carboninputformat.projection";
0:   private static final String CARBON_TABLE = "mapreduce.input.carboninputformat.table";
1:   private static final String CARBON_READ_SUPPORT = "mapreduce.input.carboninputformat.readsupport";
1:   /**
0:    * It is optional, if user does not set then it reads from store
1:    *
0:    * @param configuration
1:    * @param carbonTable
1:    * @throws IOException
1:    */
0:   public static void setCarbonTable(Configuration configuration, CarbonTable carbonTable)
1:       throws IOException {
0:     if (null != carbonTable) {
0:       configuration.set(CARBON_TABLE, ObjectSerializationUtil.convertObjectToString(carbonTable));
0:   public static CarbonTable getCarbonTable(Configuration configuration) throws IOException {
0:     String carbonTableStr = configuration.get(CARBON_TABLE);
0:     if (carbonTableStr == null) {
0:       populateCarbonTable(configuration);
0:       // read it from schema file in the store
0:       carbonTableStr = configuration.get(CARBON_TABLE);
0:       return (CarbonTable) ObjectSerializationUtil.convertStringToObject(carbonTableStr);
0:     return (CarbonTable) ObjectSerializationUtil.convertStringToObject(carbonTableStr);
0:    * this method will read the schema from the physical file and populate into CARBON_TABLE
1:    *
0:    * @param configuration
1:    * @throws IOException
0:   private static void populateCarbonTable(Configuration configuration) throws IOException {
0:     String dirs = configuration.get(INPUT_DIR, "");
0:     String[] inputPaths = StringUtils.split(dirs);
0:     if (inputPaths.length == 0) {
0:       throw new InvalidPathException("No input paths specified in job");
1:     }
0:     AbsoluteTableIdentifier absoluteTableIdentifier =
0:         AbsoluteTableIdentifier.fromTablePath(inputPaths[0]);
0:     // read the schema file to get the absoluteTableIdentifier having the correct table id
0:     // persisted in the schema
0:     CarbonTable carbonTable = SchemaReader.readCarbonTableFromStore(absoluteTableIdentifier);
0:     setCarbonTable(configuration, carbonTable);
1:   }
0:   public static void setTablePath(Configuration configuration, String tablePath)
1:       throws IOException {
0:     configuration.set(FileInputFormat.INPUT_DIR, tablePath);
0:    * It sets unresolved filter expression.
1:    *
0:    * @param configuration
0:    * @param filterExpression
0:   public static void setFilterPredicates(Configuration configuration, Expression filterExpression) {
0:     if (filterExpression == null) {
0:       return;
1:     }
0:       String filterString = ObjectSerializationUtil.convertObjectToString(filterExpression);
0:       configuration.set(FILTER_PREDICATE, filterString);
0:   public static void setColumnProjection(Configuration configuration, CarbonProjection projection) {
0:     if (projection == null || projection.isEmpty()) {
0:       return;
0:     String[] allColumns = projection.getAllColumns();
0:     StringBuilder builder = new StringBuilder();
0:     for (String column : allColumns) {
0:       builder.append(column).append(",");
0:     String columnString = builder.toString();
0:     columnString = columnString.substring(0, columnString.length() - 1);
0:     configuration.set(COLUMN_PROJECTION, columnString);
1:   }
1: 
0:   public static String getColumnProjection(Configuration configuration) {
0:     return configuration.get(COLUMN_PROJECTION);
1:   }
1: 
0:   public static void setCarbonReadSupport(Configuration configuration,
0:       Class<? extends CarbonReadSupport> readSupportClass) {
0:     if (readSupportClass != null) {
0:       configuration.set(CARBON_READ_SUPPORT, readSupportClass.getName());
1:     }
1:   }
1: 
0:   private static CarbonTablePath getTablePath(AbsoluteTableIdentifier absIdentifier) {
0:     return CarbonStorePath.getCarbonTablePath(absIdentifier);
0:    * Set list of segments to access
0:   public static void setSegmentsToAccess(Configuration configuration, List<String> validSegments) {
0:     configuration.set(INPUT_SEGMENT_NUMBERS, CarbonUtil.getSegmentString(validSegments));
1:   }
1: 
1:   /**
0:    * Set list of files to access
1:    */
0:   public static void setFilesToAccess(Configuration configuration, List<String> validFiles) {
0:     configuration.set(INPUT_FILES, CarbonUtil.getSegmentString(validFiles));
1:   }
1: 
0:   private static AbsoluteTableIdentifier getAbsoluteTableIdentifier(Configuration configuration)
1:       throws IOException {
0:     return getCarbonTable(configuration).getAbsoluteTableIdentifier();
1:   }
1: 
1:   /**
1:    * {@inheritDoc}
1:    * Configurations FileInputFormat.INPUT_DIR
1:    * are used to get table path to read.
1:    *
1:    * @param job
1:    * @return List<InputSplit> list of CarbonInputSplit
1:    * @throws IOException
1:    */
0:   @Override public List<InputSplit> getSplits(JobContext job) throws IOException {
1:     AbsoluteTableIdentifier identifier = getAbsoluteTableIdentifier(job.getConfiguration());
0:     TableDataMap blockletMap =
0:         DataMapStoreManager.getInstance().getDataMap(identifier, "blocklet", DataMapType.BLOCKLET);
0:     List<String> invalidSegments = new ArrayList<>();
1:     List<UpdateVO> invalidTimestampsList = new ArrayList<>();
0:     List<String> validSegments = Arrays.asList(getSegmentsToAccess(job));
0:     // get all valid segments and set them into the configuration
1:     if (validSegments.size() == 0) {
0:       SegmentStatusManager segmentStatusManager = new SegmentStatusManager(identifier);
0:       SegmentStatusManager.ValidAndInvalidSegmentsInfo segments =
0:           segmentStatusManager.getValidAndInvalidSegments();
0:       SegmentUpdateStatusManager updateStatusManager = new SegmentUpdateStatusManager(identifier);
0:       validSegments = segments.getValidSegments();
1:       if (validSegments.size() == 0) {
0:         return new ArrayList<>(0);
1:       }
1: 
1:       // remove entry in the segment index if there are invalid segments
1:       invalidSegments.addAll(segments.getInvalidSegments());
0:       for (String invalidSegmentId : invalidSegments) {
0:         invalidTimestampsList.add(updateStatusManager.getInvalidTimestampRange(invalidSegmentId));
1:       }
1:       if (invalidSegments.size() > 0) {
0:         List<TableSegmentUniqueIdentifier> invalidSegmentsIds =
0:             new ArrayList<>(invalidSegments.size());
0:         for (String segId : invalidSegments) {
0:           invalidSegmentsIds.add(new TableSegmentUniqueIdentifier(identifier, segId));
1:         }
0:         blockletMap.clear(invalidSegments);
1:       }
1:     }
1: 
1:     // process and resolve the expression
1:     Expression filter = getFilterPredicates(job.getConfiguration());
0:     CarbonTable carbonTable = getCarbonTable(job.getConfiguration());
1:     // this will be null in case of corrupt schema file.
1:     if (null == carbonTable) {
1:       throw new IOException("Missing/Corrupt schema file for table.");
1:     }
1: 
0:     CarbonInputFormatUtil.processFilterExpression(filter, carbonTable);
1: 
1:     // prune partitions for filter query on partition table
1:     BitSet matchedPartitions = null;
1:     if (null != filter) {
0:       PartitionInfo partitionInfo = carbonTable.getPartitionInfo(carbonTable.getFactTableName());
0:       if (null != partitionInfo) {
0:         Partitioner partitioner = PartitionUtil.getPartitioner(partitionInfo);
0:         matchedPartitions = new FilterExpressionProcessor()
0:             .getFilteredPartitions(filter, partitionInfo, partitioner);
1:         if (matchedPartitions.cardinality() == 0) {
0:           // no partition is required
1:           return new ArrayList<InputSplit>();
1:         }
0:         if (matchedPartitions.cardinality() == partitioner.numPartitions()) {
0:           // all partitions are required, no need to prune partitions
1:           matchedPartitions = null;
1:         }
1:       }
1:     }
1: 
0:     FilterResolverIntf filterInterface = CarbonInputFormatUtil.resolveFilter(filter, identifier);
1: 
1:     // do block filtering and get split
0:     List<InputSplit> splits = getSplits(job, filterInterface, validSegments, matchedPartitions);
1:     // pass the invalid segment to task side in order to remove index entry in task side
1:     if (invalidSegments.size() > 0) {
1:       for (InputSplit split : splits) {
1:         ((org.apache.carbondata.hadoop.CarbonInputSplit) split).setInvalidSegments(invalidSegments);
1:         ((org.apache.carbondata.hadoop.CarbonInputSplit) split)
1:             .setInvalidTimestampRange(invalidTimestampsList);
1:       }
1:     }
1:     return splits;
1:   }
1: 
1:   /**
1:    * {@inheritDoc}
0:    * Configurations FileInputFormat.INPUT_DIR, CarbonInputFormat.INPUT_SEGMENT_NUMBERS
1:    * are used to get table path to read.
1:    *
1:    * @return
1:    * @throws IOException
1:    */
0:   private List<InputSplit> getSplits(JobContext job, FilterResolverIntf filterResolver,
0:       List<String> validSegments, BitSet matchedPartitions) throws IOException {
1: 
1:     List<InputSplit> result = new LinkedList<InputSplit>();
1:     UpdateVO invalidBlockVOForSegmentId = null;
1:     Boolean isIUDTable = false;
1: 
0:     AbsoluteTableIdentifier absoluteTableIdentifier =
0:         getCarbonTable(job.getConfiguration()).getAbsoluteTableIdentifier();
0:     SegmentUpdateStatusManager updateStatusManager =
0:         new SegmentUpdateStatusManager(absoluteTableIdentifier);
1: 
1:     isIUDTable = (updateStatusManager.getUpdateStatusDetails().length != 0);
1: 
0:     //for each segment fetch blocks matching filter in Driver BTree
1:     List<org.apache.carbondata.hadoop.CarbonInputSplit> dataBlocksOfSegment =
0:         getDataBlocksOfSegment(job, absoluteTableIdentifier, filterResolver, matchedPartitions,
0:             validSegments);
1:     for (org.apache.carbondata.hadoop.CarbonInputSplit inputSplit : dataBlocksOfSegment) {
1: 
1:       // Get the UpdateVO for those tables on which IUD operations being performed.
1:       if (isIUDTable) {
1:         invalidBlockVOForSegmentId =
1:             updateStatusManager.getInvalidTimestampRange(inputSplit.getSegmentId());
1:       }
1:       if (isIUDTable) {
1:         // In case IUD is not performed in this table avoid searching for
1:         // invalidated blocks.
1:         if (CarbonUtil
1:             .isInvalidTableBlock(inputSplit.getSegmentId(), inputSplit.getPath().toString(),
1:                 invalidBlockVOForSegmentId, updateStatusManager)) {
1:           continue;
1:         }
1:       }
1:       String[] deleteDeltaFilePath = null;
0:       try {
0:         deleteDeltaFilePath =
0:             updateStatusManager.getDeleteDeltaFilePath(inputSplit.getPath().toString());
0:       } catch (Exception e) {
1:         throw new IOException(e);
1:       }
1:       inputSplit.setDeleteDeltaFiles(deleteDeltaFilePath);
1:       result.add(inputSplit);
1:     }
1:     return result;
1:   }
1: 
0:   protected Expression getFilterPredicates(Configuration configuration) {
0:     try {
0:       String filterExprString = configuration.get(FILTER_PREDICATE);
0:       if (filterExprString == null) {
0:         return null;
1:       }
0:       Object filter = ObjectSerializationUtil.convertStringToObject(filterExprString);
0:       return (Expression) filter;
1:     } catch (IOException e) {
0:       throw new RuntimeException("Error while reading filter expression", e);
1:     }
1:   }
1: 
1:   /**
0:    * get data blocks of given segment
1:    */
0:   private List<org.apache.carbondata.hadoop.CarbonInputSplit> getDataBlocksOfSegment(JobContext job,
0:       AbsoluteTableIdentifier absoluteTableIdentifier, FilterResolverIntf resolver,
0:       BitSet matchedPartitions, List<String> segmentIds) throws IOException {
1: 
0:     QueryStatisticsRecorder recorder = CarbonTimeStatisticsFactory.createDriverRecorder();
0:     QueryStatistic statistic = new QueryStatistic();
1: 
0:     // get tokens for all the required FileSystem for table path
0:     TokenCache.obtainTokensForNamenodes(job.getCredentials(),
0:         new Path[] { new Path(absoluteTableIdentifier.getTablePath()) }, job.getConfiguration());
1: 
0:     TableDataMap blockletMap = DataMapStoreManager.getInstance()
0:         .getDataMap(absoluteTableIdentifier, "blocklet", DataMapType.BLOCKLET);
0:     List<Blocklet> prunedBlocklets = blockletMap.prune(segmentIds, resolver);
1: 
0:     List<org.apache.carbondata.hadoop.CarbonInputSplit> resultFilterredBlocks = new ArrayList<>();
0:     for (Blocklet blocklet : prunedBlocklets) {
0:       int taskId = CarbonTablePath.DataFileUtil.getTaskIdFromTaskNo(
0:           CarbonTablePath.DataFileUtil.getTaskNo(blocklet.getPath().toString()));
1: 
0:       // matchedPartitions variable will be null in two cases as follows
0:       // 1. the table is not a partition table
0:       // 2. the table is a partition table, and all partitions are matched by query
0:       // for partition table, the task id of carbaondata file name is the partition id.
0:       // if this partition is not required, here will skip it.
0:       if (matchedPartitions == null || matchedPartitions.get(taskId)) {
0:         resultFilterredBlocks.add(convertToCarbonInputSplit(blocklet));
1:       }
1:     }
0:     statistic
0:         .addStatistics(QueryStatisticsConstants.LOAD_BLOCKS_DRIVER, System.currentTimeMillis());
0:     recorder.recordStatisticsForDriver(statistic, job.getConfiguration().get("query.id"));
0:     return resultFilterredBlocks;
1:   }
1: 
0:   private org.apache.carbondata.hadoop.CarbonInputSplit convertToCarbonInputSplit(Blocklet blocklet)
1:       throws IOException {
0:     blocklet.updateLocations();
0:     org.apache.carbondata.hadoop.CarbonInputSplit split =
0:         org.apache.carbondata.hadoop.CarbonInputSplit.from(blocklet.getSegmentId(),
0:             new FileSplit(blocklet.getPath(), 0, blocklet.getLength(), blocklet.getLocations()),
0:             ColumnarFormatVersion.valueOf((short) blocklet.getDetailInfo().getVersionNumber()));
0:     split.setDetailInfo(blocklet.getDetailInfo());
0:     return split;
1:   }
1: 
0:   @Override public RecordReader<Void, T> createRecordReader(InputSplit inputSplit,
0:       TaskAttemptContext taskAttemptContext) throws IOException, InterruptedException {
0:     Configuration configuration = taskAttemptContext.getConfiguration();
0:     QueryModel queryModel = getQueryModel(inputSplit, taskAttemptContext);
0:     CarbonReadSupport<T> readSupport = getReadSupportClass(configuration);
0:     return new CarbonRecordReader<T>(queryModel, readSupport);
1:   }
1: 
0:   public QueryModel getQueryModel(InputSplit inputSplit, TaskAttemptContext taskAttemptContext)
1:       throws IOException {
0:     Configuration configuration = taskAttemptContext.getConfiguration();
0:     CarbonTable carbonTable = getCarbonTable(configuration);
0:     // getting the table absoluteTableIdentifier from the carbonTable
0:     // to avoid unnecessary deserialization
0:     AbsoluteTableIdentifier identifier = carbonTable.getAbsoluteTableIdentifier();
1: 
0:     // query plan includes projection column
0:     String projection = getColumnProjection(configuration);
0:     CarbonQueryPlan queryPlan = CarbonInputFormatUtil.createQueryPlan(carbonTable, projection);
0:     QueryModel queryModel = QueryModel.createModel(identifier, queryPlan, carbonTable);
1: 
0:     // set the filter to the query model in order to filter blocklet before scan
0:     Expression filter = getFilterPredicates(configuration);
0:     CarbonInputFormatUtil.processFilterExpression(filter, carbonTable);
0:     FilterResolverIntf filterIntf = CarbonInputFormatUtil.resolveFilter(filter, identifier);
0:     queryModel.setFilterExpressionResolverTree(filterIntf);
1: 
0:     // update the file level index store if there are invalid segment
0:     if (inputSplit instanceof CarbonMultiBlockSplit) {
0:       CarbonMultiBlockSplit split = (CarbonMultiBlockSplit) inputSplit;
0:       List<String> invalidSegments = split.getAllSplits().get(0).getInvalidSegments();
1:       if (invalidSegments.size() > 0) {
0:         queryModel.setInvalidSegmentIds(invalidSegments);
1:       }
0:       List<UpdateVO> invalidTimestampRangeList =
0:           split.getAllSplits().get(0).getInvalidTimestampRange();
0:       if ((null != invalidTimestampRangeList) && (invalidTimestampRangeList.size() > 0)) {
0:         queryModel.setInvalidBlockForSegmentId(invalidTimestampRangeList);
1:       }
1:     }
0:     return queryModel;
1:   }
1: 
0:   public CarbonReadSupport<T> getReadSupportClass(Configuration configuration) {
0:     String readSupportClass = configuration.get(CARBON_READ_SUPPORT);
0:     //By default it uses dictionary decoder read class
0:     CarbonReadSupport<T> readSupport = null;
0:     if (readSupportClass != null) {
0:       try {
0:         Class<?> myClass = Class.forName(readSupportClass);
0:         Constructor<?> constructor = myClass.getConstructors()[0];
0:         Object object = constructor.newInstance();
0:         if (object instanceof CarbonReadSupport) {
0:           readSupport = (CarbonReadSupport) object;
1:         }
0:       } catch (ClassNotFoundException ex) {
0:         LOG.error("Class " + readSupportClass + "not found", ex);
0:       } catch (Exception ex) {
0:         LOG.error("Error while creating " + readSupportClass, ex);
1:       }
1:     } else {
0:       readSupport = new DictionaryDecodeReadSupport<>();
1:     }
0:     return readSupport;
1:   }
1: 
0:   @Override protected boolean isSplitable(JobContext context, Path filename) {
0:     try {
0:       // Don't split the file if it is local file system
0:       FileSystem fileSystem = filename.getFileSystem(context.getConfiguration());
0:       if (fileSystem instanceof LocalFileSystem) {
0:         return false;
1:       }
0:     } catch (Exception e) {
0:       return true;
1:     }
0:     return true;
1:   }
1: 
1:   /**
0:    * required to be moved to core
1:    *
0:    * @return updateExtension
1:    */
0:   private String getUpdateExtension() {
0:     // TODO: required to modify when supporting update, mostly will be update timestamp
0:     return "update";
1:   }
1: 
1:   /**
1:    * return valid segment to access
1:    */
0:   private String[] getSegmentsToAccess(JobContext job) {
1:     String segmentString = job.getConfiguration().get(INPUT_SEGMENT_NUMBERS, "");
1:     if (segmentString.trim().isEmpty()) {
0:       return new String[0];
1:     }
0:     return segmentString.split(",");
1:   }
1: 
1:   /**
1:    * Get the row count of the Block and mapping of segment and Block count.
1:    *
1:    * @param job
0:    * @param identifier
1:    * @return
1:    * @throws IOException
0:    * @throws KeyGenException
1:    */
0:   public BlockMappingVO getBlockRowCount(JobContext job, AbsoluteTableIdentifier identifier)
0:       throws IOException, KeyGenException {
0:     TableDataMap blockletMap =
0:         DataMapStoreManager.getInstance().getDataMap(identifier, "blocklet", DataMapType.BLOCKLET);
0:     SegmentUpdateStatusManager updateStatusManager = new SegmentUpdateStatusManager(identifier);
0:     SegmentStatusManager.ValidAndInvalidSegmentsInfo validAndInvalidSegments =
0:         new SegmentStatusManager(identifier).getValidAndInvalidSegments();
0:     Map<String, Long> blockRowCountMapping =
0:         new HashMap<>(CarbonCommonConstants.DEFAULT_COLLECTION_SIZE);
0:     Map<String, Long> segmentAndBlockCountMapping =
0:         new HashMap<>(CarbonCommonConstants.DEFAULT_COLLECTION_SIZE);
0:     List<Blocklet> blocklets = blockletMap.prune(validAndInvalidSegments.getValidSegments(), null);
0:     for (Blocklet blocklet : blocklets) {
0:       String blockName = blocklet.getPath().toString();
1:       blockName = CarbonTablePath.getCarbonDataFileName(blockName);
1:       blockName = blockName + CarbonTablePath.getCarbonDataExtension();
1: 
1:       long rowCount = blocklet.getDetailInfo().getRowCount();
1: 
0:       String key = CarbonUpdateUtil.getSegmentBlockNameKey(blocklet.getSegmentId(), blockName);
1: 
1:       // if block is invalid then dont add the count
1:       SegmentUpdateDetails details = updateStatusManager.getDetailsForABlock(key);
1: 
0:       if (null == details || !CarbonUpdateUtil.isBlockInvalid(details.getStatus())) {
1:         Long blockCount = blockRowCountMapping.get(key);
1:         if (blockCount == null) {
1:           blockCount = 0L;
0:           Long count = segmentAndBlockCountMapping.get(blocklet.getSegmentId());
1:           if (count == null) {
1:             count = 0L;
1:           }
0:           segmentAndBlockCountMapping.put(blocklet.getSegmentId(), count + 1);
1:         }
1:         blockCount += rowCount;
1:         blockRowCountMapping.put(key, blockCount);
1:       }
1:     }
1:     return new BlockMappingVO(blockRowCountMapping, segmentAndBlockCountMapping);
commit:988ccd8
/////////////////////////////////////////////////////////////////////////
0: 
/////////////////////////////////////////////////////////////////////////
0:       case COLUMNAR:
author:akashrn5
-------------------------------------------------------------------------------
commit:7e93d7b
/////////////////////////////////////////////////////////////////////////
1:     refreshSegmentCacheIfRequired(job, carbonTable, updateStatusManager, filteredSegmentToAccess);
/////////////////////////////////////////////////////////////////////////
1:    * Method to check and refresh segment cache
0:    *
1:    * @param job
0:    * @param carbonTable
1:    * @param updateStatusManager
1:    * @param filteredSegmentToAccess
0:    * @throws IOException
1:    */
1:   public void refreshSegmentCacheIfRequired(JobContext job, CarbonTable carbonTable,
1:       SegmentUpdateStatusManager updateStatusManager, List<Segment> filteredSegmentToAccess)
0:       throws IOException {
1:     List<Segment> toBeCleanedSegments = new ArrayList<>();
1:     for (Segment filteredSegment : filteredSegmentToAccess) {
1:       boolean refreshNeeded =
1:           DataMapStoreManager.getInstance().getTableSegmentRefresher(carbonTable)
1:               .isRefreshNeeded(filteredSegment,
1:                   updateStatusManager.getInvalidTimestampRange(filteredSegment.getSegmentNo()));
1:       if (refreshNeeded) {
1:         toBeCleanedSegments.add(filteredSegment);
0:       }
0:     }
1:     // Clean segments if refresh is needed
0:     for (Segment segment : filteredSegmentToAccess) {
1:       if (DataMapStoreManager.getInstance().getTableSegmentRefresher(carbonTable)
0:           .isRefreshNeeded(segment.getSegmentNo())) {
1:         toBeCleanedSegments.add(segment);
0:       }
0:     }
1:     if (toBeCleanedSegments.size() > 0) {
1:       DataMapStoreManager.getInstance()
1:           .clearInvalidSegments(getOrCreateCarbonTable(job.getConfiguration()),
1:               toBeCleanedSegments);
0:     }
0:   }
0: 
1:   /**
author:ajantha-bhat
-------------------------------------------------------------------------------
commit:6d40d3a
/////////////////////////////////////////////////////////////////////////
1:     /* In the select * flow, getSplits() method was clearing the segmentMap if,
1:     segment needs refreshing. same thing need for select count(*) flow also.
1:     For NonTransactional table, one of the reason for a segment refresh is below scenario.
1:     SDK is written one set of files with UUID, with same UUID it can write again.
1:     So, latest files content should reflect the new count by refreshing the segment */
0:     List<Segment> toBeCleanedSegments = new ArrayList<>();
1:     for (Segment eachSegment : filteredSegment) {
1:       boolean refreshNeeded = DataMapStoreManager.getInstance()
1:           .getTableSegmentRefresher(getOrCreateCarbonTable(job.getConfiguration()))
1:           .isRefreshNeeded(eachSegment,
1:               updateStatusManager.getInvalidTimestampRange(eachSegment.getSegmentNo()));
1:       if (refreshNeeded) {
1:         toBeCleanedSegments.add(eachSegment);
0:       }
0:     }
1:     if (toBeCleanedSegments.size() > 0) {
1:       DataMapStoreManager.getInstance()
1:           .clearInvalidSegments(getOrCreateCarbonTable(job.getConfiguration()),
1:               toBeCleanedSegments);
0:     }
commit:2f79e14
/////////////////////////////////////////////////////////////////////////
/////////////////////////////////////////////////////////////////////////
/////////////////////////////////////////////////////////////////////////
commit:1dfbcfc
/////////////////////////////////////////////////////////////////////////
0: import org.apache.carbondata.core.metadata.converter.SchemaConverter;
0: import org.apache.carbondata.core.metadata.converter.ThriftWrapperSchemaConverterImpl;
0: import org.apache.carbondata.core.metadata.schema.table.column.ColumnSchema;
/////////////////////////////////////////////////////////////////////////
0: 
0:     // For NonTransactional table, compare the schema of all index files with inferred schema.
0:     // If there is a mismatch throw exception. As all files must be of same schema.
0:     if (!carbonTable.getTableInfo().isTransactionalTable()) {
0:       SchemaConverter schemaConverter = new ThriftWrapperSchemaConverterImpl();
0:       for (Segment segment : segments.getValidSegments()) {
0:         Map<String, String> indexFiles = segment.getCommittedIndexFile();
0:         for (Map.Entry<String, String> indexFileEntry : indexFiles.entrySet()) {
0:           Path indexFile = new Path(indexFileEntry.getKey());
0:           org.apache.carbondata.format.TableInfo tableInfo = CarbonUtil.inferSchemaFromIndexFile(
0:               indexFile.toString(), carbonTable.getTableName());
0:           TableInfo wrapperTableInfo = schemaConverter.fromExternalToWrapperTableInfo(
0:               tableInfo, identifier.getDatabaseName(),
0:               identifier.getTableName(),
0:               identifier.getTablePath());
0:           List<ColumnSchema> indexFileColumnList =
0:               wrapperTableInfo.getFactTable().getListOfColumns();
0:           List<ColumnSchema> tableColumnList =
0:               carbonTable.getTableInfo().getFactTable().getListOfColumns();
0:           if (!compareColumnSchemaList(indexFileColumnList, tableColumnList)) {
0:             LOG.error("Schema of " + indexFile.getName()
0:                 + " doesn't match with the table's schema");
0:             throw new IOException("All the files doesn't have same schema. "
0:                 + "Unsupported operation on nonTransactional table. Check logs.");
0:           }
0:         }
0:       }
0:     }
0: 
/////////////////////////////////////////////////////////////////////////
0:   private boolean compareColumnSchemaList(List<ColumnSchema> indexFileColumnList,
0:       List<ColumnSchema> tableColumnList) {
0:     if (indexFileColumnList.size() != tableColumnList.size()) {
0:       return false;
0:     }
0:     for (int i = 0; i < tableColumnList.size(); i++) {
0:       return indexFileColumnList.get(i).equalsWithStrictCheck(tableColumnList.get(i));
0:     }
0:     return false;
0:   }
0: 
commit:280a400
/////////////////////////////////////////////////////////////////////////
1: import org.apache.carbondata.core.readcommitter.LatestFilesReadCommittedScope;
1: import org.apache.carbondata.core.readcommitter.ReadCommittedScope;
1: import org.apache.carbondata.core.readcommitter.TableStatusReadCommittedScope;
/////////////////////////////////////////////////////////////////////////
0:   private static final String CARBON_UNMANAGED_TABLE =
0:       "mapreduce.input.carboninputformat.unmanaged";
1:   private ReadCommittedScope readCommittedScope;
/////////////////////////////////////////////////////////////////////////
0: 
1:     this.readCommittedScope = getReadCommitted(job, identifier);
1:     LoadMetadataDetails[] loadMetadataDetails = readCommittedScope.getSegmentList();
0: 
/////////////////////////////////////////////////////////////////////////
0: 
1:       carbonTable = getOrCreateCarbonTable(job.getConfiguration());
1:       ReadCommittedScope readCommittedScope =
1:           getReadCommitted(job, carbonTable.getAbsoluteTableIdentifier());
1:       this.readCommittedScope = readCommittedScope;
0: 
/////////////////////////////////////////////////////////////////////////
0:             validSegments, partitionInfo, oldPartitionIdList, readCommittedScope);
/////////////////////////////////////////////////////////////////////////
0: 
1:     ReadCommittedScope readCommittedScope = getReadCommitted(job, identifier);
1:     LoadMetadataDetails[] loadMetadataDetails = readCommittedScope.getSegmentList();
0: 
/////////////////////////////////////////////////////////////////////////
1:     List<ExtendedBlocklet> blocklets =
0:         blockletMap.prune(filteredSegment, null, partitions, readCommittedScope);
/////////////////////////////////////////////////////////////////////////
0: 
1:   public ReadCommittedScope getReadCommitted(JobContext job, AbsoluteTableIdentifier identifier)
0:       throws IOException {
1:     if (readCommittedScope == null) {
1:       ReadCommittedScope readCommittedScope;
0:       if (job.getConfiguration().getBoolean(CARBON_UNMANAGED_TABLE, false)) {
0:         readCommittedScope = new LatestFilesReadCommittedScope(identifier.getTablePath());
1:       } else {
0:         readCommittedScope = new TableStatusReadCommittedScope(identifier);
0:       }
1:       this.readCommittedScope = readCommittedScope;
0:     }
1:     return readCommittedScope;
0:   }
0: }
author:mohammadshahidkhan
-------------------------------------------------------------------------------
commit:12c28c9
/////////////////////////////////////////////////////////////////////////
0: import org.apache.carbondata.core.indexstore.blockletindex.SegmentIndexFileStore;
/////////////////////////////////////////////////////////////////////////
0:         String segmentDir =
0:             CarbonTablePath.getSegmentPath(identifier.getTablePath(), segment.getSegmentNo());
0:           SegmentIndexFileStore segmentIndexFileStore = new SegmentIndexFileStore();
0:           segmentIndexFileStore.readAllIIndexOfSegment(segmentDir);
0:           Map<String, byte[]> carbonIndexMap = segmentIndexFileStore.getCarbonIndexMap();
0:           CarbonIndexFileReader indexReader = new CarbonIndexFileReader();
0:           for (byte[] fileData : carbonIndexMap.values()) {
0:             indexReader.openThriftReader(fileData);
commit:0c200d8
/////////////////////////////////////////////////////////////////////////
0:       streamSegments = getFilteredSegment(job,streamSegments, true);
0:       List<Segment> filteredSegmentToAccess = getFilteredSegment(job, segments.getValidSegments(),
0:           true);
/////////////////////////////////////////////////////////////////////////
0:         getFilteredSegment(job, new ArrayList<>(validAndInProgressSegments), false);
/////////////////////////////////////////////////////////////////////////
1:   private List<Segment> getFilteredSegment(JobContext job, List<Segment> validSegments,
0:       boolean validationRequired) {
/////////////////////////////////////////////////////////////////////////
1:       if (filteredSegmentToAccess.size() != segmentToAccessSet.size() && !validationRequired) {
1:         for (Segment segment : segmentToAccessSet) {
1:           if (!filteredSegmentToAccess.contains(segment)) {
1:             filteredSegmentToAccess.add(segment);
0:           }
0:         }
0:       }
/////////////////////////////////////////////////////////////////////////
0:     List<Segment> filteredSegment = getFilteredSegment(job, allSegments.getValidSegments(), false);
commit:1155d4d
/////////////////////////////////////////////////////////////////////////
0: import org.apache.carbondata.core.exception.InvalidConfigurationException;
/////////////////////////////////////////////////////////////////////////
/////////////////////////////////////////////////////////////////////////
/////////////////////////////////////////////////////////////////////////
1:   public static final String DATABASE_NAME = "mapreduce.input.carboninputformat.databaseName";
1:   public static final String TABLE_NAME = "mapreduce.input.carboninputformat.tableName";
/////////////////////////////////////////////////////////////////////////
0:     String tablePath = configuration.get(INPUT_DIR, "");
0:     try {
0:       return AbsoluteTableIdentifier
0:           .from(tablePath, getDatabaseName(configuration), getTableName(configuration));
0:     } catch (InvalidConfigurationException e) {
0:       throw new IOException(e);
/////////////////////////////////////////////////////////////////////////
0: 
0:   public static void setDatabaseName(Configuration configuration, String databaseName) {
0:     if (null != databaseName) {
0:       configuration.set(DATABASE_NAME, databaseName);
0:     }
0:   }
0: 
0:   public static String getDatabaseName(Configuration configuration)
0:       throws InvalidConfigurationException {
0:     String databseName = configuration.get(DATABASE_NAME);
0:     if (null == databseName) {
0:       throw new InvalidConfigurationException("Database name is not set.");
0:     }
0:     return databseName;
0:   }
0: 
0:   public static void setTableName(Configuration configuration, String tableName) {
0:     if (null != tableName) {
0:       configuration.set(TABLE_NAME, tableName);
0:     }
0:   }
0: 
0:   public static String getTableName(Configuration configuration)
0:       throws InvalidConfigurationException {
0:     String tableName = configuration.get(TABLE_NAME);
0:     if (tableName == null) {
0:       throw new InvalidConfigurationException("Table name is not set");
0:     }
0:     return tableName;
0:   }
author:rahulforallp
-------------------------------------------------------------------------------
commit:f1a6c7c
/////////////////////////////////////////////////////////////////////////
0:     for (Segment filteredSegment : filteredSegmentToAccess) {
0:               .isRefreshNeeded(filteredSegment,
0:                   updateStatusManager.getInvalidTimestampRange(filteredSegment.getSegmentNo()));
0:         toBeCleanedSegments.add(filteredSegment);
/////////////////////////////////////////////////////////////////////////
0: 
0: 
commit:65471f2
/////////////////////////////////////////////////////////////////////////
0:   public Segment[] getSegmentsToAccess(JobContext job) {
commit:34cb551
/////////////////////////////////////////////////////////////////////////
/////////////////////////////////////////////////////////////////////////
0: 
/////////////////////////////////////////////////////////////////////////
0:       List<String> filteredSegmentToAccess = getFilteredSegment(job, validSegments);
/////////////////////////////////////////////////////////////////////////
1:     // get updated filtered list
0:     List<String> filteredSegmentToAccess = Arrays.asList(getSegmentsToAccess(job));
commit:fd0bdf6
/////////////////////////////////////////////////////////////////////////
1: import java.util.HashSet;
0: import java.util.Set;
/////////////////////////////////////////////////////////////////////////
0:   public static final String VALIDATE_INPUT_SEGMENT_IDs =
0:             "mapreduce.input.carboninputformat.validsegments";
/////////////////////////////////////////////////////////////////////////
0:    * set list of segment to access
1:    */
0:   public static void setValidateSegmentsToAccess(Configuration configuration, Boolean validate) {
0:     configuration.set(CarbonTableInputFormat.VALIDATE_INPUT_SEGMENT_IDs, validate.toString());
0:   }
0: 
0:   /**
0:    * get list of segment to access
1:    */
0:   public static boolean getValidateSegmentsToAccess(Configuration configuration) {
0:     return configuration.get(CarbonTableInputFormat.VALIDATE_INPUT_SEGMENT_IDs, "true")
0:         .equalsIgnoreCase("true");
0:   }
0: 
0:   /**
/////////////////////////////////////////////////////////////////////////
0:     SegmentUpdateStatusManager updateStatusManager = new SegmentUpdateStatusManager(identifier);
0:     List<String> filteredSegmentToAccess = new ArrayList<>();
1:     if (getValidateSegmentsToAccess(job.getConfiguration())) {
0: 
0:       String[] segmentsToAccess = getSegmentsToAccess(job);
0:       Set<String> segmentToAccessSet = new HashSet<>();
0:       for (String segmentToAccess : segmentsToAccess) {
0:         segmentToAccessSet.add(segmentToAccess);
0:       }
0:       // get all valid segments and set them into the configuration
0:       List<String> validSegments = segments.getValidSegments();
0:       if (segmentsToAccess.length == 0 || segmentsToAccess[0].equalsIgnoreCase("*")) {
1:         filteredSegmentToAccess.addAll(validSegments);
1:       } else {
0:         for (String validSegment : validSegments) {
0:           if (segmentToAccessSet.contains(validSegment)) {
0:             filteredSegmentToAccess.add(validSegment);
0:           }
0:         }
1:         if (!filteredSegmentToAccess.containsAll(segmentToAccessSet)) {
0:           List<String> filteredSegmentToAccessTemp = new ArrayList<>();
0:           filteredSegmentToAccessTemp.addAll(filteredSegmentToAccess);
1:           filteredSegmentToAccessTemp.removeAll(segmentToAccessSet);
1:           LOG.info(
1:               "Segments ignored are : " + Arrays.toString(filteredSegmentToAccessTemp.toArray()));
0:         }
0:       }
1:       if (filteredSegmentToAccess.size() == 0) {
0:         return new ArrayList<>(0);
1:       } else {
1:         setSegmentsToAccess(job.getConfiguration(), filteredSegmentToAccess);
0:       }
/////////////////////////////////////////////////////////////////////////
0:     for (String segment : filteredSegmentToAccess) {
/////////////////////////////////////////////////////////////////////////
0:         getSplits(job, filterInterface, filteredSegmentToAccess, matchedPartitions, partitionInfo,
0:             null);
author:dhatchayani
-------------------------------------------------------------------------------
commit:2c0fa10
/////////////////////////////////////////////////////////////////////////
/////////////////////////////////////////////////////////////////////////
/////////////////////////////////////////////////////////////////////////
0:     FilterResolverIntf filterInterface = carbonTable.resolveFilter(filter);
/////////////////////////////////////////////////////////////////////////
/////////////////////////////////////////////////////////////////////////
0:       FilterResolverIntf filterInterface = carbonTable.resolveFilter(filter);
commit:531ecdf
/////////////////////////////////////////////////////////////////////////
1:   public CarbonTable getOrCreateCarbonTable(Configuration configuration) throws IOException {
commit:d3a09e2
/////////////////////////////////////////////////////////////////////////
0: import org.apache.carbondata.core.scan.filter.SingleTableProvider;
0: import org.apache.carbondata.core.scan.filter.TableProvider;
/////////////////////////////////////////////////////////////////////////
0:     TableProvider tableProvider = new SingleTableProvider(carbonTable);
/////////////////////////////////////////////////////////////////////////
0:     FilterResolverIntf filterInterface = CarbonInputFormatUtil
0:         .resolveFilter(filter, carbonTable.getAbsoluteTableIdentifier(), tableProvider);
/////////////////////////////////////////////////////////////////////////
0:       TableProvider tableProvider = new SingleTableProvider(carbonTable);
/////////////////////////////////////////////////////////////////////////
0:       FilterResolverIntf filterInterface =
0:           CarbonInputFormatUtil.resolveFilter(filter, identifier, tableProvider);
/////////////////////////////////////////////////////////////////////////
0:     TableProvider tableProvider = new SingleTableProvider(carbonTable);
/////////////////////////////////////////////////////////////////////////
0:     FilterResolverIntf filterIntf = CarbonInputFormatUtil
0:         .resolveFilter(filter, carbonTable.getAbsoluteTableIdentifier(), tableProvider);
author:sounakr
-------------------------------------------------------------------------------
commit:b7b8073
/////////////////////////////////////////////////////////////////////////
1:   private static final String CARBON_TRANSACTIONAL_TABLE =
1:       "mapreduce.input.carboninputformat.transactional";
/////////////////////////////////////////////////////////////////////////
1:       if (job.getConfiguration().getBoolean(CARBON_TRANSACTIONAL_TABLE, true)) {
1:       } else {
0:         readCommittedScope = new LatestFilesReadCommittedScope(identifier.getTablePath());
commit:c58eb43
/////////////////////////////////////////////////////////////////////////
1:     SegmentStatusManager.ValidAndInvalidSegmentsInfo segments = segmentStatusManager
1:         .getValidAndInvalidSegments(loadMetadataDetails, this.readCommittedScope);
/////////////////////////////////////////////////////////////////////////
1:         streamSegments = getFilteredSegment(job, streamSegments, true, readCommittedScope);
1:             getFilteredSegment(job, segments.getValidSegments(), true, readCommittedScope);
/////////////////////////////////////////////////////////////////////////
1:             getFilteredNormalSegments(job, segments.getValidSegments(),
1:                 getSegmentsToAccess(job, readCommittedScope));
/////////////////////////////////////////////////////////////////////////
1:         getFilteredSegment(job, new ArrayList<>(validAndInProgressSegments), false,
1:             readCommittedScope);
/////////////////////////////////////////////////////////////////////////
1:       boolean validationRequired, ReadCommittedScope readCommittedScope) {
1:     Segment[] segmentsToAccess = getSegmentsToAccess(job, readCommittedScope);
/////////////////////////////////////////////////////////////////////////
0:       List<Segment> segmentList = new ArrayList<>();
0:       segmentList.add(new Segment(targetSegment, null, readCommittedScope));
1:       setSegmentsToAccess(job.getConfiguration(), segmentList);
0: 
/////////////////////////////////////////////////////////////////////////
1:             validSegments, partitionInfo, oldPartitionIdList);
/////////////////////////////////////////////////////////////////////////
1:   public Segment[] getSegmentsToAccess(JobContext job, ReadCommittedScope readCommittedScope) {
1:     List<Segment> segments = Segment.toSegmentList(segmentString.split(","), readCommittedScope);
/////////////////////////////////////////////////////////////////////////
0:         new SegmentStatusManager(identifier)
1:             .getValidAndInvalidSegments(loadMetadataDetails, readCommittedScope);
1:     List<Segment> filteredSegment =
1:         getFilteredSegment(job, allSegments.getValidSegments(), false, readCommittedScope);
0:         blockletMap.prune(filteredSegment, null, partitions);
author:kumarvishal
-------------------------------------------------------------------------------
commit:fb1516c
/////////////////////////////////////////////////////////////////////////
1:     // to check whether only streaming segments access is enabled or not,
1:     // if access streaming segment is true then data will be read from streaming segments
1:     boolean accessStreamingSegments = getAccessStreamingSegments(job.getConfiguration());
1:       if (!accessStreamingSegments) {
0:         List<Segment> validSegments = segments.getValidSegments();
1:         streamSegments = segments.getStreamSegments();
0:         streamSegments = getFilteredSegment(job, streamSegments, true);
0:         if (validSegments.size() == 0) {
0:           return getSplitsOfStreaming(job, identifier, streamSegments);
0:         }
1:         List<Segment> filteredSegmentToAccess =
0:             getFilteredSegment(job, segments.getValidSegments(), true);
0:         if (filteredSegmentToAccess.size() == 0) {
0:           return getSplitsOfStreaming(job, identifier, streamSegments);
0:         } else {
0:           setSegmentsToAccess(job.getConfiguration(), filteredSegmentToAccess);
0:         }
1:         List<Segment> filteredNormalSegments =
0:             getFilteredNormalSegments(job, segments.getValidSegments(), getSegmentsToAccess(job));
1:         streamSegments = segments.getStreamSegments();
1:         if (filteredNormalSegments.size() == 0) {
0:           return getSplitsOfStreaming(job, identifier, streamSegments);
0:         }
1:         setSegmentsToAccess(job.getConfiguration(),filteredNormalSegments);
/////////////////////////////////////////////////////////////////////////
1:     List<Segment> validAndInProgressSegments = new ArrayList<>(segments.getValidSegments());
/////////////////////////////////////////////////////////////////////////
1:    * Below method will be used to get the filter segments when query is fired on pre Aggregate
1:    * and main table in case of streaming.
1:    * For Pre Aggregate rules it will set all the valid segments for both streaming and
1:    * and normal for fact table, so if any handoff happened in between it will
1:    * select only new hand off segments segments for fact.
1:    * @param job
1:    * @param validSegments
1:    * @param segmentsToAccess
1:    * @return
0:    */
1:   private List<Segment> getFilteredNormalSegments(JobContext job, List<Segment> validSegments,
1:       Segment[] segmentsToAccess) {
1:     List<Segment> segmentToAccessSet = Arrays.asList(segmentsToAccess);
1:     List<Segment> filteredSegment = new ArrayList<>();
1:     for (Segment seg : validSegments) {
1:       if (!segmentToAccessSet.contains(seg)) {
1:         filteredSegment.add(seg);
0:       }
0:     }
1:     return filteredSegment;
0:   }
0: 
0:   /**
author:Jacky Li
-------------------------------------------------------------------------------
commit:7e0803f
/////////////////////////////////////////////////////////////////////////
0:     SegmentUpdateStatusManager updateStatusManager =
1:         new SegmentUpdateStatusManager(carbonTable, loadMetadataDetails);
/////////////////////////////////////////////////////////////////////////
0:           DataMapStoreManager.getInstance().getTableSegmentRefresher(carbonTable)
/////////////////////////////////////////////////////////////////////////
0:       if (DataMapStoreManager.getInstance().getTableSegmentRefresher(carbonTable)
/////////////////////////////////////////////////////////////////////////
1:       List<Integer> oldPartitionIdList, PartitionInfo partitionInfo) {
/////////////////////////////////////////////////////////////////////////
/////////////////////////////////////////////////////////////////////////
1:           partitionInfo, oldPartitionIdList, new SegmentUpdateStatusManager(carbonTable));
/////////////////////////////////////////////////////////////////////////
0:         getDataBlocksOfSegment(job, carbonTable, filterResolver, matchedPartitions,
/////////////////////////////////////////////////////////////////////////
1:   public BlockMappingVO getBlockRowCount(Job job, CarbonTable table,
1:     AbsoluteTableIdentifier identifier = table.getAbsoluteTableIdentifier();
1:     TableDataMap blockletMap = DataMapStoreManager.getInstance().getDefaultDataMap(table);
1:         table, loadMetadataDetails);
commit:df5d7a9
/////////////////////////////////////////////////////////////////////////
1: import org.apache.carbondata.core.metadata.schema.SchemaReader;
/////////////////////////////////////////////////////////////////////////
/////////////////////////////////////////////////////////////////////////
1:     carbonTable.processFilterExpression(filter, null, null);
/////////////////////////////////////////////////////////////////////////
0:     FilterResolverIntf filterInterface = carbonTable.resolveFilter(filter, tableProvider);
/////////////////////////////////////////////////////////////////////////
0:       carbonTable.processFilterExpression(filter, null, null);
/////////////////////////////////////////////////////////////////////////
0:       FilterResolverIntf filterInterface = carbonTable.resolveFilter(filter, tableProvider);
commit:fc2a7eb
/////////////////////////////////////////////////////////////////////////
0: import org.apache.carbondata.core.datamap.DataMapLevel;
/////////////////////////////////////////////////////////////////////////
0: import org.apache.carbondata.core.indexstore.blockletindex.BlockletDataMapFactory;
/////////////////////////////////////////////////////////////////////////
0:     if (distributedCG || dataMapExprWrapper.getDataMapType() == DataMapLevel.FG) {
0:               BlockletDataMapFactory.class.getName());
commit:89a12af
/////////////////////////////////////////////////////////////////////////
0: import org.apache.carbondata.core.indexstore.blockletindex.BlockletIndexDataMapFactory;
/////////////////////////////////////////////////////////////////////////
0:               BlockletIndexDataMapFactory.class.getName());
commit:bf6c471
/////////////////////////////////////////////////////////////////////////
/////////////////////////////////////////////////////////////////////////
/////////////////////////////////////////////////////////////////////////
0:         String segmentDir = CarbonTablePath.getSegmentPath(identifier.getTablePath(), segment.getSegmentNo());
commit:daa6465
/////////////////////////////////////////////////////////////////////////
/////////////////////////////////////////////////////////////////////////
0:   private static final String VALIDATE_INPUT_SEGMENT_IDs =
1:   private static final String ALTER_PARTITION_ID = "mapreduce.input.carboninputformat.partitionid";
/////////////////////////////////////////////////////////////////////////
0:   private static final String PARTITIONS_TO_PRUNE =
/////////////////////////////////////////////////////////////////////////
1:   @Override
1:   public List<InputSplit> getSplits(JobContext job) throws IOException {
/////////////////////////////////////////////////////////////////////////
0:   @Override
0:   public RecordReader<Void, T> createRecordReader(InputSplit inputSplit,
0:     QueryModel queryModel = createQueryModel(inputSplit, taskAttemptContext);
0:   public QueryModel createQueryModel(InputSplit inputSplit, TaskAttemptContext taskAttemptContext)
0:     String projectionString = getColumnProjection(configuration);
0:     String[] projectionColumnNames = null;
0:     if (projectionString != null) {
0:       projectionColumnNames = projectionString.split(",");
0:     }
0:     QueryModel queryModel = carbonTable.createQueryWithProjection(
0:         projectionColumnNames, getDataTypeConverter(configuration));
/////////////////////////////////////////////////////////////////////////
0:   @Override
0:   protected boolean isSplitable(JobContext context, Path filename) {
/////////////////////////////////////////////////////////////////////////
commit:5bedd77
/////////////////////////////////////////////////////////////////////////
0:         String segmentDir = tablePath.getSegmentDir(segment.getSegmentNo());
commit:a324e5d
/////////////////////////////////////////////////////////////////////////
0:   private static TableInfo getTableInfo(Configuration configuration) throws IOException {
/////////////////////////////////////////////////////////////////////////
0:   private CarbonTable getOrCreateCarbonTable(Configuration configuration) throws IOException {
/////////////////////////////////////////////////////////////////////////
0:   public static void setTablePath(Configuration configuration, String tablePath) {
/////////////////////////////////////////////////////////////////////////
/////////////////////////////////////////////////////////////////////////
1:    * Configurations FileInputFormat.INPUT_DIR, CarbonTableInputFormat.INPUT_SEGMENT_NUMBERS
commit:c16d543
/////////////////////////////////////////////////////////////////////////
0: import org.apache.carbondata.core.util.CarbonProperties;
/////////////////////////////////////////////////////////////////////////
1: import org.apache.hadoop.mapreduce.Job;
/////////////////////////////////////////////////////////////////////////
0:    * Set `CARBON_INPUT_SEGMENTS` from property to configuration
0:    */
0:   public static void setQuerySegment(Configuration conf, AbsoluteTableIdentifier identifier) {
0:     String dbName = identifier.getCarbonTableIdentifier().getDatabaseName().toLowerCase();
0:     String tbName = identifier.getCarbonTableIdentifier().getTableName().toLowerCase();
0:     String segmentNumbersFromProperty = CarbonProperties.getInstance()
0:         .getProperty(CarbonCommonConstants.CARBON_INPUT_SEGMENTS + dbName + "." + tbName, "*");
0:     if (!segmentNumbersFromProperty.trim().equals("*")) {
0:       CarbonTableInputFormat
0:           .setSegmentsToAccess(conf, Arrays.asList(segmentNumbersFromProperty.split(",")));
0:     }
0:   }
0: 
0:   /**
/////////////////////////////////////////////////////////////////////////
0:     List<String> filteredSegmentToAccess = null;
/////////////////////////////////////////////////////////////////////////
0: 
0:       filteredSegmentToAccess = getFilteredSegment(job, validSegments);
/////////////////////////////////////////////////////////////////////////
1:    * Return segment list after filtering out valid segments and segments set by user by
1:    * `INPUT_SEGMENT_NUMBERS` in job configuration
0:    */
0:   private List<String> getFilteredSegment(JobContext job, List<String> validSegments) {
0:     String[] segmentsToAccess = getSegmentsToAccess(job);
0:     Set<String> segmentToAccessSet = new HashSet<>();
0:     segmentToAccessSet.addAll(Arrays.asList(segmentsToAccess));
0:     List<String> filteredSegmentToAccess = new ArrayList<>();
0:     if (segmentsToAccess.length == 0 || segmentsToAccess[0].equalsIgnoreCase("*")) {
0:       filteredSegmentToAccess.addAll(validSegments);
0:     } else {
0:       for (String validSegment : validSegments) {
0:         if (segmentToAccessSet.contains(validSegment)) {
0:           filteredSegmentToAccess.add(validSegment);
0:         }
0:       }
0:       if (!filteredSegmentToAccess.containsAll(segmentToAccessSet)) {
0:         List<String> filteredSegmentToAccessTemp = new ArrayList<>();
0:         filteredSegmentToAccessTemp.addAll(filteredSegmentToAccess);
0:         filteredSegmentToAccessTemp.removeAll(segmentToAccessSet);
0:         LOG.info(
0:             "Segments ignored are : " + Arrays.toString(filteredSegmentToAccessTemp.toArray()));
0:       }
0:     }
1:     return filteredSegmentToAccess;
0:   }
0: 
0:   /**
/////////////////////////////////////////////////////////////////////////
0:   public BlockMappingVO getBlockRowCount(Job job, AbsoluteTableIdentifier identifier)
1:     SegmentStatusManager.ValidAndInvalidSegmentsInfo allSegments =
1:     Map<String, Long> blockRowCountMapping = new HashMap<>();
1:     Map<String, Long> segmentAndBlockCountMapping = new HashMap<>();
0: 
1:     // TODO: currently only batch segment is supported, add support for streaming table
0:     List<String> filteredSegment = getFilteredSegment(job, allSegments.getValidSegments());
0: 
0:     List<ExtendedBlocklet> blocklets = blockletMap.prune(filteredSegment, null);
/////////////////////////////////////////////////////////////////////////
0: 
commit:ee71610
/////////////////////////////////////////////////////////////////////////
0:                         blkLocations[blkIndex].getCachedHosts(), FileFormat.ROW_V1));
0:                         blkLocations[blkIndex].getCachedHosts(), FileFormat.ROW_V1));
0:                       FileFormat.ROW_V1));
commit:5fc7f06
/////////////////////////////////////////////////////////////////////////
1:     PartitionInfo partitionInfo = carbonTable.getPartitionInfo(carbonTable.getTableName());
/////////////////////////////////////////////////////////////////////////
0:         new boolean[carbonTable.getNumberOfMeasures(carbonTable.getTableName())];
commit:2f0959a
/////////////////////////////////////////////////////////////////////////
0:       segmentToAccessSet.addAll(Arrays.asList(segmentsToAccess));
0: 
/////////////////////////////////////////////////////////////////////////
1:       if (null == details || !CarbonUpdateUtil.isBlockInvalid(details.getSegmentStatus())) {
commit:f089287
/////////////////////////////////////////////////////////////////////////
1: import org.apache.carbondata.core.datamap.DataMapStoreManager;
1: import org.apache.carbondata.core.datamap.TableDataMap;
0: import org.apache.carbondata.core.indexstore.blockletindex.BlockletDataMap;
0: import org.apache.carbondata.core.indexstore.blockletindex.BlockletDataMapFactory;
/////////////////////////////////////////////////////////////////////////
0:         DataMapStoreManager.getInstance().getDataMap(identifier, BlockletDataMap.NAME,
0:             BlockletDataMapFactory.class);
/////////////////////////////////////////////////////////////////////////
0:         .getDataMap(absoluteTableIdentifier, BlockletDataMap.NAME, BlockletDataMapFactory.class);
/////////////////////////////////////////////////////////////////////////
0:     TableDataMap blockletMap = DataMapStoreManager.getInstance()
0:         .getDataMap(identifier, BlockletDataMap.NAME, BlockletDataMapFactory.class);
author:Ajantha-Bhat
-------------------------------------------------------------------------------
commit:c09ef99
/////////////////////////////////////////////////////////////////////////
/////////////////////////////////////////////////////////////////////////
/////////////////////////////////////////////////////////////////////////
/////////////////////////////////////////////////////////////////////////
1:  * InputFormat for reading carbondata files with table level metadata support,
1:  * such as segment and explicit schema metadata.
1: public class CarbonTableInputFormat<T> extends CarbonInputFormat<T> {
0:   protected CarbonTable getOrCreateCarbonTable(Configuration configuration) throws IOException {
/////////////////////////////////////////////////////////////////////////
/////////////////////////////////////////////////////////////////////////
/////////////////////////////////////////////////////////////////////////
/////////////////////////////////////////////////////////////////////////
author:BJangir
-------------------------------------------------------------------------------
commit:65b69a9
/////////////////////////////////////////////////////////////////////////
0:       streamSegments = getFilteredSegment(job,streamSegments);
/////////////////////////////////////////////////////////////////////////
0:         return getSplitsOfStreaming(job, identifier, streamSegments);
author:xuchuanyin
-------------------------------------------------------------------------------
commit:f9291cd
/////////////////////////////////////////////////////////////////////////
1:     SegmentUpdateStatusManager updateStatusManager = new SegmentUpdateStatusManager(
0:         identifier, loadMetadataDetails);
commit:859d71c
/////////////////////////////////////////////////////////////////////////
0: import org.apache.carbondata.core.datamap.Segment;
/////////////////////////////////////////////////////////////////////////
0:         String segmentDir = CarbonTablePath.getSegmentPath(
0:             identifier.getTablePath(), segment.getSegmentNo());
commit:910d496
/////////////////////////////////////////////////////////////////////////
0:       return (DataMapJob) ObjectSerializationUtil.convertStringToObject(jobString);
/////////////////////////////////////////////////////////////////////////
0:     Set<String> segmentToAccessSet = new HashSet<>(Arrays.asList(segmentsToAccess));
/////////////////////////////////////////////////////////////////////////
0:         List<String> filteredSegmentToAccessTemp = new ArrayList<>(filteredSegmentToAccess);
author:Ravindra Pesala
-------------------------------------------------------------------------------
commit:56330ae
/////////////////////////////////////////////////////////////////////////
0: import org.apache.carbondata.core.datamap.DataMapChooser;
0: import org.apache.carbondata.core.datamap.dev.expr.DataMapExprWrapper;
/////////////////////////////////////////////////////////////////////////
/////////////////////////////////////////////////////////////////////////
1:         DataMapStoreManager.getInstance()
1:             .clearInvalidSegments(getOrCreateCarbonTable(job.getConfiguration()), invalidSegments);
/////////////////////////////////////////////////////////////////////////
0:     if (toBeCleanedSegments.size() > 0) {
0:       DataMapStoreManager.getInstance()
0:           .clearInvalidSegments(getOrCreateCarbonTable(job.getConfiguration()),
0:               toBeCleanedSegments);
0:     }
/////////////////////////////////////////////////////////////////////////
0:     DataMapExprWrapper dataMapExprWrapper =
0:         DataMapChooser.get().choose(getOrCreateCarbonTable(job.getConfiguration()), resolver);
0:     if (distributedCG || dataMapExprWrapper.getDataMapType() == DataMapType.FG) {
0:           new DistributableDataMapFormat(absoluteTableIdentifier, dataMapExprWrapper,
0:       // Apply expression on the blocklets.
0:       prunedBlocklets = dataMapExprWrapper.pruneBlocklets(prunedBlocklets);
0:       prunedBlocklets = dataMapExprWrapper.prune(segmentIds, partitionsToPrune);
/////////////////////////////////////////////////////////////////////////
0:     TableDataMap blockletMap = DataMapStoreManager.getInstance().getDefaultDataMap(identifier);
0:     SegmentUpdateStatusManager updateStatusManager = new SegmentUpdateStatusManager(identifier, loadMetadataDetails);
commit:28f78b2
/////////////////////////////////////////////////////////////////////////
0: import java.util.ArrayList;
0: import java.util.Arrays;
0: import java.util.BitSet;
0: import java.util.HashMap;
1: import java.util.LinkedList;
1: import java.util.List;
0: import java.util.Map;
1: import org.apache.carbondata.core.indexstore.ExtendedBlocklet;
/////////////////////////////////////////////////////////////////////////
0:     List<ExtendedBlocklet> prunedBlocklets;
/////////////////////////////////////////////////////////////////////////
0:     for (ExtendedBlocklet blocklet : prunedBlocklets) {
/////////////////////////////////////////////////////////////////////////
0:   private CarbonInputSplit convertToCarbonInputSplit(ExtendedBlocklet blocklet)
/////////////////////////////////////////////////////////////////////////
0:     List<ExtendedBlocklet> blocklets =
0:         blockletMap.prune(validAndInvalidSegments.getValidSegments(), null);
1:     for (ExtendedBlocklet blocklet : blocklets) {
1:       String blockName = blocklet.getPath();
commit:eb771f5
/////////////////////////////////////////////////////////////////////////
0:     SegmentUpdateStatusManager updateStatusManager = new SegmentUpdateStatusManager(identifier);
/////////////////////////////////////////////////////////////////////////
1:     // Clean the updated segments from memory if the update happens on segments
0:     List<String> toBeCleanedSegments = new ArrayList<>();
0:     for (SegmentUpdateDetails segmentUpdateDetail : updateStatusManager
0:         .getUpdateStatusDetails()) {
0:       boolean refreshNeeded =
0:           DataMapStoreManager.getInstance().getTableSegmentRefresher(identifier)
0:               .isRefreshNeeded(segmentUpdateDetail.getSegmentName(), updateStatusManager);
0:       if (refreshNeeded) {
0:         toBeCleanedSegments.add(segmentUpdateDetail.getSegmentName());
0:       }
0:     }
0:     // Clean segments if refresh is needed
0:     for (String segment : validSegments) {
0:       if (DataMapStoreManager.getInstance().getTableSegmentRefresher(identifier)
0:           .isRefreshNeeded(segment)) {
0:         toBeCleanedSegments.add(segment);
0:       }
0:     }
0:     blockletMap.clear(toBeCleanedSegments);
0: 
commit:1e21cd1
/////////////////////////////////////////////////////////////////////////
0: import java.util.*;
/////////////////////////////////////////////////////////////////////////
0:   private static final String DATA_MAP_DSTR = "mapreduce.input.carboninputformat.datamapdstr";
/////////////////////////////////////////////////////////////////////////
0: 
0:   public static void setDataMapJob(Configuration configuration, DataMapJob dataMapJob)
0:       throws IOException {
0:     if (dataMapJob != null) {
0:       String toString = ObjectSerializationUtil.convertObjectToString(dataMapJob);
0:       configuration.set(DATA_MAP_DSTR, toString);
0:     }
0:   }
0: 
0:   private static DataMapJob getDataMapJob(Configuration configuration) throws IOException {
0:     String jobString = configuration.get(DATA_MAP_DSTR);
0:     if (jobString != null) {
0:       DataMapJob dataMapJob = (DataMapJob) ObjectSerializationUtil.convertStringToObject(jobString);
0:       return dataMapJob;
0:     }
0:     return null;
0:   }
0: 
/////////////////////////////////////////////////////////////////////////
0:             BlockletDataMapFactory.class.getName());
/////////////////////////////////////////////////////////////////////////
0:         .getDataMap(absoluteTableIdentifier, BlockletDataMap.NAME,
0:             BlockletDataMapFactory.class.getName());
0:     DataMapJob dataMapJob = getDataMapJob(job.getConfiguration());
0:     List<Blocklet> prunedBlocklets;
0:     if (dataMapJob != null) {
0:       DistributableDataMapFormat datamapDstr =
0:           new DistributableDataMapFormat(absoluteTableIdentifier, BlockletDataMap.NAME,
0:               segmentIds, BlockletDataMapFactory.class.getName());
0:       prunedBlocklets = dataMapJob.execute(datamapDstr, resolver);
0:     } else {
0:       prunedBlocklets = blockletMap.prune(segmentIds, resolver);
0:     }
/////////////////////////////////////////////////////////////////////////
0:             new FileSplit(new Path(blocklet.getPath()), 0, blocklet.getLength(),
0:                 blocklet.getLocations()),
/////////////////////////////////////////////////////////////////////////
0:         .getDataMap(identifier, BlockletDataMap.NAME, BlockletDataMapFactory.class.getName());
author:anubhav100
-------------------------------------------------------------------------------
commit:0c8fa59
/////////////////////////////////////////////////////////////////////////
0:       "mapreduce.input.carboninputformat.validsegments";
/////////////////////////////////////////////////////////////////////////
0:               new ByteArrayInputStream(ObjectSerializationUtil.decodeStringToBytes(tableInfoStr))));
/////////////////////////////////////////////////////////////////////////
0:             .isInvalidTableBlock(inputSplit.getSegmentId(), inputSplit.getPath().toString(),
0:                 invalidBlockVOForSegmentId, updateStatusManager)) {
0:               updateStatusManager.getDeleteDeltaFilePath(inputSplit.getPath().toString());
/////////////////////////////////////////////////////////////////////////
0:             blocklet.getBlockletId(), new FileSplit(new Path(blocklet.getPath()), 0,
0:                 blocklet.getLength(), blocklet.getLocations()),
author:Jatin
-------------------------------------------------------------------------------
commit:64b2651
/////////////////////////////////////////////////////////////////////////
0:     // getAllMeasures returns list of visible and invisible columns
0:         new boolean[carbonTable.getAllMeasures().size()];
author:manishgupta88
-------------------------------------------------------------------------------
commit:2bad144
/////////////////////////////////////////////////////////////////////////
0:   public CarbonTable getOrCreateCarbonTable(Configuration configuration) throws IOException {
/////////////////////////////////////////////////////////////////////////
0:   public AbsoluteTableIdentifier getAbsoluteTableIdentifier(Configuration configuration)
/////////////////////////////////////////////////////////////////////////
0:   public String[] getSegmentsToAccess(JobContext job) {
author:lionelcao
-------------------------------------------------------------------------------
commit:cb51b86
/////////////////////////////////////////////////////////////////////////
1:       matchedPartitions = setMatchedPartitions(null, filter, partitionInfo, null);
/////////////////////////////////////////////////////////////////////////
1:       // matchedPartitions records partitionIndex, not partitionId
1:         matchedPartitions =
1:             setMatchedPartitions(partitionIds, filter, partitionInfo, oldPartitionIdList);
/////////////////////////////////////////////////////////////////////////
0:   /**
1:    * set the matched partition indices into a BitSet
1:    * @param partitionIds  from alter table command, for normal query, it's null
1:    * @param filter   from query
1:    * @param partitionInfo
1:    * @param oldPartitionIdList  only used in alter table command
1:    * @return
0:    */
1:       PartitionInfo partitionInfo, List<Integer> oldPartitionIdList) {
1:       // partList[0] -> use the first element to initiate BitSet, will auto expand later
1:       matchedPartitions = new BitSet(Integer.parseInt(partList[0].trim()));
1:         Integer index = oldPartitionIdList.indexOf(Integer.parseInt(partitionId.trim()));
1:         matchedPartitions.set(index);
commit:874764f
/////////////////////////////////////////////////////////////////////////
/////////////////////////////////////////////////////////////////////////
1: import org.apache.carbondata.hadoop.CarbonInputSplit;
/////////////////////////////////////////////////////////////////////////
0:   public static final String ALTER_PARTITION_ID = "mapreduce.input.carboninputformat.partitionid";
/////////////////////////////////////////////////////////////////////////
0: 
0:   public static void setPartitionIdList(Configuration configuration, List<String> partitionIds) {
0:     configuration.set(ALTER_PARTITION_ID, partitionIds.toString());
0:   }
0: 
/////////////////////////////////////////////////////////////////////////
0:     PartitionInfo partitionInfo = carbonTable.getPartitionInfo(carbonTable.getFactTableName());
1:     if (partitionInfo != null) {
0:       matchedPartitions = setMatchedPartitions(null, filter, partitionInfo);
1:       if (matchedPartitions != null) {
1:         } else if (matchedPartitions.cardinality() == partitionInfo.getNumPartitions()) {
/////////////////////////////////////////////////////////////////////////
1:     List<InputSplit> splits =
0:         getSplits(job, filterInterface, validSegments, matchedPartitions, partitionInfo, null);
/////////////////////////////////////////////////////////////////////////
1:    * Read data in one segment. For alter table partition statement
0:    * @param job
1:    * @param targetSegment
1:    * @param oldPartitionIdList  get old partitionId before partitionInfo was changed
0:    * @return
0:    * @throws IOException
0:    */
1:   public List<InputSplit> getSplitsOfOneSegment(JobContext job, String targetSegment,
0:       List<Integer> oldPartitionIdList, PartitionInfo partitionInfo)
0:       throws IOException {
0:     AbsoluteTableIdentifier identifier = getAbsoluteTableIdentifier(job.getConfiguration());
0:     List<String> invalidSegments = new ArrayList<>();
1:     List<UpdateVO> invalidTimestampsList = new ArrayList<>();
0: 
0:     List<String> segmentList = new ArrayList<>();
0:     segmentList.add(targetSegment);
0:     setSegmentsToAccess(job.getConfiguration(), segmentList);
0:     try {
0: 
1:       // process and resolve the expression
1:       Expression filter = getFilterPredicates(job.getConfiguration());
1:       CarbonTable carbonTable = getOrCreateCarbonTable(job.getConfiguration());
1:       // this will be null in case of corrupt schema file.
0:       if (null == carbonTable) {
0:         throw new IOException("Missing/Corrupt schema file for table.");
0:       }
0: 
0:       CarbonInputFormatUtil.processFilterExpression(filter, carbonTable);
0: 
1:       // prune partitions for filter query on partition table
1:       String partitionIds = job.getConfiguration().get(ALTER_PARTITION_ID);
1:       BitSet matchedPartitions = null;
0:       if (partitionInfo != null) {
0:         matchedPartitions = setMatchedPartitions(partitionIds, filter, partitionInfo);
1:         if (matchedPartitions != null) {
1:           if (matchedPartitions.cardinality() == 0) {
1:             return new ArrayList<InputSplit>();
1:           } else if (matchedPartitions.cardinality() == partitionInfo.getNumPartitions()) {
1:             matchedPartitions = null;
0:           }
0:         }
0:       }
0: 
0:       FilterResolverIntf filterInterface = CarbonInputFormatUtil.resolveFilter(filter, identifier);
1:       // do block filtering and get split
0:       List<InputSplit> splits = getSplits(job, filterInterface, segmentList, matchedPartitions,
0:           partitionInfo, oldPartitionIdList);
1:       // pass the invalid segment to task side in order to remove index entry in task side
0:       if (invalidSegments.size() > 0) {
1:         for (InputSplit split : splits) {
1:           ((CarbonInputSplit) split).setInvalidSegments(invalidSegments);
1:           ((CarbonInputSplit) split).setInvalidTimestampRange(invalidTimestampsList);
0:         }
0:       }
1:       return splits;
0:     } catch (IOException e) {
1:       throw new RuntimeException("Can't get splits of the target segment ", e);
0:     }
0:   }
0: 
1:   private BitSet setMatchedPartitions(String partitionIds, Expression filter,
0:       PartitionInfo partitionInfo) {
1:     BitSet matchedPartitions = null;
1:     if (null != partitionIds) {
1:       String[] partList = partitionIds.replace("[", "").replace("]", "").split(",");
0:       // only one partitionId in current alter table statement
0:       matchedPartitions = new BitSet(Integer.parseInt(partList[0]));
1:       for (String partitionId : partList) {
0:         matchedPartitions.set(Integer.parseInt(partitionId));
0:       }
0:     } else {
0:       if (null != filter) {
1:         matchedPartitions =
1:             new FilterExpressionProcessor().getFilteredPartitions(filter, partitionInfo);
0:       }
0:     }
1:     return matchedPartitions;
0:   }
0:   /**
/////////////////////////////////////////////////////////////////////////
0:       List<String> validSegments, BitSet matchedPartitions, PartitionInfo partitionInfo,
0:       List<Integer> oldPartitionIdList) throws IOException {
/////////////////////////////////////////////////////////////////////////
1:     // for each segment fetch blocks matching filter in Driver BTree
0:             validSegments, partitionInfo, oldPartitionIdList);
/////////////////////////////////////////////////////////////////////////
0:       BitSet matchedPartitions, List<String> segmentIds, PartitionInfo partitionInfo,
0:       List<Integer> oldPartitionIdList) throws IOException {
/////////////////////////////////////////////////////////////////////////
0:     int partitionIndex = 0;
0:     List<Integer> partitionIdList = new ArrayList<>();
0:     if (partitionInfo != null) {
0:       partitionIdList = partitionInfo.getPartitionIds();
0:     }
0:       int partitionId = CarbonTablePath.DataFileUtil.getTaskIdFromTaskNo(
0:       // OldPartitionIdList is only used in alter table partition command because it change
0:       // partition info first and then read data.
0:       // For other normal query should use newest partitionIdList
0:       if (partitionInfo != null) {
0:         if (oldPartitionIdList != null) {
0:           partitionIndex = oldPartitionIdList.indexOf(partitionId);
0:         } else {
0:           partitionIndex = partitionIdList.indexOf(partitionId);
0:         }
0:       }
0:       if (partitionIndex != -1) {
0:         // matchedPartitions variable will be null in two cases as follows
0:         // 1. the table is not a partition table
0:         // 2. the table is a partition table, and all partitions are matched by query
0:         // for partition table, the task id of carbaondata file name is the partition id.
0:         // if this partition is not required, here will skip it.
0:         if (matchedPartitions == null || matchedPartitions.get(partitionIndex)) {
0:           resultFilterredBlocks.add(convertToCarbonInputSplit(blocklet));
0:         }
author:kushalsaha
-------------------------------------------------------------------------------
commit:940f4d5
/////////////////////////////////////////////////////////////////////////
0:       String[] deleteDeltaFilePath = null;
0:                 .isInvalidTableBlock(inputSplit.getSegmentId(), inputSplit.getPath().toString(),
0:                         invalidBlockVOForSegmentId, updateStatusManager)) {
1:         // When iud is done then only get delete delta files for a block
0:         try {
0:           deleteDeltaFilePath =
0:                   updateStatusManager.getDeleteDeltaFilePath(inputSplit.getPath().toString());
0:         } catch (Exception e) {
0:           throw new IOException(e);
0:         }
author:Raghunandan S
-------------------------------------------------------------------------------
commit:1be0778
/////////////////////////////////////////////////////////////////////////
/////////////////////////////////////////////////////////////////////////
commit:79feac9
/////////////////////////////////////////////////////////////////////////
1: import org.apache.carbondata.core.metadata.schema.table.TableInfo;
/////////////////////////////////////////////////////////////////////////
0:   private static final String TABLE_INFO = "mapreduce.input.carboninputformat.tableinfo";
1:   // a cache for carbon table, it will be used in task side
1:   private CarbonTable carbonTable;
0: 
0:    * Set the `tableInfo` in `configuration`
0:   public static void setTableInfo(Configuration configuration, TableInfo tableInfo)
0:     if (null != tableInfo) {
0:       configuration.set(TABLE_INFO, ObjectSerializationUtil.convertObjectToString(tableInfo));
0:    * Get TableInfo object from `configuration`
0:   private TableInfo getTableInfo(Configuration configuration) throws IOException {
0:     String tableInfoStr = configuration.get(TABLE_INFO);
0:     return (TableInfo) ObjectSerializationUtil.convertStringToObject(tableInfoStr);
0:   }
0: 
0:   /**
1:    * Get the cached CarbonTable or create it by TableInfo in `configuration`
0:    */
0:   private CarbonTable getOrCreateCarbonTable(Configuration configuration) throws IOException {
1:     if (carbonTable == null) {
1:       // carbon table should be created either from deserialized table info (schema saved in
1:       // hive metastore) or by reading schema in HDFS (schema saved in HDFS)
1:       TableInfo tableInfo = getTableInfo(configuration);
1:       CarbonTable carbonTable;
1:       if (tableInfo != null) {
1:         carbonTable = CarbonTable.buildFromTableInfo(tableInfo);
0:       } else {
1:         carbonTable = SchemaReader.readCarbonTableFromStore(
1:             getAbsoluteTableIdentifier(configuration));
0:       }
1:       this.carbonTable = carbonTable;
1:       return carbonTable;
0:     } else {
1:       return this.carbonTable;
/////////////////////////////////////////////////////////////////////////
0:   private AbsoluteTableIdentifier getAbsoluteTableIdentifier(Configuration configuration)
0:     String dirs = configuration.get(INPUT_DIR, "");
0:     String[] inputPaths = StringUtils.split(dirs);
0:     if (inputPaths.length == 0) {
0:       throw new InvalidPathException("No input paths specified in job");
0:     }
0:     return AbsoluteTableIdentifier.fromTablePath(inputPaths[0]);
/////////////////////////////////////////////////////////////////////////
0:     CarbonTable carbonTable = getOrCreateCarbonTable(job.getConfiguration());
/////////////////////////////////////////////////////////////////////////
0:             .getFilteredPartitions(filter, partitionInfo);
/////////////////////////////////////////////////////////////////////////
0:         getOrCreateCarbonTable(job.getConfiguration()).getAbsoluteTableIdentifier();
/////////////////////////////////////////////////////////////////////////
0:     CarbonTable carbonTable = getOrCreateCarbonTable(configuration);
author:cenyuhai
-------------------------------------------------------------------------------
commit:dd809ed
/////////////////////////////////////////////////////////////////////////
0: import java.io.ByteArrayInputStream;
0: import java.io.DataInputStream;
/////////////////////////////////////////////////////////////////////////
0:       configuration.set(TABLE_INFO, ObjectSerializationUtil.encodeToString(tableInfo.serialize()));
/////////////////////////////////////////////////////////////////////////
0:     if (tableInfoStr == null) {
0:       return null;
0:     } else {
0:       TableInfo output = new TableInfo();
0:       output.readFields(
0:           new DataInputStream(
0:             new ByteArrayInputStream(ObjectSerializationUtil.decodeStringToBytes(tableInfoStr))));
0:       return output;
0:     }
author:chenliang613
-------------------------------------------------------------------------------
commit:09f7cdd
/////////////////////////////////////////////////////////////////////////
0: import org.apache.carbondata.core.util.DataTypeConverter;
0: import org.apache.carbondata.core.util.DataTypeConverterImpl;
/////////////////////////////////////////////////////////////////////////
1:   private static final String CARBON_CONVERTER = "mapreduce.input.carboninputformat.converter";
/////////////////////////////////////////////////////////////////////////
0:     QueryModel queryModel = QueryModel.createModel(identifier, queryPlan, carbonTable,
0:         getDataTypeConverter(configuration));
/////////////////////////////////////////////////////////////////////////
0: 
0:   /**
0:    * It is optional, if user does not set then it reads from store
0:    *
0:    * @param configuration
0:    * @param converter is the Data type converter for different computing engine
0:    * @throws IOException
0:    */
0:   public static void setDataTypeConverter(Configuration configuration, DataTypeConverter converter)
0:       throws IOException {
0:     if (null != converter) {
0:       configuration.set(CARBON_CONVERTER,
0:           ObjectSerializationUtil.convertObjectToString(converter));
0:     }
0:   }
0: 
0:   public static DataTypeConverter getDataTypeConverter(Configuration configuration)
0:       throws IOException {
0:     String converter = configuration.get(CARBON_CONVERTER);
0:     if (converter == null) {
0:       return new DataTypeConverterImpl();
0:     }
0:     return (DataTypeConverter) ObjectSerializationUtil.convertStringToObject(converter);
0:   }
author:jackylk
-------------------------------------------------------------------------------
commit:ce09aaa
/////////////////////////////////////////////////////////////////////////
1: import org.apache.carbondata.core.scan.expression.Expression;
1: import org.apache.carbondata.core.scan.filter.resolver.FilterResolverIntf;
/////////////////////////////////////////////////////////////////////////
commit:edb02ab
/////////////////////////////////////////////////////////////////////////
1: /*
0:  * Licensed to the Apache Software Foundation (ASF) under one
0:  * or more contributor license agreements.  See the NOTICE file
0:  * distributed with this work for additional information
0:  * regarding copyright ownership.  The ASF licenses this file
0:  * to you under the Apache License, Version 2.0 (the
0:  * "License"); you may not use this file except in compliance
0:  * with the License.  You may obtain a copy of the License at
0:  *
1:  *    http://www.apache.org/licenses/LICENSE-2.0
0:  *
0:  * Unless required by applicable law or agreed to in writing,
0:  * software distributed under the License is distributed on an
0:  * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
0:  * KIND, either express or implied.  See the License for the
0:  * specific language governing permissions and limitations
0:  * under the License.
0:  */
0: 
1: package org.apache.carbondata.hadoop.api;
0: 
1: import java.io.IOException;
0: import java.util.LinkedList;
0: import java.util.List;
0: 
0: import org.apache.carbondata.hadoop.CarbonProjection;
0: import org.apache.carbondata.hadoop.internal.CarbonInputSplit;
0: import org.apache.carbondata.hadoop.internal.segment.Segment;
0: import org.apache.carbondata.hadoop.internal.segment.SegmentManager;
0: import org.apache.carbondata.hadoop.internal.segment.SegmentManagerFactory;
0: import org.apache.carbondata.hadoop.util.CarbonInputFormatUtil;
0: import org.apache.carbondata.hadoop.util.ObjectSerializationUtil;
0: import org.apache.carbondata.scan.expression.Expression;
0: import org.apache.carbondata.scan.filter.resolver.FilterResolverIntf;
1: import org.apache.hadoop.conf.Configuration;
1: import org.apache.hadoop.mapreduce.InputSplit;
1: import org.apache.hadoop.mapreduce.JobContext;
0: import org.apache.hadoop.mapreduce.RecordReader;
0: import org.apache.hadoop.mapreduce.TaskAttemptContext;
0: import org.apache.hadoop.mapreduce.lib.input.FileInputFormat;
0: 
0: /**
0:  * Input format of CarbonData file.
1:  * @param <T>
0:  */
0: public class CarbonTableInputFormat<T> extends FileInputFormat<Void, T> {
0: 
0:   private static final String FILTER_PREDICATE =
0:       "mapreduce.input.carboninputformat.filter.predicate";
0: 
0:   private SegmentManager segmentManager;
0: 
0:   public CarbonTableInputFormat() {
0:     this.segmentManager = SegmentManagerFactory.getGlobalSegmentManager();
0:   }
0: 
0:   @Override
0:   public RecordReader<Void, T> createRecordReader(InputSplit split,
0:       TaskAttemptContext context) throws IOException, InterruptedException {
0:     switch (((CarbonInputSplit)split).formatType()) {
0:       case COLUMNR:
0:         // TODO: create record reader for columnar format
0:         break;
0:       default:
0:         throw new RuntimeException("Unsupported format type");
0:     }
0:     return null;
0:   }
0: 
0:   @Override
0:   public List<InputSplit> getSplits(JobContext job) throws IOException {
0: 
0:     // work as following steps:
0:     // get all current valid segment
0:     // for each segment, get all input split
0: 
0:     List<InputSplit> output = new LinkedList<>();
0:     Expression filter = getFilter(job.getConfiguration());
0:     Segment[] segments = segmentManager.getAllValidSegments();
0:     FilterResolverIntf filterResolver = CarbonInputFormatUtil.resolveFilter(filter, null);
0:     for (Segment segment: segments) {
0:       List<InputSplit> splits = segment.getSplits(job, filterResolver);
0:       output.addAll(splits);
0:     }
0:     return output;
0:   }
0: 
0:   /**
0:    * set the table path into configuration
0:    * @param conf configuration of the job
0:    * @param tablePath table path string
0:    */
0:   public void setTablePath(Configuration conf, String tablePath) {
0: 
0:   }
0: 
0:   /**
0:    * return the table path in the configuration
0:    * @param conf configuration of the job
0:    * @return table path string
0:    */
0:   public String getTablePath(Configuration conf) {
0:     return null;
0:   }
0: 
0:   /**
0:    * set projection columns into configuration
0:    * @param conf configuration of the job
0:    * @param projection projection
0:    */
0:   public void setProjection(Configuration conf, CarbonProjection projection) {
0: 
0:   }
0: 
0:   /**
0:    * return the projection in the configuration
0:    * @param conf configuration of the job
0:    * @return projection
0:    */
0:   public CarbonProjection getProjection(Configuration conf) {
0:     return null;
0:   }
0: 
0:   /**
0:    * set filter expression into the configuration
0:    * @param conf configuration of the job
0:    * @param filter filter expression
0:    */
0:   public void setFilter(Configuration conf, Expression filter) {
0:     try {
0:       String filterString = ObjectSerializationUtil.convertObjectToString(filter);
0:       conf.set(FILTER_PREDICATE, filterString);
0:     } catch (Exception e) {
0:       throw new RuntimeException("Error while setting filter expression to Job", e);
0:     }
0:   }
0: 
0:   /**
0:    * return filter expression in the configuration
0:    * @param conf configuration of the job
0:    * @return filter expression
0:    */
0:   public Expression getFilter(Configuration conf) {
0:     Object filter;
0:     String filterExprString = conf.get(FILTER_PREDICATE);
0:     if (filterExprString == null) {
0:       return null;
0:     }
0:     try {
0:       filter = ObjectSerializationUtil.convertStringToObject(filterExprString);
0:     } catch (IOException e) {
0:       throw new RuntimeException("Error while reading filter expression", e);
0:     }
0:     assert(filter instanceof Expression);
0:     return (Expression) filter;
0:   }
0: 
0:   /**
0:    * Optional API. It can be used by query optimizer to select index based on filter
0:    * in the configuration of the job. After selecting index internally, index' name will be set
0:    * in the configuration.
0:    *
0:    * The process of selection is simple, just use the default index. Subclass can provide a more
0:    * advanced selection logic like cost based.
0:    * @param conf job configuration
0:    */
0:   public void selectIndex(Configuration conf) {
0:     // set the default index in configuration
0:   }
0: }
============================================================================