1:cd6a4ff: /*
1:41347d8:  * Licensed to the Apache Software Foundation (ASF) under one or more
1:41347d8:  * contributor license agreements.  See the NOTICE file distributed with
1:41347d8:  * this work for additional information regarding copyright ownership.
1:41347d8:  * The ASF licenses this file to You under the Apache License, Version 2.0
1:41347d8:  * (the "License"); you may not use this file except in compliance with
1:41347d8:  * the License.  You may obtain a copy of the License at
3:cd6a4ff:  *
1:cd6a4ff:  *    http://www.apache.org/licenses/LICENSE-2.0
1:cd6a4ff:  *
1:41347d8:  * Unless required by applicable law or agreed to in writing, software
1:41347d8:  * distributed under the License is distributed on an "AS IS" BASIS,
1:41347d8:  * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
1:41347d8:  * See the License for the specific language governing permissions and
1:41347d8:  * limitations under the License.
31:cd6a4ff:  */
51:cd6a4ff: 
1:cd6a4ff: package org.apache.carbondata.processing.store;
1:b13ead9: 
1:cd6a4ff: import java.io.IOException;
1:cd6a4ff: import java.util.ArrayList;
1:cd6a4ff: import java.util.List;
1:cd6a4ff: import java.util.concurrent.Callable;
1:cd6a4ff: import java.util.concurrent.ExecutionException;
1:cd6a4ff: import java.util.concurrent.ExecutorService;
1:cd6a4ff: import java.util.concurrent.Executors;
1:cd6a4ff: import java.util.concurrent.Future;
1:cd6a4ff: import java.util.concurrent.Semaphore;
1:cd6a4ff: import java.util.concurrent.TimeUnit;
1:cd6a4ff: import java.util.concurrent.atomic.AtomicBoolean;
1:cd6a4ff: import java.util.concurrent.atomic.AtomicInteger;
1:b13ead9: 
1:cd6a4ff: import org.apache.carbondata.common.logging.LogService;
1:cd6a4ff: import org.apache.carbondata.common.logging.LogServiceFactory;
1:cd6a4ff: import org.apache.carbondata.core.constants.CarbonCommonConstants;
1:2cf1104: import org.apache.carbondata.core.constants.CarbonV3DataFormatConstants;
1:6f1767b: import org.apache.carbondata.core.datastore.compression.SnappyCompressor;
1:dc83b2a: import org.apache.carbondata.core.datastore.exception.CarbonDataWriterException;
1:dc83b2a: import org.apache.carbondata.core.datastore.row.CarbonRow;
1:6f1767b: import org.apache.carbondata.core.datastore.row.WriteStepRowUtil;
1:cd6a4ff: import org.apache.carbondata.core.keygenerator.KeyGenException;
1:cd6a4ff: import org.apache.carbondata.core.keygenerator.columnar.impl.MultiDimKeyVarLengthEquiSplitGenerator;
1:7359601: import org.apache.carbondata.core.memory.MemoryException;
1:ce09aaa: import org.apache.carbondata.core.metadata.ColumnarFormatVersion;
1:98df130: import org.apache.carbondata.core.metadata.datatype.DataType;
1:956833e: import org.apache.carbondata.core.metadata.datatype.DataTypes;
1:a4083bf: import org.apache.carbondata.core.metadata.schema.table.column.CarbonDimension;
1:cd6a4ff: import org.apache.carbondata.core.util.CarbonProperties;
1:a734add: import org.apache.carbondata.core.util.CarbonThreadFactory;
1:cd6a4ff: import org.apache.carbondata.core.util.CarbonUtil;
1:cd6a4ff: import org.apache.carbondata.processing.datatypes.GenericDataType;
1:cd6a4ff: import org.apache.carbondata.processing.store.writer.CarbonFactDataWriter;
1:cbf8797: 
31:cd6a4ff: /**
1:cd6a4ff:  * Fact data handler class to handle the fact data
1:cd6a4ff:  */
1:cd6a4ff: public class CarbonFactDataHandlerColumnar implements CarbonFactHandler {
1:496cde4: 
1:cd6a4ff:   /**
1:cd6a4ff:    * LOGGER
1:cd6a4ff:    */
1:cd6a4ff:   private static final LogService LOGGER =
1:cd6a4ff:       LogServiceFactory.getLogService(CarbonFactDataHandlerColumnar.class.getName());
1:cd6a4ff: 
1:353272e:   private CarbonFactDataHandlerModel model;
1:cd6a4ff: 
1:cd6a4ff:   /**
1:cd6a4ff:    * data writer
1:cd6a4ff:    */
1:cd6a4ff:   private CarbonFactDataWriter dataWriter;
1:cd6a4ff: 
1:cd6a4ff:   /**
1:cd6a4ff:    * total number of entries in blocklet
1:cd6a4ff:    */
1:cd6a4ff:   private int entryCount;
1:cd6a4ff: 
1:cd6a4ff:   /**
1:98df130:    * blocklet size (for V1 and V2) or page size (for V3). A Producer thread will start to process
1:98df130:    * once this size of input is reached
1:cd6a4ff:    */
1:f089287:   private int pageSize;
1:cd6a4ff: 
1:cd6a4ff:   private long processedDataCount;
1:cd6a4ff:   private ExecutorService producerExecutorService;
1:cd6a4ff:   private List<Future<Void>> producerExecutorServiceTaskList;
1:cd6a4ff:   private ExecutorService consumerExecutorService;
1:cd6a4ff:   private List<Future<Void>> consumerExecutorServiceTaskList;
1:353272e:   private List<CarbonRow> dataRows;
1:6f1767b:   private int[] varcharColumnSizeInByte;
1:cd6a4ff:   /**
1:cd6a4ff:    * semaphore which will used for managing node holder objects
1:cd6a4ff:    */
1:cd6a4ff:   private Semaphore semaphore;
1:cd6a4ff:   /**
1:cd6a4ff:    * counter that incremented for every job submitted to data writer thread
1:cd6a4ff:    */
1:cd6a4ff:   private int writerTaskSequenceCounter;
1:cd6a4ff:   /**
1:cd6a4ff:    * a private class that will hold the data for blocklets
1:cd6a4ff:    */
1:f089287:   private TablePageList tablePageList;
1:cd6a4ff:   /**
1:cd6a4ff:    * number of cores configured
1:cd6a4ff:    */
1:cd6a4ff:   private int numberOfCores;
1:cd6a4ff:   /**
1:cd6a4ff:    * integer that will be incremented for every new blocklet submitted to producer for processing
1:cd6a4ff:    * the data and decremented every time consumer fetches the blocklet for writing
1:cd6a4ff:    */
1:cd6a4ff:   private AtomicInteger blockletProcessingCount;
1:cd6a4ff:   /**
1:cd6a4ff:    * flag to check whether all blocklets have been finished writing
1:cd6a4ff:    */
1:cd6a4ff:   private boolean processingComplete;
1:cd6a4ff: 
1:cd6a4ff:   /**
1:2cf1104:    * current data format version
1:cd6a4ff:    */
1:2cf1104:   private ColumnarFormatVersion version;
1:cd6a4ff: 
1:cd6a4ff:   /**
1:cd6a4ff:    * CarbonFactDataHandler constructor
1:cd6a4ff:    */
1:353272e:   public CarbonFactDataHandlerColumnar(CarbonFactDataHandlerModel model) {
1:353272e:     this.model = model;
1:353272e:     initParameters(model);
1:dc83b2a:     this.version = CarbonProperties.getInstance().getFormatVersion();
1:06b0d08:     StringBuffer noInvertedIdxCol = new StringBuffer();
1:a4083bf:     for (CarbonDimension cd : model.getSegmentProperties().getDimensions()) {
1:a4083bf:       if (!cd.isUseInvertedIndex()) {
1:06b0d08:         noInvertedIdxCol.append(cd.getColName()).append(",");
134:cd6a4ff:       }
1:a89587e:     }
1:496cde4: 
1:06b0d08:     LOGGER.info("Columns considered as NoInverted Index are " + noInvertedIdxCol.toString());
1:496cde4:   }
1:496cde4: 
1:353272e:   private void initParameters(CarbonFactDataHandlerModel model) {
1:e710339:     this.numberOfCores = model.getNumberOfCores();
1:cd6a4ff:     blockletProcessingCount = new AtomicInteger(0);
1:e710339:     producerExecutorService = Executors.newFixedThreadPool(model.getNumberOfCores(),
1:e710339:         new CarbonThreadFactory(
1:e710339:             "ProducerPool_" + System.nanoTime() + ":" + model.getTableName() + ", range: " + model
1:e710339:                 .getBucketId()));
1:cd6a4ff:     producerExecutorServiceTaskList =
1:cd6a4ff:         new ArrayList<>(CarbonCommonConstants.DEFAULT_COLLECTION_SIZE);
1:cd6a4ff:     LOGGER.info("Initializing writer executors");
1:e710339:     consumerExecutorService = Executors.newFixedThreadPool(1, new CarbonThreadFactory(
1:e710339:         "ConsumerPool_" + System.nanoTime() + ":" + model.getTableName() + ", range: " + model
1:e710339:             .getBucketId()));
1:cd6a4ff:     consumerExecutorServiceTaskList = new ArrayList<>(1);
1:cd6a4ff:     semaphore = new Semaphore(numberOfCores);
1:f089287:     tablePageList = new TablePageList();
1:496cde4: 
1:98df130:     // Start the consumer which will take each blocklet/page in order and write to a file
1:f089287:     Consumer consumer = new Consumer(tablePageList);
1:cd6a4ff:     consumerExecutorServiceTaskList.add(consumerExecutorService.submit(consumer));
1:496cde4:   }
1:496cde4: 
1:cd6a4ff:   private void setComplexMapSurrogateIndex(int dimensionCount) {
1:cd6a4ff:     int surrIndex = 0;
3:cd6a4ff:     for (int i = 0; i < dimensionCount; i++) {
1:353272e:       GenericDataType complexDataType = model.getComplexIndexMap().get(i);
1:496cde4:       if (complexDataType != null) {
1:cd6a4ff:         List<GenericDataType> primitiveTypes = new ArrayList<GenericDataType>();
1:cd6a4ff:         complexDataType.getAllPrimitiveChildren(primitiveTypes);
1:cd6a4ff:         for (GenericDataType eachPrimitive : primitiveTypes) {
1:06d38ff:           if (eachPrimitive.getIsColumnDictionary()) {
1:06d38ff:             eachPrimitive.setSurrogateIndex(surrIndex++);
1:06d38ff:           }
1:496cde4:         }
5:496cde4:       } else {
1:cd6a4ff:         surrIndex++;
1:496cde4:       }
1:496cde4:     }
1:496cde4:   }
1:496cde4: 
1:cd6a4ff:   /**
1:cd6a4ff:    * This method will be used to get and update the step properties which will
1:cd6a4ff:    * required to run this step
1:cd6a4ff:    *
1:cd6a4ff:    * @throws CarbonDataWriterException
1:cd6a4ff:    */
1:cd6a4ff:   public void initialise() throws CarbonDataWriterException {
1:cd6a4ff:     setWritingConfiguration();
1:496cde4:   }
1:496cde4: 
1:cd6a4ff:   /**
1:cd6a4ff:    * below method will be used to add row to store
1:cd6a4ff:    *
1:cd6a4ff:    * @param row
1:cd6a4ff:    * @throws CarbonDataWriterException
1:cd6a4ff:    */
1:353272e:   public void addDataToStore(CarbonRow row) throws CarbonDataWriterException {
1:cd6a4ff:     dataRows.add(row);
1:cd6a4ff:     this.entryCount++;
1:353272e:     // if entry count reaches to leaf node size then we are ready to write
1:cd6a4ff:     // this to leaf node file and update the intermediate files
1:6f1767b:     if (this.entryCount == this.pageSize || isVarcharColumnFull(row)) {
1:496cde4:       try {
1:cd6a4ff:         semaphore.acquire();
1:496cde4: 
1:98df130:         producerExecutorServiceTaskList.add(
1:98df130:             producerExecutorService.submit(
1:f089287:                 new Producer(tablePageList, dataRows, ++writerTaskSequenceCounter, false)
1:98df130:             )
1:98df130:         );
2:cd6a4ff:         blockletProcessingCount.incrementAndGet();
1:cd6a4ff:         // set the entry count to zero
2:cd6a4ff:         processedDataCount += entryCount;
1:cd6a4ff:         LOGGER.info("Total Number Of records added to store: " + processedDataCount);
1:f089287:         dataRows = new ArrayList<>(this.pageSize);
1:cd6a4ff:         this.entryCount = 0;
3:cd6a4ff:       } catch (InterruptedException e) {
5:cd6a4ff:         LOGGER.error(e, e.getMessage());
1:496cde4:         throw new CarbonDataWriterException(e.getMessage(), e);
1:496cde4:       }
1:496cde4:     }
1:496cde4:   }
1:496cde4: 
1:cd6a4ff:   /**
1:6f1767b:    * Check if column page can be added more rows after adding this row to page.
1:6f1767b:    *
1:6f1767b:    * A varchar column page uses SafeVarLengthColumnPage/UnsafeVarLengthColumnPage to store data
1:6f1767b:    * and encoded using HighCardDictDimensionIndexCodec which will call getByteArrayPage() from
1:6f1767b:    * column page and flatten into byte[] for compression.
1:6f1767b:    * Limited by the index of array, we can only put number of Integer.MAX_VALUE bytes in a page.
1:6f1767b:    *
1:6f1767b:    * Another limitation is from Compressor. Currently we use snappy as default compressor,
1:6f1767b:    * and it will call MaxCompressedLength method to estimate the result size for preparing output.
1:6f1767b:    * For safety, the estimate result is oversize: `32 + source_len + source_len/6`.
1:6f1767b:    * So the maximum bytes to compress by snappy is (2GB-32)*6/7â‰?1.71GB.
1:6f1767b:    *
1:6f1767b:    * Size of a row does not exceed 2MB since UnsafeSortDataRows uses 2MB byte[] as rowBuffer.
1:6f1767b:    * Such that we can stop adding more row here if any long string column reach this limit.
1:6f1767b:    *
1:6f1767b:    * If use unsafe column page, please ensure the memory configured is enough.
1:6f1767b:    * @param row
1:6f1767b:    * @return false if any varchar column page cannot add one more value(2MB)
1:6f1767b:    */
1:6f1767b:   private boolean isVarcharColumnFull(CarbonRow row) {
1:6f1767b:     if (model.getVarcharDimIdxInNoDict().size() > 0) {
1:6f1767b:       byte[][] nonDictArray = WriteStepRowUtil.getNoDictAndComplexDimension(row);
1:6f1767b:       for (int i = 0; i < model.getVarcharDimIdxInNoDict().size(); i++) {
1:6f1767b:         varcharColumnSizeInByte[i] += nonDictArray[model.getVarcharDimIdxInNoDict().get(i)].length;
1:6f1767b:         if (SnappyCompressor.MAX_BYTE_TO_COMPRESS -
1:6f1767b:                 (varcharColumnSizeInByte[i] + dataRows.size() * 4) < (2 << 20)) {
1:6f1767b:           LOGGER.info("Limited by varchar column, page size is " + dataRows.size());
1:6f1767b:           // re-init for next page
1:6f1767b:           varcharColumnSizeInByte = new int[model.getVarcharDimIdxInNoDict().size()];
1:6f1767b:           return true;
1:6f1767b:         }
1:6f1767b:       }
1:6f1767b:     }
1:6f1767b:     return false;
1:6f1767b:   }
1:6f1767b: 
1:6f1767b:   /**
1:bc3e684:    * generate the EncodedTablePage from the input rows (one page in case of V3 format)
1:cd6a4ff:    */
1:f089287:   private TablePage processDataRows(List<CarbonRow> dataRows)
1:eadfea7:       throws CarbonDataWriterException, KeyGenException, MemoryException, IOException {
1:1fa2df9:     if (dataRows.size() == 0) {
1:f089287:       return new TablePage(model, 0);
1:496cde4:     }
1:353272e:     TablePage tablePage = new TablePage(model, dataRows.size());
1:98df130:     int rowId = 0;
1:496cde4: 
1:98df130:     // convert row to columnar data
1:353272e:     for (CarbonRow row : dataRows) {
1:bc3e684:       tablePage.addRow(rowId++, row);
1:496cde4:     }
1:cd6a4ff: 
1:f089287:     tablePage.encode();
1:cd6a4ff: 
1:496cde4:     LOGGER.info("Number Of records processed: " + dataRows.size());
1:f089287:     return tablePage;
1:496cde4:   }
1:cd6a4ff: 
1:cd6a4ff:   /**
1:cd6a4ff:    * below method will be used to finish the data handler
1:cd6a4ff:    *
1:cd6a4ff:    * @throws CarbonDataWriterException
1:cd6a4ff:    */
1:cd6a4ff:   public void finish() throws CarbonDataWriterException {
1:cd6a4ff:     // still some data is present in stores if entryCount is more
1:cd6a4ff:     // than 0
1:a734add:     if (null == dataWriter) {
1:a734add:       return;
1:496cde4:     }
1:a734add:     if (producerExecutorService.isShutdown()) {
1:a734add:       return;
1:496cde4:     }
1:a734add:     LOGGER.info("Started Finish Operation");
9:cd6a4ff:     try {
1:f715126:       semaphore.acquire();
1:1fa2df9:       producerExecutorServiceTaskList.add(producerExecutorService
1:f089287:           .submit(new Producer(tablePageList, dataRows, ++writerTaskSequenceCounter, true)));
1:1fa2df9:       blockletProcessingCount.incrementAndGet();
1:1fa2df9:       processedDataCount += entryCount;
1:a734add:       LOGGER.info("Total Number Of records added to store: " + processedDataCount);
1:cd6a4ff:       closeWriterExecutionService(producerExecutorService);
1:cd6a4ff:       processWriteTaskSubmitList(producerExecutorServiceTaskList);
1:cd6a4ff:       processingComplete = true;
1:cd6a4ff:     } catch (InterruptedException e) {
1:cd6a4ff:       LOGGER.error(e, e.getMessage());
1:496cde4:       throw new CarbonDataWriterException(e.getMessage(), e);
1:496cde4:     }
1:496cde4:   }
1:cd6a4ff: 
1:cd6a4ff:   /**
1:cd6a4ff:    * This method will close writer execution service and get the node holders and
1:cd6a4ff:    * add them to node holder list
1:cd6a4ff:    *
1:cd6a4ff:    * @param service the service to shutdown
1:cd6a4ff:    * @throws CarbonDataWriterException
1:cd6a4ff:    */
1:cd6a4ff:   private void closeWriterExecutionService(ExecutorService service)
1:496cde4:       throws CarbonDataWriterException {
1:496cde4:     try {
1:cd6a4ff:       service.shutdown();
1:cd6a4ff:       service.awaitTermination(1, TimeUnit.DAYS);
1:496cde4:     } catch (InterruptedException e) {
1:496cde4:       LOGGER.error(e, e.getMessage());
4:cd6a4ff:       throw new CarbonDataWriterException(e.getMessage());
1:1fa2df9:     }
1:496cde4:   }
1:496cde4: 
1:cd6a4ff:   /**
1:cd6a4ff:    * This method will iterate through future task list and check if any exception
1:cd6a4ff:    * occurred during the thread execution
1:cd6a4ff:    *
1:cd6a4ff:    * @param taskList
1:cd6a4ff:    * @throws CarbonDataWriterException
1:cd6a4ff:    */
1:cd6a4ff:   private void processWriteTaskSubmitList(List<Future<Void>> taskList)
1:496cde4:       throws CarbonDataWriterException {
1:cd6a4ff:     for (int i = 0; i < taskList.size(); i++) {
1:496cde4:       try {
1:cd6a4ff:         taskList.get(i).get();
1:353272e:       } catch (InterruptedException | ExecutionException e) {
1:496cde4:         LOGGER.error(e, e.getMessage());
1:496cde4:         throw new CarbonDataWriterException(e.getMessage(), e);
1:496cde4:       }
1:496cde4:     }
1:496cde4:   }
1:496cde4: 
1:98df130:   // return the number of complex column after complex columns are expanded
1:98df130:   private int getExpandedComplexColsCount() {
1:dc83b2a:     return model.getExpandedComplexColsCount();
1:496cde4:   }
1:496cde4: 
1:cd6a4ff:   /**
1:cd6a4ff:    * below method will be used to close the handler
1:cd6a4ff:    */
1:cd6a4ff:   public void closeHandler() throws CarbonDataWriterException {
1:cd6a4ff:     if (null != this.dataWriter) {
1:cd6a4ff:       // wait until all blocklets have been finished writing
1:cd6a4ff:       while (blockletProcessingCount.get() > 0) {
1:cd6a4ff:         try {
1:cd6a4ff:           Thread.sleep(50);
1:cd6a4ff:         } catch (InterruptedException e) {
1:cd6a4ff:           throw new CarbonDataWriterException(e.getMessage());
1:496cde4:         }
1:496cde4:       }
1:cd6a4ff:       consumerExecutorService.shutdownNow();
1:cd6a4ff:       processWriteTaskSubmitList(consumerExecutorServiceTaskList);
1:bc3e684:       this.dataWriter.writeFooterToFile();
1:cd6a4ff:       LOGGER.info("All blocklets have been finished writing");
1:cd6a4ff:       // close all the open stream for both the files
1:cd6a4ff:       this.dataWriter.closeWriter();
1:496cde4:     }
1:cd6a4ff:     this.dataWriter = null;
1:496cde4:   }
1:cd6a4ff: 
1:cd6a4ff:   /**
1:cd6a4ff:    * Below method will be to configure fact file writing configuration
1:cd6a4ff:    *
1:cd6a4ff:    * @throws CarbonDataWriterException
1:cd6a4ff:    */
1:cd6a4ff:   private void setWritingConfiguration() throws CarbonDataWriterException {
1:cd6a4ff:     // get blocklet size
2:f089287:     this.pageSize = Integer.parseInt(CarbonProperties.getInstance()
1:cd6a4ff:         .getProperty(CarbonCommonConstants.BLOCKLET_SIZE,
1:cd6a4ff:             CarbonCommonConstants.BLOCKLET_SIZE_DEFAULT_VAL));
1:091a28b:     // support less than 32000 rows in one page, because we support super long string,
1:6f1767b:     // if it is long enough, a column page with 32000 rows will exceed 2GB
1:b16c308:     if (version == ColumnarFormatVersion.V3) {
1:091a28b:       this.pageSize =
1:091a28b:           pageSize < CarbonV3DataFormatConstants.NUMBER_OF_ROWS_PER_BLOCKLET_COLUMN_PAGE_DEFAULT ?
1:091a28b:               pageSize :
1:091a28b:               CarbonV3DataFormatConstants.NUMBER_OF_ROWS_PER_BLOCKLET_COLUMN_PAGE_DEFAULT;
1:496cde4:     }
1:6f1767b:     LOGGER.info("Number of rows per column page is configured as pageSize = " + pageSize);
1:f089287:     dataRows = new ArrayList<>(this.pageSize);
1:6f1767b: 
1:6f1767b:     if (model.getVarcharDimIdxInNoDict().size() > 0) {
1:6f1767b:       LOGGER.info("Number of rows per column blocklet is constrained by pageSize and actual size " +
1:6f1767b:               "of long string column(s)");
1:6f1767b:       varcharColumnSizeInByte = new int[model.getVarcharDimIdxInNoDict().size()];
1:6f1767b:     }
1:6f1767b: 
1:cd6a4ff:     int dimSet =
1:cd6a4ff:         Integer.parseInt(CarbonCommonConstants.DIMENSION_SPLIT_VALUE_IN_COLUMNAR_DEFAULTVALUE);
1:f089287:     // if at least one dimension is present then initialize column splitter otherwise null
1:f911403:     int[] keyBlockSize = new int[getExpandedComplexColsCount()];
1:cd6a4ff: 
1:cd6a4ff:     // agg type
1:cd6a4ff:     List<Integer> otherMeasureIndexList =
1:cd6a4ff:         new ArrayList<Integer>(CarbonCommonConstants.DEFAULT_COLLECTION_SIZE);
1:cd6a4ff:     List<Integer> customMeasureIndexList =
1:cd6a4ff:         new ArrayList<Integer>(CarbonCommonConstants.DEFAULT_COLLECTION_SIZE);
1:353272e:     DataType[] type = model.getMeasureDataType();
1:cd6a4ff:     for (int j = 0; j < type.length; j++) {
1:f209e8e:       if (type[j] != DataTypes.BYTE && !DataTypes.isDecimal(type[j])) {
1:cd6a4ff:         otherMeasureIndexList.add(j);
1:496cde4:       } else {
1:cd6a4ff:         customMeasureIndexList.add(j);
1:496cde4:       }
1:496cde4:     }
1:cd6a4ff: 
1:98df130:     int[] otherMeasureIndex = new int[otherMeasureIndexList.size()];
1:98df130:     int[] customMeasureIndex = new int[customMeasureIndexList.size()];
2:cd6a4ff:     for (int i = 0; i < otherMeasureIndex.length; i++) {
1:cd6a4ff:       otherMeasureIndex[i] = otherMeasureIndexList.get(i);
1:496cde4:     }
1:496cde4:     for (int i = 0; i < customMeasureIndex.length; i++) {
1:cd6a4ff:       customMeasureIndex[i] = customMeasureIndexList.get(i);
1:496cde4:     }
1:353272e:     setComplexMapSurrogateIndex(model.getDimensionCount());
1:cd6a4ff:     int[] blockKeySize = getBlockKeySizeWithComplexTypes(new MultiDimKeyVarLengthEquiSplitGenerator(
1:353272e:         CarbonUtil.getIncrementedCardinalityFullyFilled(model.getDimLens().clone()), (byte) dimSet)
1:cd6a4ff:         .getBlockKeySize());
1:f911403:     System.arraycopy(blockKeySize, 0, keyBlockSize, 0, blockKeySize.length);
1:f089287:     this.dataWriter = getFactDataWriter();
1:cd6a4ff:     // initialize the channel;
1:cd6a4ff:     this.dataWriter.initializeWriter();
1:496cde4:   }
1:496cde4: 
1:cd6a4ff:   /**
1:cd6a4ff:    * This method combines primitive dimensions with complex metadata columns
1:cd6a4ff:    *
1:cd6a4ff:    * @param primitiveBlockKeySize
1:cd6a4ff:    * @return all dimensions cardinality including complex dimension metadata column
1:cd6a4ff:    */
1:cd6a4ff:   private int[] getBlockKeySizeWithComplexTypes(int[] primitiveBlockKeySize) {
1:98df130:     int allColsCount = getExpandedComplexColsCount();
1:f911403:     int[] blockKeySizeWithComplexTypes = new int[allColsCount];
1:cd6a4ff: 
1:cd6a4ff:     List<Integer> blockKeySizeWithComplex =
1:cd6a4ff:         new ArrayList<Integer>(blockKeySizeWithComplexTypes.length);
1:353272e:     int dictDimensionCount = model.getDimensionCount();
1:353272e:     for (int i = 0; i < dictDimensionCount; i++) {
1:353272e:       GenericDataType complexDataType = model.getComplexIndexMap().get(i);
7:cd6a4ff:       if (complexDataType != null) {
1:cd6a4ff:         complexDataType.fillBlockKeySize(blockKeySizeWithComplex, primitiveBlockKeySize);
1:496cde4:       } else {
1:cd6a4ff:         blockKeySizeWithComplex.add(primitiveBlockKeySize[i]);
1:496cde4:       }
1:496cde4:     }
1:cd6a4ff:     for (int i = 0; i < blockKeySizeWithComplexTypes.length; i++) {
1:cd6a4ff:       blockKeySizeWithComplexTypes[i] = blockKeySizeWithComplex.get(i);
1:496cde4:     }
1:cd6a4ff: 
1:cd6a4ff:     return blockKeySizeWithComplexTypes;
1:496cde4:   }
1:cd6a4ff: 
1:cd6a4ff:   /**
1:d54dc64:    * Below method will be used to get the fact data writer instance
1:cd6a4ff:    *
1:d54dc64:    * @return data writer instance
1:cd6a4ff:    */
1:349c59c:   private CarbonFactDataWriter getFactDataWriter() {
1:c9e5842:     return CarbonDataWriterFactory.getInstance().getFactDataWriter(version, model);
1:496cde4:   }
1:cd6a4ff: 
1:cd6a4ff:   /**
1:cd6a4ff:    * This method will reset the block processing count
1:cd6a4ff:    */
1:cd6a4ff:   private void resetBlockletProcessingCount() {
1:cd6a4ff:     blockletProcessingCount.set(0);
1:496cde4:   }
1:cd6a4ff: 
1:cd6a4ff:   /**
1:f089287:    * This class will hold the table page data
1:cd6a4ff:    */
1:f089287:   private final class TablePageList {
1:cd6a4ff:     /**
1:f089287:      * array of table page added by Producer and get by Consumer
1:cd6a4ff:      */
1:f089287:     private TablePage[] tablePages;
1:cd6a4ff:     /**
1:cd6a4ff:      * flag to check whether the producer has completed processing for holder
1:cd6a4ff:      * object which is required to be picked form an index
1:cd6a4ff:      */
1:cd6a4ff:     private AtomicBoolean available;
1:cd6a4ff:     /**
1:cd6a4ff:      * index from which data node holder object needs to be picked for writing
1:cd6a4ff:      */
1:cd6a4ff:     private int currentIndex;
1:cd6a4ff: 
1:f089287:     private TablePageList() {
1:f089287:       tablePages = new TablePage[numberOfCores];
1:cd6a4ff:       available = new AtomicBoolean(false);
1:496cde4:     }
1:cd6a4ff: 
1:cd6a4ff:     /**
1:cd6a4ff:      * @return a node holder object
1:cd6a4ff:      * @throws InterruptedException if consumer thread is interrupted
1:cd6a4ff:      */
1:f089287:     public synchronized TablePage get() throws InterruptedException {
1:f089287:       TablePage tablePage = tablePages[currentIndex];
1:cd6a4ff:       // if node holder is null means producer thread processing the data which has to
1:cd6a4ff:       // be inserted at this current index has not completed yet
1:f089287:       if (null == tablePage && !processingComplete) {
1:cd6a4ff:         available.set(false);
1:496cde4:       }
1:cd6a4ff:       while (!available.get()) {
1:cd6a4ff:         wait();
1:496cde4:       }
1:f089287:       tablePage = tablePages[currentIndex];
1:f089287:       tablePages[currentIndex] = null;
1:cd6a4ff:       currentIndex++;
1:cd6a4ff:       // reset current index when it reaches length of node holder array
1:f089287:       if (currentIndex >= tablePages.length) {
1:cd6a4ff:         currentIndex = 0;
1:496cde4:       }
1:f089287:       return tablePage;
1:496cde4:     }
1:cd6a4ff: 
1:cd6a4ff:     /**
1:a734add:      * @param tablePage
1:cd6a4ff:      * @param index
1:cd6a4ff:      */
1:f089287:     public synchronized void put(TablePage tablePage, int index) {
1:f089287:       tablePages[index] = tablePage;
1:cd6a4ff:       // notify the consumer thread when index at which object is to be inserted
1:cd6a4ff:       // becomes equal to current index from where data has to be picked for writing
1:cd6a4ff:       if (index == currentIndex) {
1:cd6a4ff:         available.set(true);
1:cd6a4ff:         notifyAll();
1:496cde4:       }
1:cd6a4ff:     }
1:cd6a4ff:   }
1:cd6a4ff: 
1:cd6a4ff:   /**
1:cd6a4ff:    * Producer which will process data equivalent to 1 blocklet size
1:cd6a4ff:    */
1:cd6a4ff:   private final class Producer implements Callable<Void> {
1:cd6a4ff: 
1:f089287:     private TablePageList tablePageList;
1:353272e:     private List<CarbonRow> dataRows;
1:f089287:     private int pageId;
1:bc3e684:     private boolean isLastPage;
1:cd6a4ff: 
1:f089287:     private Producer(TablePageList tablePageList, List<CarbonRow> dataRows,
1:f089287:         int pageId, boolean isLastPage) {
1:f089287:       this.tablePageList = tablePageList;
1:cd6a4ff:       this.dataRows = dataRows;
1:f089287:       this.pageId = pageId;
1:bc3e684:       this.isLastPage = isLastPage;
1:cd6a4ff:     }
1:cd6a4ff: 
1:cd6a4ff:     /**
1:cd6a4ff:      * Computes a result, or throws an exception if unable to do so.
1:cd6a4ff:      *
1:cd6a4ff:      * @return computed result
1:cd6a4ff:      * @throws Exception if unable to compute a result
1:cd6a4ff:      */
1:cd6a4ff:     @Override public Void call() throws Exception {
1:cd6a4ff:       try {
1:f089287:         TablePage tablePage = processDataRows(dataRows);
1:dded5d5:         dataRows = null;
1:f089287:         tablePage.setIsLastPage(isLastPage);
1:cd6a4ff:         // insert the object in array according to sequence number
1:f089287:         int indexInNodeHolderArray = (pageId - 1) % numberOfCores;
1:f089287:         tablePageList.put(tablePage, indexInNodeHolderArray);
1:cd6a4ff:         return null;
1:cd6a4ff:       } catch (Throwable throwable) {
1:aca59ce:         LOGGER.error(throwable, "Error in producer");
1:cd6a4ff:         consumerExecutorService.shutdownNow();
1:cd6a4ff:         resetBlockletProcessingCount();
1:496cde4:         throw new CarbonDataWriterException(throwable.getMessage(), throwable);
1:cd6a4ff:       }
1:cd6a4ff:     }
1:cd6a4ff:   }
1:cd6a4ff: 
1:cd6a4ff:   /**
1:cd6a4ff:    * Consumer class will get one blocklet data at a time and submit for writing
1:cd6a4ff:    */
1:cd6a4ff:   private final class Consumer implements Callable<Void> {
1:cd6a4ff: 
1:f089287:     private TablePageList tablePageList;
1:cd6a4ff: 
1:f089287:     private Consumer(TablePageList tablePageList) {
1:f089287:       this.tablePageList = tablePageList;
1:cd6a4ff:     }
1:cd6a4ff: 
1:cd6a4ff:     /**
1:cd6a4ff:      * Computes a result, or throws an exception if unable to do so.
1:cd6a4ff:      *
1:cd6a4ff:      * @return computed result
1:cd6a4ff:      * @throws Exception if unable to compute a result
1:cd6a4ff:      */
1:cd6a4ff:     @Override public Void call() throws Exception {
1:cd6a4ff:       while (!processingComplete || blockletProcessingCount.get() > 0) {
1:f089287:         TablePage tablePage = null;
1:cd6a4ff:         try {
1:f089287:           tablePage = tablePageList.get();
1:f089287:           if (null != tablePage) {
1:f089287:             dataWriter.writeTablePage(tablePage);
1:f089287:             tablePage.freeMemory();
1:cd6a4ff:           }
1:cd6a4ff:           blockletProcessingCount.decrementAndGet();
1:cd6a4ff:         } catch (Throwable throwable) {
1:cd6a4ff:           if (!processingComplete || blockletProcessingCount.get() > 0) {
1:cd6a4ff:             producerExecutorService.shutdownNow();
1:cd6a4ff:             resetBlockletProcessingCount();
1:d54dc64:             LOGGER.error(throwable, "Problem while writing the carbon data file");
2:cd6a4ff:             throw new CarbonDataWriterException(throwable.getMessage());
1:cd6a4ff:           }
1:cd6a4ff:         } finally {
1:cd6a4ff:           semaphore.release();
1:cd6a4ff:         }
1:cd6a4ff:       }
1:cd6a4ff:       return null;
1:cd6a4ff:     }
1:cd6a4ff:   }
1:cd6a4ff: }
============================================================================
author:Indhumathi27
-------------------------------------------------------------------------------
commit:06d38ff
/////////////////////////////////////////////////////////////////////////
1:           if (eachPrimitive.getIsColumnDictionary()) {
1:             eachPrimitive.setSurrogateIndex(surrIndex++);
1:           }
author:Manhua
-------------------------------------------------------------------------------
commit:6f1767b
/////////////////////////////////////////////////////////////////////////
1: import org.apache.carbondata.core.datastore.compression.SnappyCompressor;
1: import org.apache.carbondata.core.datastore.row.WriteStepRowUtil;
/////////////////////////////////////////////////////////////////////////
1:   private int[] varcharColumnSizeInByte;
/////////////////////////////////////////////////////////////////////////
1:     if (this.entryCount == this.pageSize || isVarcharColumnFull(row)) {
/////////////////////////////////////////////////////////////////////////
1:    * Check if column page can be added more rows after adding this row to page.
1:    *
1:    * A varchar column page uses SafeVarLengthColumnPage/UnsafeVarLengthColumnPage to store data
1:    * and encoded using HighCardDictDimensionIndexCodec which will call getByteArrayPage() from
1:    * column page and flatten into byte[] for compression.
1:    * Limited by the index of array, we can only put number of Integer.MAX_VALUE bytes in a page.
1:    *
1:    * Another limitation is from Compressor. Currently we use snappy as default compressor,
1:    * and it will call MaxCompressedLength method to estimate the result size for preparing output.
1:    * For safety, the estimate result is oversize: `32 + source_len + source_len/6`.
1:    * So the maximum bytes to compress by snappy is (2GB-32)*6/7â‰?1.71GB.
1:    *
1:    * Size of a row does not exceed 2MB since UnsafeSortDataRows uses 2MB byte[] as rowBuffer.
1:    * Such that we can stop adding more row here if any long string column reach this limit.
1:    *
1:    * If use unsafe column page, please ensure the memory configured is enough.
1:    * @param row
1:    * @return false if any varchar column page cannot add one more value(2MB)
1:    */
1:   private boolean isVarcharColumnFull(CarbonRow row) {
1:     if (model.getVarcharDimIdxInNoDict().size() > 0) {
1:       byte[][] nonDictArray = WriteStepRowUtil.getNoDictAndComplexDimension(row);
1:       for (int i = 0; i < model.getVarcharDimIdxInNoDict().size(); i++) {
1:         varcharColumnSizeInByte[i] += nonDictArray[model.getVarcharDimIdxInNoDict().get(i)].length;
1:         if (SnappyCompressor.MAX_BYTE_TO_COMPRESS -
1:                 (varcharColumnSizeInByte[i] + dataRows.size() * 4) < (2 << 20)) {
1:           LOGGER.info("Limited by varchar column, page size is " + dataRows.size());
1:           // re-init for next page
1:           varcharColumnSizeInByte = new int[model.getVarcharDimIdxInNoDict().size()];
1:           return true;
1:         }
1:       }
1:     }
1:     return false;
1:   }
1: 
1:   /**
/////////////////////////////////////////////////////////////////////////
1:     // if it is long enough, a column page with 32000 rows will exceed 2GB
1:     LOGGER.info("Number of rows per column page is configured as pageSize = " + pageSize);
1: 
1:     if (model.getVarcharDimIdxInNoDict().size() > 0) {
1:       LOGGER.info("Number of rows per column blocklet is constrained by pageSize and actual size " +
1:               "of long string column(s)");
1:       varcharColumnSizeInByte = new int[model.getVarcharDimIdxInNoDict().size()];
1:     }
1: 
author:sraghunandan
-------------------------------------------------------------------------------
commit:f911403
/////////////////////////////////////////////////////////////////////////
/////////////////////////////////////////////////////////////////////////
/////////////////////////////////////////////////////////////////////////
/////////////////////////////////////////////////////////////////////////
1:     int[] keyBlockSize = new int[getExpandedComplexColsCount()];
/////////////////////////////////////////////////////////////////////////
1:     System.arraycopy(blockKeySize, 0, keyBlockSize, 0, blockKeySize.length);
/////////////////////////////////////////////////////////////////////////
1:     int[] blockKeySizeWithComplexTypes = new int[allColsCount];
author:kumarvishal09
-------------------------------------------------------------------------------
commit:e710339
/////////////////////////////////////////////////////////////////////////
/////////////////////////////////////////////////////////////////////////
1:     this.numberOfCores = model.getNumberOfCores();
1:     producerExecutorService = Executors.newFixedThreadPool(model.getNumberOfCores(),
1:         new CarbonThreadFactory(
1:             "ProducerPool_" + System.nanoTime() + ":" + model.getTableName() + ", range: " + model
1:                 .getBucketId()));
1:     consumerExecutorService = Executors.newFixedThreadPool(1, new CarbonThreadFactory(
1:         "ConsumerPool_" + System.nanoTime() + ":" + model.getTableName() + ", range: " + model
1:             .getBucketId()));
author:xuchuanyin
-------------------------------------------------------------------------------
commit:091a28b
/////////////////////////////////////////////////////////////////////////
1:     // support less than 32000 rows in one page, because we support super long string,
0:     // if it is long enough, a clomun page with 32000 rows will exceed 2GB
1:       this.pageSize =
1:           pageSize < CarbonV3DataFormatConstants.NUMBER_OF_ROWS_PER_BLOCKLET_COLUMN_PAGE_DEFAULT ?
1:               pageSize :
1:               CarbonV3DataFormatConstants.NUMBER_OF_ROWS_PER_BLOCKLET_COLUMN_PAGE_DEFAULT;
commit:d5396b1
/////////////////////////////////////////////////////////////////////////
0:         new CarbonThreadFactory("ProducerPool:" + model.getTableName()
0:             + ", range: " + model.getBucketId()));
0:         .newFixedThreadPool(1, new CarbonThreadFactory("ConsumerPool:" + model.getTableName()
0:             + ", range: " + model.getBucketId()));
commit:ded8b41
/////////////////////////////////////////////////////////////////////////
0:     // todo: the fileManager seems to be useless, remove it later
0:     fileManager.setName(new File(model.getStoreLocation()[0]).getName());
author:ravipesala
-------------------------------------------------------------------------------
commit:dded5d5
/////////////////////////////////////////////////////////////////////////
1:         dataRows = null;
commit:a89587e
/////////////////////////////////////////////////////////////////////////
0:     // Overriding it to the task specified cores.
0:     if (model.getWritingCoresCount() > 0) {
0:       numberOfCores = model.getWritingCoresCount();
1:     }
commit:1fa2df9
/////////////////////////////////////////////////////////////////////////
1:     if (dataRows.size() == 0) {
0:       return new NodeHolder();
1:     }
/////////////////////////////////////////////////////////////////////////
1:     producerExecutorServiceTaskList.add(producerExecutorService
0:         .submit(new Producer(blockletDataHolder, dataRows, ++writerTaskSequenceCounter, true)));
1:     blockletProcessingCount.incrementAndGet();
1:     processedDataCount += entryCount;
commit:e6b6090
/////////////////////////////////////////////////////////////////////////
0: import org.apache.carbondata.processing.store.file.FileManager;
0: import org.apache.carbondata.processing.store.file.IFileManagerComposite;
/////////////////////////////////////////////////////////////////////////
/////////////////////////////////////////////////////////////////////////
/////////////////////////////////////////////////////////////////////////
0:   private NodeHolder processDataRows(List<Object[]> dataRows)
/////////////////////////////////////////////////////////////////////////
0:     CarbonWriteDataHolder keyDataHolder = initialiseKeyBlockHolder(dataRows.size());
/////////////////////////////////////////////////////////////////////////
0:         getNodeHolderObject(writableMeasureDataArray, byteArrayValues, dataRows.size(),
0:   private NodeHolder getNodeHolderObject(byte[][] dataHolderLocal,
/////////////////////////////////////////////////////////////////////////
0:     keyDataHolder.initialiseByteArrayValuesForKey(size);
/////////////////////////////////////////////////////////////////////////
0:         NodeHolder nodeHolder = processDataRows(dataRows);
commit:b13ead9
/////////////////////////////////////////////////////////////////////////
0: import org.apache.carbondata.processing.util.NonDictionaryUtil;
/////////////////////////////////////////////////////////////////////////
1: 
0:   private int taskExtension;
1: 
/////////////////////////////////////////////////////////////////////////
0:     this.taskExtension = carbonFactDataHandlerModel.getTaskExtension();
/////////////////////////////////////////////////////////////////////////
0:         byte[][] splitKey = NonDictionaryUtil
/////////////////////////////////////////////////////////////////////////
0:           NonDictionaryUtil.packByteBufferIntoSingleByteArray(noDictionaryStartKey);
0:           NonDictionaryUtil.packByteBufferIntoSingleByteArray(noDictionaryEndKey);
/////////////////////////////////////////////////////////////////////////
0:     carbonDataWriterVo.setTaskExtension(taskExtension);
commit:aca59ce
/////////////////////////////////////////////////////////////////////////
1:         LOGGER.error(throwable, "Error in producer");
commit:cbf8797
/////////////////////////////////////////////////////////////////////////
0:   private int bucketNumber;
1: 
/////////////////////////////////////////////////////////////////////////
0:     this.bucketNumber = carbonFactDataHandlerModel.getBucketId();
/////////////////////////////////////////////////////////////////////////
0:     carbonDataWriterVo.setBucketNumber(bucketNumber);
commit:496cde4
/////////////////////////////////////////////////////////////////////////
0:   private boolean useKettle;
1: 
/////////////////////////////////////////////////////////////////////////
0:     this.useKettle = carbonFactDataHandlerModel.isUseKettle();
/////////////////////////////////////////////////////////////////////////
1:         throw new CarbonDataWriterException(e.getMessage(), e);
0:   // TODO remove after kettle flow is removed
/////////////////////////////////////////////////////////////////////////
0:   private NodeHolder processDataRowsWithOutKettle(List<Object[]> dataRows)
1:       throws CarbonDataWriterException {
0:     Object[] max = new Object[measureCount];
0:     Object[] min = new Object[measureCount];
0:     int[] decimal = new int[measureCount];
0:     Object[] uniqueValue = new Object[measureCount];
0:     // to store index of the measure columns which are null
0:     BitSet[] nullValueIndexBitSet = getMeasureNullValueIndexBitSet(measureCount);
0:     for (int i = 0; i < max.length; i++) {
0:       if (type[i] == CarbonCommonConstants.BIG_INT_MEASURE) {
0:         max[i] = Long.MIN_VALUE;
0:       } else if (type[i] == CarbonCommonConstants.SUM_COUNT_VALUE_MEASURE) {
0:         max[i] = -Double.MAX_VALUE;
0:       } else if (type[i] == CarbonCommonConstants.BIG_DECIMAL_MEASURE) {
0:         max[i] = new BigDecimal(0.0);
1:       } else {
0:         max[i] = 0.0;
1:       }
1:     }
0:     for (int i = 0; i < min.length; i++) {
0:       if (type[i] == CarbonCommonConstants.BIG_INT_MEASURE) {
0:         min[i] = Long.MAX_VALUE;
0:         uniqueValue[i] = Long.MIN_VALUE;
0:       } else if (type[i] == CarbonCommonConstants.SUM_COUNT_VALUE_MEASURE) {
0:         min[i] = Double.MAX_VALUE;
0:         uniqueValue[i] = Double.MIN_VALUE;
0:       } else if (type[i] == CarbonCommonConstants.BIG_DECIMAL_MEASURE) {
0:         min[i] = new BigDecimal(Double.MAX_VALUE);
0:         uniqueValue[i] = new BigDecimal(Double.MIN_VALUE);
1:       } else {
0:         min[i] = 0.0;
0:         uniqueValue[i] = 0.0;
1:       }
1:     }
0:     for (int i = 0; i < decimal.length; i++) {
0:       decimal[i] = 0;
1:     }
1: 
0:     byte[] startKey = null;
0:     byte[] endKey = null;
0:     byte[][] noDictStartKey = null;
0:     byte[][] noDictEndKey = null;
0:     CarbonWriteDataHolder[] dataHolder = initialiseDataHolder(dataRows.size());
0:     CarbonWriteDataHolder keyDataHolder = initialiseKeyBlockHolderWithOutKettle(dataRows.size());
0:     CarbonWriteDataHolder noDictionaryKeyDataHolder = null;
0:     if ((noDictionaryCount + complexColCount) > 0) {
0:       noDictionaryKeyDataHolder = initialiseKeyBlockHolderForNonDictionary(dataRows.size());
1:     }
1: 
0:     for (int count = 0; count < dataRows.size(); count++) {
0:       Object[] row = dataRows.get(count);
0:       byte[] mdKey = (byte[]) row[this.mdKeyIndex];
0:       byte[][] noDictionaryKey = null;
0:       if (noDictionaryCount > 0 || complexIndexMap.size() > 0) {
0:         noDictionaryKey = (byte[][]) row[this.mdKeyIndex - 1];
1:       }
0:       ByteBuffer byteBuffer = null;
0:       byte[] b = null;
0:       if (count == 0) {
0:         startKey = mdKey;
0:         noDictStartKey = noDictionaryKey;
1:       }
0:       endKey = mdKey;
0:       noDictEndKey = noDictionaryKey;
0:       // add to key store
0:       if (mdKey.length > 0) {
0:         keyDataHolder.setWritableByteArrayValueByIndex(count, mdKey);
1:       }
0:       // for storing the byte [] for high card.
0:       if (noDictionaryCount > 0 || complexIndexMap.size() > 0) {
0:         noDictionaryKeyDataHolder.setWritableNonDictByteArrayValueByIndex(count, noDictionaryKey);
1:       }
1: 
0:       for (int k = 0; k < otherMeasureIndex.length; k++) {
0:         if (type[otherMeasureIndex[k]] == CarbonCommonConstants.BIG_INT_MEASURE) {
0:           if (null == row[otherMeasureIndex[k]]) {
0:             nullValueIndexBitSet[otherMeasureIndex[k]].set(count);
0:             dataHolder[otherMeasureIndex[k]].setWritableLongValueByIndex(count, 0L);
1:           } else {
0:             dataHolder[otherMeasureIndex[k]]
0:                 .setWritableLongValueByIndex(count, row[otherMeasureIndex[k]]);
1:           }
1:         } else {
0:           if (null == row[otherMeasureIndex[k]]) {
0:             nullValueIndexBitSet[otherMeasureIndex[k]].set(count);
0:             dataHolder[otherMeasureIndex[k]].setWritableDoubleValueByIndex(count, 0.0);
1:           } else {
0:             dataHolder[otherMeasureIndex[k]]
0:                 .setWritableDoubleValueByIndex(count, row[otherMeasureIndex[k]]);
1:           }
1:         }
1:       }
0:       calculateMaxMin(max, min, decimal, otherMeasureIndex, row);
1:       for (int i = 0; i < customMeasureIndex.length; i++) {
0:         if (null == row[customMeasureIndex[i]]
0:             && type[customMeasureIndex[i]] == CarbonCommonConstants.BIG_DECIMAL_MEASURE) {
0:           BigDecimal val = BigDecimal.valueOf(0);
0:           b = DataTypeUtil.bigDecimalToByte(val);
0:           nullValueIndexBitSet[customMeasureIndex[i]].set(count);
1:         } else {
0:           if (this.compactionFlow) {
0:             BigDecimal bigDecimal = ((Decimal) row[customMeasureIndex[i]]).toJavaBigDecimal();
0:             b = DataTypeUtil.bigDecimalToByte(bigDecimal);
1:           } else {
0:             b = (byte[]) row[customMeasureIndex[i]];
1:           }
1:         }
0:         byteBuffer = ByteBuffer.allocate(b.length + CarbonCommonConstants.INT_SIZE_IN_BYTE);
0:         byteBuffer.putInt(b.length);
0:         byteBuffer.put(b);
0:         byteBuffer.flip();
0:         b = byteBuffer.array();
0:         dataHolder[customMeasureIndex[i]].setWritableByteArrayValueByIndex(count, b);
1:       }
0:       calculateMaxMin(max, min, decimal, customMeasureIndex, row);
1:     }
0:     calculateUniqueValue(min, uniqueValue);
0:     byte[][] byteArrayValues = keyDataHolder.getByteArrayValues().clone();
0:     byte[][][] noDictionaryValueHolder = null;
0:     if ((noDictionaryCount + complexColCount) > 0) {
0:       noDictionaryValueHolder = noDictionaryKeyDataHolder.getNonDictByteArrayValues();
1:     }
0:     ValueCompressionModel compressionModel = ValueCompressionUtil
0:         .getValueCompressionModel(max, min, decimal, uniqueValue, type, new byte[max.length]);
0:     byte[][] writableMeasureDataArray =
0:         StoreFactory.createDataStore(compressionModel).getWritableMeasureDataArray(dataHolder)
0:             .clone();
0:     NodeHolder nodeHolder =
0:         getNodeHolderObjectWithOutKettle(writableMeasureDataArray, byteArrayValues, dataRows.size(),
0:             startKey, endKey, compressionModel, noDictionaryValueHolder, noDictStartKey,
0:             noDictEndKey);
0:     nodeHolder.setMeasureNullValueIndex(nullValueIndexBitSet);
1:     LOGGER.info("Number Of records processed: " + dataRows.size());
0:     return nodeHolder;
1:   }
1: 
0:   // TODO remove after kettle flow is removed
/////////////////////////////////////////////////////////////////////////
0:   private NodeHolder getNodeHolderObjectWithOutKettle(byte[][] dataHolderLocal,
0:       byte[][] byteArrayValues, int entryCountLocal, byte[] startkeyLocal, byte[] endKeyLocal,
0:       ValueCompressionModel compressionModel, byte[][][] noDictionaryData,
0:       byte[][] noDictionaryStartKey, byte[][] noDictionaryEndKey)
1:       throws CarbonDataWriterException {
0:     byte[][][] noDictionaryColumnsData = null;
0:     List<ArrayList<byte[]>> colsAndValues = new ArrayList<ArrayList<byte[]>>();
0:     int complexColCount = getComplexColsCount();
1: 
0:     for (int i = 0; i < complexColCount; i++) {
0:       colsAndValues.add(new ArrayList<byte[]>());
1:     }
0:     int noOfColumn = colGrpModel.getNoOfColumnStore();
0:     DataHolder[] dataHolders = getDataHolders(noOfColumn, byteArrayValues.length);
0:     for (int i = 0; i < byteArrayValues.length; i++) {
0:       byte[][] splitKey = columnarSplitter.splitKey(byteArrayValues[i]);
1: 
0:       for (int j = 0; j < splitKey.length; j++) {
0:         dataHolders[j].addData(splitKey[j], i);
1:       }
1:     }
0:     if (noDictionaryCount > 0 || complexIndexMap.size() > 0) {
0:       noDictionaryColumnsData = new byte[noDictionaryCount][noDictionaryData.length][];
0:       for (int i = 0; i < noDictionaryData.length; i++) {
0:         int complexColumnIndex = primitiveDimLens.length + noDictionaryCount;
0:         byte[][] splitKey = noDictionaryData[i];
1: 
0:         int complexTypeIndex = 0;
0:         for (int j = 0; j < splitKey.length; j++) {
0:           //nodictionary Columns
0:           if (j < noDictionaryCount) {
0:             int keyLength = splitKey[j].length;
0:             byte[] newKey = new byte[keyLength + 2];
0:             ByteBuffer buffer = ByteBuffer.wrap(newKey);
0:             buffer.putShort((short)keyLength);
0:             System.arraycopy(splitKey[j], 0, newKey, 2, keyLength);
0:             noDictionaryColumnsData[j][i] = newKey;
1:           }
0:           //complex types
0:           else {
0:             // Need to write columnar block from complex byte array
0:             int index = complexColumnIndex - noDictionaryCount;
0:             GenericDataType complexDataType = complexIndexMap.get(index);
0:             complexColumnIndex++;
1:             if (complexDataType != null) {
0:               List<ArrayList<byte[]>> columnsArray = new ArrayList<ArrayList<byte[]>>();
0:               for (int k = 0; k < complexDataType.getColsCount(); k++) {
0:                 columnsArray.add(new ArrayList<byte[]>());
1:               }
1: 
1:               try {
0:                 ByteBuffer byteArrayInput = ByteBuffer.wrap(splitKey[j]);
0:                 ByteArrayOutputStream byteArrayOutput = new ByteArrayOutputStream();
0:                 DataOutputStream dataOutputStream = new DataOutputStream(byteArrayOutput);
0:                 complexDataType
0:                     .parseAndBitPack(byteArrayInput, dataOutputStream, this.complexKeyGenerator);
0:                 complexDataType.getColumnarDataForComplexType(columnsArray,
0:                     ByteBuffer.wrap(byteArrayOutput.toByteArray()));
0:                 byteArrayOutput.close();
0:               } catch (IOException e) {
0:                 throw new CarbonDataWriterException(
0:                     "Problem while bit packing and writing complex datatype", e);
0:               } catch (KeyGenException e) {
0:                 throw new CarbonDataWriterException(
0:                     "Problem while bit packing and writing complex datatype", e);
1:               }
1: 
0:               for (ArrayList<byte[]> eachColumn : columnsArray) {
0:                 colsAndValues.get(complexTypeIndex++).addAll(eachColumn);
1:               }
0:             } else {
0:               // This case not possible as ComplexType is the last columns
1:             }
1:           }
1:         }
1:       }
1:     }
0:     thread_pool_size = Integer.parseInt(CarbonProperties.getInstance()
0:         .getProperty(CarbonCommonConstants.NUM_CORES_BLOCK_SORT,
0:             CarbonCommonConstants.NUM_CORES_BLOCK_SORT_DEFAULT_VAL));
0:     ExecutorService executorService = Executors.newFixedThreadPool(thread_pool_size);
0:     List<Future<IndexStorage>> submit = new ArrayList<Future<IndexStorage>>(
0:         primitiveDimLens.length + noDictionaryCount + complexColCount);
0:     int i = 0;
0:     int dictionaryColumnCount = -1;
0:     int noDictionaryColumnCount = -1;
0:     for (i = 0; i < dimensionType.length; i++) {
0:       if (dimensionType[i]) {
0:         dictionaryColumnCount++;
0:         if (colGrpModel.isColumnar(dictionaryColumnCount)) {
0:           submit.add(executorService
0:               .submit(new BlockSortThread(i, dataHolders[dictionaryColumnCount].getData(),
0:                   true, isUseInvertedIndex[i])));
0:         } else {
0:           submit.add(
0:               executorService.submit(new ColGroupBlockStorage(dataHolders[dictionaryColumnCount])));
1:         }
0:       } else {
0:         submit.add(executorService.submit(
0:             new BlockSortThread(i, noDictionaryColumnsData[++noDictionaryColumnCount], false, true,
0:                 true, isUseInvertedIndex[i])));
1:       }
1:     }
0:     for (int k = 0; k < complexColCount; k++) {
0:       submit.add(executorService.submit(new BlockSortThread(i++,
0:           colsAndValues.get(k).toArray(new byte[colsAndValues.get(k).size()][]), false, true)));
1:     }
0:     executorService.shutdown();
1:     try {
0:       executorService.awaitTermination(1, TimeUnit.DAYS);
1:     } catch (InterruptedException e) {
1:       LOGGER.error(e, e.getMessage());
1:     }
0:     IndexStorage[] blockStorage =
0:         new IndexStorage[colGrpModel.getNoOfColumnStore() + noDictionaryCount + complexColCount];
1:     try {
0:       for (int k = 0; k < blockStorage.length; k++) {
0:         blockStorage[k] = submit.get(k).get();
1:       }
0:     } catch (Exception e) {
1:       LOGGER.error(e, e.getMessage());
1:     }
0:     byte[] composedNonDictStartKey = null;
0:     byte[] composedNonDictEndKey = null;
0:     if (noDictionaryStartKey != null) {
0:       composedNonDictStartKey =
0:           RemoveDictionaryUtil.packByteBufferIntoSingleByteArray(noDictionaryStartKey);
0:       composedNonDictEndKey =
0:           RemoveDictionaryUtil.packByteBufferIntoSingleByteArray(noDictionaryEndKey);
1:     }
0:     return this.dataWriter
0:         .buildDataNodeHolder(blockStorage, dataHolderLocal, entryCountLocal, startkeyLocal,
0:             endKeyLocal, compressionModel, composedNonDictStartKey, composedNonDictEndKey);
1:   }
1: 
1: 
/////////////////////////////////////////////////////////////////////////
1:         throw new CarbonDataWriterException(e.getMessage(), e);
1:         throw new CarbonDataWriterException(e.getMessage(), e);
/////////////////////////////////////////////////////////////////////////
0:    * @param value
/////////////////////////////////////////////////////////////////////////
0:   // TODO Remove after kettle flow got removed.
0:   private CarbonWriteDataHolder initialiseKeyBlockHolderWithOutKettle(int size) {
0:     CarbonWriteDataHolder keyDataHolder = new CarbonWriteDataHolder();
0:     keyDataHolder.initialiseByteArrayValuesWithOutKettle(size);
0:     return keyDataHolder;
1:   }
1: 
0:   private CarbonWriteDataHolder initialiseKeyBlockHolderForNonDictionary(int size) {
0:     CarbonWriteDataHolder keyDataHolder = new CarbonWriteDataHolder();
0:     keyDataHolder.initialiseByteArrayValuesForNonDictionary(size);
0:     return keyDataHolder;
1:   }
1: 
/////////////////////////////////////////////////////////////////////////
0:         NodeHolder nodeHolder;
0:         if (useKettle) {
0:           nodeHolder = processDataRows(dataRows);
0:         } else {
0:           nodeHolder = processDataRowsWithOutKettle(dataRows);
1:         }
/////////////////////////////////////////////////////////////////////////
1:         throw new CarbonDataWriterException(throwable.getMessage(), throwable);
/////////////////////////////////////////////////////////////////////////
0:             throw new CarbonDataWriterException(throwable.getMessage(), throwable);
commit:cd6a4ff
/////////////////////////////////////////////////////////////////////////
1: /*
0:  * Licensed to the Apache Software Foundation (ASF) under one
0:  * or more contributor license agreements.  See the NOTICE file
0:  * distributed with this work for additional information
0:  * regarding copyright ownership.  The ASF licenses this file
0:  * to you under the Apache License, Version 2.0 (the
0:  * "License"); you may not use this file except in compliance
0:  * with the License.  You may obtain a copy of the License at
1:  *
1:  *    http://www.apache.org/licenses/LICENSE-2.0
1:  *
0:  * Unless required by applicable law or agreed to in writing,
0:  * software distributed under the License is distributed on an
0:  * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
0:  * KIND, either express or implied.  See the License for the
0:  * specific language governing permissions and limitations
0:  * under the License.
1:  */
1: 
1: package org.apache.carbondata.processing.store;
1: 
0: import java.io.ByteArrayOutputStream;
0: import java.io.DataOutputStream;
0: import java.io.File;
1: import java.io.IOException;
0: import java.math.BigDecimal;
0: import java.nio.ByteBuffer;
1: import java.util.ArrayList;
0: import java.util.BitSet;
1: import java.util.List;
0: import java.util.Map;
1: import java.util.concurrent.Callable;
1: import java.util.concurrent.ExecutionException;
1: import java.util.concurrent.ExecutorService;
1: import java.util.concurrent.Executors;
1: import java.util.concurrent.Future;
1: import java.util.concurrent.Semaphore;
1: import java.util.concurrent.TimeUnit;
1: import java.util.concurrent.atomic.AtomicBoolean;
1: import java.util.concurrent.atomic.AtomicInteger;
1: 
1: import org.apache.carbondata.common.logging.LogService;
1: import org.apache.carbondata.common.logging.LogServiceFactory;
0: import org.apache.carbondata.core.carbon.datastore.block.SegmentProperties;
0: import org.apache.carbondata.core.carbon.metadata.CarbonMetadata;
0: import org.apache.carbondata.core.carbon.metadata.schema.table.CarbonTable;
0: import org.apache.carbondata.core.carbon.metadata.schema.table.column.ColumnSchema;
1: import org.apache.carbondata.core.constants.CarbonCommonConstants;
0: import org.apache.carbondata.core.datastorage.store.columnar.BlockIndexerStorageForInt;
0: import org.apache.carbondata.core.datastorage.store.columnar.BlockIndexerStorageForNoInvertedIndex;
0: import org.apache.carbondata.core.datastorage.store.columnar.ColumnGroupModel;
0: import org.apache.carbondata.core.datastorage.store.columnar.IndexStorage;
0: import org.apache.carbondata.core.datastorage.store.compression.ValueCompressionModel;
0: import org.apache.carbondata.core.datastorage.store.dataholder.CarbonWriteDataHolder;
0: import org.apache.carbondata.core.datastorage.util.StoreFactory;
1: import org.apache.carbondata.core.keygenerator.KeyGenException;
0: import org.apache.carbondata.core.keygenerator.KeyGenerator;
0: import org.apache.carbondata.core.keygenerator.columnar.ColumnarSplitter;
1: import org.apache.carbondata.core.keygenerator.columnar.impl.MultiDimKeyVarLengthEquiSplitGenerator;
0: import org.apache.carbondata.core.keygenerator.factory.KeyGeneratorFactory;
1: import org.apache.carbondata.core.util.CarbonProperties;
1: import org.apache.carbondata.core.util.CarbonUtil;
0: import org.apache.carbondata.core.util.DataTypeUtil;
0: import org.apache.carbondata.core.util.ValueCompressionUtil;
1: import org.apache.carbondata.processing.datatypes.GenericDataType;
0: import org.apache.carbondata.processing.mdkeygen.file.FileManager;
0: import org.apache.carbondata.processing.mdkeygen.file.IFileManagerComposite;
0: import org.apache.carbondata.processing.store.colgroup.ColGroupBlockStorage;
0: import org.apache.carbondata.processing.store.colgroup.ColGroupDataHolder;
0: import org.apache.carbondata.processing.store.colgroup.ColGroupMinMax;
0: import org.apache.carbondata.processing.store.colgroup.ColumnDataHolder;
0: import org.apache.carbondata.processing.store.colgroup.DataHolder;
1: import org.apache.carbondata.processing.store.writer.CarbonFactDataWriter;
0: import org.apache.carbondata.processing.store.writer.CarbonFactDataWriterImplForIntIndexAndAggBlock;
0: import org.apache.carbondata.processing.store.writer.NodeHolder;
0: import org.apache.carbondata.processing.store.writer.exception.CarbonDataWriterException;
0: import org.apache.carbondata.processing.util.RemoveDictionaryUtil;
1: 
0: import org.apache.spark.sql.types.Decimal;
1: 
1: /**
1:  * Fact data handler class to handle the fact data
1:  */
1: public class CarbonFactDataHandlerColumnar implements CarbonFactHandler {
1: 
1:   /**
1:    * LOGGER
1:    */
1:   private static final LogService LOGGER =
1:       LogServiceFactory.getLogService(CarbonFactDataHandlerColumnar.class.getName());
1:   /**
1:    * data writer
1:    */
1:   private CarbonFactDataWriter dataWriter;
1:   /**
0:    * File manager
1:    */
0:   private IFileManagerComposite fileManager;
1:   /**
1:    * total number of entries in blocklet
1:    */
1:   private int entryCount;
0:   private Map<Integer, GenericDataType> complexIndexMap;
1:   /**
0:    * measure count
1:    */
0:   private int measureCount;
1:   /**
0:    * measure count
1:    */
0:   private int dimensionCount;
1:   /**
0:    * index of mdkey in incoming rows
1:    */
0:   private int mdKeyIndex;
1:   /**
0:    * blocklet size
1:    */
0:   private int blockletSize;
1:   /**
0:    * mdkeyLength
1:    */
0:   private int mdkeyLength;
1:   /**
0:    * storeLocation
1:    */
0:   private String storeLocation;
1:   /**
0:    * databaseName
1:    */
0:   private String databaseName;
1:   /**
0:    * tableName
1:    */
0:   private String tableName;
1:   /**
0:    * otherMeasureIndex
1:    */
0:   private int[] otherMeasureIndex;
1:   /**
0:    * customMeasureIndex
1:    */
0:   private int[] customMeasureIndex;
1:   /**
0:    * dimLens
1:    */
0:   private int[] dimLens;
1:   /**
0:    * keyGenerator
1:    */
0:   private ColumnarSplitter columnarSplitter;
1:   /**
0:    * keyBlockHolder
1:    */
0:   private CarbonKeyBlockHolder[] keyBlockHolder;
0:   private boolean[] aggKeyBlock;
0:   private boolean[] isNoDictionary;
0:   private boolean isAggKeyBlock;
0:   private boolean enableInvertedIndex;
1:   private long processedDataCount;
1:   /**
0:    * thread pool size to be used for block sort
1:    */
0:   private int thread_pool_size;
1:   /**
0:    * factLevels
1:    */
0:   private int[] surrogateIndex;
1:   /**
0:    * factKeyGenerator
1:    */
0:   private KeyGenerator factKeyGenerator;
1:   /**
0:    * aggKeyGenerator
1:    */
0:   private KeyGenerator keyGenerator;
0:   private KeyGenerator[] complexKeyGenerator;
1:   /**
0:    * maskedByteRanges
1:    */
0:   private int[] maskedByte;
1:   /**
0:    * isDataWritingRequest
1:    */
0:   //    private boolean isDataWritingRequest;
1: 
1:   private ExecutorService producerExecutorService;
1:   private List<Future<Void>> producerExecutorServiceTaskList;
1:   private ExecutorService consumerExecutorService;
1:   private List<Future<Void>> consumerExecutorServiceTaskList;
0:   private List<Object[]> dataRows;
0:   private int noDictionaryCount;
0:   private ColumnGroupModel colGrpModel;
0:   private int[] primitiveDimLens;
0:   private char[] type;
0:   private int[] completeDimLens;
0:   private boolean[] isUseInvertedIndex;
1:   /**
0:    * data file attributes which will used for file construction
1:    */
0:   private CarbonDataFileAttributes carbonDataFileAttributes;
1:   /**
1:    * semaphore which will used for managing node holder objects
1:    */
1:   private Semaphore semaphore;
1:   /**
1:    * counter that incremented for every job submitted to data writer thread
1:    */
1:   private int writerTaskSequenceCounter;
1:   /**
1:    * a private class that will hold the data for blocklets
1:    */
0:   private BlockletDataHolder blockletDataHolder;
1:   /**
0:    * a private class which will take each blocklet in order and write to a file
1:    */
0:   private Consumer consumer;
1:   /**
1:    * number of cores configured
1:    */
1:   private int numberOfCores;
1:   /**
1:    * integer that will be incremented for every new blocklet submitted to producer for processing
1:    * the data and decremented every time consumer fetches the blocklet for writing
1:    */
1:   private AtomicInteger blockletProcessingCount;
1:   /**
1:    * flag to check whether all blocklets have been finished writing
1:    */
1:   private boolean processingComplete;
1:   /**
0:    * data directory location in carbon store path
1:    */
0:   private String carbonDataDirectoryPath;
1:   /**
0:    * no of complex dimensions
1:    */
0:   private int complexColCount;
1: 
1:   /**
0:    * no of column blocks
1:    */
0:   private int columnStoreCount;
1: 
1:   /**
0:    * column schema present in the table
1:    */
0:   private List<ColumnSchema> wrapperColumnSchemaList;
1: 
1:   /**
0:    * boolean to check whether dimension
0:    * is of dictionary type or no dictionary time
1:    */
0:   private boolean[] dimensionType;
1: 
1:   /**
0:    * colCardinality for the merge case.
1:    */
0:   private int[] colCardinality;
1: 
1:   /**
0:    * Segment properties
1:    */
0:   private SegmentProperties segmentProperties;
1:   /**
0:    * flag to check for compaction flow
1:    */
0:   private boolean compactionFlow;
1: 
1:   /**
1:    * CarbonFactDataHandler constructor
1:    */
0:   public CarbonFactDataHandlerColumnar(CarbonFactDataHandlerModel carbonFactDataHandlerModel) {
0:     initParameters(carbonFactDataHandlerModel);
0:     this.dimensionCount = carbonFactDataHandlerModel.getDimensionCount();
0:     this.complexIndexMap = carbonFactDataHandlerModel.getComplexIndexMap();
0:     this.primitiveDimLens = carbonFactDataHandlerModel.getPrimitiveDimLens();
0:     this.isAggKeyBlock = Boolean.parseBoolean(CarbonProperties.getInstance()
0:         .getProperty(CarbonCommonConstants.AGGREAGATE_COLUMNAR_KEY_BLOCK,
0:             CarbonCommonConstants.AGGREAGATE_COLUMNAR_KEY_BLOCK_DEFAULTVALUE));
0:     this.carbonDataDirectoryPath = carbonFactDataHandlerModel.getCarbonDataDirectoryPath();
0:     this.complexColCount = getComplexColsCount();
0:     this.columnStoreCount =
0:         this.colGrpModel.getNoOfColumnStore() + noDictionaryCount + complexColCount;
1: 
0:     this.aggKeyBlock = new boolean[columnStoreCount];
0:     this.isNoDictionary = new boolean[columnStoreCount];
0:     this.isUseInvertedIndex = new boolean[columnStoreCount];
0:     if (null != carbonFactDataHandlerModel.getIsUseInvertedIndex()) {
0:       for (int i = 0; i < isUseInvertedIndex.length; i++) {
0:         if (i < carbonFactDataHandlerModel.getIsUseInvertedIndex().length) {
0:           isUseInvertedIndex[i] = carbonFactDataHandlerModel.getIsUseInvertedIndex()[i];
0:         } else {
0:           isUseInvertedIndex[i] = true;
1:         }
1:       }
1:     }
0:     int noDictStartIndex = this.colGrpModel.getNoOfColumnStore();
0:     // setting true value for dims of high card
0:     for (int i = 0; i < noDictionaryCount; i++) {
0:       this.isNoDictionary[noDictStartIndex + i] = true;
1:     }
1: 
0:     if (isAggKeyBlock) {
0:       int noDictionaryValue = Integer.parseInt(CarbonProperties.getInstance()
0:           .getProperty(CarbonCommonConstants.HIGH_CARDINALITY_VALUE,
0:               CarbonCommonConstants.HIGH_CARDINALITY_VALUE_DEFAULTVALUE));
0:       int[] columnSplits = colGrpModel.getColumnSplit();
0:       int dimCardinalityIndex = 0;
0:       int aggIndex = 0;
0:       for (int i = 0; i < columnSplits.length; i++) {
0:         if (colGrpModel.isColumnar(i) && dimLens[dimCardinalityIndex] < noDictionaryValue) {
0:           this.aggKeyBlock[aggIndex++] = true;
0:           continue;
1:         }
0:         dimCardinalityIndex += columnSplits[i];
0:         aggIndex++;
1:       }
1: 
0:       if (dimensionCount < dimLens.length) {
0:         int allColsCount = getColsCount(dimensionCount);
0:         List<Boolean> aggKeyBlockWithComplex = new ArrayList<Boolean>(allColsCount);
1:         for (int i = 0; i < dimensionCount; i++) {
0:           GenericDataType complexDataType = complexIndexMap.get(i);
1:           if (complexDataType != null) {
0:             complexDataType.fillAggKeyBlock(aggKeyBlockWithComplex, this.aggKeyBlock);
0:           } else {
0:             aggKeyBlockWithComplex.add(this.aggKeyBlock[i]);
1:           }
1:         }
0:         this.aggKeyBlock = new boolean[allColsCount];
0:         for (int i = 0; i < allColsCount; i++) {
0:           this.aggKeyBlock[i] = aggKeyBlockWithComplex.get(i);
1:         }
1:       }
0:       aggKeyBlock = arrangeUniqueBlockType(aggKeyBlock);
1:     }
1:   }
1: 
0:   private void initParameters(CarbonFactDataHandlerModel carbonFactDataHandlerModel) {
0:     this.databaseName = carbonFactDataHandlerModel.getDatabaseName();
0:     this.tableName = carbonFactDataHandlerModel.getTableName();
0:     this.type = carbonFactDataHandlerModel.getAggType();
0:     this.segmentProperties = carbonFactDataHandlerModel.getSegmentProperties();
0:     this.wrapperColumnSchemaList = carbonFactDataHandlerModel.getWrapperColumnSchema();
0:     this.colCardinality = carbonFactDataHandlerModel.getColCardinality();
0:     this.storeLocation = carbonFactDataHandlerModel.getStoreLocation();
0:     this.measureCount = carbonFactDataHandlerModel.getMeasureCount();
0:     this.mdkeyLength = carbonFactDataHandlerModel.getMdKeyLength();
0:     this.mdKeyIndex = carbonFactDataHandlerModel.getMdKeyIndex();
0:     this.noDictionaryCount = carbonFactDataHandlerModel.getNoDictionaryCount();
0:     this.colGrpModel = segmentProperties.getColumnGroupModel();
0:     this.completeDimLens = carbonFactDataHandlerModel.getDimLens();
0:     this.dimLens = this.segmentProperties.getDimColumnsCardinality();
0:     this.carbonDataFileAttributes = carbonFactDataHandlerModel.getCarbonDataFileAttributes();
0:     //TODO need to pass carbon table identifier to metadata
0:     CarbonTable carbonTable = CarbonMetadata.getInstance()
0:         .getCarbonTable(databaseName + CarbonCommonConstants.UNDERSCORE + tableName);
0:     dimensionType =
0:         CarbonUtil.identifyDimensionType(carbonTable.getDimensionByTableName(tableName));
1: 
0:     this.compactionFlow = carbonFactDataHandlerModel.isCompactionFlow();
0:     // in compaction flow the measure with decimal type will come as spark decimal.
0:     // need to convert it to byte array.
0:     if (compactionFlow) {
1:       try {
0:         numberOfCores = Integer.parseInt(CarbonProperties.getInstance()
0:             .getProperty(CarbonCommonConstants.NUM_CORES_COMPACTING,
0:                 CarbonCommonConstants.NUM_CORES_DEFAULT_VAL));
0:       } catch (NumberFormatException exc) {
0:         LOGGER.error("Configured value for property " + CarbonCommonConstants.NUM_CORES_COMPACTING
0:             + "is wrong.Falling back to the default value "
0:             + CarbonCommonConstants.NUM_CORES_DEFAULT_VAL);
0:         numberOfCores = Integer.parseInt(CarbonCommonConstants.NUM_CORES_DEFAULT_VAL);
1:       }
0:     } else {
1:       try {
0:         numberOfCores = Integer.parseInt(CarbonProperties.getInstance()
0:             .getProperty(CarbonCommonConstants.NUM_CORES_LOADING,
0:                 CarbonCommonConstants.NUM_CORES_DEFAULT_VAL));
0:       } catch (NumberFormatException exc) {
0:         LOGGER.error("Configured value for property " + CarbonCommonConstants.NUM_CORES_LOADING
0:             + "is wrong.Falling back to the default value "
0:             + CarbonCommonConstants.NUM_CORES_DEFAULT_VAL);
0:         numberOfCores = Integer.parseInt(CarbonCommonConstants.NUM_CORES_DEFAULT_VAL);
1:       }
1:     }
1: 
1:     blockletProcessingCount = new AtomicInteger(0);
0:     producerExecutorService = Executors.newFixedThreadPool(numberOfCores);
1:     producerExecutorServiceTaskList =
1:         new ArrayList<>(CarbonCommonConstants.DEFAULT_COLLECTION_SIZE);
1:     LOGGER.info("Initializing writer executors");
0:     consumerExecutorService = Executors.newFixedThreadPool(1);
1:     consumerExecutorServiceTaskList = new ArrayList<>(1);
1:     semaphore = new Semaphore(numberOfCores);
0:     blockletDataHolder = new BlockletDataHolder();
0:     consumer = new Consumer(blockletDataHolder);
1:     consumerExecutorServiceTaskList.add(consumerExecutorService.submit(consumer));
1:   }
1: 
0:   private boolean[] arrangeUniqueBlockType(boolean[] aggKeyBlock) {
0:     int counter = 0;
0:     boolean[] uniqueBlock = new boolean[aggKeyBlock.length];
0:     for (int i = 0; i < dimensionType.length; i++) {
0:       if (dimensionType[i]) {
0:         uniqueBlock[i] = aggKeyBlock[counter++];
0:       } else {
0:         uniqueBlock[i] = false;
1:       }
1:     }
0:     return uniqueBlock;
1:   }
1: 
1:   private void setComplexMapSurrogateIndex(int dimensionCount) {
1:     int surrIndex = 0;
1:     for (int i = 0; i < dimensionCount; i++) {
0:       GenericDataType complexDataType = complexIndexMap.get(i);
1:       if (complexDataType != null) {
1:         List<GenericDataType> primitiveTypes = new ArrayList<GenericDataType>();
1:         complexDataType.getAllPrimitiveChildren(primitiveTypes);
1:         for (GenericDataType eachPrimitive : primitiveTypes) {
0:           eachPrimitive.setSurrogateIndex(surrIndex++);
1:         }
0:       } else {
1:         surrIndex++;
1:       }
1:     }
1:   }
1: 
1:   /**
1:    * This method will be used to get and update the step properties which will
1:    * required to run this step
1:    *
1:    * @throws CarbonDataWriterException
1:    */
1:   public void initialise() throws CarbonDataWriterException {
0:     fileManager = new FileManager();
0:     fileManager.setName(new File(this.storeLocation).getName());
1:     setWritingConfiguration();
1:   }
1: 
1:   /**
1:    * below method will be used to add row to store
1:    *
1:    * @param row
1:    * @throws CarbonDataWriterException
1:    */
0:   public void addDataToStore(Object[] row) throws CarbonDataWriterException {
1:     dataRows.add(row);
1:     this.entryCount++;
0:     // if entry count reaches to leaf node size then we are ready to
0:     // write
1:     // this to leaf node file and update the intermediate files
0:     if (this.entryCount == this.blockletSize) {
1:       try {
1:         semaphore.acquire();
0:         producerExecutorServiceTaskList.add(producerExecutorService
0:             .submit(new Producer(blockletDataHolder, dataRows, ++writerTaskSequenceCounter)));
1:         blockletProcessingCount.incrementAndGet();
1:         // set the entry count to zero
1:         processedDataCount += entryCount;
1:         LOGGER.info("Total Number Of records added to store: " + processedDataCount);
0:         dataRows = new ArrayList<>(this.blockletSize);
1:         this.entryCount = 0;
1:       } catch (InterruptedException e) {
1:         LOGGER.error(e, e.getMessage());
1:         throw new CarbonDataWriterException(e.getMessage());
1:       }
1:     }
1:   }
1: 
0:   private NodeHolder processDataRows(List<Object[]> dataRows) throws CarbonDataWriterException {
0:     Object[] max = new Object[measureCount];
0:     Object[] min = new Object[measureCount];
0:     int[] decimal = new int[measureCount];
0:     Object[] uniqueValue = new Object[measureCount];
0:     // to store index of the measure columns which are null
0:     BitSet[] nullValueIndexBitSet = getMeasureNullValueIndexBitSet(measureCount);
0:     for (int i = 0; i < max.length; i++) {
0:       if (type[i] == CarbonCommonConstants.BIG_INT_MEASURE) {
0:         max[i] = Long.MIN_VALUE;
0:       } else if (type[i] == CarbonCommonConstants.SUM_COUNT_VALUE_MEASURE) {
0:         max[i] = -Double.MAX_VALUE;
0:       } else if (type[i] == CarbonCommonConstants.BIG_DECIMAL_MEASURE) {
0:         max[i] = new BigDecimal(0.0);
0:       } else {
0:         max[i] = 0.0;
1:       }
1:     }
0:     for (int i = 0; i < min.length; i++) {
0:       if (type[i] == CarbonCommonConstants.BIG_INT_MEASURE) {
0:         min[i] = Long.MAX_VALUE;
0:         uniqueValue[i] = Long.MIN_VALUE;
0:       } else if (type[i] == CarbonCommonConstants.SUM_COUNT_VALUE_MEASURE) {
0:         min[i] = Double.MAX_VALUE;
0:         uniqueValue[i] = Double.MIN_VALUE;
0:       } else if (type[i] == CarbonCommonConstants.BIG_DECIMAL_MEASURE) {
0:         min[i] = new BigDecimal(Double.MAX_VALUE);
0:         uniqueValue[i] = new BigDecimal(Double.MIN_VALUE);
0:       } else {
0:         min[i] = 0.0;
0:         uniqueValue[i] = 0.0;
1:       }
1:     }
0:     for (int i = 0; i < decimal.length; i++) {
0:       decimal[i] = 0;
1:     }
1: 
0:     byte[] startKey = null;
0:     byte[] endKey = null;
0:     byte[] noDictStartKey = null;
0:     byte[] noDictEndKey = null;
0:     CarbonWriteDataHolder[] dataHolder = initialiseDataHolder(dataRows.size());
0:     CarbonWriteDataHolder keyDataHolder = initialiseKeyBlockHolder(dataRows.size());
0:     CarbonWriteDataHolder noDictionaryKeyDataHolder = null;
0:     if ((noDictionaryCount + complexColCount) > 0) {
0:       noDictionaryKeyDataHolder = initialiseKeyBlockHolder(dataRows.size());
1:     }
1: 
0:     for (int count = 0; count < dataRows.size(); count++) {
0:       Object[] row = dataRows.get(count);
0:       byte[] mdKey = (byte[]) row[this.mdKeyIndex];
0:       byte[] noDictionaryKey = null;
0:       if (noDictionaryCount > 0 || complexIndexMap.size() > 0) {
0:         noDictionaryKey = (byte[]) row[this.mdKeyIndex - 1];
1:       }
0:       ByteBuffer byteBuffer = null;
0:       byte[] b = null;
0:       if (count == 0) {
0:         startKey = mdKey;
0:         noDictStartKey = noDictionaryKey;
1:       }
0:       endKey = mdKey;
0:       noDictEndKey = noDictionaryKey;
0:       // add to key store
0:       if (mdKey.length > 0) {
0:         keyDataHolder.setWritableByteArrayValueByIndex(count, mdKey);
1:       }
0:       // for storing the byte [] for high card.
0:       if (noDictionaryCount > 0 || complexIndexMap.size() > 0) {
0:         noDictionaryKeyDataHolder.setWritableByteArrayValueByIndex(count, noDictionaryKey);
1:       }
0:       //Add all columns to keyDataHolder
0:       keyDataHolder.setWritableByteArrayValueByIndex(count, this.mdKeyIndex, row);
0:       // CHECKSTYLE:OFF Approval No:Approval-351
0:       for (int k = 0; k < otherMeasureIndex.length; k++) {
0:         if (type[otherMeasureIndex[k]] == CarbonCommonConstants.BIG_INT_MEASURE) {
0:           if (null == row[otherMeasureIndex[k]]) {
0:             nullValueIndexBitSet[otherMeasureIndex[k]].set(count);
0:             dataHolder[otherMeasureIndex[k]].setWritableLongValueByIndex(count, 0L);
0:           } else {
0:             dataHolder[otherMeasureIndex[k]]
0:                 .setWritableLongValueByIndex(count, row[otherMeasureIndex[k]]);
1:           }
0:         } else {
0:           if (null == row[otherMeasureIndex[k]]) {
0:             nullValueIndexBitSet[otherMeasureIndex[k]].set(count);
0:             dataHolder[otherMeasureIndex[k]].setWritableDoubleValueByIndex(count, 0.0);
0:           } else {
0:             dataHolder[otherMeasureIndex[k]]
0:                 .setWritableDoubleValueByIndex(count, row[otherMeasureIndex[k]]);
1:           }
1:         }
1:       }
0:       calculateMaxMin(max, min, decimal, otherMeasureIndex, row);
0:       for (int i = 0; i < customMeasureIndex.length; i++) {
0:         if (null == row[customMeasureIndex[i]]
0:             && type[customMeasureIndex[i]] == CarbonCommonConstants.BIG_DECIMAL_MEASURE) {
0:           BigDecimal val = BigDecimal.valueOf(0);
0:           b = DataTypeUtil.bigDecimalToByte(val);
0:           nullValueIndexBitSet[customMeasureIndex[i]].set(count);
0:         } else {
0:           if (this.compactionFlow) {
0:             BigDecimal bigDecimal = ((Decimal) row[customMeasureIndex[i]]).toJavaBigDecimal();
0:             b = DataTypeUtil.bigDecimalToByte(bigDecimal);
0:           } else {
0:             b = (byte[]) row[customMeasureIndex[i]];
1:           }
1:         }
0:         byteBuffer = ByteBuffer.allocate(b.length + CarbonCommonConstants.INT_SIZE_IN_BYTE);
0:         byteBuffer.putInt(b.length);
0:         byteBuffer.put(b);
0:         byteBuffer.flip();
0:         b = byteBuffer.array();
0:         dataHolder[customMeasureIndex[i]].setWritableByteArrayValueByIndex(count, b);
1:       }
0:       calculateMaxMin(max, min, decimal, customMeasureIndex, row);
1:     }
0:     calculateUniqueValue(min, uniqueValue);
0:     byte[][] byteArrayValues = keyDataHolder.getByteArrayValues().clone();
0:     byte[][] noDictionaryValueHolder = null;
0:     if ((noDictionaryCount + complexColCount) > 0) {
0:       noDictionaryValueHolder = noDictionaryKeyDataHolder.getByteArrayValues();
1:     }
0:     ValueCompressionModel compressionModel = ValueCompressionUtil
0:         .getValueCompressionModel(max, min, decimal, uniqueValue, type, new byte[max.length]);
0:     byte[][] writableMeasureDataArray =
0:         StoreFactory.createDataStore(compressionModel).getWritableMeasureDataArray(dataHolder)
0:             .clone();
0:     NodeHolder nodeHolder =
0:         getNodeHolderObject(writableMeasureDataArray, byteArrayValues, dataRows.size(), startKey,
0:             endKey, compressionModel, noDictionaryValueHolder, noDictStartKey, noDictEndKey);
0:     nodeHolder.setMeasureNullValueIndex(nullValueIndexBitSet);
0:     LOGGER.info("Number Of records processed: " + dataRows.size());
0:     return nodeHolder;
1:   }
1: 
0:   private NodeHolder getNodeHolderObject(byte[][] dataHolderLocal, byte[][] byteArrayValues,
0:       int entryCountLocal, byte[] startkeyLocal, byte[] endKeyLocal,
0:       ValueCompressionModel compressionModel, byte[][] noDictionaryData,
0:       byte[] noDictionaryStartKey, byte[] noDictionaryEndKey)
0:       throws CarbonDataWriterException {
0:     byte[][][] noDictionaryColumnsData = null;
0:     List<ArrayList<byte[]>> colsAndValues = new ArrayList<ArrayList<byte[]>>();
0:     int complexColCount = getComplexColsCount();
1: 
0:     for (int i = 0; i < complexColCount; i++) {
0:       colsAndValues.add(new ArrayList<byte[]>());
1:     }
0:     int noOfColumn = colGrpModel.getNoOfColumnStore();
0:     DataHolder[] dataHolders = getDataHolders(noOfColumn, byteArrayValues.length);
0:     for (int i = 0; i < byteArrayValues.length; i++) {
0:       byte[][] splitKey = columnarSplitter.splitKey(byteArrayValues[i]);
1: 
0:       for (int j = 0; j < splitKey.length; j++) {
0:         dataHolders[j].addData(splitKey[j], i);
1:       }
1:     }
0:     if (noDictionaryCount > 0 || complexIndexMap.size() > 0) {
0:       noDictionaryColumnsData = new byte[noDictionaryCount][noDictionaryData.length][];
0:       for (int i = 0; i < noDictionaryData.length; i++) {
0:         int complexColumnIndex = primitiveDimLens.length + noDictionaryCount;
0:         byte[][] splitKey = RemoveDictionaryUtil
0:             .splitNoDictionaryKey(noDictionaryData[i], noDictionaryCount + complexIndexMap.size());
1: 
0:         int complexTypeIndex = 0;
0:         for (int j = 0; j < splitKey.length; j++) {
0:           //nodictionary Columns
0:           if (j < noDictionaryCount) {
0:             noDictionaryColumnsData[j][i] = splitKey[j];
1:           }
0:           //complex types
0:           else {
0:             // Need to write columnar block from complex byte array
0:             int index = complexColumnIndex - noDictionaryCount;
0:             GenericDataType complexDataType = complexIndexMap.get(index);
0:             complexColumnIndex++;
1:             if (complexDataType != null) {
0:               List<ArrayList<byte[]>> columnsArray = new ArrayList<ArrayList<byte[]>>();
0:               for (int k = 0; k < complexDataType.getColsCount(); k++) {
0:                 columnsArray.add(new ArrayList<byte[]>());
1:               }
1: 
1:               try {
0:                 ByteBuffer complexDataWithoutBitPacking = ByteBuffer.wrap(splitKey[j]);
0:                 byte[] complexTypeData = new byte[complexDataWithoutBitPacking.getShort()];
0:                 complexDataWithoutBitPacking.get(complexTypeData);
1: 
0:                 ByteBuffer byteArrayInput = ByteBuffer.wrap(complexTypeData);
0:                 ByteArrayOutputStream byteArrayOutput = new ByteArrayOutputStream();
0:                 DataOutputStream dataOutputStream = new DataOutputStream(byteArrayOutput);
0:                 complexDataType
0:                     .parseAndBitPack(byteArrayInput, dataOutputStream, this.complexKeyGenerator);
0:                 complexDataType.getColumnarDataForComplexType(columnsArray,
0:                     ByteBuffer.wrap(byteArrayOutput.toByteArray()));
0:                 byteArrayOutput.close();
0:               } catch (IOException e) {
0:                 throw new CarbonDataWriterException(
0:                     "Problem while bit packing and writing complex datatype", e);
0:               } catch (KeyGenException e) {
0:                 throw new CarbonDataWriterException(
0:                     "Problem while bit packing and writing complex datatype", e);
1:               }
1: 
0:               for (ArrayList<byte[]> eachColumn : columnsArray) {
0:                 colsAndValues.get(complexTypeIndex++).addAll(eachColumn);
1:               }
0:             } else {
0:               // This case not possible as ComplexType is the last columns
1:             }
1:           }
1:         }
1:       }
1:     }
0:     thread_pool_size = Integer.parseInt(CarbonProperties.getInstance()
0:         .getProperty(CarbonCommonConstants.NUM_CORES_BLOCK_SORT,
0:             CarbonCommonConstants.NUM_CORES_BLOCK_SORT_DEFAULT_VAL));
0:     ExecutorService executorService = Executors.newFixedThreadPool(thread_pool_size);
0:     List<Future<IndexStorage>> submit = new ArrayList<Future<IndexStorage>>(
0:         primitiveDimLens.length + noDictionaryCount + complexColCount);
0:     int i = 0;
0:     int dictionaryColumnCount = -1;
0:     int noDictionaryColumnCount = -1;
0:     for (i = 0; i < dimensionType.length; i++) {
0:       if (dimensionType[i]) {
0:         dictionaryColumnCount++;
0:         if (colGrpModel.isColumnar(dictionaryColumnCount)) {
0:           submit.add(executorService
0:               .submit(new BlockSortThread(i, dataHolders[dictionaryColumnCount].getData(),
0:                   true, isUseInvertedIndex[i])));
0:         } else {
0:           submit.add(
0:               executorService.submit(new ColGroupBlockStorage(dataHolders[dictionaryColumnCount])));
1:         }
0:       } else {
0:         submit.add(executorService.submit(
0:             new BlockSortThread(i, noDictionaryColumnsData[++noDictionaryColumnCount], false, true,
0:                 true, isUseInvertedIndex[i])));
1:       }
1:     }
0:     for (int k = 0; k < complexColCount; k++) {
0:       submit.add(executorService.submit(new BlockSortThread(i++,
0:           colsAndValues.get(k).toArray(new byte[colsAndValues.get(k).size()][]), false, true)));
1:     }
0:     executorService.shutdown();
1:     try {
0:       executorService.awaitTermination(1, TimeUnit.DAYS);
1:     } catch (InterruptedException e) {
1:       LOGGER.error(e, e.getMessage());
1:     }
0:     IndexStorage[] blockStorage =
0:         new IndexStorage[colGrpModel.getNoOfColumnStore() + noDictionaryCount + complexColCount];
1:     try {
0:       for (int k = 0; k < blockStorage.length; k++) {
0:         blockStorage[k] = submit.get(k).get();
1:       }
0:     } catch (Exception e) {
1:       LOGGER.error(e, e.getMessage());
1:     }
0:     return this.dataWriter
0:         .buildDataNodeHolder(blockStorage, dataHolderLocal, entryCountLocal, startkeyLocal,
0:             endKeyLocal, compressionModel, noDictionaryStartKey, noDictionaryEndKey);
1:   }
1: 
1:   /**
0:    * DataHolder will have all row mdkey data
1:    *
0:    * @param noOfColumn : no of column participated in mdkey
0:    * @param noOfRow    : total no of row
0:    * @return : dataholder
1:    */
0:   private DataHolder[] getDataHolders(int noOfColumn, int noOfRow) {
0:     DataHolder[] dataHolders = new DataHolder[noOfColumn];
0:     int colGrpId = -1;
0:     for (int colGrp = 0; colGrp < noOfColumn; colGrp++) {
0:       if (colGrpModel.isColumnar(colGrp)) {
0:         dataHolders[colGrp] = new ColumnDataHolder(noOfRow);
0:       } else {
0:         ColGroupMinMax colGrpMinMax = new ColGroupMinMax(segmentProperties, ++colGrpId);
0:         dataHolders[colGrp] =
0:             new ColGroupDataHolder(this.columnarSplitter.getBlockKeySize()[colGrp], noOfRow,
0:                 colGrpMinMax);
1:       }
1:     }
0:     return dataHolders;
1:   }
1: 
1:   /**
1:    * below method will be used to finish the data handler
1:    *
1:    * @throws CarbonDataWriterException
1:    */
1:   public void finish() throws CarbonDataWriterException {
1:     // still some data is present in stores if entryCount is more
1:     // than 0
0:     if (this.entryCount > 0) {
0:       producerExecutorServiceTaskList.add(producerExecutorService
0:           .submit(new Producer(blockletDataHolder, dataRows, ++writerTaskSequenceCounter)));
1:       blockletProcessingCount.incrementAndGet();
1:       processedDataCount += entryCount;
1:     }
1:     closeWriterExecutionService(producerExecutorService);
1:     processWriteTaskSubmitList(producerExecutorServiceTaskList);
1:     processingComplete = true;
1:   }
1: 
1:   /**
1:    * This method will close writer execution service and get the node holders and
1:    * add them to node holder list
1:    *
1:    * @param service the service to shutdown
1:    * @throws CarbonDataWriterException
1:    */
1:   private void closeWriterExecutionService(ExecutorService service)
0:       throws CarbonDataWriterException {
1:     try {
1:       service.shutdown();
1:       service.awaitTermination(1, TimeUnit.DAYS);
1:     } catch (InterruptedException e) {
1:       LOGGER.error(e, e.getMessage());
1:       throw new CarbonDataWriterException(e.getMessage());
1:     }
1:   }
1: 
1:   /**
1:    * This method will iterate through future task list and check if any exception
1:    * occurred during the thread execution
1:    *
1:    * @param taskList
1:    * @throws CarbonDataWriterException
1:    */
1:   private void processWriteTaskSubmitList(List<Future<Void>> taskList)
0:       throws CarbonDataWriterException {
1:     for (int i = 0; i < taskList.size(); i++) {
1:       try {
1:         taskList.get(i).get();
1:       } catch (InterruptedException e) {
1:         LOGGER.error(e, e.getMessage());
1:         throw new CarbonDataWriterException(e.getMessage());
0:       } catch (ExecutionException e) {
1:         LOGGER.error(e, e.getMessage());
1:         throw new CarbonDataWriterException(e.getMessage());
1:       }
1:     }
1:   }
1: 
0:   private byte[] getAggregateTableMdkey(byte[] maksedKey) throws CarbonDataWriterException {
0:     long[] keyArray = this.factKeyGenerator.getKeyArray(maksedKey, maskedByte);
1: 
0:     int[] aggSurrogateKey = new int[surrogateIndex.length];
1: 
0:     for (int j = 0; j < aggSurrogateKey.length; j++) {
0:       aggSurrogateKey[j] = (int) keyArray[surrogateIndex[j]];
1:     }
1: 
1:     try {
0:       return keyGenerator.generateKey(aggSurrogateKey);
0:     } catch (KeyGenException e) {
0:       throw new CarbonDataWriterException("Problem while generating the mdkeyfor aggregate ", e);
1:     }
1:   }
1: 
0:   private int getColsCount(int columnSplit) {
0:     int count = 0;
0:     for (int i = 0; i < columnSplit; i++) {
0:       GenericDataType complexDataType = complexIndexMap.get(i);
1:       if (complexDataType != null) {
0:         count += complexDataType.getColsCount();
0:       } else count++;
1:     }
0:     return count;
1:   }
1: 
0:   private int getComplexColsCount() {
0:     int count = 0;
1:     for (int i = 0; i < dimensionCount; i++) {
0:       GenericDataType complexDataType = complexIndexMap.get(i);
1:       if (complexDataType != null) {
0:         count += complexDataType.getColsCount();
1:       }
1:     }
0:     return count;
1:   }
1: 
1:   /**
1:    * below method will be used to close the handler
1:    */
1:   public void closeHandler() throws CarbonDataWriterException {
1:     if (null != this.dataWriter) {
1:       // wait until all blocklets have been finished writing
1:       while (blockletProcessingCount.get() > 0) {
1:         try {
1:           Thread.sleep(50);
1:         } catch (InterruptedException e) {
1:           throw new CarbonDataWriterException(e.getMessage());
1:         }
1:       }
1:       consumerExecutorService.shutdownNow();
1:       processWriteTaskSubmitList(consumerExecutorServiceTaskList);
0:       this.dataWriter.writeBlockletInfoToFile();
1:       LOGGER.info("All blocklets have been finished writing");
1:       // close all the open stream for both the files
1:       this.dataWriter.closeWriter();
1:     }
1:     this.dataWriter = null;
0:     this.keyBlockHolder = null;
1:   }
1: 
1:   /**
0:    * This method will be used to update the max value for each measure
1:    */
0:   private void calculateMaxMin(Object[] max, Object[] min, int[] decimal, int[] msrIndex,
0:       Object[] row) {
0:     // Update row level min max
0:     for (int i = 0; i < msrIndex.length; i++) {
0:       int count = msrIndex[i];
0:       if (row[count] != null) {
0:         if (type[count] == CarbonCommonConstants.SUM_COUNT_VALUE_MEASURE) {
0:           double value = (double) row[count];
0:           double maxVal = (double) max[count];
0:           double minVal = (double) min[count];
0:           max[count] = (maxVal > value ? max[count] : value);
0:           min[count] = (minVal < value ? min[count] : value);
0:           int num = (value % 1 == 0) ? 0 : CarbonCommonConstants.CARBON_DECIMAL_POINTERS_DEFAULT;
0:           decimal[count] = (decimal[count] > num ? decimal[count] : num);
0:         } else if (type[count] == CarbonCommonConstants.BIG_INT_MEASURE) {
0:           long value = (long) row[count];
0:           long maxVal = (long) max[count];
0:           long minVal = (long) min[count];
0:           max[count] = (maxVal > value ? max[count] : value);
0:           min[count] = (minVal < value ? min[count] : value);
0:           int num = (value % 1 == 0) ? 0 : CarbonCommonConstants.CARBON_DECIMAL_POINTERS_DEFAULT;
0:           decimal[count] = (decimal[count] > num ? decimal[count] : num);
0:         } else if (type[count] == CarbonCommonConstants.BIG_DECIMAL_MEASURE) {
0:           byte[] buff = null;
0:           // in compaction flow the measure with decimal type will come as spark decimal.
0:           // need to convert it to byte array.
0:           if (this.compactionFlow) {
0:             BigDecimal bigDecimal = ((Decimal) row[count]).toJavaBigDecimal();
0:             buff = DataTypeUtil.bigDecimalToByte(bigDecimal);
0:           } else {
0:             buff = (byte[]) row[count];
1:           }
0:           BigDecimal value = DataTypeUtil.byteToBigDecimal(buff);
0:           BigDecimal minVal = (BigDecimal) min[count];
0:           min[count] = minVal.min(value);
1:         }
1:       }
1:     }
1:   }
1: 
1:   /**
0:    * This method will calculate the unique value which will be used as storage
0:    * key for null values of measures
1:    *
0:    * @param minValue
0:    * @param uniqueValue
1:    */
0:   private void calculateUniqueValue(Object[] minValue, Object[] uniqueValue) {
0:     for (int i = 0; i < measureCount; i++) {
0:       if (type[i] == CarbonCommonConstants.BIG_INT_MEASURE) {
0:         uniqueValue[i] = (long) minValue[i] - 1;
0:       } else if (type[i] == CarbonCommonConstants.BIG_DECIMAL_MEASURE) {
0:         BigDecimal val = (BigDecimal) minValue[i];
0:         uniqueValue[i] = (val.subtract(new BigDecimal(1.0)));
0:       } else {
0:         uniqueValue[i] = (double) minValue[i] - 1;
1:       }
1:     }
1:   }
1: 
1:   /**
1:    * Below method will be to configure fact file writing configuration
1:    *
1:    * @throws CarbonDataWriterException
1:    */
1:   private void setWritingConfiguration() throws CarbonDataWriterException {
1:     // get blocklet size
0:     this.blockletSize = Integer.parseInt(CarbonProperties.getInstance()
1:         .getProperty(CarbonCommonConstants.BLOCKLET_SIZE,
1:             CarbonCommonConstants.BLOCKLET_SIZE_DEFAULT_VAL));
0:     LOGGER.info("Blocklet Size: " + blockletSize);
0:     dataRows = new ArrayList<>(this.blockletSize);
1:     int dimSet =
1:         Integer.parseInt(CarbonCommonConstants.DIMENSION_SPLIT_VALUE_IN_COLUMNAR_DEFAULTVALUE);
0:     // if atleast one dimension is present then initialize column splitter otherwise null
0:     int noOfColStore = colGrpModel.getNoOfColumnStore();
0:     int[] keyBlockSize = new int[noOfColStore + complexColCount];
1: 
0:     if (dimLens.length > 0) {
0:       //Using Variable length variable split generator
0:       //This will help in splitting mdkey to columns. variable split is required because all
0:       // columns which are part of
0:       //row store will be in single column store
0:       //e.g if {0,1,2,3,4,5} is dimension and {0,1,2) is row store dimension
0:       //than below splitter will return column as {0,1,2}{3}{4}{5}
0:       this.columnarSplitter = this.segmentProperties.getFixedLengthKeySplitter();
0:       System.arraycopy(columnarSplitter.getBlockKeySize(), 0, keyBlockSize, 0, noOfColStore);
0:       this.keyBlockHolder =
0:           new CarbonKeyBlockHolder[this.columnarSplitter.getBlockKeySize().length];
0:     } else {
0:       this.keyBlockHolder = new CarbonKeyBlockHolder[0];
1:     }
0:     this.complexKeyGenerator = new KeyGenerator[completeDimLens.length];
0:     for (int i = 0; i < completeDimLens.length; i++) {
0:       complexKeyGenerator[i] =
0:           KeyGeneratorFactory.getKeyGenerator(new int[] { completeDimLens[i] });
1:     }
1: 
0:     for (int i = 0; i < keyBlockHolder.length; i++) {
0:       this.keyBlockHolder[i] = new CarbonKeyBlockHolder(blockletSize);
0:       this.keyBlockHolder[i].resetCounter();
1:     }
1: 
1:     // agg type
1:     List<Integer> otherMeasureIndexList =
1:         new ArrayList<Integer>(CarbonCommonConstants.DEFAULT_COLLECTION_SIZE);
1:     List<Integer> customMeasureIndexList =
1:         new ArrayList<Integer>(CarbonCommonConstants.DEFAULT_COLLECTION_SIZE);
1:     for (int j = 0; j < type.length; j++) {
0:       if (type[j] != CarbonCommonConstants.BYTE_VALUE_MEASURE
0:           && type[j] != CarbonCommonConstants.BIG_DECIMAL_MEASURE) {
1:         otherMeasureIndexList.add(j);
0:       } else {
1:         customMeasureIndexList.add(j);
1:       }
1:     }
0:     otherMeasureIndex = new int[otherMeasureIndexList.size()];
0:     customMeasureIndex = new int[customMeasureIndexList.size()];
1:     for (int i = 0; i < otherMeasureIndex.length; i++) {
1:       otherMeasureIndex[i] = otherMeasureIndexList.get(i);
1:     }
0:     for (int i = 0; i < customMeasureIndex.length; i++) {
1:       customMeasureIndex[i] = customMeasureIndexList.get(i);
1:     }
0:     setComplexMapSurrogateIndex(this.dimensionCount);
1:     int[] blockKeySize = getBlockKeySizeWithComplexTypes(new MultiDimKeyVarLengthEquiSplitGenerator(
0:         CarbonUtil.getIncrementedCardinalityFullyFilled(completeDimLens.clone()), (byte) dimSet)
1:         .getBlockKeySize());
0:     System.arraycopy(blockKeySize, noOfColStore, keyBlockSize, noOfColStore,
0:         blockKeySize.length - noOfColStore);
0:     this.dataWriter =
0:         getFactDataWriter(this.storeLocation, this.measureCount, this.mdkeyLength, this.tableName,
0:             fileManager, keyBlockSize);
0:     this.dataWriter.setIsNoDictionary(isNoDictionary);
1:     // initialize the channel;
1:     this.dataWriter.initializeWriter();
0:     //initializeColGrpMinMax();
1:   }
1: 
1:   /**
1:    * This method combines primitive dimensions with complex metadata columns
1:    *
1:    * @param primitiveBlockKeySize
1:    * @return all dimensions cardinality including complex dimension metadata column
1:    */
1:   private int[] getBlockKeySizeWithComplexTypes(int[] primitiveBlockKeySize) {
0:     int allColsCount = getComplexColsCount();
0:     int[] blockKeySizeWithComplexTypes =
0:         new int[this.colGrpModel.getNoOfColumnStore() + allColsCount];
1: 
1:     List<Integer> blockKeySizeWithComplex =
1:         new ArrayList<Integer>(blockKeySizeWithComplexTypes.length);
0:     for (int i = 0; i < this.dimensionCount; i++) {
0:       GenericDataType complexDataType = complexIndexMap.get(i);
1:       if (complexDataType != null) {
1:         complexDataType.fillBlockKeySize(blockKeySizeWithComplex, primitiveBlockKeySize);
0:       } else {
1:         blockKeySizeWithComplex.add(primitiveBlockKeySize[i]);
1:       }
1:     }
1:     for (int i = 0; i < blockKeySizeWithComplexTypes.length; i++) {
1:       blockKeySizeWithComplexTypes[i] = blockKeySizeWithComplex.get(i);
1:     }
1: 
1:     return blockKeySizeWithComplexTypes;
1:   }
1: 
0:   private CarbonWriteDataHolder initialiseKeyBlockHolder(int size) {
0:     CarbonWriteDataHolder keyDataHolder = new CarbonWriteDataHolder();
0:     keyDataHolder.initialiseByteArrayValues(size);
0:     return keyDataHolder;
1:   }
1: 
0:   private CarbonWriteDataHolder[] initialiseDataHolder(int size) {
0:     CarbonWriteDataHolder[] dataHolder = new CarbonWriteDataHolder[this.measureCount];
1:     for (int i = 0; i < otherMeasureIndex.length; i++) {
0:       dataHolder[otherMeasureIndex[i]] = new CarbonWriteDataHolder();
0:       if (type[otherMeasureIndex[i]] == CarbonCommonConstants.BIG_INT_MEASURE) {
0:         dataHolder[otherMeasureIndex[i]].initialiseLongValues(size);
0:       } else {
0:         dataHolder[otherMeasureIndex[i]].initialiseDoubleValues(size);
1:       }
1:     }
0:     for (int i = 0; i < customMeasureIndex.length; i++) {
0:       dataHolder[customMeasureIndex[i]] = new CarbonWriteDataHolder();
0:       dataHolder[customMeasureIndex[i]].initialiseByteArrayValues(size);
1:     }
0:     return dataHolder;
1:   }
1: 
1:   /**
0:    * Below method will be used to get the bit set array for
0:    * all the measure, which will store the indexes which are null
1:    *
0:    * @param measureCount
0:    * @return bit set to store null value index
1:    */
0:   private BitSet[] getMeasureNullValueIndexBitSet(int measureCount) {
0:     BitSet[] nullvalueIndexBitset = new BitSet[measureCount];
0:     // creating a bit set for all the measure column
0:     nullvalueIndexBitset = new BitSet[this.measureCount];
0:     for (int i = 0; i < nullvalueIndexBitset.length; i++) {
0:       // bitset size will be blocklet size
0:       nullvalueIndexBitset[i] = new BitSet(this.blockletSize);
1:     }
0:     return nullvalueIndexBitset;
1:   }
1: 
0:   private CarbonFactDataWriter<?> getFactDataWriter(String storeLocation, int measureCount,
0:       int mdKeyLength, String tableName, IFileManagerComposite fileManager, int[] keyBlockSize) {
0:     return new CarbonFactDataWriterImplForIntIndexAndAggBlock(storeLocation, measureCount,
0:         mdKeyLength, tableName, fileManager, keyBlockSize, aggKeyBlock, isComplexTypes(),
0:         noDictionaryCount, carbonDataFileAttributes, databaseName, wrapperColumnSchemaList,
0:         noDictionaryCount, dimensionType, carbonDataDirectoryPath, colCardinality,
0:         segmentProperties);
1:   }
1: 
0:   private boolean[] isComplexTypes() {
0:     int noOfColumn = colGrpModel.getNoOfColumnStore() + noDictionaryCount + complexIndexMap.size();
0:     int allColsCount = getColsCount(noOfColumn);
0:     boolean[] isComplexType = new boolean[allColsCount];
1: 
0:     List<Boolean> complexTypesList = new ArrayList<Boolean>(allColsCount);
0:     for (int i = 0; i < noOfColumn; i++) {
0:       GenericDataType complexDataType = complexIndexMap.get(i - noDictionaryCount);
1:       if (complexDataType != null) {
0:         int count = complexDataType.getColsCount();
0:         for (int j = 0; j < count; j++) {
0:           complexTypesList.add(true);
1:         }
0:       } else {
0:         complexTypesList.add(false);
1:       }
1:     }
0:     for (int i = 0; i < allColsCount; i++) {
0:       isComplexType[i] = complexTypesList.get(i);
1:     }
1: 
0:     return isComplexType;
1:   }
1: 
1:   /**
1:    * This method will reset the block processing count
1:    */
1:   private void resetBlockletProcessingCount() {
1:     blockletProcessingCount.set(0);
1:   }
1: 
1:   /**
0:    * This class will hold the holder objects and manage producer and consumer for reading
0:    * and writing the blocklet data
1:    */
0:   private final class BlockletDataHolder {
1:     /**
0:      * array of blocklet data holder objects
1:      */
0:     private NodeHolder[] nodeHolders;
1:     /**
1:      * flag to check whether the producer has completed processing for holder
1:      * object which is required to be picked form an index
1:      */
1:     private AtomicBoolean available;
1:     /**
1:      * index from which data node holder object needs to be picked for writing
1:      */
1:     private int currentIndex;
1: 
0:     private BlockletDataHolder() {
0:       nodeHolders = new NodeHolder[numberOfCores];
1:       available = new AtomicBoolean(false);
1:     }
1: 
1:     /**
1:      * @return a node holder object
1:      * @throws InterruptedException if consumer thread is interrupted
1:      */
0:     public synchronized NodeHolder get() throws InterruptedException {
0:       NodeHolder nodeHolder = nodeHolders[currentIndex];
1:       // if node holder is null means producer thread processing the data which has to
1:       // be inserted at this current index has not completed yet
0:       if (null == nodeHolder && !processingComplete) {
1:         available.set(false);
1:       }
1:       while (!available.get()) {
1:         wait();
1:       }
0:       nodeHolder = nodeHolders[currentIndex];
0:       nodeHolders[currentIndex] = null;
1:       currentIndex++;
1:       // reset current index when it reaches length of node holder array
0:       if (currentIndex >= nodeHolders.length) {
1:         currentIndex = 0;
1:       }
0:       return nodeHolder;
1:     }
1: 
1:     /**
0:      * @param nodeHolder
1:      * @param index
1:      */
0:     public synchronized void put(NodeHolder nodeHolder, int index) {
0:       nodeHolders[index] = nodeHolder;
1:       // notify the consumer thread when index at which object is to be inserted
1:       // becomes equal to current index from where data has to be picked for writing
1:       if (index == currentIndex) {
1:         available.set(true);
1:         notifyAll();
1:       }
1:     }
1:   }
1: 
1:   /**
1:    * Producer which will process data equivalent to 1 blocklet size
1:    */
1:   private final class Producer implements Callable<Void> {
1: 
0:     private BlockletDataHolder blockletDataHolder;
0:     private List<Object[]> dataRows;
0:     private int sequenceNumber;
1: 
0:     private Producer(BlockletDataHolder blockletDataHolder, List<Object[]> dataRows,
0:         int sequenceNumber) {
0:       this.blockletDataHolder = blockletDataHolder;
1:       this.dataRows = dataRows;
0:       this.sequenceNumber = sequenceNumber;
1:     }
1: 
1:     /**
1:      * Computes a result, or throws an exception if unable to do so.
1:      *
1:      * @return computed result
1:      * @throws Exception if unable to compute a result
1:      */
1:     @Override public Void call() throws Exception {
1:       try {
0:         NodeHolder nodeHolder = processDataRows(dataRows);
1:         // insert the object in array according to sequence number
0:         int indexInNodeHolderArray = (sequenceNumber - 1) % numberOfCores;
0:         blockletDataHolder.put(nodeHolder, indexInNodeHolderArray);
1:         return null;
1:       } catch (Throwable throwable) {
1:         consumerExecutorService.shutdownNow();
1:         resetBlockletProcessingCount();
1:         throw new CarbonDataWriterException(throwable.getMessage());
1:       }
1:     }
1:   }
1: 
1:   /**
1:    * Consumer class will get one blocklet data at a time and submit for writing
1:    */
1:   private final class Consumer implements Callable<Void> {
1: 
0:     private BlockletDataHolder blockletDataHolder;
1: 
0:     private Consumer(BlockletDataHolder blockletDataHolder) {
0:       this.blockletDataHolder = blockletDataHolder;
1:     }
1: 
1:     /**
1:      * Computes a result, or throws an exception if unable to do so.
1:      *
1:      * @return computed result
1:      * @throws Exception if unable to compute a result
1:      */
1:     @Override public Void call() throws Exception {
1:       while (!processingComplete || blockletProcessingCount.get() > 0) {
0:         NodeHolder nodeHolder = null;
1:         try {
0:           nodeHolder = blockletDataHolder.get();
0:           if (null != nodeHolder) {
0:             dataWriter.writeBlockletData(nodeHolder);
1:           }
1:           blockletProcessingCount.decrementAndGet();
1:         } catch (Throwable throwable) {
1:           if (!processingComplete || blockletProcessingCount.get() > 0) {
1:             producerExecutorService.shutdownNow();
1:             resetBlockletProcessingCount();
1:             throw new CarbonDataWriterException(throwable.getMessage());
1:           }
1:         } finally {
1:           semaphore.release();
1:         }
1:       }
1:       return null;
1:     }
1:   }
1: 
0:   private final class BlockSortThread implements Callable<IndexStorage> {
0:     private int index;
1: 
0:     private byte[][] data;
0:     private boolean isSortRequired;
0:     private boolean isCompressionReq;
0:     private boolean isUseInvertedIndex;
1: 
0:     private boolean isNoDictionary;
1: 
0:     private BlockSortThread(int index, byte[][] data, boolean isSortRequired,
0:         boolean isUseInvertedIndex) {
0:       this.index = index;
0:       this.data = data;
0:       isCompressionReq = aggKeyBlock[this.index];
0:       this.isSortRequired = isSortRequired;
0:       this.isUseInvertedIndex = isUseInvertedIndex;
1:     }
1: 
0:     public BlockSortThread(int index, byte[][] data, boolean b, boolean isNoDictionary,
0:         boolean isSortRequired, boolean isUseInvertedIndex) {
0:       this.index = index;
0:       this.data = data;
0:       isCompressionReq = b;
0:       this.isNoDictionary = isNoDictionary;
0:       this.isSortRequired = isSortRequired;
0:       this.isUseInvertedIndex = isUseInvertedIndex;
1:     }
1: 
0:     @Override public IndexStorage call() throws Exception {
0:       if (isUseInvertedIndex) {
0:         return new BlockIndexerStorageForInt(this.data, isCompressionReq, isNoDictionary,
0:             isSortRequired);
0:       } else {
0:         return new BlockIndexerStorageForNoInvertedIndex(this.data, isCompressionReq,
0:             isNoDictionary);
1:       }
1: 
1:     }
1: 
1:   }
1: }
author:Jacky Li
-------------------------------------------------------------------------------
commit:c9e5842
/////////////////////////////////////////////////////////////////////////
/////////////////////////////////////////////////////////////////////////
1:     return CarbonDataWriterFactory.getInstance().getFactDataWriter(version, model);
commit:5fc7f06
/////////////////////////////////////////////////////////////////////////
/////////////////////////////////////////////////////////////////////////
/////////////////////////////////////////////////////////////////////////
/////////////////////////////////////////////////////////////////////////
/////////////////////////////////////////////////////////////////////////
/////////////////////////////////////////////////////////////////////////
/////////////////////////////////////////////////////////////////////////
/////////////////////////////////////////////////////////////////////////
/////////////////////////////////////////////////////////////////////////
/////////////////////////////////////////////////////////////////////////
/////////////////////////////////////////////////////////////////////////
/////////////////////////////////////////////////////////////////////////
/////////////////////////////////////////////////////////////////////////
commit:f209e8e
/////////////////////////////////////////////////////////////////////////
1:       if (type[j] != DataTypes.BYTE && !DataTypes.isDecimal(type[j])) {
commit:956833e
/////////////////////////////////////////////////////////////////////////
1: import org.apache.carbondata.core.metadata.datatype.DataTypes;
/////////////////////////////////////////////////////////////////////////
0:       if (type[j] != DataTypes.BYTE && type[j] != DataTypes.DECIMAL) {
commit:349c59c
/////////////////////////////////////////////////////////////////////////
0: import org.apache.carbondata.processing.loading.sort.SortScopeOptions;
/////////////////////////////////////////////////////////////////////////
/////////////////////////////////////////////////////////////////////////
/////////////////////////////////////////////////////////////////////////
/////////////////////////////////////////////////////////////////////////
1:   private CarbonFactDataWriter getFactDataWriter() {
/////////////////////////////////////////////////////////////////////////
commit:e6a4f64
/////////////////////////////////////////////////////////////////////////
commit:f089287
/////////////////////////////////////////////////////////////////////////
/////////////////////////////////////////////////////////////////////////
0: import org.apache.carbondata.processing.datatypes.GenericDataType;
/////////////////////////////////////////////////////////////////////////
0: 
/////////////////////////////////////////////////////////////////////////
1:   private int pageSize;
/////////////////////////////////////////////////////////////////////////
1:   private TablePageList tablePageList;
/////////////////////////////////////////////////////////////////////////
/////////////////////////////////////////////////////////////////////////
0: 
0:     SortScopeOptions.SortScope sortScope = model.getSortScope();
/////////////////////////////////////////////////////////////////////////
1:     tablePageList = new TablePageList();
1:     Consumer consumer = new Consumer(tablePageList);
/////////////////////////////////////////////////////////////////////////
0:     if (this.entryCount == this.pageSize) {
1:                 new Producer(tablePageList, dataRows, ++writerTaskSequenceCounter, false)
1:         dataRows = new ArrayList<>(this.pageSize);
/////////////////////////////////////////////////////////////////////////
1:   private TablePage processDataRows(List<CarbonRow> dataRows)
1:       return new TablePage(model, 0);
/////////////////////////////////////////////////////////////////////////
1:     tablePage.encode();
1:     return tablePage;
/////////////////////////////////////////////////////////////////////////
1:           .submit(new Producer(tablePageList, dataRows, ++writerTaskSequenceCounter, true)));
/////////////////////////////////////////////////////////////////////////
1:     this.pageSize = Integer.parseInt(CarbonProperties.getInstance()
1:       this.pageSize = Integer.parseInt(CarbonProperties.getInstance()
0:     LOGGER.info("Number of rows per column blocklet " + pageSize);
1:     dataRows = new ArrayList<>(this.pageSize);
1:     // if at least one dimension is present then initialize column splitter otherwise null
/////////////////////////////////////////////////////////////////////////
0:       ColumnarSplitter columnarSplitter = model.getSegmentProperties().getFixedLengthKeySplitter();
0:           new CarbonKeyBlockHolder[columnarSplitter.getBlockKeySize().length];
0:       this.keyBlockHolder[i] = new CarbonKeyBlockHolder(pageSize);
/////////////////////////////////////////////////////////////////////////
1:     this.dataWriter = getFactDataWriter();
/////////////////////////////////////////////////////////////////////////
0:   private CarbonFactDataWriter<?> getFactDataWriter() {
0:         .getFactDataWriter(version, getDataWriterVo());
0:   private CarbonDataWriterVo getDataWriterVo() {
/////////////////////////////////////////////////////////////////////////
0:     carbonDataWriterVo.setListener(model.getDataMapWriterlistener());
/////////////////////////////////////////////////////////////////////////
1:    * This class will hold the table page data
1:   private final class TablePageList {
1:      * array of table page added by Producer and get by Consumer
1:     private TablePage[] tablePages;
/////////////////////////////////////////////////////////////////////////
1:     private TablePageList() {
1:       tablePages = new TablePage[numberOfCores];
/////////////////////////////////////////////////////////////////////////
1:     public synchronized TablePage get() throws InterruptedException {
1:       TablePage tablePage = tablePages[currentIndex];
1:       if (null == tablePage && !processingComplete) {
1:       tablePage = tablePages[currentIndex];
1:       tablePages[currentIndex] = null;
1:       if (currentIndex >= tablePages.length) {
1:       return tablePage;
1:     public synchronized void put(TablePage tablePage, int index) {
1:       tablePages[index] = tablePage;
/////////////////////////////////////////////////////////////////////////
1:     private TablePageList tablePageList;
1:     private int pageId;
1:     private Producer(TablePageList tablePageList, List<CarbonRow> dataRows,
1:         int pageId, boolean isLastPage) {
1:       this.tablePageList = tablePageList;
1:       this.pageId = pageId;
/////////////////////////////////////////////////////////////////////////
1:         TablePage tablePage = processDataRows(dataRows);
1:         tablePage.setIsLastPage(isLastPage);
1:         int indexInNodeHolderArray = (pageId - 1) % numberOfCores;
1:         tablePageList.put(tablePage, indexInNodeHolderArray);
/////////////////////////////////////////////////////////////////////////
1:     private TablePageList tablePageList;
1:     private Consumer(TablePageList tablePageList) {
1:       this.tablePageList = tablePageList;
/////////////////////////////////////////////////////////////////////////
1:         TablePage tablePage = null;
1:           tablePage = tablePageList.get();
1:           if (null != tablePage) {
1:             dataWriter.writeTablePage(tablePage);
1:             tablePage.freeMemory();
author:Zhang Zhichao
-------------------------------------------------------------------------------
commit:9e9d689
/////////////////////////////////////////////////////////////////////////
0:       numberOfCores = CarbonProperties.getInstance().getNumberOfCores();
author:kumarvishal
-------------------------------------------------------------------------------
commit:a734add
/////////////////////////////////////////////////////////////////////////
1: import org.apache.carbondata.core.util.CarbonThreadFactory;
/////////////////////////////////////////////////////////////////////////
0:     producerExecutorService = Executors.newFixedThreadPool(numberOfCores,
0:         new CarbonThreadFactory("ProducerPool:" + model.getTableName()));
0:     consumerExecutorService = Executors
0:         .newFixedThreadPool(1, new CarbonThreadFactory("ConsumerPool:" + model.getTableName()));
/////////////////////////////////////////////////////////////////////////
1:     if (null == dataWriter) {
1:       return;
0:     }
1:     if (producerExecutorService.isShutdown()) {
1:       return;
0:     }
1:     LOGGER.info("Started Finish Operation");
1:       LOGGER.info("Total Number Of records added to store: " + processedDataCount);
/////////////////////////////////////////////////////////////////////////
0:     /**
1:      * @param tablePage
0:      * @param index
0:      */
commit:e240855
/////////////////////////////////////////////////////////////////////////
0:         producerExecutorServiceTaskList.add(producerExecutorService.submit(
0:             new Producer(blockletDataHolder, dataRows, ++writerTaskSequenceCounter, false)));
/////////////////////////////////////////////////////////////////////////
0:           .submit(new Producer(blockletDataHolder, dataRows, ++writerTaskSequenceCounter, true)));
/////////////////////////////////////////////////////////////////////////
0:     private boolean isWriteAll;
0:         int sequenceNumber, boolean isWriteAll) {
0:       this.isWriteAll = isWriteAll;
/////////////////////////////////////////////////////////////////////////
0:         nodeHolder.setWriteAll(isWriteAll);
commit:ebf13dc
/////////////////////////////////////////////////////////////////////////
0:             endKey, compressionModel, noDictionaryValueHolder, noDictStartKey, noDictEndKey,
0:             nullValueIndexBitSet);
/////////////////////////////////////////////////////////////////////////
0:             noDictEndKey, nullValueIndexBitSet);
/////////////////////////////////////////////////////////////////////////
0:       byte[] noDictionaryEndKey, BitSet[] nullValueIndexBitSet) throws CarbonDataWriterException {
/////////////////////////////////////////////////////////////////////////
0:             endKeyLocal, compressionModel, noDictionaryStartKey, noDictionaryEndKey,
0:             nullValueIndexBitSet);
0:       byte[][] noDictionaryStartKey, byte[][] noDictionaryEndKey, BitSet[] nullValueIndexBitSet)
0:       throws CarbonDataWriterException {
/////////////////////////////////////////////////////////////////////////
0:             endKeyLocal, compressionModel, composedNonDictStartKey, composedNonDictEndKey,
0:             nullValueIndexBitSet);
commit:b16c308
/////////////////////////////////////////////////////////////////////////
0: import org.apache.carbondata.core.datastore.columnar.BlockIndexerStorageForNoInvertedIndexForShort;
/////////////////////////////////////////////////////////////////////////
1:         if (version == ColumnarFormatVersion.V3) {
0:           return new BlockIndexerStorageForNoInvertedIndexForShort(this.data, isNoDictionary);
0:         } else {
0:           return new BlockIndexerStorageForNoInvertedIndex(this.data, isNoDictionary);
0:         }
commit:2cf1104
/////////////////////////////////////////////////////////////////////////
1: import org.apache.carbondata.core.constants.CarbonV3DataFormatConstants;
0: import org.apache.carbondata.core.datastore.columnar.BlockIndexerStorageForShort;
/////////////////////////////////////////////////////////////////////////
0: import org.apache.carbondata.core.util.NodeHolder;
/////////////////////////////////////////////////////////////////////////
/////////////////////////////////////////////////////////////////////////
1:    * current data format version
0:    */
1:   private ColumnarFormatVersion version;
0: 
0:   /**
/////////////////////////////////////////////////////////////////////////
0:     version = CarbonProperties.getInstance().getFormatVersion();
/////////////////////////////////////////////////////////////////////////
0:         max[i] = new BigDecimal(-Double.MAX_VALUE);
/////////////////////////////////////////////////////////////////////////
0:       WriterCompressModel compressionModel, byte[][] noDictionaryData, byte[] noDictionaryStartKey,
0:       byte[] noDictionaryEndKey) throws CarbonDataWriterException {
/////////////////////////////////////////////////////////////////////////
0:           submit.add(executorService.submit(
0:               new BlockSortThread(i, dataHolders[dictionaryColumnCount].getData(), true,
0:                   isUseInvertedIndex[i])));
/////////////////////////////////////////////////////////////////////////
0:       byte[][] noDictionaryStartKey, byte[][] noDictionaryEndKey) throws CarbonDataWriterException {
/////////////////////////////////////////////////////////////////////////
0:             buffer.putShort((short) keyLength);
/////////////////////////////////////////////////////////////////////////
0:           submit.add(executorService.submit(
0:               new BlockSortThread(i, dataHolders[dictionaryColumnCount].getData(), true,
0:                   isUseInvertedIndex[i])));
/////////////////////////////////////////////////////////////////////////
/////////////////////////////////////////////////////////////////////////
0: 
/////////////////////////////////////////////////////////////////////////
0:     if (version == ColumnarFormatVersion.V3) {
0:       this.blockletSize = Integer.parseInt(CarbonProperties.getInstance()
0:           .getProperty(CarbonV3DataFormatConstants.NUMBER_OF_ROWS_PER_BLOCKLET_COLUMN_PAGE,
0:               CarbonV3DataFormatConstants.NUMBER_OF_ROWS_PER_BLOCKLET_COLUMN_PAGE_DEFAULT));
0:     }
0:     LOGGER.info("Number of rows per column blocklet " + blockletSize);
/////////////////////////////////////////////////////////////////////////
0:     this.dataWriter = getFactDataWriter(keyBlockSize);
/////////////////////////////////////////////////////////////////////////
/////////////////////////////////////////////////////////////////////////
0:         if (version == ColumnarFormatVersion.V3) {
0:           return new BlockIndexerStorageForShort(this.data, isCompressionReq, isNoDictionary,
0:               isSortRequired);
0:         } else {
0:           return new BlockIndexerStorageForInt(this.data, isCompressionReq, isNoDictionary,
0:               isSortRequired);
0:         }
commit:d54dc64
/////////////////////////////////////////////////////////////////////////
0: import org.apache.carbondata.processing.store.writer.CarbonDataWriterVo;
/////////////////////////////////////////////////////////////////////////
0:         getFactDataWriter(keyBlockSize);
/////////////////////////////////////////////////////////////////////////
0:   /**
1:    * Below method will be used to get the fact data writer instance
0:    *
0:    * @param keyBlockSize
1:    * @return data writer instance
0:    */
0:   private CarbonFactDataWriter<?> getFactDataWriter(int[] keyBlockSize) {
0:     short version = Short.parseShort(
0:         CarbonProperties.getInstance().getProperty(CarbonCommonConstants.CARBON_DATA_FILE_VERSION));
0:     return CarbonDataWriterFactory.getInstance()
0:         .getFactDataWriter(version, getDataWriterVo(keyBlockSize));
0:   }
0: 
0:   /**
0:    * Below method will be used to get the writer vo
0:    *
0:    * @param keyBlockSize size of each key block
0:    * @return data writer vo object
0:    */
0:   private CarbonDataWriterVo getDataWriterVo(int[] keyBlockSize) {
0:     CarbonDataWriterVo carbonDataWriterVo = new CarbonDataWriterVo();
0:     carbonDataWriterVo.setStoreLocation(storeLocation);
0:     carbonDataWriterVo.setMeasureCount(measureCount);
0:     carbonDataWriterVo.setMdKeyLength(mdkeyLength);
0:     carbonDataWriterVo.setTableName(tableName);
0:     carbonDataWriterVo.setKeyBlockSize(keyBlockSize);
0:     carbonDataWriterVo.setFileManager(fileManager);
0:     carbonDataWriterVo.setAggBlocks(aggKeyBlock);
0:     carbonDataWriterVo.setIsComplexType(isComplexTypes());
0:     carbonDataWriterVo.setNoDictionaryCount(noDictionaryCount);
0:     carbonDataWriterVo.setCarbonDataFileAttributes(carbonDataFileAttributes);
0:     carbonDataWriterVo.setDatabaseName(databaseName);
0:     carbonDataWriterVo.setWrapperColumnSchemaList(wrapperColumnSchemaList);
0:     carbonDataWriterVo.setIsDictionaryColumn(dimensionType);
0:     carbonDataWriterVo.setCarbonDataDirectoryPath(carbonDataDirectoryPath);
0:     carbonDataWriterVo.setColCardinality(colCardinality);
0:     carbonDataWriterVo.setSegmentProperties(segmentProperties);
0:     carbonDataWriterVo.setTableBlocksize(tableBlockSize);
0:     return carbonDataWriterVo;
/////////////////////////////////////////////////////////////////////////
1:             LOGGER.error(throwable, "Problem while writing the carbon data file");
0:             throw new CarbonDataWriterException(throwable.getMessage());
author:dhatchayani
-------------------------------------------------------------------------------
commit:435ea26
/////////////////////////////////////////////////////////////////////////
0:       this.pageSize = CarbonV3DataFormatConstants.NUMBER_OF_ROWS_PER_BLOCKLET_COLUMN_PAGE_DEFAULT;
author:Raghunandan S
-------------------------------------------------------------------------------
commit:06b0d08
/////////////////////////////////////////////////////////////////////////
1:     StringBuffer noInvertedIdxCol = new StringBuffer();
1:         noInvertedIdxCol.append(cd.getColName()).append(",");
1:     LOGGER.info("Columns considered as NoInverted Index are " + noInvertedIdxCol.toString());
author:jackylk
-------------------------------------------------------------------------------
commit:a5af0ff
/////////////////////////////////////////////////////////////////////////
/////////////////////////////////////////////////////////////////////////
0:       for (int i = 0; i < model.getTableSpec().getNumSimpleDimensions(); i++) {
0:         if (model.getSegmentProperties().getDimensions().get(i).isGlobalDictionaryEncoding()) {
commit:bc3e684
/////////////////////////////////////////////////////////////////////////
0: import org.apache.carbondata.core.datastore.page.EncodedTablePage;
/////////////////////////////////////////////////////////////////////////
/////////////////////////////////////////////////////////////////////////
/////////////////////////////////////////////////////////////////////////
/////////////////////////////////////////////////////////////////////////
1:    * generate the EncodedTablePage from the input rows (one page in case of V3 format)
0:   private EncodedTablePage processDataRows(List<CarbonRow> dataRows)
0:       return EncodedTablePage.newEmptyInstance();
1:       tablePage.addRow(rowId++, row);
0:     EncodedTablePage encoded = tablePage.encode();
0:     return encoded;
/////////////////////////////////////////////////////////////////////////
1:       this.dataWriter.writeFooterToFile();
/////////////////////////////////////////////////////////////////////////
0:     private EncodedTablePage[] encodedTablePages;
/////////////////////////////////////////////////////////////////////////
0:       encodedTablePages = new EncodedTablePage[numberOfCores];
/////////////////////////////////////////////////////////////////////////
0:     public synchronized EncodedTablePage get() throws InterruptedException {
0:       EncodedTablePage encodedTablePage = encodedTablePages[currentIndex];
0:       if (null == encodedTablePage && !processingComplete) {
0:       encodedTablePage = encodedTablePages[currentIndex];
0:       encodedTablePages[currentIndex] = null;
0:       if (currentIndex >= encodedTablePages.length) {
0:       return encodedTablePage;
0:      * @param encodedTablePage
0:     public synchronized void put(EncodedTablePage encodedTablePage, int index) {
0:       encodedTablePages[index] = encodedTablePage;
/////////////////////////////////////////////////////////////////////////
1:     private boolean isLastPage;
0:         int sequenceNumber, boolean isLastPage) {
1:       this.isLastPage = isLastPage;
/////////////////////////////////////////////////////////////////////////
0:         EncodedTablePage encodedTablePage = processDataRows(dataRows);
0:         encodedTablePage.setIsLastPage(isLastPage);
0:         blockletDataHolder.put(encodedTablePage, indexInNodeHolderArray);
/////////////////////////////////////////////////////////////////////////
0:         EncodedTablePage encodedTablePage = null;
0:           encodedTablePage = blockletDataHolder.get();
0:           if (null != encodedTablePage) {
0:             dataWriter.writeTablePage(encodedTablePage);
commit:eadfea7
/////////////////////////////////////////////////////////////////////////
0: import java.io.IOException;
/////////////////////////////////////////////////////////////////////////
1:       throws CarbonDataWriterException, KeyGenException, MemoryException, IOException {
commit:7359601
/////////////////////////////////////////////////////////////////////////
1: import org.apache.carbondata.core.memory.MemoryException;
/////////////////////////////////////////////////////////////////////////
0:       throws CarbonDataWriterException, KeyGenException, MemoryException {
/////////////////////////////////////////////////////////////////////////
0:     NodeHolder nodeHolder = dataWriter.buildDataNodeHolder(encodedData, tablePageStatistics, keys);
0:     tablePage.freeMemory();
0:     LOGGER.info("Number Of records processed: " + dataRows.size());
0:     return nodeHolder;
commit:edda248
/////////////////////////////////////////////////////////////////////////
0: import org.apache.carbondata.core.datastore.page.encoding.EncodedData;
/////////////////////////////////////////////////////////////////////////
/////////////////////////////////////////////////////////////////////////
0:   private TablePageEncoder encoder;
/////////////////////////////////////////////////////////////////////////
0:     this.encoder = new TablePageEncoder(model);
/////////////////////////////////////////////////////////////////////////
0:     // apply and compress dimensions and measure
0:     EncodedData encodedData = encoder.encode(tablePage);
commit:dc83b2a
/////////////////////////////////////////////////////////////////////////
/////////////////////////////////////////////////////////////////////////
0: import org.apache.carbondata.core.datastore.GenericDataType;
1: import org.apache.carbondata.core.datastore.exception.CarbonDataWriterException;
1: import org.apache.carbondata.core.datastore.row.CarbonRow;
/////////////////////////////////////////////////////////////////////////
0: import org.apache.carbondata.processing.store.writer.Encoder;
/////////////////////////////////////////////////////////////////////////
0: 
0:   // This variable is true if it is dictionary dimension and its cardinality is lower than
0:   // property of CarbonCommonConstants.HIGH_CARDINALITY_VALUE
0:   // It decides whether it will do RLE encoding on data page for this dimension
0:   private boolean[] rleEncodingForDictDimension;
/////////////////////////////////////////////////////////////////////////
/////////////////////////////////////////////////////////////////////////
0:   private DefaultEncoder encoder;
0: 
/////////////////////////////////////////////////////////////////////////
0:     this.rleEncodingForDictDimension = new boolean[numDimColumns];
0: 
/////////////////////////////////////////////////////////////////////////
0:       int dimCardinalityIndex = -1;
0:       int aggIndex = -1;
0:         if (colGrpModel.isColumnar(i) && dimLens[dimCardinalityIndex] < noDictionaryValue) {
0:           this.rleEncodingForDictDimension[aggIndex] = true;
0:         }
0:         List<Boolean> rleWithComplex = new ArrayList<Boolean>(allColsCount);
0:             complexDataType.fillAggKeyBlock(rleWithComplex, this.rleEncodingForDictDimension);
0:             rleWithComplex.add(this.rleEncodingForDictDimension[i]);
0:         this.rleEncodingForDictDimension = new boolean[allColsCount];
0:           this.rleEncodingForDictDimension[i] = rleWithComplex.get(i);
0:       rleEncodingForDictDimension = arrangeUniqueBlockType(rleEncodingForDictDimension);
1:     this.version = CarbonProperties.getInstance().getFormatVersion();
0:     this.encoder = new DefaultEncoder(model);
/////////////////////////////////////////////////////////////////////////
/////////////////////////////////////////////////////////////////////////
0:     TablePageKey keys = new TablePageKey(model, dataRows.size());
/////////////////////////////////////////////////////////////////////////
0:     Encoder.EncodedData encodedData = encoder.encode(tablePage);
0:     TablePageStatistics tablePageStatistics = new TablePageStatistics(
0:         model.getTableSpec(), tablePage, encodedData, tablePage.getMeasureStats());
0:     return dataWriter.buildDataNodeHolder(encodedData, tablePageStatistics, keys);
/////////////////////////////////////////////////////////////////////////
1:     return model.getExpandedComplexColsCount();
/////////////////////////////////////////////////////////////////////////
0:     carbonDataWriterVo.setRleEncodingForDictDim(rleEncodingForDictDimension);
/////////////////////////////////////////////////////////////////////////
0:     carbonDataWriterVo.setBucketNumber(model.getBucketId());
0:     carbonDataWriterVo.setTaskExtension(model.getTaskExtension());
/////////////////////////////////////////////////////////////////////////
0: }
commit:353272e
/////////////////////////////////////////////////////////////////////////
/////////////////////////////////////////////////////////////////////////
0: import org.apache.carbondata.processing.newflow.row.CarbonRow;
0: import org.apache.carbondata.processing.newflow.row.WriteStepRowUtil;
/////////////////////////////////////////////////////////////////////////
/////////////////////////////////////////////////////////////////////////
0: 
1:   private CarbonFactDataHandlerModel model;
0: 
/////////////////////////////////////////////////////////////////////////
0: 
/////////////////////////////////////////////////////////////////////////
1:   private List<CarbonRow> dataRows;
/////////////////////////////////////////////////////////////////////////
/////////////////////////////////////////////////////////////////////////
/////////////////////////////////////////////////////////////////////////
1:   public CarbonFactDataHandlerColumnar(CarbonFactDataHandlerModel model) {
1:     this.model = model;
1:     initParameters(model);
0:     int numDimColumns = colGrpModel.getNoOfColumnStore() + model.getNoDictionaryCount()
0:         + getExpandedComplexColsCount();
0:     this.bucketNumber = model.getBucketId();
0:     this.taskExtension = model.getTaskExtension();
0:     if (null != model.getIsUseInvertedIndex()) {
0:         if (i < model.getIsUseInvertedIndex().length) {
0:           isUseInvertedIndex[i] = model.getIsUseInvertedIndex()[i];
/////////////////////////////////////////////////////////////////////////
0:     for (int i = 0; i < model.getNoDictionaryCount(); i++) {
0:     boolean isAggKeyBlock = Boolean.parseBoolean(
0:         CarbonProperties.getInstance().getProperty(
0:             CarbonCommonConstants.AGGREAGATE_COLUMNAR_KEY_BLOCK,
0:       int noDictionaryValue = Integer.parseInt(
0:           CarbonProperties.getInstance().getProperty(
0:               CarbonCommonConstants.HIGH_CARDINALITY_VALUE,
0:       int[] dimLens = model.getSegmentProperties().getDimColumnsCardinality();
/////////////////////////////////////////////////////////////////////////
0:       if (model.getDimensionCount() < dimLens.length) {
0:         int allColsCount = getColsCount(model.getDimensionCount());
0:         for (int i = 0; i < model.getDimensionCount(); i++) {
1:           GenericDataType complexDataType = model.getComplexIndexMap().get(i);
/////////////////////////////////////////////////////////////////////////
1:   private void initParameters(CarbonFactDataHandlerModel model) {
0: 
0:     this.colGrpModel = model.getSegmentProperties().getColumnGroupModel();
0:     CarbonTable carbonTable =
0:         CarbonMetadata.getInstance().getCarbonTable(
0:             model.getDatabaseName() + CarbonCommonConstants.UNDERSCORE + model.getTableName());
0:         CarbonUtil.identifyDimensionType(carbonTable.getDimensionByTableName(model.getTableName()));
0:     if (model.isCompactionFlow()) {
/////////////////////////////////////////////////////////////////////////
1:       GenericDataType complexDataType = model.getComplexIndexMap().get(i);
/////////////////////////////////////////////////////////////////////////
0:     fileManager.setName(new File(model.getStoreLocation()).getName());
/////////////////////////////////////////////////////////////////////////
1:   public void addDataToStore(CarbonRow row) throws CarbonDataWriterException {
1:     // if entry count reaches to leaf node size then we are ready to write
/////////////////////////////////////////////////////////////////////////
0:     void update(int rowId, CarbonRow row) throws KeyGenException {
0:       currentMDKey = WriteStepRowUtil.getMdk(row, model.getMDKeyGenerator());
0:       if (model.getNoDictionaryCount() > 0 || model.getComplexIndexMap().size() > 0) {
0:         currentNoDictionaryKey = WriteStepRowUtil.getNoDictAndComplexDimension(row);
/////////////////////////////////////////////////////////////////////////
0:       int numberOfDictSortColumns = model.getSegmentProperties().getNumberOfDictSortColumns();
/////////////////////////////////////////////////////////////////////////
0:       int numberOfNoDictSortColumns = model.getSegmentProperties().getNumberOfNoDictSortColumns();
/////////////////////////////////////////////////////////////////////////
0:   private NodeHolder processDataRows(List<CarbonRow> dataRows)
0:       throws CarbonDataWriterException, KeyGenException {
1:     TablePage tablePage = new TablePage(model, dataRows.size());
1:     for (CarbonRow row : dataRows) {
/////////////////////////////////////////////////////////////////////////
0:     Codec codec = new Codec(model.getMeasureDataType());
0:     BitSet[] nullBitSet = new BitSet[tablePage.getMeasurePage().length];
0:     FixLengthColumnPage[] measurePages = tablePage.getMeasurePage();
0:       nullBitSet[i] = measurePages[i].getNullBitSet();
/////////////////////////////////////////////////////////////////////////
1:       } catch (InterruptedException | ExecutionException e) {
/////////////////////////////////////////////////////////////////////////
0:       GenericDataType complexDataType = model.getComplexIndexMap().get(i);
/////////////////////////////////////////////////////////////////////////
1:     int dictDimensionCount = model.getDimensionCount();
1:     for (int i = 0; i < dictDimensionCount; i++) {
0:       GenericDataType complexDataType = model.getComplexIndexMap().get(i);
/////////////////////////////////////////////////////////////////////////
0:     return model.getComplexIndexMap().size();
/////////////////////////////////////////////////////////////////////////
0:     int[] keyBlockSize = new int[noOfColStore + getExpandedComplexColsCount()];
0:     if (model.getDimLens().length > 0) {
0:       this.columnarSplitter = model.getSegmentProperties().getFixedLengthKeySplitter();
/////////////////////////////////////////////////////////////////////////
1:     DataType[] type = model.getMeasureDataType();
/////////////////////////////////////////////////////////////////////////
1:     setComplexMapSurrogateIndex(model.getDimensionCount());
1:         CarbonUtil.getIncrementedCardinalityFullyFilled(model.getDimLens().clone()), (byte) dimSet)
/////////////////////////////////////////////////////////////////////////
0:     int dictDimensionCount = model.getDimensionCount();
0:     for (int i = 0; i < dictDimensionCount; i++) {
0:       GenericDataType complexDataType = model.getComplexIndexMap().get(i);
/////////////////////////////////////////////////////////////////////////
0:     carbonDataWriterVo.setStoreLocation(model.getStoreLocation());
0:     carbonDataWriterVo.setMeasureCount(model.getMeasureCount());
0:     carbonDataWriterVo.setTableName(model.getTableName());
0:     carbonDataWriterVo.setNoDictionaryCount(model.getNoDictionaryCount());
0:     carbonDataWriterVo.setCarbonDataFileAttributes(model.getCarbonDataFileAttributes());
0:     carbonDataWriterVo.setDatabaseName(model.getDatabaseName());
0:     carbonDataWriterVo.setWrapperColumnSchemaList(model.getWrapperColumnSchema());
0:     carbonDataWriterVo.setCarbonDataDirectoryPath(model.getCarbonDataDirectoryPath());
0:     carbonDataWriterVo.setColCardinality(model.getColCardinality());
0:     carbonDataWriterVo.setSegmentProperties(model.getSegmentProperties());
0:     carbonDataWriterVo.setTableBlocksize(model.getBlockSizeInMB());
0:     carbonDataWriterVo.setSchemaUpdatedTimeStamp(model.getSchemaUpdatedTimeStamp());
0:     int noDictionaryCount = model.getNoDictionaryCount();
0:     int noOfColumn = colGrpModel.getNoOfColumnStore() + noDictionaryCount + getComplexColumnCount();
0:       GenericDataType complexDataType = model.getComplexIndexMap().get(i - noDictionaryCount);
/////////////////////////////////////////////////////////////////////////
1:     private List<CarbonRow> dataRows;
0:     private Producer(BlockletDataHolder blockletDataHolder, List<CarbonRow> dataRows,
/////////////////////////////////////////////////////////////////////////
0:     public BlockSortThread(byte[][] data, boolean compression, boolean isNoDictionary,
0:       this.isCompressionReq = compression;
0:       if (index == 1) {
0:         int dd = 1 + 1;
0:       }
/////////////////////////////////////////////////////////////////////////
0:     private DataType[] measureType;
0:     Codec(DataType[] measureType) {
0:       this.measureType = measureType;
/////////////////////////////////////////////////////////////////////////
0:       FixLengthColumnPage[] measurePage = tablePage.getMeasurePage();
/////////////////////////////////////////////////////////////////////////
0:       compressionModel =
0:           ValueCompressionUtil.getWriterCompressModel(max, min, decimal, uniqueValue, measureType,
0:               new byte[measureCount]);
/////////////////////////////////////////////////////////////////////////
0:       int noDictionaryCount = tablePage.getNoDictDimensionPage().length;
0:       int complexColCount = tablePage.getComplexDimensionPage().length;
/////////////////////////////////////////////////////////////////////////
0:           model.getPrimitiveDimLens().length + noDictionaryCount + complexColCount);
0:       SegmentProperties segmentProperties = model.getSegmentProperties();
/////////////////////////////////////////////////////////////////////////
0:                     tablePage.getKeyColumnPage().getKeyVector(dictionaryColumnCount),
0:                     true,
0:                     false,
/////////////////////////////////////////////////////////////////////////
0:                 tablePage.getKeyColumnPage().getKeyVector(dictionaryColumnCount));
0:                   tablePage.getNoDictDimensionPage()[++noDictionaryColumnCount].getByteArrayPage(),
/////////////////////////////////////////////////////////////////////////
0:         Iterator<byte[][]> iterator = tablePage.getComplexDimensionPage()[index].iterator();
0:           byte[][] data = iterator.next();
0:                   data,
commit:98df130
/////////////////////////////////////////////////////////////////////////
0: import java.util.Iterator;
/////////////////////////////////////////////////////////////////////////
0: import org.apache.carbondata.core.datastore.compression.ValueCompressionHolder;
0: import org.apache.carbondata.core.datastore.page.ComplexColumnPage;
0: import org.apache.carbondata.core.datastore.page.FixLengthColumnPage;
0: import org.apache.carbondata.core.datastore.page.VarLengthColumnPage;
/////////////////////////////////////////////////////////////////////////
1: import org.apache.carbondata.core.metadata.datatype.DataType;
/////////////////////////////////////////////////////////////////////////
/////////////////////////////////////////////////////////////////////////
1:    * blocklet size (for V1 and V2) or page size (for V3). A Producer thread will start to process
1:    * once this size of input is reached
/////////////////////////////////////////////////////////////////////////
/////////////////////////////////////////////////////////////////////////
/////////////////////////////////////////////////////////////////////////
0:   private DataType[] type;
/////////////////////////////////////////////////////////////////////////
/////////////////////////////////////////////////////////////////////////
0:    * is of dictionary type or no dictionary type
0:   private boolean[] isDictDimension;
/////////////////////////////////////////////////////////////////////////
/////////////////////////////////////////////////////////////////////////
0:     this.complexColCount = getExpandedComplexColsCount();
0:     int numDimColumns = colGrpModel.getNoOfColumnStore() + noDictionaryCount + complexColCount;
0:     this.aggKeyBlock = new boolean[numDimColumns];
0:     this.isNoDictionary = new boolean[numDimColumns];
0:     this.isUseInvertedIndex = new boolean[numDimColumns];
/////////////////////////////////////////////////////////////////////////
0:     boolean isAggKeyBlock = Boolean.parseBoolean(CarbonProperties.getInstance()
0:         .getProperty(CarbonCommonConstants.AGGREAGATE_COLUMNAR_KEY_BLOCK,
0:             CarbonCommonConstants.AGGREAGATE_COLUMNAR_KEY_BLOCK_DEFAULTVALUE));
/////////////////////////////////////////////////////////////////////////
0:     this.type = carbonFactDataHandlerModel.getMeasureDataType();
/////////////////////////////////////////////////////////////////////////
0: 
0:     isDictDimension =
/////////////////////////////////////////////////////////////////////////
0: 
1:     // Start the consumer which will take each blocklet/page in order and write to a file
0:     Consumer consumer = new Consumer(blockletDataHolder);
0:     for (int i = 0; i < isDictDimension.length; i++) {
0:       if (isDictDimension[i]) {
/////////////////////////////////////////////////////////////////////////
0: 
1:         producerExecutorServiceTaskList.add(
1:             producerExecutorService.submit(
0:                 new Producer(blockletDataHolder, dataRows, ++writerTaskSequenceCounter, false)
1:             )
1:         );
/////////////////////////////////////////////////////////////////////////
0:     private int pageSize;
0:     byte[] packedNoDictStartKey = null;
0:     byte[] packedNoDictEndKey = null;
0: 
0:     IndexKey(int pageSize) {
0:       this.pageSize = pageSize;
0:     }
0:     void update(int rowId, Object[] row) {
0:       if (rowId == 0) {
0:       if (rowId == pageSize - 1) {
0:         finalizeKeys();
0:       }
0:     }
0: 
0:     /** update all keys if SORT_COLUMNS option is used when creating table */
0:     private void finalizeKeys() {
0:       // If SORT_COLUMNS is used, may need to update start/end keys since the they may
0:       // contains dictionary columns that are not in SORT_COLUMNS, which need to be removed from
0:       // start/end key
0:       int numberOfDictSortColumns = segmentProperties.getNumberOfDictSortColumns();
0:       if (numberOfDictSortColumns > 0) {
0:         // if SORT_COLUMNS contain dictionary columns
0:         int[] keySize = columnarSplitter.getBlockKeySize();
0:         if (keySize.length > numberOfDictSortColumns) {
0:           // if there are some dictionary columns that are not in SORT_COLUMNS, it will come to here
0:           int newMdkLength = 0;
0:           for (int i = 0; i < numberOfDictSortColumns; i++) {
0:             newMdkLength += keySize[i];
0:           }
0:           byte[] newStartKeyOfSortKey = new byte[newMdkLength];
0:           byte[] newEndKeyOfSortKey = new byte[newMdkLength];
0:           System.arraycopy(startKey, 0, newStartKeyOfSortKey, 0, newMdkLength);
0:           System.arraycopy(endKey, 0, newEndKeyOfSortKey, 0, newMdkLength);
0:           startKey = newStartKeyOfSortKey;
0:           endKey = newEndKeyOfSortKey;
0:         }
0:       } else {
0:         startKey = new byte[0];
0:         endKey = new byte[0];
0:       }
0: 
0:       // Do the same update for noDictionary start/end Key
0:       int numberOfNoDictSortColumns = segmentProperties.getNumberOfNoDictSortColumns();
0:       if (numberOfNoDictSortColumns > 0) {
0:         // if sort_columns contain no-dictionary columns
0:         if (noDictStartKey.length > numberOfNoDictSortColumns) {
0:           byte[][] newNoDictionaryStartKey = new byte[numberOfNoDictSortColumns][];
0:           byte[][] newNoDictionaryEndKey = new byte[numberOfNoDictSortColumns][];
0:           System.arraycopy(
0:               noDictStartKey, 0, newNoDictionaryStartKey, 0, numberOfNoDictSortColumns);
0:           System.arraycopy(
0:               noDictEndKey, 0, newNoDictionaryEndKey, 0, numberOfNoDictSortColumns);
0:           noDictStartKey = newNoDictionaryStartKey;
0:           noDictEndKey = newNoDictionaryEndKey;
0:         }
0:         packedNoDictStartKey =
0:             NonDictionaryUtil.packByteBufferIntoSingleByteArray(noDictStartKey);
0:         packedNoDictEndKey =
0:             NonDictionaryUtil.packByteBufferIntoSingleByteArray(noDictEndKey);
0:       }
0:   /**
0:    * Represent a page data for all columns, we store its data in columnar layout, so that
0:    * all processing apply to TablePage can be done in vectorized fashion.
0:    */
0:   class TablePage {
0: 
0:     // For all dimension and measure columns, we store the column data directly in the page,
0:     // the length of the page is the number of rows.
0: 
0:     // TODO: we should have separate class for key columns so that keys are stored together in
0:     // one vector to make it efficient for sorting
0:     VarLengthColumnPage[] dictDimensionPage;
0:     VarLengthColumnPage[] noDictDimensionPage;
0:     ComplexColumnPage[] complexDimensionPage;
0:     FixLengthColumnPage[] measurePage;
0: 
0:     // the num of rows in this page, it must be less than short value (65536)
0:     int pageSize;
0: 
0:     TablePage(int pageSize) {
0:       this.pageSize = pageSize;
0:       dictDimensionPage = new VarLengthColumnPage[dimensionCount];
0:       for (int i = 0; i < dictDimensionPage.length; i++) {
0:         dictDimensionPage[i] = new VarLengthColumnPage(pageSize);
0:       }
0:       noDictDimensionPage = new VarLengthColumnPage[noDictionaryCount];
0:       for (int i = 0; i < noDictDimensionPage.length; i++) {
0:         noDictDimensionPage[i] = new VarLengthColumnPage(pageSize);
0:       }
0:       complexDimensionPage = new ComplexColumnPage[getComplexColumnCount()];
0:       for (int i = 0; i < complexDimensionPage.length; i++) {
0:         // here we still do not the depth of the complex column, it will be initialized when
0:         // we get the first row.
0:         complexDimensionPage[i] = null;
0:       }
0:       measurePage = new FixLengthColumnPage[measureCount];
0:       for (int i = 0; i < measurePage.length; i++) {
0:         measurePage[i] = new FixLengthColumnPage(type[i], pageSize);
0:       }
0:     }
0: 
0:     /**
0:      * Add one row to the internal store, it will be converted into columnar layout
0:      * @param rowId Id of the input row
0:      * @param rows row object
0:      */
0:     void addRow(int rowId, Object[] rows) {
0: 
0:       // convert dictionary columns
0:       byte[] MDKey = (byte[]) rows[mdKeyIndex];
0:       if (columnarSplitter != null) {
0:         byte[][] splitKey = columnarSplitter.splitKey(MDKey);
0:         for (int i = 0; i < splitKey.length; i++) {
0:           dictDimensionPage[i].putByteArray(rowId, splitKey[i]);
0:         }
0:       }
0: 
0:       // convert noDictionary columns and complex columns.
0:       if (noDictionaryCount > 0 || complexColCount > 0) {
0:         byte[][] noDictAndComplex = (byte[][])(rows[mdKeyIndex - 1]);
0:         for (int i = 0; i < noDictAndComplex.length; i++) {
0:           if (i < noDictionaryCount) {
0:             // noDictionary columns, since it is variable length, we need to prepare each
0:             // element as LV encoded byte array (first two bytes are the length of the array)
0:             byte[] valueWithLength = addLengthToByteArray(noDictAndComplex[i]);
0:             noDictDimensionPage[i].putByteArray(rowId, valueWithLength);
0:           } else {
0:             // complex columns
0:             addComplexColumn(i - noDictionaryCount, rowId, noDictAndComplex[i]);
0:           }
0:         }
0:       }
0: 
0:       // convert measure columns
0:       for (int i = 0; i < type.length; i++) {
0:         Object value = rows[i];
0: 
0:         // in compaction flow the measure with decimal type will come as spark decimal.
0:         // need to convert it to byte array.
0:         if (type[i] == DataType.DECIMAL && compactionFlow) {
0:           BigDecimal bigDecimal = ((Decimal) rows[i]).toJavaBigDecimal();
0:           value = DataTypeUtil.bigDecimalToByte(bigDecimal);
0:         }
0:         measurePage[i].putData(rowId, value);
0:       }
0:     }
0: 
0:     /**
0:      * add a complex column into internal member compleDimensionPage
0:      * @param index index of the complexDimensionPage
0:      * @param rowId Id of the input row
0:      * @param complexColumns byte array the complex columm to be added, extracted of input row
0:      */
0:     // TODO: this function should be refactoried, ColumnPage should support complex type encoding
0:     // directly instead of doing it here
0:     private void addComplexColumn(int index, int rowId, byte[] complexColumns) {
0:       GenericDataType complexDataType = complexIndexMap.get(index + primitiveDimLens.length);
0: 
0:       // initialize the page if first row
0:       if (rowId == 0) {
0:         int depthInComplexColumn = complexDataType.getColsCount();
0:         complexDimensionPage[index] = new ComplexColumnPage(pageSize, depthInComplexColumn);
0:       }
0: 
0:       int depthInComplexColumn = complexDimensionPage[index].getDepth();
0:       // this is the encoded columnar data which will be added to page,
0:       // size of this list is the depth of complex column, we will fill it by input data
0:       List<ArrayList<byte[]>> encodedComplexColumnar = new ArrayList<>();
0:       for (int k = 0; k < depthInComplexColumn; k++) {
0:         encodedComplexColumnar.add(new ArrayList<byte[]>());
0:       }
0: 
0:       // encode the complex type data and fill columnsArray
0:       try {
0:         ByteBuffer byteArrayInput = ByteBuffer.wrap(complexColumns);
0:         ByteArrayOutputStream byteArrayOutput = new ByteArrayOutputStream();
0:         DataOutputStream dataOutputStream = new DataOutputStream(byteArrayOutput);
0:         complexDataType.parseAndBitPack(byteArrayInput, dataOutputStream, complexKeyGenerator);
0:         complexDataType.getColumnarDataForComplexType(encodedComplexColumnar,
0:             ByteBuffer.wrap(byteArrayOutput.toByteArray()));
0:         byteArrayOutput.close();
0:       } catch (IOException | KeyGenException e) {
0:         throw new CarbonDataWriterException(
0:             "Problem while bit packing and writing complex datatype", e);
0:       }
0: 
0:       for (int depth = 0; depth < depthInComplexColumn; depth++) {
0:         complexDimensionPage[index].putComplexData(rowId, depth, encodedComplexColumnar.get(depth));
0:       }
0:     }
0: 
0:     // Adds length as a short element (first 2 bytes) to the head of the input byte array
0:     private byte[] addLengthToByteArray(byte[] input) {
0:       byte[] output = new byte[input.length + 2];
0:       ByteBuffer buffer = ByteBuffer.wrap(output);
0:       buffer.putShort((short) input.length);
0:       buffer.put(input, 0, input.length);
0:       return output;
0:     }
0: 
0:   }
0: 
0:   /**
0:    * generate the NodeHolder from the input rows (one page in case of V3 format)
0:    */
0:     TablePage tablePage = new TablePage(dataRows.size());
0:     IndexKey keys = new IndexKey(dataRows.size());
1:     int rowId = 0;
1:     // convert row to columnar data
0:     for (Object[] row : dataRows) {
0:       tablePage.addRow(rowId, row);
0:       keys.update(rowId, row);
0:       rowId++;
0:     // encode and compress dimensions and measure
0:     // TODO: To make the encoding more transparent to the user, user should be enable to specify
0:     // the encoding and compression method for each type when creating table.
0: 
0:     Codec codec = new Codec();
0:     IndexStorage[] dimColumns = codec.encodeAndCompressDimensions(tablePage);
0:     Codec encodedMeasure = codec.encodeAndCompressMeasures(tablePage);
0: 
0:     // prepare nullBitSet for writer, remove this after writer can accept TablePage
0:     BitSet[] nullBitSet = new BitSet[tablePage.measurePage.length];
0:     for (int i = 0; i < nullBitSet.length; i++) {
0:       nullBitSet[i] = tablePage.measurePage[i].getNullBitSet();
0:     // TODO: writer interface should be modified to use TablePage
0:     return dataWriter.buildDataNodeHolder(dimColumns, encodedMeasure.getEncodedMeasure(),
0:         dataRows.size(), keys.startKey, keys.endKey, encodedMeasure.getCompressionModel(),
0:         keys.packedNoDictStartKey, keys.packedNoDictEndKey, nullBitSet);
/////////////////////////////////////////////////////////////////////////
1:   // return the number of complex column after complex columns are expanded
1:   private int getExpandedComplexColsCount() {
/////////////////////////////////////////////////////////////////////////
0:   // return the number of complex column
0:   private int getComplexColumnCount() {
0:     return complexIndexMap.size();
0:   }
0: 
/////////////////////////////////////////////////////////////////////////
/////////////////////////////////////////////////////////////////////////
0:       if (type[j] != DataType.BYTE && type[j] != DataType.DECIMAL) {
0: 
1:     int[] otherMeasureIndex = new int[otherMeasureIndexList.size()];
1:     int[] customMeasureIndex = new int[customMeasureIndexList.size()];
/////////////////////////////////////////////////////////////////////////
1:     int allColsCount = getExpandedComplexColsCount();
/////////////////////////////////////////////////////////////////////////
/////////////////////////////////////////////////////////////////////////
0:     carbonDataWriterVo.setIsDictionaryColumn(isDictDimension);
/////////////////////////////////////////////////////////////////////////
0:           return new BlockIndexerStorageForNoInvertedIndexForShort(this.data);
0:           return new BlockIndexerStorageForNoInvertedIndex(this.data);
0: 
0:   public class Codec {
0:     private WriterCompressModel compressionModel;
0:     private byte[][] encodedMeasureArray;
0: 
0:     Codec() {
0:     }
0: 
0:     public WriterCompressModel getCompressionModel() {
0:       return compressionModel;
0:     }
0: 
0:     public byte[][] getEncodedMeasure() {
0:       return encodedMeasureArray;
0:     }
0: 
0:     public Codec encodeAndCompressMeasures(TablePage tablePage) {
0:       // TODO: following conversion is required only because compress model requires them,
0:       // remove then after the compress framework is refactoried
0:       FixLengthColumnPage[] measurePage = tablePage.measurePage;
0:       int measureCount = measurePage.length;
0:       Object[] min = new Object[measurePage.length];
0:       Object[] max = new Object[measurePage.length];
0:       Object[] uniqueValue = new Object[measurePage.length];
0:       int[] decimal = new int[measurePage.length];
0:       for (int i = 0; i < measurePage.length; i++) {
0:         min[i] = measurePage[i].getStatistics().getMin();
0:         max[i] = measurePage[i].getStatistics().getMax();
0:         uniqueValue[i] = measurePage[i].getStatistics().getUniqueValue();
0:         decimal[i] = measurePage[i].getStatistics().getDecimal();
0:       }
0:       // encode and compress measure column page
0:       compressionModel = ValueCompressionUtil
0:           .getWriterCompressModel(max, min, decimal, uniqueValue, type, new byte[measureCount]);
0:       encodedMeasureArray = encodeMeasure(compressionModel, measurePage);
0:       return this;
0:     }
0: 
0:     // this method first invokes encoding routine to encode the data chunk,
0:     // followed by invoking compression routine for preparing the data chunk for writing.
0:     private byte[][] encodeMeasure(WriterCompressModel compressionModel,
0:         FixLengthColumnPage[] columnPages) {
0: 
0:       CarbonWriteDataHolder[] holders = new CarbonWriteDataHolder[columnPages.length];
0:       for (int i = 0; i < holders.length; i++) {
0:         holders[i] = new CarbonWriteDataHolder();
0:         switch (columnPages[i].getDataType()) {
0:           case SHORT:
0:           case INT:
0:           case LONG:
0:             holders[i].setWritableLongPage(columnPages[i].getLongPage());
0:             break;
0:           case DOUBLE:
0:             holders[i].setWritableDoublePage(columnPages[i].getDoublePage());
0:             break;
0:           case DECIMAL:
0:             holders[i].setWritableDecimalPage(columnPages[i].getDecimalPage());
0:             break;
0:           default:
0:             throw new RuntimeException("Unsupported data type: " + columnPages[i].getDataType());
0:         }
0:       }
0: 
0:       DataType[] dataType = compressionModel.getDataType();
0:       ValueCompressionHolder[] values =
0:           new ValueCompressionHolder[compressionModel.getValueCompressionHolder().length];
0:       byte[][] returnValue = new byte[values.length][];
0:       for (int i = 0; i < compressionModel.getValueCompressionHolder().length; i++) {
0:         values[i] = compressionModel.getValueCompressionHolder()[i];
0:         if (dataType[i] != DataType.DECIMAL) {
0:           values[i].setValue(
0:               ValueCompressionUtil.getValueCompressor(compressionModel.getCompressionFinders()[i])
0:                   .getCompressedValues(compressionModel.getCompressionFinders()[i], holders[i],
0:                       compressionModel.getMaxValue()[i],
0:                       compressionModel.getMantissa()[i]));
0:         } else {
0:           values[i].setValue(holders[i].getWritableByteArrayValues());
0:         }
0:         values[i].compress();
0:         returnValue[i] = values[i].getCompressedData();
0:       }
0: 
0:       return returnValue;
0:     }
0: 
0:     /**
0:      * Encode and compress each column page. The work is done using a thread pool.
0:      */
0:     private IndexStorage[] encodeAndCompressDimensions(TablePage tablePage) {
0:       int noDictionaryCount = tablePage.noDictDimensionPage.length;
0:       int complexColCount = tablePage.complexDimensionPage.length;
0: 
0:       // thread pool size to be used for encoding dimension
0:       // each thread will sort the column page data and compress it
0:       int thread_pool_size = Integer.parseInt(CarbonProperties.getInstance()
0:           .getProperty(CarbonCommonConstants.NUM_CORES_BLOCK_SORT,
0:               CarbonCommonConstants.NUM_CORES_BLOCK_SORT_DEFAULT_VAL));
0:       ExecutorService executorService = Executors.newFixedThreadPool(thread_pool_size);
0:       Callable<IndexStorage> callable;
0:       List<Future<IndexStorage>> submit = new ArrayList<Future<IndexStorage>>(
0:           primitiveDimLens.length + noDictionaryCount + complexColCount);
0:       int i = 0;
0:       int dictionaryColumnCount = -1;
0:       int noDictionaryColumnCount = -1;
0:       int colGrpId = -1;
0:       boolean isSortColumn = false;
0:       for (i = 0; i < isDictDimension.length; i++) {
0:         isSortColumn = i < segmentProperties.getNumberOfSortColumns();
0:         if (isDictDimension[i]) {
0:           dictionaryColumnCount++;
0:           if (colGrpModel.isColumnar(dictionaryColumnCount)) {
0:             // dictionary dimension
0:             callable =
0:                 new BlockSortThread(
0:                     i,
0:                     tablePage.dictDimensionPage[dictionaryColumnCount].getByteArrayPage(),
0:                     isSortColumn,
0:                     isUseInvertedIndex[i] & isSortColumn);
0: 
0:           } else {
0:             // column group
0:             callable = new ColGroupBlockStorage(
0:                 segmentProperties,
0:                 ++colGrpId,
0:                 tablePage.dictDimensionPage[dictionaryColumnCount].getByteArrayPage());
0:           }
0:         } else {
0:           // no dictionary dimension
0:           callable =
0:               new BlockSortThread(
0:                   i,
0:                   tablePage.noDictDimensionPage[++noDictionaryColumnCount].getByteArrayPage(),
0:                   false,
0:                   true,
0:                   isSortColumn,
0:                   isUseInvertedIndex[i] & isSortColumn);
0:         }
0:         // start a thread to sort the page data
0:         submit.add(executorService.submit(callable));
0:       }
0: 
0:       // complex type column
0:       for (int index = 0; index < getComplexColumnCount(); index++) {
0:         Iterator<byte[][]> iterator = tablePage.complexDimensionPage[index].iterator();
0:         while (iterator.hasNext()) {
0:           callable =
0:               new BlockSortThread(
0:                   i++,
0:                   iterator.next(),
0:                   false,
0:                   true);
0:           submit.add(executorService.submit(callable));
0:         }
0:       }
0:       executorService.shutdown();
0:       try {
0:         executorService.awaitTermination(1, TimeUnit.DAYS);
0:       } catch (InterruptedException e) {
0:         LOGGER.error(e, e.getMessage());
0:       }
0:       IndexStorage[] dimColumns = new IndexStorage[
0:           colGrpModel.getNoOfColumnStore() + noDictionaryCount + getExpandedComplexColsCount()];
0:       try {
0:         for (int k = 0; k < dimColumns.length; k++) {
0:           dimColumns[k] = submit.get(k).get();
0:         }
0:       } catch (Exception e) {
0:         LOGGER.error(e, e.getMessage());
0:       }
0:       return dimColumns;
0:     }
0:   }
commit:8cca0af
/////////////////////////////////////////////////////////////////////////
0: import org.apache.carbondata.core.datastore.impl.data.compressed.HeavyCompressedDoubleArrayDataStore;
/////////////////////////////////////////////////////////////////////////
0:   /** statics for one blocklet/page */
0:   class Statistics {
0:     /** min and max value of the measures */
0:     Object[] min, max;
0: 
0:     /**
0:      * the unique value is the non-exist value in the row,
0:      * and will be used as storage key for null values of measures
0:      */
0:     Object[] uniqueValue;
0: 
0:     /** decimal count of the measures */
0:     int[] decimal;
0: 
0:     Statistics(int measureCount) {
0:       max = new Object[measureCount];
0:       min = new Object[measureCount];
0:       uniqueValue = new Object[measureCount];
0:       decimal = new int[measureCount];
0:       for (int i = 0; i < measureCount; i++) {
0:         if (type[i] == CarbonCommonConstants.BIG_INT_MEASURE) {
0:           max[i] = Long.MIN_VALUE;
0:           min[i] = Long.MAX_VALUE;
0:           uniqueValue[i] = Long.MIN_VALUE;
0:         } else if (type[i] == CarbonCommonConstants.DOUBLE_MEASURE) {
0:           max[i] = Double.MIN_VALUE;
0:           min[i] = Double.MAX_VALUE;
0:           uniqueValue[i] = Double.MIN_VALUE;
0:         } else if (type[i] == CarbonCommonConstants.BIG_DECIMAL_MEASURE) {
0:           max[i] = new BigDecimal(Double.MIN_VALUE);
0:           min[i] = new BigDecimal(Double.MAX_VALUE);
0:           uniqueValue[i] = new BigDecimal(Double.MIN_VALUE);
0:         } else {
0:           max[i] = 0.0;
0:           min[i] = 0.0;
0:           uniqueValue[i] = 0.0;
0:         }
0:         decimal[i] = 0;
0:     /**
0:      * update the statistics for the input row
0:      */
0:     void update(int[] msrIndex, Object[] row, boolean compactionFlow) {
0:       // Update row level min max
0:       for (int i = 0; i < msrIndex.length; i++) {
0:         int count = msrIndex[i];
0:         if (row[count] != null) {
0:           if (type[count] == CarbonCommonConstants.DOUBLE_MEASURE) {
0:             double value = (double) row[count];
0:             double maxVal = (double) max[count];
0:             double minVal = (double) min[count];
0:             max[count] = (maxVal > value ? max[count] : value);
0:             min[count] = (minVal < value ? min[count] : value);
0:             int num = getDecimalCount(value);
0:             decimal[count] = (decimal[count] > num ? decimal[count] : num);
0:             uniqueValue[count] = (double) min[count] - 1;
0:           } else if (type[count] == CarbonCommonConstants.BIG_INT_MEASURE) {
0:             long value = (long) row[count];
0:             long maxVal = (long) max[count];
0:             long minVal = (long) min[count];
0:             max[count] = (maxVal > value ? max[count] : value);
0:             min[count] = (minVal < value ? min[count] : value);
0:             uniqueValue[count] = (long) min[count] - 1;
0:           } else if (type[count] == CarbonCommonConstants.BIG_DECIMAL_MEASURE) {
0:             byte[] buff = null;
0:             // in compaction flow the measure with decimal type will come as spark decimal.
0:             // need to convert it to byte array.
0:             if (compactionFlow) {
0:               BigDecimal bigDecimal = ((Decimal) row[count]).toJavaBigDecimal();
0:               buff = DataTypeUtil.bigDecimalToByte(bigDecimal);
0:             } else {
0:               buff = (byte[]) row[count];
0:             }
0:             BigDecimal value = DataTypeUtil.byteToBigDecimal(buff);
0:             decimal[count] = value.scale();
0:             BigDecimal val = (BigDecimal) min[count];
0:             uniqueValue[count] = (val.subtract(new BigDecimal(1.0)));
0:           }
0:         }
0:       }
0:     }
0:   }
0: 
0:   class IndexKey {
0:     byte[] currentMDKey = null;
0:     byte[][] currentNoDictionaryKey = null;
0: 
0:     /** update all keys based on the input row */
0:     void update(Object[] row, boolean firstRow) {
0:       currentMDKey = (byte[]) row[mdKeyIndex];
0:       if (noDictionaryCount > 0 || complexIndexMap.size() > 0) {
0:         currentNoDictionaryKey = (byte[][]) row[mdKeyIndex - 1];
0:       }
0:       if (firstRow) {
0:         startKey = currentMDKey;
0:         noDictStartKey = currentNoDictionaryKey;
0:       }
0:       endKey = currentMDKey;
0:       noDictEndKey = currentNoDictionaryKey;
0:     }
0:   }
0: 
0:   /** generate the NodeHolder from the input rows */
0:   private NodeHolder processDataRows(List<Object[]> dataRows)
0:       throws CarbonDataWriterException {
0:     // to store index of the measure columns which are null
0:     BitSet[] nullValueIndexBitSet = getMeasureNullValueIndexBitSet(measureCount);
0:     // statistics for one blocklet/page
0:     Statistics stats = new Statistics(measureCount);
0:     IndexKey keys = new IndexKey();
0: 
0:     // initialize measureHolder, mdKeyHolder and noDictionaryHolder, these three Holders
0:     // are the input for final encoding
0:     CarbonWriteDataHolder[] measureHolder = initialiseDataHolder(dataRows.size());
0:     CarbonWriteDataHolder mdKeyHolder = initialiseKeyBlockHolder(dataRows.size());
0:     CarbonWriteDataHolder noDictionaryHolder = null;
0:       noDictionaryHolder = initialiseKeyBlockHolderForNonDictionary(dataRows.size());
0:     // loop on the input rows, fill measureHolder, mdKeyHolder and noDictionaryHolder
0:       keys.update(row, (count == 0));
0:       if (keys.currentMDKey.length > 0) {
0:         mdKeyHolder.setWritableByteArrayValueByIndex(count, keys.currentMDKey);
0:       }
0:         noDictionaryHolder.setWritableNonDictByteArrayValueByIndex(count,
0:             keys.currentNoDictionaryKey);
0:       fillMeasureHolder(row, count, measureHolder, nullValueIndexBitSet);
0:       stats.update(otherMeasureIndex, row, compactionFlow);
0:       stats.update(customMeasureIndex, row, compactionFlow);
0:     }
0:     // generate encoded byte array for 3 holders
0:     // for measure columns: encode and compress the measureHolder
0:     WriterCompressModel compressionModel =
0:         ValueCompressionUtil.getWriterCompressModel(
0:             stats.max, stats.min, stats.decimal, stats.uniqueValue, type, new byte[measureCount]);
0:     byte[][] encodedMeasureArray =
0:         HeavyCompressedDoubleArrayDataStore.encodeMeasureDataArray(
0:             compressionModel, measureHolder);
0: 
0:     // for mdkey and noDictionary, it is already in bytes, just get the array from holder
0:     byte[][] mdKeyArray = mdKeyHolder.getByteArrayValues();
0:     byte[][][] noDictionaryArray = null;
0:       noDictionaryArray = noDictionaryHolder.getNonDictByteArrayValues();
0: 
0:     // create NodeHolder using these encoded byte arrays
0:         createNodeHolderObjectWithOutKettle(
0:             encodedMeasureArray, mdKeyArray, noDictionaryArray, dataRows.size(),
0:             keys.startKey, keys.endKey, compressionModel, keys.noDictStartKey, keys.noDictEndKey,
0:             nullValueIndexBitSet);
0:   private void fillMeasureHolder(Object[] row, int count, CarbonWriteDataHolder[] measureHolder,
0:       BitSet[] nullValueIndexBitSet) {
0:     for (int k = 0; k < otherMeasureIndex.length; k++) {
0:       if (type[otherMeasureIndex[k]] == CarbonCommonConstants.BIG_INT_MEASURE) {
0:         if (null == row[otherMeasureIndex[k]]) {
0:           nullValueIndexBitSet[otherMeasureIndex[k]].set(count);
0:           measureHolder[otherMeasureIndex[k]].setWritableLongValueByIndex(count, 0L);
0:         } else {
0:           measureHolder[otherMeasureIndex[k]]
0:               .setWritableLongValueByIndex(count, row[otherMeasureIndex[k]]);
0:         }
0:       } else {
0:         if (null == row[otherMeasureIndex[k]]) {
0:           nullValueIndexBitSet[otherMeasureIndex[k]].set(count);
0:           measureHolder[otherMeasureIndex[k]].setWritableDoubleValueByIndex(count, 0.0);
0:         } else {
0:           measureHolder[otherMeasureIndex[k]]
0:               .setWritableDoubleValueByIndex(count, row[otherMeasureIndex[k]]);
0:         }
0:       }
0:     }
0:     ByteBuffer byteBuffer = null;
0:     byte[] measureBytes = null;
0:     for (int i = 0; i < customMeasureIndex.length; i++) {
0:       if (null == row[customMeasureIndex[i]]
0:           && type[customMeasureIndex[i]] == CarbonCommonConstants.BIG_DECIMAL_MEASURE) {
0:         measureBytes = DataTypeUtil.zeroBigDecimalBytes;
0:         nullValueIndexBitSet[customMeasureIndex[i]].set(count);
0:       } else {
0:         if (this.compactionFlow) {
0:           BigDecimal bigDecimal = ((Decimal) row[customMeasureIndex[i]]).toJavaBigDecimal();
0:           measureBytes = DataTypeUtil.bigDecimalToByte(bigDecimal);
0:         } else {
0:           measureBytes = (byte[]) row[customMeasureIndex[i]];
0:         }
0:       }
0:       byteBuffer = ByteBuffer.allocate(measureBytes.length +
0:           CarbonCommonConstants.INT_SIZE_IN_BYTE);
0:       byteBuffer.putInt(measureBytes.length);
0:       byteBuffer.put(measureBytes);
0:       byteBuffer.flip();
0:       measureBytes = byteBuffer.array();
0:       measureHolder[customMeasureIndex[i]].setWritableByteArrayValueByIndex(count, measureBytes);
0:     }
0:   }
0: 
0:   private NodeHolder createNodeHolderObjectWithOutKettle(byte[][] measureArray, byte[][] mdKeyArray,
0:       byte[][][] noDictionaryArray, int entryCountLocal, byte[] startkeyLocal, byte[] endKeyLocal,
0:       WriterCompressModel compressionModel, byte[][] noDictionaryStartKey,
0:       byte[][] noDictionaryEndKey, BitSet[] nullValueIndexBitSet)
/////////////////////////////////////////////////////////////////////////
0:     DataHolder[] dataHolders = getDataHolders(noOfColumn, mdKeyArray.length);
0:     for (int i = 0; i < mdKeyArray.length; i++) {
0:       byte[][] splitKey = columnarSplitter.splitKey(mdKeyArray[i]);
0:       noDictionaryColumnsData = new byte[noDictionaryCount][noDictionaryArray.length][];
0:       for (int i = 0; i < noDictionaryArray.length; i++) {
0:         byte[][] splitKey = noDictionaryArray[i];
/////////////////////////////////////////////////////////////////////////
0:         .buildDataNodeHolder(blockStorage, measureArray, entryCountLocal, startkeyLocal,
/////////////////////////////////////////////////////////////////////////
0:         if (type[count] == CarbonCommonConstants.DOUBLE_MEASURE) {
commit:78a1c19
/////////////////////////////////////////////////////////////////////////
0:     BitSet[] nullvalueIndexBitset = new BitSet[measureCount];
commit:ce09aaa
/////////////////////////////////////////////////////////////////////////
0: import org.apache.carbondata.core.datastore.block.SegmentProperties;
0: import org.apache.carbondata.core.datastore.columnar.BlockIndexerStorageForInt;
0: import org.apache.carbondata.core.datastore.columnar.BlockIndexerStorageForNoInvertedIndex;
0: import org.apache.carbondata.core.datastore.columnar.ColumnGroupModel;
0: import org.apache.carbondata.core.datastore.columnar.IndexStorage;
0: import org.apache.carbondata.core.datastore.compression.WriterCompressModel;
0: import org.apache.carbondata.core.datastore.dataholder.CarbonWriteDataHolder;
0: import org.apache.carbondata.core.metadata.CarbonMetadata;
1: import org.apache.carbondata.core.metadata.ColumnarFormatVersion;
0: import org.apache.carbondata.core.metadata.schema.table.CarbonTable;
0: import org.apache.carbondata.core.metadata.schema.table.column.ColumnSchema;
commit:3fe6903
/////////////////////////////////////////////////////////////////////////
/////////////////////////////////////////////////////////////////////////
commit:360edc8
/////////////////////////////////////////////////////////////////////////
0: import org.apache.carbondata.core.datastorage.store.compression.WriterCompressModel;
/////////////////////////////////////////////////////////////////////////
0:     WriterCompressModel compressionModel = ValueCompressionUtil
0:         .getWriterCompressModel(max, min, decimal, uniqueValue, type, new byte[max.length]);
/////////////////////////////////////////////////////////////////////////
0:     WriterCompressModel compressionModel = ValueCompressionUtil
0:         .getWriterCompressModel(max, min, decimal, uniqueValue, type, new byte[max.length]);
/////////////////////////////////////////////////////////////////////////
0:       WriterCompressModel compressionModel, byte[][] noDictionaryData,
/////////////////////////////////////////////////////////////////////////
0:       WriterCompressModel compressionModel, byte[][][] noDictionaryData,
commit:0ef3fb8
/////////////////////////////////////////////////////////////////////////
0: import org.apache.carbondata.core.carbon.ColumnarFormatVersion;
/////////////////////////////////////////////////////////////////////////
0:     ColumnarFormatVersion version = CarbonProperties.getInstance().getFormatVersion();
author:rahulforallp
-------------------------------------------------------------------------------
commit:a4083bf
/////////////////////////////////////////////////////////////////////////
1: import org.apache.carbondata.core.metadata.schema.table.column.CarbonDimension;
/////////////////////////////////////////////////////////////////////////
0:     String noInvertedIdxCol = "";
1:     for (CarbonDimension cd : model.getSegmentProperties().getDimensions()) {
1:       if (!cd.isUseInvertedIndex()) {
0:         noInvertedIdxCol += (cd.getColName() + ",");
0:       }
0:     }
0:     LOGGER.info("Columns considered as NoInverted Index are " + noInvertedIdxCol);
commit:f1f0e3e
/////////////////////////////////////////////////////////////////////////
0:           return new BlockIndexerStorageForNoInvertedIndexForShort(this.data,isNoDictionary);
author:manishgupta88
-------------------------------------------------------------------------------
commit:83aae94
/////////////////////////////////////////////////////////////////////////
0: import org.apache.carbondata.processing.util.CarbonDataProcessorUtil;
/////////////////////////////////////////////////////////////////////////
0:       for (int i = 0; i < model.getTableSpec().getDimensionSpec().getNumSimpleDimensions(); i++) {
0:         if (CarbonDataProcessorUtil
0:             .isRleApplicableForColumn(model.getTableSpec().getDimensionSpec().getType(i))) {
0:           this.rleEncodingForDictDimension[i] = true;
/////////////////////////////////////////////////////////////////////////
commit:fc1af96
/////////////////////////////////////////////////////////////////////////
0:   private long schemaUpdatedTimeStamp;
0: 
/////////////////////////////////////////////////////////////////////////
0:     this.schemaUpdatedTimeStamp = carbonFactDataHandlerModel.getSchemaUpdatedTimeStamp();
/////////////////////////////////////////////////////////////////////////
0:     carbonDataWriterVo.setSchemaUpdatedTimeStamp(schemaUpdatedTimeStamp);
author:Yadong Qi
-------------------------------------------------------------------------------
commit:82741c1
/////////////////////////////////////////////////////////////////////////
0: import org.apache.carbondata.processing.newflow.sort.SortScopeOptions;
/////////////////////////////////////////////////////////////////////////
0:   private SortScopeOptions.SortScope sortScope;
0: 
/////////////////////////////////////////////////////////////////////////
0:     this.sortScope = model.getSortScope();
/////////////////////////////////////////////////////////////////////////
0:     if (sortScope != null && sortScope.equals(SortScopeOptions.SortScope.GLOBAL_SORT)) {
0:       numberOfCores = 1;
0:     }
0: 
/////////////////////////////////////////////////////////////////////////
0: }
commit:f715126
/////////////////////////////////////////////////////////////////////////
0:     try {
1:       semaphore.acquire();
0:       producerExecutorServiceTaskList.add(producerExecutorService
0:           .submit(new Producer(blockletDataHolder, dataRows, ++writerTaskSequenceCounter, true)));
0:       blockletProcessingCount.incrementAndGet();
0:       processedDataCount += entryCount;
0:       closeWriterExecutionService(producerExecutorService);
0:       processWriteTaskSubmitList(producerExecutorServiceTaskList);
0:       processingComplete = true;
0:     } catch (InterruptedException e) {
0:       LOGGER.error(e, e.getMessage());
0:       throw new CarbonDataWriterException(e.getMessage(), e);
0:     }
author:QiangCai
-------------------------------------------------------------------------------
commit:9f94529
/////////////////////////////////////////////////////////////////////////
0:     boolean isSortColumn = false;
0:       isSortColumn = i < segmentProperties.getNumberOfSortColumns();
0:               new BlockSortThread(i, dataHolders[dictionaryColumnCount].getData(), isSortColumn,
0:                   isUseInvertedIndex[i] & isSortColumn)));
/////////////////////////////////////////////////////////////////////////
0:                 isSortColumn, isUseInvertedIndex[i] & isSortColumn)));
/////////////////////////////////////////////////////////////////////////
0: 
0:     int numberOfDictSortColumns = segmentProperties.getNumberOfDictSortColumns();
0:     // generate start/end key by sort_columns
0:     if (numberOfDictSortColumns > 0) {
0:       // if sort_columns contain dictionary columns
0:       int[] keySize = columnarSplitter.getBlockKeySize();
0:       if (keySize.length > numberOfDictSortColumns) {
0:         int newMdkLength = 0;
0:         for (int index = 0; index < numberOfDictSortColumns; index++) {
0:           newMdkLength += keySize[index];
0:         }
0:         byte[] newStartKeyOfSortKey = new byte[newMdkLength];
0:         byte[] newEndKeyOfSortKey = new byte[newMdkLength];
0:         System.arraycopy(startkeyLocal, 0, newStartKeyOfSortKey, 0, newMdkLength);
0:         System.arraycopy(endKeyLocal, 0, newEndKeyOfSortKey, 0, newMdkLength);
0:         startkeyLocal = newStartKeyOfSortKey;
0:         endKeyLocal = newEndKeyOfSortKey;
0:       }
0:     } else {
0:       startkeyLocal = new byte[0];
0:       endKeyLocal = new byte[0];
0:     }
0: 
0:     int numberOfNoDictSortColumns = segmentProperties.getNumberOfNoDictSortColumns();
0:     if (numberOfNoDictSortColumns > 0) {
0:       // if sort_columns contain no-dictionary columns
0:       if (noDictionaryStartKey.length > numberOfNoDictSortColumns) {
0:         byte[][] newNoDictionaryStartKey = new byte[numberOfNoDictSortColumns][];
0:         byte[][] newNoDictionaryEndKey = new byte[numberOfNoDictSortColumns][];
0:         System.arraycopy(noDictionaryStartKey, 0, newNoDictionaryStartKey, 0,
0:             numberOfNoDictSortColumns);
0:         System
0:             .arraycopy(noDictionaryEndKey, 0, newNoDictionaryEndKey, 0, numberOfNoDictSortColumns);
0:         noDictionaryStartKey = newNoDictionaryStartKey;
0:         noDictionaryEndKey = newNoDictionaryEndKey;
0:       }
commit:256dbed
/////////////////////////////////////////////////////////////////////////
0:           BigDecimal bd = new BigDecimal(CarbonCommonConstants.POINT + bigdVals[1]);
0:           bigDvalue[1] = (long) (bd.doubleValue() * Math.pow(10, value.scale()));
commit:41347d8
/////////////////////////////////////////////////////////////////////////
1:  * Licensed to the Apache Software Foundation (ASF) under one or more
1:  * contributor license agreements.  See the NOTICE file distributed with
1:  * this work for additional information regarding copyright ownership.
1:  * The ASF licenses this file to You under the Apache License, Version 2.0
1:  * (the "License"); you may not use this file except in compliance with
1:  * the License.  You may obtain a copy of the License at
1:  * Unless required by applicable law or agreed to in writing, software
1:  * distributed under the License is distributed on an "AS IS" BASIS,
1:  * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
1:  * See the License for the specific language governing permissions and
1:  * limitations under the License.
commit:498cf98
/////////////////////////////////////////////////////////////////////////
/////////////////////////////////////////////////////////////////////////
/////////////////////////////////////////////////////////////////////////
0:         Long[] bigdMinVal = new Long[2];
0:         bigdMinVal[0] = Long.MIN_VALUE;
0:         bigdMinVal[1] = Long.MIN_VALUE;
0:         max[i] = bigdMinVal;
/////////////////////////////////////////////////////////////////////////
0:         Long[] bigdMaxVal = new Long[2];
0:         bigdMaxVal[0] = Long.MAX_VALUE;
0:         bigdMaxVal[1] = Long.MAX_VALUE;
0:         min[i] = bigdMaxVal;
0:         Long[] bigdUniqueVal = new Long[2];
0:         bigdUniqueVal[0] = Long.MIN_VALUE;
0:         bigdUniqueVal[1] = Long.MIN_VALUE;
0:         uniqueValue[i] = bigdUniqueVal;
/////////////////////////////////////////////////////////////////////////
0:         BigDecimal value = DataTypeUtil.byteToBigDecimal(b);
0:         String[] bigdVals = value.toPlainString().split("\\.");
0:         long[] bigDvalue = new long[2];
0:         if (bigdVals.length == 2) {
0:           bigDvalue[0] = Long.parseLong(bigdVals[0]);
0:           BigDecimal bd = new BigDecimal(CarbonCommonConstants.POINT+bigdVals[1]);
0:           bigDvalue[1] = (long)(bd.doubleValue()*Math.pow(10, value.scale()));
0:         } else {
0:           bigDvalue[0] = Long.parseLong(bigdVals[0]);
0:         }
0:         dataHolder[customMeasureIndex[i]].setWritableBigDecimalValueByIndex(count, bigDvalue);
author:mohammadshahidkhan
-------------------------------------------------------------------------------
commit:3af2d65
/////////////////////////////////////////////////////////////////////////
/////////////////////////////////////////////////////////////////////////
commit:3251c89
/////////////////////////////////////////////////////////////////////////
0: import org.apache.carbondata.processing.util.CarbonDataProcessorUtil;
/////////////////////////////////////////////////////////////////////////
0:   private String segmentId;
/////////////////////////////////////////////////////////////////////////
0:     this.segmentId = carbonFactDataHandlerModel.getSegmentId();
/////////////////////////////////////////////////////////////////////////
0:       // rename the bad record in progress to normal
0:       CarbonDataProcessorUtil.renameBadRecordsFromInProgressToNormal(
0:           this.databaseName + File.separator + this.tableName + File.separator + this.segmentId
0:               + File.separator + this.carbonDataFileAttributes.getTaskId());
author:ashok.blend
-------------------------------------------------------------------------------
commit:9e24a3d
/////////////////////////////////////////////////////////////////////////
0:         max[i] = new BigDecimal(0.0);
/////////////////////////////////////////////////////////////////////////
0:         min[i] = new BigDecimal(Double.MAX_VALUE);
0:         uniqueValue[i] = new BigDecimal(Double.MIN_VALUE);
/////////////////////////////////////////////////////////////////////////
/////////////////////////////////////////////////////////////////////////
0:         max[i] = new BigDecimal(0.0);
/////////////////////////////////////////////////////////////////////////
0:         min[i] = new BigDecimal(Double.MAX_VALUE);
0:         uniqueValue[i] = new BigDecimal(Double.MIN_VALUE);
/////////////////////////////////////////////////////////////////////////
/////////////////////////////////////////////////////////////////////////
/////////////////////////////////////////////////////////////////////////
0:         BigDecimal val = (BigDecimal) minValue[i];
0:         uniqueValue[i] = (val.subtract(new BigDecimal(1.0)));
/////////////////////////////////////////////////////////////////////////
commit:63d6626
/////////////////////////////////////////////////////////////////////////
0:         //max[i] = new BigDecimal(0.0);
0:         Long[] bigdMinVal = new Long[2];
0:         bigdMinVal[0] = Long.MIN_VALUE;
0:         bigdMinVal[1] = Long.MIN_VALUE;
0:         max[i] = bigdMinVal;
/////////////////////////////////////////////////////////////////////////
0:         Long[] bigdMaxVal = new Long[2];
0:         bigdMaxVal[0] = Long.MAX_VALUE;
0:         bigdMaxVal[1] = Long.MAX_VALUE;
0:         //min[i] = new BigDecimal(Double.MAX_VALUE);
0:         min[i] = bigdMaxVal;
0:         Long[] bigdUniqueVal = new Long[2];
0:         bigdUniqueVal[0] = Long.MIN_VALUE;
0:         bigdUniqueVal[1] = Long.MIN_VALUE;
0:         uniqueValue[i] = bigdUniqueVal;
/////////////////////////////////////////////////////////////////////////
0:         BigDecimal value = DataTypeUtil.byteToBigDecimal(b);
0:         String[] bigdVals = value.toPlainString().split("\\.");
0:         long[] bigDvalue = new long[2];
0:         if (bigdVals.length == 2) {
0:           bigDvalue[0] = Long.parseLong(bigdVals[0]);
0:           BigDecimal bd = new BigDecimal(CarbonCommonConstants.POINT+bigdVals[1]);
0:           bigDvalue[1] = (long)(bd.doubleValue()*Math.pow(10, value.scale()));
0:           //bigDvalue[1] = Long.parseLong(bigdVals[1]);
0:         } else {
0:           bigDvalue[0] = Long.parseLong(bigdVals[0]);
0:         }
0:         dataHolder[customMeasureIndex[i]].setWritableBigDecimalValueByIndex(count, bigDvalue);
/////////////////////////////////////////////////////////////////////////
0:           decimal[count] = value.scale();
0:           String[] bigdVals = value.toPlainString().split("\\.");
0:           Long[] maxVal = (Long[])max[count];
0:           Long[] minVal = (Long[])min[count];
0:           long maxLeftVal = (long)maxVal[0];
0:           long maxRightVal = (long)maxVal[1];
0:           long minLeftVal = (long) minVal[0];
0:           long minRightVal = (long)minVal[1];
0:           if (bigdVals.length == 2) {
0:             long leftPart = Long.parseLong(bigdVals[0]);
0:             BigDecimal bd = new BigDecimal(CarbonCommonConstants.POINT+bigdVals[1]);
0:             long rightPart = (long)(bd.doubleValue()*Math.pow(10, value.scale()));
0:             //Long.parseLong(bigdVals[1]);
0:             maxVal[0] = (maxLeftVal > leftPart ? maxLeftVal : leftPart);
0:             maxVal[1] = (maxRightVal > rightPart ? maxRightVal : rightPart);
0:             minVal[0] = (minLeftVal < leftPart ? minLeftVal : leftPart);
0:             minVal[1] = (minRightVal < rightPart ? minRightVal : rightPart);
0:           } else {
0:             long leftPart = Long.parseLong(bigdVals[0]);
0:             maxVal[0] = (maxLeftVal > leftPart ? maxLeftVal : leftPart);
0:             minVal[0] = (minLeftVal < leftPart ? minLeftVal : leftPart);
0:           }
/////////////////////////////////////////////////////////////////////////
0:         Long[] bigdMinVal = (Long[])minValue[i];
0:         Long[] bigdUniqVal = (Long[])uniqueValue[i];
0:         bigdUniqVal[0] = bigdMinVal[0] -1;
0:         bigdUniqVal[1] = bigdMinVal[1] -1;
/////////////////////////////////////////////////////////////////////////
0:       dataHolder[customMeasureIndex[i]].initialiseBigDecimalValues(size);
author:Ashok Kumar
-------------------------------------------------------------------------------
commit:20af74b
/////////////////////////////////////////////////////////////////////////
commit:276a3ff
/////////////////////////////////////////////////////////////////////////
0:    * @param double value
0:    * @return it return no of value after decimal
0:    */
0:   private int getDecimalCount(double value) {
0:     String strValue = BigDecimal.valueOf(Math.abs(value)).toPlainString();
0:     int integerPlaces = strValue.indexOf('.');
0:     int decimalPlaces = 0;
0:     if (-1 != integerPlaces) {
0:       decimalPlaces = strValue.length() - integerPlaces - 1;
0:     }
0:     return decimalPlaces;
0:   }
0:   /**
/////////////////////////////////////////////////////////////////////////
0:           int num = getDecimalCount(value);
/////////////////////////////////////////////////////////////////////////
0:           int num = getDecimalCount(value);
author:Jay357089
-------------------------------------------------------------------------------
commit:cd61beb
/////////////////////////////////////////////////////////////////////////
0:    * table block size in MB
/////////////////////////////////////////////////////////////////////////
0:     this.tableBlockSize = carbonFactDataHandlerModel.getBlockSizeInMB();
author:Zhangshunyu
-------------------------------------------------------------------------------
commit:793d690
/////////////////////////////////////////////////////////////////////////
0:    * table block size
0:    */
0:   private int tableBlockSize;
0:   /**
/////////////////////////////////////////////////////////////////////////
0:     this.tableBlockSize = carbonFactDataHandlerModel.getBlocksize();
/////////////////////////////////////////////////////////////////////////
0:         segmentProperties, tableBlockSize);
============================================================================