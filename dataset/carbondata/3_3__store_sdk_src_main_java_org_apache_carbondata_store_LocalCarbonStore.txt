1:cfb8ed9: /*
1:cfb8ed9:  * Licensed to the Apache Software Foundation (ASF) under one or more
1:cfb8ed9:  * contributor license agreements.  See the NOTICE file distributed with
1:cfb8ed9:  * this work for additional information regarding copyright ownership.
1:cfb8ed9:  * The ASF licenses this file to You under the Apache License, Version 2.0
1:cfb8ed9:  * (the "License"); you may not use this file except in compliance with
1:cfb8ed9:  * the License.  You may obtain a copy of the License at
1:cfb8ed9:  *
1:cfb8ed9:  *    http://www.apache.org/licenses/LICENSE-2.0
1:cfb8ed9:  *
1:cfb8ed9:  * Unless required by applicable law or agreed to in writing, software
1:cfb8ed9:  * distributed under the License is distributed on an "AS IS" BASIS,
1:cfb8ed9:  * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
1:cfb8ed9:  * See the License for the specific language governing permissions and
1:cfb8ed9:  * limitations under the License.
1:cfb8ed9:  */
1:cfb8ed9: 
1:cfb8ed9: package org.apache.carbondata.store;
1:cfb8ed9: 
1:cfb8ed9: import java.io.IOException;
1:cfb8ed9: import java.util.ArrayList;
1:cfb8ed9: import java.util.Iterator;
1:cfb8ed9: import java.util.List;
1:cfb8ed9: import java.util.Objects;
1:cfb8ed9: 
1:cfb8ed9: import org.apache.carbondata.common.annotations.InterfaceAudience;
1:7ef9164: import org.apache.carbondata.common.logging.LogService;
1:7ef9164: import org.apache.carbondata.common.logging.LogServiceFactory;
1:cfb8ed9: import org.apache.carbondata.core.datastore.row.CarbonRow;
1:9f42fbf: import org.apache.carbondata.core.metadata.AbsoluteTableIdentifier;
1:cfb8ed9: import org.apache.carbondata.core.metadata.schema.table.CarbonTable;
1:cfb8ed9: import org.apache.carbondata.core.scan.expression.Expression;
1:cfb8ed9: import org.apache.carbondata.hadoop.CarbonProjection;
1:cfb8ed9: import org.apache.carbondata.hadoop.api.CarbonInputFormat;
1:cfb8ed9: import org.apache.carbondata.hadoop.api.CarbonTableInputFormat;
1:cfb8ed9: 
1:cfb8ed9: import org.apache.hadoop.conf.Configuration;
1:cfb8ed9: import org.apache.hadoop.mapreduce.InputSplit;
1:cfb8ed9: import org.apache.hadoop.mapreduce.Job;
1:cfb8ed9: import org.apache.hadoop.mapreduce.JobID;
1:cfb8ed9: import org.apache.hadoop.mapreduce.RecordReader;
1:cfb8ed9: import org.apache.hadoop.mapreduce.TaskAttemptID;
1:cfb8ed9: import org.apache.hadoop.mapreduce.task.JobContextImpl;
1:cfb8ed9: import org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl;
1:cfb8ed9: 
1:cfb8ed9: /**
1:cfb8ed9:  * A CarbonStore implementation that works locally, without other compute framework dependency.
1:cfb8ed9:  * It can be used to read data in local disk.
1:cfb8ed9:  *
1:cfb8ed9:  * Note that this class is experimental, it is not intended to be used in production.
1:cfb8ed9:  */
1:cfb8ed9: @InterfaceAudience.Internal
1:cfb8ed9: class LocalCarbonStore extends MetaCachedCarbonStore {
1:cfb8ed9: 
1:7ef9164:   private static final LogService LOGGER =
1:7ef9164:       LogServiceFactory.getLogService(LocalCarbonStore.class.getName());
1:7ef9164: 
2:cfb8ed9:   @Override
1:9f42fbf:   public Iterator<CarbonRow> scan(AbsoluteTableIdentifier tableIdentifier, String[] projectColumns)
1:9f42fbf:       throws IOException {
1:9f42fbf:     return scan(tableIdentifier, projectColumns, null);
1:cfb8ed9:   }
1:cfb8ed9: 
1:9f42fbf:   @Override
1:9f42fbf:   public Iterator<CarbonRow> scan(AbsoluteTableIdentifier tableIdentifier, String[] projectColumns,
1:9f42fbf:       Expression filter) throws IOException {
1:9f42fbf:     Objects.requireNonNull(tableIdentifier);
1:cfb8ed9:     Objects.requireNonNull(projectColumns);
1:cfb8ed9: 
1:9f42fbf:     CarbonTable table = getTable(tableIdentifier.getTablePath());
1:2ea3b2d:     if (table.isStreamingSink() || table.isHivePartitionTable()) {
1:cfb8ed9:       throw new UnsupportedOperationException("streaming and partition table is not supported");
1:cfb8ed9:     }
1:cfb8ed9:     // TODO: use InputFormat to prune data and read data
1:cfb8ed9: 
1:cfb8ed9:     final CarbonTableInputFormat format = new CarbonTableInputFormat();
1:cfb8ed9:     final Job job = new Job(new Configuration());
1:cfb8ed9:     CarbonInputFormat.setTableInfo(job.getConfiguration(), table.getTableInfo());
1:cfb8ed9:     CarbonInputFormat.setTablePath(job.getConfiguration(), table.getTablePath());
1:cfb8ed9:     CarbonInputFormat.setTableName(job.getConfiguration(), table.getTableName());
1:cfb8ed9:     CarbonInputFormat.setDatabaseName(job.getConfiguration(), table.getDatabaseName());
1:cfb8ed9:     CarbonInputFormat.setCarbonReadSupport(job.getConfiguration(), CarbonRowReadSupport.class);
1:7ef9164:     CarbonInputFormat
1:7ef9164:         .setColumnProjection(job.getConfiguration(), new CarbonProjection(projectColumns));
1:cfb8ed9:     if (filter != null) {
1:cfb8ed9:       CarbonInputFormat.setFilterPredicates(job.getConfiguration(), filter);
1:cfb8ed9:     }
1:cfb8ed9: 
1:cfb8ed9:     final List<InputSplit> splits =
1:cfb8ed9:         format.getSplits(new JobContextImpl(job.getConfiguration(), new JobID()));
1:cfb8ed9: 
1:cfb8ed9:     List<RecordReader<Void, Object>> readers = new ArrayList<>(splits.size());
1:cfb8ed9: 
1:cfb8ed9:     List<CarbonRow> rows = new ArrayList<>();
1:7ef9164: 
1:cfb8ed9:     try {
1:cfb8ed9:       for (InputSplit split : splits) {
1:cfb8ed9:         TaskAttemptContextImpl attempt =
1:cfb8ed9:             new TaskAttemptContextImpl(job.getConfiguration(), new TaskAttemptID());
1:cfb8ed9:         RecordReader reader = format.createRecordReader(split, attempt);
1:cfb8ed9:         reader.initialize(split, attempt);
1:cfb8ed9:         readers.add(reader);
1:cfb8ed9:       }
1:cfb8ed9: 
1:cfb8ed9:       for (RecordReader<Void, Object> reader : readers) {
1:cfb8ed9:         while (reader.nextKeyValue()) {
1:7ef9164:           rows.add((CarbonRow) reader.getCurrentValue());
1:cfb8ed9:         }
1:cfb8ed9:         try {
1:7ef9164:           reader.close();
1:7ef9164:         } catch (IOException e) {
1:7ef9164:           LOGGER.error(e);
1:cfb8ed9:         }
1:cfb8ed9:       }
2:cfb8ed9:     } catch (InterruptedException e) {
2:cfb8ed9:       throw new IOException(e);
1:7ef9164:     } finally {
1:7ef9164:       for (RecordReader<Void, Object> reader : readers) {
2:7ef9164:         try {
1:7ef9164:           reader.close();
1:7ef9164:         } catch (IOException e) {
1:7ef9164:           LOGGER.error(e);
1:cfb8ed9:         }
2:7ef9164:       }
1:7ef9164:     }
1:cfb8ed9:     return rows.iterator();
1:cfb8ed9:   }
1:cfb8ed9: 
1:cfb8ed9:   @Override
1:cfb8ed9:   public Iterator<CarbonRow> sql(String sqlString) throws IOException {
1:cfb8ed9:     throw new UnsupportedOperationException();
1:cfb8ed9:   }
1:cfb8ed9: }
============================================================================
author:rahul
-------------------------------------------------------------------------------
commit:9f42fbf
/////////////////////////////////////////////////////////////////////////
1: import org.apache.carbondata.core.metadata.AbsoluteTableIdentifier;
/////////////////////////////////////////////////////////////////////////
1:   public Iterator<CarbonRow> scan(AbsoluteTableIdentifier tableIdentifier, String[] projectColumns)
1:       throws IOException {
1:     return scan(tableIdentifier, projectColumns, null);
1:   @Override
1:   public Iterator<CarbonRow> scan(AbsoluteTableIdentifier tableIdentifier, String[] projectColumns,
1:       Expression filter) throws IOException {
1:     Objects.requireNonNull(tableIdentifier);
1:     CarbonTable table = getTable(tableIdentifier.getTablePath());
author:Jacky Li
-------------------------------------------------------------------------------
commit:2ea3b2d
/////////////////////////////////////////////////////////////////////////
1:     if (table.isStreamingSink() || table.isHivePartitionTable()) {
commit:cfb8ed9
/////////////////////////////////////////////////////////////////////////
1: /*
1:  * Licensed to the Apache Software Foundation (ASF) under one or more
1:  * contributor license agreements.  See the NOTICE file distributed with
1:  * this work for additional information regarding copyright ownership.
1:  * The ASF licenses this file to You under the Apache License, Version 2.0
1:  * (the "License"); you may not use this file except in compliance with
1:  * the License.  You may obtain a copy of the License at
1:  *
1:  *    http://www.apache.org/licenses/LICENSE-2.0
1:  *
1:  * Unless required by applicable law or agreed to in writing, software
1:  * distributed under the License is distributed on an "AS IS" BASIS,
1:  * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
1:  * See the License for the specific language governing permissions and
1:  * limitations under the License.
1:  */
1: 
1: package org.apache.carbondata.store;
1: 
1: import java.io.IOException;
1: import java.util.ArrayList;
1: import java.util.Iterator;
1: import java.util.List;
1: import java.util.Objects;
1: 
1: import org.apache.carbondata.common.annotations.InterfaceAudience;
1: import org.apache.carbondata.core.datastore.row.CarbonRow;
1: import org.apache.carbondata.core.metadata.schema.table.CarbonTable;
1: import org.apache.carbondata.core.scan.expression.Expression;
1: import org.apache.carbondata.hadoop.CarbonProjection;
1: import org.apache.carbondata.hadoop.api.CarbonInputFormat;
1: import org.apache.carbondata.hadoop.api.CarbonTableInputFormat;
1: 
1: import org.apache.hadoop.conf.Configuration;
1: import org.apache.hadoop.mapreduce.InputSplit;
1: import org.apache.hadoop.mapreduce.Job;
1: import org.apache.hadoop.mapreduce.JobID;
1: import org.apache.hadoop.mapreduce.RecordReader;
1: import org.apache.hadoop.mapreduce.TaskAttemptID;
1: import org.apache.hadoop.mapreduce.task.JobContextImpl;
1: import org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl;
1: 
1: /**
1:  * A CarbonStore implementation that works locally, without other compute framework dependency.
1:  * It can be used to read data in local disk.
1:  *
1:  * Note that this class is experimental, it is not intended to be used in production.
1:  */
1: @InterfaceAudience.Internal
1: class LocalCarbonStore extends MetaCachedCarbonStore {
1: 
1:   @Override
0:   public Iterator<CarbonRow> scan(String path, String[] projectColumns) throws IOException {
0:     return scan(path, projectColumns, null);
1:   }
1: 
1:   @Override
0:   public Iterator<CarbonRow> scan(String path, String[] projectColumns, Expression filter)
0:       throws IOException {
0:     Objects.requireNonNull(path);
1:     Objects.requireNonNull(projectColumns);
1: 
0:     CarbonTable table = getTable(path);
0:     if (table.isStreamingTable() || table.isHivePartitionTable()) {
1:       throw new UnsupportedOperationException("streaming and partition table is not supported");
1:     }
1:     // TODO: use InputFormat to prune data and read data
1: 
1:     final CarbonTableInputFormat format = new CarbonTableInputFormat();
1:     final Job job = new Job(new Configuration());
1:     CarbonInputFormat.setTableInfo(job.getConfiguration(), table.getTableInfo());
1:     CarbonInputFormat.setTablePath(job.getConfiguration(), table.getTablePath());
1:     CarbonInputFormat.setTableName(job.getConfiguration(), table.getTableName());
1:     CarbonInputFormat.setDatabaseName(job.getConfiguration(), table.getDatabaseName());
1:     CarbonInputFormat.setCarbonReadSupport(job.getConfiguration(), CarbonRowReadSupport.class);
0:     CarbonInputFormat.setColumnProjection(
0:         job.getConfiguration(), new CarbonProjection(projectColumns));
1:     if (filter != null) {
1:       CarbonInputFormat.setFilterPredicates(job.getConfiguration(), filter);
1:     }
1: 
1:     final List<InputSplit> splits =
1:         format.getSplits(new JobContextImpl(job.getConfiguration(), new JobID()));
1: 
1:     List<RecordReader<Void, Object>> readers = new ArrayList<>(splits.size());
1: 
1:     try {
1:       for (InputSplit split : splits) {
1:         TaskAttemptContextImpl attempt =
1:             new TaskAttemptContextImpl(job.getConfiguration(), new TaskAttemptID());
1:         RecordReader reader = format.createRecordReader(split, attempt);
1:         reader.initialize(split, attempt);
1:         readers.add(reader);
1:       }
1:     } catch (InterruptedException e) {
1:       throw new IOException(e);
1:     }
1: 
1:     List<CarbonRow> rows = new ArrayList<>();
1:     try {
1:       for (RecordReader<Void, Object> reader : readers) {
1:         while (reader.nextKeyValue()) {
0:           rows.add((CarbonRow)reader.getCurrentValue());
1:         }
1:       }
1:     } catch (InterruptedException e) {
1:       throw new IOException(e);
1:     }
1:     return rows.iterator();
1:   }
1: 
1:   @Override
1:   public Iterator<CarbonRow> sql(String sqlString) throws IOException {
1:     throw new UnsupportedOperationException();
1:   }
1: }
author:Raghunandan S
-------------------------------------------------------------------------------
commit:7ef9164
/////////////////////////////////////////////////////////////////////////
1: import org.apache.carbondata.common.logging.LogService;
1: import org.apache.carbondata.common.logging.LogServiceFactory;
/////////////////////////////////////////////////////////////////////////
1:   private static final LogService LOGGER =
1:       LogServiceFactory.getLogService(LocalCarbonStore.class.getName());
1: 
0:   @Override public Iterator<CarbonRow> scan(String path, String[] projectColumns, Expression filter)
/////////////////////////////////////////////////////////////////////////
1:     CarbonInputFormat
1:         .setColumnProjection(job.getConfiguration(), new CarbonProjection(projectColumns));
/////////////////////////////////////////////////////////////////////////
0:     List<CarbonRow> rows = new ArrayList<>();
1: 
/////////////////////////////////////////////////////////////////////////
1:           rows.add((CarbonRow) reader.getCurrentValue());
1:         }
1:         try {
1:           reader.close();
1:         } catch (IOException e) {
1:           LOGGER.error(e);
1:     } finally {
1:       for (RecordReader<Void, Object> reader : readers) {
1:         try {
1:           reader.close();
1:         } catch (IOException e) {
1:           LOGGER.error(e);
1:         }
1:       }
============================================================================