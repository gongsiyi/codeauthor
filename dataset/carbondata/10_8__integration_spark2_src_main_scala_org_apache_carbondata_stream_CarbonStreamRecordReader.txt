1:d7393da: /*
1:d7393da:  * Licensed to the Apache Software Foundation (ASF) under one or more
1:d7393da:  * contributor license agreements.  See the NOTICE file distributed with
1:d7393da:  * this work for additional information regarding copyright ownership.
1:d7393da:  * The ASF licenses this file to You under the Apache License, Version 2.0
1:d7393da:  * (the "License"); you may not use this file except in compliance with
1:d7393da:  * the License.  You may obtain a copy of the License at
1:d7393da:  *
1:d7393da:  *    http://www.apache.org/licenses/LICENSE-2.0
1:d7393da:  *
1:d7393da:  * Unless required by applicable law or agreed to in writing, software
1:d7393da:  * distributed under the License is distributed on an "AS IS" BASIS,
1:d7393da:  * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
1:d7393da:  * See the License for the specific language governing permissions and
1:d7393da:  * limitations under the License.
1:d7393da:  */
1:bcef656: 
1:bcef656: package org.apache.carbondata.stream;
4:d7393da: 
1:d7393da: import java.io.IOException;
1:d7393da: import java.math.BigDecimal;
1:d7393da: import java.nio.ByteBuffer;
1:d7393da: import java.util.BitSet;
1:d7393da: import java.util.HashMap;
1:d7393da: import java.util.List;
1:d7393da: import java.util.Map;
1:d7393da: 
1:d7393da: import org.apache.carbondata.core.cache.Cache;
1:d7393da: import org.apache.carbondata.core.cache.CacheProvider;
1:d7393da: import org.apache.carbondata.core.cache.CacheType;
1:d7393da: import org.apache.carbondata.core.cache.dictionary.Dictionary;
1:d7393da: import org.apache.carbondata.core.cache.dictionary.DictionaryColumnUniqueIdentifier;
1:d7393da: import org.apache.carbondata.core.constants.CarbonCommonConstants;
1:d7393da: import org.apache.carbondata.core.datastore.block.SegmentProperties;
1:8f08c4a: import org.apache.carbondata.core.datastore.compression.CompressorFactory;
1:d7393da: import org.apache.carbondata.core.keygenerator.directdictionary.DirectDictionaryGenerator;
1:d7393da: import org.apache.carbondata.core.keygenerator.directdictionary.DirectDictionaryKeyGeneratorFactory;
1:21a72bf: import org.apache.carbondata.core.metadata.blocklet.index.BlockletMinMaxIndex;
1:933e30c: import org.apache.carbondata.core.metadata.datatype.DataType;
1:d7393da: import org.apache.carbondata.core.metadata.datatype.DataTypes;
1:d7393da: import org.apache.carbondata.core.metadata.encoder.Encoding;
1:d7393da: import org.apache.carbondata.core.metadata.schema.table.CarbonTable;
1:d7393da: import org.apache.carbondata.core.metadata.schema.table.column.CarbonColumn;
1:d7393da: import org.apache.carbondata.core.metadata.schema.table.column.CarbonDimension;
1:d7393da: import org.apache.carbondata.core.metadata.schema.table.column.CarbonMeasure;
1:d7393da: import org.apache.carbondata.core.metadata.schema.table.column.ColumnSchema;
1:d7393da: import org.apache.carbondata.core.reader.CarbonHeaderReader;
1:d7393da: import org.apache.carbondata.core.scan.expression.exception.FilterUnsupportedException;
1:d7393da: import org.apache.carbondata.core.scan.filter.FilterUtil;
1:d7393da: import org.apache.carbondata.core.scan.filter.GenericQueryType;
1:d7393da: import org.apache.carbondata.core.scan.filter.executer.FilterExecuter;
1:d7393da: import org.apache.carbondata.core.scan.filter.intf.RowImpl;
1:d7393da: import org.apache.carbondata.core.scan.filter.intf.RowIntf;
1:d7393da: import org.apache.carbondata.core.scan.filter.resolver.FilterResolverIntf;
1:d7393da: import org.apache.carbondata.core.scan.model.QueryModel;
1:21a72bf: import org.apache.carbondata.core.util.CarbonMetadataUtil;
1:d7393da: import org.apache.carbondata.core.util.CarbonUtil;
1:d7393da: import org.apache.carbondata.core.util.DataTypeUtil;
1:d7393da: import org.apache.carbondata.format.BlockletHeader;
1:d7393da: import org.apache.carbondata.format.FileHeader;
1:d7393da: import org.apache.carbondata.hadoop.CarbonInputSplit;
1:d7393da: import org.apache.carbondata.hadoop.CarbonMultiBlockSplit;
1:eae9064: import org.apache.carbondata.hadoop.InputMetricsStats;
1:d7393da: import org.apache.carbondata.hadoop.api.CarbonTableInputFormat;
1:d7393da: import org.apache.carbondata.processing.util.CarbonDataProcessorUtil;
1:bcef656: import org.apache.carbondata.streaming.CarbonStreamInputFormat;
1:bcef656: import org.apache.carbondata.streaming.StreamBlockletReader;
1:d7393da: 
1:d7393da: import org.apache.hadoop.conf.Configuration;
1:d7393da: import org.apache.hadoop.fs.FSDataInputStream;
1:d7393da: import org.apache.hadoop.fs.FileSystem;
1:d7393da: import org.apache.hadoop.fs.Path;
1:d7393da: import org.apache.hadoop.mapreduce.InputSplit;
1:d7393da: import org.apache.hadoop.mapreduce.RecordReader;
1:d7393da: import org.apache.hadoop.mapreduce.TaskAttemptContext;
1:d7393da: import org.apache.hadoop.mapreduce.lib.input.FileSplit;
1:d7393da: import org.apache.spark.memory.MemoryMode;
1:74c3eb1: import org.apache.spark.sql.CarbonVectorProxy;
1:d7393da: import org.apache.spark.sql.catalyst.InternalRow;
1:d7393da: import org.apache.spark.sql.catalyst.expressions.GenericInternalRow;
1:74c3eb1: import org.apache.spark.sql.types.*;
1:d7393da: 
1:d7393da: /**
1:d7393da:  * Stream record reader
1:d7393da:  */
1:d7393da: public class CarbonStreamRecordReader extends RecordReader<Void, Object> {
1:d7393da:   // vector reader
1:d7393da:   private boolean isVectorReader;
1:d7393da: 
1:d7393da:   // metadata
1:d7393da:   private CarbonTable carbonTable;
1:d7393da:   private CarbonColumn[] storageColumns;
1:d7393da:   private boolean[] isRequired;
1:933e30c:   private DataType[] measureDataTypes;
1:d7393da:   private int dimensionCount;
1:d7393da:   private int measureCount;
1:d7393da: 
1:d7393da:   // input
1:d7393da:   private FileSplit fileSplit;
1:d7393da:   private Configuration hadoopConf;
1:d7393da:   private StreamBlockletReader input;
1:d7393da:   private boolean isFirstRow = true;
1:d7393da:   private QueryModel model;
1:d7393da: 
1:d7393da:   // decode data
1:d7393da:   private BitSet allNonNull;
1:d7393da:   private boolean[] isNoDictColumn;
1:d7393da:   private DirectDictionaryGenerator[] directDictionaryGenerators;
1:d7393da:   private CacheProvider cacheProvider;
1:d7393da:   private Cache<DictionaryColumnUniqueIdentifier, Dictionary> cache;
1:d7393da:   private GenericQueryType[] queryTypes;
1:8f08c4a:   private String compressorName;
1:d7393da: 
1:d7393da:   // vectorized reader
1:d7393da:   private StructType outputSchema;
1:74c3eb1:   private CarbonVectorProxy vectorProxy;
1:d7393da:   private boolean isFinished = false;
1:d7393da: 
1:d7393da:   // filter
1:d7393da:   private FilterExecuter filter;
1:d7393da:   private boolean[] isFilterRequired;
1:d7393da:   private Object[] filterValues;
1:d7393da:   private RowIntf filterRow;
1:d7393da:   private int[] filterMap;
1:d7393da: 
1:d7393da:   // output
1:d7393da:   private CarbonColumn[] projection;
1:d7393da:   private boolean[] isProjectionRequired;
1:d7393da:   private int[] projectionMap;
1:d7393da:   private Object[] outputValues;
1:d7393da:   private InternalRow outputRow;
1:d7393da: 
1:d7393da:   // empty project, null filter
1:d7393da:   private boolean skipScanData;
1:d7393da: 
1:f8e0585:   // return raw row for handoff
1:f8e0585:   private boolean useRawRow = false;
1:f8e0585: 
1:eae9064:   // InputMetricsStats
1:eae9064:   private InputMetricsStats inputMetricsStats;
1:bcef656: 
1:bcef656:   public CarbonStreamRecordReader(boolean isVectorReader, InputMetricsStats inputMetricsStats,
1:bcef656:       QueryModel mdl, boolean useRawRow) {
1:bcef656:     this.isVectorReader = isVectorReader;
1:bcef656:     this.inputMetricsStats = inputMetricsStats;
1:bcef656:     this.model = mdl;
1:bcef656:     this.useRawRow = useRawRow;
1:bcef656: 
1:bcef656:   }
1:d7393da:   @Override public void initialize(InputSplit split, TaskAttemptContext context)
1:d7393da:       throws IOException, InterruptedException {
1:d7393da:     // input
1:d7393da:     if (split instanceof CarbonInputSplit) {
1:d7393da:       fileSplit = (CarbonInputSplit) split;
1:d7393da:     } else if (split instanceof CarbonMultiBlockSplit) {
1:d7393da:       fileSplit = ((CarbonMultiBlockSplit) split).getAllSplits().get(0);
5:d7393da:     } else {
1:d7393da:       fileSplit = (FileSplit) split;
21:d7393da:     }
1:d7393da: 
1:d7393da:     // metadata
1:d7393da:     hadoopConf = context.getConfiguration();
1:d7393da:     if (model == null) {
1:d7393da:       CarbonTableInputFormat format = new CarbonTableInputFormat<Object>();
1:daa6465:       model = format.createQueryModel(split, context);
1:d7393da:     }
1:d7393da:     carbonTable = model.getTable();
1:d7393da:     List<CarbonDimension> dimensions =
1:5fc7f06:         carbonTable.getDimensionByTableName(carbonTable.getTableName());
1:d7393da:     dimensionCount = dimensions.size();
1:d7393da:     List<CarbonMeasure> measures =
1:5fc7f06:         carbonTable.getMeasureByTableName(carbonTable.getTableName());
1:d7393da:     measureCount = measures.size();
1:d7393da:     List<CarbonColumn> carbonColumnList =
1:5fc7f06:         carbonTable.getStreamStorageOrderColumn(carbonTable.getTableName());
1:d7393da:     storageColumns = carbonColumnList.toArray(new CarbonColumn[carbonColumnList.size()]);
1:d7393da:     isNoDictColumn = CarbonDataProcessorUtil.getNoDictionaryMapping(storageColumns);
1:d7393da:     directDictionaryGenerators = new DirectDictionaryGenerator[storageColumns.length];
1:d7393da:     for (int i = 0; i < storageColumns.length; i++) {
1:d7393da:       if (storageColumns[i].hasEncoding(Encoding.DIRECT_DICTIONARY)) {
1:d7393da:         directDictionaryGenerators[i] = DirectDictionaryKeyGeneratorFactory
1:d7393da:             .getDirectDictionaryGenerator(storageColumns[i].getDataType());
1:d7393da:       }
1:d7393da:     }
1:933e30c:     measureDataTypes = new DataType[measureCount];
1:d7393da:     for (int i = 0; i < measureCount; i++) {
1:933e30c:       measureDataTypes[i] = storageColumns[dimensionCount + i].getDataType();
1:d7393da:     }
1:d7393da: 
1:d7393da:     // decode data
1:d7393da:     allNonNull = new BitSet(storageColumns.length);
1:d7393da:     projection = model.getProjectionColumns();
1:d7393da: 
1:d7393da:     isRequired = new boolean[storageColumns.length];
1:d7393da:     boolean[] isFiltlerDimensions = model.getIsFilterDimensions();
1:d7393da:     boolean[] isFiltlerMeasures = model.getIsFilterMeasures();
1:d7393da:     isFilterRequired = new boolean[storageColumns.length];
1:d7393da:     filterMap = new int[storageColumns.length];
1:d7393da:     for (int i = 0; i < storageColumns.length; i++) {
1:d7393da:       if (storageColumns[i].isDimension()) {
1:d7393da:         if (isFiltlerDimensions[storageColumns[i].getOrdinal()]) {
1:d7393da:           isRequired[i] = true;
1:d7393da:           isFilterRequired[i] = true;
1:d7393da:           filterMap[i] = storageColumns[i].getOrdinal();
1:d7393da:         }
1:d7393da:       } else {
1:d7393da:         if (isFiltlerMeasures[storageColumns[i].getOrdinal()]) {
1:d7393da:           isRequired[i] = true;
1:d7393da:           isFilterRequired[i] = true;
1:d7393da:           filterMap[i] = carbonTable.getDimensionOrdinalMax() + storageColumns[i].getOrdinal();
1:d7393da:         }
1:d7393da:       }
1:d7393da:     }
1:d7393da: 
1:d7393da:     isProjectionRequired = new boolean[storageColumns.length];
1:d7393da:     projectionMap = new int[storageColumns.length];
1:d7393da:     for (int j = 0; j < projection.length; j++) {
1:d7393da:       for (int i = 0; i < storageColumns.length; i++) {
1:d7393da:         if (storageColumns[i].getColName().equals(projection[j].getColName())) {
1:d7393da:           isRequired[i] = true;
1:d7393da:           isProjectionRequired[i] = true;
1:d7393da:           projectionMap[i] = j;
1:d7393da:           break;
1:d7393da:         }
1:d7393da:       }
1:d7393da:     }
1:d7393da: 
1:d7393da:     // initialize filter
1:d7393da:     if (null != model.getFilterExpressionResolverTree()) {
1:d7393da:       initializeFilter();
1:d7393da:     } else if (projection.length == 0) {
1:d7393da:       skipScanData = true;
1:d7393da:     }
1:d7393da: 
1:d7393da:   }
1:d7393da: 
1:d7393da:   private void initializeFilter() {
1:d7393da: 
1:d7393da:     List<ColumnSchema> wrapperColumnSchemaList = CarbonUtil
1:5fc7f06:         .getColumnSchemaList(carbonTable.getDimensionByTableName(carbonTable.getTableName()),
1:5fc7f06:             carbonTable.getMeasureByTableName(carbonTable.getTableName()));
1:d7393da:     int[] dimLensWithComplex = new int[wrapperColumnSchemaList.size()];
1:d7393da:     for (int i = 0; i < dimLensWithComplex.length; i++) {
1:d7393da:       dimLensWithComplex[i] = Integer.MAX_VALUE;
1:d7393da:     }
1:d7393da: 
1:d7393da:     int[] dictionaryColumnCardinality =
1:d7393da:         CarbonUtil.getFormattedCardinality(dimLensWithComplex, wrapperColumnSchemaList);
1:d7393da:     SegmentProperties segmentProperties =
1:d7393da:         new SegmentProperties(wrapperColumnSchemaList, dictionaryColumnCardinality);
1:d7393da:     Map<Integer, GenericQueryType> complexDimensionInfoMap = new HashMap<>();
1:d7393da: 
1:d7393da:     FilterResolverIntf resolverIntf = model.getFilterExpressionResolverTree();
1:d7393da:     filter = FilterUtil.getFilterExecuterTree(resolverIntf, segmentProperties,
1:d7393da:         complexDimensionInfoMap);
1:d7393da:     // for row filter, we need update column index
1:d7393da:     FilterUtil.updateIndexOfColumnExpression(resolverIntf.getFilterExpression(),
1:d7393da:         carbonTable.getDimensionOrdinalMax());
1:d7393da: 
1:d7393da:   }
1:d7393da: 
1:d7393da:   private byte[] getSyncMarker(String filePath) throws IOException {
1:d7393da:     CarbonHeaderReader headerReader = new CarbonHeaderReader(filePath);
1:d7393da:     FileHeader header = headerReader.readHeader();
1:8f08c4a:     // legacy store does not have this member
1:8f08c4a:     if (header.isSetCompressor_name()) {
1:8f08c4a:       compressorName = header.getCompressor_name();
1:8f08c4a:     } else {
1:8f08c4a:       compressorName = CompressorFactory.SupportedCompressor.SNAPPY.getName();
1:8f08c4a:     }
1:d7393da:     return header.getSync_marker();
1:f8e0585:   }
1:f8e0585: 
1:d7393da:   private void initializeAtFirstRow() throws IOException {
1:d7393da:     filterValues = new Object[carbonTable.getDimensionOrdinalMax() + measureCount];
1:d7393da:     filterRow = new RowImpl();
1:d7393da:     filterRow.setValues(filterValues);
1:d7393da: 
1:d7393da:     outputValues = new Object[projection.length];
1:d7393da:     outputRow = new GenericInternalRow(outputValues);
1:d7393da: 
1:d7393da:     Path file = fileSplit.getPath();
1:d7393da: 
1:e7f9422:     byte[] syncMarker = getSyncMarker(file.toString());
1:d7393da: 
1:d7393da:     FileSystem fs = file.getFileSystem(hadoopConf);
1:d7393da: 
1:d7393da:     int bufferSize = Integer.parseInt(hadoopConf.get(CarbonStreamInputFormat.READ_BUFFER_SIZE,
1:d7393da:         CarbonStreamInputFormat.READ_BUFFER_SIZE_DEFAULT));
1:d7393da: 
1:d7393da:     FSDataInputStream fileIn = fs.open(file, bufferSize);
1:d7393da:     fileIn.seek(fileSplit.getStart());
1:d7393da:     input = new StreamBlockletReader(syncMarker, fileIn, fileSplit.getLength(),
1:8f08c4a:         fileSplit.getStart() == 0, compressorName);
1:d7393da: 
1:d7393da:     cacheProvider = CacheProvider.getInstance();
1:1155d4d:     cache = cacheProvider.createCache(CacheType.FORWARD_DICTIONARY);
1:d7393da:     queryTypes = CarbonStreamInputFormat.getComplexDimensions(carbonTable, storageColumns, cache);
1:d7393da: 
1:c723947:     outputSchema = new StructType((StructField[])
1:c723947:         DataTypeUtil.getDataTypeConverter().convertCarbonSchemaToSparkSchema(projection));
1:d7393da:   }
1:d7393da: 
1:d7393da:   @Override public boolean nextKeyValue() throws IOException, InterruptedException {
1:d7393da:     if (isFirstRow) {
1:d7393da:       isFirstRow = false;
1:d7393da:       initializeAtFirstRow();
1:d7393da:     }
1:d7393da:     if (isFinished) {
1:d7393da:       return false;
1:d7393da:     }
1:d7393da: 
2:d7393da:     if (isVectorReader) {
1:d7393da:       return nextColumnarBatch();
1:d7393da:     }
1:d7393da: 
1:d7393da:     return nextRow();
1:d7393da:   }
1:d7393da: 
1:d7393da:   /**
1:d7393da:    * for vector reader, check next columnar batch
1:d7393da:    */
1:d7393da:   private boolean nextColumnarBatch() throws IOException {
1:d7393da:     boolean hasNext;
1:d7393da:     boolean scanMore = false;
1:d7393da:     do {
1:d7393da:       // move to the next blocklet
1:d7393da:       hasNext = input.nextBlocklet();
1:d7393da:       if (hasNext) {
1:d7393da:         // read blocklet header
1:d7393da:         BlockletHeader header = input.readBlockletHeader();
1:d7393da:         if (isScanRequired(header)) {
1:d7393da:           scanMore = !scanBlockletAndFillVector(header);
1:d7393da:         } else {
3:d7393da:           input.skipBlockletData(true);
1:d7393da:           scanMore = true;
1:d7393da:         }
1:d7393da:       } else {
1:d7393da:         isFinished = true;
1:d7393da:         scanMore = false;
1:d7393da:       }
1:d7393da:     } while (scanMore);
1:d7393da:     return hasNext;
1:d7393da:   }
1:d7393da: 
1:d7393da:   /**
1:d7393da:    * check next Row
1:d7393da:    */
1:d7393da:   private boolean nextRow() throws IOException {
1:d7393da:     // read row one by one
1:bcef656:     try {
1:d7393da:       boolean hasNext;
1:d7393da:       boolean scanMore = false;
1:d7393da:       do {
1:d7393da:         hasNext = input.hasNext();
1:d7393da:         if (hasNext) {
1:bcef656:           if (skipScanData) {
1:d7393da:             input.nextRow();
1:d7393da:             scanMore = false;
1:d7393da:           } else {
1:f8e0585:             if (useRawRow) {
1:f8e0585:               // read raw row for streaming handoff which does not require decode raw row
1:f8e0585:               readRawRowFromStream();
1:f8e0585:             } else {
1:bcef656:               readRowFromStream();
1:bcef656:             }
1:d7393da:             if (null != filter) {
1:d7393da:               scanMore = !filter.applyFilter(filterRow, carbonTable.getDimensionOrdinalMax());
1:d7393da:             } else {
1:d7393da:               scanMore = false;
1:f8e0585:             }
1:d7393da:           }
1:d7393da:         } else {
1:d7393da:           if (input.nextBlocklet()) {
1:d7393da:             BlockletHeader header = input.readBlockletHeader();
1:d7393da:             if (isScanRequired(header)) {
3:d7393da:               if (skipScanData) {
1:d7393da:                 input.skipBlockletData(false);
1:d7393da:               } else {
1:bcef656:                 input.readBlockletData(header);
1:d7393da:               }
1:bcef656:             } else {
1:bcef656:               input.skipBlockletData(true);
1:d7393da:             }
1:d7393da:             scanMore = true;
1:d7393da:           } else {
1:d7393da:             isFinished = true;
1:d7393da:             scanMore = false;
1:d7393da:           }
1:d7393da:         }
1:d7393da:       } while (scanMore);
1:d7393da:       return hasNext;
1:bcef656:     } catch (FilterUnsupportedException e) {
1:d7393da:       throw new IOException("Failed to filter row in detail reader", e);
1:d7393da:     }
1:d7393da:   }
1:bcef656: 
1:d7393da:   @Override public Void getCurrentKey() throws IOException, InterruptedException {
1:d7393da:     return null;
1:d7393da:   }
1:bcef656: 
1:74c3eb1:     @Override public Object getCurrentValue() throws IOException, InterruptedException {
1:74c3eb1:         if (isVectorReader) {
1:74c3eb1:             int value = vectorProxy.numRows();
1:74c3eb1:             if (inputMetricsStats != null) {
1:74c3eb1:                 inputMetricsStats.incrementRecordRead((long) value);
1:74c3eb1:             }
1:74c3eb1: 
1:74c3eb1:             return vectorProxy.getColumnarBatch();
1:bcef656:         }
1:d7393da: 
1:bcef656:     if (inputMetricsStats != null) {
1:eae9064:       inputMetricsStats.incrementRecordRead(1L);
1:d7393da:     }
1:d7393da: 
1:d7393da:     return outputRow;
1:d7393da:   }
1:d7393da: 
1:d7393da:   private boolean isScanRequired(BlockletHeader header) {
1:21a72bf:     if (filter != null && header.getBlocklet_index() != null) {
1:21a72bf:       BlockletMinMaxIndex minMaxIndex = CarbonMetadataUtil.convertExternalMinMaxIndex(
1:21a72bf:           header.getBlocklet_index().getMin_max_index());
1:21a72bf:       if (minMaxIndex != null) {
1:21a72bf:         BitSet bitSet =
1:21a72bf:             filter.isScanRequired(minMaxIndex.getMaxValues(), minMaxIndex.getMinValues());
1:21a72bf:         if (bitSet.isEmpty()) {
1:21a72bf:           return false;
1:21a72bf:         } else {
1:21a72bf:           return true;
1:21a72bf:         }
1:21a72bf:       }
1:21a72bf:     }
2:d7393da:     return true;
1:d7393da:   }
1:d7393da: 
1:74c3eb1:     private boolean scanBlockletAndFillVector(BlockletHeader header) throws IOException {
1:74c3eb1:         // if filter is null and output projection is empty, use the row number of blocklet header
1:74c3eb1:         if (skipScanData) {
1:74c3eb1:             int rowNums = header.getBlocklet_info().getNum_rows();
1:74c3eb1:             vectorProxy= new CarbonVectorProxy(MemoryMode.OFF_HEAP,outputSchema,rowNums);
1:74c3eb1:             vectorProxy.setNumRows(rowNums);
1:74c3eb1:             input.skipBlockletData(true);
1:74c3eb1:             return rowNums > 0;
1:74c3eb1:         }
1:d7393da: 
1:74c3eb1:         input.readBlockletData(header);
1:74c3eb1:         vectorProxy= new CarbonVectorProxy(MemoryMode.OFF_HEAP,outputSchema,input.getRowNums());
1:74c3eb1:         int rowNum = 0;
1:74c3eb1:         if (null == filter) {
1:74c3eb1:             while (input.hasNext()) {
1:74c3eb1:                 readRowFromStream();
1:74c3eb1:                 putRowToColumnBatch(rowNum++);
1:bcef656:             }
1:74c3eb1:         } else {
1:74c3eb1:             try {
1:74c3eb1:                 while (input.hasNext()) {
1:74c3eb1:                     readRowFromStream();
1:74c3eb1:                     if (filter.applyFilter(filterRow, carbonTable.getDimensionOrdinalMax())) {
1:74c3eb1:                         putRowToColumnBatch(rowNum++);
1:74c3eb1:                     }
1:74c3eb1:                 }
1:74c3eb1:             } catch (FilterUnsupportedException e) {
1:74c3eb1:                 throw new IOException("Failed to filter row in vector reader", e);
1:74c3eb1:             }
1:bcef656:         }
1:74c3eb1:         vectorProxy.setNumRows(rowNum);
1:74c3eb1:         return rowNum > 0;
1:bcef656:     }
1:d7393da: 
1:d7393da:   private void readRowFromStream() {
1:d7393da:     input.nextRow();
1:d7393da:     short nullLen = input.readShort();
1:d7393da:     BitSet nullBitSet = allNonNull;
1:d7393da:     if (nullLen > 0) {
1:d7393da:       nullBitSet = BitSet.valueOf(input.readBytes(nullLen));
1:d7393da:     }
1:d7393da:     int colCount = 0;
1:d7393da:     // primitive type dimension
1:d7393da:     for (; colCount < isNoDictColumn.length; colCount++) {
1:d7393da:       if (nullBitSet.get(colCount)) {
1:d7393da:         if (isFilterRequired[colCount]) {
1:d7393da:           filterValues[filterMap[colCount]] = CarbonCommonConstants.MEMBER_DEFAULT_VAL_ARRAY;
1:d7393da:         }
1:d7393da:         if (isProjectionRequired[colCount]) {
1:d7393da:           outputValues[projectionMap[colCount]] = null;
1:d7393da:         }
1:d7393da:       } else {
1:d7393da:         if (isNoDictColumn[colCount]) {
1:d7393da:           int v = input.readShort();
1:d7393da:           if (isRequired[colCount]) {
1:d7393da:             byte[] b = input.readBytes(v);
1:d7393da:             if (isFilterRequired[colCount]) {
1:d7393da:               filterValues[filterMap[colCount]] = b;
1:d7393da:             }
1:d7393da:             if (isProjectionRequired[colCount]) {
1:d7393da:               outputValues[projectionMap[colCount]] =
1:d7393da:                   DataTypeUtil.getDataBasedOnDataTypeForNoDictionaryColumn(b,
1:d7393da:                       storageColumns[colCount].getDataType());
1:d7393da:             }
1:d7393da:           } else {
1:d7393da:             input.skipBytes(v);
1:d7393da:           }
1:d7393da:         } else if (null != directDictionaryGenerators[colCount]) {
1:d7393da:           if (isRequired[colCount]) {
1:d7393da:             if (isFilterRequired[colCount]) {
1:d7393da:               filterValues[filterMap[colCount]] = input.copy(4);
1:d7393da:             }
1:d7393da:             if (isProjectionRequired[colCount]) {
1:d7393da:               outputValues[projectionMap[colCount]] =
1:d7393da:                   directDictionaryGenerators[colCount].getValueFromSurrogate(input.readInt());
1:d7393da:             } else {
1:d7393da:               input.skipBytes(4);
1:d7393da:             }
1:d7393da:           } else {
1:d7393da:             input.skipBytes(4);
1:d7393da:           }
1:d7393da:         } else {
1:d7393da:           if (isRequired[colCount]) {
1:d7393da:             if (isFilterRequired[colCount]) {
1:d7393da:               filterValues[filterMap[colCount]] = input.copy(4);
1:d7393da:             }
1:d7393da:             if (isProjectionRequired[colCount]) {
1:d7393da:               outputValues[projectionMap[colCount]] = input.readInt();
1:d7393da:             } else {
1:d7393da:               input.skipBytes(4);
1:d7393da:             }
1:d7393da:           } else {
1:d7393da:             input.skipBytes(4);
1:d7393da:           }
1:d7393da:         }
1:d7393da:       }
1:d7393da:     }
1:d7393da:     // complex type dimension
1:d7393da:     for (; colCount < dimensionCount; colCount++) {
1:d7393da:       if (nullBitSet.get(colCount)) {
1:d7393da:         if (isFilterRequired[colCount]) {
1:d7393da:           filterValues[filterMap[colCount]] = null;
1:d7393da:         }
1:d7393da:         if (isProjectionRequired[colCount]) {
1:d7393da:           outputValues[projectionMap[colCount]] = null;
1:d7393da:         }
1:d7393da:       } else {
1:d7393da:         short v = input.readShort();
1:d7393da:         if (isRequired[colCount]) {
1:d7393da:           byte[] b = input.readBytes(v);
1:d7393da:           if (isFilterRequired[colCount]) {
1:d7393da:             filterValues[filterMap[colCount]] = b;
1:d7393da:           }
1:d7393da:           if (isProjectionRequired[colCount]) {
1:d7393da:             outputValues[projectionMap[colCount]] = queryTypes[colCount]
1:3202cf5:                 .getDataBasedOnDataType(ByteBuffer.wrap(b));
1:d7393da:           }
1:d7393da:         } else {
1:d7393da:           input.skipBytes(v);
1:d7393da:         }
1:d7393da:       }
1:d7393da:     }
1:d7393da:     // measure
1:933e30c:     DataType dataType;
1:d7393da:     for (int msrCount = 0; msrCount < measureCount; msrCount++, colCount++) {
1:d7393da:       if (nullBitSet.get(colCount)) {
1:d7393da:         if (isFilterRequired[colCount]) {
1:d7393da:           filterValues[filterMap[colCount]] = null;
1:d7393da:         }
1:d7393da:         if (isProjectionRequired[colCount]) {
1:d7393da:           outputValues[projectionMap[colCount]] = null;
1:d7393da:         }
1:d7393da:       } else {
1:d7393da:         dataType = measureDataTypes[msrCount];
1:933e30c:         if (dataType == DataTypes.BOOLEAN) {
1:d7393da:           if (isRequired[colCount]) {
1:d7393da:             boolean v = input.readBoolean();
1:d7393da:             if (isFilterRequired[colCount]) {
1:d7393da:               filterValues[filterMap[colCount]] = v;
1:d7393da:             }
1:d7393da:             if (isProjectionRequired[colCount]) {
2:d7393da:               outputValues[projectionMap[colCount]] = v;
1:d7393da:             }
1:d7393da:           } else {
1:d7393da:             input.skipBytes(1);
1:d7393da:           }
1:933e30c:         } else if (dataType == DataTypes.SHORT) {
1:d7393da:           if (isRequired[colCount]) {
1:d7393da:             short v = input.readShort();
1:d7393da:             if (isFilterRequired[colCount]) {
1:d7393da:               filterValues[filterMap[colCount]] = v;
1:d7393da:             }
1:d7393da:             if (isProjectionRequired[colCount]) {
1:d7393da:               outputValues[projectionMap[colCount]] = v;
1:d7393da:             }
1:d7393da:           } else {
1:d7393da:             input.skipBytes(2);
1:d7393da:           }
1:933e30c:         } else if (dataType == DataTypes.INT) {
1:d7393da:           if (isRequired[colCount]) {
1:d7393da:             int v = input.readInt();
1:d7393da:             if (isFilterRequired[colCount]) {
1:d7393da:               filterValues[filterMap[colCount]] = v;
1:d7393da:             }
1:d7393da:             if (isProjectionRequired[colCount]) {
1:d7393da:               outputValues[projectionMap[colCount]] = v;
1:d7393da:             }
1:d7393da:           } else {
1:d7393da:             input.skipBytes(4);
1:d7393da:           }
1:933e30c:         } else if (dataType == DataTypes.LONG) {
1:d7393da:           if (isRequired[colCount]) {
1:d7393da:             long v = input.readLong();
1:d7393da:             if (isFilterRequired[colCount]) {
1:d7393da:               filterValues[filterMap[colCount]] = v;
1:d7393da:             }
1:d7393da:             if (isProjectionRequired[colCount]) {
1:d7393da:               outputValues[projectionMap[colCount]] = v;
1:d7393da:             }
1:d7393da:           } else {
1:d7393da:             input.skipBytes(8);
1:d7393da:           }
1:933e30c:         } else if (dataType == DataTypes.DOUBLE) {
1:d7393da:           if (isRequired[colCount]) {
1:d7393da:             double v = input.readDouble();
1:d7393da:             if (isFilterRequired[colCount]) {
1:d7393da:               filterValues[filterMap[colCount]] = v;
1:d7393da:             }
1:d7393da:             if (isProjectionRequired[colCount]) {
1:d7393da:               outputValues[projectionMap[colCount]] = v;
1:d7393da:             }
1:d7393da:           } else {
1:d7393da:             input.skipBytes(8);
1:d7393da:           }
1:933e30c:         } else if (DataTypes.isDecimal(dataType)) {
1:d7393da:           int len = input.readShort();
1:d7393da:           if (isRequired[colCount]) {
1:d7393da:             BigDecimal v = DataTypeUtil.byteToBigDecimal(input.readBytes(len));
1:d7393da:             if (isFilterRequired[colCount]) {
1:d7393da:               filterValues[filterMap[colCount]] = v;
1:d7393da:             }
1:d7393da:             if (isProjectionRequired[colCount]) {
1:c723947:               outputValues[projectionMap[colCount]] =
1:c723947:                   DataTypeUtil.getDataTypeConverter().convertFromBigDecimalToDecimal(v);
1:d7393da:             }
1:d7393da:           } else {
1:d7393da:             input.skipBytes(len);
1:d7393da:           }
1:d7393da:         }
1:d7393da:       }
1:d7393da:     }
1:d7393da:   }
1:d7393da: 
1:f8e0585:   private void readRawRowFromStream() {
1:f8e0585:     input.nextRow();
1:f8e0585:     short nullLen = input.readShort();
1:f8e0585:     BitSet nullBitSet = allNonNull;
1:f8e0585:     if (nullLen > 0) {
1:f8e0585:       nullBitSet = BitSet.valueOf(input.readBytes(nullLen));
1:f8e0585:     }
1:f8e0585:     int colCount = 0;
1:f8e0585:     // primitive type dimension
1:f8e0585:     for (; colCount < isNoDictColumn.length; colCount++) {
1:f8e0585:       if (nullBitSet.get(colCount)) {
1:f8e0585:         outputValues[colCount] = CarbonCommonConstants.MEMBER_DEFAULT_VAL_ARRAY;
1:f8e0585:       } else {
1:f8e0585:         if (isNoDictColumn[colCount]) {
1:f8e0585:           int v = input.readShort();
1:f8e0585:           outputValues[colCount] = input.readBytes(v);
1:f8e0585:         } else {
1:f8e0585:           outputValues[colCount] = input.readInt();
1:f8e0585:         }
1:f8e0585:       }
1:f8e0585:     }
1:f8e0585:     // complex type dimension
1:f8e0585:     for (; colCount < dimensionCount; colCount++) {
1:f8e0585:       if (nullBitSet.get(colCount)) {
1:f8e0585:         outputValues[colCount] = null;
1:f8e0585:       } else {
1:f8e0585:         short v = input.readShort();
1:f8e0585:         outputValues[colCount] = input.readBytes(v);
1:f8e0585:       }
1:f8e0585:     }
1:f8e0585:     // measure
1:f8e0585:     DataType dataType;
1:f8e0585:     for (int msrCount = 0; msrCount < measureCount; msrCount++, colCount++) {
1:f8e0585:       if (nullBitSet.get(colCount)) {
1:f8e0585:         outputValues[colCount] = null;
1:f8e0585:       } else {
1:f8e0585:         dataType = measureDataTypes[msrCount];
1:f8e0585:         if (dataType == DataTypes.BOOLEAN) {
1:f8e0585:           outputValues[colCount] = input.readBoolean();
1:f8e0585:         } else if (dataType == DataTypes.SHORT) {
1:f8e0585:           outputValues[colCount] = input.readShort();
1:f8e0585:         } else if (dataType == DataTypes.INT) {
1:f8e0585:           outputValues[colCount] = input.readInt();
1:f8e0585:         } else if (dataType == DataTypes.LONG) {
1:f8e0585:           outputValues[colCount] = input.readLong();
1:f8e0585:         } else if (dataType == DataTypes.DOUBLE) {
1:f8e0585:           outputValues[colCount] = input.readDouble();
1:f8e0585:         } else if (DataTypes.isDecimal(dataType)) {
1:f8e0585:           int len = input.readShort();
1:f8e0585:           outputValues[colCount] = DataTypeUtil.byteToBigDecimal(input.readBytes(len));
1:f8e0585:         }
1:f8e0585:       }
1:f8e0585:     }
1:f8e0585:   }
1:f8e0585: 
1:74c3eb1:     private void putRowToColumnBatch(int rowId) {
1:74c3eb1:         for (int i = 0; i < projection.length; i++) {
1:74c3eb1:             Object value = outputValues[i];
1:74c3eb1:             vectorProxy.putRowToColumnBatch(rowId,value,i);
1:d7393da: 
1:74c3eb1:         }
1:d7393da:     }
1:d7393da: 
1:74c3eb1:     @Override public float getProgress() throws IOException, InterruptedException {
1:74c3eb1:         return 0;
1:74c3eb1:     }
1:d7393da: 
1:74c3eb1:     @Override public void close() throws IOException {
1:74c3eb1:         if (null != input) {
1:74c3eb1:             input.close();
1:74c3eb1:         }
1:74c3eb1:         if (null != vectorProxy) {
1:74c3eb1:             vectorProxy.close();
1:74c3eb1:         }
1:bcef656:     }
1:d7393da: }
============================================================================
author:xuchuanyin
-------------------------------------------------------------------------------
commit:8f08c4a
/////////////////////////////////////////////////////////////////////////
/////////////////////////////////////////////////////////////////////////
1: import org.apache.carbondata.core.datastore.compression.CompressorFactory;
/////////////////////////////////////////////////////////////////////////
/////////////////////////////////////////////////////////////////////////
1:   private String compressorName;
/////////////////////////////////////////////////////////////////////////
1:     // legacy store does not have this member
1:     if (header.isSetCompressor_name()) {
1:       compressorName = header.getCompressor_name();
1:     } else {
1:       compressorName = CompressorFactory.SupportedCompressor.SNAPPY.getName();
1:     }
/////////////////////////////////////////////////////////////////////////
1:         fileSplit.getStart() == 0, compressorName);
author:sandeep-katta
-------------------------------------------------------------------------------
commit:74c3eb1
/////////////////////////////////////////////////////////////////////////
0: import java.math.BigInteger;
/////////////////////////////////////////////////////////////////////////
1: import org.apache.spark.sql.CarbonVectorProxy;
1: import org.apache.spark.sql.types.*;
/////////////////////////////////////////////////////////////////////////
1:   private CarbonVectorProxy vectorProxy;
/////////////////////////////////////////////////////////////////////////
/////////////////////////////////////////////////////////////////////////
1:     @Override public Object getCurrentValue() throws IOException, InterruptedException {
1:         if (isVectorReader) {
1:             int value = vectorProxy.numRows();
1:             if (inputMetricsStats != null) {
1:                 inputMetricsStats.incrementRecordRead((long) value);
1:             }
1: 
1:             return vectorProxy.getColumnarBatch();
/////////////////////////////////////////////////////////////////////////
1:     private boolean scanBlockletAndFillVector(BlockletHeader header) throws IOException {
1:         // if filter is null and output projection is empty, use the row number of blocklet header
1:         if (skipScanData) {
1:             int rowNums = header.getBlocklet_info().getNum_rows();
1:             vectorProxy= new CarbonVectorProxy(MemoryMode.OFF_HEAP,outputSchema,rowNums);
1:             vectorProxy.setNumRows(rowNums);
1:             input.skipBlockletData(true);
1:             return rowNums > 0;
1:         }
1:         input.readBlockletData(header);
1:         vectorProxy= new CarbonVectorProxy(MemoryMode.OFF_HEAP,outputSchema,input.getRowNums());
1:         int rowNum = 0;
1:         if (null == filter) {
1:             while (input.hasNext()) {
1:                 readRowFromStream();
1:                 putRowToColumnBatch(rowNum++);
1:         } else {
1:             try {
1:                 while (input.hasNext()) {
1:                     readRowFromStream();
1:                     if (filter.applyFilter(filterRow, carbonTable.getDimensionOrdinalMax())) {
1:                         putRowToColumnBatch(rowNum++);
1:                     }
1:                 }
1:             } catch (FilterUnsupportedException e) {
1:                 throw new IOException("Failed to filter row in vector reader", e);
1:             }
1:         vectorProxy.setNumRows(rowNum);
1:         return rowNum > 0;
/////////////////////////////////////////////////////////////////////////
1:     private void putRowToColumnBatch(int rowId) {
1:         for (int i = 0; i < projection.length; i++) {
1:             Object value = outputValues[i];
1:             vectorProxy.putRowToColumnBatch(rowId,value,i);
1:         }
1:     @Override public float getProgress() throws IOException, InterruptedException {
1:         return 0;
1:     }
1:     @Override public void close() throws IOException {
1:         if (null != input) {
1:             input.close();
1:         }
1:         if (null != vectorProxy) {
1:             vectorProxy.close();
1:         }
author:sujith71955
-------------------------------------------------------------------------------
commit:bcef656
/////////////////////////////////////////////////////////////////////////
1: package org.apache.carbondata.stream;
0: import java.lang.reflect.Constructor;
0: import java.lang.reflect.Method;
0: import org.apache.carbondata.common.logging.LogService;
0: import org.apache.carbondata.common.logging.LogServiceFactory;
/////////////////////////////////////////////////////////////////////////
1: import org.apache.carbondata.streaming.CarbonStreamInputFormat;
0: import org.apache.carbondata.streaming.CarbonStreamUtils;
1: import org.apache.carbondata.streaming.StreamBlockletReader;
/////////////////////////////////////////////////////////////////////////
1: 
/////////////////////////////////////////////////////////////////////////
0:   private Object vectorProxy;
/////////////////////////////////////////////////////////////////////////
0:   private static final LogService LOGGER =
0:           LogServiceFactory.getLogService(CarbonStreamRecordReader.class.getName());
1: 
1:   public CarbonStreamRecordReader(boolean isVectorReader, InputMetricsStats inputMetricsStats,
1:       QueryModel mdl, boolean useRawRow) {
1:     this.isVectorReader = isVectorReader;
1:     this.inputMetricsStats = inputMetricsStats;
1:     this.model = mdl;
1:     this.useRawRow = useRawRow;
1: 
1:   }
/////////////////////////////////////////////////////////////////////////
/////////////////////////////////////////////////////////////////////////
0:       Method method = null;
1:       try {
0:         method = vectorProxy.getClass().getMethod("numRows");
0:         int value = (int) method.invoke(vectorProxy);
1:         if (inputMetricsStats != null) {
0:           inputMetricsStats.incrementRecordRead((long) value);
1:         }
0:         method = vectorProxy.getClass().getMethod("getColumnarBatch");
0:         return method.invoke(vectorProxy);
0:       } catch (Exception e) {
0:         throw new IOException(e);
/////////////////////////////////////////////////////////////////////////
0:     Constructor cons = null;
0:     String methodName = "setNumRows";
0:     try {
0:       String vectorReaderClassName = "org.apache.spark.sql.CarbonVectorProxy";
0:       cons = CarbonStreamUtils.getConstructorWithReflection(vectorReaderClassName, MemoryMode.class,
0:               StructType.class, int.class);
1:       if (skipScanData) {
1: 
0:         int rowNums = header.getBlocklet_info().getNum_rows();
0:         vectorProxy = cons.newInstance(MemoryMode.OFF_HEAP, outputSchema, rowNums);
0:         Method setNumRowsMethod = vectorProxy.getClass().getMethod(methodName, int.class);
0:         setNumRowsMethod.invoke(vectorProxy, rowNums);
1:         input.skipBlockletData(true);
0:         return rowNums > 0;
1:       input.readBlockletData(header);
0:       vectorProxy = cons.newInstance(MemoryMode.OFF_HEAP,outputSchema, input.getRowNums());
0:       if (null == filter) {
0:           putRowToColumnBatch(rowNum++);
1:       } else {
0:         try {
0:           while (input.hasNext()) {
1:             readRowFromStream();
0:             if (filter.applyFilter(filterRow, carbonTable.getDimensionOrdinalMax())) {
0:               putRowToColumnBatch(rowNum++);
1:             }
1:           }
1:         } catch (FilterUnsupportedException e) {
0:           throw new IOException("Failed to filter row in vector reader", e);
1:         }
0:       Method setNumRowsMethod = vectorProxy.getClass().getMethod(methodName, int.class);
0:       setNumRowsMethod.invoke(vectorProxy, rowNum);
0:     } catch (Exception e) {
0:       throw new IOException("Failed to fill row in  vector reader", e);
/////////////////////////////////////////////////////////////////////////
0:     Class<?>[] paramTypes = {int.class, Object.class, int.class};
0:     Method putRowToColumnBatch = null;
0:     try {
0:       putRowToColumnBatch = vectorProxy.getClass().getMethod("putRowToColumnBatch", paramTypes);
1: 
0:     } catch (Exception e) {
0:       LOGGER.error(
0:               "Unable to put the row in the vector" + "rowid: " + rowId + e);
1:     }
0:       try {
0:         putRowToColumnBatch.invoke(vectorProxy, rowId, value, i);
0:       } catch (Exception e) {
0:         LOGGER.error(
0:                 "Unable to put the row in the vector" + "rowid: " + rowId + e);
/////////////////////////////////////////////////////////////////////////
0:     if (null != vectorProxy) {
0:       try {
0:         Method closeMethod = vectorProxy.getClass().getMethod("close");
0:         closeMethod.invoke(vectorProxy);
0:       } catch (Exception e) {
0:         LOGGER.error(
0:                 "Unable to close the stream vector reader" + e);
1:       }
author:QiangCai
-------------------------------------------------------------------------------
commit:21a72bf
/////////////////////////////////////////////////////////////////////////
1: import org.apache.carbondata.core.metadata.blocklet.index.BlockletMinMaxIndex;
/////////////////////////////////////////////////////////////////////////
1: import org.apache.carbondata.core.util.CarbonMetadataUtil;
/////////////////////////////////////////////////////////////////////////
1:     if (filter != null && header.getBlocklet_index() != null) {
1:       BlockletMinMaxIndex minMaxIndex = CarbonMetadataUtil.convertExternalMinMaxIndex(
1:           header.getBlocklet_index().getMin_max_index());
1:       if (minMaxIndex != null) {
1:         BitSet bitSet =
1:             filter.isScanRequired(minMaxIndex.getMaxValues(), minMaxIndex.getMinValues());
1:         if (bitSet.isEmpty()) {
1:           return false;
1:         } else {
1:           return true;
1:         }
1:       }
1:     }
commit:9360370
/////////////////////////////////////////////////////////////////////////
0:               outputValues[projectionMap[colCount]] = Decimal.apply(v);
commit:f8e0585
/////////////////////////////////////////////////////////////////////////
1:   // return raw row for handoff
1:   private boolean useRawRow = false;
1: 
/////////////////////////////////////////////////////////////////////////
0:   public void setUseRawRow(boolean useRawRow) {
0:     this.useRawRow = useRawRow;
1:   }
1: 
/////////////////////////////////////////////////////////////////////////
1:             if (useRawRow) {
1:               // read raw row for streaming handoff which does not require decode raw row
1:               readRawRowFromStream();
1:             } else {
0:               readRowFromStream();
1:             }
/////////////////////////////////////////////////////////////////////////
1:   private void readRawRowFromStream() {
1:     input.nextRow();
1:     short nullLen = input.readShort();
1:     BitSet nullBitSet = allNonNull;
1:     if (nullLen > 0) {
1:       nullBitSet = BitSet.valueOf(input.readBytes(nullLen));
1:     }
1:     int colCount = 0;
1:     // primitive type dimension
1:     for (; colCount < isNoDictColumn.length; colCount++) {
1:       if (nullBitSet.get(colCount)) {
1:         outputValues[colCount] = CarbonCommonConstants.MEMBER_DEFAULT_VAL_ARRAY;
1:       } else {
1:         if (isNoDictColumn[colCount]) {
1:           int v = input.readShort();
1:           outputValues[colCount] = input.readBytes(v);
1:         } else {
1:           outputValues[colCount] = input.readInt();
1:         }
1:       }
1:     }
1:     // complex type dimension
1:     for (; colCount < dimensionCount; colCount++) {
1:       if (nullBitSet.get(colCount)) {
1:         outputValues[colCount] = null;
1:       } else {
1:         short v = input.readShort();
1:         outputValues[colCount] = input.readBytes(v);
1:       }
1:     }
1:     // measure
1:     DataType dataType;
1:     for (int msrCount = 0; msrCount < measureCount; msrCount++, colCount++) {
1:       if (nullBitSet.get(colCount)) {
1:         outputValues[colCount] = null;
1:       } else {
1:         dataType = measureDataTypes[msrCount];
1:         if (dataType == DataTypes.BOOLEAN) {
1:           outputValues[colCount] = input.readBoolean();
1:         } else if (dataType == DataTypes.SHORT) {
1:           outputValues[colCount] = input.readShort();
1:         } else if (dataType == DataTypes.INT) {
1:           outputValues[colCount] = input.readInt();
1:         } else if (dataType == DataTypes.LONG) {
1:           outputValues[colCount] = input.readLong();
1:         } else if (dataType == DataTypes.DOUBLE) {
1:           outputValues[colCount] = input.readDouble();
1:         } else if (DataTypes.isDecimal(dataType)) {
1:           int len = input.readShort();
1:           outputValues[colCount] = DataTypeUtil.byteToBigDecimal(input.readBytes(len));
1:         }
1:       }
1:     }
1:   }
1: 
commit:e7f9422
/////////////////////////////////////////////////////////////////////////
1:     byte[] syncMarker = getSyncMarker(file.toString());
commit:d7393da
/////////////////////////////////////////////////////////////////////////
1: /*
1:  * Licensed to the Apache Software Foundation (ASF) under one or more
1:  * contributor license agreements.  See the NOTICE file distributed with
1:  * this work for additional information regarding copyright ownership.
1:  * The ASF licenses this file to You under the Apache License, Version 2.0
1:  * (the "License"); you may not use this file except in compliance with
1:  * the License.  You may obtain a copy of the License at
1:  *
1:  *    http://www.apache.org/licenses/LICENSE-2.0
1:  *
1:  * Unless required by applicable law or agreed to in writing, software
1:  * distributed under the License is distributed on an "AS IS" BASIS,
1:  * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
1:  * See the License for the specific language governing permissions and
1:  * limitations under the License.
1:  */
1: 
0: package org.apache.carbondata.hadoop.streaming;
1: 
1: import java.io.IOException;
1: import java.math.BigDecimal;
0: import java.math.BigInteger;
1: import java.nio.ByteBuffer;
1: import java.util.BitSet;
1: import java.util.HashMap;
1: import java.util.List;
1: import java.util.Map;
1: 
1: import org.apache.carbondata.core.cache.Cache;
1: import org.apache.carbondata.core.cache.CacheProvider;
1: import org.apache.carbondata.core.cache.CacheType;
1: import org.apache.carbondata.core.cache.dictionary.Dictionary;
1: import org.apache.carbondata.core.cache.dictionary.DictionaryColumnUniqueIdentifier;
1: import org.apache.carbondata.core.constants.CarbonCommonConstants;
1: import org.apache.carbondata.core.datastore.block.SegmentProperties;
1: import org.apache.carbondata.core.keygenerator.directdictionary.DirectDictionaryGenerator;
1: import org.apache.carbondata.core.keygenerator.directdictionary.DirectDictionaryKeyGeneratorFactory;
1: import org.apache.carbondata.core.metadata.datatype.DataTypes;
1: import org.apache.carbondata.core.metadata.encoder.Encoding;
1: import org.apache.carbondata.core.metadata.schema.table.CarbonTable;
1: import org.apache.carbondata.core.metadata.schema.table.column.CarbonColumn;
1: import org.apache.carbondata.core.metadata.schema.table.column.CarbonDimension;
1: import org.apache.carbondata.core.metadata.schema.table.column.CarbonMeasure;
1: import org.apache.carbondata.core.metadata.schema.table.column.ColumnSchema;
1: import org.apache.carbondata.core.reader.CarbonHeaderReader;
1: import org.apache.carbondata.core.scan.expression.exception.FilterUnsupportedException;
1: import org.apache.carbondata.core.scan.filter.FilterUtil;
1: import org.apache.carbondata.core.scan.filter.GenericQueryType;
1: import org.apache.carbondata.core.scan.filter.executer.FilterExecuter;
1: import org.apache.carbondata.core.scan.filter.intf.RowImpl;
1: import org.apache.carbondata.core.scan.filter.intf.RowIntf;
1: import org.apache.carbondata.core.scan.filter.resolver.FilterResolverIntf;
1: import org.apache.carbondata.core.scan.model.QueryModel;
1: import org.apache.carbondata.core.util.CarbonUtil;
1: import org.apache.carbondata.core.util.DataTypeUtil;
1: import org.apache.carbondata.format.BlockletHeader;
1: import org.apache.carbondata.format.FileHeader;
1: import org.apache.carbondata.hadoop.CarbonInputSplit;
1: import org.apache.carbondata.hadoop.CarbonMultiBlockSplit;
1: import org.apache.carbondata.hadoop.api.CarbonTableInputFormat;
0: import org.apache.carbondata.hadoop.util.CarbonTypeUtil;
1: import org.apache.carbondata.processing.util.CarbonDataProcessorUtil;
1: 
1: import org.apache.hadoop.conf.Configuration;
1: import org.apache.hadoop.fs.FSDataInputStream;
1: import org.apache.hadoop.fs.FileSystem;
1: import org.apache.hadoop.fs.Path;
1: import org.apache.hadoop.mapreduce.InputSplit;
1: import org.apache.hadoop.mapreduce.RecordReader;
1: import org.apache.hadoop.mapreduce.TaskAttemptContext;
1: import org.apache.hadoop.mapreduce.lib.input.FileSplit;
1: import org.apache.spark.memory.MemoryMode;
1: import org.apache.spark.sql.catalyst.InternalRow;
1: import org.apache.spark.sql.catalyst.expressions.GenericInternalRow;
0: import org.apache.spark.sql.execution.vectorized.ColumnVector;
0: import org.apache.spark.sql.execution.vectorized.ColumnarBatch;
0: import org.apache.spark.sql.types.CalendarIntervalType;
0: import org.apache.spark.sql.types.DataType;
0: import org.apache.spark.sql.types.DateType;
0: import org.apache.spark.sql.types.Decimal;
0: import org.apache.spark.sql.types.DecimalType;
0: import org.apache.spark.sql.types.StructType;
0: import org.apache.spark.sql.types.TimestampType;
0: import org.apache.spark.unsafe.types.CalendarInterval;
0: import org.apache.spark.unsafe.types.UTF8String;
1: 
1: /**
1:  * Stream record reader
1:  */
1: public class CarbonStreamRecordReader extends RecordReader<Void, Object> {
1:   // vector reader
1:   private boolean isVectorReader;
1: 
1:   // metadata
1:   private CarbonTable carbonTable;
1:   private CarbonColumn[] storageColumns;
1:   private boolean[] isRequired;
0:   private int[] measureDataTypes;
1:   private int dimensionCount;
1:   private int measureCount;
1: 
1:   // input
1:   private FileSplit fileSplit;
1:   private Configuration hadoopConf;
1:   private StreamBlockletReader input;
1:   private boolean isFirstRow = true;
1:   private QueryModel model;
1: 
1:   // decode data
1:   private BitSet allNonNull;
1:   private boolean[] isNoDictColumn;
1:   private DirectDictionaryGenerator[] directDictionaryGenerators;
1:   private CacheProvider cacheProvider;
1:   private Cache<DictionaryColumnUniqueIdentifier, Dictionary> cache;
1:   private GenericQueryType[] queryTypes;
1: 
1:   // vectorized reader
1:   private StructType outputSchema;
0:   private ColumnarBatch columnarBatch;
1:   private boolean isFinished = false;
1: 
1:   // filter
1:   private FilterExecuter filter;
1:   private boolean[] isFilterRequired;
1:   private Object[] filterValues;
1:   private RowIntf filterRow;
1:   private int[] filterMap;
1: 
1:   // output
1:   private CarbonColumn[] projection;
1:   private boolean[] isProjectionRequired;
1:   private int[] projectionMap;
1:   private Object[] outputValues;
1:   private InternalRow outputRow;
1: 
1:   // empty project, null filter
1:   private boolean skipScanData;
1: 
1:   @Override public void initialize(InputSplit split, TaskAttemptContext context)
1:       throws IOException, InterruptedException {
1:     // input
1:     if (split instanceof CarbonInputSplit) {
1:       fileSplit = (CarbonInputSplit) split;
1:     } else if (split instanceof CarbonMultiBlockSplit) {
1:       fileSplit = ((CarbonMultiBlockSplit) split).getAllSplits().get(0);
1:     } else {
1:       fileSplit = (FileSplit) split;
1:     }
1: 
1:     // metadata
1:     hadoopConf = context.getConfiguration();
1:     if (model == null) {
1:       CarbonTableInputFormat format = new CarbonTableInputFormat<Object>();
0:       model = format.getQueryModel(split, context);
1:     }
1:     carbonTable = model.getTable();
1:     List<CarbonDimension> dimensions =
0:         carbonTable.getDimensionByTableName(carbonTable.getFactTableName());
1:     dimensionCount = dimensions.size();
1:     List<CarbonMeasure> measures =
0:         carbonTable.getMeasureByTableName(carbonTable.getFactTableName());
1:     measureCount = measures.size();
1:     List<CarbonColumn> carbonColumnList =
0:         carbonTable.getStreamStorageOrderColumn(carbonTable.getFactTableName());
1:     storageColumns = carbonColumnList.toArray(new CarbonColumn[carbonColumnList.size()]);
1:     isNoDictColumn = CarbonDataProcessorUtil.getNoDictionaryMapping(storageColumns);
1:     directDictionaryGenerators = new DirectDictionaryGenerator[storageColumns.length];
1:     for (int i = 0; i < storageColumns.length; i++) {
1:       if (storageColumns[i].hasEncoding(Encoding.DIRECT_DICTIONARY)) {
1:         directDictionaryGenerators[i] = DirectDictionaryKeyGeneratorFactory
1:             .getDirectDictionaryGenerator(storageColumns[i].getDataType());
1:       }
1:     }
0:     measureDataTypes = new int[measureCount];
1:     for (int i = 0; i < measureCount; i++) {
0:       measureDataTypes[i] = storageColumns[dimensionCount + i].getDataType().getId();
1:     }
1: 
1:     // decode data
1:     allNonNull = new BitSet(storageColumns.length);
1:     projection = model.getProjectionColumns();
1: 
1:     isRequired = new boolean[storageColumns.length];
1:     boolean[] isFiltlerDimensions = model.getIsFilterDimensions();
1:     boolean[] isFiltlerMeasures = model.getIsFilterMeasures();
1:     isFilterRequired = new boolean[storageColumns.length];
1:     filterMap = new int[storageColumns.length];
1:     for (int i = 0; i < storageColumns.length; i++) {
1:       if (storageColumns[i].isDimension()) {
1:         if (isFiltlerDimensions[storageColumns[i].getOrdinal()]) {
1:           isRequired[i] = true;
1:           isFilterRequired[i] = true;
1:           filterMap[i] = storageColumns[i].getOrdinal();
1:         }
1:       } else {
1:         if (isFiltlerMeasures[storageColumns[i].getOrdinal()]) {
1:           isRequired[i] = true;
1:           isFilterRequired[i] = true;
1:           filterMap[i] = carbonTable.getDimensionOrdinalMax() + storageColumns[i].getOrdinal();
1:         }
1:       }
1:     }
1: 
1:     isProjectionRequired = new boolean[storageColumns.length];
1:     projectionMap = new int[storageColumns.length];
1:     for (int i = 0; i < storageColumns.length; i++) {
1:       for (int j = 0; j < projection.length; j++) {
1:         if (storageColumns[i].getColName().equals(projection[j].getColName())) {
1:           isRequired[i] = true;
1:           isProjectionRequired[i] = true;
1:           projectionMap[i] = j;
1:           break;
1:         }
1:       }
1:     }
1: 
1:     // initialize filter
1:     if (null != model.getFilterExpressionResolverTree()) {
1:       initializeFilter();
1:     } else if (projection.length == 0) {
1:       skipScanData = true;
1:     }
1: 
1:   }
1: 
1:   private void initializeFilter() {
1: 
1:     List<ColumnSchema> wrapperColumnSchemaList = CarbonUtil
0:         .getColumnSchemaList(carbonTable.getDimensionByTableName(carbonTable.getFactTableName()),
0:             carbonTable.getMeasureByTableName(carbonTable.getFactTableName()));
1:     int[] dimLensWithComplex = new int[wrapperColumnSchemaList.size()];
1:     for (int i = 0; i < dimLensWithComplex.length; i++) {
1:       dimLensWithComplex[i] = Integer.MAX_VALUE;
1:     }
1: 
1:     int[] dictionaryColumnCardinality =
1:         CarbonUtil.getFormattedCardinality(dimLensWithComplex, wrapperColumnSchemaList);
1:     SegmentProperties segmentProperties =
1:         new SegmentProperties(wrapperColumnSchemaList, dictionaryColumnCardinality);
1:     Map<Integer, GenericQueryType> complexDimensionInfoMap = new HashMap<>();
1: 
1:     FilterResolverIntf resolverIntf = model.getFilterExpressionResolverTree();
1:     filter = FilterUtil.getFilterExecuterTree(resolverIntf, segmentProperties,
1:         complexDimensionInfoMap);
1:     // for row filter, we need update column index
1:     FilterUtil.updateIndexOfColumnExpression(resolverIntf.getFilterExpression(),
1:         carbonTable.getDimensionOrdinalMax());
1: 
1:   }
1: 
0:   public void setQueryModel(QueryModel model) {
0:     this.model = model;
1:   }
1: 
1:   private byte[] getSyncMarker(String filePath) throws IOException {
1:     CarbonHeaderReader headerReader = new CarbonHeaderReader(filePath);
1:     FileHeader header = headerReader.readHeader();
1:     return header.getSync_marker();
1:   }
1: 
1:   private void initializeAtFirstRow() throws IOException {
1:     filterValues = new Object[carbonTable.getDimensionOrdinalMax() + measureCount];
1:     filterRow = new RowImpl();
1:     filterRow.setValues(filterValues);
1: 
1:     outputValues = new Object[projection.length];
1:     outputRow = new GenericInternalRow(outputValues);
1: 
1:     Path file = fileSplit.getPath();
1: 
0:     byte[] syncMarker = getSyncMarker(file.toUri().getPath());
1: 
1:     FileSystem fs = file.getFileSystem(hadoopConf);
1: 
1:     int bufferSize = Integer.parseInt(hadoopConf.get(CarbonStreamInputFormat.READ_BUFFER_SIZE,
1:         CarbonStreamInputFormat.READ_BUFFER_SIZE_DEFAULT));
1: 
1:     FSDataInputStream fileIn = fs.open(file, bufferSize);
1:     fileIn.seek(fileSplit.getStart());
1:     input = new StreamBlockletReader(syncMarker, fileIn, fileSplit.getLength(),
0:         fileSplit.getStart() == 0);
1: 
1:     cacheProvider = CacheProvider.getInstance();
0:     cache = cacheProvider.createCache(CacheType.FORWARD_DICTIONARY, carbonTable.getStorePath());
1:     queryTypes = CarbonStreamInputFormat.getComplexDimensions(carbonTable, storageColumns, cache);
1: 
0:     outputSchema = new StructType(CarbonTypeUtil.convertCarbonSchemaToSparkSchema(projection));
1:   }
1: 
1:   @Override public boolean nextKeyValue() throws IOException, InterruptedException {
1:     if (isFirstRow) {
1:       isFirstRow = false;
1:       initializeAtFirstRow();
1:     }
1:     if (isFinished) {
1:       return false;
1:     }
1: 
1:     if (isVectorReader) {
1:       return nextColumnarBatch();
1:     }
1: 
1:     return nextRow();
1:   }
1: 
1:   /**
1:    * for vector reader, check next columnar batch
1:    */
1:   private boolean nextColumnarBatch() throws IOException {
1:     boolean hasNext;
1:     boolean scanMore = false;
1:     do {
1:       // move to the next blocklet
1:       hasNext = input.nextBlocklet();
1:       if (hasNext) {
1:         // read blocklet header
1:         BlockletHeader header = input.readBlockletHeader();
1:         if (isScanRequired(header)) {
1:           scanMore = !scanBlockletAndFillVector(header);
1:         } else {
1:           input.skipBlockletData(true);
1:           scanMore = true;
1:         }
1:       } else {
1:         isFinished = true;
1:         scanMore = false;
1:       }
1:     } while (scanMore);
1:     return hasNext;
1:   }
1: 
1:   /**
1:    * check next Row
1:    */
1:   private boolean nextRow() throws IOException {
1:     // read row one by one
0:     try {
1:       boolean hasNext;
1:       boolean scanMore = false;
1:       do {
1:         hasNext = input.hasNext();
1:         if (hasNext) {
1:           if (skipScanData) {
1:             input.nextRow();
1:             scanMore = false;
1:           } else {
0:             readRowFromStream();
1:             if (null != filter) {
1:               scanMore = !filter.applyFilter(filterRow, carbonTable.getDimensionOrdinalMax());
1:             } else {
1:               scanMore = false;
1:             }
1:           }
1:         } else {
1:           if (input.nextBlocklet()) {
1:             BlockletHeader header = input.readBlockletHeader();
1:             if (isScanRequired(header)) {
1:               if (skipScanData) {
1:                 input.skipBlockletData(false);
1:               } else {
0:                 input.readBlockletData(header);
1:               }
1:             } else {
1:               input.skipBlockletData(true);
1:             }
1:             scanMore = true;
1:           } else {
1:             isFinished = true;
1:             scanMore = false;
1:           }
1:         }
1:       } while (scanMore);
1:       return hasNext;
0:     } catch (FilterUnsupportedException e) {
1:       throw new IOException("Failed to filter row in detail reader", e);
1:     }
1:   }
1: 
1:   @Override public Void getCurrentKey() throws IOException, InterruptedException {
1:     return null;
1:   }
1: 
0:   @Override public Object getCurrentValue() throws IOException, InterruptedException {
1:     if (isVectorReader) {
0:       return columnarBatch;
1:     }
1:     return outputRow;
1:   }
1: 
1:   private boolean isScanRequired(BlockletHeader header) {
0:     // TODO require to implement min-max index
0:     if (null == filter) {
1:       return true;
1:     }
1:     return true;
1:   }
1: 
0:   private boolean scanBlockletAndFillVector(BlockletHeader header) throws IOException {
0:     // if filter is null and output projection is empty, use the row number of blocklet header
1:     if (skipScanData) {
0:       int rowNums = header.getBlocklet_info().getNum_rows();
0:       columnarBatch = ColumnarBatch.allocate(outputSchema, MemoryMode.OFF_HEAP, rowNums);
0:       columnarBatch.setNumRows(rowNums);
1:       input.skipBlockletData(true);
0:       return rowNums > 0;
1:     }
1: 
0:     input.readBlockletData(header);
0:     columnarBatch = ColumnarBatch.allocate(outputSchema, MemoryMode.OFF_HEAP, input.getRowNums());
0:     int rowNum = 0;
0:     if (null == filter) {
0:       while (input.hasNext()) {
0:         readRowFromStream();
0:         putRowToColumnBatch(rowNum++);
1:       }
1:     } else {
0:       try {
0:         while (input.hasNext()) {
0:           readRowFromStream();
0:           if (filter.applyFilter(filterRow, carbonTable.getDimensionOrdinalMax())) {
0:             putRowToColumnBatch(rowNum++);
1:           }
1:         }
0:       } catch (FilterUnsupportedException e) {
0:         throw new IOException("Failed to filter row in vector reader", e);
1:       }
1:     }
0:     columnarBatch.setNumRows(rowNum);
0:     return rowNum > 0;
1:   }
1: 
1:   private void readRowFromStream() {
1:     input.nextRow();
1:     short nullLen = input.readShort();
1:     BitSet nullBitSet = allNonNull;
1:     if (nullLen > 0) {
1:       nullBitSet = BitSet.valueOf(input.readBytes(nullLen));
1:     }
1:     int colCount = 0;
1:     // primitive type dimension
1:     for (; colCount < isNoDictColumn.length; colCount++) {
1:       if (nullBitSet.get(colCount)) {
1:         if (isFilterRequired[colCount]) {
1:           filterValues[filterMap[colCount]] = CarbonCommonConstants.MEMBER_DEFAULT_VAL_ARRAY;
1:         }
1:         if (isProjectionRequired[colCount]) {
1:           outputValues[projectionMap[colCount]] = null;
1:         }
1:       } else {
1:         if (isNoDictColumn[colCount]) {
1:           int v = input.readShort();
1:           if (isRequired[colCount]) {
1:             byte[] b = input.readBytes(v);
1:             if (isFilterRequired[colCount]) {
1:               filterValues[filterMap[colCount]] = b;
1:             }
1:             if (isProjectionRequired[colCount]) {
1:               outputValues[projectionMap[colCount]] =
1:                   DataTypeUtil.getDataBasedOnDataTypeForNoDictionaryColumn(b,
1:                       storageColumns[colCount].getDataType());
1:             }
1:           } else {
1:             input.skipBytes(v);
1:           }
1:         } else if (null != directDictionaryGenerators[colCount]) {
1:           if (isRequired[colCount]) {
1:             if (isFilterRequired[colCount]) {
1:               filterValues[filterMap[colCount]] = input.copy(4);
1:             }
1:             if (isProjectionRequired[colCount]) {
1:               outputValues[projectionMap[colCount]] =
1:                   directDictionaryGenerators[colCount].getValueFromSurrogate(input.readInt());
1:             } else {
1:               input.skipBytes(4);
1:             }
1:           } else {
1:             input.skipBytes(4);
1:           }
1:         } else {
1:           if (isRequired[colCount]) {
1:             if (isFilterRequired[colCount]) {
1:               filterValues[filterMap[colCount]] = input.copy(4);
1:             }
1:             if (isProjectionRequired[colCount]) {
1:               outputValues[projectionMap[colCount]] = input.readInt();
1:             } else {
1:               input.skipBytes(4);
1:             }
1:           } else {
1:             input.skipBytes(4);
1:           }
1:         }
1:       }
1:     }
1:     // complex type dimension
1:     for (; colCount < dimensionCount; colCount++) {
1:       if (nullBitSet.get(colCount)) {
1:         if (isFilterRequired[colCount]) {
1:           filterValues[filterMap[colCount]] = null;
1:         }
1:         if (isProjectionRequired[colCount]) {
1:           outputValues[projectionMap[colCount]] = null;
1:         }
1:       } else {
1:         short v = input.readShort();
1:         if (isRequired[colCount]) {
1:           byte[] b = input.readBytes(v);
1:           if (isFilterRequired[colCount]) {
1:             filterValues[filterMap[colCount]] = b;
1:           }
1:           if (isProjectionRequired[colCount]) {
1:             outputValues[projectionMap[colCount]] = queryTypes[colCount]
0:                 .getDataBasedOnDataTypeFromSurrogates(ByteBuffer.wrap(b));
1:           }
1:         } else {
1:           input.skipBytes(v);
1:         }
1:       }
1:     }
1:     // measure
0:     int dataType;
1:     for (int msrCount = 0; msrCount < measureCount; msrCount++, colCount++) {
1:       if (nullBitSet.get(colCount)) {
1:         if (isFilterRequired[colCount]) {
1:           filterValues[filterMap[colCount]] = null;
1:         }
1:         if (isProjectionRequired[colCount]) {
1:           outputValues[projectionMap[colCount]] = null;
1:         }
1:       } else {
1:         dataType = measureDataTypes[msrCount];
0:         if (dataType == DataTypes.BOOLEAN_TYPE_ID) {
1:           if (isRequired[colCount]) {
1:             boolean v = input.readBoolean();
1:             if (isFilterRequired[colCount]) {
1:               filterValues[filterMap[colCount]] = v;
1:             }
1:             if (isProjectionRequired[colCount]) {
1:               outputValues[projectionMap[colCount]] = v;
1:             }
1:           } else {
1:             input.skipBytes(1);
1:           }
0:         } else if (dataType == DataTypes.SHORT_TYPE_ID) {
1:           if (isRequired[colCount]) {
1:             short v = input.readShort();
1:             if (isFilterRequired[colCount]) {
1:               filterValues[filterMap[colCount]] = v;
1:             }
1:             if (isProjectionRequired[colCount]) {
1:               outputValues[projectionMap[colCount]] = v;
1:             }
1:           } else {
1:             input.skipBytes(2);
1:           }
0:         } else if (dataType == DataTypes.INT_TYPE_ID) {
1:           if (isRequired[colCount]) {
1:             int v = input.readInt();
1:             if (isFilterRequired[colCount]) {
1:               filterValues[filterMap[colCount]] = v;
1:             }
1:             if (isProjectionRequired[colCount]) {
1:               outputValues[projectionMap[colCount]] = v;
1:             }
1:           } else {
1:             input.skipBytes(4);
1:           }
0:         } else if (dataType == DataTypes.LONG_TYPE_ID) {
1:           if (isRequired[colCount]) {
1:             long v = input.readLong();
1:             if (isFilterRequired[colCount]) {
1:               filterValues[filterMap[colCount]] = v;
1:             }
1:             if (isProjectionRequired[colCount]) {
1:               outputValues[projectionMap[colCount]] = v;
1:             }
1:           } else {
1:             input.skipBytes(8);
1:           }
0:         } else if (dataType == DataTypes.DOUBLE_TYPE_ID) {
1:           if (isRequired[colCount]) {
1:             double v = input.readDouble();
1:             if (isFilterRequired[colCount]) {
1:               filterValues[filterMap[colCount]] = v;
1:             }
1:             if (isProjectionRequired[colCount]) {
1:               outputValues[projectionMap[colCount]] = v;
1:             }
1:           } else {
1:             input.skipBytes(8);
1:           }
0:         } else if (dataType == DataTypes.DECIMAL_TYPE_ID) {
1:           int len = input.readShort();
1:           if (isRequired[colCount]) {
1:             BigDecimal v = DataTypeUtil.byteToBigDecimal(input.readBytes(len));
1:             if (isFilterRequired[colCount]) {
1:               filterValues[filterMap[colCount]] = v;
1:             }
1:             if (isProjectionRequired[colCount]) {
1:               outputValues[projectionMap[colCount]] = v;
1:             }
1:           } else {
1:             input.skipBytes(len);
1:           }
1:         }
1:       }
1:     }
1:   }
1: 
0:   private void putRowToColumnBatch(int rowId) {
0:     for (int i = 0; i < projection.length; i++) {
0:       Object value = outputValues[i];
0:       ColumnVector col = columnarBatch.column(i);
0:       DataType t = col.dataType();
0:       if (null == value) {
0:         col.putNull(rowId);
1:       } else {
0:         if (t == org.apache.spark.sql.types.DataTypes.BooleanType) {
0:           col.putBoolean(rowId, (boolean)value);
0:         } else if (t == org.apache.spark.sql.types.DataTypes.ByteType) {
0:           col.putByte(rowId, (byte) value);
0:         } else if (t == org.apache.spark.sql.types.DataTypes.ShortType) {
0:           col.putShort(rowId, (short) value);
0:         } else if (t == org.apache.spark.sql.types.DataTypes.IntegerType) {
0:           col.putInt(rowId, (int) value);
0:         } else if (t == org.apache.spark.sql.types.DataTypes.LongType) {
0:           col.putLong(rowId, (long) value);
0:         } else if (t == org.apache.spark.sql.types.DataTypes.FloatType) {
0:           col.putFloat(rowId, (float) value);
0:         } else if (t == org.apache.spark.sql.types.DataTypes.DoubleType) {
0:           col.putDouble(rowId, (double) value);
0:         } else if (t == org.apache.spark.sql.types.DataTypes.StringType) {
0:           UTF8String v = (UTF8String) value;
0:           col.putByteArray(rowId, v.getBytes());
0:         } else if (t instanceof DecimalType) {
0:           DecimalType dt = (DecimalType)t;
0:           Decimal d = (Decimal) value;
0:           if (dt.precision() <= Decimal.MAX_INT_DIGITS()) {
0:             col.putInt(rowId, (int)d.toUnscaledLong());
0:           } else if (dt.precision() <= Decimal.MAX_LONG_DIGITS()) {
0:             col.putLong(rowId, d.toUnscaledLong());
1:           } else {
0:             final BigInteger integer = d.toJavaBigDecimal().unscaledValue();
0:             byte[] bytes = integer.toByteArray();
0:             col.putByteArray(rowId, bytes, 0, bytes.length);
1:           }
0:         } else if (t instanceof CalendarIntervalType) {
0:           CalendarInterval c = (CalendarInterval) value;
0:           col.getChildColumn(0).putInt(rowId, c.months);
0:           col.getChildColumn(1).putLong(rowId, c.microseconds);
0:         } else if (t instanceof DateType) {
0:           col.putInt(rowId, (int) value);
0:         } else if (t instanceof TimestampType) {
0:           col.putLong(rowId, (long) value);
1:         }
1:       }
1:     }
1:   }
1: 
0:   @Override public float getProgress() throws IOException, InterruptedException {
0:     return 0;
1:   }
1: 
0:   public void setVectorReader(boolean isVectorReader) {
0:     this.isVectorReader = isVectorReader;
1:   }
1: 
0:   @Override public void close() throws IOException {
0:     if (null != input) {
0:       input.close();
1:     }
0:     if (null != columnarBatch) {
0:       columnarBatch.close();
1:     }
1:   }
1: }
author:Raghunandan S
-------------------------------------------------------------------------------
commit:7ef9164
/////////////////////////////////////////////////////////////////////////
author:sounakr
-------------------------------------------------------------------------------
commit:3202cf5
/////////////////////////////////////////////////////////////////////////
1:                 .getDataBasedOnDataType(ByteBuffer.wrap(b));
author:Jacky Li
-------------------------------------------------------------------------------
commit:3ff574d
/////////////////////////////////////////////////////////////////////////
0:     for (int j = 0; j < projection.length; j++) {
0:       for (int i = 0; i < storageColumns.length; i++) {
commit:c723947
/////////////////////////////////////////////////////////////////////////
0: package org.apache.carbondata.streaming;
/////////////////////////////////////////////////////////////////////////
/////////////////////////////////////////////////////////////////////////
0: import org.apache.spark.sql.types.StructField;
/////////////////////////////////////////////////////////////////////////
1:     outputSchema = new StructType((StructField[])
1:         DataTypeUtil.getDataTypeConverter().convertCarbonSchemaToSparkSchema(projection));
/////////////////////////////////////////////////////////////////////////
1:               outputValues[projectionMap[colCount]] =
1:                   DataTypeUtil.getDataTypeConverter().convertFromBigDecimalToDecimal(v);
commit:daa6465
/////////////////////////////////////////////////////////////////////////
1:       model = format.createQueryModel(split, context);
commit:5fc7f06
/////////////////////////////////////////////////////////////////////////
1:         carbonTable.getDimensionByTableName(carbonTable.getTableName());
1:         carbonTable.getMeasureByTableName(carbonTable.getTableName());
1:         carbonTable.getStreamStorageOrderColumn(carbonTable.getTableName());
/////////////////////////////////////////////////////////////////////////
1:         .getColumnSchemaList(carbonTable.getDimensionByTableName(carbonTable.getTableName()),
1:             carbonTable.getMeasureByTableName(carbonTable.getTableName()));
commit:933e30c
/////////////////////////////////////////////////////////////////////////
1: import org.apache.carbondata.core.metadata.datatype.DataType;
/////////////////////////////////////////////////////////////////////////
/////////////////////////////////////////////////////////////////////////
1:   private DataType[] measureDataTypes;
/////////////////////////////////////////////////////////////////////////
1:     measureDataTypes = new DataType[measureCount];
1:       measureDataTypes[i] = storageColumns[dimensionCount + i].getDataType();
/////////////////////////////////////////////////////////////////////////
1:     DataType dataType;
/////////////////////////////////////////////////////////////////////////
1:         if (dataType == DataTypes.BOOLEAN) {
/////////////////////////////////////////////////////////////////////////
1:         } else if (dataType == DataTypes.SHORT) {
/////////////////////////////////////////////////////////////////////////
1:         } else if (dataType == DataTypes.INT) {
/////////////////////////////////////////////////////////////////////////
1:         } else if (dataType == DataTypes.LONG) {
/////////////////////////////////////////////////////////////////////////
1:         } else if (dataType == DataTypes.DOUBLE) {
/////////////////////////////////////////////////////////////////////////
1:         } else if (DataTypes.isDecimal(dataType)) {
/////////////////////////////////////////////////////////////////////////
0:       org.apache.spark.sql.types.DataType t = col.dataType();
/////////////////////////////////////////////////////////////////////////
0:         } else if (t instanceof org.apache.spark.sql.types.DecimalType) {
/////////////////////////////////////////////////////////////////////////
0:         } else if (t instanceof org.apache.spark.sql.types.DateType) {
0:         } else if (t instanceof org.apache.spark.sql.types.TimestampType) {
author:BJangir
-------------------------------------------------------------------------------
commit:eae9064
/////////////////////////////////////////////////////////////////////////
1: import org.apache.carbondata.hadoop.InputMetricsStats;
/////////////////////////////////////////////////////////////////////////
1:   // InputMetricsStats
1:   private InputMetricsStats inputMetricsStats;
0: 
/////////////////////////////////////////////////////////////////////////
0:       int value = columnarBatch.numValidRows();
0:       if (inputMetricsStats != null) {
0:         inputMetricsStats.incrementRecordRead((long) value);
0:       }
0: 
0: 
0:     if (inputMetricsStats != null) {
1:       inputMetricsStats.incrementRecordRead(1L);
0:     }
0: 
/////////////////////////////////////////////////////////////////////////
0:   public void setInputMetricsStats(InputMetricsStats inputMetricsStats) {
0:     this.inputMetricsStats = inputMetricsStats;
0:   }
0: 
author:anubhav100
-------------------------------------------------------------------------------
commit:241b265
/////////////////////////////////////////////////////////////////////////
0:           Decimal d = Decimal.fromDecimal(value);
author:mohammadshahidkhan
-------------------------------------------------------------------------------
commit:1155d4d
/////////////////////////////////////////////////////////////////////////
1:     cache = cacheProvider.createCache(CacheType.FORWARD_DICTIONARY);
============================================================================