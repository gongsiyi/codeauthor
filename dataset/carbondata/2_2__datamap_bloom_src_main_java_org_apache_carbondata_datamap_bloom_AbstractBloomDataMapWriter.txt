1:5c483f3: /*
1:5c483f3:  * Licensed to the Apache Software Foundation (ASF) under one or more
1:5c483f3:  * contributor license agreements.  See the NOTICE file distributed with
1:5c483f3:  * this work for additional information regarding copyright ownership.
1:5c483f3:  * The ASF licenses this file to You under the Apache License, Version 2.0
1:5c483f3:  * (the "License"); you may not use this file except in compliance with
1:5c483f3:  * the License.  You may obtain a copy of the License at
1:5c483f3:  *
1:5c483f3:  *    http://www.apache.org/licenses/LICENSE-2.0
1:5c483f3:  *
1:5c483f3:  * Unless required by applicable law or agreed to in writing, software
1:5c483f3:  * distributed under the License is distributed on an "AS IS" BASIS,
1:5c483f3:  * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
1:5c483f3:  * See the License for the specific language governing permissions and
1:5c483f3:  * limitations under the License.
1:5c483f3:  */
4:5c483f3: 
1:5c483f3: package org.apache.carbondata.datamap.bloom;
1:5c483f3: 
1:5c483f3: import java.io.DataOutputStream;
1:5c483f3: import java.io.IOException;
1:5c483f3: import java.util.ArrayList;
1:5c483f3: import java.util.List;
1:5c483f3: 
1:5c483f3: import org.apache.carbondata.common.annotations.InterfaceAudience;
1:5c483f3: import org.apache.carbondata.common.logging.LogService;
1:5c483f3: import org.apache.carbondata.common.logging.LogServiceFactory;
1:5c483f3: import org.apache.carbondata.core.constants.CarbonCommonConstants;
1:5c483f3: import org.apache.carbondata.core.datamap.Segment;
1:5c483f3: import org.apache.carbondata.core.datamap.dev.DataMapWriter;
1:5c483f3: import org.apache.carbondata.core.datastore.block.SegmentProperties;
1:5c483f3: import org.apache.carbondata.core.datastore.impl.FileFactory;
1:5c483f3: import org.apache.carbondata.core.datastore.page.ColumnPage;
1:7551cc6: import org.apache.carbondata.core.datastore.page.encoding.bool.BooleanConvert;
1:7551cc6: import org.apache.carbondata.core.metadata.datatype.DataTypes;
1:5c483f3: import org.apache.carbondata.core.metadata.encoder.Encoding;
1:5c483f3: import org.apache.carbondata.core.metadata.schema.table.column.CarbonColumn;
1:5c483f3: import org.apache.carbondata.core.util.CarbonUtil;
1:5c483f3: 
1:5c483f3: import org.apache.hadoop.util.bloom.CarbonBloomFilter;
1:5c483f3: import org.apache.hadoop.util.bloom.Key;
1:5c483f3: import org.apache.hadoop.util.hash.Hash;
1:5c483f3: 
1:5c483f3: @InterfaceAudience.Internal
1:5c483f3: public abstract class AbstractBloomDataMapWriter extends DataMapWriter {
1:5c483f3:   private static final LogService LOG = LogServiceFactory.getLogService(
1:5c483f3:       BloomDataMapWriter.class.getCanonicalName());
1:5c483f3:   private int bloomFilterSize;
1:5c483f3:   private double bloomFilterFpp;
1:5c483f3:   private boolean compressBloom;
1:5c483f3:   protected int currentBlockletId;
1:5c483f3:   private List<String> currentDMFiles;
1:5c483f3:   private List<DataOutputStream> currentDataOutStreams;
1:5c483f3:   protected List<CarbonBloomFilter> indexBloomFilters;
1:5c483f3: 
1:5c483f3:   AbstractBloomDataMapWriter(String tablePath, String dataMapName, List<CarbonColumn> indexColumns,
1:5c483f3:       Segment segment, String shardName, SegmentProperties segmentProperties,
1:5c483f3:       int bloomFilterSize, double bloomFilterFpp, boolean compressBloom)
1:5c483f3:       throws IOException {
1:5c483f3:     super(tablePath, dataMapName, indexColumns, segment, shardName);
1:5c483f3:     this.bloomFilterSize = bloomFilterSize;
1:5c483f3:     this.bloomFilterFpp = bloomFilterFpp;
1:5c483f3:     this.compressBloom = compressBloom;
1:5c483f3:     currentDMFiles = new ArrayList<>(indexColumns.size());
1:5c483f3:     currentDataOutStreams = new ArrayList<>(indexColumns.size());
1:5c483f3:     indexBloomFilters = new ArrayList<>(indexColumns.size());
1:5c483f3:     initDataMapFile();
1:5c483f3:     resetBloomFilters();
10:5c483f3:   }
1:5c483f3: 
1:5c483f3:   @Override
1:5c483f3:   public void onBlockStart(String blockId) throws IOException {
1:5c483f3:   }
1:5c483f3: 
1:5c483f3:   @Override
1:5c483f3:   public void onBlockEnd(String blockId) throws IOException {
1:5c483f3:   }
1:5c483f3: 
1:5c483f3:   @Override
1:5c483f3:   public void onBlockletStart(int blockletId) {
1:5c483f3:   }
1:5c483f3: 
1:5c483f3:   protected void resetBloomFilters() {
1:5c483f3:     indexBloomFilters.clear();
1:5c483f3:     int[] stats = calculateBloomStats();
1:5c483f3:     for (int i = 0; i < indexColumns.size(); i++) {
1:5c483f3:       indexBloomFilters
1:5c483f3:           .add(new CarbonBloomFilter(stats[0], stats[1], Hash.MURMUR_HASH, compressBloom));
1:5c483f3:     }
1:5c483f3:   }
1:5c483f3: 
1:5c483f3:   /**
1:5c483f3:    * It calculates the bits size and number of hash functions to calculate bloom.
1:5c483f3:    */
1:5c483f3:   private int[] calculateBloomStats() {
1:5c483f3:     /*
1:5c483f3:      * n: how many items you expect to have in your filter
1:5c483f3:      * p: your acceptable false positive rate
1:5c483f3:      * Number of bits (m) = -n*ln(p) / (ln(2)^2)
1:5c483f3:      * Number of hashes(k) = m/n * ln(2)
1:5c483f3:      */
1:5c483f3:     double sizeinBits = -bloomFilterSize * Math.log(bloomFilterFpp) / (Math.pow(Math.log(2), 2));
1:5c483f3:     double numberOfHashes = sizeinBits / bloomFilterSize * Math.log(2);
1:5c483f3:     int[] stats = new int[2];
1:5c483f3:     stats[0] = (int) Math.ceil(sizeinBits);
1:5c483f3:     stats[1] = (int) Math.ceil(numberOfHashes);
1:5c483f3:     return stats;
1:5c483f3:   }
1:5c483f3: 
1:5c483f3:   @Override
1:5c483f3:   public void onBlockletEnd(int blockletId) {
1:5c483f3:     writeBloomDataMapFile();
1:5c483f3:     currentBlockletId++;
1:5c483f3:   }
1:5c483f3: 
1:5c483f3:   @Override
1:5c483f3:   public void onPageAdded(int blockletId, int pageId, int pageSize, ColumnPage[] pages)
1:5c483f3:       throws IOException {
1:5c483f3:     for (int rowId = 0; rowId < pageSize; rowId++) {
1:5c483f3:       // for each indexed column, add the data to index
1:5c483f3:       for (int i = 0; i < indexColumns.size(); i++) {
1:5c483f3:         Object data = pages[i].getData(rowId);
1:5c483f3:         addValue2BloomIndex(i, data);
1:5c483f3:       }
1:5c483f3:     }
1:5c483f3:   }
1:5c483f3: 
1:5c483f3:   protected void addValue2BloomIndex(int indexColIdx, Object value) {
1:5c483f3:     byte[] indexValue;
1:5c483f3:     // convert measure to bytes
1:5c483f3:     // convert non-dict dimensions to simple bytes without length
1:5c483f3:     // convert internal-dict dimensions to simple bytes without any encode
1:5c483f3:     if (indexColumns.get(indexColIdx).isMeasure()) {
1:7551cc6:       // NULL value of all measures are already processed in `ColumnPage.getData`
1:7551cc6:       // or `RawBytesReadSupport.readRow` with actual data type
1:7551cc6: 
1:7551cc6:       // Carbon stores boolean as byte. Here we convert it for `getValueAsBytes`
1:7551cc6:       if (indexColumns.get(indexColIdx).getDataType().equals(DataTypes.BOOLEAN)) {
1:7551cc6:         value = BooleanConvert.boolean2Byte((Boolean)value);
1:5c483f3:       }
1:5c483f3:       indexValue = CarbonUtil.getValueAsBytes(indexColumns.get(indexColIdx).getDataType(), value);
2:5c483f3:     } else {
1:5c483f3:       if (indexColumns.get(indexColIdx).hasEncoding(Encoding.DICTIONARY)
1:5c483f3:           || indexColumns.get(indexColIdx).hasEncoding(Encoding.DIRECT_DICTIONARY)) {
1:81038f5:         indexValue = convertDictionaryValue(indexColIdx, value);
1:5c483f3:       } else {
1:5c483f3:         indexValue = convertNonDictionaryValue(indexColIdx, (byte[]) value);
1:5c483f3:       }
1:5c483f3:     }
1:5c483f3:     if (indexValue.length == 0) {
1:5c483f3:       indexValue = CarbonCommonConstants.MEMBER_DEFAULT_VAL_ARRAY;
1:5c483f3:     }
1:5c483f3:     indexBloomFilters.get(indexColIdx).add(new Key(indexValue));
1:5c483f3:   }
1:5c483f3: 
1:81038f5:   protected abstract byte[] convertDictionaryValue(int indexColIdx, Object value);
1:5c483f3: 
1:5c483f3:   protected abstract byte[] convertNonDictionaryValue(int indexColIdx, byte[] value);
1:5c483f3: 
1:5c483f3:   private void initDataMapFile() throws IOException {
1:5c483f3:     if (!FileFactory.isFileExist(dataMapPath)) {
1:5c483f3:       if (!FileFactory.mkdirs(dataMapPath, FileFactory.getFileType(dataMapPath))) {
1:5c483f3:         throw new IOException("Failed to create directory " + dataMapPath);
1:5c483f3:       }
1:5c483f3:     }
1:5c483f3:     for (int indexColId = 0; indexColId < indexColumns.size(); indexColId++) {
1:7b31b91:       String dmFile = BloomIndexFileStore.getBloomIndexFile(dataMapPath,
1:5c483f3:           indexColumns.get(indexColId).getColName());
1:5c483f3:       DataOutputStream dataOutStream = null;
1:5c483f3:       try {
1:5c483f3:         FileFactory.createNewFile(dmFile, FileFactory.getFileType(dmFile));
1:5c483f3:         dataOutStream = FileFactory.getDataOutputStream(dmFile,
1:5c483f3:             FileFactory.getFileType(dmFile));
1:5c483f3:       } catch (IOException e) {
1:5c483f3:         CarbonUtil.closeStreams(dataOutStream);
1:5c483f3:         throw new IOException(e);
1:5c483f3:       }
1:5c483f3: 
1:5c483f3:       this.currentDMFiles.add(dmFile);
1:5c483f3:       this.currentDataOutStreams.add(dataOutStream);
1:5c483f3:     }
1:5c483f3:   }
1:5c483f3: 
1:5c483f3:   protected void writeBloomDataMapFile() {
1:5c483f3:     try {
1:5c483f3:       for (int indexColId = 0; indexColId < indexColumns.size(); indexColId++) {
1:5c483f3:         CarbonBloomFilter bloomFilter = indexBloomFilters.get(indexColId);
1:5c483f3:         bloomFilter.setBlockletNo(currentBlockletId);
1:5c483f3:         // only in higher version of guava-bloom-filter, it provides readFrom/writeTo interface.
1:5c483f3:         // In lower version, we use default java serializer to write bloomfilter.
1:5c483f3:         bloomFilter.write(this.currentDataOutStreams.get(indexColId));
1:5c483f3:         this.currentDataOutStreams.get(indexColId).flush();
1:5c483f3:       }
1:5c483f3:     } catch (Exception e) {
1:5c483f3:       for (DataOutputStream dataOutputStream : currentDataOutStreams) {
1:5c483f3:         CarbonUtil.closeStreams(dataOutputStream);
1:5c483f3:       }
1:5c483f3:       throw new RuntimeException(e);
1:5c483f3:     } finally {
1:5c483f3:       resetBloomFilters();
1:5c483f3:     }
1:5c483f3:   }
1:5c483f3: 
1:5c483f3:   @Override
1:5c483f3:   public void finish() throws IOException {
1:5c483f3:     if (!isWritingFinished()) {
1:5c483f3:       releaseResouce();
1:5c483f3:       setWritingFinished(true);
1:5c483f3:     }
1:5c483f3:   }
1:5c483f3: 
1:5c483f3:   protected void releaseResouce() {
1:5c483f3:     for (int indexColId = 0; indexColId < indexColumns.size(); indexColId++) {
1:5c483f3:       CarbonUtil.closeStreams(
1:5c483f3:           currentDataOutStreams.get(indexColId));
1:5c483f3:     }
1:5c483f3:   }
1:5c483f3: }
============================================================================
author:Manhua
-------------------------------------------------------------------------------
commit:7b31b91
/////////////////////////////////////////////////////////////////////////
1:       String dmFile = BloomIndexFileStore.getBloomIndexFile(dataMapPath,
commit:7551cc6
/////////////////////////////////////////////////////////////////////////
1: import org.apache.carbondata.core.datastore.page.encoding.bool.BooleanConvert;
1: import org.apache.carbondata.core.metadata.datatype.DataTypes;
/////////////////////////////////////////////////////////////////////////
1:       // NULL value of all measures are already processed in `ColumnPage.getData`
1:       // or `RawBytesReadSupport.readRow` with actual data type
1: 
1:       // Carbon stores boolean as byte. Here we convert it for `getValueAsBytes`
1:       if (indexColumns.get(indexColIdx).getDataType().equals(DataTypes.BOOLEAN)) {
1:         value = BooleanConvert.boolean2Byte((Boolean)value);
commit:81038f5
/////////////////////////////////////////////////////////////////////////
/////////////////////////////////////////////////////////////////////////
/////////////////////////////////////////////////////////////////////////
/////////////////////////////////////////////////////////////////////////
/////////////////////////////////////////////////////////////////////////
1:         indexValue = convertDictionaryValue(indexColIdx, value);
/////////////////////////////////////////////////////////////////////////
1:   protected abstract byte[] convertDictionaryValue(int indexColIdx, Object value);
/////////////////////////////////////////////////////////////////////////
author:xuchuanyin
-------------------------------------------------------------------------------
commit:98c7581
/////////////////////////////////////////////////////////////////////////
/////////////////////////////////////////////////////////////////////////
/////////////////////////////////////////////////////////////////////////
/////////////////////////////////////////////////////////////////////////
commit:5c483f3
/////////////////////////////////////////////////////////////////////////
1: /*
1:  * Licensed to the Apache Software Foundation (ASF) under one or more
1:  * contributor license agreements.  See the NOTICE file distributed with
1:  * this work for additional information regarding copyright ownership.
1:  * The ASF licenses this file to You under the Apache License, Version 2.0
1:  * (the "License"); you may not use this file except in compliance with
1:  * the License.  You may obtain a copy of the License at
1:  *
1:  *    http://www.apache.org/licenses/LICENSE-2.0
1:  *
1:  * Unless required by applicable law or agreed to in writing, software
1:  * distributed under the License is distributed on an "AS IS" BASIS,
1:  * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
1:  * See the License for the specific language governing permissions and
1:  * limitations under the License.
1:  */
1: 
1: package org.apache.carbondata.datamap.bloom;
1: 
1: import java.io.DataOutputStream;
1: import java.io.IOException;
1: import java.util.ArrayList;
0: import java.util.HashMap;
1: import java.util.List;
0: import java.util.Map;
1: 
1: import org.apache.carbondata.common.annotations.InterfaceAudience;
1: import org.apache.carbondata.common.logging.LogService;
1: import org.apache.carbondata.common.logging.LogServiceFactory;
1: import org.apache.carbondata.core.constants.CarbonCommonConstants;
1: import org.apache.carbondata.core.datamap.Segment;
1: import org.apache.carbondata.core.datamap.dev.DataMapWriter;
1: import org.apache.carbondata.core.datastore.block.SegmentProperties;
1: import org.apache.carbondata.core.datastore.impl.FileFactory;
1: import org.apache.carbondata.core.datastore.page.ColumnPage;
0: import org.apache.carbondata.core.keygenerator.KeyGenerator;
0: import org.apache.carbondata.core.keygenerator.columnar.ColumnarSplitter;
0: import org.apache.carbondata.core.metadata.datatype.DataTypes;
1: import org.apache.carbondata.core.metadata.encoder.Encoding;
1: import org.apache.carbondata.core.metadata.schema.table.column.CarbonColumn;
0: import org.apache.carbondata.core.metadata.schema.table.column.CarbonDimension;
1: import org.apache.carbondata.core.util.CarbonUtil;
1: 
0: import org.apache.commons.collections.CollectionUtils;
0: import org.apache.commons.collections.Predicate;
1: import org.apache.hadoop.util.bloom.CarbonBloomFilter;
1: import org.apache.hadoop.util.bloom.Key;
1: import org.apache.hadoop.util.hash.Hash;
1: 
1: @InterfaceAudience.Internal
1: public abstract class AbstractBloomDataMapWriter extends DataMapWriter {
1:   private static final LogService LOG = LogServiceFactory.getLogService(
1:       BloomDataMapWriter.class.getCanonicalName());
1:   private int bloomFilterSize;
1:   private double bloomFilterFpp;
1:   private boolean compressBloom;
1:   protected int currentBlockletId;
1:   private List<String> currentDMFiles;
1:   private List<DataOutputStream> currentDataOutStreams;
1:   protected List<CarbonBloomFilter> indexBloomFilters;
0:   private KeyGenerator keyGenerator;
0:   private ColumnarSplitter columnarSplitter;
0:   // for the dict/sort/date column, they are encoded in MDK,
0:   // this maps the index column name to the index in MDK
0:   private Map<String, Integer> indexCol2MdkIdx;
0:   // this gives the reverse map to indexCol2MdkIdx
0:   private Map<Integer, String> mdkIdx2IndexCol;
1: 
1:   AbstractBloomDataMapWriter(String tablePath, String dataMapName, List<CarbonColumn> indexColumns,
1:       Segment segment, String shardName, SegmentProperties segmentProperties,
1:       int bloomFilterSize, double bloomFilterFpp, boolean compressBloom)
1:       throws IOException {
1:     super(tablePath, dataMapName, indexColumns, segment, shardName);
1:     this.bloomFilterSize = bloomFilterSize;
1:     this.bloomFilterFpp = bloomFilterFpp;
1:     this.compressBloom = compressBloom;
1:     currentDMFiles = new ArrayList<>(indexColumns.size());
1:     currentDataOutStreams = new ArrayList<>(indexColumns.size());
1:     indexBloomFilters = new ArrayList<>(indexColumns.size());
1:     initDataMapFile();
1:     resetBloomFilters();
1: 
0:     keyGenerator = segmentProperties.getDimensionKeyGenerator();
0:     columnarSplitter = segmentProperties.getFixedLengthKeySplitter();
0:     this.indexCol2MdkIdx = new HashMap<>();
0:     this.mdkIdx2IndexCol = new HashMap<>();
0:     int idx = 0;
0:     for (final CarbonDimension dimension : segmentProperties.getDimensions()) {
0:       if (!dimension.isGlobalDictionaryEncoding() && !dimension.isDirectDictionaryEncoding()) {
0:         continue;
1:       }
0:       boolean isExistInIndex = CollectionUtils.exists(indexColumns, new Predicate() {
0:         @Override public boolean evaluate(Object object) {
0:           return ((CarbonColumn) object).getColName().equalsIgnoreCase(dimension.getColName());
1:         }
0:       });
0:       if (isExistInIndex) {
0:         this.indexCol2MdkIdx.put(dimension.getColName(), idx);
0:         this.mdkIdx2IndexCol.put(idx, dimension.getColName());
1:       }
0:       idx++;
1:     }
1:   }
1: 
1:   @Override
1:   public void onBlockStart(String blockId) throws IOException {
1:   }
1: 
1:   @Override
1:   public void onBlockEnd(String blockId) throws IOException {
1:   }
1: 
1:   @Override
1:   public void onBlockletStart(int blockletId) {
1:   }
1: 
1:   protected void resetBloomFilters() {
1:     indexBloomFilters.clear();
0:     List<CarbonColumn> indexColumns = getIndexColumns();
1:     int[] stats = calculateBloomStats();
1:     for (int i = 0; i < indexColumns.size(); i++) {
1:       indexBloomFilters
1:           .add(new CarbonBloomFilter(stats[0], stats[1], Hash.MURMUR_HASH, compressBloom));
1:     }
1:   }
1: 
1:   /**
1:    * It calculates the bits size and number of hash functions to calculate bloom.
1:    */
1:   private int[] calculateBloomStats() {
1:     /*
1:      * n: how many items you expect to have in your filter
1:      * p: your acceptable false positive rate
1:      * Number of bits (m) = -n*ln(p) / (ln(2)^2)
1:      * Number of hashes(k) = m/n * ln(2)
1:      */
1:     double sizeinBits = -bloomFilterSize * Math.log(bloomFilterFpp) / (Math.pow(Math.log(2), 2));
1:     double numberOfHashes = sizeinBits / bloomFilterSize * Math.log(2);
1:     int[] stats = new int[2];
1:     stats[0] = (int) Math.ceil(sizeinBits);
1:     stats[1] = (int) Math.ceil(numberOfHashes);
1:     return stats;
1:   }
1: 
1:   @Override
1:   public void onBlockletEnd(int blockletId) {
1:     writeBloomDataMapFile();
1:     currentBlockletId++;
1:   }
1: 
1:   @Override
1:   public void onPageAdded(int blockletId, int pageId, int pageSize, ColumnPage[] pages)
1:       throws IOException {
1:     for (int rowId = 0; rowId < pageSize; rowId++) {
1:       // for each indexed column, add the data to index
1:       for (int i = 0; i < indexColumns.size(); i++) {
1:         Object data = pages[i].getData(rowId);
1:         addValue2BloomIndex(i, data);
1:       }
1:     }
1:   }
1: 
1:   protected void addValue2BloomIndex(int indexColIdx, Object value) {
1:     byte[] indexValue;
1:     // convert measure to bytes
1:     // convert non-dict dimensions to simple bytes without length
1:     // convert internal-dict dimensions to simple bytes without any encode
1:     if (indexColumns.get(indexColIdx).isMeasure()) {
0:       if (value == null) {
0:         value = DataConvertUtil.getNullValueForMeasure(indexColumns.get(indexColIdx).getDataType());
1:       }
1:       indexValue = CarbonUtil.getValueAsBytes(indexColumns.get(indexColIdx).getDataType(), value);
1:     } else {
1:       if (indexColumns.get(indexColIdx).hasEncoding(Encoding.DICTIONARY)
1:           || indexColumns.get(indexColIdx).hasEncoding(Encoding.DIRECT_DICTIONARY)) {
0:         indexValue = convertDictionaryValue(indexColIdx, (byte[]) value);
1:       } else {
1:         indexValue = convertNonDictionaryValue(indexColIdx, (byte[]) value);
1:       }
1:     }
1:     if (indexValue.length == 0) {
1:       indexValue = CarbonCommonConstants.MEMBER_DEFAULT_VAL_ARRAY;
1:     }
1:     indexBloomFilters.get(indexColIdx).add(new Key(indexValue));
1:   }
1: 
0:   protected byte[] convertDictionaryValue(int indexColIdx, byte[] value) {
0:     byte[] fakeMdkBytes;
0:     // this means that we need to pad some fake bytes
0:     // to get the whole MDK in corresponding position
0:     if (columnarSplitter.getBlockKeySize().length > indexCol2MdkIdx.size()) {
0:       int totalSize = 0;
0:       for (int size : columnarSplitter.getBlockKeySize()) {
0:         totalSize += size;
1:       }
0:       fakeMdkBytes = new byte[totalSize];
1: 
0:       // put this bytes to corresponding position
0:       int thisKeyIdx = indexCol2MdkIdx.get(indexColumns.get(indexColIdx).getColName());
0:       int destPos = 0;
0:       for (int keyIdx = 0; keyIdx < columnarSplitter.getBlockKeySize().length; keyIdx++) {
0:         if (thisKeyIdx == keyIdx) {
0:           System.arraycopy(value, 0,
0:               fakeMdkBytes, destPos, columnarSplitter.getBlockKeySize()[thisKeyIdx]);
0:           break;
1:         }
0:         destPos += columnarSplitter.getBlockKeySize()[keyIdx];
1:       }
1:     } else {
0:       fakeMdkBytes = value;
1:     }
0:     // for dict columns including dictionary and date columns
0:     // decode value to get the surrogate key
0:     int surrogateKey = (int) keyGenerator.getKey(fakeMdkBytes,
0:         indexCol2MdkIdx.get(indexColumns.get(indexColIdx).getColName()));
0:     // store the dictionary key in bloom
0:     return CarbonUtil.getValueAsBytes(DataTypes.INT, surrogateKey);
1:   }
1: 
1:   protected abstract byte[] convertNonDictionaryValue(int indexColIdx, byte[] value);
1: 
1:   private void initDataMapFile() throws IOException {
1:     if (!FileFactory.isFileExist(dataMapPath)) {
1:       if (!FileFactory.mkdirs(dataMapPath, FileFactory.getFileType(dataMapPath))) {
1:         throw new IOException("Failed to create directory " + dataMapPath);
1:       }
1:     }
0:     List<CarbonColumn> indexColumns = getIndexColumns();
1:     for (int indexColId = 0; indexColId < indexColumns.size(); indexColId++) {
0:       String dmFile = BloomCoarseGrainDataMap.getBloomIndexFile(dataMapPath,
1:           indexColumns.get(indexColId).getColName());
1:       DataOutputStream dataOutStream = null;
1:       try {
1:         FileFactory.createNewFile(dmFile, FileFactory.getFileType(dmFile));
1:         dataOutStream = FileFactory.getDataOutputStream(dmFile,
1:             FileFactory.getFileType(dmFile));
1:       } catch (IOException e) {
1:         CarbonUtil.closeStreams(dataOutStream);
1:         throw new IOException(e);
1:       }
1: 
1:       this.currentDMFiles.add(dmFile);
1:       this.currentDataOutStreams.add(dataOutStream);
1:     }
1:   }
1: 
1:   protected void writeBloomDataMapFile() {
0:     List<CarbonColumn> indexColumns = getIndexColumns();
1:     try {
1:       for (int indexColId = 0; indexColId < indexColumns.size(); indexColId++) {
1:         CarbonBloomFilter bloomFilter = indexBloomFilters.get(indexColId);
1:         bloomFilter.setBlockletNo(currentBlockletId);
1:         // only in higher version of guava-bloom-filter, it provides readFrom/writeTo interface.
1:         // In lower version, we use default java serializer to write bloomfilter.
1:         bloomFilter.write(this.currentDataOutStreams.get(indexColId));
1:         this.currentDataOutStreams.get(indexColId).flush();
1:       }
1:     } catch (Exception e) {
1:       for (DataOutputStream dataOutputStream : currentDataOutStreams) {
1:         CarbonUtil.closeStreams(dataOutputStream);
1:       }
1:       throw new RuntimeException(e);
1:     } finally {
1:       resetBloomFilters();
1:     }
1:   }
1: 
1:   @Override
1:   public void finish() throws IOException {
1:     if (!isWritingFinished()) {
1:       releaseResouce();
1:       setWritingFinished(true);
1:     }
1:   }
1: 
1:   protected void releaseResouce() {
0:     List<CarbonColumn> indexColumns = getIndexColumns();
1:     for (int indexColId = 0; indexColId < indexColumns.size(); indexColId++) {
1:       CarbonUtil.closeStreams(
1:           currentDataOutStreams.get(indexColId));
1:     }
1:   }
1: 
1: }
============================================================================