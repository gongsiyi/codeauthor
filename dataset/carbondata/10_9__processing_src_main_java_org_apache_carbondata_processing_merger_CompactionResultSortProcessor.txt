1:cc59b24: /*
1:cc59b24:  * Licensed to the Apache Software Foundation (ASF) under one or more
1:cc59b24:  * contributor license agreements.  See the NOTICE file distributed with
1:cc59b24:  * this work for additional information regarding copyright ownership.
1:cc59b24:  * The ASF licenses this file to You under the Apache License, Version 2.0
1:cc59b24:  * (the "License"); you may not use this file except in compliance with
1:cc59b24:  * the License.  You may obtain a copy of the License at
1:cc59b24:  *
1:cc59b24:  *    http://www.apache.org/licenses/LICENSE-2.0
1:cc59b24:  *
1:cc59b24:  * Unless required by applicable law or agreed to in writing, software
1:cc59b24:  * distributed under the License is distributed on an "AS IS" BASIS,
1:cc59b24:  * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
1:cc59b24:  * See the License for the specific language governing permissions and
1:cc59b24:  * limitations under the License.
3:cc59b24:  */
1:cc59b24: package org.apache.carbondata.processing.merger;
5:cc59b24: 
1:cc59b24: import java.io.File;
1:cc59b24: import java.io.IOException;
1:cc59b24: import java.util.List;
1:cc59b24: 
1:cc59b24: import org.apache.carbondata.common.logging.LogService;
1:cc59b24: import org.apache.carbondata.common.logging.LogServiceFactory;
1:cc59b24: import org.apache.carbondata.core.constants.CarbonCommonConstants;
1:cc59b24: import org.apache.carbondata.core.datastore.block.SegmentProperties;
1:dc83b2a: import org.apache.carbondata.core.datastore.exception.CarbonDataWriterException;
1:dc83b2a: import org.apache.carbondata.core.datastore.row.CarbonRow;
1:8d3c774: import org.apache.carbondata.core.indexstore.PartitionSpec;
1:8d3c774: import org.apache.carbondata.core.metadata.SegmentFileStore;
1:98df130: import org.apache.carbondata.core.metadata.datatype.DataType;
1:956833e: import org.apache.carbondata.core.metadata.datatype.DataTypes;
1:cc59b24: import org.apache.carbondata.core.metadata.encoder.Encoding;
1:cc59b24: import org.apache.carbondata.core.metadata.schema.table.CarbonTable;
1:cc59b24: import org.apache.carbondata.core.metadata.schema.table.column.CarbonDimension;
1:cc59b24: import org.apache.carbondata.core.scan.result.iterator.RawResultIterator;
1:cc59b24: import org.apache.carbondata.core.scan.wrappers.ByteArrayWrapper;
1:cc59b24: import org.apache.carbondata.core.util.CarbonUtil;
1:982d03f: import org.apache.carbondata.core.util.DataTypeUtil;
1:349c59c: import org.apache.carbondata.processing.loading.model.CarbonLoadModel;
1:349c59c: import org.apache.carbondata.processing.sort.exception.CarbonSortKeyAndGroupByException;
1:349c59c: import org.apache.carbondata.processing.sort.sortdata.SingleThreadFinalSortFilesMerger;
1:349c59c: import org.apache.carbondata.processing.sort.sortdata.SortDataRows;
1:349c59c: import org.apache.carbondata.processing.sort.sortdata.SortIntermediateFileMerger;
1:349c59c: import org.apache.carbondata.processing.sort.sortdata.SortParameters;
1:cc59b24: import org.apache.carbondata.processing.store.CarbonFactDataHandlerModel;
1:cc59b24: import org.apache.carbondata.processing.store.CarbonFactHandler;
1:cc59b24: import org.apache.carbondata.processing.store.CarbonFactHandlerFactory;
1:cc59b24: import org.apache.carbondata.processing.util.CarbonDataProcessorUtil;
1:cc59b24: 
3:cc59b24: /**
1:cc59b24:  * This class will process the query result and convert the data
1:cc59b24:  * into a format compatible for data load
1:cc59b24:  */
1:cc59b24: public class CompactionResultSortProcessor extends AbstractResultProcessor {
1:cc59b24: 
1:cc59b24:   /**
1:cc59b24:    * LOGGER
1:cc59b24:    */
1:cc59b24:   private static final LogService LOGGER =
1:cc59b24:       LogServiceFactory.getLogService(CompactionResultSortProcessor.class.getName());
1:cc59b24:   /**
1:cc59b24:    * carbon load model that contains all the required information for load
1:cc59b24:    */
1:cc59b24:   private CarbonLoadModel carbonLoadModel;
1:cc59b24:   /**
1:cc59b24:    * carbon table
1:cc59b24:    */
1:cc59b24:   private CarbonTable carbonTable;
1:cc59b24:   /**
1:cc59b24:    * sortDataRows instance for sorting each row read ad writing to sort temp file
1:cc59b24:    */
1:cc59b24:   private SortDataRows sortDataRows;
1:cc59b24:   /**
1:cc59b24:    * final merger for merge sort
1:cc59b24:    */
1:cc59b24:   private SingleThreadFinalSortFilesMerger finalMerger;
1:cc59b24:   /**
1:cc59b24:    * data handler VO object
1:cc59b24:    */
1:cc59b24:   private CarbonFactHandler dataHandler;
1:cc59b24:   /**
1:cc59b24:    * segment properties for getting dimension cardinality and other required information of a block
1:cc59b24:    */
1:cc59b24:   private SegmentProperties segmentProperties;
1:cc59b24:   /**
1:cc59b24:    * compaction type to decide whether taskID need to be extracted from carbondata files
1:cc59b24:    */
1:cc59b24:   private CompactionType compactionType;
1:cc59b24:   /**
1:cc59b24:    * boolean mapping for no dictionary columns in schema
1:cc59b24:    */
1:cc59b24:   private boolean[] noDictionaryColMapping;
1:cc59b24:   /**
1:dc53dee:    * boolean mapping for long string dimension
1:dc53dee:    */
1:dc53dee:   private boolean[] isVarcharDimMapping;
1:dc53dee:   /**
1:cc59b24:    * agg type defined for measures
1:cc59b24:    */
1:353272e:   private DataType[] dataTypes;
1:cc59b24:   /**
1:cc59b24:    * segment id
1:cc59b24:    */
1:cc59b24:   private String segmentId;
1:cc59b24:   /**
1:cc59b24:    * temp store location to be sued during data load
1:cc59b24:    */
1:ded8b41:   private String[] tempStoreLocation;
1:cc59b24:   /**
1:cc59b24:    * table name
1:cc59b24:    */
1:cc59b24:   private String tableName;
1:cc59b24:   /**
1:cc59b24:    * no dictionary column count in schema
1:cc59b24:    */
1:cc59b24:   private int noDictionaryCount;
1:cc59b24:   /**
1:cc59b24:    * total count of measures in schema
1:cc59b24:    */
1:cc59b24:   private int measureCount;
1:cc59b24:   /**
1:cc59b24:    * dimension count excluding complex dimension and no dictionary column count
1:cc59b24:    */
1:cc59b24:   private int dimensionColumnCount;
1:cc59b24:   /**
1:cc59b24:    * whether the allocated tasks has any record
1:cc59b24:    */
1:cc59b24:   private boolean isRecordFound;
1:89ddf5a:   /**
1:89ddf5a:    * intermediate sort merger
1:89ddf5a:    */
1:89ddf5a:   private SortIntermediateFileMerger intermediateFileMerger;
1:5ed39de: 
1:8d3c774:   private PartitionSpec partitionSpec;
1:8d3c774: 
1:c100251:   private SortParameters sortParameters;
1:5ed39de: 
1:cc59b24:   public CompactionResultSortProcessor(CarbonLoadModel carbonLoadModel, CarbonTable carbonTable,
1:5ed39de:       SegmentProperties segmentProperties, CompactionType compactionType, String tableName,
1:8d3c774:       PartitionSpec partitionSpec) {
1:cc59b24:     this.carbonLoadModel = carbonLoadModel;
1:cc59b24:     this.carbonTable = carbonTable;
1:cc59b24:     this.segmentProperties = segmentProperties;
1:cc59b24:     this.segmentId = carbonLoadModel.getSegmentId();
1:cc59b24:     this.compactionType = compactionType;
1:cc59b24:     this.tableName = tableName;
1:8d3c774:     this.partitionSpec = partitionSpec;
1:cc59b24:   }
1:8d8b589: 
1:cc59b24:   /**
1:cc59b24:    * This method will iterate over the query result and convert it into a format compatible
1:cc59b24:    * for data loading
1:cc59b24:    *
1:cc59b24:    * @param resultIteratorList
1:cc59b24:    */
1:f1d8464:   public boolean execute(List<RawResultIterator> resultIteratorList) throws Exception {
1:cc59b24:     boolean isCompactionSuccess = false;
2:cc59b24:     try {
1:cc59b24:       initTempStoreLocation();
1:cc59b24:       initSortDataRows();
1:4d70a21:       dataTypes = CarbonDataProcessorUtil.initDataType(carbonTable, tableName, measureCount);
1:cc59b24:       processResult(resultIteratorList);
1:cc59b24:       // After delete command, if no records are fetched from one split,
1:cc59b24:       // below steps are not required to be initialized.
1:cc59b24:       if (isRecordFound) {
1:cc59b24:         initializeFinalThreadMergerForMergeSort();
1:cc59b24:         initDataHandler();
1:cc59b24:         readAndLoadDataFromSortTempFiles();
1:cc59b24:       }
1:cc59b24:       isCompactionSuccess = true;
1:cc59b24:     } catch (Exception e) {
1:f1d8464:       throw e;
1:cc59b24:     } finally {
1:8d3c774:       if (partitionSpec != null) {
1:5ed39de:         try {
1:8d3c774:           SegmentFileStore
1:8d3c774:               .writeSegmentFile(carbonLoadModel.getTablePath(), carbonLoadModel.getTaskNo(),
1:8d3c774:                   partitionSpec.getLocation().toString(), carbonLoadModel.getFactTimeStamp() + "",
1:8d3c774:                   partitionSpec.getPartitions());
1:5ed39de:         } catch (IOException e) {
1:f1d8464:           throw e;
1:5ed39de:         }
1:5ed39de:       }
1:cc59b24:       // clear temp files and folders created during compaction
1:cc59b24:       deleteTempStoreLocation();
1:cc59b24:     }
1:cc59b24:     return isCompactionSuccess;
1:cc59b24:   }
1:ded8b41: 
1:7978b97:   @Override
1:7978b97:   public void close() {
1:7978b97:     // close the sorter executor service
1:7978b97:     if (null != sortDataRows) {
1:7978b97:       sortDataRows.close();
1:7978b97:     }
1:7978b97:     // close the final merger
1:7978b97:     if (null != finalMerger) {
1:7978b97:       finalMerger.close();
1:7978b97:     }
1:7978b97:     // close data handler
1:7978b97:     if (null != dataHandler) {
1:7978b97:       dataHandler.closeHandler();
1:7978b97:     }
1:7978b97:   }
1:7978b97: 
1:cc59b24:   /**
1:cc59b24:    * This method will clean up the local folders and files created during compaction process
1:cc59b24:    */
1:cc59b24:   private void deleteTempStoreLocation() {
1:cc59b24:     if (null != tempStoreLocation) {
1:ded8b41:       for (String tempLoc : tempStoreLocation) {
1:ded8b41:         try {
1:ded8b41:           CarbonUtil.deleteFoldersAndFiles(new File(tempLoc));
1:ded8b41:         } catch (IOException | InterruptedException e) {
1:ded8b41:           LOGGER.error("Problem deleting local folders during compaction: " + e.getMessage());
1:ded8b41:         }
1:cc59b24:       }
1:cc59b24:     }
1:cc59b24:   }
1:cc59b24: 
1:cc59b24:   /**
1:cc59b24:    * This method will iterate over the query result and perform row sorting operation
1:cc59b24:    *
1:cc59b24:    * @param resultIteratorList
1:cc59b24:    */
1:8ed7931:   private void processResult(List<RawResultIterator> resultIteratorList) throws Exception {
1:cc59b24:     for (RawResultIterator resultIterator : resultIteratorList) {
1:f8e0585:       if (CompactionType.STREAMING == compactionType) {
1:cc59b24:         while (resultIterator.hasNext()) {
1:f8e0585:           // the input iterator of streaming segment is already using raw row
1:f8e0585:           addRowForSorting(resultIterator.next());
2:f8e0585:           isRecordFound = true;
2:f8e0585:         }
1:f8e0585:       } else {
2:f8e0585:         while (resultIterator.hasNext()) {
1:cc59b24:           addRowForSorting(prepareRowObjectForSorting(resultIterator.next()));
1:cc59b24:           isRecordFound = true;
1:cc59b24:         }
1:cc59b24:       }
1:e26cccc:       resultIterator.close();
1:cc59b24:     }
1:cc59b24:     try {
1:cc59b24:       sortDataRows.startSorting();
1:cc59b24:     } catch (CarbonSortKeyAndGroupByException e) {
2:cc59b24:       LOGGER.error(e);
4:cc59b24:       throw new Exception("Problem loading data during compaction: " + e.getMessage());
1:cc59b24:     }
1:cc59b24:   }
1:cc59b24: 
1:cc59b24:   /**
1:cc59b24:    * This method will prepare the data from raw object that will take part in sorting
1:cc59b24:    *
1:cc59b24:    * @param row
1:cc59b24:    * @return
1:cc59b24:    */
1:cc59b24:   private Object[] prepareRowObjectForSorting(Object[] row) {
1:cc59b24:     ByteArrayWrapper wrapper = (ByteArrayWrapper) row[0];
1:cc59b24:     // ByteBuffer[] noDictionaryBuffer = new ByteBuffer[noDictionaryCount];
1:cc59b24:     List<CarbonDimension> dimensions = segmentProperties.getDimensions();
1:cc59b24:     Object[] preparedRow = new Object[dimensions.size() + measureCount];
1:cc59b24:     // convert the dictionary from MDKey to surrogate key
1:cc59b24:     byte[] dictionaryKey = wrapper.getDictionaryKey();
1:cc59b24:     long[] keyArray = segmentProperties.getDimensionKeyGenerator().getKeyArray(dictionaryKey);
1:cc59b24:     Object[] dictionaryValues = new Object[dimensionColumnCount + measureCount];
1:cc59b24:     for (int i = 0; i < keyArray.length; i++) {
1:cc59b24:       dictionaryValues[i] = Long.valueOf(keyArray[i]).intValue();
1:956833e:     }
1:cc59b24:     int noDictionaryIndex = 0;
1:cc59b24:     int dictionaryIndex = 0;
1:cc59b24:     for (int i = 0; i < dimensions.size(); i++) {
1:cc59b24:       CarbonDimension dims = dimensions.get(i);
1:cc59b24:       if (dims.hasEncoding(Encoding.DICTIONARY)) {
1:cc59b24:         // dictionary
1:cc59b24:         preparedRow[i] = dictionaryValues[dictionaryIndex++];
1:cc59b24:       } else {
1:cc59b24:         // no dictionary dims
1:cc59b24:         preparedRow[i] = wrapper.getNoDictionaryKeyByIndex(noDictionaryIndex++);
1:cc59b24:       }
1:cc59b24:     }
1:cc59b24:     // fill all the measures
1:cc59b24:     // measures will always start from 1st index in the row object array
1:cc59b24:     int measureIndexInRow = 1;
1:cc59b24:     for (int i = 0; i < measureCount; i++) {
1:cc59b24:       preparedRow[dimensionColumnCount + i] =
1:353272e:           getConvertedMeasureValue(row[measureIndexInRow++], dataTypes[i]);
1:cc59b24:     }
1:cc59b24:     return preparedRow;
1:cc59b24:   }
1:cc59b24: 
1:cc59b24:   /**
1:cc59b24:    * This method will convert the spark decimal to java big decimal type
1:cc59b24:    *
1:cc59b24:    * @param value
1:98df130:    * @param type
1:cc59b24:    * @return
1:cc59b24:    */
1:98df130:   private Object getConvertedMeasureValue(Object value, DataType type) {
1:f209e8e:     if (DataTypes.isDecimal(type)) {
1:8ed7931:       if (value != null) {
1:982d03f:         value = DataTypeUtil.getDataTypeConverter().convertFromDecimalToBigDecimal(value);
1:8ed7931:       }
1:956833e:       return value;
1:956833e:     } else {
1:956833e:       return value;
1:cc59b24:     }
1:cc59b24:   }
1:cc59b24: 
1:cc59b24:   /**
1:cc59b24:    * This method will read sort temp files, perform merge sort and add it to store for data loading
1:cc59b24:    */
1:cc59b24:   private void readAndLoadDataFromSortTempFiles() throws Exception {
1:cc59b24:     try {
1:89ddf5a:       intermediateFileMerger.finish();
1:cc59b24:       finalMerger.startFinalMerge();
1:cc59b24:       while (finalMerger.hasNext()) {
1:353272e:         Object[] row = finalMerger.next();
1:353272e:         dataHandler.addDataToStore(new CarbonRow(row));
1:cc59b24:       }
1:cc59b24:       dataHandler.finish();
1:cc59b24:     } catch (CarbonDataWriterException e) {
1:cc59b24:       LOGGER.error(e);
1:910d496:       throw new Exception("Problem loading data during compaction.", e);
1:cc59b24:     } catch (Exception e) {
1:cc59b24:       LOGGER.error(e);
1:910d496:       throw new Exception("Problem loading data during compaction.", e);
1:cc59b24:     } finally {
1:cc59b24:       if (null != dataHandler) {
1:cc59b24:         try {
1:cc59b24:           dataHandler.closeHandler();
1:cc59b24:         } catch (CarbonDataWriterException e) {
1:910d496:           LOGGER.error(e, "Error in close data handler");
1:910d496:           throw new Exception("Error in close data handler", e);
1:cc59b24:         }
1:cc59b24:       }
1:cc59b24:     }
1:cc59b24:   }
1:cc59b24: 
1:cc59b24:   /**
1:cc59b24:    * add row to a temp array which will we written to a sort temp file after sorting
1:cc59b24:    *
1:cc59b24:    * @param row
1:cc59b24:    */
1:cc59b24:   private void addRowForSorting(Object[] row) throws Exception {
1:cc59b24:     try {
1:cc59b24:       sortDataRows.addRow(row);
1:cc59b24:     } catch (CarbonSortKeyAndGroupByException e) {
1:cc59b24:       LOGGER.error(e);
1:cc59b24:       throw new Exception("Row addition for sorting failed during compaction: " + e.getMessage());
1:cc59b24:     }
1:cc59b24:   }
1:cc59b24: 
1:cc59b24:   /**
1:cc59b24:    * create an instance of sort data rows
1:cc59b24:    */
1:cc59b24:   private void initSortDataRows() throws Exception {
1:cc59b24:     measureCount = carbonTable.getMeasureByTableName(tableName).size();
1:cc59b24:     List<CarbonDimension> dimensions = carbonTable.getDimensionByTableName(tableName);
1:cc59b24:     noDictionaryColMapping = new boolean[dimensions.size()];
1:dc53dee:     isVarcharDimMapping = new boolean[dimensions.size()];
1:cc59b24:     int i = 0;
1:cc59b24:     for (CarbonDimension dimension : dimensions) {
1:cc59b24:       if (CarbonUtil.hasEncoding(dimension.getEncoder(), Encoding.DICTIONARY)) {
1:cc59b24:         i++;
1:cc59b24:         continue;
1:cc59b24:       }
1:5195d7f:       noDictionaryColMapping[i] = true;
1:dc53dee:       if (dimension.getColumnSchema().getDataType() == DataTypes.VARCHAR) {
1:5195d7f:         isVarcharDimMapping[i] = true;
1:dc53dee:       }
1:5195d7f:       i++;
1:cc59b24:       noDictionaryCount++;
1:cc59b24:     }
1:cc59b24:     dimensionColumnCount = dimensions.size();
1:c100251:     sortParameters = createSortParameters();
1:c100251:     intermediateFileMerger = new SortIntermediateFileMerger(sortParameters);
1:cc59b24:     // TODO: Now it is only supported onheap merge, but we can have unsafe merge
1:cc59b24:     // as well by using UnsafeSortDataRows.
1:c100251:     this.sortDataRows = new SortDataRows(sortParameters, intermediateFileMerger);
1:cc59b24:     try {
1:cc59b24:       this.sortDataRows.initialize();
1:cc59b24:     } catch (CarbonSortKeyAndGroupByException e) {
1:cc59b24:       LOGGER.error(e);
1:cc59b24:       throw new Exception(
1:cc59b24:           "Error initializing sort data rows object during compaction: " + e.getMessage());
1:cc59b24:     }
1:cc59b24:   }
1:cc59b24: 
1:cc59b24:   /**
1:cc59b24:    * This method will create the sort parameters VO object
1:cc59b24:    *
1:cc59b24:    * @return
1:cc59b24:    */
1:cc59b24:   private SortParameters createSortParameters() {
1:81149f6:     return SortParameters
1:89ddf5a:         .createSortParameters(carbonTable, carbonLoadModel.getDatabaseName(), tableName,
1:89ddf5a:             dimensionColumnCount, segmentProperties.getComplexDimensions().size(), measureCount,
1:5bedd77:             noDictionaryCount, segmentId,
1:dc53dee:             carbonLoadModel.getTaskNo(), noDictionaryColMapping, isVarcharDimMapping, true);
1:cc59b24:   }
1:cc59b24: 
1:cc59b24:   /**
1:cc59b24:    * create an instance of finalThread merger which will perform merge sort on all the
1:cc59b24:    * sort temp files
1:cc59b24:    */
1:cc59b24:   private void initializeFinalThreadMergerForMergeSort() {
1:9f94529:     boolean[] noDictionarySortColumnMapping = null;
1:9f94529:     if (noDictionaryColMapping.length == this.segmentProperties.getNumberOfSortColumns()) {
1:9f94529:       noDictionarySortColumnMapping = noDictionaryColMapping;
1:9f94529:     } else {
1:9f94529:       noDictionarySortColumnMapping = new boolean[this.segmentProperties.getNumberOfSortColumns()];
1:9f94529:       System.arraycopy(noDictionaryColMapping, 0,
1:9f94529:           noDictionarySortColumnMapping, 0, noDictionarySortColumnMapping.length);
1:cc59b24:     }
1:c100251:     sortParameters.setNoDictionarySortColumn(noDictionarySortColumnMapping);
1:ded8b41:     String[] sortTempFileLocation = CarbonDataProcessorUtil.arrayAppend(tempStoreLocation,
1:ded8b41:         CarbonCommonConstants.FILE_SEPARATOR, CarbonCommonConstants.SORT_TEMP_FILE_LOCATION);
1:cc59b24:     finalMerger =
1:c100251:         new SingleThreadFinalSortFilesMerger(sortTempFileLocation, tableName, sortParameters);
1:cc59b24:   }
1:cc59b24: 
1:cc59b24:   /**
1:cc59b24:    * initialise carbon data writer instance
1:cc59b24:    */
1:cc59b24:   private void initDataHandler() throws Exception {
1:8d3c774:     String carbonStoreLocation;
1:8d3c774:     if (partitionSpec != null) {
1:8d3c774:       carbonStoreLocation =
1:8d3c774:           partitionSpec.getLocation().toString() + CarbonCommonConstants.FILE_SEPARATOR
1:8d3c774:               + carbonLoadModel.getFactTimeStamp() + ".tmp";
1:8d3c774:     } else {
1:8d3c774:       carbonStoreLocation = CarbonDataProcessorUtil
1:8d8b589:           .createCarbonStoreLocation(carbonLoadModel.getDatabaseName(), tableName,
1:8d8b589:               carbonLoadModel.getSegmentId());
1:8d3c774:     }
1:cc59b24:     CarbonFactDataHandlerModel carbonFactDataHandlerModel = CarbonFactDataHandlerModel
1:cc59b24:         .getCarbonFactDataHandlerModel(carbonLoadModel, carbonTable, segmentProperties, tableName,
1:8d3c774:             tempStoreLocation, carbonStoreLocation);
1:60dfdd3:     carbonFactDataHandlerModel.setSegmentId(carbonLoadModel.getSegmentId());
1:bf6c471:     setDataFileAttributesInModel(carbonLoadModel, compactionType, carbonFactDataHandlerModel);
1:f911403:     dataHandler = CarbonFactHandlerFactory.createCarbonFactHandler(carbonFactDataHandlerModel);
1:cc59b24:     try {
1:cc59b24:       dataHandler.initialise();
1:cc59b24:     } catch (CarbonDataWriterException e) {
1:cc59b24:       LOGGER.error(e);
1:cc59b24:       throw new Exception("Problem initialising data handler during compaction: " + e.getMessage());
1:cc59b24:     }
1:cc59b24:   }
1:cc59b24: 
1:cc59b24:   /**
1:cc59b24:    * initialise temporary store location
1:cc59b24:    */
1:cc59b24:   private void initTempStoreLocation() {
1:cc59b24:     tempStoreLocation = CarbonDataProcessorUtil
1:5bedd77:         .getLocalDataFolderLocation(carbonTable, carbonLoadModel.getTaskNo(),
1:5bedd77:            segmentId, true, false);
1:cc59b24:   }
1:cc59b24: }
============================================================================
author:sraghunandan
-------------------------------------------------------------------------------
commit:f911403
/////////////////////////////////////////////////////////////////////////
1:     dataHandler = CarbonFactHandlerFactory.createCarbonFactHandler(carbonFactDataHandlerModel);
author:akashrn5
-------------------------------------------------------------------------------
commit:5195d7f
/////////////////////////////////////////////////////////////////////////
1:       noDictionaryColMapping[i] = true;
1:         isVarcharDimMapping[i] = true;
1:       i++;
author:xuchuanyin
-------------------------------------------------------------------------------
commit:dc53dee
/////////////////////////////////////////////////////////////////////////
1:    * boolean mapping for long string dimension
1:    */
1:   private boolean[] isVarcharDimMapping;
1:   /**
/////////////////////////////////////////////////////////////////////////
1:     isVarcharDimMapping = new boolean[dimensions.size()];
0:     int j = 0;
1:       if (dimension.getColumnSchema().getDataType() == DataTypes.VARCHAR) {
0:         isVarcharDimMapping[j++] = true;
1:       }
/////////////////////////////////////////////////////////////////////////
1:             carbonLoadModel.getTaskNo(), noDictionaryColMapping, isVarcharDimMapping, true);
commit:e26cccc
/////////////////////////////////////////////////////////////////////////
1:       resultIterator.close();
commit:2b41f14
/////////////////////////////////////////////////////////////////////////
commit:8d8b589
/////////////////////////////////////////////////////////////////////////
1: 
/////////////////////////////////////////////////////////////////////////
1:           .createCarbonStoreLocation(carbonLoadModel.getDatabaseName(), tableName,
1:               carbonLoadModel.getSegmentId());
commit:21704cf
/////////////////////////////////////////////////////////////////////////
commit:910d496
/////////////////////////////////////////////////////////////////////////
1:       throw new Exception("Problem loading data during compaction.", e);
1:       throw new Exception("Problem loading data during compaction.", e);
1:           LOGGER.error(e, "Error in close data handler");
1:           throw new Exception("Error in close data handler", e);
commit:c100251
/////////////////////////////////////////////////////////////////////////
1:   private SortParameters sortParameters;
/////////////////////////////////////////////////////////////////////////
1:     sortParameters = createSortParameters();
1:     intermediateFileMerger = new SortIntermediateFileMerger(sortParameters);
1:     this.sortDataRows = new SortDataRows(sortParameters, intermediateFileMerger);
/////////////////////////////////////////////////////////////////////////
1:     sortParameters.setNoDictionarySortColumn(noDictionarySortColumnMapping);
1:         new SingleThreadFinalSortFilesMerger(sortTempFileLocation, tableName, sortParameters);
commit:ded8b41
/////////////////////////////////////////////////////////////////////////
1:   private String[] tempStoreLocation;
/////////////////////////////////////////////////////////////////////////
1:       for (String tempLoc : tempStoreLocation) {
1:         try {
1:           CarbonUtil.deleteFoldersAndFiles(new File(tempLoc));
1:         } catch (IOException | InterruptedException e) {
1:           LOGGER.error("Problem deleting local folders during compaction: " + e.getMessage());
1:         }
/////////////////////////////////////////////////////////////////////////
/////////////////////////////////////////////////////////////////////////
1: 
1:     String[] sortTempFileLocation = CarbonDataProcessorUtil.arrayAppend(tempStoreLocation,
1:         CarbonCommonConstants.FILE_SEPARATOR, CarbonCommonConstants.SORT_TEMP_FILE_LOCATION);
author:ravipesala
-------------------------------------------------------------------------------
commit:60dfdd3
/////////////////////////////////////////////////////////////////////////
/////////////////////////////////////////////////////////////////////////
1:     carbonFactDataHandlerModel.setSegmentId(carbonLoadModel.getSegmentId());
commit:8d3c774
/////////////////////////////////////////////////////////////////////////
1: import org.apache.carbondata.core.indexstore.PartitionSpec;
1: import org.apache.carbondata.core.metadata.SegmentFileStore;
/////////////////////////////////////////////////////////////////////////
/////////////////////////////////////////////////////////////////////////
1:   private PartitionSpec partitionSpec;
1: 
1:       PartitionSpec partitionSpec) {
1:     this.partitionSpec = partitionSpec;
/////////////////////////////////////////////////////////////////////////
1:       if (partitionSpec != null) {
1:           SegmentFileStore
1:               .writeSegmentFile(carbonLoadModel.getTablePath(), carbonLoadModel.getTaskNo(),
1:                   partitionSpec.getLocation().toString(), carbonLoadModel.getFactTimeStamp() + "",
1:                   partitionSpec.getPartitions());
/////////////////////////////////////////////////////////////////////////
1:     String carbonStoreLocation;
1:     if (partitionSpec != null) {
1:       carbonStoreLocation =
1:           partitionSpec.getLocation().toString() + CarbonCommonConstants.FILE_SEPARATOR
1:               + carbonLoadModel.getFactTimeStamp() + ".tmp";
1:     } else {
1:       carbonStoreLocation = CarbonDataProcessorUtil
0:           .createCarbonStoreLocation(carbonTable.getTablePath(), carbonLoadModel.getDatabaseName(),
0:               tableName, carbonLoadModel.getPartitionId(), carbonLoadModel.getSegmentId());
1:     }
1:             tempStoreLocation, carbonStoreLocation);
commit:5ed39de
/////////////////////////////////////////////////////////////////////////
0: import org.apache.carbondata.core.metadata.PartitionMapFileStore;
/////////////////////////////////////////////////////////////////////////
0: import org.apache.carbondata.core.util.path.CarbonTablePath;
/////////////////////////////////////////////////////////////////////////
0:   private List<String> partitionNames;
1: 
1: 
1:       SegmentProperties segmentProperties, CompactionType compactionType, String tableName,
0:       List<String> partitionNames) {
0:     this.partitionNames = partitionNames;
/////////////////////////////////////////////////////////////////////////
0:       if (partitionNames != null) {
1:         try {
0:           new PartitionMapFileStore().writePartitionMapFile(
0:               CarbonTablePath.getSegmentPath(
0:                   carbonLoadModel.getTablePath(),
0:                   carbonLoadModel.getSegmentId()),
0:               carbonLoadModel.getTaskNo(),
0:               partitionNames);
1:         } catch (IOException e) {
0:           LOGGER.error(e, "Compaction failed: " + e.getMessage());
0:           isCompactionSuccess = false;
1:         }
1:       }
author:rahulforallp
-------------------------------------------------------------------------------
commit:f1d8464
/////////////////////////////////////////////////////////////////////////
1:   public boolean execute(List<RawResultIterator> resultIteratorList) throws Exception {
/////////////////////////////////////////////////////////////////////////
1:       throw e;
/////////////////////////////////////////////////////////////////////////
1:           throw e;
commit:65471f2
/////////////////////////////////////////////////////////////////////////
0:         .getLocalDataFolderLocation(carbonTable, tableName, carbonLoadModel.getTaskNo(),
0:             carbonLoadModel.getPartitionId(), segmentId, true, false);
commit:8ed7931
/////////////////////////////////////////////////////////////////////////
1:   private void processResult(List<RawResultIterator> resultIteratorList) throws Exception {
/////////////////////////////////////////////////////////////////////////
1:         if (value != null) {
0:           value = ((org.apache.spark.sql.types.Decimal) value).toJavaBigDecimal();
1:         }
author:Jacky Li
-------------------------------------------------------------------------------
commit:982d03f
/////////////////////////////////////////////////////////////////////////
1: import org.apache.carbondata.core.util.DataTypeUtil;
/////////////////////////////////////////////////////////////////////////
/////////////////////////////////////////////////////////////////////////
1:         value = DataTypeUtil.getDataTypeConverter().convertFromDecimalToBigDecimal(value);
commit:bf6c471
/////////////////////////////////////////////////////////////////////////
1:     setDataFileAttributesInModel(carbonLoadModel, compactionType, carbonFactDataHandlerModel);
commit:5bedd77
/////////////////////////////////////////////////////////////////////////
1:             noDictionaryCount, segmentId,
/////////////////////////////////////////////////////////////////////////
0:               tableName, carbonLoadModel.getSegmentId());
/////////////////////////////////////////////////////////////////////////
1:         .getLocalDataFolderLocation(carbonTable, carbonLoadModel.getTaskNo(),
1:            segmentId, true, false);
commit:f209e8e
/////////////////////////////////////////////////////////////////////////
1:     if (DataTypes.isDecimal(type)) {
commit:4d70a21
/////////////////////////////////////////////////////////////////////////
1:       dataTypes = CarbonDataProcessorUtil.initDataType(carbonTable, tableName, measureCount);
/////////////////////////////////////////////////////////////////////////
commit:956833e
/////////////////////////////////////////////////////////////////////////
1: import org.apache.carbondata.core.metadata.datatype.DataTypes;
/////////////////////////////////////////////////////////////////////////
0:     if (type == DataTypes.DECIMAL) {
0:       if (value != null) {
0:         value = ((Decimal) value).toJavaBigDecimal();
1:       }
1:       return value;
1:     } else {
1:       return value;
commit:349c59c
/////////////////////////////////////////////////////////////////////////
1: import org.apache.carbondata.processing.loading.model.CarbonLoadModel;
1: import org.apache.carbondata.processing.sort.exception.CarbonSortKeyAndGroupByException;
1: import org.apache.carbondata.processing.sort.sortdata.SingleThreadFinalSortFilesMerger;
1: import org.apache.carbondata.processing.sort.sortdata.SortDataRows;
1: import org.apache.carbondata.processing.sort.sortdata.SortIntermediateFileMerger;
1: import org.apache.carbondata.processing.sort.sortdata.SortParameters;
author:manishgupta88
-------------------------------------------------------------------------------
commit:7978b97
/////////////////////////////////////////////////////////////////////////
1:   @Override
1:   public void close() {
1:     // close the sorter executor service
1:     if (null != sortDataRows) {
1:       sortDataRows.close();
1:     }
1:     // close the final merger
1:     if (null != finalMerger) {
1:       finalMerger.close();
1:     }
1:     // close data handler
1:     if (null != dataHandler) {
1:       dataHandler.closeHandler();
1:     }
1:   }
1: 
commit:89ddf5a
/////////////////////////////////////////////////////////////////////////
1:   /**
1:    * intermediate sort merger
1:    */
1:   private SortIntermediateFileMerger intermediateFileMerger;
/////////////////////////////////////////////////////////////////////////
1:       intermediateFileMerger.finish();
/////////////////////////////////////////////////////////////////////////
0:     intermediateFileMerger = new SortIntermediateFileMerger(parameters);
/////////////////////////////////////////////////////////////////////////
1:         .createSortParameters(carbonTable, carbonLoadModel.getDatabaseName(), tableName,
1:             dimensionColumnCount, segmentProperties.getComplexDimensions().size(), measureCount,
0:             noDictionaryCount, carbonLoadModel.getPartitionId(), segmentId,
0:             carbonLoadModel.getTaskNo(), noDictionaryColMapping, true);
commit:f890d00
/////////////////////////////////////////////////////////////////////////
0:             noDictionaryColMapping, true);
/////////////////////////////////////////////////////////////////////////
0:             carbonLoadModel.getTaskNo(), carbonLoadModel.getPartitionId(), segmentId, true);
commit:cc59b24
/////////////////////////////////////////////////////////////////////////
1: /*
1:  * Licensed to the Apache Software Foundation (ASF) under one or more
1:  * contributor license agreements.  See the NOTICE file distributed with
1:  * this work for additional information regarding copyright ownership.
1:  * The ASF licenses this file to You under the Apache License, Version 2.0
1:  * (the "License"); you may not use this file except in compliance with
1:  * the License.  You may obtain a copy of the License at
1:  *
1:  *    http://www.apache.org/licenses/LICENSE-2.0
1:  *
1:  * Unless required by applicable law or agreed to in writing, software
1:  * distributed under the License is distributed on an "AS IS" BASIS,
1:  * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
1:  * See the License for the specific language governing permissions and
1:  * limitations under the License.
1:  */
1: package org.apache.carbondata.processing.merger;
1: 
1: import java.io.File;
1: import java.io.IOException;
1: import java.util.List;
1: 
1: import org.apache.carbondata.common.logging.LogService;
1: import org.apache.carbondata.common.logging.LogServiceFactory;
1: import org.apache.carbondata.core.constants.CarbonCommonConstants;
1: import org.apache.carbondata.core.datastore.block.SegmentProperties;
1: import org.apache.carbondata.core.metadata.encoder.Encoding;
1: import org.apache.carbondata.core.metadata.schema.table.CarbonTable;
1: import org.apache.carbondata.core.metadata.schema.table.column.CarbonDimension;
1: import org.apache.carbondata.core.scan.result.iterator.RawResultIterator;
1: import org.apache.carbondata.core.scan.wrappers.ByteArrayWrapper;
1: import org.apache.carbondata.core.util.CarbonUtil;
0: import org.apache.carbondata.processing.model.CarbonLoadModel;
0: import org.apache.carbondata.processing.newflow.row.CarbonRow;
0: import org.apache.carbondata.processing.sortandgroupby.exception.CarbonSortKeyAndGroupByException;
0: import org.apache.carbondata.processing.sortandgroupby.sortdata.SortDataRows;
0: import org.apache.carbondata.processing.sortandgroupby.sortdata.SortIntermediateFileMerger;
0: import org.apache.carbondata.processing.sortandgroupby.sortdata.SortParameters;
1: import org.apache.carbondata.processing.store.CarbonFactDataHandlerModel;
1: import org.apache.carbondata.processing.store.CarbonFactHandler;
1: import org.apache.carbondata.processing.store.CarbonFactHandlerFactory;
0: import org.apache.carbondata.processing.store.SingleThreadFinalSortFilesMerger;
0: import org.apache.carbondata.processing.store.writer.exception.CarbonDataWriterException;
1: import org.apache.carbondata.processing.util.CarbonDataProcessorUtil;
1: 
1: /**
1:  * This class will process the query result and convert the data
1:  * into a format compatible for data load
1:  */
1: public class CompactionResultSortProcessor extends AbstractResultProcessor {
1: 
1:   /**
1:    * LOGGER
1:    */
1:   private static final LogService LOGGER =
1:       LogServiceFactory.getLogService(CompactionResultSortProcessor.class.getName());
1:   /**
1:    * carbon load model that contains all the required information for load
1:    */
1:   private CarbonLoadModel carbonLoadModel;
1:   /**
1:    * carbon table
1:    */
1:   private CarbonTable carbonTable;
1:   /**
1:    * sortDataRows instance for sorting each row read ad writing to sort temp file
1:    */
1:   private SortDataRows sortDataRows;
1:   /**
1:    * final merger for merge sort
1:    */
1:   private SingleThreadFinalSortFilesMerger finalMerger;
1:   /**
1:    * data handler VO object
1:    */
1:   private CarbonFactHandler dataHandler;
1:   /**
1:    * segment properties for getting dimension cardinality and other required information of a block
1:    */
1:   private SegmentProperties segmentProperties;
1:   /**
1:    * compaction type to decide whether taskID need to be extracted from carbondata files
1:    */
1:   private CompactionType compactionType;
1:   /**
1:    * boolean mapping for no dictionary columns in schema
1:    */
1:   private boolean[] noDictionaryColMapping;
1:   /**
1:    * agg type defined for measures
1:    */
0:   private char[] aggType;
1:   /**
1:    * segment id
1:    */
1:   private String segmentId;
1:   /**
1:    * temp store location to be sued during data load
1:    */
0:   private String tempStoreLocation;
1:   /**
1:    * table name
1:    */
1:   private String tableName;
1:   /**
1:    * no dictionary column count in schema
1:    */
1:   private int noDictionaryCount;
1:   /**
1:    * total count of measures in schema
1:    */
1:   private int measureCount;
1:   /**
1:    * dimension count excluding complex dimension and no dictionary column count
1:    */
1:   private int dimensionColumnCount;
1:   /**
1:    * whether the allocated tasks has any record
1:    */
1:   private boolean isRecordFound;
1: 
1:   /**
0:    * @param carbonLoadModel
0:    * @param carbonTable
0:    * @param segmentProperties
0:    * @param compactionType
0:    * @param tableName
1:    */
1:   public CompactionResultSortProcessor(CarbonLoadModel carbonLoadModel, CarbonTable carbonTable,
0:       SegmentProperties segmentProperties, CompactionType compactionType, String tableName) {
1:     this.carbonLoadModel = carbonLoadModel;
1:     this.carbonTable = carbonTable;
1:     this.segmentProperties = segmentProperties;
1:     this.segmentId = carbonLoadModel.getSegmentId();
1:     this.compactionType = compactionType;
1:     this.tableName = tableName;
1:   }
1: 
1:   /**
1:    * This method will iterate over the query result and convert it into a format compatible
1:    * for data loading
1:    *
1:    * @param resultIteratorList
1:    */
0:   public boolean execute(List<RawResultIterator> resultIteratorList) {
1:     boolean isCompactionSuccess = false;
1:     try {
1:       initTempStoreLocation();
1:       initSortDataRows();
0:       initAggType();
1:       processResult(resultIteratorList);
1:       // After delete command, if no records are fetched from one split,
1:       // below steps are not required to be initialized.
1:       if (isRecordFound) {
1:         initializeFinalThreadMergerForMergeSort();
1:         initDataHandler();
1:         readAndLoadDataFromSortTempFiles();
1:       }
1:       isCompactionSuccess = true;
1:     } catch (Exception e) {
0:       LOGGER.error(e, "Compaction failed: " + e.getMessage());
1:     } finally {
1:       // clear temp files and folders created during compaction
1:       deleteTempStoreLocation();
1:     }
1:     return isCompactionSuccess;
1:   }
1: 
1:   /**
1:    * This method will clean up the local folders and files created during compaction process
1:    */
1:   private void deleteTempStoreLocation() {
1:     if (null != tempStoreLocation) {
1:       try {
0:         CarbonUtil.deleteFoldersAndFiles(new File[] { new File(tempStoreLocation) });
0:       } catch (IOException | InterruptedException e) {
0:         LOGGER.error("Problem deleting local folders during compaction: " + e.getMessage());
1:       }
1:     }
1:   }
1: 
1:   /**
1:    * This method will iterate over the query result and perform row sorting operation
1:    *
1:    * @param resultIteratorList
1:    */
0:   private void processResult(List<RawResultIterator> resultIteratorList)
0:       throws Exception {
1:     for (RawResultIterator resultIterator : resultIteratorList) {
1:       while (resultIterator.hasNext()) {
1:         addRowForSorting(prepareRowObjectForSorting(resultIterator.next()));
1:         isRecordFound = true;
1:       }
1:     }
1:     try {
1:       sortDataRows.startSorting();
1:     } catch (CarbonSortKeyAndGroupByException e) {
1:       LOGGER.error(e);
1:       throw new Exception("Problem loading data during compaction: " + e.getMessage());
1:     }
1:   }
1: 
1:   /**
1:    * This method will prepare the data from raw object that will take part in sorting
1:    *
1:    * @param row
1:    * @return
1:    */
1:   private Object[] prepareRowObjectForSorting(Object[] row) {
1:     ByteArrayWrapper wrapper = (ByteArrayWrapper) row[0];
1:     // ByteBuffer[] noDictionaryBuffer = new ByteBuffer[noDictionaryCount];
1:     List<CarbonDimension> dimensions = segmentProperties.getDimensions();
1:     Object[] preparedRow = new Object[dimensions.size() + measureCount];
1:     // convert the dictionary from MDKey to surrogate key
1:     byte[] dictionaryKey = wrapper.getDictionaryKey();
1:     long[] keyArray = segmentProperties.getDimensionKeyGenerator().getKeyArray(dictionaryKey);
1:     Object[] dictionaryValues = new Object[dimensionColumnCount + measureCount];
1:     for (int i = 0; i < keyArray.length; i++) {
1:       dictionaryValues[i] = Long.valueOf(keyArray[i]).intValue();
1:     }
1:     int noDictionaryIndex = 0;
1:     int dictionaryIndex = 0;
1:     for (int i = 0; i < dimensions.size(); i++) {
1:       CarbonDimension dims = dimensions.get(i);
1:       if (dims.hasEncoding(Encoding.DICTIONARY)) {
1:         // dictionary
1:         preparedRow[i] = dictionaryValues[dictionaryIndex++];
1:       } else {
1:         // no dictionary dims
1:         preparedRow[i] = wrapper.getNoDictionaryKeyByIndex(noDictionaryIndex++);
1:       }
1:     }
1:     // fill all the measures
1:     // measures will always start from 1st index in the row object array
1:     int measureIndexInRow = 1;
1:     for (int i = 0; i < measureCount; i++) {
1:       preparedRow[dimensionColumnCount + i] =
0:           getConvertedMeasureValue(row[measureIndexInRow++], aggType[i]);
1:     }
1:     return preparedRow;
1:   }
1: 
1:   /**
1:    * This method will convert the spark decimal to java big decimal type
1:    *
1:    * @param value
0:    * @param aggType
1:    * @return
1:    */
0:   private Object getConvertedMeasureValue(Object value, char aggType) {
0:     switch (aggType) {
0:       case CarbonCommonConstants.BIG_DECIMAL_MEASURE:
0:         value = ((org.apache.spark.sql.types.Decimal) value).toJavaBigDecimal();
0:         return value;
0:       default:
0:         return value;
1:     }
1:   }
1: 
1:   /**
1:    * This method will read sort temp files, perform merge sort and add it to store for data loading
1:    */
1:   private void readAndLoadDataFromSortTempFiles() throws Exception {
1:     try {
1:       finalMerger.startFinalMerge();
1:       while (finalMerger.hasNext()) {
0:         Object[] rowRead = finalMerger.next();
0:         CarbonRow row = new CarbonRow(rowRead);
0:         // convert the row from surrogate key to MDKey
0:         Object[] outputRow = CarbonDataProcessorUtil
0:             .convertToMDKeyAndFillRow(row, segmentProperties, measureCount, noDictionaryCount,
0:                 segmentProperties.getComplexDimensions().size());
0:         dataHandler.addDataToStore(outputRow);
1:       }
1:       dataHandler.finish();
1:     } catch (CarbonDataWriterException e) {
1:       LOGGER.error(e);
1:       throw new Exception("Problem loading data during compaction: " + e.getMessage());
1:     } catch (Exception e) {
1:       LOGGER.error(e);
1:       throw new Exception("Problem loading data during compaction: " + e.getMessage());
1:     } finally {
1:       if (null != dataHandler) {
1:         try {
1:           dataHandler.closeHandler();
1:         } catch (CarbonDataWriterException e) {
1:           LOGGER.error(e);
1:           throw new Exception("Problem loading data during compaction: " + e.getMessage());
1:         }
1:       }
1:     }
1:   }
1: 
1:   /**
1:    * add row to a temp array which will we written to a sort temp file after sorting
1:    *
1:    * @param row
1:    */
1:   private void addRowForSorting(Object[] row) throws Exception {
1:     try {
0:       // prepare row array using RemoveDictionaryUtil class
1:       sortDataRows.addRow(row);
1:     } catch (CarbonSortKeyAndGroupByException e) {
1:       LOGGER.error(e);
1:       throw new Exception("Row addition for sorting failed during compaction: " + e.getMessage());
1:     }
1:   }
1: 
1:   /**
1:    * create an instance of sort data rows
1:    */
1:   private void initSortDataRows() throws Exception {
1:     measureCount = carbonTable.getMeasureByTableName(tableName).size();
1:     List<CarbonDimension> dimensions = carbonTable.getDimensionByTableName(tableName);
1:     noDictionaryColMapping = new boolean[dimensions.size()];
1:     int i = 0;
1:     for (CarbonDimension dimension : dimensions) {
1:       if (CarbonUtil.hasEncoding(dimension.getEncoder(), Encoding.DICTIONARY)) {
1:         i++;
1:         continue;
1:       }
0:       noDictionaryColMapping[i++] = true;
1:       noDictionaryCount++;
1:     }
1:     dimensionColumnCount = dimensions.size();
0:     SortParameters parameters = createSortParameters();
0:     SortIntermediateFileMerger intermediateFileMerger = new SortIntermediateFileMerger(parameters);
1:     // TODO: Now it is only supported onheap merge, but we can have unsafe merge
1:     // as well by using UnsafeSortDataRows.
0:     this.sortDataRows = new SortDataRows(parameters, intermediateFileMerger);
1:     try {
1:       this.sortDataRows.initialize();
1:     } catch (CarbonSortKeyAndGroupByException e) {
1:       LOGGER.error(e);
1:       throw new Exception(
1:           "Error initializing sort data rows object during compaction: " + e.getMessage());
1:     }
1:   }
1: 
1:   /**
1:    * This method will create the sort parameters VO object
1:    *
1:    * @return
1:    */
1:   private SortParameters createSortParameters() {
0:     SortParameters parameters = SortParameters
0:         .createSortParameters(carbonLoadModel.getDatabaseName(), tableName, dimensionColumnCount,
0:             segmentProperties.getComplexDimensions().size(), measureCount, noDictionaryCount,
0:             carbonLoadModel.getPartitionId(), segmentId, carbonLoadModel.getTaskNo(),
0:             noDictionaryColMapping);
0:     return parameters;
1:   }
1: 
1:   /**
1:    * create an instance of finalThread merger which will perform merge sort on all the
1:    * sort temp files
1:    */
1:   private void initializeFinalThreadMergerForMergeSort() {
0:     String sortTempFileLocation = tempStoreLocation + CarbonCommonConstants.FILE_SEPARATOR
0:         + CarbonCommonConstants.SORT_TEMP_FILE_LOCATION;
1:     finalMerger =
0:         new SingleThreadFinalSortFilesMerger(sortTempFileLocation, tableName, dimensionColumnCount,
0:             segmentProperties.getComplexDimensions().size(), measureCount, noDictionaryCount,
0:             aggType, noDictionaryColMapping);
1:   }
1: 
1:   /**
1:    * initialise carbon data writer instance
1:    */
1:   private void initDataHandler() throws Exception {
1:     CarbonFactDataHandlerModel carbonFactDataHandlerModel = CarbonFactDataHandlerModel
1:         .getCarbonFactDataHandlerModel(carbonLoadModel, carbonTable, segmentProperties, tableName,
0:             tempStoreLocation);
0:     setDataFileAttributesInModel(carbonLoadModel, compactionType, carbonTable,
0:         carbonFactDataHandlerModel);
0:     dataHandler = CarbonFactHandlerFactory.createCarbonFactHandler(carbonFactDataHandlerModel,
0:         CarbonFactHandlerFactory.FactHandlerType.COLUMNAR);
1:     try {
1:       dataHandler.initialise();
1:     } catch (CarbonDataWriterException e) {
1:       LOGGER.error(e);
1:       throw new Exception("Problem initialising data handler during compaction: " + e.getMessage());
1:     }
1:   }
1: 
1:   /**
1:    * initialise temporary store location
1:    */
1:   private void initTempStoreLocation() {
1:     tempStoreLocation = CarbonDataProcessorUtil
0:         .getLocalDataFolderLocation(carbonLoadModel.getDatabaseName(), tableName,
0:             carbonLoadModel.getTaskNo(), carbonLoadModel.getPartitionId(), segmentId, false);
1:   }
1: 
1:   /**
0:    * initialise aggregation type for measures for their storage format
1:    */
0:   private void initAggType() {
0:     aggType = CarbonDataProcessorUtil.initAggType(carbonTable, tableName, measureCount);
1:   }
1: }
author:QiangCai
-------------------------------------------------------------------------------
commit:f8e0585
/////////////////////////////////////////////////////////////////////////
1:       if (CompactionType.STREAMING == compactionType) {
1:         while (resultIterator.hasNext()) {
1:           // the input iterator of streaming segment is already using raw row
1:           addRowForSorting(resultIterator.next());
1:           isRecordFound = true;
1:         }
1:       } else {
1:         while (resultIterator.hasNext()) {
0:           addRowForSorting(prepareRowObjectForSorting(resultIterator.next()));
1:           isRecordFound = true;
1:         }
commit:81149f6
/////////////////////////////////////////////////////////////////////////
1:     return SortParameters
commit:9f94529
/////////////////////////////////////////////////////////////////////////
1:     boolean[] noDictionarySortColumnMapping = null;
1:     if (noDictionaryColMapping.length == this.segmentProperties.getNumberOfSortColumns()) {
1:       noDictionarySortColumnMapping = noDictionaryColMapping;
1:     } else {
1:       noDictionarySortColumnMapping = new boolean[this.segmentProperties.getNumberOfSortColumns()];
1:       System.arraycopy(noDictionaryColMapping, 0,
1:           noDictionarySortColumnMapping, 0, noDictionarySortColumnMapping.length);
0:     }
0:             aggType, noDictionaryColMapping, noDictionarySortColumnMapping);
author:lionelcao
-------------------------------------------------------------------------------
commit:874764f
/////////////////////////////////////////////////////////////////////////
0:             carbonLoadModel.getTaskNo(), carbonLoadModel.getPartitionId(), segmentId,
0:             true, false);
author:jackylk
-------------------------------------------------------------------------------
commit:dc83b2a
/////////////////////////////////////////////////////////////////////////
1: import org.apache.carbondata.core.datastore.exception.CarbonDataWriterException;
1: import org.apache.carbondata.core.datastore.row.CarbonRow;
/////////////////////////////////////////////////////////////////////////
/////////////////////////////////////////////////////////////////////////
commit:353272e
/////////////////////////////////////////////////////////////////////////
1:   private DataType[] dataTypes;
/////////////////////////////////////////////////////////////////////////
1:           getConvertedMeasureValue(row[measureIndexInRow++], dataTypes[i]);
/////////////////////////////////////////////////////////////////////////
1:         Object[] row = finalMerger.next();
1:         dataHandler.addDataToStore(new CarbonRow(row));
/////////////////////////////////////////////////////////////////////////
/////////////////////////////////////////////////////////////////////////
0:             dataTypes, noDictionaryColMapping, noDictionarySortColumnMapping);
/////////////////////////////////////////////////////////////////////////
0:     dataTypes = CarbonDataProcessorUtil.initDataType(carbonTable, tableName, measureCount);
commit:98df130
/////////////////////////////////////////////////////////////////////////
1: import org.apache.carbondata.core.metadata.datatype.DataType;
/////////////////////////////////////////////////////////////////////////
0: import org.apache.spark.sql.types.Decimal;
0: 
/////////////////////////////////////////////////////////////////////////
0:   private DataType[] aggType;
/////////////////////////////////////////////////////////////////////////
1:    * @param type
1:   private Object getConvertedMeasureValue(Object value, DataType type) {
0:     switch (type) {
0:       case DECIMAL:
0:           value = ((Decimal) value).toJavaBigDecimal();
/////////////////////////////////////////////////////////////////////////
0:     aggType = CarbonDataProcessorUtil.initDataType(carbonTable, tableName, measureCount);
============================================================================