1:b86ff92: /*
1:b86ff92:  * Licensed to the Apache Software Foundation (ASF) under one or more
1:b86ff92:  * contributor license agreements.  See the NOTICE file distributed with
1:b86ff92:  * this work for additional information regarding copyright ownership.
1:b86ff92:  * The ASF licenses this file to You under the Apache License, Version 2.0
1:b86ff92:  * (the "License"); you may not use this file except in compliance with
1:b86ff92:  * the License.  You may obtain a copy of the License at
1:b86ff92:  *
1:b86ff92:  *    http://www.apache.org/licenses/LICENSE-2.0
1:b86ff92:  *
1:b86ff92:  * Unless required by applicable law or agreed to in writing, software
1:b86ff92:  * distributed under the License is distributed on an "AS IS" BASIS,
1:b86ff92:  * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
1:b86ff92:  * See the License for the specific language governing permissions and
1:b86ff92:  * limitations under the License.
1:b86ff92:  */
1:b86ff92: package org.apache.carbondata.datamap.bloom;
1:b86ff92: 
1:7b31b91: import java.io.File;
1:b86ff92: import java.io.IOException;
1:589fe18: import java.util.*;
1:641ec09: import java.util.concurrent.ConcurrentHashMap;
1:b86ff92: 
1:b86ff92: import org.apache.carbondata.common.annotations.InterfaceAudience;
1:b86ff92: import org.apache.carbondata.common.exceptions.sql.MalformedDataMapCommandException;
1:b86ff92: import org.apache.carbondata.common.logging.LogService;
1:b86ff92: import org.apache.carbondata.common.logging.LogServiceFactory;
1:047c502: import org.apache.carbondata.core.cache.Cache;
1:047c502: import org.apache.carbondata.core.cache.CacheProvider;
1:047c502: import org.apache.carbondata.core.cache.CacheType;
1:77a1110: import org.apache.carbondata.core.constants.CarbonV3DataFormatConstants;
1:b86ff92: import org.apache.carbondata.core.datamap.DataMapDistributable;
1:b86ff92: import org.apache.carbondata.core.datamap.DataMapLevel;
1:b86ff92: import org.apache.carbondata.core.datamap.DataMapMeta;
1:b86ff92: import org.apache.carbondata.core.datamap.Segment;
1:747be9b: import org.apache.carbondata.core.datamap.dev.DataMapBuilder;
1:b86ff92: import org.apache.carbondata.core.datamap.dev.DataMapFactory;
1:b86ff92: import org.apache.carbondata.core.datamap.dev.DataMapWriter;
1:b86ff92: import org.apache.carbondata.core.datamap.dev.cgdatamap.CoarseGrainDataMap;
1:cd7c210: import org.apache.carbondata.core.datastore.block.SegmentProperties;
1:b86ff92: import org.apache.carbondata.core.datastore.filesystem.CarbonFile;
1:b86ff92: import org.apache.carbondata.core.datastore.impl.FileFactory;
1:5229443: import org.apache.carbondata.core.features.TableOperation;
1:b86ff92: import org.apache.carbondata.core.metadata.schema.table.CarbonTable;
1:b86ff92: import org.apache.carbondata.core.metadata.schema.table.DataMapSchema;
1:b86ff92: import org.apache.carbondata.core.metadata.schema.table.column.CarbonColumn;
1:b86ff92: import org.apache.carbondata.core.scan.filter.intf.ExpressionType;
1:b86ff92: import org.apache.carbondata.core.statusmanager.SegmentStatusManager;
1:b86ff92: import org.apache.carbondata.core.util.CarbonUtil;
1:b86ff92: import org.apache.carbondata.core.util.path.CarbonTablePath;
1:b86ff92: import org.apache.carbondata.events.Event;
1:b86ff92: 
1:b86ff92: import org.apache.commons.lang3.StringUtils;
1:b86ff92: 
1:6b94971: /**
1:9db662a:  * This class is for Bloom Filter for blocklet level
2:77a1110:  */
1:b86ff92: @InterfaceAudience.Internal
1:9db662a: public class BloomCoarseGrainDataMapFactory extends DataMapFactory<CoarseGrainDataMap> {
1:b86ff92:   private static final LogService LOGGER = LogServiceFactory.getLogService(
1:b86ff92:       BloomCoarseGrainDataMapFactory.class.getName());
1:b86ff92:   /**
1:b86ff92:    * property for size of bloom filter
1:b86ff92:    */
1:b86ff92:   private static final String BLOOM_SIZE = "bloom_size";
1:b86ff92:   /**
1:77a1110:    * default size for bloom filter, cardinality of the column.
1:b86ff92:    */
1:77a1110:   private static final int DEFAULT_BLOOM_FILTER_SIZE =
1:6351c3a:       CarbonV3DataFormatConstants.NUMBER_OF_ROWS_PER_BLOCKLET_COLUMN_PAGE_DEFAULT * 20;
1:b86ff92:   /**
1:6b94971:    * property for fpp(false-positive-probability) of bloom filter
1:6b94971:    */
1:6b94971:   private static final String BLOOM_FPP = "bloom_fpp";
1:6b94971:   /**
1:6351c3a:    * default value for fpp of bloom filter is 0.001%
1:6b94971:    */
1:6351c3a:   private static final double DEFAULT_BLOOM_FILTER_FPP = 0.00001d;
1:b86ff92: 
1:b86ff92:   /**
1:77a1110:    * property for compressing bloom while saving to disk.
1:b86ff92:    */
1:77a1110:   private static final String COMPRESS_BLOOM = "bloom_compress";
1:b86ff92:   /**
1:77a1110:    * Default value of compressing bloom while save to disk.
1:b86ff92:    */
1:77a1110:   private static final boolean DEFAULT_BLOOM_COMPRESS = true;
1:77a1110: 
1:b86ff92:   private DataMapMeta dataMapMeta;
1:b86ff92:   private String dataMapName;
1:b86ff92:   private int bloomFilterSize;
1:6b94971:   private double bloomFilterFpp;
1:77a1110:   private boolean bloomCompress;
1:047c502:   private Cache<BloomCacheKeyValue.CacheKey, BloomCacheKeyValue.CacheValue> cache;
1:047c502:   // segmentId -> list of index file
1:641ec09:   private Map<String, Set<String>> segmentMap = new ConcurrentHashMap<>();
1:b86ff92: 
1:9db662a:   public BloomCoarseGrainDataMapFactory(CarbonTable carbonTable, DataMapSchema dataMapSchema)
1:9db662a:       throws MalformedDataMapCommandException {
1:9db662a:     super(carbonTable, dataMapSchema);
1:b86ff92:     Objects.requireNonNull(carbonTable);
1:b86ff92:     Objects.requireNonNull(dataMapSchema);
1:b86ff92: 
1:b86ff92:     this.dataMapName = dataMapSchema.getDataMapName();
1:b86ff92: 
1:9db662a:     List<CarbonColumn> indexedColumns = carbonTable.getIndexedColumns(dataMapSchema);
1:b86ff92:     this.bloomFilterSize = validateAndGetBloomFilterSize(dataMapSchema);
1:6b94971:     this.bloomFilterFpp = validateAndGetBloomFilterFpp(dataMapSchema);
1:77a1110:     this.bloomCompress = validateAndGetBloomCompress(dataMapSchema);
1:b86ff92:     List<ExpressionType> optimizedOperations = new ArrayList<ExpressionType>();
1:b86ff92:     // todo: support more optimize operations
1:b86ff92:     optimizedOperations.add(ExpressionType.EQUALS);
1:202d099:     optimizedOperations.add(ExpressionType.IN);
1:b86ff92:     this.dataMapMeta = new DataMapMeta(this.dataMapName, indexedColumns, optimizedOperations);
1:b86ff92:     LOGGER.info(String.format("DataMap %s works for %s with bloom size %d",
1:b86ff92:         this.dataMapName, this.dataMapMeta, this.bloomFilterSize));
1:b86ff92:     try {
1:047c502:       this.cache = CacheProvider.getInstance()
1:047c502:           .createCache(new CacheType("bloom_cache"), BloomDataMapCache.class.getName());
1:047c502:     } catch (Exception e) {
1:047c502:       LOGGER.error(e);
1:047c502:       throw new MalformedDataMapCommandException(e.getMessage());
1:452c42b:     }
1:b86ff92:   }
1:b86ff92: 
1:452c42b:   /**
1:b86ff92:    * validate Lucene DataMap BLOOM_SIZE
1:b86ff92:    * 1. BLOOM_SIZE property is optional, 32000 * 20 will be the default size.
1:b86ff92:    * 2. BLOOM_SIZE should be an integer that greater than 0
1:452c42b:    */
1:b86ff92:   private int validateAndGetBloomFilterSize(DataMapSchema dmSchema)
1:b86ff92:       throws MalformedDataMapCommandException {
1:b86ff92:     String bloomFilterSizeStr = dmSchema.getProperties().get(BLOOM_SIZE);
1:b86ff92:     if (StringUtils.isBlank(bloomFilterSizeStr)) {
1:b86ff92:       LOGGER.warn(
1:b86ff92:           String.format("Bloom filter size is not configured for datamap %s, use default value %d",
1:b86ff92:               dataMapName, DEFAULT_BLOOM_FILTER_SIZE));
1:b86ff92:       return DEFAULT_BLOOM_FILTER_SIZE;
1:b86ff92:     }
1:b86ff92:     int bloomFilterSize;
1:452c42b:     try {
1:b86ff92:       bloomFilterSize = Integer.parseInt(bloomFilterSizeStr);
1:b86ff92:     } catch (NumberFormatException e) {
3:b86ff92:       throw new MalformedDataMapCommandException(
1:b86ff92:           String.format("Invalid value of bloom filter size '%s', it should be an integer",
1:b86ff92:               bloomFilterSizeStr));
1:b86ff92:     }
1:b86ff92:     // todo: reconsider the boundaries of bloom filter size
1:b86ff92:     if (bloomFilterSize <= 0) {
1:b86ff92:       throw new MalformedDataMapCommandException(
1:b86ff92:           String.format("Invalid value of bloom filter size '%s', it should be greater than 0",
1:b86ff92:               bloomFilterSizeStr));
1:b86ff92:     }
1:b86ff92:     return bloomFilterSize;
1:b86ff92:   }
1:b86ff92: 
1:6b94971:   /**
1:6b94971:    * validate bloom DataMap BLOOM_FPP
1:6b94971:    * 1. BLOOM_FPP property is optional, 0.00001 will be the default value.
1:6b94971:    * 2. BLOOM_FPP should be (0, 1)
1:6b94971:    */
1:6b94971:   private double validateAndGetBloomFilterFpp(DataMapSchema dmSchema)
1:6b94971:       throws MalformedDataMapCommandException {
1:6b94971:     String bloomFilterFppStr = dmSchema.getProperties().get(BLOOM_FPP);
1:6b94971:     if (StringUtils.isBlank(bloomFilterFppStr)) {
1:6b94971:       LOGGER.warn(
1:6b94971:           String.format("Bloom filter FPP is not configured for datamap %s, use default value %f",
1:6b94971:               dataMapName, DEFAULT_BLOOM_FILTER_FPP));
1:6b94971:       return DEFAULT_BLOOM_FILTER_FPP;
1:6b94971:     }
1:6b94971:     double bloomFilterFpp;
1:6b94971:     try {
1:6b94971:       bloomFilterFpp = Double.parseDouble(bloomFilterFppStr);
1:6b94971:     } catch (NumberFormatException e) {
1:6b94971:       throw new MalformedDataMapCommandException(
1:6b94971:           String.format("Invalid value of bloom filter fpp '%s', it should be an numeric",
1:6b94971:               bloomFilterFppStr));
1:6b94971:     }
1:6b94971:     if (bloomFilterFpp < 0 || bloomFilterFpp - 1 >= 0) {
1:6b94971:       throw new MalformedDataMapCommandException(
1:6b94971:           String.format("Invalid value of bloom filter fpp '%s', it should be in range 0~1",
1:6b94971:               bloomFilterFppStr));
1:6b94971:     }
1:6b94971:     return bloomFilterFpp;
1:6b94971:   }
1:6b94971: 
2:77a1110:   /**
1:77a1110:    * validate bloom DataMap COMPRESS_BLOOM
1:77a1110:    * Default value is true
1:b86ff92:    */
1:77a1110:   private boolean validateAndGetBloomCompress(DataMapSchema dmSchema) {
1:77a1110:     String bloomCompress = dmSchema.getProperties().get(COMPRESS_BLOOM);
1:77a1110:     if (StringUtils.isBlank(bloomCompress)) {
1:77a1110:       LOGGER.warn(
1:77a1110:           String.format("Bloom compress is not configured for datamap %s, use default value %b",
1:77a1110:               dataMapName, DEFAULT_BLOOM_COMPRESS));
1:77a1110:       return DEFAULT_BLOOM_COMPRESS;
1:b86ff92:     }
1:77a1110:     return Boolean.parseBoolean(bloomCompress);
1:b86ff92:   }
1:77a1110: 
1:4612e00:   @Override
1:cd7c210:   public DataMapWriter createWriter(Segment segment, String shardName,
1:cd7c210:       SegmentProperties segmentProperties) throws IOException {
1:b86ff92:     LOGGER.info(
1:b86ff92:         String.format("Data of BloomCoarseGranDataMap %s for table %s will be written to %s",
1:9db662a:             this.dataMapName, getCarbonTable().getTableName() , shardName));
1:9db662a:     return new BloomDataMapWriter(getCarbonTable().getTablePath(), this.dataMapName,
1:cd7c210:         this.dataMapMeta.getIndexedColumns(), segment, shardName, segmentProperties,
1:77a1110:         this.bloomFilterSize, this.bloomFilterFpp, bloomCompress);
1:4612e00:   }
1:b86ff92: 
1:b86ff92:   @Override
1:cd7c210:   public DataMapBuilder createBuilder(Segment segment, String shardName,
1:cd7c210:       SegmentProperties segmentProperties) throws IOException {
1:747be9b:     return new BloomDataMapBuilder(getCarbonTable().getTablePath(), this.dataMapName,
1:cd7c210:         this.dataMapMeta.getIndexedColumns(), segment, shardName, segmentProperties,
1:77a1110:         this.bloomFilterSize, this.bloomFilterFpp, bloomCompress);
1:4612e00:   }
1:b86ff92: 
1:7b31b91:   /**
1:7b31b91:    * returns all shard directories of bloom index files for query
1:7b31b91:    * if bloom index files are merged we should get only one shard path
1:7b31b91:    */
1:7b31b91:   private Set<String> getAllShardPaths(String tablePath, String segmentId) {
1:7b31b91:     String dataMapStorePath = CarbonTablePath.getDataMapStorePath(
1:7b31b91:         tablePath, segmentId, dataMapName);
1:7b31b91:     CarbonFile[] carbonFiles = FileFactory.getCarbonFile(dataMapStorePath).listFiles();
1:7b31b91:     Set<String> shardPaths = new HashSet<>();
1:7b31b91:     boolean mergeShardInprogress = false;
1:7b31b91:     CarbonFile mergeShardFile = null;
1:7b31b91:     for (CarbonFile carbonFile : carbonFiles) {
1:7b31b91:       if (carbonFile.getName().equals(BloomIndexFileStore.MERGE_BLOOM_INDEX_SHARD_NAME)) {
1:7b31b91:         mergeShardFile = carbonFile;
1:7b31b91:       } else if (carbonFile.getName().equals(BloomIndexFileStore.MERGE_INPROGRESS_FILE)) {
1:7b31b91:         mergeShardInprogress = true;
1:7b31b91:       } else if (carbonFile.isDirectory()) {
1:7b31b91:         shardPaths.add(carbonFile.getAbsolutePath());
1:7b31b91:       }
1:7b31b91:     }
1:7b31b91:     if (mergeShardFile != null && !mergeShardInprogress) {
1:7b31b91:       // should only get one shard path if mergeShard is generated successfully
1:7b31b91:       shardPaths.clear();
1:7b31b91:       shardPaths.add(mergeShardFile.getAbsolutePath());
1:7b31b91:     }
1:7b31b91:     return shardPaths;
1:7b31b91:   }
1:7b31b91: 
1:b86ff92:   @Override
1:b86ff92:   public List<CoarseGrainDataMap> getDataMaps(Segment segment) throws IOException {
1:7b31b91:     List<CoarseGrainDataMap> dataMaps = new ArrayList<>();
1:d8562e5:     try {
1:047c502:       Set<String> shardPaths = segmentMap.get(segment.getSegmentNo());
1:047c502:       if (shardPaths == null) {
1:7b31b91:         shardPaths = getAllShardPaths(getCarbonTable().getTablePath(), segment.getSegmentNo());
1:047c502:         segmentMap.put(segment.getSegmentNo(), shardPaths);
1:452c42b:       }
1:7b31b91:       Set<String> filteredShards = segment.getFilteredIndexShardNames();
1:047c502:       for (String shard : shardPaths) {
1:7b31b91:         if (shard.endsWith(BloomIndexFileStore.MERGE_BLOOM_INDEX_SHARD_NAME) ||
1:7b31b91:             filteredShards.contains(new File(shard).getName())) {
1:7b31b91:           // Filter out the tasks which are filtered through Main datamap.
1:7b31b91:           // for merge shard, shard pruning delay to be done before pruning blocklet
1:7b31b91:           BloomCoarseGrainDataMap bloomDM = new BloomCoarseGrainDataMap();
1:7b31b91:           bloomDM.init(new BloomDataMapModel(shard, cache, segment.getConfiguration()));
1:7b31b91:           bloomDM.initIndexColumnConverters(getCarbonTable(), dataMapMeta.getIndexedColumns());
1:7b31b91:           bloomDM.setFilteredShard(filteredShards);
1:7b31b91:           dataMaps.add(bloomDM);
1:7b31b91:         }
1:452c42b:       }
1:8b33ab2:     } catch (Exception e) {
1:b86ff92:       throw new IOException("Error occurs while init Bloom DataMap", e);
1:452c42b:     }
1:b86ff92:     return dataMaps;
1:b86ff92:   }
1:b86ff92: 
1:b86ff92:   @Override
1:b86ff92:   public List<CoarseGrainDataMap> getDataMaps(DataMapDistributable distributable)
1:b86ff92:       throws IOException {
1:7b31b91:     List<CoarseGrainDataMap> dataMaps = new ArrayList<>();
1:452c42b:     String indexPath = ((BloomDataMapDistributable) distributable).getIndexPath();
1:7b31b91:     Set<String> filteredShards = ((BloomDataMapDistributable) distributable).getFilteredShards();
1:7b31b91:     BloomCoarseGrainDataMap bloomDM = new BloomCoarseGrainDataMap();
1:7b31b91:     bloomDM.init(new BloomDataMapModel(indexPath, cache, FileFactory.getConfiguration()));
1:7b31b91:     bloomDM.initIndexColumnConverters(getCarbonTable(), dataMapMeta.getIndexedColumns());
1:7b31b91:     bloomDM.setFilteredShard(filteredShards);
1:7b31b91:     dataMaps.add(bloomDM);
1:7b31b91:     return dataMaps;
1:452c42b:   }
1:452c42b: 
1:b86ff92: 
1:452c42b:   @Override
1:b86ff92:   public List<DataMapDistributable> toDistributable(Segment segment) {
1:452c42b:     List<DataMapDistributable> dataMapDistributableList = new ArrayList<>();
1:7b31b91:     Set<String> shardPaths = segmentMap.get(segment.getSegmentNo());
1:7b31b91:     if (shardPaths == null) {
1:7b31b91:       shardPaths = getAllShardPaths(getCarbonTable().getTablePath(), segment.getSegmentNo());
1:7b31b91:       segmentMap.put(segment.getSegmentNo(), shardPaths);
1:7b31b91:     }
1:7b31b91:     Set<String> filteredShards = segment.getFilteredIndexShardNames();
1:7b31b91:     for (String shardPath : shardPaths) {
1:7b31b91:       // Filter out the tasks which are filtered through Main datamap.
1:7b31b91:       // for merge shard, shard pruning delay to be done before pruning blocklet
1:7b31b91:       if (shardPath.endsWith(BloomIndexFileStore.MERGE_BLOOM_INDEX_SHARD_NAME) ||
1:7b31b91:           filteredShards.contains(new File(shardPath).getName())) {
1:7b31b91:         DataMapDistributable bloomDataMapDistributable =
1:7b31b91:             new BloomDataMapDistributable(shardPath, filteredShards);
1:452c42b:         dataMapDistributableList.add(bloomDataMapDistributable);
1:452c42b:       }
1:452c42b:     }
1:452c42b:     return dataMapDistributableList;
1:b86ff92:   }
1:b86ff92: 
1:b86ff92:   @Override
1:b86ff92:   public void fireEvent(Event event) {
1:b86ff92: 
1:b86ff92:   }
1:b86ff92: 
1:b86ff92:   @Override
1:b86ff92:   public void clear(Segment segment) {
1:047c502:     Set<String> shards = segmentMap.remove(segment.getSegmentNo());
1:047c502:     if (shards != null) {
1:047c502:       for (String shard : shards) {
1:047c502:         for (CarbonColumn carbonColumn : dataMapMeta.getIndexedColumns()) {
1:047c502:           cache.invalidate(new BloomCacheKeyValue.CacheKey(shard, carbonColumn.getColName()));
1:b86ff92:         }
1:b86ff92:       }
1:b86ff92:     }
1:b86ff92:   }
1:b86ff92: 
1:b86ff92:   @Override
1:641ec09:   public synchronized void clear() {
1:cb10d03:     if (segmentMap.size() > 0) {
1:641ec09:       List<String> segments = new ArrayList<>(segmentMap.keySet());
1:641ec09:       for (String segmentId : segments) {
1:cb10d03:         clear(new Segment(segmentId, null, null));
1:cb10d03:       }
1:b86ff92:     }
1:b86ff92:   }
1:b86ff92: 
1:b86ff92:   @Override
1:1fd3703:   public void deleteDatamapData(Segment segment) throws IOException {
1:b86ff92:     try {
1:d8562e5:       String segmentId = segment.getSegmentNo();
1:d8562e5:       String datamapPath = CarbonTablePath
1:d8562e5:           .getDataMapStorePath(getCarbonTable().getTablePath(), segmentId, dataMapName);
1:d8562e5:       if (FileFactory.isFileExist(datamapPath)) {
1:d8562e5:         CarbonFile file = FileFactory.getCarbonFile(datamapPath,
1:d8562e5:             FileFactory.getFileType(datamapPath));
1:d8562e5:         CarbonUtil.deleteFoldersAndFilesSilent(file);
1:d8562e5:       }
1:1fd3703:     } catch (InterruptedException ex) {
1:1fd3703:       throw new IOException("Failed to delete datamap for segment_" + segment.getSegmentNo());
1:d8562e5:     }
1:d8562e5:   }
1:d8562e5: 
1:d8562e5:   @Override
1:b86ff92:   public void deleteDatamapData() {
1:9db662a:     SegmentStatusManager ssm =
1:9db662a:         new SegmentStatusManager(getCarbonTable().getAbsoluteTableIdentifier());
1:b86ff92:     try {
1:b86ff92:       List<Segment> validSegments = ssm.getValidAndInvalidSegments().getValidSegments();
1:b86ff92:       for (Segment segment : validSegments) {
1:d8562e5:         deleteDatamapData(segment);
1:b86ff92:       }
1:d8562e5:     } catch (IOException e) {
1:b86ff92:       LOGGER.error("drop datamap failed, failed to delete datamap directory");
1:b86ff92:     }
1:b86ff92:   }
1:b86ff92: 
1:b86ff92:   @Override
1:1c4358e:   public boolean willBecomeStale(TableOperation operation) {
1:cdee81d:     switch (operation) {
1:cdee81d:       case ALTER_RENAME:
1:cdee81d:         return false;
1:cdee81d:       case ALTER_DROP:
3:cdee81d:         return true;
1:cdee81d:       case ALTER_ADD_COLUMN:
1:81038f5:         return false;
1:cdee81d:       case ALTER_CHANGE_DATATYPE:
1:cdee81d:         return true;
1:cdee81d:       case STREAMING:
1:b9e5106:         return false;
1:cdee81d:       case DELETE:
1:cdee81d:         return true;
1:cdee81d:       case UPDATE:
1:cdee81d:         return true;
1:cdee81d:       case PARTITION:
1:cdee81d:         return true;
1:cdee81d:       default:
1:cdee81d:         return false;
1:b86ff92:     }
1:b86ff92:   }
1:77a1110: 
1:b86ff92:   @Override
1:1c4358e:   public boolean isOperationBlocked(TableOperation operation, Object... targets) {
1:1c4358e:     switch (operation) {
1:1c4358e:       case ALTER_DROP: {
1:1c4358e:         // alter table drop columns
1:1c4358e:         // will be blocked if the columns in bloomfilter datamap
1:1c4358e:         List<String> columnsToDrop = (List<String>) targets[0];
1:1c4358e:         List<String> indexedColumnNames = dataMapMeta.getIndexedColumnNames();
1:1c4358e:         for (String indexedcolumn : indexedColumnNames) {
1:1c4358e:           for (String column : columnsToDrop) {
1:1c4358e:             if (column.equalsIgnoreCase(indexedcolumn)) {
1:1c4358e:               return true;
1:b86ff92:             }
2:1c4358e:           }
1:1c4358e:         }
1:1c4358e:         return false;
1:1c4358e:       }
1:1c4358e:       case ALTER_CHANGE_DATATYPE: {
1:1c4358e:         // alter table change one column datatype
1:1c4358e:         // will be blocked if the column in bloomfilter datamap
1:1c4358e:         String columnToChangeDatatype = (String) targets[0];
1:1c4358e:         List<String> indexedColumnNames = dataMapMeta.getIndexedColumnNames();
1:1c4358e:         for (String indexedcolumn : indexedColumnNames) {
1:1c4358e:           if (indexedcolumn.equalsIgnoreCase(columnToChangeDatatype)) {
1:1c4358e:             return true;
1:1c4358e:           }
1:1c4358e:         }
1:1c4358e:         return false;
1:1c4358e:       }
1:1c4358e:       default:
1:1c4358e:         return false;
1:1c4358e:     }
1:1c4358e:   }
1:1c4358e: 
1:b86ff92:   @Override
1:b86ff92:   public DataMapMeta getMeta() {
1:b86ff92:     return this.dataMapMeta;
1:b86ff92:   }
1:b86ff92: 
1:b86ff92:   @Override
1:9db662a:   public DataMapLevel getDataMapLevel() {
1:b86ff92:     return DataMapLevel.CG;
1:b86ff92:   }
1:b86ff92: }
============================================================================
author:Manhua
-------------------------------------------------------------------------------
commit:7b31b91
/////////////////////////////////////////////////////////////////////////
1: import java.io.File;
/////////////////////////////////////////////////////////////////////////
/////////////////////////////////////////////////////////////////////////
1:   /**
1:    * returns all shard directories of bloom index files for query
1:    * if bloom index files are merged we should get only one shard path
1:    */
1:   private Set<String> getAllShardPaths(String tablePath, String segmentId) {
1:     String dataMapStorePath = CarbonTablePath.getDataMapStorePath(
1:         tablePath, segmentId, dataMapName);
1:     CarbonFile[] carbonFiles = FileFactory.getCarbonFile(dataMapStorePath).listFiles();
1:     Set<String> shardPaths = new HashSet<>();
1:     boolean mergeShardInprogress = false;
1:     CarbonFile mergeShardFile = null;
1:     for (CarbonFile carbonFile : carbonFiles) {
1:       if (carbonFile.getName().equals(BloomIndexFileStore.MERGE_BLOOM_INDEX_SHARD_NAME)) {
1:         mergeShardFile = carbonFile;
1:       } else if (carbonFile.getName().equals(BloomIndexFileStore.MERGE_INPROGRESS_FILE)) {
1:         mergeShardInprogress = true;
1:       } else if (carbonFile.isDirectory()) {
1:         shardPaths.add(carbonFile.getAbsolutePath());
1:       }
1:     }
1:     if (mergeShardFile != null && !mergeShardInprogress) {
1:       // should only get one shard path if mergeShard is generated successfully
1:       shardPaths.clear();
1:       shardPaths.add(mergeShardFile.getAbsolutePath());
1:     }
1:     return shardPaths;
1:   }
1: 
1:     List<CoarseGrainDataMap> dataMaps = new ArrayList<>();
1:         shardPaths = getAllShardPaths(getCarbonTable().getTablePath(), segment.getSegmentNo());
1:       Set<String> filteredShards = segment.getFilteredIndexShardNames();
1:         if (shard.endsWith(BloomIndexFileStore.MERGE_BLOOM_INDEX_SHARD_NAME) ||
1:             filteredShards.contains(new File(shard).getName())) {
1:           // Filter out the tasks which are filtered through Main datamap.
1:           // for merge shard, shard pruning delay to be done before pruning blocklet
1:           BloomCoarseGrainDataMap bloomDM = new BloomCoarseGrainDataMap();
1:           bloomDM.init(new BloomDataMapModel(shard, cache, segment.getConfiguration()));
1:           bloomDM.initIndexColumnConverters(getCarbonTable(), dataMapMeta.getIndexedColumns());
1:           bloomDM.setFilteredShard(filteredShards);
1:           dataMaps.add(bloomDM);
1:         }
/////////////////////////////////////////////////////////////////////////
1:     List<CoarseGrainDataMap> dataMaps = new ArrayList<>();
1:     Set<String> filteredShards = ((BloomDataMapDistributable) distributable).getFilteredShards();
1:     BloomCoarseGrainDataMap bloomDM = new BloomCoarseGrainDataMap();
1:     bloomDM.init(new BloomDataMapModel(indexPath, cache, FileFactory.getConfiguration()));
1:     bloomDM.initIndexColumnConverters(getCarbonTable(), dataMapMeta.getIndexedColumns());
1:     bloomDM.setFilteredShard(filteredShards);
1:     dataMaps.add(bloomDM);
1:     return dataMaps;
1:     Set<String> shardPaths = segmentMap.get(segment.getSegmentNo());
1:     if (shardPaths == null) {
1:       shardPaths = getAllShardPaths(getCarbonTable().getTablePath(), segment.getSegmentNo());
1:       segmentMap.put(segment.getSegmentNo(), shardPaths);
1:     }
1:     Set<String> filteredShards = segment.getFilteredIndexShardNames();
1:     for (String shardPath : shardPaths) {
1:       // Filter out the tasks which are filtered through Main datamap.
1:       // for merge shard, shard pruning delay to be done before pruning blocklet
1:       if (shardPath.endsWith(BloomIndexFileStore.MERGE_BLOOM_INDEX_SHARD_NAME) ||
1:           filteredShards.contains(new File(shardPath).getName())) {
1:         DataMapDistributable bloomDataMapDistributable =
1:             new BloomDataMapDistributable(shardPath, filteredShards);
commit:46f0c85
/////////////////////////////////////////////////////////////////////////
commit:4612e00
/////////////////////////////////////////////////////////////////////////
0:         // different from lucene, bloom only get corresponding directory of current datamap
0:         if (dataMap.getDataMapSchema().getDataMapName().equals(this.dataMapName)) {
0:           List<CarbonFile> indexFiles;
0:           String dmPath = CarbonTablePath.getDataMapStorePath(tablePath, segmentId,
0:               dataMap.getDataMapSchema().getDataMapName());
0:           FileFactory.FileType fileType = FileFactory.getFileType(dmPath);
0:           final CarbonFile dirPath = FileFactory.getCarbonFile(dmPath, fileType);
0:           indexFiles = Arrays.asList(dirPath.listFiles(new CarbonFileFilter() {
1:             @Override
0:             public boolean accept(CarbonFile file) {
0:               return file.isDirectory();
1:             }
0:           }));
0:           indexDirs.addAll(indexFiles);
1:         }
commit:81038f5
/////////////////////////////////////////////////////////////////////////
1:         return false;
author:kunal642
-------------------------------------------------------------------------------
commit:8f1a029
/////////////////////////////////////////////////////////////////////////
0:         bloomDM.init(new BloomDataMapModel(shard, cache, segment.getConfiguration()));
/////////////////////////////////////////////////////////////////////////
0:     bloomCoarseGrainDataMap
0:         .init(new BloomDataMapModel(indexPath, cache, FileFactory.getConfiguration()));
commit:cb10d03
/////////////////////////////////////////////////////////////////////////
1:     if (segmentMap.size() > 0) {
0:       for (String segmentId : segmentMap.keySet().toArray(new String[segmentMap.size()])) {
1:         clear(new Segment(segmentId, null, null));
1:       }
author:xuchuanyin
-------------------------------------------------------------------------------
commit:b9e5106
/////////////////////////////////////////////////////////////////////////
1:         return false;
commit:6351c3a
/////////////////////////////////////////////////////////////////////////
1:       CarbonV3DataFormatConstants.NUMBER_OF_ROWS_PER_BLOCKLET_COLUMN_PAGE_DEFAULT * 20;
1:    * default value for fpp of bloom filter is 0.001%
1:   private static final double DEFAULT_BLOOM_FILTER_FPP = 0.00001d;
commit:1fd3703
/////////////////////////////////////////////////////////////////////////
1:   public void deleteDatamapData(Segment segment) throws IOException {
/////////////////////////////////////////////////////////////////////////
1:     } catch (InterruptedException ex) {
1:       throw new IOException("Failed to delete datamap for segment_" + segment.getSegmentNo());
commit:202d099
/////////////////////////////////////////////////////////////////////////
1:     optimizedOperations.add(ExpressionType.IN);
commit:641ec09
/////////////////////////////////////////////////////////////////////////
1: import java.util.concurrent.ConcurrentHashMap;
/////////////////////////////////////////////////////////////////////////
1:   private Map<String, Set<String>> segmentMap = new ConcurrentHashMap<>();
/////////////////////////////////////////////////////////////////////////
1:   public synchronized void clear() {
1:       List<String> segments = new ArrayList<>(segmentMap.keySet());
1:       for (String segmentId : segments) {
commit:d8562e5
/////////////////////////////////////////////////////////////////////////
0:   public void deleteDatamapData(Segment segment) {
1:     try {
1:       String segmentId = segment.getSegmentNo();
1:       String datamapPath = CarbonTablePath
1:           .getDataMapStorePath(getCarbonTable().getTablePath(), segmentId, dataMapName);
1:       if (FileFactory.isFileExist(datamapPath)) {
1:         CarbonFile file = FileFactory.getCarbonFile(datamapPath,
1:             FileFactory.getFileType(datamapPath));
1:         CarbonUtil.deleteFoldersAndFilesSilent(file);
1:       }
0:     } catch (IOException | InterruptedException ex) {
0:       LOGGER.error("Failed to delete datamap for segment_" + segment.getSegmentNo());
1:     }
1:   }
1: 
1:   @Override
1:         deleteDatamapData(segment);
1:     } catch (IOException e) {
commit:cd7c210
/////////////////////////////////////////////////////////////////////////
1: import org.apache.carbondata.core.datastore.block.SegmentProperties;
/////////////////////////////////////////////////////////////////////////
1:   public DataMapWriter createWriter(Segment segment, String shardName,
1:       SegmentProperties segmentProperties) throws IOException {
1:         this.dataMapMeta.getIndexedColumns(), segment, shardName, segmentProperties,
1:   public DataMapBuilder createBuilder(Segment segment, String shardName,
1:       SegmentProperties segmentProperties) throws IOException {
1:         this.dataMapMeta.getIndexedColumns(), segment, shardName, segmentProperties,
/////////////////////////////////////////////////////////////////////////
0:         bloomDM.init(new BloomDataMapModel(shard, cache));
0:         bloomDM.initIndexColumnConverters(getCarbonTable(), dataMapMeta.getIndexedColumns());
/////////////////////////////////////////////////////////////////////////
0:     bloomCoarseGrainDataMap.init(new BloomDataMapModel(indexPath, cache));
0:     bloomCoarseGrainDataMap.initIndexColumnConverters(getCarbonTable(),
0:         dataMapMeta.getIndexedColumns());
commit:d14c403
/////////////////////////////////////////////////////////////////////////
0: import java.util.HashSet;
/////////////////////////////////////////////////////////////////////////
0:         bloomDM.setIndexedColumn(new HashSet<String>(dataMapMeta.getIndexedColumnNames()));
/////////////////////////////////////////////////////////////////////////
0:     bloomCoarseGrainDataMap.setIndexedColumn(
0:         new HashSet<String>(dataMapMeta.getIndexedColumnNames()));
commit:452c42b
/////////////////////////////////////////////////////////////////////////
0: import java.util.Arrays;
/////////////////////////////////////////////////////////////////////////
0: import org.apache.carbondata.core.datamap.DataMapStoreManager;
0: import org.apache.carbondata.core.datamap.TableDataMap;
0: import org.apache.carbondata.core.datastore.filesystem.CarbonFileFilter;
/////////////////////////////////////////////////////////////////////////
0:     List<CoarseGrainDataMap> coarseGrainDataMaps = new ArrayList<>();
0:     BloomCoarseGrainDataMap bloomCoarseGrainDataMap = new BloomCoarseGrainDataMap();
1:     String indexPath = ((BloomDataMapDistributable) distributable).getIndexPath();
0:     bloomCoarseGrainDataMap.init(new DataMapModel(indexPath));
0:     coarseGrainDataMaps.add(bloomCoarseGrainDataMap);
0:     return coarseGrainDataMaps;
1:   }
1: 
1:   /**
0:    * returns all the directories of lucene index files for query
0:    * Note: copied from luceneDataMapFactory, will extract to a common interface
1:    */
0:   private CarbonFile[] getAllIndexDirs(String tablePath, String segmentId) {
0:     List<CarbonFile> indexDirs = new ArrayList<>();
0:     List<TableDataMap> dataMaps;
1:     try {
0:       // there can be multiple bloom datamaps present on a table, so get all datamaps and form
0:       // the path till the index file directories in all datamaps folders present in each segment
0:       dataMaps = DataMapStoreManager.getInstance().getAllDataMap(getCarbonTable());
0:     } catch (IOException ex) {
0:       LOGGER.error(ex, String.format("failed to get datamaps for tablePath %s, segmentId %s",
0:           tablePath, segmentId));
0:       throw new RuntimeException(ex);
1:     }
0:     if (dataMaps.size() > 0) {
0:       for (TableDataMap dataMap : dataMaps) {
0:         List<CarbonFile> indexFiles;
0:         String dmPath = CarbonTablePath.getSegmentPath(tablePath, segmentId) + File.separator
0:             + dataMap.getDataMapSchema().getDataMapName();
0:         FileFactory.FileType fileType = FileFactory.getFileType(dmPath);
0:         final CarbonFile dirPath = FileFactory.getCarbonFile(dmPath, fileType);
0:         indexFiles = Arrays.asList(dirPath.listFiles(new CarbonFileFilter() {
1:           @Override
0:           public boolean accept(CarbonFile file) {
0:             return file.isDirectory();
1:           }
0:         }));
0:         indexDirs.addAll(indexFiles);
1:       }
1:     }
0:     return indexDirs.toArray(new CarbonFile[0]);
1:     List<DataMapDistributable> dataMapDistributableList = new ArrayList<>();
0:     CarbonFile[] indexDirs =
0:         getAllIndexDirs(getCarbonTable().getTablePath(), segment.getSegmentNo());
0:     for (CarbonFile indexDir : indexDirs) {
0:       // Filter out the tasks which are filtered through CG datamap.
0:       if (!segment.getFilteredIndexShardNames().contains(indexDir.getName())) {
0:         continue;
1:       }
0:       DataMapDistributable bloomDataMapDistributable = new BloomDataMapDistributable(
0:           indexDir.getAbsolutePath());
1:       dataMapDistributableList.add(bloomDataMapDistributable);
1:     }
1:     return dataMapDistributableList;
commit:6b94971
/////////////////////////////////////////////////////////////////////////
1:   /**
1:    * property for fpp(false-positive-probability) of bloom filter
1:    */
1:   private static final String BLOOM_FPP = "bloom_fpp";
1:   /**
0:    * default value for fpp of bloom filter
1:    */
0:   private static final double DEFAULT_BLOOM_FILTER_FPP = 0.00001d;
1:   private double bloomFilterFpp;
/////////////////////////////////////////////////////////////////////////
1:     this.bloomFilterFpp = validateAndGetBloomFilterFpp(dataMapSchema);
/////////////////////////////////////////////////////////////////////////
1:   /**
1:    * validate bloom DataMap BLOOM_FPP
1:    * 1. BLOOM_FPP property is optional, 0.00001 will be the default value.
1:    * 2. BLOOM_FPP should be (0, 1)
1:    */
1:   private double validateAndGetBloomFilterFpp(DataMapSchema dmSchema)
1:       throws MalformedDataMapCommandException {
1:     String bloomFilterFppStr = dmSchema.getProperties().get(BLOOM_FPP);
1:     if (StringUtils.isBlank(bloomFilterFppStr)) {
1:       LOGGER.warn(
1:           String.format("Bloom filter FPP is not configured for datamap %s, use default value %f",
1:               dataMapName, DEFAULT_BLOOM_FILTER_FPP));
1:       return DEFAULT_BLOOM_FILTER_FPP;
1:     }
1:     double bloomFilterFpp;
1:     try {
1:       bloomFilterFpp = Double.parseDouble(bloomFilterFppStr);
1:     } catch (NumberFormatException e) {
1:       throw new MalformedDataMapCommandException(
1:           String.format("Invalid value of bloom filter fpp '%s', it should be an numeric",
1:               bloomFilterFppStr));
1:     }
1:     if (bloomFilterFpp < 0 || bloomFilterFpp - 1 >= 0) {
1:       throw new MalformedDataMapCommandException(
1:           String.format("Invalid value of bloom filter fpp '%s', it should be in range 0~1",
1:               bloomFilterFppStr));
1:     }
1:     return bloomFilterFpp;
1:   }
1: 
0:         this.dataMapMeta.getIndexedColumns(), segment, shardName,
0:         this.bloomFilterSize, this.bloomFilterFpp);
0:         this.dataMapMeta.getIndexedColumns(), segment, shardName,
0:         this.bloomFilterSize, this.bloomFilterFpp);
commit:b86ff92
/////////////////////////////////////////////////////////////////////////
1: /*
1:  * Licensed to the Apache Software Foundation (ASF) under one or more
1:  * contributor license agreements.  See the NOTICE file distributed with
1:  * this work for additional information regarding copyright ownership.
1:  * The ASF licenses this file to You under the Apache License, Version 2.0
1:  * (the "License"); you may not use this file except in compliance with
1:  * the License.  You may obtain a copy of the License at
1:  *
1:  *    http://www.apache.org/licenses/LICENSE-2.0
1:  *
1:  * Unless required by applicable law or agreed to in writing, software
1:  * distributed under the License is distributed on an "AS IS" BASIS,
1:  * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
1:  * See the License for the specific language governing permissions and
1:  * limitations under the License.
1:  */
1: package org.apache.carbondata.datamap.bloom;
1: 
0: import java.io.File;
1: import java.io.IOException;
0: import java.util.ArrayList;
0: import java.util.HashSet;
0: import java.util.List;
0: import java.util.Objects;
0: import java.util.Set;
1: 
1: import org.apache.carbondata.common.annotations.InterfaceAudience;
1: import org.apache.carbondata.common.exceptions.sql.MalformedDataMapCommandException;
1: import org.apache.carbondata.common.logging.LogService;
1: import org.apache.carbondata.common.logging.LogServiceFactory;
1: import org.apache.carbondata.core.datamap.DataMapDistributable;
1: import org.apache.carbondata.core.datamap.DataMapLevel;
1: import org.apache.carbondata.core.datamap.DataMapMeta;
1: import org.apache.carbondata.core.datamap.Segment;
1: import org.apache.carbondata.core.datamap.dev.DataMapFactory;
0: import org.apache.carbondata.core.datamap.dev.DataMapModel;
1: import org.apache.carbondata.core.datamap.dev.DataMapWriter;
1: import org.apache.carbondata.core.datamap.dev.cgdatamap.CoarseGrainDataMap;
1: import org.apache.carbondata.core.datastore.filesystem.CarbonFile;
1: import org.apache.carbondata.core.datastore.impl.FileFactory;
0: import org.apache.carbondata.core.memory.MemoryException;
1: import org.apache.carbondata.core.metadata.schema.table.CarbonTable;
1: import org.apache.carbondata.core.metadata.schema.table.DataMapSchema;
1: import org.apache.carbondata.core.metadata.schema.table.column.CarbonColumn;
1: import org.apache.carbondata.core.scan.filter.intf.ExpressionType;
1: import org.apache.carbondata.core.statusmanager.SegmentStatusManager;
1: import org.apache.carbondata.core.util.CarbonUtil;
1: import org.apache.carbondata.core.util.path.CarbonTablePath;
1: import org.apache.carbondata.events.Event;
1: 
1: import org.apache.commons.lang3.StringUtils;
1: 
1: @InterfaceAudience.Internal
0: public class BloomCoarseGrainDataMapFactory implements DataMapFactory<CoarseGrainDataMap> {
1:   private static final LogService LOGGER = LogServiceFactory.getLogService(
1:       BloomCoarseGrainDataMapFactory.class.getName());
1:   /**
0:    * property for indexed column
1:    */
0:   private static final String BLOOM_COLUMNS = "bloom_columns";
1:   /**
1:    * property for size of bloom filter
1:    */
1:   private static final String BLOOM_SIZE = "bloom_size";
1:   /**
0:    * default size for bloom filter: suppose one blocklet contains 20 pages
0:    * and all the indexed value is distinct.
1:    */
0:   private static final int DEFAULT_BLOOM_FILTER_SIZE = 32000 * 20;
0:   private CarbonTable carbonTable;
1:   private DataMapMeta dataMapMeta;
1:   private String dataMapName;
1:   private int bloomFilterSize;
1: 
1:   @Override
0:   public void init(CarbonTable carbonTable, DataMapSchema dataMapSchema)
0:       throws IOException, MalformedDataMapCommandException {
1:     Objects.requireNonNull(carbonTable);
1:     Objects.requireNonNull(dataMapSchema);
1: 
0:     this.carbonTable = carbonTable;
1:     this.dataMapName = dataMapSchema.getDataMapName();
1: 
0:     List<String> indexedColumns = validateAndGetIndexedColumns(dataMapSchema, carbonTable);
1:     this.bloomFilterSize = validateAndGetBloomFilterSize(dataMapSchema);
1:     List<ExpressionType> optimizedOperations = new ArrayList<ExpressionType>();
1:     // todo: support more optimize operations
1:     optimizedOperations.add(ExpressionType.EQUALS);
1:     this.dataMapMeta = new DataMapMeta(this.dataMapName, indexedColumns, optimizedOperations);
1:     LOGGER.info(String.format("DataMap %s works for %s with bloom size %d",
1:         this.dataMapName, this.dataMapMeta, this.bloomFilterSize));
1:   }
1: 
1:   /**
0:    * validate Lucene DataMap BLOOM_COLUMNS
0:    * 1. require BLOOM_COLUMNS property
0:    * 2. BLOOM_COLUMNS can't contains illegal argument(empty, blank)
0:    * 3. BLOOM_COLUMNS can't contains duplicate same columns
0:    * 4. BLOOM_COLUMNS should be exists in table columns
1:    */
0:   private List<String> validateAndGetIndexedColumns(DataMapSchema dmSchema,
0:       CarbonTable carbonTable) throws MalformedDataMapCommandException {
0:     String bloomColumnsStr = dmSchema.getProperties().get(BLOOM_COLUMNS);
0:     if (StringUtils.isBlank(bloomColumnsStr)) {
1:       throw new MalformedDataMapCommandException(
0:           String.format("Bloom coarse datamap require proper %s property", BLOOM_COLUMNS));
1:     }
0:     String[] bloomColumns = StringUtils.split(bloomColumnsStr, ",", -1);
0:     List<String> bloomColumnList = new ArrayList<String>(bloomColumns.length);
0:     Set<String> bloomColumnSet = new HashSet<String>(bloomColumns.length);
0:     for (String bloomCol : bloomColumns) {
0:       CarbonColumn column = carbonTable.getColumnByName(carbonTable.getTableName(),
0:           bloomCol.trim().toLowerCase());
0:       if (null == column) {
1:         throw new MalformedDataMapCommandException(
0:             String.format("%s: %s does not exist in table. Please check create datamap statement",
0:                 BLOOM_COLUMNS, bloomCol));
1:       }
0:       if (!bloomColumnSet.add(column.getColName())) {
0:         throw new MalformedDataMapCommandException(String.format("%s has duplicate column: %s",
0:             BLOOM_COLUMNS, bloomCol));
1:       }
0:       bloomColumnList.add(column.getColName());
1:     }
0:     return bloomColumnList;
1:   }
1: 
1:   /**
1:    * validate Lucene DataMap BLOOM_SIZE
1:    * 1. BLOOM_SIZE property is optional, 32000 * 20 will be the default size.
1:    * 2. BLOOM_SIZE should be an integer that greater than 0
1:    */
1:   private int validateAndGetBloomFilterSize(DataMapSchema dmSchema)
1:       throws MalformedDataMapCommandException {
1:     String bloomFilterSizeStr = dmSchema.getProperties().get(BLOOM_SIZE);
1:     if (StringUtils.isBlank(bloomFilterSizeStr)) {
1:       LOGGER.warn(
1:           String.format("Bloom filter size is not configured for datamap %s, use default value %d",
1:               dataMapName, DEFAULT_BLOOM_FILTER_SIZE));
1:       return DEFAULT_BLOOM_FILTER_SIZE;
1:     }
1:     int bloomFilterSize;
1:     try {
1:       bloomFilterSize = Integer.parseInt(bloomFilterSizeStr);
1:     } catch (NumberFormatException e) {
1:       throw new MalformedDataMapCommandException(
1:           String.format("Invalid value of bloom filter size '%s', it should be an integer",
1:               bloomFilterSizeStr));
1:     }
1:     // todo: reconsider the boundaries of bloom filter size
1:     if (bloomFilterSize <= 0) {
1:       throw new MalformedDataMapCommandException(
1:           String.format("Invalid value of bloom filter size '%s', it should be greater than 0",
1:               bloomFilterSizeStr));
1:     }
1:     return bloomFilterSize;
1:   }
1: 
1:   @Override
0:   public DataMapWriter createWriter(Segment segment, String writeDirectoryPath) {
1:     LOGGER.info(
1:         String.format("Data of BloomCoarseGranDataMap %s for table %s will be written to %s",
0:             this.dataMapName, this.carbonTable.getTableName() , writeDirectoryPath));
0:     return new BloomDataMapWriter(this.carbonTable.getAbsoluteTableIdentifier(),
0:         this.dataMapMeta, this.bloomFilterSize, segment, writeDirectoryPath);
1:   }
1: 
1:   @Override
1:   public List<CoarseGrainDataMap> getDataMaps(Segment segment) throws IOException {
0:     List<CoarseGrainDataMap> dataMaps = new ArrayList<CoarseGrainDataMap>(1);
0:     BloomCoarseGrainDataMap bloomDM = new BloomCoarseGrainDataMap();
1:     try {
0:       bloomDM.init(new DataMapModel(BloomDataMapWriter.genDataMapStorePath(
0:           CarbonTablePath.getSegmentPath(carbonTable.getTablePath(), segment.getSegmentNo()),
0:           dataMapName)));
0:     } catch (MemoryException e) {
1:       throw new IOException("Error occurs while init Bloom DataMap", e);
1:     }
0:     dataMaps.add(bloomDM);
1:     return dataMaps;
1:   }
1: 
1:   @Override
1:   public List<CoarseGrainDataMap> getDataMaps(DataMapDistributable distributable)
1:       throws IOException {
0:     return null;
1:   }
1: 
1:   @Override
1:   public List<DataMapDistributable> toDistributable(Segment segment) {
0:     return null;
1:   }
1: 
1:   @Override
1:   public void fireEvent(Event event) {
1: 
1:   }
1: 
1:   @Override
1:   public void clear(Segment segment) {
1: 
1:   }
1: 
1:   @Override
0:   public void clear() {
1: 
1:   }
1: 
1:   @Override
1:   public void deleteDatamapData() {
0:     SegmentStatusManager ssm = new SegmentStatusManager(carbonTable.getAbsoluteTableIdentifier());
1:     try {
1:       List<Segment> validSegments = ssm.getValidAndInvalidSegments().getValidSegments();
1:       for (Segment segment : validSegments) {
0:         String segmentId = segment.getSegmentNo();
0:         String datamapPath = CarbonTablePath.getSegmentPath(
0:             carbonTable.getAbsoluteTableIdentifier().getTablePath(), segmentId)
0:             + File.separator + dataMapName;
0:         if (FileFactory.isFileExist(datamapPath)) {
0:           CarbonFile file = FileFactory.getCarbonFile(datamapPath,
0:               FileFactory.getFileType(datamapPath));
0:           CarbonUtil.deleteFoldersAndFilesSilent(file);
1:         }
1:       }
0:     } catch (IOException | InterruptedException ex) {
1:       LOGGER.error("drop datamap failed, failed to delete datamap directory");
1:     }
1:   }
1:   @Override
1:   public DataMapMeta getMeta() {
1:     return this.dataMapMeta;
1:   }
1: 
1:   @Override
0:   public DataMapLevel getDataMapType() {
1:     return DataMapLevel.CG;
1:   }
1: }
author:Sssan520
-------------------------------------------------------------------------------
commit:1c4358e
/////////////////////////////////////////////////////////////////////////
0:   @Override
1:   public boolean willBecomeStale(TableOperation operation) {
/////////////////////////////////////////////////////////////////////////
1:   public boolean isOperationBlocked(TableOperation operation, Object... targets) {
1:     switch (operation) {
1:       case ALTER_DROP: {
1:         // alter table drop columns
1:         // will be blocked if the columns in bloomfilter datamap
1:         List<String> columnsToDrop = (List<String>) targets[0];
1:         List<String> indexedColumnNames = dataMapMeta.getIndexedColumnNames();
1:         for (String indexedcolumn : indexedColumnNames) {
1:           for (String column : columnsToDrop) {
1:             if (column.equalsIgnoreCase(indexedcolumn)) {
1:               return true;
1:             }
1:           }
1:         }
1:         return false;
1:       }
1:       case ALTER_CHANGE_DATATYPE: {
1:         // alter table change one column datatype
1:         // will be blocked if the column in bloomfilter datamap
1:         String columnToChangeDatatype = (String) targets[0];
1:         List<String> indexedColumnNames = dataMapMeta.getIndexedColumnNames();
1:         for (String indexedcolumn : indexedColumnNames) {
1:           if (indexedcolumn.equalsIgnoreCase(columnToChangeDatatype)) {
1:             return true;
1:           }
1:         }
1:         return false;
1:       }
1:       default:
1:         return false;
1:     }
1:   }
1: 
0:   @Override
author:ndwangsen
-------------------------------------------------------------------------------
commit:cdee81d
/////////////////////////////////////////////////////////////////////////
1:     switch (operation) {
1:       case ALTER_RENAME:
1:         return false;
1:       case ALTER_DROP:
1:         return true;
1:       case ALTER_ADD_COLUMN:
1:         return true;
1:       case ALTER_CHANGE_DATATYPE:
1:         return true;
1:       case STREAMING:
1:         return true;
1:       case DELETE:
1:         return true;
1:       case UPDATE:
1:         return true;
1:       case PARTITION:
1:         return true;
1:       default:
1:         return false;
0:     }
author:ravipesala
-------------------------------------------------------------------------------
commit:589fe18
/////////////////////////////////////////////////////////////////////////
1: import java.util.*;
/////////////////////////////////////////////////////////////////////////
0:     List<String> segments = new ArrayList<>(segmentMap.keySet());
0:     for (String segmentId : segments) {
commit:047c502
/////////////////////////////////////////////////////////////////////////
0: import java.util.HashMap;
0: import java.util.Map;
0: import java.util.Set;
1: import org.apache.carbondata.core.cache.Cache;
1: import org.apache.carbondata.core.cache.CacheProvider;
1: import org.apache.carbondata.core.cache.CacheType;
/////////////////////////////////////////////////////////////////////////
/////////////////////////////////////////////////////////////////////////
1:   private Cache<BloomCacheKeyValue.CacheKey, BloomCacheKeyValue.CacheValue> cache;
1:   // segmentId -> list of index file
0:   private Map<String, Set<String>> segmentMap = new HashMap<>();
/////////////////////////////////////////////////////////////////////////
0:     try {
1:       this.cache = CacheProvider.getInstance()
1:           .createCache(new CacheType("bloom_cache"), BloomDataMapCache.class.getName());
1:     } catch (Exception e) {
1:       LOGGER.error(e);
1:       throw new MalformedDataMapCommandException(e.getMessage());
0:     }
/////////////////////////////////////////////////////////////////////////
1:       Set<String> shardPaths = segmentMap.get(segment.getSegmentNo());
1:       if (shardPaths == null) {
0:         String dataMapStorePath = DataMapWriter.getDefaultDataMapPath(
0:             getCarbonTable().getTablePath(), segment.getSegmentNo(), dataMapName);
0:         CarbonFile[] carbonFiles = FileFactory.getCarbonFile(dataMapStorePath).listFiles();
0:         shardPaths = new HashSet<>();
0:         for (CarbonFile carbonFile : carbonFiles) {
0:           shardPaths.add(carbonFile.getAbsolutePath());
0:         }
1:         segmentMap.put(segment.getSegmentNo(), shardPaths);
0:       }
1:       for (String shard : shardPaths) {
0:         bloomDM.init(new BloomDataMapModel(shard, cache,
0:             new HashSet<>(dataMapMeta.getIndexedColumnNames())));
/////////////////////////////////////////////////////////////////////////
0:     bloomCoarseGrainDataMap.init(new BloomDataMapModel(indexPath, cache,
0:         new HashSet<>(dataMapMeta.getIndexedColumnNames())));
/////////////////////////////////////////////////////////////////////////
1:     Set<String> shards = segmentMap.remove(segment.getSegmentNo());
1:     if (shards != null) {
1:       for (String shard : shards) {
1:         for (CarbonColumn carbonColumn : dataMapMeta.getIndexedColumns()) {
1:           cache.invalidate(new BloomCacheKeyValue.CacheKey(shard, carbonColumn.getColName()));
0:         }
0:       }
0:     }
0:     for (String segmentId : segmentMap.keySet().toArray(new String[segmentMap.size()])) {
0:       clear(new Segment(segmentId, null, null));
0:     }
commit:60dfdd3
/////////////////////////////////////////////////////////////////////////
/////////////////////////////////////////////////////////////////////////
0:         String dmPath = CarbonTablePath
0:             .getDataMapStorePath(tablePath, segmentId, dataMap.getDataMapSchema().getDataMapName());
/////////////////////////////////////////////////////////////////////////
0:         String datamapPath = CarbonTablePath
0:             .getDataMapStorePath(getCarbonTable().getTablePath(), segmentId, dataMapName);
commit:77a1110
/////////////////////////////////////////////////////////////////////////
1: import org.apache.carbondata.core.constants.CarbonV3DataFormatConstants;
/////////////////////////////////////////////////////////////////////////
1:    * default size for bloom filter, cardinality of the column.
1:   private static final int DEFAULT_BLOOM_FILTER_SIZE =
0:       CarbonV3DataFormatConstants.NUMBER_OF_ROWS_PER_BLOCKLET_COLUMN_PAGE_DEFAULT;
0:    * default value for fpp of bloom filter is 1%
0:   private static final double DEFAULT_BLOOM_FILTER_FPP = 0.01d;
1: 
1:   /**
1:    * property for compressing bloom while saving to disk.
1:    */
1:   private static final String COMPRESS_BLOOM = "bloom_compress";
1:   /**
1:    * Default value of compressing bloom while save to disk.
1:    */
1:   private static final boolean DEFAULT_BLOOM_COMPRESS = true;
1: 
1:   private boolean bloomCompress;
/////////////////////////////////////////////////////////////////////////
1:     this.bloomCompress = validateAndGetBloomCompress(dataMapSchema);
/////////////////////////////////////////////////////////////////////////
0:   /**
1:    * validate bloom DataMap COMPRESS_BLOOM
1:    * Default value is true
0:    */
1:   private boolean validateAndGetBloomCompress(DataMapSchema dmSchema) {
1:     String bloomCompress = dmSchema.getProperties().get(COMPRESS_BLOOM);
1:     if (StringUtils.isBlank(bloomCompress)) {
1:       LOGGER.warn(
1:           String.format("Bloom compress is not configured for datamap %s, use default value %b",
1:               dataMapName, DEFAULT_BLOOM_COMPRESS));
1:       return DEFAULT_BLOOM_COMPRESS;
0:     }
1:     return Boolean.parseBoolean(bloomCompress);
0:   }
1: 
/////////////////////////////////////////////////////////////////////////
1:         this.bloomFilterSize, this.bloomFilterFpp, bloomCompress);
1:         this.bloomFilterSize, this.bloomFilterFpp, bloomCompress);
commit:8b33ab2
/////////////////////////////////////////////////////////////////////////
/////////////////////////////////////////////////////////////////////////
0:       String dataMapStorePath = BloomDataMapWriter.genDataMapStorePath(
0:           dataMapName);
0:       CarbonFile[] carbonFiles = FileFactory.getCarbonFile(dataMapStorePath).listFiles();
0:       for (CarbonFile carbonFile : carbonFiles) {
0:         BloomCoarseGrainDataMap bloomDM = new BloomCoarseGrainDataMap();
0:         bloomDM.init(new DataMapModel(carbonFile.getAbsolutePath()));
0:         dataMaps.add(bloomDM);
0:       }
1:     } catch (Exception e) {
author:akashrn5
-------------------------------------------------------------------------------
commit:2018048
/////////////////////////////////////////////////////////////////////////
0:     if (segment.getFilteredIndexShardNames().size() == 0) {
0:       for (CarbonFile indexDir : indexDirs) {
0:         DataMapDistributable bloomDataMapDistributable = new BloomDataMapDistributable(
0:             indexDir.getAbsolutePath());
0:         dataMapDistributableList.add(bloomDataMapDistributable);
0:       }
0:       return dataMapDistributableList;
0:     }
commit:5229443
/////////////////////////////////////////////////////////////////////////
1: import org.apache.carbondata.core.features.TableOperation;
/////////////////////////////////////////////////////////////////////////
0: 
0:   @Override public boolean willBecomeStale(TableOperation operation) {
0:     return false;
0:   }
0: 
author:Jacky Li
-------------------------------------------------------------------------------
commit:747be9b
/////////////////////////////////////////////////////////////////////////
1: import org.apache.carbondata.core.datamap.dev.DataMapBuilder;
/////////////////////////////////////////////////////////////////////////
0:   public DataMapBuilder createBuilder(Segment segment, String shardName) throws IOException {
1:     return new BloomDataMapBuilder(getCarbonTable().getTablePath(), this.dataMapName,
commit:9db662a
/////////////////////////////////////////////////////////////////////////
/////////////////////////////////////////////////////////////////////////
0: import org.apache.carbondata.core.datamap.dev.DataMapRefresher;
/////////////////////////////////////////////////////////////////////////
0: /**
1:  * This class is for Bloom Filter for blocklet level
0:  */
1: public class BloomCoarseGrainDataMapFactory extends DataMapFactory<CoarseGrainDataMap> {
/////////////////////////////////////////////////////////////////////////
1:   public BloomCoarseGrainDataMapFactory(CarbonTable carbonTable, DataMapSchema dataMapSchema)
1:       throws MalformedDataMapCommandException {
1:     super(carbonTable, dataMapSchema);
1:     List<CarbonColumn> indexedColumns = carbonTable.getIndexedColumns(dataMapSchema);
/////////////////////////////////////////////////////////////////////////
/////////////////////////////////////////////////////////////////////////
0:   public DataMapWriter createWriter(Segment segment, String shardName) throws IOException {
1:             this.dataMapName, getCarbonTable().getTableName() , shardName));
1:     return new BloomDataMapWriter(getCarbonTable().getTablePath(), this.dataMapName,
0:         this.dataMapMeta.getIndexedColumns(), segment, shardName, this.bloomFilterSize);
0:   }
0: 
0:   @Override
0:   public DataMapRefresher createRefresher(Segment segment, String shardName) throws IOException {
0:     return new BloomDataMapRefresher(getCarbonTable().getTablePath(), this.dataMapName,
0:         this.dataMapMeta.getIndexedColumns(), segment, shardName, this.bloomFilterSize);
0:       String dataMapStorePath = DataMapWriter.getDefaultDataMapPath(
0:           getCarbonTable().getTablePath(), segment.getSegmentNo(), dataMapName);
/////////////////////////////////////////////////////////////////////////
1:     SegmentStatusManager ssm =
1:         new SegmentStatusManager(getCarbonTable().getAbsoluteTableIdentifier());
0:             getCarbonTable().getAbsoluteTableIdentifier().getTablePath(), segmentId)
/////////////////////////////////////////////////////////////////////////
1:   public DataMapLevel getDataMapLevel() {
============================================================================