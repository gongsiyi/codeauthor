1:bbb1092: /*
1:bbb1092:  * Licensed to the Apache Software Foundation (ASF) under one or more
1:bbb1092:  * contributor license agreements.  See the NOTICE file distributed with
1:bbb1092:  * this work for additional information regarding copyright ownership.
1:bbb1092:  * The ASF licenses this file to You under the Apache License, Version 2.0
1:bbb1092:  * (the "License"); you may not use this file except in compliance with
1:bbb1092:  * the License.  You may obtain a copy of the License at
1:bbb1092:  *
1:bbb1092:  *    http://www.apache.org/licenses/LICENSE-2.0
1:bbb1092:  *
1:bbb1092:  * Unless required by applicable law or agreed to in writing, software
1:bbb1092:  * distributed under the License is distributed on an "AS IS" BASIS,
1:bbb1092:  * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
1:bbb1092:  * See the License for the specific language governing permissions and
1:bbb1092:  * limitations under the License.
3:bbb1092:  */
15:bbb1092: 
1:bbb1092: package org.apache.carbondata.datamap.lucene;
1:bbb1092: 
1:bbb1092: import java.io.IOException;
1:f184de8: import java.nio.ByteBuffer;
1:8b33ab2: import java.util.ArrayList;
1:8b33ab2: import java.util.HashMap;
1:8b33ab2: import java.util.List;
1:8b33ab2: import java.util.Map;
1:bbb1092: 
1:bbb1092: import org.apache.carbondata.common.annotations.InterfaceAudience;
1:bbb1092: import org.apache.carbondata.common.logging.LogService;
1:bbb1092: import org.apache.carbondata.common.logging.LogServiceFactory;
1:bbb1092: import org.apache.carbondata.core.datamap.dev.DataMapModel;
1:bbb1092: import org.apache.carbondata.core.datamap.dev.fgdatamap.FineGrainBlocklet;
1:bbb1092: import org.apache.carbondata.core.datamap.dev.fgdatamap.FineGrainDataMap;
1:bbb1092: import org.apache.carbondata.core.datastore.block.SegmentProperties;
1:f184de8: import org.apache.carbondata.core.datastore.filesystem.CarbonFile;
1:bbb1092: import org.apache.carbondata.core.datastore.impl.FileFactory;
1:5397c05: import org.apache.carbondata.core.indexstore.PartitionSpec;
1:f184de8: import org.apache.carbondata.core.metadata.schema.table.DataMapSchema;
1:bbb1092: import org.apache.carbondata.core.scan.expression.Expression;
1:fae457a: import org.apache.carbondata.core.scan.expression.MatchExpression;
1:bbb1092: import org.apache.carbondata.core.scan.filter.intf.ExpressionType;
1:bbb1092: import org.apache.carbondata.core.scan.filter.resolver.FilterResolverIntf;
1:bbb1092: 
1:bbb1092: import org.apache.hadoop.fs.Path;
1:bbb1092: import org.apache.lucene.analysis.Analyzer;
1:bbb1092: import org.apache.lucene.analysis.standard.StandardAnalyzer;
1:bbb1092: import org.apache.lucene.document.Document;
1:bbb1092: import org.apache.lucene.index.DirectoryReader;
1:bbb1092: import org.apache.lucene.index.IndexReader;
1:bbb1092: import org.apache.lucene.index.IndexableField;
1:bbb1092: import org.apache.lucene.queryparser.classic.MultiFieldQueryParser;
1:bbb1092: import org.apache.lucene.queryparser.classic.ParseException;
1:bbb1092: import org.apache.lucene.queryparser.classic.QueryParser;
1:8b33ab2: import org.apache.lucene.search.IndexSearcher;
1:8b33ab2: import org.apache.lucene.search.Query;
1:8b33ab2: import org.apache.lucene.search.ScoreDoc;
1:8b33ab2: import org.apache.lucene.search.TopDocs;
1:bbb1092: import org.apache.lucene.store.Directory;
1:f184de8: import org.apache.lucene.util.BytesRef;
1:bbb1092: import org.apache.solr.store.hdfs.HdfsDirectory;
1:bbb1092: 
1:bbb1092: @InterfaceAudience.Internal
1:bbb1092: public class LuceneFineGrainDataMap extends FineGrainDataMap {
1:bbb1092: 
3:bbb1092:   /**
1:bbb1092:    * log information
1:bbb1092:    */
1:bbb1092:   private static final LogService LOGGER =
1:bbb1092:       LogServiceFactory.getLogService(LuceneFineGrainDataMap.class.getName());
1:860e144: 
1:860e144:   /**
1:0e01197:    * search limit will help in deciding the size of priority queue which is used by lucene to store
1:0e01197:    * the documents in heap. By default it is 100 means in one search max of 10 documents can be
1:0e01197:    * stored in heap by lucene. This way it will help in reducing the GC.
1:0e01197:    * Note: If it is removed or it's value is increased it will lead to almost 90%
1:0e01197:    * of the query time in GC in worst case scenarios if it's value is increased beyond a limit
1:860e144:    */
1:0e01197:   private static final int SEARCH_LIMIT = 100;
1:0e01197: 
1:0e01197:   /**
1:bbb1092:    * searcher object for this datamap
1:0e01197:    */
1:f184de8:   private Map<String, IndexSearcher> indexSearcherMap = null;
1:bbb1092: 
1:bbb1092:   /**
1:bbb1092:    * analyzer for lucene index
1:bbb1092:    */
1:bbb1092:   private Analyzer analyzer;
1:8b33ab2: 
1:9db662a:   private String filePath;
1:bbb1092: 
1:f184de8:   private int writeCacheSize;
1:f184de8: 
1:f184de8:   private boolean storeBlockletWise;
1:f184de8: 
1:07a77fa:   private IndexReader indexReader;
1:07a77fa: 
1:f184de8:   LuceneFineGrainDataMap(Analyzer analyzer, DataMapSchema schema) {
1:bbb1092:     this.analyzer = analyzer;
1:f184de8:     writeCacheSize = LuceneDataMapFactoryBase.validateAndGetWriteCacheSize(schema);
1:f184de8:     storeBlockletWise = LuceneDataMapFactoryBase.validateAndGetStoreBlockletWise(schema);
7:bbb1092:   }
1:bbb1092: 
1:bbb1092:   /**
1:bbb1092:    * It is called to load the data map to memory or to initialize it.
1:bbb1092:    */
1:8b33ab2:   public void init(DataMapModel dataMapModel) throws IOException {
1:f184de8:     long startTime = System.currentTimeMillis();
1:bbb1092:     // get this path from file path
1:bbb1092:     Path indexPath = FileFactory.getPath(dataMapModel.getFilePath());
1:bbb1092: 
1:bbb1092:     LOGGER.info("Lucene index read path " + indexPath.toString());
1:8b33ab2: 
1:9db662a:     this.filePath = indexPath.getName();
1:bbb1092: 
1:f184de8:     this.indexSearcherMap = new HashMap<>();
1:f184de8: 
1:bbb1092:     // get file system , use hdfs file system , realized in solr project
1:f184de8:     CarbonFile indexFilePath = FileFactory.getCarbonFile(indexPath.toString());
1:bbb1092: 
1:bbb1092:     // check this path valid
1:f184de8:     if (!indexFilePath.exists()) {
1:bbb1092:       String errorMessage = String.format("index directory %s not exists.", indexPath);
2:bbb1092:       LOGGER.error(errorMessage);
2:bbb1092:       throw new IOException(errorMessage);
1:0e01197:     }
1:bbb1092: 
1:f184de8:     if (!indexFilePath.isDirectory()) {
1:bbb1092:       String errorMessage = String.format("error index path %s, must be directory", indexPath);
1:bbb1092:       LOGGER.error(errorMessage);
1:bbb1092:       throw new IOException(errorMessage);
1:bbb1092:     }
1:bbb1092: 
1:f184de8:     if (storeBlockletWise) {
1:f184de8:       CarbonFile[] blockletDirs = indexFilePath.listFiles();
1:f184de8:       for (CarbonFile blockletDir : blockletDirs) {
1:f184de8:         IndexSearcher indexSearcher = createIndexSearcher(new Path(blockletDir.getAbsolutePath()));
1:f184de8:         indexSearcherMap.put(blockletDir.getName(), indexSearcher);
1:f184de8:       }
1:f184de8: 
1:f184de8:     } else {
1:f184de8:       IndexSearcher indexSearcher = createIndexSearcher(indexPath);
1:f184de8:       indexSearcherMap.put("-1", indexSearcher);
1:f184de8: 
1:f184de8:     }
1:f184de8:     LOGGER.info(
1:e30a84c:         "Time taken to initialize lucene searcher: " + (System.currentTimeMillis() - startTime));
1:f184de8:   }
1:f184de8: 
1:f184de8:   private IndexSearcher createIndexSearcher(Path indexPath) throws IOException {
1:bbb1092:     // open this index path , use HDFS default configuration
1:bbb1092:     Directory indexDir = new HdfsDirectory(indexPath, FileFactory.getConfiguration());
1:bbb1092: 
1:07a77fa:     this.indexReader = DirectoryReader.open(indexDir);
1:bbb1092:     if (indexReader == null) {
1:bbb1092:       throw new RuntimeException("failed to create index reader object");
1:bbb1092:     }
1:bbb1092: 
1:bbb1092:     // create a index searcher object
1:f184de8:     return new IndexSearcher(indexReader);
1:bbb1092:   }
1:bbb1092: 
1:bbb1092:   /**
1:bbb1092:    * Return the query string in the first TEXT_MATCH expression in the expression tree
1:bbb1092:    */
1:bbb1092:   private String getQueryString(Expression expression) {
1:bbb1092:     if (expression.getFilterExpressionType() == ExpressionType.TEXT_MATCH) {
1:bbb1092:       return expression.getString();
1:bbb1092:     }
1:bbb1092: 
1:bbb1092:     for (Expression child : expression.getChildren()) {
1:bbb1092:       String queryString = getQueryString(child);
1:bbb1092:       if (queryString != null) {
1:bbb1092:         return queryString;
1:bbb1092:       }
1:bbb1092:     }
1:bbb1092:     return null;
1:bbb1092:   }
1:bbb1092: 
1:bbb1092:   /**
1:fae457a:    * Return Maximum records
1:fae457a:    * @return
1:fae457a:    */
1:5cad92f:   private int getMaxDoc(Expression expression) {
1:5cad92f:     if (expression.getFilterExpressionType() == ExpressionType.TEXT_MATCH) {
1:5cad92f:       int maxDoc = ((MatchExpression) expression).getMaxDoc();
1:5cad92f:       if (maxDoc < 0) {
1:5cad92f:         maxDoc = Integer.MAX_VALUE;
1:5cad92f:       }
1:5cad92f:       return maxDoc;
1:5cad92f:     }
1:5cad92f: 
1:5cad92f:     for (Expression child : expression.getChildren()) {
1:5cad92f:       return getMaxDoc(child);
1:5cad92f:     }
1:5cad92f:     return Integer.MAX_VALUE;
1:fae457a:   }
1:fae457a: 
1:fae457a:   /**
1:bbb1092:    * Prune the datamap with filter expression. It returns the list of
1:bbb1092:    * blocklets where these filters can exist.
1:bbb1092:    */
1:07a77fa:   @Override
1:bbb1092:   public List<FineGrainBlocklet> prune(FilterResolverIntf filterExp,
1:5397c05:       SegmentProperties segmentProperties, List<PartitionSpec> partitions) throws IOException {
1:bbb1092: 
1:bbb1092:     // convert filter expr into lucene list query
1:bbb1092:     List<String> fields = new ArrayList<String>();
1:bbb1092: 
1:bbb1092:     // only for test , query all data
1:bbb1092:     String strQuery = getQueryString(filterExp.getFilterExpression());
1:fae457a:     int maxDocs;
1:fae457a:     try {
1:5cad92f:       maxDocs = getMaxDoc(filterExp.getFilterExpression());
1:fae457a:     } catch (NumberFormatException e) {
1:fae457a:       maxDocs = Integer.MAX_VALUE;
1:07a77fa:     }
1:bbb1092: 
1:860e144:     if (null == strQuery) {
1:860e144:       return null;
1:860e144:     }
1:860e144: 
1:bbb1092:     String[] sFields = new String[fields.size()];
1:bbb1092:     fields.toArray(sFields);
1:bbb1092: 
1:bbb1092:     // get analyzer
1:bbb1092:     if (analyzer == null) {
1:bbb1092:       analyzer = new StandardAnalyzer();
1:fae457a:     }
1:bbb1092: 
1:bbb1092:     // use MultiFieldQueryParser to parser query
1:bbb1092:     QueryParser queryParser = new MultiFieldQueryParser(sFields, analyzer);
1:860e144:     queryParser.setAllowLeadingWildcard(true);
1:bbb1092:     Query query;
2:bbb1092:     try {
1:61afa42:       query = queryParser.parse(strQuery);
1:bbb1092:     } catch (ParseException e) {
1:bbb1092:       String errorMessage = String.format(
1:bbb1092:           "failed to filter block with query %s, detail is %s", strQuery, e.getMessage());
1:bbb1092:       LOGGER.error(errorMessage);
1:bbb1092:       return null;
1:bbb1092:     }
1:bbb1092:     // temporary data, delete duplicated data
1:bbb1092:     // Map<BlockId, Map<BlockletId, Map<PageId, Set<RowId>>>>
1:f184de8:     Map<String, Map<Integer, List<Short>>> mapBlocks = new HashMap<>();
1:bbb1092: 
1:0e01197:     long luceneSearchStartTime = System.currentTimeMillis();
1:f184de8:     for (Map.Entry<String, IndexSearcher> searcherEntry : indexSearcherMap.entrySet()) {
1:f184de8:       IndexSearcher indexSearcher = searcherEntry.getValue();
1:0e01197:       // take the min of total documents available in the reader and limit if set by the user
1:0e01197:       maxDocs = Math.min(maxDocs, indexSearcher.getIndexReader().maxDoc());
1:f184de8:       // execute index search
1:5229443:       TopDocs result = null;
1:0e01197:       // the number of documents to be queried in one search. It will always be minimum of
1:0e01197:       // search result and maxDocs
1:0e01197:       int numberOfDocumentsToBeQueried = 0;
1:0e01197:       // counter for maintaining the total number of documents finished querying
1:0e01197:       int documentHitCounter = 0;
1:f184de8:       try {
1:0e01197:         numberOfDocumentsToBeQueried = Math.min(maxDocs, SEARCH_LIMIT);
1:0e01197:         result = indexSearcher.search(query, numberOfDocumentsToBeQueried);
1:0e01197:         documentHitCounter += numberOfDocumentsToBeQueried;
1:f184de8:       } catch (IOException e) {
1:f184de8:         String errorMessage =
1:f184de8:             String.format("failed to search lucene data, detail is %s", e.getMessage());
1:f184de8:         LOGGER.error(errorMessage);
1:f184de8:         throw new IOException(errorMessage);
1:bbb1092:       }
1:f184de8: 
1:f184de8:       ByteBuffer intBuffer = ByteBuffer.allocate(4);
1:0e01197:       // last scoreDoc in a result to be used in searchAfter API
1:0e01197:       ScoreDoc lastScoreDoc = null;
1:0e01197:       while (true) {
1:0e01197:         for (ScoreDoc scoreDoc : result.scoreDocs) {
1:0e01197:           // get a document
1:0e01197:           Document doc = indexSearcher.doc(scoreDoc.doc);
1:0e01197:           // get all fields
1:0e01197:           List<IndexableField> fieldsInDoc = doc.getFields();
1:0e01197:           if (writeCacheSize > 0) {
1:0e01197:             // It fills rowids to the map, its value is combined with multiple rows.
1:0e01197:             fillMapForCombineRows(intBuffer, mapBlocks, fieldsInDoc, searcherEntry.getKey());
1:0e01197:           } else {
1:0e01197:             // Fill rowids to the map
1:0e01197:             fillMap(intBuffer, mapBlocks, fieldsInDoc, searcherEntry.getKey());
1:0e01197:           }
1:0e01197:           lastScoreDoc = scoreDoc;
1:f184de8:         }
1:0e01197:         // result will have the total number of hits therefore we always need to query on the
1:0e01197:         // left over documents
1:0e01197:         int remainingHits = result.totalHits - documentHitCounter;
1:0e01197:         // break the loop if count reaches maxDocs to be searched or remaining hits become <=0
1:0e01197:         if (remainingHits <= 0 || documentHitCounter >= maxDocs) {
1:0e01197:           break;
1:0e01197:         }
1:0e01197:         numberOfDocumentsToBeQueried = Math.min(remainingHits, SEARCH_LIMIT);
1:0e01197:         result = indexSearcher.searchAfter(lastScoreDoc, query, numberOfDocumentsToBeQueried);
1:0e01197:         documentHitCounter += numberOfDocumentsToBeQueried;
1:f184de8:       }
1:bbb1092:     }
1:0e01197:     LOGGER.info(
1:0e01197:         "Time taken for lucene search: " + (System.currentTimeMillis() - luceneSearchStartTime)
1:0e01197:             + " ms");
1:bbb1092: 
1:bbb1092:     // result blocklets
1:bbb1092:     List<FineGrainBlocklet> blocklets = new ArrayList<>();
1:bbb1092: 
1:bbb1092:     // transform all blocks into result type blocklets
1:bbb1092:     // Map<BlockId, Map<BlockletId, Map<PageId, Set<RowId>>>>
1:f184de8:     for (Map.Entry<String, Map<Integer, List<Short>>> mapBlocklet :
1:bbb1092:         mapBlocks.entrySet()) {
1:8b33ab2:       String blockletId = mapBlocklet.getKey();
1:f184de8:       Map<Integer, List<Short>> mapPageIds = mapBlocklet.getValue();
1:8b33ab2:       List<FineGrainBlocklet.Page> pages = new ArrayList<FineGrainBlocklet.Page>();
1:bbb1092: 
1:8b33ab2:       // for pages in this blocklet Map<PageId, Set<RowId>>>
1:f184de8:       for (Map.Entry<Integer, List<Short>> mapPageId : mapPageIds.entrySet()) {
1:8b33ab2:         // construct array rowid
1:8b33ab2:         int[] rowIds = new int[mapPageId.getValue().size()];
1:8b33ab2:         int i = 0;
1:8b33ab2:         // for rowids in this page Set<RowId>
1:f184de8:         for (Short rowid : mapPageId.getValue()) {
1:8b33ab2:           rowIds[i++] = rowid;
1:bbb1092:         }
1:8b33ab2:         // construct one page
1:8b33ab2:         FineGrainBlocklet.Page page = new FineGrainBlocklet.Page();
1:8b33ab2:         page.setPageId(mapPageId.getKey());
1:8b33ab2:         page.setRowId(rowIds);
1:bbb1092: 
1:8b33ab2:         // add this page into list pages
1:8b33ab2:         pages.add(page);
1:bbb1092:       }
1:8b33ab2: 
1:8b33ab2:       // add a FineGrainBlocklet
1:9db662a:       blocklets.add(new FineGrainBlocklet(filePath, blockletId, pages));
1:bbb1092:     }
1:bbb1092: 
1:bbb1092:     return blocklets;
1:bbb1092:   }
1:bbb1092: 
1:f184de8:   /**
1:f184de8:    * It fills the rowids to the map, its value is combined with multiple rowids as we store group
1:f184de8:    * rows and combine as per there uniqueness.
1:f184de8:    */
1:f184de8:   private void fillMapForCombineRows(ByteBuffer intBuffer,
1:f184de8:       Map<String, Map<Integer, List<Short>>> mapBlocks, List<IndexableField> fieldsInDoc,
1:f184de8:       String blockletId) {
1:f184de8:     for (int i = 0; i < fieldsInDoc.size(); i++) {
1:f184de8:       BytesRef bytesRef = fieldsInDoc.get(i).binaryValue();
1:f184de8:       ByteBuffer buffer = ByteBuffer.wrap(bytesRef.bytes);
1:f184de8: 
1:f184de8:       int pageId;
1:f184de8:       if (storeBlockletWise) {
1:f184de8:         // If we store as per blockletwise then just read pageid only we don't store blockletid
1:f184de8:         pageId = buffer.getShort();
1:f184de8:       } else {
1:f184de8:         int combineKey = buffer.getInt();
1:f184de8:         intBuffer.clear();
1:f184de8:         intBuffer.putInt(combineKey);
1:f184de8:         intBuffer.rewind();
1:f184de8:         blockletId = String.valueOf(intBuffer.getShort());
1:f184de8:         pageId = intBuffer.getShort();
1:f184de8:       }
1:f184de8: 
1:f184de8:       Map<Integer, List<Short>> mapPageIds = mapBlocks.get(blockletId);
1:f184de8:       if (mapPageIds == null) {
1:f184de8:         mapPageIds = new HashMap<>();
1:f184de8:         mapBlocks.put(blockletId, mapPageIds);
1:f184de8:       }
1:f184de8:       List<Short> setRowId = mapPageIds.get(pageId);
1:f184de8:       if (setRowId == null) {
1:f184de8:         setRowId = new ArrayList<>();
1:f184de8:         mapPageIds.put(pageId, setRowId);
1:f184de8:       }
1:f184de8: 
1:f184de8:       while (buffer.hasRemaining()) {
1:f184de8:         setRowId.add(buffer.getShort());
1:f184de8:       }
1:f184de8:     }
1:f184de8:   }
1:f184de8: 
1:f184de8:   /**
1:f184de8:    * Fill the map with rowids from documents
1:f184de8:    */
1:f184de8:   private void fillMap(ByteBuffer intBuffer, Map<String, Map<Integer, List<Short>>> mapBlocks,
1:f184de8:       List<IndexableField> fieldsInDoc, String blockletId) {
1:f184de8:     int combineKey = fieldsInDoc.get(0).numericValue().intValue();
1:f184de8:     intBuffer.clear();
1:f184de8:     intBuffer.putInt(combineKey);
1:f184de8:     intBuffer.rewind();
1:f184de8:     short rowId;
1:f184de8:     int pageId;
1:f184de8:     if (storeBlockletWise) {
1:f184de8:       // If we store as per blockletwise then just read pageid and rowid
1:f184de8:       // only we don't store blockletid
1:f184de8:       pageId = intBuffer.getShort();
1:f184de8:       rowId = intBuffer.getShort();
2:f184de8:     } else {
1:f184de8:       blockletId = String.valueOf(intBuffer.getShort());
1:f184de8:       pageId = intBuffer.getShort();
1:f184de8:       rowId = fieldsInDoc.get(1).numericValue().shortValue();
1:f184de8:     }
1:f184de8:     Map<Integer, List<Short>> mapPageIds = mapBlocks.get(blockletId);
1:f184de8:     if (mapPageIds == null) {
1:f184de8:       mapPageIds = new HashMap<>();
1:f184de8:       mapBlocks.put(blockletId, mapPageIds);
1:f184de8:     }
1:f184de8:     List<Short> setRowId = mapPageIds.get(pageId);
1:f184de8:     if (setRowId == null) {
1:f184de8:       setRowId = new ArrayList<>();
1:f184de8:       mapPageIds.put(pageId, setRowId);
1:f184de8:     }
1:f184de8:     setRowId.add(rowId);
1:f184de8:   }
1:f184de8: 
1:bbb1092:   @Override
1:bbb1092:   public boolean isScanRequired(FilterResolverIntf filterExp) {
1:bbb1092:     return true;
1:bbb1092:   }
1:bbb1092: 
1:bbb1092:   /**
1:bbb1092:    * Clear complete index table and release memory.
1:bbb1092:    */
1:bbb1092:   @Override
1:bbb1092:   public void clear() {
1:bbb1092: 
1:bbb1092:   }
1:bbb1092: 
1:bbb1092:   @Override
1:07a77fa:   public void finish() {
1:07a77fa:     if (null != indexReader) {
1:07a77fa:       try {
1:07a77fa:         int referenceCount = indexReader.getRefCount();
1:07a77fa:         if (referenceCount > 0) {
1:07a77fa:           indexReader.decRef();
1:07a77fa:           if (null != indexSearcherMap) {
1:07a77fa:             indexSearcherMap.clear();
1:07a77fa:           }
1:07a77fa:         }
1:07a77fa:       } catch (IOException e) {
1:07a77fa:         LOGGER.error(e, "Ignoring the exception, Error while closing the lucene index reader");
1:07a77fa:       }
1:07a77fa:     }
1:07a77fa:   }
1:bbb1092: }
============================================================================
author:xubo245
-------------------------------------------------------------------------------
commit:e30a84c
/////////////////////////////////////////////////////////////////////////
1:         "Time taken to initialize lucene searcher: " + (System.currentTimeMillis() - startTime));
author:akashrn5
-------------------------------------------------------------------------------
commit:07a77fa
/////////////////////////////////////////////////////////////////////////
1:   private IndexReader indexReader;
1: 
/////////////////////////////////////////////////////////////////////////
1:     this.indexReader = DirectoryReader.open(indexDir);
/////////////////////////////////////////////////////////////////////////
/////////////////////////////////////////////////////////////////////////
1:   @Override
1:   public void finish() {
1:     if (null != indexReader) {
1:       try {
1:         int referenceCount = indexReader.getRefCount();
1:         if (referenceCount > 0) {
1:           indexReader.decRef();
1:           if (null != indexSearcherMap) {
1:             indexSearcherMap.clear();
1:           }
1:         }
1:       } catch (IOException e) {
1:         LOGGER.error(e, "Ignoring the exception, Error while closing the lucene index reader");
1:       }
1:     }
1:   }
1: }
commit:5229443
/////////////////////////////////////////////////////////////////////////
0:     // initialize to null, else ScoreDoc objects will get accumulated in memory
1:     TopDocs result = null;
commit:860e144
/////////////////////////////////////////////////////////////////////////
0:    * index Reader object to create searcher object
1:    */
0:   private IndexReader indexReader = null;
1: 
1:   /**
/////////////////////////////////////////////////////////////////////////
0:     indexReader = DirectoryReader.open(indexDir);
/////////////////////////////////////////////////////////////////////////
1:     if (null == strQuery) {
1:       return null;
1:     }
1: 
/////////////////////////////////////////////////////////////////////////
1:     queryParser.setAllowLeadingWildcard(true);
0:       // always send lowercase string to lucene as it is case sensitive
0:       query = queryParser.parse(strQuery.toLowerCase());
/////////////////////////////////////////////////////////////////////////
0:       result = indexSearcher.search(query, indexReader.maxDoc());
author:manishgupta88
-------------------------------------------------------------------------------
commit:0e01197
/////////////////////////////////////////////////////////////////////////
1:    * search limit will help in deciding the size of priority queue which is used by lucene to store
1:    * the documents in heap. By default it is 100 means in one search max of 10 documents can be
1:    * stored in heap by lucene. This way it will help in reducing the GC.
1:    * Note: If it is removed or it's value is increased it will lead to almost 90%
1:    * of the query time in GC in worst case scenarios if it's value is increased beyond a limit
1:    */
1:   private static final int SEARCH_LIMIT = 100;
1: 
1:   /**
/////////////////////////////////////////////////////////////////////////
1:     long luceneSearchStartTime = System.currentTimeMillis();
1:       // take the min of total documents available in the reader and limit if set by the user
1:       maxDocs = Math.min(maxDocs, indexSearcher.getIndexReader().maxDoc());
1:       // the number of documents to be queried in one search. It will always be minimum of
1:       // search result and maxDocs
1:       int numberOfDocumentsToBeQueried = 0;
1:       // counter for maintaining the total number of documents finished querying
1:       int documentHitCounter = 0;
1:         numberOfDocumentsToBeQueried = Math.min(maxDocs, SEARCH_LIMIT);
1:         result = indexSearcher.search(query, numberOfDocumentsToBeQueried);
1:         documentHitCounter += numberOfDocumentsToBeQueried;
/////////////////////////////////////////////////////////////////////////
1:       // last scoreDoc in a result to be used in searchAfter API
1:       ScoreDoc lastScoreDoc = null;
1:       while (true) {
1:         for (ScoreDoc scoreDoc : result.scoreDocs) {
1:           // get a document
1:           Document doc = indexSearcher.doc(scoreDoc.doc);
1:           // get all fields
1:           List<IndexableField> fieldsInDoc = doc.getFields();
1:           if (writeCacheSize > 0) {
1:             // It fills rowids to the map, its value is combined with multiple rows.
1:             fillMapForCombineRows(intBuffer, mapBlocks, fieldsInDoc, searcherEntry.getKey());
1:           } else {
1:             // Fill rowids to the map
1:             fillMap(intBuffer, mapBlocks, fieldsInDoc, searcherEntry.getKey());
1:           }
1:           lastScoreDoc = scoreDoc;
1:         // result will have the total number of hits therefore we always need to query on the
1:         // left over documents
1:         int remainingHits = result.totalHits - documentHitCounter;
1:         // break the loop if count reaches maxDocs to be searched or remaining hits become <=0
1:         if (remainingHits <= 0 || documentHitCounter >= maxDocs) {
1:           break;
1:         }
1:         numberOfDocumentsToBeQueried = Math.min(remainingHits, SEARCH_LIMIT);
1:         result = indexSearcher.searchAfter(lastScoreDoc, query, numberOfDocumentsToBeQueried);
1:         documentHitCounter += numberOfDocumentsToBeQueried;
1:     LOGGER.info(
1:         "Time taken for lucene search: " + (System.currentTimeMillis() - luceneSearchStartTime)
1:             + " ms");
/////////////////////////////////////////////////////////////////////////
1: }
author:ravipesala
-------------------------------------------------------------------------------
commit:f184de8
/////////////////////////////////////////////////////////////////////////
1: import java.nio.ByteBuffer;
/////////////////////////////////////////////////////////////////////////
1: import org.apache.carbondata.core.datastore.filesystem.CarbonFile;
1: import org.apache.carbondata.core.metadata.schema.table.DataMapSchema;
/////////////////////////////////////////////////////////////////////////
1: import org.apache.lucene.util.BytesRef;
/////////////////////////////////////////////////////////////////////////
1:   private Map<String, IndexSearcher> indexSearcherMap = null;
/////////////////////////////////////////////////////////////////////////
1:   private int writeCacheSize;
1: 
1:   private boolean storeBlockletWise;
1: 
1:   LuceneFineGrainDataMap(Analyzer analyzer, DataMapSchema schema) {
1:     writeCacheSize = LuceneDataMapFactoryBase.validateAndGetWriteCacheSize(schema);
1:     storeBlockletWise = LuceneDataMapFactoryBase.validateAndGetStoreBlockletWise(schema);
1:     long startTime = System.currentTimeMillis();
/////////////////////////////////////////////////////////////////////////
1:     this.indexSearcherMap = new HashMap<>();
1: 
1:     CarbonFile indexFilePath = FileFactory.getCarbonFile(indexPath.toString());
1:     if (!indexFilePath.exists()) {
1:     if (!indexFilePath.isDirectory()) {
1:     if (storeBlockletWise) {
1:       CarbonFile[] blockletDirs = indexFilePath.listFiles();
1:       for (CarbonFile blockletDir : blockletDirs) {
1:         IndexSearcher indexSearcher = createIndexSearcher(new Path(blockletDir.getAbsolutePath()));
1:         indexSearcherMap.put(blockletDir.getName(), indexSearcher);
1:       }
1: 
1:     } else {
1:       IndexSearcher indexSearcher = createIndexSearcher(indexPath);
1:       indexSearcherMap.put("-1", indexSearcher);
1: 
1:     }
1:     LOGGER.info(
0:         "Time taken to intialize lucene searcher: " + (System.currentTimeMillis() - startTime));
1:   }
1: 
1:   private IndexSearcher createIndexSearcher(Path indexPath) throws IOException {
0:     IndexReader indexReader = DirectoryReader.open(indexDir);
1:     return new IndexSearcher(indexReader);
/////////////////////////////////////////////////////////////////////////
1:     Map<String, Map<Integer, List<Short>>> mapBlocks = new HashMap<>();
1:     for (Map.Entry<String, IndexSearcher> searcherEntry : indexSearcherMap.entrySet()) {
1:       IndexSearcher indexSearcher = searcherEntry.getValue();
1:       // execute index search
0:       // initialize to null, else ScoreDoc objects will get accumulated in memory
0:       TopDocs result = null;
1:       try {
0:         result = indexSearcher.search(query, maxDocs);
1:       } catch (IOException e) {
1:         String errorMessage =
1:             String.format("failed to search lucene data, detail is %s", e.getMessage());
1:         LOGGER.error(errorMessage);
1:         throw new IOException(errorMessage);
1:       ByteBuffer intBuffer = ByteBuffer.allocate(4);
0:       for (ScoreDoc scoreDoc : result.scoreDocs) {
0:         // get a document
0:         Document doc = indexSearcher.doc(scoreDoc.doc);
1: 
0:         // get all fields
0:         List<IndexableField> fieldsInDoc = doc.getFields();
0:         if (writeCacheSize > 0) {
0:           // It fills rowids to the map, its value is combined with multiple rows.
0:           fillMapForCombineRows(intBuffer, mapBlocks, fieldsInDoc, searcherEntry.getKey());
1:         } else {
0:           // Fill rowids to the map
0:           fillMap(intBuffer, mapBlocks, fieldsInDoc, searcherEntry.getKey());
1:         }
1:       }
/////////////////////////////////////////////////////////////////////////
1:     for (Map.Entry<String, Map<Integer, List<Short>>> mapBlocklet :
1:       Map<Integer, List<Short>> mapPageIds = mapBlocklet.getValue();
1:       for (Map.Entry<Integer, List<Short>> mapPageId : mapPageIds.entrySet()) {
1:         for (Short rowid : mapPageId.getValue()) {
/////////////////////////////////////////////////////////////////////////
1:   /**
1:    * It fills the rowids to the map, its value is combined with multiple rowids as we store group
1:    * rows and combine as per there uniqueness.
1:    */
1:   private void fillMapForCombineRows(ByteBuffer intBuffer,
1:       Map<String, Map<Integer, List<Short>>> mapBlocks, List<IndexableField> fieldsInDoc,
1:       String blockletId) {
1:     for (int i = 0; i < fieldsInDoc.size(); i++) {
1:       BytesRef bytesRef = fieldsInDoc.get(i).binaryValue();
1:       ByteBuffer buffer = ByteBuffer.wrap(bytesRef.bytes);
1: 
1:       int pageId;
1:       if (storeBlockletWise) {
1:         // If we store as per blockletwise then just read pageid only we don't store blockletid
1:         pageId = buffer.getShort();
1:       } else {
1:         int combineKey = buffer.getInt();
1:         intBuffer.clear();
1:         intBuffer.putInt(combineKey);
1:         intBuffer.rewind();
1:         blockletId = String.valueOf(intBuffer.getShort());
1:         pageId = intBuffer.getShort();
1:       }
1: 
1:       Map<Integer, List<Short>> mapPageIds = mapBlocks.get(blockletId);
1:       if (mapPageIds == null) {
1:         mapPageIds = new HashMap<>();
1:         mapBlocks.put(blockletId, mapPageIds);
1:       }
1:       List<Short> setRowId = mapPageIds.get(pageId);
1:       if (setRowId == null) {
1:         setRowId = new ArrayList<>();
1:         mapPageIds.put(pageId, setRowId);
1:       }
1: 
1:       while (buffer.hasRemaining()) {
1:         setRowId.add(buffer.getShort());
1:       }
1:     }
1:   }
1: 
1:   /**
1:    * Fill the map with rowids from documents
1:    */
1:   private void fillMap(ByteBuffer intBuffer, Map<String, Map<Integer, List<Short>>> mapBlocks,
1:       List<IndexableField> fieldsInDoc, String blockletId) {
1:     int combineKey = fieldsInDoc.get(0).numericValue().intValue();
1:     intBuffer.clear();
1:     intBuffer.putInt(combineKey);
1:     intBuffer.rewind();
1:     short rowId;
1:     int pageId;
1:     if (storeBlockletWise) {
1:       // If we store as per blockletwise then just read pageid and rowid
1:       // only we don't store blockletid
1:       pageId = intBuffer.getShort();
1:       rowId = intBuffer.getShort();
1:     } else {
1:       blockletId = String.valueOf(intBuffer.getShort());
1:       pageId = intBuffer.getShort();
1:       rowId = fieldsInDoc.get(1).numericValue().shortValue();
1:     }
1:     Map<Integer, List<Short>> mapPageIds = mapBlocks.get(blockletId);
1:     if (mapPageIds == null) {
1:       mapPageIds = new HashMap<>();
1:       mapBlocks.put(blockletId, mapPageIds);
1:     }
1:     List<Short> setRowId = mapPageIds.get(pageId);
1:     if (setRowId == null) {
1:       setRowId = new ArrayList<>();
1:       mapPageIds.put(pageId, setRowId);
1:     }
1:     setRowId.add(rowId);
1:   }
1: 
commit:5cad92f
/////////////////////////////////////////////////////////////////////////
1:   private int getMaxDoc(Expression expression) {
1:     if (expression.getFilterExpressionType() == ExpressionType.TEXT_MATCH) {
1:       int maxDoc = ((MatchExpression) expression).getMaxDoc();
1:       if (maxDoc < 0) {
1:         maxDoc = Integer.MAX_VALUE;
1:       }
1:       return maxDoc;
1:     }
1: 
1:     for (Expression child : expression.getChildren()) {
1:       return getMaxDoc(child);
1:     }
1:     return Integer.MAX_VALUE;
/////////////////////////////////////////////////////////////////////////
1:       maxDocs = getMaxDoc(filterExp.getFilterExpression());
commit:8b33ab2
/////////////////////////////////////////////////////////////////////////
1: import java.util.ArrayList;
1: import java.util.HashMap;
0: import java.util.HashSet;
1: import java.util.List;
1: import java.util.Map;
0: import java.util.Set;
/////////////////////////////////////////////////////////////////////////
/////////////////////////////////////////////////////////////////////////
1: import org.apache.lucene.search.IndexSearcher;
1: import org.apache.lucene.search.Query;
1: import org.apache.lucene.search.ScoreDoc;
1: import org.apache.lucene.search.TopDocs;
0:   private static final int BLOCKLETID_ID = 0;
0:   private static final int PAGEID_ID = 1;
0:   private static final int ROWID_ID = 2;
/////////////////////////////////////////////////////////////////////////
0:   private String taskName;
1: 
/////////////////////////////////////////////////////////////////////////
1:   public void init(DataMapModel dataMapModel) throws IOException {
0:     this.taskName = indexPath.getName();
1: 
/////////////////////////////////////////////////////////////////////////
0:     Map<String, Map<Integer, Set<Integer>>> mapBlocks = new HashMap<>();
/////////////////////////////////////////////////////////////////////////
0:       Map<Integer, Set<Integer>> mapPageIds = mapBlocks.get(blockletId);
0:         mapBlocks.put(blockletId, mapPageIds);
/////////////////////////////////////////////////////////////////////////
0:     for (Map.Entry<String, Map<Integer, Set<Integer>>> mapBlocklet :
1:       String blockletId = mapBlocklet.getKey();
0:       Map<Integer, Set<Integer>> mapPageIds = mapBlocklet.getValue();
1:       List<FineGrainBlocklet.Page> pages = new ArrayList<FineGrainBlocklet.Page>();
1:       // for pages in this blocklet Map<PageId, Set<RowId>>>
0:       for (Map.Entry<Integer, Set<Integer>> mapPageId : mapPageIds.entrySet()) {
1:         // construct array rowid
1:         int[] rowIds = new int[mapPageId.getValue().size()];
1:         int i = 0;
1:         // for rowids in this page Set<RowId>
0:         for (Integer rowid : mapPageId.getValue()) {
1:           rowIds[i++] = rowid;
1:         // construct one page
1:         FineGrainBlocklet.Page page = new FineGrainBlocklet.Page();
1:         page.setPageId(mapPageId.getKey());
1:         page.setRowId(rowIds);
1:         // add this page into list pages
1:         pages.add(page);
1: 
1:       // add a FineGrainBlocklet
0:       blocklets.add(new FineGrainBlocklet(taskName, blockletId, pages));
author:Indhumathi27
-------------------------------------------------------------------------------
commit:61afa42
/////////////////////////////////////////////////////////////////////////
1:       query = queryParser.parse(strQuery);
commit:fae457a
/////////////////////////////////////////////////////////////////////////
1: import org.apache.carbondata.core.scan.expression.MatchExpression;
/////////////////////////////////////////////////////////////////////////
1:    * Return Maximum records
1:    * @return
1:    */
0:   private int getMaxDoc() {
0:     return Integer.parseInt(MatchExpression.maxDoc);
1:   }
1: 
1:   /**
/////////////////////////////////////////////////////////////////////////
1:     int maxDocs;
1:     try {
0:       maxDocs = getMaxDoc();
1:     } catch (NumberFormatException e) {
1:       maxDocs = Integer.MAX_VALUE;
1:     }
/////////////////////////////////////////////////////////////////////////
0:       result = indexSearcher.search(query, maxDocs);
author:Jacky Li
-------------------------------------------------------------------------------
commit:9db662a
/////////////////////////////////////////////////////////////////////////
1:   private String filePath;
/////////////////////////////////////////////////////////////////////////
1:     this.filePath = indexPath.getName();
/////////////////////////////////////////////////////////////////////////
1:       blocklets.add(new FineGrainBlocklet(filePath, blockletId, pages));
commit:bbb1092
/////////////////////////////////////////////////////////////////////////
1: /*
1:  * Licensed to the Apache Software Foundation (ASF) under one or more
1:  * contributor license agreements.  See the NOTICE file distributed with
1:  * this work for additional information regarding copyright ownership.
1:  * The ASF licenses this file to You under the Apache License, Version 2.0
1:  * (the "License"); you may not use this file except in compliance with
1:  * the License.  You may obtain a copy of the License at
1:  *
1:  *    http://www.apache.org/licenses/LICENSE-2.0
1:  *
1:  * Unless required by applicable law or agreed to in writing, software
1:  * distributed under the License is distributed on an "AS IS" BASIS,
1:  * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
1:  * See the License for the specific language governing permissions and
1:  * limitations under the License.
1:  */
1: 
1: package org.apache.carbondata.datamap.lucene;
1: 
1: import java.io.IOException;
0: import java.util.*;
1: 
1: import org.apache.carbondata.common.annotations.InterfaceAudience;
1: import org.apache.carbondata.common.logging.LogService;
1: import org.apache.carbondata.common.logging.LogServiceFactory;
1: import org.apache.carbondata.core.datamap.dev.DataMapModel;
1: import org.apache.carbondata.core.datamap.dev.fgdatamap.FineGrainBlocklet;
1: import org.apache.carbondata.core.datamap.dev.fgdatamap.FineGrainDataMap;
1: import org.apache.carbondata.core.datastore.block.SegmentProperties;
1: import org.apache.carbondata.core.datastore.impl.FileFactory;
0: import org.apache.carbondata.core.memory.MemoryException;
1: import org.apache.carbondata.core.scan.expression.Expression;
1: import org.apache.carbondata.core.scan.filter.intf.ExpressionType;
1: import org.apache.carbondata.core.scan.filter.resolver.FilterResolverIntf;
1: 
0: import org.apache.hadoop.fs.FileSystem;
1: import org.apache.hadoop.fs.Path;
1: import org.apache.lucene.analysis.Analyzer;
1: import org.apache.lucene.analysis.standard.StandardAnalyzer;
1: import org.apache.lucene.document.Document;
1: import org.apache.lucene.index.DirectoryReader;
1: import org.apache.lucene.index.IndexReader;
1: import org.apache.lucene.index.IndexableField;
1: import org.apache.lucene.queryparser.classic.MultiFieldQueryParser;
1: import org.apache.lucene.queryparser.classic.ParseException;
1: import org.apache.lucene.queryparser.classic.QueryParser;
0: import org.apache.lucene.search.*;
1: import org.apache.lucene.store.Directory;
1: import org.apache.solr.store.hdfs.HdfsDirectory;
1: 
1: @InterfaceAudience.Internal
1: public class LuceneFineGrainDataMap extends FineGrainDataMap {
1: 
0:   private static final int BLOCKID_ID = 0;
1: 
0:   private static final int BLOCKLETID_ID = 1;
1: 
0:   private static final int PAGEID_ID = 2;
1: 
0:   private static final int ROWID_ID = 3;
1: 
1:   /**
1:    * log information
1:    */
1:   private static final LogService LOGGER =
1:       LogServiceFactory.getLogService(LuceneFineGrainDataMap.class.getName());
1: 
1:   /**
1:    * searcher object for this datamap
1:    */
0:   private IndexSearcher indexSearcher = null;
1: 
1:   /**
0:    * default max values to return
1:    */
0:   private static int MAX_RESULT_NUMBER = 100;
1: 
1:   /**
1:    * analyzer for lucene index
1:    */
1:   private Analyzer analyzer;
1: 
0:   LuceneFineGrainDataMap(Analyzer analyzer) {
1:     this.analyzer = analyzer;
1:   }
1: 
1:   /**
1:    * It is called to load the data map to memory or to initialize it.
1:    */
0:   public void init(DataMapModel dataMapModel) throws MemoryException, IOException {
1:     // get this path from file path
1:     Path indexPath = FileFactory.getPath(dataMapModel.getFilePath());
1: 
1:     LOGGER.info("Lucene index read path " + indexPath.toString());
1: 
1:     // get file system , use hdfs file system , realized in solr project
0:     FileSystem fs = FileFactory.getFileSystem(indexPath);
1: 
1:     // check this path valid
0:     if (!fs.exists(indexPath)) {
1:       String errorMessage = String.format("index directory %s not exists.", indexPath);
1:       LOGGER.error(errorMessage);
1:       throw new IOException(errorMessage);
1:     }
1: 
0:     if (!fs.isDirectory(indexPath)) {
1:       String errorMessage = String.format("error index path %s, must be directory", indexPath);
1:       LOGGER.error(errorMessage);
1:       throw new IOException(errorMessage);
1:     }
1: 
1:     // open this index path , use HDFS default configuration
1:     Directory indexDir = new HdfsDirectory(indexPath, FileFactory.getConfiguration());
1: 
0:     IndexReader indexReader = DirectoryReader.open(indexDir);
1:     if (indexReader == null) {
1:       throw new RuntimeException("failed to create index reader object");
1:     }
1: 
1:     // create a index searcher object
0:     indexSearcher = new IndexSearcher(indexReader);
1:   }
1: 
1:   /**
1:    * Return the query string in the first TEXT_MATCH expression in the expression tree
1:    */
1:   private String getQueryString(Expression expression) {
1:     if (expression.getFilterExpressionType() == ExpressionType.TEXT_MATCH) {
1:       return expression.getString();
1:     }
1: 
1:     for (Expression child : expression.getChildren()) {
1:       String queryString = getQueryString(child);
1:       if (queryString != null) {
1:         return queryString;
1:       }
1:     }
1:     return null;
1:   }
1: 
1:   /**
1:    * Prune the datamap with filter expression. It returns the list of
1:    * blocklets where these filters can exist.
1:    */
1:   @Override
1:   public List<FineGrainBlocklet> prune(FilterResolverIntf filterExp,
0:       SegmentProperties segmentProperties, List<String> partitions) throws IOException {
1: 
1:     // convert filter expr into lucene list query
1:     List<String> fields = new ArrayList<String>();
1: 
1:     // only for test , query all data
1:     String strQuery = getQueryString(filterExp.getFilterExpression());
1: 
1:     String[] sFields = new String[fields.size()];
1:     fields.toArray(sFields);
1: 
1:     // get analyzer
1:     if (analyzer == null) {
1:       analyzer = new StandardAnalyzer();
1:     }
1: 
1:     // use MultiFieldQueryParser to parser query
1:     QueryParser queryParser = new MultiFieldQueryParser(sFields, analyzer);
1:     Query query;
1:     try {
0:       query = queryParser.parse(strQuery);
1:     } catch (ParseException e) {
1:       String errorMessage = String.format(
1:           "failed to filter block with query %s, detail is %s", strQuery, e.getMessage());
1:       LOGGER.error(errorMessage);
1:       return null;
1:     }
1: 
0:     // execute index search
0:     TopDocs result;
1:     try {
0:       result = indexSearcher.search(query, MAX_RESULT_NUMBER);
0:     } catch (IOException e) {
0:       String errorMessage =
0:           String.format("failed to search lucene data, detail is %s", e.getMessage());
1:       LOGGER.error(errorMessage);
1:       throw new IOException(errorMessage);
1:     }
1: 
1:     // temporary data, delete duplicated data
1:     // Map<BlockId, Map<BlockletId, Map<PageId, Set<RowId>>>>
0:     Map<String, Map<String, Map<Integer, Set<Integer>>>> mapBlocks = new HashMap<>();
1: 
0:     for (ScoreDoc scoreDoc : result.scoreDocs) {
0:       // get a document
0:       Document doc = indexSearcher.doc(scoreDoc.doc);
1: 
0:       // get all fields
0:       List<IndexableField> fieldsInDoc = doc.getFields();
1: 
0:       // get this block id Map<BlockId, Map<BlockletId, Map<PageId, Set<RowId>>>>
0:       String blockId = fieldsInDoc.get(BLOCKID_ID).stringValue();
0:       Map<String, Map<Integer, Set<Integer>>> mapBlocklets = mapBlocks.get(blockId);
0:       if (mapBlocklets == null) {
0:         mapBlocklets = new HashMap<>();
0:         mapBlocks.put(blockId, mapBlocklets);
1:       }
1: 
0:       // get the blocklet id Map<BlockletId, Map<PageId, Set<RowId>>>
0:       String blockletId = fieldsInDoc.get(BLOCKLETID_ID).stringValue();
0:       Map<Integer, Set<Integer>> mapPageIds = mapBlocklets.get(blockletId);
0:       if (mapPageIds == null) {
0:         mapPageIds = new HashMap<>();
0:         mapBlocklets.put(blockletId, mapPageIds);
1:       }
1: 
0:       // get the page id Map<PageId, Set<RowId>>
0:       Number pageId = fieldsInDoc.get(PAGEID_ID).numericValue();
0:       Set<Integer> setRowId = mapPageIds.get(pageId.intValue());
0:       if (setRowId == null) {
0:         setRowId = new HashSet<>();
0:         mapPageIds.put(pageId.intValue(), setRowId);
1:       }
1: 
0:       // get the row id Set<RowId>
0:       Number rowId = fieldsInDoc.get(ROWID_ID).numericValue();
0:       setRowId.add(rowId.intValue());
1:     }
1: 
1:     // result blocklets
1:     List<FineGrainBlocklet> blocklets = new ArrayList<>();
1: 
1:     // transform all blocks into result type blocklets
1:     // Map<BlockId, Map<BlockletId, Map<PageId, Set<RowId>>>>
0:     for (Map.Entry<String, Map<String, Map<Integer, Set<Integer>>>> mapBlock :
1:         mapBlocks.entrySet()) {
0:       String blockId = mapBlock.getKey();
0:       Map<String, Map<Integer, Set<Integer>>> mapBlocklets = mapBlock.getValue();
0:       // for blocklets in this block Map<BlockletId, Map<PageId, Set<RowId>>>
0:       for (Map.Entry<String, Map<Integer, Set<Integer>>> mapBlocklet : mapBlocklets.entrySet()) {
0:         String blockletId = mapBlocklet.getKey();
0:         Map<Integer, Set<Integer>> mapPageIds = mapBlocklet.getValue();
0:         List<FineGrainBlocklet.Page> pages = new ArrayList<FineGrainBlocklet.Page>();
1: 
0:         // for pages in this blocklet Map<PageId, Set<RowId>>>
0:         for (Map.Entry<Integer, Set<Integer>> mapPageId : mapPageIds.entrySet()) {
0:           // construct array rowid
0:           int[] rowIds = new int[mapPageId.getValue().size()];
0:           int i = 0;
0:           // for rowids in this page Set<RowId>
0:           for (Integer rowid : mapPageId.getValue()) {
0:             rowIds[i++] = rowid;
1:           }
0:           // construct one page
0:           FineGrainBlocklet.Page page = new FineGrainBlocklet.Page();
0:           page.setPageId(mapPageId.getKey());
0:           page.setRowId(rowIds);
1: 
0:           // add this page into list pages
0:           pages.add(page);
1:         }
1: 
0:         // add a FineGrainBlocklet
0:         blocklets.add(new FineGrainBlocklet(blockId, blockletId, pages));
1:       }
1:     }
1: 
1:     return blocklets;
1:   }
1: 
1:   @Override
1:   public boolean isScanRequired(FilterResolverIntf filterExp) {
1:     return true;
1:   }
1: 
1:   /**
1:    * Clear complete index table and release memory.
1:    */
1:   @Override
1:   public void clear() {
1: 
1:   }
1: 
1: }
author:xuchuanyin
-------------------------------------------------------------------------------
commit:5397c05
/////////////////////////////////////////////////////////////////////////
1: import org.apache.carbondata.core.indexstore.PartitionSpec;
/////////////////////////////////////////////////////////////////////////
1:       SegmentProperties segmentProperties, List<PartitionSpec> partitions) throws IOException {
============================================================================