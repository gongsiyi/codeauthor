1:9a25dc6: /*
1:9a25dc6:  * Licensed to the Apache Software Foundation (ASF) under one or more
1:9a25dc6:  * contributor license agreements.  See the NOTICE file distributed with
1:9a25dc6:  * this work for additional information regarding copyright ownership.
1:9a25dc6:  * The ASF licenses this file to You under the Apache License, Version 2.0
1:9a25dc6:  * (the "License"); you may not use this file except in compliance with
1:9a25dc6:  * the License.  You may obtain a copy of the License at
2:9a25dc6:  *
1:9a25dc6:  *    http://www.apache.org/licenses/LICENSE-2.0
1:9a25dc6:  *
1:9a25dc6:  * Unless required by applicable law or agreed to in writing, software
1:9a25dc6:  * distributed under the License is distributed on an "AS IS" BASIS,
1:9a25dc6:  * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
1:9a25dc6:  * See the License for the specific language governing permissions and
1:9a25dc6:  * limitations under the License.
12:9a25dc6:  */
1:5f68a79: 
1:9a25dc6: package org.apache.carbondata.hadoop.api;
53:9a25dc6: 
1:9a25dc6: import java.io.IOException;
1:9a25dc6: import java.io.Serializable;
1:9a25dc6: import java.util.ArrayList;
1:9a25dc6: import java.util.BitSet;
1:9a25dc6: import java.util.LinkedList;
1:9a25dc6: import java.util.List;
1:9a25dc6: 
1:9a25dc6: import org.apache.carbondata.common.annotations.InterfaceAudience;
1:9a25dc6: import org.apache.carbondata.common.annotations.InterfaceStability;
1:9a25dc6: import org.apache.carbondata.core.datamap.Segment;
1:9a25dc6: import org.apache.carbondata.core.datastore.impl.FileFactory;
1:9a25dc6: import org.apache.carbondata.core.metadata.AbsoluteTableIdentifier;
1:9a25dc6: import org.apache.carbondata.core.metadata.schema.PartitionInfo;
1:df5d7a9: import org.apache.carbondata.core.metadata.schema.SchemaReader;
1:9a25dc6: import org.apache.carbondata.core.metadata.schema.table.CarbonTable;
1:9a25dc6: import org.apache.carbondata.core.metadata.schema.table.TableInfo;
1:280a400: import org.apache.carbondata.core.readcommitter.LatestFilesReadCommittedScope;
1:280a400: import org.apache.carbondata.core.readcommitter.ReadCommittedScope;
1:9a25dc6: import org.apache.carbondata.core.scan.expression.Expression;
1:4b8dc0a: import org.apache.carbondata.core.statusmanager.LoadMetadataDetails;
1:9a25dc6: import org.apache.carbondata.core.util.path.CarbonTablePath;
1:9a25dc6: import org.apache.carbondata.hadoop.CarbonInputSplit;
1:9a25dc6: 
1:9a25dc6: import org.apache.hadoop.conf.Configuration;
1:9a25dc6: import org.apache.hadoop.mapreduce.InputSplit;
1:9a25dc6: import org.apache.hadoop.mapreduce.JobContext;
1:9a25dc6: 
12:9a25dc6: /**
1:c09ef99:  * InputFormat for reading carbondata files without table level metadata support,
1:c09ef99:  * schema is inferred as following steps:
1:c09ef99:  * 1. read from schema file is exists
1:c09ef99:  * 2. read from data file footer
1:9a25dc6:  *
1:9a25dc6:  * @param <T>
1:9a25dc6:  */
1:9a25dc6: @InterfaceAudience.User
1:9a25dc6: @InterfaceStability.Evolving
1:c09ef99: public class CarbonFileInputFormat<T> extends CarbonInputFormat<T> implements Serializable {
1:9a25dc6: 
1:9a25dc6:   // a cache for carbon table, it will be used in task side
1:9a25dc6:   private CarbonTable carbonTable;
1:9a25dc6: 
1:531ecdf:   public CarbonTable getOrCreateCarbonTable(Configuration configuration) throws IOException {
1:9a25dc6:     CarbonTable carbonTableTemp;
1:9a25dc6:     if (carbonTable == null) {
1:9a25dc6:       // carbon table should be created either from deserialized table info (schema saved in
1:9a25dc6:       // hive metastore) or by reading schema in HDFS (schema saved in HDFS)
1:9a25dc6:       TableInfo tableInfo = getTableInfo(configuration);
1:9a25dc6:       CarbonTable localCarbonTable;
1:9a25dc6:       if (tableInfo != null) {
1:9a25dc6:         localCarbonTable = CarbonTable.buildFromTableInfo(tableInfo);
1:4b8dc0a:       } else {
1:9a25dc6:         String schemaPath = CarbonTablePath
1:9a25dc6:             .getSchemaFilePath(getAbsoluteTableIdentifier(configuration).getTablePath());
1:9a25dc6:         if (!FileFactory.isFileExist(schemaPath, FileFactory.getFileType(schemaPath))) {
1:9a25dc6:           TableInfo tableInfoInfer =
1:280a400:               SchemaReader.inferSchema(getAbsoluteTableIdentifier(configuration), true);
1:9a25dc6:           localCarbonTable = CarbonTable.buildFromTableInfo(tableInfoInfer);
6:9a25dc6:         } else {
1:9a25dc6:           localCarbonTable =
1:9a25dc6:               SchemaReader.readCarbonTableFromStore(getAbsoluteTableIdentifier(configuration));
1:4b8dc0a:         }
77:9a25dc6:       }
1:9a25dc6:       this.carbonTable = localCarbonTable;
1:9a25dc6:       return localCarbonTable;
1:9a25dc6:     } else {
1:9a25dc6:       carbonTableTemp = this.carbonTable;
1:9a25dc6:       return carbonTableTemp;
1:9a25dc6:     }
1:9a25dc6:   }
1:9a25dc6: 
1:9a25dc6:   /**
1:9a25dc6:    * {@inheritDoc}
1:9a25dc6:    * Configurations FileInputFormat.INPUT_DIR
1:9a25dc6:    * are used to get table path to read.
1:9a25dc6:    *
1:9a25dc6:    * @param job
1:9a25dc6:    * @return List<InputSplit> list of CarbonInputSplit
1:9a25dc6:    * @throws IOException
1:9a25dc6:    */
3:9a25dc6:   @Override
1:9a25dc6:   public List<InputSplit> getSplits(JobContext job) throws IOException {
1:9a25dc6:     CarbonTable carbonTable = getOrCreateCarbonTable(job.getConfiguration());
1:9a25dc6:     if (null == carbonTable) {
1:9a25dc6:       throw new IOException("Missing/Corrupt schema file for table.");
1:9a25dc6:     }
1:637a974:     AbsoluteTableIdentifier identifier = carbonTable.getAbsoluteTableIdentifier();
1:5f68a79: 
1:9a25dc6:     if (getValidateSegmentsToAccess(job.getConfiguration())) {
1:9a25dc6:       // get all valid segments and set them into the configuration
1:9a25dc6:       // check for externalTable segment (Segment_null)
1:9a25dc6:       // process and resolve the expression
1:5f68a79: 
1:4b8dc0a:       ReadCommittedScope readCommittedScope = null;
1:4b8dc0a:       if (carbonTable.isTransactionalTable()) {
1:4b8dc0a:         readCommittedScope = new LatestFilesReadCommittedScope(
1:8f1a029:             identifier.getTablePath() + "/Fact/Part0/Segment_null/", job.getConfiguration());
1:4b8dc0a:       } else {
1:347b8e1:         readCommittedScope = getReadCommittedScope(job.getConfiguration());
1:347b8e1:         if (readCommittedScope == null) {
1:8f1a029:           readCommittedScope = new LatestFilesReadCommittedScope(identifier.getTablePath(), job
1:8f1a029:               .getConfiguration());
1:8f1a029:         } else {
1:8f1a029:           readCommittedScope.setConfiguration(job.getConfiguration());
1:347b8e1:         }
1:4b8dc0a:       }
1:9a25dc6:       // this will be null in case of corrupt schema file.
1:9a25dc6:       PartitionInfo partitionInfo = carbonTable.getPartitionInfo(carbonTable.getTableName());
1:3894e1d:       Expression filter = getFilterPredicates(job.getConfiguration());
1:9a25dc6: 
1:9a25dc6: 
1:5f68a79:       // if external table Segments are found, add it to the List
1:5f68a79:       List<Segment> externalTableSegments = new ArrayList<Segment>();
1:5f68a79:       Segment seg;
2:4b8dc0a:       if (carbonTable.isTransactionalTable()) {
1:5f68a79:         // SDK some cases write into the Segment Path instead of Table Path i.e. inside
1:5f68a79:         // the "Fact/Part0/Segment_null". The segment in this case is named as "null".
1:5f68a79:         // The table is denoted by default as a transactional table and goes through
1:5f68a79:         // the path of CarbonFileInputFormat. The above scenario is handled in the below code.
1:5f68a79:         seg = new Segment("null", null, readCommittedScope);
1:5f68a79:         externalTableSegments.add(seg);
1:4b8dc0a:       } else {
1:5f68a79:         LoadMetadataDetails[] loadMetadataDetails = readCommittedScope.getSegmentList();
1:5f68a79:         for (LoadMetadataDetails load : loadMetadataDetails) {
1:5f68a79:           seg = new Segment(load.getLoadName(), null, readCommittedScope);
2:4b8dc0a:           externalTableSegments.add(seg);
1:4b8dc0a:         }
1:4b8dc0a:       }
1:5f68a79:       // do block filtering and get split
1:5f68a79:       List<InputSplit> splits =
1:3894e1d:           getSplits(job, filter, externalTableSegments, null, partitionInfo, null);
1:5f68a79:       if (getColumnProjection(job.getConfiguration()) == null) {
1:5f68a79:         // If the user projection is empty, use default all columns as projections.
1:5f68a79:         // All column name will be filled inside getSplits, so can update only here.
1:5f68a79:         String[]  projectionColumns = projectAllColumns(carbonTable);
1:5f68a79:         setColumnProjection(job.getConfiguration(), projectionColumns);
1:5f68a79:       }
1:5f68a79:       return splits;
1:9a25dc6:     }
6:9a25dc6:     return null;
1:9a25dc6:   }
1:9a25dc6: 
1:9a25dc6:   /**
1:9a25dc6:    * {@inheritDoc}
1:9a25dc6:    * Configurations FileInputFormat.INPUT_DIR, CarbonTableInputFormat.INPUT_SEGMENT_NUMBERS
1:9a25dc6:    * are used to get table path to read.
1:9a25dc6:    *
1:9a25dc6:    * @return
1:9a25dc6:    * @throws IOException
1:9a25dc6:    */
1:3894e1d:   private List<InputSplit> getSplits(JobContext job, Expression expression,
1:9a25dc6:       List<Segment> validSegments, BitSet matchedPartitions, PartitionInfo partitionInfo,
1:c58eb43:       List<Integer> oldPartitionIdList) throws IOException {
1:9a25dc6: 
1:d5bec4d:     numSegments = validSegments.size();
1:9a25dc6:     List<InputSplit> result = new LinkedList<InputSplit>();
1:9a25dc6: 
1:9a25dc6:     // for each segment fetch blocks matching filter in Driver BTree
1:9a25dc6:     List<CarbonInputSplit> dataBlocksOfSegment =
1:3894e1d:         getDataBlocksOfSegment(job, carbonTable, expression, matchedPartitions,
1:c58eb43:             validSegments, partitionInfo, oldPartitionIdList);
1:d5bec4d:     numBlocks = dataBlocksOfSegment.size();
1:5f68a79:     result.addAll(dataBlocksOfSegment);
1:9a25dc6:     return result;
1:9a25dc6:   }
1:9a25dc6: }
============================================================================
author:kunal642
-------------------------------------------------------------------------------
commit:8f1a029
/////////////////////////////////////////////////////////////////////////
1:             identifier.getTablePath() + "/Fact/Part0/Segment_null/", job.getConfiguration());
1:           readCommittedScope = new LatestFilesReadCommittedScope(identifier.getTablePath(), job
1:               .getConfiguration());
1:         } else {
1:           readCommittedScope.setConfiguration(job.getConfiguration());
commit:637a974
/////////////////////////////////////////////////////////////////////////
1:     AbsoluteTableIdentifier identifier = carbonTable.getAbsoluteTableIdentifier();
author:ravipesala
-------------------------------------------------------------------------------
commit:3894e1d
/////////////////////////////////////////////////////////////////////////
/////////////////////////////////////////////////////////////////////////
1:       Expression filter = getFilterPredicates(job.getConfiguration());
/////////////////////////////////////////////////////////////////////////
1:           getSplits(job, filter, externalTableSegments, null, partitionInfo, null);
/////////////////////////////////////////////////////////////////////////
1:   private List<InputSplit> getSplits(JobContext job, Expression expression,
/////////////////////////////////////////////////////////////////////////
1:         getDataBlocksOfSegment(job, carbonTable, expression, matchedPartitions,
commit:347b8e1
/////////////////////////////////////////////////////////////////////////
1:         readCommittedScope = getReadCommittedScope(job.getConfiguration());
1:         if (readCommittedScope == null) {
0:           readCommittedScope = new LatestFilesReadCommittedScope(identifier.getTablePath());
1:         }
author:ajantha-bhat
-------------------------------------------------------------------------------
commit:5f68a79
/////////////////////////////////////////////////////////////////////////
/////////////////////////////////////////////////////////////////////////
1: 
1: 
/////////////////////////////////////////////////////////////////////////
1: 
/////////////////////////////////////////////////////////////////////////
1:       // if external table Segments are found, add it to the List
1:       List<Segment> externalTableSegments = new ArrayList<Segment>();
1:       Segment seg;
1:         // SDK some cases write into the Segment Path instead of Table Path i.e. inside
1:         // the "Fact/Part0/Segment_null". The segment in this case is named as "null".
1:         // The table is denoted by default as a transactional table and goes through
1:         // the path of CarbonFileInputFormat. The above scenario is handled in the below code.
1:         seg = new Segment("null", null, readCommittedScope);
1:         externalTableSegments.add(seg);
1:         LoadMetadataDetails[] loadMetadataDetails = readCommittedScope.getSegmentList();
1:         for (LoadMetadataDetails load : loadMetadataDetails) {
1:           seg = new Segment(load.getLoadName(), null, readCommittedScope);
1:       // do block filtering and get split
1:       List<InputSplit> splits =
0:           getSplits(job, filterInterface, externalTableSegments, null, partitionInfo, null);
1:       if (getColumnProjection(job.getConfiguration()) == null) {
1:         // If the user projection is empty, use default all columns as projections.
1:         // All column name will be filled inside getSplits, so can update only here.
1:         String[]  projectionColumns = projectAllColumns(carbonTable);
1:         setColumnProjection(job.getConfiguration(), projectionColumns);
1:       }
1:       return splits;
/////////////////////////////////////////////////////////////////////////
1:     result.addAll(dataBlocksOfSegment);
commit:280a400
/////////////////////////////////////////////////////////////////////////
1: import org.apache.carbondata.core.readcommitter.LatestFilesReadCommittedScope;
1: import org.apache.carbondata.core.readcommitter.ReadCommittedScope;
/////////////////////////////////////////////////////////////////////////
1:               SchemaReader.inferSchema(getAbsoluteTableIdentifier(configuration), true);
/////////////////////////////////////////////////////////////////////////
0:       ReadCommittedScope readCommittedScope = new LatestFilesReadCommittedScope(
0:           identifier.getTablePath() + "/Fact/Part0/Segment_null/");
/////////////////////////////////////////////////////////////////////////
0:             getSplits(job, filterInterface, externalTableSegments, null, partitionInfo, null,
0:                 readCommittedScope);
/////////////////////////////////////////////////////////////////////////
0:       List<Integer> oldPartitionIdList, ReadCommittedScope readCommittedScope) throws IOException {
/////////////////////////////////////////////////////////////////////////
0:             validSegments, partitionInfo, oldPartitionIdList, readCommittedScope);
author:dhatchayani
-------------------------------------------------------------------------------
commit:2c0fa10
/////////////////////////////////////////////////////////////////////////
/////////////////////////////////////////////////////////////////////////
0:       FilterResolverIntf filterInterface = carbonTable.resolveFilter(filter);
commit:531ecdf
/////////////////////////////////////////////////////////////////////////
1:   public CarbonTable getOrCreateCarbonTable(Configuration configuration) throws IOException {
author:sounakr
-------------------------------------------------------------------------------
commit:4b8dc0a
/////////////////////////////////////////////////////////////////////////
1: import org.apache.carbondata.core.statusmanager.LoadMetadataDetails;
/////////////////////////////////////////////////////////////////////////
1:       ReadCommittedScope readCommittedScope = null;
1:       if (carbonTable.isTransactionalTable()) {
1:         readCommittedScope = new LatestFilesReadCommittedScope(
0:             identifier.getTablePath() + "/Fact/Part0/Segment_null/");
1:       } else {
0:         readCommittedScope = new LatestFilesReadCommittedScope(identifier.getTablePath());
1:       }
/////////////////////////////////////////////////////////////////////////
0:       String segmentDir = null;
1:       if (carbonTable.isTransactionalTable()) {
0:         segmentDir = CarbonTablePath.getSegmentPath(identifier.getTablePath(), "null");
1:       } else {
0:         segmentDir = identifier.getTablePath();
1:       }
0:         Segment seg;
1:         if (carbonTable.isTransactionalTable()) {
0:           // SDK some cases write into the Segment Path instead of Table Path i.e. inside
0:           // the "Fact/Part0/Segment_null". The segment in this case is named as "null".
0:           // The table is denoted by default as a transactional table and goes through
0:           // the path of CarbonFileInputFormat. The above scenario is handled in the below code.
0:           seg = new Segment("null", null, readCommittedScope);
1:           externalTableSegments.add(seg);
1:         } else {
0:           LoadMetadataDetails[] loadMetadataDetails = readCommittedScope.getSegmentList();
0:           for (LoadMetadataDetails load : loadMetadataDetails) {
0:             seg = new Segment(load.getLoadName(), null, readCommittedScope);
1:             externalTableSegments.add(seg);
1:           }
1:         }
commit:c58eb43
/////////////////////////////////////////////////////////////////////////
0:         Segment seg = new Segment("null", null, readCommittedScope);
/////////////////////////////////////////////////////////////////////////
0:             getSplits(job, filterInterface, externalTableSegments, null, partitionInfo, null);
/////////////////////////////////////////////////////////////////////////
1:       List<Integer> oldPartitionIdList) throws IOException {
/////////////////////////////////////////////////////////////////////////
1:             validSegments, partitionInfo, oldPartitionIdList);
commit:9a25dc6
/////////////////////////////////////////////////////////////////////////
1: /*
1:  * Licensed to the Apache Software Foundation (ASF) under one or more
1:  * contributor license agreements.  See the NOTICE file distributed with
1:  * this work for additional information regarding copyright ownership.
1:  * The ASF licenses this file to You under the Apache License, Version 2.0
1:  * (the "License"); you may not use this file except in compliance with
1:  * the License.  You may obtain a copy of the License at
1:  *
1:  *    http://www.apache.org/licenses/LICENSE-2.0
1:  *
1:  * Unless required by applicable law or agreed to in writing, software
1:  * distributed under the License is distributed on an "AS IS" BASIS,
1:  * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
1:  * See the License for the specific language governing permissions and
1:  * limitations under the License.
1:  */
1: 
1: package org.apache.carbondata.hadoop.api;
1: 
0: import java.io.ByteArrayInputStream;
0: import java.io.DataInputStream;
1: import java.io.IOException;
1: import java.io.Serializable;
0: import java.lang.reflect.Constructor;
1: import java.util.ArrayList;
1: import java.util.BitSet;
1: import java.util.LinkedList;
1: import java.util.List;
0: import java.util.Map;
1: 
1: import org.apache.carbondata.common.annotations.InterfaceAudience;
1: import org.apache.carbondata.common.annotations.InterfaceStability;
0: import org.apache.carbondata.core.constants.CarbonCommonConstants;
0: import org.apache.carbondata.core.datamap.DataMapChooser;
0: import org.apache.carbondata.core.datamap.DataMapLevel;
1: import org.apache.carbondata.core.datamap.Segment;
0: import org.apache.carbondata.core.datamap.dev.expr.DataMapExprWrapper;
1: import org.apache.carbondata.core.datastore.impl.FileFactory;
0: import org.apache.carbondata.core.exception.InvalidConfigurationException;
0: import org.apache.carbondata.core.indexstore.ExtendedBlocklet;
0: import org.apache.carbondata.core.indexstore.PartitionSpec;
0: import org.apache.carbondata.core.indexstore.blockletindex.BlockletDataMapFactory;
0: import org.apache.carbondata.core.indexstore.blockletindex.SegmentIndexFileStore;
1: import org.apache.carbondata.core.metadata.AbsoluteTableIdentifier;
0: import org.apache.carbondata.core.metadata.ColumnarFormatVersion;
1: import org.apache.carbondata.core.metadata.schema.PartitionInfo;
0: import org.apache.carbondata.core.metadata.schema.partition.PartitionType;
1: import org.apache.carbondata.core.metadata.schema.table.CarbonTable;
1: import org.apache.carbondata.core.metadata.schema.table.TableInfo;
0: import org.apache.carbondata.core.mutate.UpdateVO;
1: import org.apache.carbondata.core.scan.expression.Expression;
0: import org.apache.carbondata.core.scan.filter.SingleTableProvider;
0: import org.apache.carbondata.core.scan.filter.TableProvider;
0: import org.apache.carbondata.core.scan.filter.resolver.FilterResolverIntf;
0: import org.apache.carbondata.core.scan.model.QueryModel;
0: import org.apache.carbondata.core.stats.QueryStatistic;
0: import org.apache.carbondata.core.stats.QueryStatisticsConstants;
0: import org.apache.carbondata.core.stats.QueryStatisticsRecorder;
0: import org.apache.carbondata.core.statusmanager.SegmentUpdateStatusManager;
0: import org.apache.carbondata.core.util.CarbonProperties;
0: import org.apache.carbondata.core.util.CarbonTimeStatisticsFactory;
0: import org.apache.carbondata.core.util.CarbonUtil;
0: import org.apache.carbondata.core.util.DataTypeConverter;
0: import org.apache.carbondata.core.util.DataTypeConverterImpl;
1: import org.apache.carbondata.core.util.path.CarbonTablePath;
1: import org.apache.carbondata.hadoop.CarbonInputSplit;
0: import org.apache.carbondata.hadoop.CarbonMultiBlockSplit;
0: import org.apache.carbondata.hadoop.CarbonProjection;
0: import org.apache.carbondata.hadoop.CarbonRecordReader;
0: import org.apache.carbondata.hadoop.readsupport.CarbonReadSupport;
0: import org.apache.carbondata.hadoop.readsupport.impl.DictionaryDecodeReadSupport;
0: import org.apache.carbondata.hadoop.util.CarbonInputFormatUtil;
0: import org.apache.carbondata.hadoop.util.ObjectSerializationUtil;
0: import org.apache.carbondata.hadoop.util.SchemaReader;
1: 
0: import org.apache.commons.logging.Log;
0: import org.apache.commons.logging.LogFactory;
1: import org.apache.hadoop.conf.Configuration;
0: import org.apache.hadoop.fs.FileSystem;
0: import org.apache.hadoop.fs.LocalFileSystem;
0: import org.apache.hadoop.fs.Path;
0: import org.apache.hadoop.mapred.JobConf;
0: import org.apache.hadoop.mapred.Reporter;
1: import org.apache.hadoop.mapreduce.InputSplit;
1: import org.apache.hadoop.mapreduce.JobContext;
0: import org.apache.hadoop.mapreduce.RecordReader;
0: import org.apache.hadoop.mapreduce.TaskAttemptContext;
0: import org.apache.hadoop.mapreduce.lib.input.FileInputFormat;
0: import org.apache.hadoop.mapreduce.lib.input.FileSplit;
0: import org.apache.hadoop.mapreduce.security.TokenCache;
1: 
1: /**
0:  * Input format of CarbonData file.
1:  *
1:  * @param <T>
1:  */
1: @InterfaceAudience.User
1: @InterfaceStability.Evolving
0: public class CarbonFileInputFormat<T> extends FileInputFormat<Void, T> implements Serializable {
1: 
0:   public static final String READ_SUPPORT_CLASS = "carbon.read.support.class";
0:   // comma separated list of input segment numbers
0:   public static final String INPUT_SEGMENT_NUMBERS =
0:       "mapreduce.input.carboninputformat.segmentnumbers";
0:   private static final String VALIDATE_INPUT_SEGMENT_IDs =
0:       "mapreduce.input.carboninputformat.validsegments";
0:   // comma separated list of input files
0:   public static final String INPUT_FILES = "mapreduce.input.carboninputformat.files";
0:   private static final String ALTER_PARTITION_ID = "mapreduce.input.carboninputformat.partitionid";
0:   private static final Log LOG = LogFactory.getLog(CarbonFileInputFormat.class);
0:   private static final String FILTER_PREDICATE =
0:       "mapreduce.input.carboninputformat.filter.predicate";
0:   private static final String COLUMN_PROJECTION = "mapreduce.input.carboninputformat.projection";
0:   private static final String TABLE_INFO = "mapreduce.input.carboninputformat.tableinfo";
0:   private static final String CARBON_READ_SUPPORT = "mapreduce.input.carboninputformat.readsupport";
0:   private static final String CARBON_CONVERTER = "mapreduce.input.carboninputformat.converter";
0:   private static final String DATA_MAP_DSTR = "mapreduce.input.carboninputformat.datamapdstr";
0:   public static final String DATABASE_NAME = "mapreduce.input.carboninputformat.databaseName";
0:   public static final String TABLE_NAME = "mapreduce.input.carboninputformat.tableName";
0:   private static final String PARTITIONS_TO_PRUNE =
0:       "mapreduce.input.carboninputformat.partitions.to.prune";
0:   public static final String UPADTE_T =
0:       "mapreduce.input.carboninputformat.partitions.to.prune";
1: 
1:   // a cache for carbon table, it will be used in task side
1:   private CarbonTable carbonTable;
1: 
1:   /**
0:    * Set the `tableInfo` in `configuration`
1:    */
0:   public static void setTableInfo(Configuration configuration, TableInfo tableInfo)
0:       throws IOException {
0:     if (null != tableInfo) {
0:       configuration.set(TABLE_INFO, CarbonUtil.encodeToString(tableInfo.serialize()));
1:     }
1:   }
1: 
1:   /**
0:    * Get TableInfo object from `configuration`
1:    */
0:   private static TableInfo getTableInfo(Configuration configuration) throws IOException {
0:     String tableInfoStr = configuration.get(TABLE_INFO);
0:     if (tableInfoStr == null) {
1:       return null;
1:     } else {
0:       TableInfo output = new TableInfo();
0:       output.readFields(
0:           new DataInputStream(
0:               new ByteArrayInputStream(CarbonUtil.decodeStringToBytes(tableInfoStr))));
0:       return output;
1:     }
1:   }
1: 
1: 
0:   public static void setTablePath(Configuration configuration, String tablePath) {
0:     configuration.set(FileInputFormat.INPUT_DIR, tablePath);
1:   }
1: 
0:   public static void setPartitionIdList(Configuration configuration, List<String> partitionIds) {
0:     configuration.set(ALTER_PARTITION_ID, partitionIds.toString());
1:   }
1: 
1: 
0:   public static void setDataMapJob(Configuration configuration, DataMapJob dataMapJob)
0:       throws IOException {
0:     if (dataMapJob != null) {
0:       String toString = ObjectSerializationUtil.convertObjectToString(dataMapJob);
0:       configuration.set(DATA_MAP_DSTR, toString);
1:     }
1:   }
1: 
0:   public static DataMapJob getDataMapJob(Configuration configuration) throws IOException {
0:     String jobString = configuration.get(DATA_MAP_DSTR);
0:     if (jobString != null) {
0:       return (DataMapJob) ObjectSerializationUtil.convertStringToObject(jobString);
1:     }
1:     return null;
1:   }
1: 
1:   /**
0:    * It sets unresolved filter expression.
1:    *
0:    * @param configuration
0:    * @param filterExpression
1:    */
0:   public static void setFilterPredicates(Configuration configuration, Expression filterExpression) {
0:     if (filterExpression == null) {
0:       return;
1:     }
0:     try {
0:       String filterString = ObjectSerializationUtil.convertObjectToString(filterExpression);
0:       configuration.set(FILTER_PREDICATE, filterString);
0:     } catch (Exception e) {
0:       throw new RuntimeException("Error while setting filter expression to Job", e);
1:     }
1:   }
1: 
0:   public static void setColumnProjection(Configuration configuration, CarbonProjection projection) {
0:     if (projection == null || projection.isEmpty()) {
0:       return;
1:     }
0:     String[] allColumns = projection.getAllColumns();
0:     StringBuilder builder = new StringBuilder();
0:     for (String column : allColumns) {
0:       builder.append(column).append(",");
1:     }
0:     String columnString = builder.toString();
0:     columnString = columnString.substring(0, columnString.length() - 1);
0:     configuration.set(COLUMN_PROJECTION, columnString);
1:   }
1: 
0:   public static String getColumnProjection(Configuration configuration) {
0:     return configuration.get(COLUMN_PROJECTION);
1:   }
1: 
1: 
1:   /**
0:    * Set list of segments to access
1:    */
0:   public static void setSegmentsToAccess(Configuration configuration, List<Segment> validSegments) {
0:     configuration.set(INPUT_SEGMENT_NUMBERS, CarbonUtil.convertToString(validSegments));
1:   }
1: 
1:   /**
0:    * Set `CARBON_INPUT_SEGMENTS` from property to configuration
1:    */
0:   public static void setQuerySegment(Configuration conf, AbsoluteTableIdentifier identifier) {
0:     String dbName = identifier.getCarbonTableIdentifier().getDatabaseName().toLowerCase();
0:     String tbName = identifier.getCarbonTableIdentifier().getTableName().toLowerCase();
0:     String segmentNumbersFromProperty = CarbonProperties.getInstance()
0:         .getProperty(CarbonCommonConstants.CARBON_INPUT_SEGMENTS + dbName + "." + tbName, "*");
0:     if (!segmentNumbersFromProperty.trim().equals("*")) {
0:       CarbonFileInputFormat
0:           .setSegmentsToAccess(conf, Segment.toSegmentList(segmentNumbersFromProperty.split(",")));
1:     }
1:   }
1: 
1:   /**
0:    * set list of segment to access
1:    */
0:   public static void setValidateSegmentsToAccess(Configuration configuration, Boolean validate) {
0:     configuration.set(CarbonFileInputFormat.VALIDATE_INPUT_SEGMENT_IDs, validate.toString());
1:   }
1: 
1:   /**
0:    * get list of segment to access
1:    */
0:   public static boolean getValidateSegmentsToAccess(Configuration configuration) {
0:     return configuration.get(CarbonFileInputFormat.VALIDATE_INPUT_SEGMENT_IDs, "true")
0:         .equalsIgnoreCase("true");
1:   }
1: 
1:   /**
0:    * set list of partitions to prune
1:    */
0:   public static void setPartitionsToPrune(Configuration configuration,
0:       List<PartitionSpec> partitions) {
0:     if (partitions == null) {
0:       return;
1:     }
0:     try {
0:       String partitionString =
0:           ObjectSerializationUtil.convertObjectToString(new ArrayList<>(partitions));
0:       configuration.set(PARTITIONS_TO_PRUNE, partitionString);
0:     } catch (Exception e) {
0:       throw new RuntimeException("Error while setting patition information to Job", e);
1:     }
1:   }
1: 
1:   /**
0:    * get list of partitions to prune
1:    */
0:   private static List<PartitionSpec> getPartitionsToPrune(Configuration configuration)
0:       throws IOException {
0:     String partitionString = configuration.get(PARTITIONS_TO_PRUNE);
0:     if (partitionString != null) {
0:       return (List<PartitionSpec>) ObjectSerializationUtil.convertStringToObject(partitionString);
1:     }
1:     return null;
1:   }
1: 
0:   public AbsoluteTableIdentifier getAbsoluteTableIdentifier(Configuration configuration)
0:       throws IOException {
0:     String tablePath = configuration.get(INPUT_DIR, "");
0:     try {
0:       return AbsoluteTableIdentifier
0:           .from(tablePath, getDatabaseName(configuration), getTableName(configuration));
0:     } catch (InvalidConfigurationException e) {
0:       throw new IOException(e);
1:     }
1:   }
1: 
1:   /**
1:    * {@inheritDoc}
1:    * Configurations FileInputFormat.INPUT_DIR
1:    * are used to get table path to read.
1:    *
1:    * @param job
1:    * @return List<InputSplit> list of CarbonInputSplit
1:    * @throws IOException
1:    */
1:   @Override
1:   public List<InputSplit> getSplits(JobContext job) throws IOException {
0:     AbsoluteTableIdentifier identifier = getAbsoluteTableIdentifier(job.getConfiguration());
1:     CarbonTable carbonTable = getOrCreateCarbonTable(job.getConfiguration());
1:     if (null == carbonTable) {
1:       throw new IOException("Missing/Corrupt schema file for table.");
1:     }
0:     //    TableDataMap blockletMap = DataMapStoreManager.getInstance()
0:     //        .getDataMap(identifier, BlockletDataMap.NAME, BlockletDataMapFactory.class.getName());
1: 
1:     if (getValidateSegmentsToAccess(job.getConfiguration())) {
1:       // get all valid segments and set them into the configuration
1:       // check for externalTable segment (Segment_null)
1:       // process and resolve the expression
0:       Expression filter = getFilterPredicates(job.getConfiguration());
0:       TableProvider tableProvider = new SingleTableProvider(carbonTable);
1:       // this will be null in case of corrupt schema file.
1:       PartitionInfo partitionInfo = carbonTable.getPartitionInfo(carbonTable.getTableName());
0:       CarbonInputFormatUtil.processFilterExpression(filter, carbonTable, null, null);
1: 
0:       FilterResolverIntf filterInterface = CarbonInputFormatUtil
0:           .resolveFilter(filter, carbonTable.getAbsoluteTableIdentifier(), tableProvider);
1: 
0:       String segmentDir = CarbonTablePath.getSegmentPath(identifier.getTablePath(), "null");
0:       FileFactory.FileType fileType = FileFactory.getFileType(segmentDir);
0:       if (FileFactory.isFileExist(segmentDir, fileType)) {
0:         // if external table Segments are found, add it to the List
0:         List<Segment> externalTableSegments = new ArrayList<Segment>();
0:         Segment seg = new Segment("null", null);
0:         externalTableSegments.add(seg);
1: 
0:         Map<String, String> indexFiles =
0:             new SegmentIndexFileStore().getIndexFilesFromSegment(segmentDir);
1: 
0:         if (indexFiles.size() == 0) {
0:           throw new RuntimeException("Index file not present to read the carbondata file");
1:         }
0:         // do block filtering and get split
0:         List<InputSplit> splits =
0:             getSplits(job, filterInterface, externalTableSegments, null, partitionInfo, null);
1: 
0:         return splits;
1:       }
1:     }
1:     return null;
1:   }
1: 
1: 
1: 
1:   /**
1:    * {@inheritDoc}
1:    * Configurations FileInputFormat.INPUT_DIR, CarbonTableInputFormat.INPUT_SEGMENT_NUMBERS
1:    * are used to get table path to read.
1:    *
1:    * @return
1:    * @throws IOException
1:    */
0:   private List<InputSplit> getSplits(JobContext job, FilterResolverIntf filterResolver,
1:       List<Segment> validSegments, BitSet matchedPartitions, PartitionInfo partitionInfo,
0:       List<Integer> oldPartitionIdList) throws IOException {
1: 
1:     List<InputSplit> result = new LinkedList<InputSplit>();
0:     UpdateVO invalidBlockVOForSegmentId = null;
0:     Boolean isIUDTable = false;
1: 
0:     AbsoluteTableIdentifier absoluteTableIdentifier =
0:         getOrCreateCarbonTable(job.getConfiguration()).getAbsoluteTableIdentifier();
0:     SegmentUpdateStatusManager updateStatusManager =
0:         new SegmentUpdateStatusManager(absoluteTableIdentifier);
1: 
0:     isIUDTable = (updateStatusManager.getUpdateStatusDetails().length != 0);
1: 
1:     // for each segment fetch blocks matching filter in Driver BTree
1:     List<CarbonInputSplit> dataBlocksOfSegment =
0:         getDataBlocksOfSegment(job, absoluteTableIdentifier, filterResolver, matchedPartitions,
0:             validSegments, partitionInfo, oldPartitionIdList);
0:     for (CarbonInputSplit inputSplit : dataBlocksOfSegment) {
1: 
0:       // Get the UpdateVO for those tables on which IUD operations being performed.
0:       if (isIUDTable) {
0:         invalidBlockVOForSegmentId =
0:             updateStatusManager.getInvalidTimestampRange(inputSplit.getSegmentId());
1:       }
0:       String[] deleteDeltaFilePath = null;
0:       if (isIUDTable) {
0:         // In case IUD is not performed in this table avoid searching for
0:         // invalidated blocks.
0:         if (CarbonUtil
0:             .isInvalidTableBlock(inputSplit.getSegmentId(), inputSplit.getPath().toString(),
0:                 invalidBlockVOForSegmentId, updateStatusManager)) {
0:           continue;
1:         }
0:         // When iud is done then only get delete delta files for a block
0:         try {
0:           deleteDeltaFilePath = updateStatusManager
0:               .getDeleteDeltaFilePath(inputSplit.getPath().toString(), inputSplit.getSegmentId());
0:         } catch (Exception e) {
0:           throw new IOException(e);
1:         }
1:       }
0:       inputSplit.setDeleteDeltaFiles(deleteDeltaFilePath);
0:       result.add(inputSplit);
1:     }
1:     return result;
1:   }
1: 
0:   protected Expression getFilterPredicates(Configuration configuration) {
0:     try {
0:       String filterExprString = configuration.get(FILTER_PREDICATE);
0:       if (filterExprString == null) {
1:         return null;
1:       }
0:       Object filter = ObjectSerializationUtil.convertStringToObject(filterExprString);
0:       return (Expression) filter;
0:     } catch (IOException e) {
0:       throw new RuntimeException("Error while reading filter expression", e);
1:     }
1:   }
1: 
1:   /**
0:    * get data blocks of given segment
1:    */
0:   private List<CarbonInputSplit> getDataBlocksOfSegment(JobContext job,
0:       AbsoluteTableIdentifier absoluteTableIdentifier, FilterResolverIntf resolver,
0:       BitSet matchedPartitions, List<Segment> segmentIds, PartitionInfo partitionInfo,
0:       List<Integer> oldPartitionIdList) throws IOException {
1: 
0:     QueryStatisticsRecorder recorder = CarbonTimeStatisticsFactory.createDriverRecorder();
0:     QueryStatistic statistic = new QueryStatistic();
1: 
0:     // get tokens for all the required FileSystem for table path
0:     TokenCache.obtainTokensForNamenodes(job.getCredentials(),
0:         new Path[] { new Path(absoluteTableIdentifier.getTablePath()) }, job.getConfiguration());
0:     boolean distributedCG = Boolean.parseBoolean(CarbonProperties.getInstance()
0:         .getProperty(CarbonCommonConstants.USE_DISTRIBUTED_DATAMAP,
0:             CarbonCommonConstants.USE_DISTRIBUTED_DATAMAP_DEFAULT));
0:     DataMapExprWrapper dataMapExprWrapper =
0:         DataMapChooser.get().choose(getOrCreateCarbonTable(job.getConfiguration()), resolver);
0:     DataMapJob dataMapJob = getDataMapJob(job.getConfiguration());
0:     List<PartitionSpec> partitionsToPrune = getPartitionsToPrune(job.getConfiguration());
0:     List<ExtendedBlocklet> prunedBlocklets;
0:     if (distributedCG || dataMapExprWrapper.getDataMapType() == DataMapLevel.FG) {
0:       DistributableDataMapFormat datamapDstr =
0:           new DistributableDataMapFormat(absoluteTableIdentifier, dataMapExprWrapper,
0:               segmentIds, partitionsToPrune,
0:               BlockletDataMapFactory.class.getName());
0:       prunedBlocklets = dataMapJob.execute(datamapDstr, resolver);
0:       // Apply expression on the blocklets.
0:       prunedBlocklets = dataMapExprWrapper.pruneBlocklets(prunedBlocklets);
1:     } else {
0:       prunedBlocklets = dataMapExprWrapper.prune(segmentIds, partitionsToPrune);
1:     }
1: 
0:     List<CarbonInputSplit> resultFilterredBlocks = new ArrayList<>();
0:     int partitionIndex = 0;
0:     List<Integer> partitionIdList = new ArrayList<>();
0:     if (partitionInfo != null && partitionInfo.getPartitionType() != PartitionType.NATIVE_HIVE) {
0:       partitionIdList = partitionInfo.getPartitionIds();
1:     }
0:     for (ExtendedBlocklet blocklet : prunedBlocklets) {
0:       long partitionId = CarbonTablePath.DataFileUtil.getTaskIdFromTaskNo(
0:           CarbonTablePath.DataFileUtil.getTaskNo(blocklet.getPath()));
1: 
0:       // OldPartitionIdList is only used in alter table partition command because it change
0:       // partition info first and then read data.
0:       // For other normal query should use newest partitionIdList
0:       if (partitionInfo != null && partitionInfo.getPartitionType() != PartitionType.NATIVE_HIVE) {
0:         if (oldPartitionIdList != null) {
0:           partitionIndex = oldPartitionIdList.indexOf((int)partitionId);
1:         } else {
0:           partitionIndex = partitionIdList.indexOf((int)partitionId);
1:         }
1:       }
0:       if (partitionIndex != -1) {
0:         // matchedPartitions variable will be null in two cases as follows
0:         // 1. the table is not a partition table
0:         // 2. the table is a partition table, and all partitions are matched by query
0:         // for partition table, the task id of carbaondata file name is the partition id.
0:         // if this partition is not required, here will skip it.
0:         if (matchedPartitions == null || matchedPartitions.get(partitionIndex)) {
0:           CarbonInputSplit inputSplit = convertToCarbonInputSplit(blocklet);
0:           if (inputSplit != null) {
0:             resultFilterredBlocks.add(inputSplit);
1:           }
1:         }
1:       }
1:     }
0:     statistic
0:         .addStatistics(QueryStatisticsConstants.LOAD_BLOCKS_DRIVER, System.currentTimeMillis());
0:     recorder.recordStatisticsForDriver(statistic, job.getConfiguration().get("query.id"));
0:     return resultFilterredBlocks;
1:   }
1: 
0:   private CarbonInputSplit convertToCarbonInputSplit(ExtendedBlocklet blocklet) throws IOException {
0:     CarbonInputSplit split =
0:         CarbonInputSplit.from(blocklet.getSegmentId(),
0:             blocklet.getBlockletId(), new FileSplit(new Path(blocklet.getPath()), 0,
0:                 blocklet.getLength(), blocklet.getLocations()),
0:             ColumnarFormatVersion.valueOf((short) blocklet.getDetailInfo().getVersionNumber()),
0:             blocklet.getDataMapWriterPath());
0:     split.setDetailInfo(blocklet.getDetailInfo());
0:     return split;
1:   }
1: 
1:   @Override
0:   public RecordReader<Void, T> createRecordReader(InputSplit inputSplit,
0:       TaskAttemptContext taskAttemptContext) throws IOException, InterruptedException {
0:     Configuration configuration = taskAttemptContext.getConfiguration();
0:     QueryModel queryModel = createQueryModel(inputSplit, taskAttemptContext);
0:     CarbonReadSupport<T> readSupport = getReadSupportClass(configuration);
0:     return new CarbonRecordReader<T>(queryModel, readSupport);
1:   }
1: 
0:   public QueryModel createQueryModel(InputSplit inputSplit, TaskAttemptContext taskAttemptContext)
0:       throws IOException {
0:     Configuration configuration = taskAttemptContext.getConfiguration();
0:     CarbonTable carbonTable = getOrCreateCarbonTable(configuration);
0:     TableProvider tableProvider = new SingleTableProvider(carbonTable);
1: 
0:     // query plan includes projection column
0:     String projectionString = getColumnProjection(configuration);
0:     String[] projectionColumnNames = null;
0:     if (projectionString != null) {
0:       projectionColumnNames = projectionString.split(",");
1:     }
0:     QueryModel queryModel = carbonTable.createQueryWithProjection(
0:         projectionColumnNames, getDataTypeConverter(configuration));
1: 
0:     // set the filter to the query model in order to filter blocklet before scan
0:     Expression filter = getFilterPredicates(configuration);
0:     boolean[] isFilterDimensions = new boolean[carbonTable.getDimensionOrdinalMax()];
0:     // getAllMeasures returns list of visible and invisible columns
0:     boolean[] isFilterMeasures =
0:         new boolean[carbonTable.getAllMeasures().size()];
0:     CarbonInputFormatUtil.processFilterExpression(filter, carbonTable, isFilterDimensions,
0:         isFilterMeasures);
0:     queryModel.setIsFilterDimensions(isFilterDimensions);
0:     queryModel.setIsFilterMeasures(isFilterMeasures);
0:     FilterResolverIntf filterIntf = CarbonInputFormatUtil
0:         .resolveFilter(filter, carbonTable.getAbsoluteTableIdentifier(), tableProvider);
0:     queryModel.setFilterExpressionResolverTree(filterIntf);
1: 
0:     // update the file level index store if there are invalid segment
0:     if (inputSplit instanceof CarbonMultiBlockSplit) {
0:       CarbonMultiBlockSplit split = (CarbonMultiBlockSplit) inputSplit;
0:       List<String> invalidSegments = split.getAllSplits().get(0).getInvalidSegments();
0:       if (invalidSegments.size() > 0) {
0:         queryModel.setInvalidSegmentIds(invalidSegments);
1:       }
0:       List<UpdateVO> invalidTimestampRangeList =
0:           split.getAllSplits().get(0).getInvalidTimestampRange();
0:       if ((null != invalidTimestampRangeList) && (invalidTimestampRangeList.size() > 0)) {
0:         queryModel.setInvalidBlockForSegmentId(invalidTimestampRangeList);
1:       }
1:     }
0:     return queryModel;
1:   }
1: 
0:   private CarbonTable getOrCreateCarbonTable(Configuration configuration) throws IOException {
1:     CarbonTable carbonTableTemp;
1:     if (carbonTable == null) {
1:       // carbon table should be created either from deserialized table info (schema saved in
1:       // hive metastore) or by reading schema in HDFS (schema saved in HDFS)
1:       TableInfo tableInfo = getTableInfo(configuration);
1:       CarbonTable localCarbonTable;
1:       if (tableInfo != null) {
1:         localCarbonTable = CarbonTable.buildFromTableInfo(tableInfo);
1:       } else {
1:         String schemaPath = CarbonTablePath
1:             .getSchemaFilePath(getAbsoluteTableIdentifier(configuration).getTablePath());
1:         if (!FileFactory.isFileExist(schemaPath, FileFactory.getFileType(schemaPath))) {
1:           TableInfo tableInfoInfer =
0:               SchemaReader.inferSchema(getAbsoluteTableIdentifier(configuration));
1:           localCarbonTable = CarbonTable.buildFromTableInfo(tableInfoInfer);
1:         } else {
1:           localCarbonTable =
1:               SchemaReader.readCarbonTableFromStore(getAbsoluteTableIdentifier(configuration));
1:         }
1:       }
1:       this.carbonTable = localCarbonTable;
1:       return localCarbonTable;
1:     } else {
1:       carbonTableTemp = this.carbonTable;
1:       return carbonTableTemp;
1:     }
1:   }
1: 
1: 
0:   public CarbonReadSupport<T> getReadSupportClass(Configuration configuration) {
0:     String readSupportClass = configuration.get(CARBON_READ_SUPPORT);
0:     //By default it uses dictionary decoder read class
0:     CarbonReadSupport<T> readSupport = null;
0:     if (readSupportClass != null) {
0:       try {
0:         Class<?> myClass = Class.forName(readSupportClass);
0:         Constructor<?> constructor = myClass.getConstructors()[0];
0:         Object object = constructor.newInstance();
0:         if (object instanceof CarbonReadSupport) {
0:           readSupport = (CarbonReadSupport) object;
1:         }
0:       } catch (ClassNotFoundException ex) {
0:         LOG.error("Class " + readSupportClass + "not found", ex);
0:       } catch (Exception ex) {
0:         LOG.error("Error while creating " + readSupportClass, ex);
1:       }
1:     } else {
0:       readSupport = new DictionaryDecodeReadSupport<>();
1:     }
0:     return readSupport;
1:   }
1: 
1:   @Override
0:   protected boolean isSplitable(JobContext context, Path filename) {
0:     try {
0:       // Don't split the file if it is local file system
0:       FileSystem fileSystem = filename.getFileSystem(context.getConfiguration());
0:       if (fileSystem instanceof LocalFileSystem) {
0:         return false;
1:       }
0:     } catch (Exception e) {
0:       return true;
1:     }
0:     return true;
1:   }
1: 
1:   /**
0:    * return valid segment to access
1:    */
0:   private String[] getSegmentsToAccess(JobContext job) {
0:     String segmentString = job.getConfiguration().get(INPUT_SEGMENT_NUMBERS, "");
0:     if (segmentString.trim().isEmpty()) {
0:       return new String[0];
1:     }
0:     return segmentString.split(",");
1:   }
1: 
0:   public static DataTypeConverter getDataTypeConverter(Configuration configuration)
0:       throws IOException {
0:     String converter = configuration.get(CARBON_CONVERTER);
0:     if (converter == null) {
0:       return new DataTypeConverterImpl();
1:     }
0:     return (DataTypeConverter) ObjectSerializationUtil.convertStringToObject(converter);
1:   }
1: 
0:   public static void setDatabaseName(Configuration configuration, String databaseName) {
0:     if (null != databaseName) {
0:       configuration.set(DATABASE_NAME, databaseName);
1:     }
1:   }
1: 
0:   public static String getDatabaseName(Configuration configuration)
0:       throws InvalidConfigurationException {
0:     String databseName = configuration.get(DATABASE_NAME);
0:     if (null == databseName) {
0:       throw new InvalidConfigurationException("Database name is not set.");
1:     }
0:     return databseName;
1:   }
1: 
0:   public static void setTableName(Configuration configuration, String tableName) {
0:     if (null != tableName) {
0:       configuration.set(TABLE_NAME, tableName);
1:     }
1:   }
1: 
0:   public static String getTableName(Configuration configuration)
0:       throws InvalidConfigurationException {
0:     String tableName = configuration.get(TABLE_NAME);
0:     if (tableName == null) {
0:       throw new InvalidConfigurationException("Table name is not set");
1:     }
0:     return tableName;
1:   }
1: 
0:   public org.apache.hadoop.mapred.RecordReader<Void, T> getRecordReader(
0:       org.apache.hadoop.mapred.InputSplit split, JobConf job, Reporter reporter)
0:       throws IOException {
1:     return null;
1:   }
1: }
author:Jacky Li
-------------------------------------------------------------------------------
commit:7e0803f
/////////////////////////////////////////////////////////////////////////
0:     SegmentUpdateStatusManager updateStatusManager = new SegmentUpdateStatusManager(carbonTable);
0:         getDataBlocksOfSegment(job, carbonTable, filterResolver, matchedPartitions,
commit:df5d7a9
/////////////////////////////////////////////////////////////////////////
1: import org.apache.carbondata.core.metadata.schema.SchemaReader;
/////////////////////////////////////////////////////////////////////////
/////////////////////////////////////////////////////////////////////////
0:       carbonTable.processFilterExpression(filter, null, null);
0:       FilterResolverIntf filterInterface = carbonTable.resolveFilter(filter, tableProvider);
author:QiangCai
-------------------------------------------------------------------------------
commit:d5bec4d
/////////////////////////////////////////////////////////////////////////
1:     numSegments = validSegments.size();
/////////////////////////////////////////////////////////////////////////
1:     numBlocks = dataBlocksOfSegment.size();
author:Ajantha-Bhat
-------------------------------------------------------------------------------
commit:c09ef99
/////////////////////////////////////////////////////////////////////////
/////////////////////////////////////////////////////////////////////////
/////////////////////////////////////////////////////////////////////////
1:  * InputFormat for reading carbondata files without table level metadata support,
1:  * schema is inferred as following steps:
1:  * 1. read from schema file is exists
1:  * 2. read from data file footer
1: public class CarbonFileInputFormat<T> extends CarbonInputFormat<T> implements Serializable {
0:   protected CarbonTable getOrCreateCarbonTable(Configuration configuration) throws IOException {
0:     CarbonTable carbonTableTemp;
0:     if (carbonTable == null) {
0:       // carbon table should be created either from deserialized table info (schema saved in
0:       // hive metastore) or by reading schema in HDFS (schema saved in HDFS)
0:       TableInfo tableInfo = getTableInfo(configuration);
0:       CarbonTable localCarbonTable;
0:       if (tableInfo != null) {
0:         localCarbonTable = CarbonTable.buildFromTableInfo(tableInfo);
0:       } else {
0:         String schemaPath = CarbonTablePath
0:             .getSchemaFilePath(getAbsoluteTableIdentifier(configuration).getTablePath());
0:         if (!FileFactory.isFileExist(schemaPath, FileFactory.getFileType(schemaPath))) {
0:           TableInfo tableInfoInfer =
0:               SchemaReader.inferSchema(getAbsoluteTableIdentifier(configuration));
0:           localCarbonTable = CarbonTable.buildFromTableInfo(tableInfoInfer);
0:         } else {
0:           localCarbonTable =
0:               SchemaReader.readCarbonTableFromStore(getAbsoluteTableIdentifier(configuration));
0:         }
0:       }
0:       this.carbonTable = localCarbonTable;
0:       return localCarbonTable;
0:       carbonTableTemp = this.carbonTable;
0:       return carbonTableTemp;
/////////////////////////////////////////////////////////////////////////
/////////////////////////////////////////////////////////////////////////
/////////////////////////////////////////////////////////////////////////
============================================================================