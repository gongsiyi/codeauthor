1:91e6f6f: /*
1:91e6f6f:  * Licensed to the Apache Software Foundation (ASF) under one or more
1:91e6f6f:  * contributor license agreements.  See the NOTICE file distributed with
1:91e6f6f:  * this work for additional information regarding copyright ownership.
1:91e6f6f:  * The ASF licenses this file to You under the Apache License, Version 2.0
1:91e6f6f:  * (the "License"); you may not use this file except in compliance with
1:91e6f6f:  * the License.  You may obtain a copy of the License at
1:91e6f6f:  *
1:91e6f6f:  *    http://www.apache.org/licenses/LICENSE-2.0
1:91e6f6f:  *
1:91e6f6f:  * Unless required by applicable law or agreed to in writing, software
1:91e6f6f:  * distributed under the License is distributed on an "AS IS" BASIS,
1:91e6f6f:  * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
1:91e6f6f:  * See the License for the specific language governing permissions and
1:91e6f6f:  * limitations under the License.
2:91e6f6f:  */
1:91e6f6f: 
1:91e6f6f: package org.apache.carbondata.hadoop.api;
1:91e6f6f: 
1:91e6f6f: import java.io.IOException;
1:8d3c774: import java.util.ArrayList;
1:8d3c774: import java.util.HashSet;
1:8d3c774: import java.util.List;
1:8d3c774: import java.util.Set;
1:91e6f6f: 
1:91e6f6f: import org.apache.carbondata.common.logging.LogService;
1:91e6f6f: import org.apache.carbondata.common.logging.LogServiceFactory;
1:8d3c774: import org.apache.carbondata.core.constants.CarbonCommonConstants;
1:8d3c774: import org.apache.carbondata.core.datamap.Segment;
1:9fba684: import org.apache.carbondata.core.datamap.status.DataMapStatusManager;
1:8d3c774: import org.apache.carbondata.core.datastore.filesystem.CarbonFile;
1:8d3c774: import org.apache.carbondata.core.datastore.impl.FileFactory;
1:8d3c774: import org.apache.carbondata.core.indexstore.PartitionSpec;
1:8d3c774: import org.apache.carbondata.core.locks.CarbonLockFactory;
1:8d3c774: import org.apache.carbondata.core.locks.ICarbonLock;
1:8d3c774: import org.apache.carbondata.core.locks.LockUsage;
1:8d3c774: import org.apache.carbondata.core.metadata.SegmentFileStore;
1:a89587e: import org.apache.carbondata.core.metadata.schema.table.CarbonTable;
1:a89587e: import org.apache.carbondata.core.mutate.CarbonUpdateUtil;
1:91e6f6f: import org.apache.carbondata.core.statusmanager.LoadMetadataDetails;
1:91e6f6f: import org.apache.carbondata.core.statusmanager.SegmentStatus;
1:a89587e: import org.apache.carbondata.core.statusmanager.SegmentStatusManager;
1:3a6136d: import org.apache.carbondata.core.util.CarbonSessionInfo;
1:3a6136d: import org.apache.carbondata.core.util.ThreadLocalSessionInfo;
1:4430178: import org.apache.carbondata.core.util.path.CarbonTablePath;
1:829e7aa: import org.apache.carbondata.events.OperationContext;
1:829e7aa: import org.apache.carbondata.events.OperationListenerBus;
1:829e7aa: import org.apache.carbondata.processing.loading.events.LoadEvents;
1:91e6f6f: import org.apache.carbondata.processing.loading.model.CarbonLoadModel;
1:91e6f6f: import org.apache.carbondata.processing.util.CarbonLoaderUtil;
1:91e6f6f: 
1:91e6f6f: import org.apache.hadoop.fs.Path;
1:91e6f6f: import org.apache.hadoop.mapreduce.JobContext;
1:91e6f6f: import org.apache.hadoop.mapreduce.JobStatus;
1:91e6f6f: import org.apache.hadoop.mapreduce.TaskAttemptContext;
1:91e6f6f: import org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter;
1:91e6f6f: 
2:91e6f6f: /**
1:91e6f6f:  * Outputcommitter which manages the segments during loading.It commits segment information to the
1:91e6f6f:  * tablestatus file upon success or fail.
1:91e6f6f:  */
1:91e6f6f: public class CarbonOutputCommitter extends FileOutputCommitter {
1:91e6f6f: 
1:91e6f6f:   private static final LogService LOGGER =
1:91e6f6f:       LogServiceFactory.getLogService(CarbonOutputCommitter.class.getName());
1:91e6f6f: 
1:8d3c774:   private ICarbonLock segmentLock;
1:8d3c774: 
1:91e6f6f:   public CarbonOutputCommitter(Path outputPath, TaskAttemptContext context) throws IOException {
1:91e6f6f:     super(outputPath, context);
3:91e6f6f:   }
1:91e6f6f: 
1:91e6f6f:   /**
1:91e6f6f:    * Update the tablestatus with inprogress while setup the job.
1:91e6f6f:    *
1:91e6f6f:    * @param context
1:91e6f6f:    * @throws IOException
1:d7852ab:    */
1:91e6f6f:   @Override public void setupJob(JobContext context) throws IOException {
1:91e6f6f:     super.setupJob(context);
1:91e6f6f:     boolean overwriteSet = CarbonTableOutputFormat.isOverwriteSet(context.getConfiguration());
2:91e6f6f:     CarbonLoadModel loadModel = CarbonTableOutputFormat.getLoadModel(context.getConfiguration());
1:8d3c774:     if (loadModel.getSegmentId() == null) {
1:8d3c774:       CarbonLoaderUtil.readAndUpdateLoadProgressInTableMeta(loadModel, overwriteSet);
1:8d3c774:     }
1:8d3c774:     // Take segment lock
1:8d3c774:     segmentLock = CarbonLockFactory.getCarbonLockObj(
1:8d3c774:         loadModel.getCarbonDataLoadSchema().getCarbonTable().getAbsoluteTableIdentifier(),
1:8d3c774:         CarbonTablePath.addSegmentPrefix(loadModel.getSegmentId()) + LockUsage.LOCK);
1:8d3c774:     if (!segmentLock.lockWithRetries()) {
1:8d3c774:       throw new RuntimeException("Already segment is locked for loading, not supposed happen");
1:8d3c774:     }
1:91e6f6f:     CarbonTableOutputFormat.setLoadModel(context.getConfiguration(), loadModel);
1:91e6f6f:   }
1:91e6f6f: 
1:4430178:   @Override public void setupTask(TaskAttemptContext context) throws IOException {
1:4430178:     super.setupTask(context);
1:4430178:   }
1:4430178: 
1:91e6f6f:   /**
1:91e6f6f:    * Update the tablestatus as success after job is success
1:91e6f6f:    *
1:91e6f6f:    * @param context
1:91e6f6f:    * @throws IOException
1:91e6f6f:    */
1:91e6f6f:   @Override public void commitJob(JobContext context) throws IOException {
1:af4277e:     try {
1:af4277e:       super.commitJob(context);
1:af4277e:     } catch (IOException e) {
1:af4277e:       // ignore, in case of concurrent load it try to remove temporary folders by other load may
1:af4277e:       // cause file not found exception. This will not impact carbon load,
1:af4277e:       LOGGER.warn(e.getMessage());
1:af4277e:     }
1:91e6f6f:     boolean overwriteSet = CarbonTableOutputFormat.isOverwriteSet(context.getConfiguration());
1:91e6f6f:     CarbonLoadModel loadModel = CarbonTableOutputFormat.getLoadModel(context.getConfiguration());
1:a89587e:     LoadMetadataDetails newMetaEntry = loadModel.getCurrentLoadMetadataDetail();
1:8d3c774:     String readPath = CarbonTablePath.getSegmentFilesLocation(loadModel.getTablePath())
1:8d3c774:         + CarbonCommonConstants.FILE_SEPARATOR
1:8d3c774:         + loadModel.getSegmentId() + "_" + loadModel.getFactTimeStamp() + ".tmp";
1:3ff55a2:     // Merge all partition files into a single file.
1:7e0803f:     String segmentFileName = SegmentFileStore.genSegmentFileName(
1:7e0803f:         loadModel.getSegmentId(), String.valueOf(loadModel.getFactTimeStamp()));
1:8d3c774:     SegmentFileStore.SegmentFile segmentFile = SegmentFileStore
1:8d3c774:         .mergeSegmentFiles(readPath, segmentFileName,
1:8d3c774:             CarbonTablePath.getSegmentFilesLocation(loadModel.getTablePath()));
1:8d3c774:     if (segmentFile != null) {
1:7ef9164:       if (null == newMetaEntry) {
1:7ef9164:         throw new RuntimeException("Internal Error");
1:7ef9164:       }
1:8d3c774:       // Move all files from temp directory of each segment to partition directory
1:8d3c774:       SegmentFileStore.moveFromTempFolder(segmentFile,
1:8d3c774:           loadModel.getSegmentId() + "_" + loadModel.getFactTimeStamp() + ".tmp",
1:8d3c774:           loadModel.getTablePath());
1:8d3c774:       newMetaEntry.setSegmentFile(segmentFileName + CarbonTablePath.SEGMENT_EXT);
1:8d3c774:     }
1:5508460:     OperationContext operationContext = (OperationContext) getOperationContext();
1:5508460:     String uuid = "";
1:5508460:     if (loadModel.getCarbonDataLoadSchema().getCarbonTable().isChildDataMap() &&
1:5508460:         operationContext != null) {
1:5508460:       uuid = operationContext.getProperty("uuid").toString();
1:5508460:     }
1:8d3c774:     CarbonLoaderUtil
1:8d3c774:         .populateNewLoadMetaEntry(newMetaEntry, SegmentStatus.SUCCESS, loadModel.getFactTimeStamp(),
1:8d3c774:             true);
1:a89587e:     CarbonTable carbonTable = loadModel.getCarbonDataLoadSchema().getCarbonTable();
1:a89587e:     long segmentSize = CarbonLoaderUtil
1:a89587e:         .addDataIndexSizeIntoMetaEntry(newMetaEntry, loadModel.getSegmentId(), carbonTable);
1:315f41c:     if (segmentSize > 0 || overwriteSet) {
1:5508460:       if (operationContext != null && carbonTable.hasAggregationDataMap()) {
1:5508460:         operationContext
1:8d3c774:             .setProperty("current.segmentfile", newMetaEntry.getSegmentFile());
1:829e7aa:         LoadEvents.LoadTablePreStatusUpdateEvent event =
1:829e7aa:             new LoadEvents.LoadTablePreStatusUpdateEvent(carbonTable.getCarbonTableIdentifier(),
1:829e7aa:                 loadModel);
1:829e7aa:         try {
1:5508460:           OperationListenerBus.getInstance().fireEvent(event, operationContext);
1:829e7aa:         } catch (Exception e) {
1:829e7aa:           throw new IOException(e);
1:829e7aa:         }
1:829e7aa:       }
1:dded5d5:       String uniqueId = null;
1:dded5d5:       if (overwriteSet) {
1:b7b8073:         if (!loadModel.isCarbonTransactionalTable()) {
1:b7b8073:           CarbonLoaderUtil.deleteNonTransactionalTableForInsertOverwrite(loadModel);
1:b7b8073:         } else {
1:b7b8073:           if (segmentSize == 0) {
1:b7b8073:             newMetaEntry.setSegmentStatus(SegmentStatus.MARKED_FOR_DELETE);
1:b7b8073:           }
1:5508460:           uniqueId = overwritePartitions(loadModel, newMetaEntry, uuid);
1:8d3c774:         }
1:8d3c774:       } else {
1:5508460:         CarbonLoaderUtil.recordNewLoadMetadata(newMetaEntry, loadModel, false, false, uuid);
1:dded5d5:       }
1:747be9b:       DataMapStatusManager.disableAllLazyDataMaps(carbonTable);
1:54a381c:       if (operationContext != null) {
1:d680e9c:         LoadEvents.LoadTablePostStatusUpdateEvent postStatusUpdateEvent =
1:d680e9c:             new LoadEvents.LoadTablePostStatusUpdateEvent(loadModel);
1:dded5d5:         try {
1:8d3c774:           OperationListenerBus.getInstance()
1:5508460:               .fireEvent(postStatusUpdateEvent, operationContext);
1:dded5d5:         } catch (Exception e) {
1:54a381c:           throw new IOException(e);
1:d7852ab:         }
1:d7852ab:       }
1:a89587e:       String updateTime =
1:a89587e:           context.getConfiguration().get(CarbonTableOutputFormat.UPADTE_TIMESTAMP, null);
1:cdf2c02:       String segmentsToBeDeleted =
1:cdf2c02:           context.getConfiguration().get(CarbonTableOutputFormat.SEGMENTS_TO_BE_DELETED, "");
1:c58eb43:       List<Segment> segmentDeleteList = Segment.toSegmentList(segmentsToBeDeleted.split(","), null);
1:8d3c774:       Set<Segment> segmentSet = new HashSet<>(
1:8f1a029:           new SegmentStatusManager(carbonTable.getAbsoluteTableIdentifier(),
1:8f1a029:               context.getConfiguration()).getValidAndInvalidSegments().getValidSegments());
1:a89587e:       if (updateTime != null) {
1:8d3c774:         CarbonUpdateUtil.updateTableMetadataStatus(segmentSet, carbonTable, updateTime, true,
1:cdf2c02:             segmentDeleteList);
1:dded5d5:       } else if (uniqueId != null) {
1:8d3c774:         CarbonUpdateUtil.updateTableMetadataStatus(segmentSet, carbonTable, uniqueId, true,
1:8d3c774:             segmentDeleteList);
1:d7852ab:       }
1:a89587e:     } else {
1:a89587e:       CarbonLoaderUtil.updateTableStatusForFailure(loadModel);
1:a89587e:     }
1:8d3c774:     if (segmentLock != null) {
1:8d3c774:       segmentLock.unlock();
1:8d3c774:     }
1:a89587e:   }
1:d7852ab: 
1:dded5d5:   /**
1:dded5d5:    * Overwrite the partitions in case of overwrite query. It just updates the partition map files
1:dded5d5:    * of all segment files.
1:dded5d5:    *
1:dded5d5:    * @param loadModel
1:dded5d5:    * @return
1:dded5d5:    * @throws IOException
1:dded5d5:    */
1:5508460:   private String overwritePartitions(CarbonLoadModel loadModel, LoadMetadataDetails newMetaEntry,
1:5508460:       String uuid) throws IOException {
1:dded5d5:     CarbonTable table = loadModel.getCarbonDataLoadSchema().getCarbonTable();
1:8d3c774:     SegmentFileStore fileStore = new SegmentFileStore(loadModel.getTablePath(),
1:8d3c774:         loadModel.getSegmentId() + "_" + loadModel.getFactTimeStamp()
1:8d3c774:             + CarbonTablePath.SEGMENT_EXT);
1:8d3c774:     List<PartitionSpec> partitionSpecs = fileStore.getPartitionSpecs();
1:8d3c774: 
1:8d3c774:     if (partitionSpecs != null && partitionSpecs.size() > 0) {
1:8d3c774:       List<Segment> validSegments =
1:8f1a029:           new SegmentStatusManager(table.getAbsoluteTableIdentifier())
1:8f1a029:               .getValidAndInvalidSegments().getValidSegments();
1:dded5d5:       String uniqueId = String.valueOf(System.currentTimeMillis());
1:8d3c774:       List<String> tobeUpdatedSegs = new ArrayList<>();
1:8d3c774:       List<String> tobeDeletedSegs = new ArrayList<>();
1:8d3c774:       // First drop the partitions from partition mapper files of each segment
1:8d3c774:       for (Segment segment : validSegments) {
1:8d3c774:         new SegmentFileStore(table.getTablePath(), segment.getSegmentFileName())
1:8d3c774:             .dropPartitions(segment, partitionSpecs, uniqueId, tobeDeletedSegs, tobeUpdatedSegs);
1:dded5d5: 
1:dded5d5:       }
1:8d3c774:       newMetaEntry.setUpdateStatusFileName(uniqueId);
1:dded5d5:       // Commit the removed partitions in carbon store.
1:5508460:       CarbonLoaderUtil.recordNewLoadMetadata(newMetaEntry, loadModel, false, false, uuid,
1:c58eb43:           Segment.toSegmentList(tobeDeletedSegs, null),
1:c58eb43:           Segment.toSegmentList(tobeUpdatedSegs, null));
1:dded5d5:       return uniqueId;
1:dded5d5:     }
1:dded5d5:     return null;
1:dded5d5:   }
1:dded5d5: 
1:3a6136d:   private Object getOperationContext() {
1:3a6136d:     // when validate segments is disabled in thread local update it to CarbonTableInputFormat
1:3a6136d:     CarbonSessionInfo carbonSessionInfo = ThreadLocalSessionInfo.getCarbonSessionInfo();
1:3a6136d:     if (carbonSessionInfo != null) {
1:3a6136d:       return carbonSessionInfo.getThreadParams().getExtraInfo("partition.operationcontext");
1:dded5d5:     }
1:3a6136d:     return null;
1:dded5d5:   }
1:91e6f6f: 
1:d7852ab:   /**
1:8d3c774:    * Update the tablestatus as fail if any fail happens.And also clean up the temp folders if any
1:8d3c774:    * are existed.
1:91e6f6f:    *
1:91e6f6f:    * @param context
1:91e6f6f:    * @param state
1:91e6f6f:    * @throws IOException
1:91e6f6f:    */
1:91e6f6f:   @Override public void abortJob(JobContext context, JobStatus.State state) throws IOException {
1:8d3c774:     try {
1:8d3c774:       super.abortJob(context, state);
1:8d3c774:       CarbonLoadModel loadModel = CarbonTableOutputFormat.getLoadModel(context.getConfiguration());
1:8d3c774:       CarbonLoaderUtil.updateTableStatusForFailure(loadModel);
1:8d3c774:       String segmentFileName = loadModel.getSegmentId() + "_" + loadModel.getFactTimeStamp();
1:8d3c774:       LoadMetadataDetails metadataDetail = loadModel.getCurrentLoadMetadataDetail();
1:8d3c774:       if (metadataDetail != null) {
1:8d3c774:         // In case the segment file is already created for this job then just link it so that it
1:8d3c774:         // will be used while cleaning.
1:8d3c774:         if (!metadataDetail.getSegmentStatus().equals(SegmentStatus.SUCCESS)) {
1:8d3c774:           String readPath = CarbonTablePath.getSegmentFilesLocation(loadModel.getTablePath())
1:8d3c774:               + CarbonCommonConstants.FILE_SEPARATOR + segmentFileName
1:8d3c774:               + CarbonTablePath.SEGMENT_EXT;
1:8d3c774:           if (FileFactory.getCarbonFile(readPath).exists()) {
1:8d3c774:             metadataDetail.setSegmentFile(segmentFileName + CarbonTablePath.SEGMENT_EXT);
1:8d3c774:           }
1:8d3c774:         }
1:8d3c774:       }
1:8d3c774:       // Clean the temp files
1:8d3c774:       CarbonFile segTmpFolder = FileFactory.getCarbonFile(
1:8d3c774:           CarbonTablePath.getSegmentFilesLocation(loadModel.getTablePath())
1:8d3c774:               + CarbonCommonConstants.FILE_SEPARATOR + segmentFileName + ".tmp");
1:8d3c774:       // delete temp segment folder
1:8d3c774:       if (segTmpFolder.exists()) {
1:8d3c774:         FileFactory.deleteAllCarbonFilesOfDir(segTmpFolder);
1:8d3c774:       }
1:8d3c774:       CarbonFile segmentFilePath = FileFactory.getCarbonFile(
1:8d3c774:           CarbonTablePath.getSegmentFilesLocation(loadModel.getTablePath())
1:8d3c774:               + CarbonCommonConstants.FILE_SEPARATOR + segmentFileName
1:8d3c774:               + CarbonTablePath.SEGMENT_EXT);
1:8d3c774:       // Delete the temp data folders of this job if exists
1:8d3c774:       if (segmentFilePath.exists()) {
1:8d3c774:         SegmentFileStore fileStore = new SegmentFileStore(loadModel.getTablePath(),
1:8d3c774:             segmentFileName + CarbonTablePath.SEGMENT_EXT);
1:8d3c774:         SegmentFileStore.removeTempFolder(fileStore.getLocationMap(), segmentFileName + ".tmp",
1:8d3c774:             loadModel.getTablePath());
1:8d3c774:       }
1:8d3c774:       LOGGER.error("Loading failed with job status : " + state);
1:8d3c774:     } finally {
1:8d3c774:       if (segmentLock != null) {
1:8d3c774:         segmentLock.unlock();
1:8d3c774:       }
1:8d3c774:     }
1:dded5d5:   }
1:91e6f6f: 
1:91e6f6f: }
============================================================================
author:kunal642
-------------------------------------------------------------------------------
commit:8f1a029
/////////////////////////////////////////////////////////////////////////
1:           new SegmentStatusManager(carbonTable.getAbsoluteTableIdentifier(),
1:               context.getConfiguration()).getValidAndInvalidSegments().getValidSegments());
/////////////////////////////////////////////////////////////////////////
1:           new SegmentStatusManager(table.getAbsoluteTableIdentifier())
1:               .getValidAndInvalidSegments().getValidSegments();
commit:5508460
/////////////////////////////////////////////////////////////////////////
1:     OperationContext operationContext = (OperationContext) getOperationContext();
1:     String uuid = "";
1:     if (loadModel.getCarbonDataLoadSchema().getCarbonTable().isChildDataMap() &&
1:         operationContext != null) {
1:       uuid = operationContext.getProperty("uuid").toString();
1:     }
/////////////////////////////////////////////////////////////////////////
1:       if (operationContext != null && carbonTable.hasAggregationDataMap()) {
1:         operationContext
1:           OperationListenerBus.getInstance().fireEvent(event, operationContext);
/////////////////////////////////////////////////////////////////////////
1:         uniqueId = overwritePartitions(loadModel, newMetaEntry, uuid);
1:         CarbonLoaderUtil.recordNewLoadMetadata(newMetaEntry, loadModel, false, false, uuid);
/////////////////////////////////////////////////////////////////////////
1:               .fireEvent(postStatusUpdateEvent, operationContext);
/////////////////////////////////////////////////////////////////////////
1:   private String overwritePartitions(CarbonLoadModel loadModel, LoadMetadataDetails newMetaEntry,
1:       String uuid) throws IOException {
/////////////////////////////////////////////////////////////////////////
1:       CarbonLoaderUtil.recordNewLoadMetadata(newMetaEntry, loadModel, false, false, uuid,
commit:d680e9c
/////////////////////////////////////////////////////////////////////////
1:         LoadEvents.LoadTablePostStatusUpdateEvent postStatusUpdateEvent =
1:             new LoadEvents.LoadTablePostStatusUpdateEvent(loadModel);
0:           OperationListenerBus.getInstance().fireEvent(postStatusUpdateEvent,
0:               (OperationContext) operationContext);
author:Raghunandan S
-------------------------------------------------------------------------------
commit:7ef9164
/////////////////////////////////////////////////////////////////////////
1:       if (null == newMetaEntry) {
1:         throw new RuntimeException("Internal Error");
1:       }
author:Jacky Li
-------------------------------------------------------------------------------
commit:747be9b
/////////////////////////////////////////////////////////////////////////
1:       DataMapStatusManager.disableAllLazyDataMaps(carbonTable);
commit:7e0803f
/////////////////////////////////////////////////////////////////////////
1:     String segmentFileName = SegmentFileStore.genSegmentFileName(
1:         loadModel.getSegmentId(), String.valueOf(loadModel.getFactTimeStamp()));
author:sounakr
-------------------------------------------------------------------------------
commit:b7b8073
/////////////////////////////////////////////////////////////////////////
1:         if (!loadModel.isCarbonTransactionalTable()) {
1:           CarbonLoaderUtil.deleteNonTransactionalTableForInsertOverwrite(loadModel);
1:         } else {
1:           if (segmentSize == 0) {
1:             newMetaEntry.setSegmentStatus(SegmentStatus.MARKED_FOR_DELETE);
1:           }
0:           uniqueId = overwritePartitions(loadModel, newMetaEntry, uuid);
commit:c58eb43
/////////////////////////////////////////////////////////////////////////
1:       List<Segment> segmentDeleteList = Segment.toSegmentList(segmentsToBeDeleted.split(","), null);
/////////////////////////////////////////////////////////////////////////
1:           Segment.toSegmentList(tobeDeletedSegs, null),
1:           Segment.toSegmentList(tobeUpdatedSegs, null));
author:ravipesala
-------------------------------------------------------------------------------
commit:9fba684
/////////////////////////////////////////////////////////////////////////
1: import org.apache.carbondata.core.datamap.status.DataMapStatusManager;
/////////////////////////////////////////////////////////////////////////
0:       DataMapStatusManager.disableDataMapsOfTable(carbonTable);
commit:8d3c774
/////////////////////////////////////////////////////////////////////////
1: import java.util.ArrayList;
1: import java.util.HashSet;
1: import java.util.List;
1: import java.util.Set;
1: import org.apache.carbondata.core.constants.CarbonCommonConstants;
1: import org.apache.carbondata.core.datamap.Segment;
1: import org.apache.carbondata.core.datastore.filesystem.CarbonFile;
1: import org.apache.carbondata.core.datastore.impl.FileFactory;
1: import org.apache.carbondata.core.indexstore.PartitionSpec;
1: import org.apache.carbondata.core.locks.CarbonLockFactory;
1: import org.apache.carbondata.core.locks.ICarbonLock;
1: import org.apache.carbondata.core.locks.LockUsage;
1: import org.apache.carbondata.core.metadata.SegmentFileStore;
/////////////////////////////////////////////////////////////////////////
1:   private ICarbonLock segmentLock;
1: 
/////////////////////////////////////////////////////////////////////////
1:     if (loadModel.getSegmentId() == null) {
1:       CarbonLoaderUtil.readAndUpdateLoadProgressInTableMeta(loadModel, overwriteSet);
1:     }
1:     // Take segment lock
1:     segmentLock = CarbonLockFactory.getCarbonLockObj(
1:         loadModel.getCarbonDataLoadSchema().getCarbonTable().getAbsoluteTableIdentifier(),
1:         CarbonTablePath.addSegmentPrefix(loadModel.getSegmentId()) + LockUsage.LOCK);
1:     if (!segmentLock.lockWithRetries()) {
1:       throw new RuntimeException("Already segment is locked for loading, not supposed happen");
1:     }
/////////////////////////////////////////////////////////////////////////
1:     String readPath = CarbonTablePath.getSegmentFilesLocation(loadModel.getTablePath())
1:         + CarbonCommonConstants.FILE_SEPARATOR
1:         + loadModel.getSegmentId() + "_" + loadModel.getFactTimeStamp() + ".tmp";
1:     String segmentFileName = loadModel.getSegmentId() + "_" + loadModel.getFactTimeStamp();
1:     SegmentFileStore.SegmentFile segmentFile = SegmentFileStore
1:         .mergeSegmentFiles(readPath, segmentFileName,
1:             CarbonTablePath.getSegmentFilesLocation(loadModel.getTablePath()));
1:     if (segmentFile != null) {
1:       // Move all files from temp directory of each segment to partition directory
1:       SegmentFileStore.moveFromTempFolder(segmentFile,
1:           loadModel.getSegmentId() + "_" + loadModel.getFactTimeStamp() + ".tmp",
1:           loadModel.getTablePath());
1:       newMetaEntry.setSegmentFile(segmentFileName + CarbonTablePath.SEGMENT_EXT);
1:     }
1:     CarbonLoaderUtil
1:         .populateNewLoadMetaEntry(newMetaEntry, SegmentStatus.SUCCESS, loadModel.getFactTimeStamp(),
1:             true);
0:         ((OperationContext) operationContext)
1:             .setProperty("current.segmentfile", newMetaEntry.getSegmentFile());
0:         if (segmentSize == 0) {
0:           newMetaEntry.setSegmentStatus(SegmentStatus.MARKED_FOR_DELETE);
1:         }
0:         uniqueId = overwritePartitions(loadModel, newMetaEntry);
1:       } else {
0:         CarbonLoaderUtil.recordNewLoadMetadata(newMetaEntry, loadModel, false, false);
0:         LoadEvents.LoadTablePostStatusUpdateEvent postStatusUpdateEvent =
0:             new LoadEvents.LoadTablePostStatusUpdateEvent(loadModel);
0:             new LoadEvents.LoadTableMergePartitionEvent(readPath);
0:               .fireEvent(postStatusUpdateEvent, (OperationContext) operationContext);
1:           OperationListenerBus.getInstance()
/////////////////////////////////////////////////////////////////////////
0:       List<Segment> segmentDeleteList = Segment.toSegmentList(segmentsToBeDeleted.split(","));
1:       Set<Segment> segmentSet = new HashSet<>(
1:         CarbonUpdateUtil.updateTableMetadataStatus(segmentSet, carbonTable, updateTime, true,
1:         CarbonUpdateUtil.updateTableMetadataStatus(segmentSet, carbonTable, uniqueId, true,
1:             segmentDeleteList);
1:     if (segmentLock != null) {
1:       segmentLock.unlock();
1:     }
/////////////////////////////////////////////////////////////////////////
0:   private String overwritePartitions(CarbonLoadModel loadModel, LoadMetadataDetails newMetaEntry)
0:       throws IOException {
1:     SegmentFileStore fileStore = new SegmentFileStore(loadModel.getTablePath(),
1:         loadModel.getSegmentId() + "_" + loadModel.getFactTimeStamp()
1:             + CarbonTablePath.SEGMENT_EXT);
1:     List<PartitionSpec> partitionSpecs = fileStore.getPartitionSpecs();
1: 
1:     if (partitionSpecs != null && partitionSpecs.size() > 0) {
1:       List<Segment> validSegments =
1:       List<String> tobeUpdatedSegs = new ArrayList<>();
1:       List<String> tobeDeletedSegs = new ArrayList<>();
1:       // First drop the partitions from partition mapper files of each segment
1:       for (Segment segment : validSegments) {
1:         new SegmentFileStore(table.getTablePath(), segment.getSegmentFileName())
1:             .dropPartitions(segment, partitionSpecs, uniqueId, tobeDeletedSegs, tobeUpdatedSegs);
1:       newMetaEntry.setUpdateStatusFileName(uniqueId);
0:       CarbonLoaderUtil.recordNewLoadMetadata(newMetaEntry, loadModel, false, false, "",
0:           Segment.toSegmentList(tobeDeletedSegs), Segment.toSegmentList(tobeUpdatedSegs));
/////////////////////////////////////////////////////////////////////////
1:    * Update the tablestatus as fail if any fail happens.And also clean up the temp folders if any
1:    * are existed.
1:     try {
1:       super.abortJob(context, state);
1:       CarbonLoadModel loadModel = CarbonTableOutputFormat.getLoadModel(context.getConfiguration());
1:       CarbonLoaderUtil.updateTableStatusForFailure(loadModel);
0:       String segmentFileName = loadModel.getSegmentId() + "_" + loadModel.getFactTimeStamp();
1:       LoadMetadataDetails metadataDetail = loadModel.getCurrentLoadMetadataDetail();
1:       if (metadataDetail != null) {
1:         // In case the segment file is already created for this job then just link it so that it
1:         // will be used while cleaning.
1:         if (!metadataDetail.getSegmentStatus().equals(SegmentStatus.SUCCESS)) {
1:           String readPath = CarbonTablePath.getSegmentFilesLocation(loadModel.getTablePath())
1:               + CarbonCommonConstants.FILE_SEPARATOR + segmentFileName
1:               + CarbonTablePath.SEGMENT_EXT;
1:           if (FileFactory.getCarbonFile(readPath).exists()) {
1:             metadataDetail.setSegmentFile(segmentFileName + CarbonTablePath.SEGMENT_EXT);
1:           }
1:         }
1:       }
1:       // Clean the temp files
1:       CarbonFile segTmpFolder = FileFactory.getCarbonFile(
1:           CarbonTablePath.getSegmentFilesLocation(loadModel.getTablePath())
1:               + CarbonCommonConstants.FILE_SEPARATOR + segmentFileName + ".tmp");
1:       // delete temp segment folder
1:       if (segTmpFolder.exists()) {
1:         FileFactory.deleteAllCarbonFilesOfDir(segTmpFolder);
1:       }
1:       CarbonFile segmentFilePath = FileFactory.getCarbonFile(
1:           CarbonTablePath.getSegmentFilesLocation(loadModel.getTablePath())
1:               + CarbonCommonConstants.FILE_SEPARATOR + segmentFileName
1:               + CarbonTablePath.SEGMENT_EXT);
1:       // Delete the temp data folders of this job if exists
1:       if (segmentFilePath.exists()) {
1:         SegmentFileStore fileStore = new SegmentFileStore(loadModel.getTablePath(),
1:             segmentFileName + CarbonTablePath.SEGMENT_EXT);
1:         SegmentFileStore.removeTempFolder(fileStore.getLocationMap(), segmentFileName + ".tmp",
1:             loadModel.getTablePath());
1:       }
1:       LOGGER.error("Loading failed with job status : " + state);
1:     } finally {
1:       if (segmentLock != null) {
1:         segmentLock.unlock();
1:       }
1:     }
commit:dded5d5
/////////////////////////////////////////////////////////////////////////
0: import java.util.*;
/////////////////////////////////////////////////////////////////////////
1:       String uniqueId = null;
1:       if (overwriteSet) {
0:         uniqueId = overwritePartitions(loadModel);
1:       }
0:       CarbonLoaderUtil.recordNewLoadMetadata(newMetaEntry, loadModel, false, false);
/////////////////////////////////////////////////////////////////////////
0:       Set<String> segmentSet = new HashSet<>(
0:           new SegmentStatusManager(carbonTable.getAbsoluteTableIdentifier())
0:               .getValidAndInvalidSegments().getValidSegments());
1:       } else if (uniqueId != null) {
0:         // Update the loadstatus with update time to clear cache from driver.
0:         CarbonUpdateUtil.updateTableMetadataStatus(
0:             segmentSet,
0:             carbonTable,
0:             uniqueId,
0:             true,
0:             new ArrayList<String>());
1:   /**
1:    * Overwrite the partitions in case of overwrite query. It just updates the partition map files
1:    * of all segment files.
1:    *
1:    * @param loadModel
1:    * @return
1:    * @throws IOException
1:    */
0:   private String overwritePartitions(CarbonLoadModel loadModel) throws IOException {
1:     CarbonTable table = loadModel.getCarbonDataLoadSchema().getCarbonTable();
0:     String currentSegmentPath =
0:         CarbonTablePath.getSegmentPath(loadModel.getTablePath(), loadModel.getSegmentId());
0:     PartitionMapFileStore partitionMapFileStore = new PartitionMapFileStore();
0:     partitionMapFileStore.readAllPartitionsOfSegment(currentSegmentPath);
0:     List<List<String>> partitionsToDrop =
0:         new ArrayList<List<String>>(partitionMapFileStore.getPartitionMap().values());
0:     if (partitionsToDrop.size() > 0) {
0:       List<String> validSegments =
0:           new SegmentStatusManager(table.getAbsoluteTableIdentifier()).getValidAndInvalidSegments()
0:               .getValidSegments();
1:       String uniqueId = String.valueOf(System.currentTimeMillis());
1:       try {
0:         // First drop the partitions from partition mapper files of each segment
0:         for (String segment : validSegments) {
0:           new PartitionMapFileStore()
0:               .dropPartitions(CarbonTablePath.getSegmentPath(table.getTablePath(), segment),
0:                   new ArrayList<List<String>>(partitionsToDrop), uniqueId, false);
1: 
1:         }
1:       } catch (Exception e) {
0:         // roll back the drop partitions from carbon store
0:         for (String segment : validSegments) {
0:           new PartitionMapFileStore()
0:               .commitPartitions(CarbonTablePath.getSegmentPath(table.getTablePath(), segment),
0:                   uniqueId, false, table.getTablePath(), partitionsToDrop.get(0));
1:         }
1:       }
1:       // Commit the removed partitions in carbon store.
0:       for (String segment : validSegments) {
0:         new PartitionMapFileStore()
0:             .commitPartitions(CarbonTablePath.getSegmentPath(table.getTablePath(), segment),
0:                 uniqueId, true, table.getTablePath(), partitionsToDrop.get(0));
1:       }
1:       return uniqueId;
1:     }
1:     return null;
1:   }
1: 
commit:af4277e
/////////////////////////////////////////////////////////////////////////
1:     try {
1:       super.commitJob(context);
1:     } catch (IOException e) {
1:       // ignore, in case of concurrent load it try to remove temporary folders by other load may
1:       // cause file not found exception. This will not impact carbon load,
1:       LOGGER.warn(e.getMessage());
1:     }
commit:829e7aa
/////////////////////////////////////////////////////////////////////////
1: import org.apache.carbondata.events.OperationContext;
1: import org.apache.carbondata.events.OperationListenerBus;
0: import org.apache.carbondata.hadoop.util.ObjectSerializationUtil;
1: import org.apache.carbondata.processing.loading.events.LoadEvents;
/////////////////////////////////////////////////////////////////////////
0:       String operationContextStr =
0:           context.getConfiguration().get(
0:               CarbonTableOutputFormat.OPERATION_CONTEXT,
0:               null);
0:       if (operationContextStr != null) {
0:         OperationContext operationContext =
0:             (OperationContext) ObjectSerializationUtil.convertStringToObject(operationContextStr);
1:         LoadEvents.LoadTablePreStatusUpdateEvent event =
1:             new LoadEvents.LoadTablePreStatusUpdateEvent(carbonTable.getCarbonTableIdentifier(),
1:                 loadModel);
1:         try {
0:           OperationListenerBus.getInstance().fireEvent(event, operationContext);
1:         } catch (Exception e) {
1:           throw new IOException(e);
1:         }
1:       }
commit:cdf2c02
/////////////////////////////////////////////////////////////////////////
0: import java.util.*;
/////////////////////////////////////////////////////////////////////////
1:       String segmentsToBeDeleted =
1:           context.getConfiguration().get(CarbonTableOutputFormat.SEGMENTS_TO_BE_DELETED, "");
0:       List<String> segmentDeleteList = Arrays.asList(segmentsToBeDeleted.split(","));
0:         CarbonUpdateUtil.updateTableMetadataStatus(
0:             segmentSet,
0:             carbonTable,
0:             updateTime,
0:             true,
1:             segmentDeleteList);
commit:d7852ab
/////////////////////////////////////////////////////////////////////////
0: import org.apache.carbondata.core.constants.CarbonCommonConstants;
0: import org.apache.carbondata.core.util.CarbonProperties;
/////////////////////////////////////////////////////////////////////////
0:       mergeCarbonIndexFiles(segmentPath);
/////////////////////////////////////////////////////////////////////////
0:    * Merge index files to a new single file.
1:    */
0:   private void mergeCarbonIndexFiles(String segmentPath) throws IOException {
0:     boolean mergeIndex = false;
0:     try {
0:       mergeIndex = Boolean.parseBoolean(CarbonProperties.getInstance().getProperty(
0:           CarbonCommonConstants.CARBON_MERGE_INDEX_IN_SEGMENT,
0:           CarbonCommonConstants.CARBON_MERGE_INDEX_IN_SEGMENT_DEFAULT));
0:     } catch (Exception e) {
0:       mergeIndex = Boolean.parseBoolean(
0:           CarbonCommonConstants.CARBON_MERGE_INDEX_IN_SEGMENT_DEFAULT);
1:     }
0:     if (mergeIndex) {
0:       new CarbonIndexFileMergeWriter().mergeCarbonIndexFilesOfSegment(segmentPath);
1:     }
1:   }
1: 
1:   /**
commit:a89587e
/////////////////////////////////////////////////////////////////////////
0: import java.util.ArrayList;
0: import java.util.HashSet;
0: import java.util.Set;
1: import org.apache.carbondata.core.metadata.schema.table.CarbonTable;
1: import org.apache.carbondata.core.mutate.CarbonUpdateUtil;
1: import org.apache.carbondata.core.statusmanager.SegmentStatusManager;
/////////////////////////////////////////////////////////////////////////
1:     LoadMetadataDetails newMetaEntry = loadModel.getCurrentLoadMetadataDetail();
1:     CarbonTable carbonTable = loadModel.getCarbonDataLoadSchema().getCarbonTable();
1:     long segmentSize = CarbonLoaderUtil
1:         .addDataIndexSizeIntoMetaEntry(newMetaEntry, loadModel.getSegmentId(), carbonTable);
0:     if (segmentSize > 0) {
0:       CarbonLoaderUtil.recordNewLoadMetadata(newMetaEntry, loadModel, false, overwriteSet);
0:       new CarbonIndexFileMergeWriter().mergeCarbonIndexFilesOfSegment(segmentPath);
1:       String updateTime =
1:           context.getConfiguration().get(CarbonTableOutputFormat.UPADTE_TIMESTAMP, null);
1:       if (updateTime != null) {
0:         Set<String> segmentSet = new HashSet<>(
0:             new SegmentStatusManager(carbonTable.getAbsoluteTableIdentifier())
0:                 .getValidAndInvalidSegments().getValidSegments());
0:         CarbonUpdateUtil.updateTableMetadataStatus(segmentSet, carbonTable, updateTime, true,
0:             new ArrayList<String>());
1:       }
1:     } else {
1:       CarbonLoaderUtil.updateTableStatusForFailure(loadModel);
1:     }
commit:3ff55a2
/////////////////////////////////////////////////////////////////////////
0: import org.apache.carbondata.core.writer.CarbonIndexFileMergeWriter;
/////////////////////////////////////////////////////////////////////////
0:     String segmentPath =
0:         CarbonTablePath.getSegmentPath(loadModel.getTablePath(), loadModel.getSegmentId());
1:     // Merge all partition files into a single file.
0:     new PartitionMapFileStore().mergePartitionMapFiles(segmentPath,
0:         loadModel.getFactTimeStamp() + "");
0:     new CarbonIndexFileMergeWriter().mergeCarbonIndexFilesOfSegment(segmentPath);
commit:4430178
/////////////////////////////////////////////////////////////////////////
0: import org.apache.carbondata.core.metadata.PartitionMapFileStore;
1: import org.apache.carbondata.core.util.path.CarbonTablePath;
/////////////////////////////////////////////////////////////////////////
1:   @Override public void setupTask(TaskAttemptContext context) throws IOException {
1:     super.setupTask(context);
1:   }
1: 
/////////////////////////////////////////////////////////////////////////
0:     String segmentPath =
0:         CarbonTablePath.getSegmentPath(loadModel.getTablePath(), loadModel.getSegmentId());
0:     // Merge all partition files into a single file.
0:     new PartitionMapFileStore().mergePartitionMapFiles(segmentPath);
commit:91e6f6f
/////////////////////////////////////////////////////////////////////////
1: /*
1:  * Licensed to the Apache Software Foundation (ASF) under one or more
1:  * contributor license agreements.  See the NOTICE file distributed with
1:  * this work for additional information regarding copyright ownership.
1:  * The ASF licenses this file to You under the Apache License, Version 2.0
1:  * (the "License"); you may not use this file except in compliance with
1:  * the License.  You may obtain a copy of the License at
1:  *
1:  *    http://www.apache.org/licenses/LICENSE-2.0
1:  *
1:  * Unless required by applicable law or agreed to in writing, software
1:  * distributed under the License is distributed on an "AS IS" BASIS,
1:  * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
1:  * See the License for the specific language governing permissions and
1:  * limitations under the License.
1:  */
1: 
1: package org.apache.carbondata.hadoop.api;
1: 
1: import java.io.IOException;
1: 
1: import org.apache.carbondata.common.logging.LogService;
1: import org.apache.carbondata.common.logging.LogServiceFactory;
1: import org.apache.carbondata.core.statusmanager.LoadMetadataDetails;
1: import org.apache.carbondata.core.statusmanager.SegmentStatus;
1: import org.apache.carbondata.processing.loading.model.CarbonLoadModel;
1: import org.apache.carbondata.processing.util.CarbonLoaderUtil;
1: 
1: import org.apache.hadoop.fs.Path;
1: import org.apache.hadoop.mapreduce.JobContext;
1: import org.apache.hadoop.mapreduce.JobStatus;
1: import org.apache.hadoop.mapreduce.TaskAttemptContext;
1: import org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter;
1: 
1: /**
1:  * Outputcommitter which manages the segments during loading.It commits segment information to the
1:  * tablestatus file upon success or fail.
1:  */
1: public class CarbonOutputCommitter extends FileOutputCommitter {
1: 
1:   private static final LogService LOGGER =
1:       LogServiceFactory.getLogService(CarbonOutputCommitter.class.getName());
1: 
1:   public CarbonOutputCommitter(Path outputPath, TaskAttemptContext context) throws IOException {
1:     super(outputPath, context);
1:   }
1: 
1:   /**
1:    * Update the tablestatus with inprogress while setup the job.
1:    *
1:    * @param context
1:    * @throws IOException
1:    */
1:   @Override public void setupJob(JobContext context) throws IOException {
1:     super.setupJob(context);
1:     boolean overwriteSet = CarbonTableOutputFormat.isOverwriteSet(context.getConfiguration());
1:     CarbonLoadModel loadModel = CarbonTableOutputFormat.getLoadModel(context.getConfiguration());
0:     CarbonLoaderUtil.readAndUpdateLoadProgressInTableMeta(loadModel, overwriteSet);
1:     CarbonTableOutputFormat.setLoadModel(context.getConfiguration(), loadModel);
1:   }
1: 
1:   /**
1:    * Update the tablestatus as success after job is success
1:    *
1:    * @param context
1:    * @throws IOException
1:    */
1:   @Override public void commitJob(JobContext context) throws IOException {
0:     super.commitJob(context);
1:     boolean overwriteSet = CarbonTableOutputFormat.isOverwriteSet(context.getConfiguration());
1:     CarbonLoadModel loadModel = CarbonTableOutputFormat.getLoadModel(context.getConfiguration());
0:     LoadMetadataDetails newMetaEntry = loadModel.getCurrentLoadMetadataDetail();
0:     CarbonLoaderUtil.populateNewLoadMetaEntry(newMetaEntry, SegmentStatus.SUCCESS,
0:         loadModel.getFactTimeStamp(), true);
0:     CarbonLoaderUtil.addDataIndexSizeIntoMetaEntry(newMetaEntry, loadModel.getSegmentId(),
0:         loadModel.getCarbonDataLoadSchema().getCarbonTable());
0:     CarbonLoaderUtil.recordNewLoadMetadata(newMetaEntry, loadModel, false, overwriteSet);
1:   }
1: 
1:   /**
0:    * Update the tablestatus as fail if any fail happens.
1:    *
1:    * @param context
1:    * @param state
1:    * @throws IOException
1:    */
1:   @Override public void abortJob(JobContext context, JobStatus.State state) throws IOException {
0:     super.abortJob(context, state);
1:     CarbonLoadModel loadModel = CarbonTableOutputFormat.getLoadModel(context.getConfiguration());
0:     CarbonLoaderUtil.updateTableStatusForFailure(loadModel);
0:     LOGGER.error("Loading failed with job status : " + state);
1:   }
1: 
1: }
author:dhatchayani
-------------------------------------------------------------------------------
commit:f5cdd5c
/////////////////////////////////////////////////////////////////////////
commit:54a381c
/////////////////////////////////////////////////////////////////////////
/////////////////////////////////////////////////////////////////////////
1:       if (operationContext != null) {
0:         LoadEvents.LoadTableMergePartitionEvent loadTableMergePartitionEvent =
0:             new LoadEvents.LoadTableMergePartitionEvent(segmentPath);
0:         try {
0:           OperationListenerBus.getInstance()
0:               .fireEvent(loadTableMergePartitionEvent, (OperationContext) operationContext);
0:         } catch (Exception e) {
1:           throw new IOException(e);
0:         }
0:       }
/////////////////////////////////////////////////////////////////////////
author:kumarvishal
-------------------------------------------------------------------------------
commit:3a6136d
/////////////////////////////////////////////////////////////////////////
0: import java.util.Arrays;
0: import java.util.HashSet;
0: import java.util.List;
0: import java.util.Set;
/////////////////////////////////////////////////////////////////////////
1: import org.apache.carbondata.core.util.CarbonSessionInfo;
1: import org.apache.carbondata.core.util.ThreadLocalSessionInfo;
/////////////////////////////////////////////////////////////////////////
0:       Object operationContext = getOperationContext();
0:       if (operationContext != null) {
0:           OperationListenerBus.getInstance().fireEvent(event, (OperationContext) operationContext);
/////////////////////////////////////////////////////////////////////////
1:   private Object getOperationContext() {
1:     // when validate segments is disabled in thread local update it to CarbonTableInputFormat
1:     CarbonSessionInfo carbonSessionInfo = ThreadLocalSessionInfo.getCarbonSessionInfo();
1:     if (carbonSessionInfo != null) {
1:       return carbonSessionInfo.getThreadParams().getExtraInfo("partition.operationcontext");
0:     }
1:     return null;
0:   }
0: 
author:akashrn5
-------------------------------------------------------------------------------
commit:315f41c
/////////////////////////////////////////////////////////////////////////
0:     CarbonLoaderUtil.checkAndCreateCarbonDataLocation(loadModel.getSegmentId(),
0:         loadModel.getCarbonDataLoadSchema().getCarbonTable());
/////////////////////////////////////////////////////////////////////////
1:     if (segmentSize > 0 || overwriteSet) {
============================================================================