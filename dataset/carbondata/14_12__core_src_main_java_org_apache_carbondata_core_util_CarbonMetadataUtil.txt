1:cd6a4ff: /*
1:41347d8:  * Licensed to the Apache Software Foundation (ASF) under one or more
1:41347d8:  * contributor license agreements.  See the NOTICE file distributed with
1:41347d8:  * this work for additional information regarding copyright ownership.
1:41347d8:  * The ASF licenses this file to You under the Apache License, Version 2.0
1:41347d8:  * (the "License"); you may not use this file except in compliance with
1:41347d8:  * the License.  You may obtain a copy of the License at
1:cd6a4ff:  *
1:cd6a4ff:  *    http://www.apache.org/licenses/LICENSE-2.0
1:cd6a4ff:  *
1:41347d8:  * Unless required by applicable law or agreed to in writing, software
1:41347d8:  * distributed under the License is distributed on an "AS IS" BASIS,
1:41347d8:  * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
1:41347d8:  * See the License for the specific language governing permissions and
1:41347d8:  * limitations under the License.
1:cd6a4ff:  */
1:cd6a4ff: package org.apache.carbondata.core.util;
13:cd6a4ff: 
1:cd6a4ff: import java.io.IOException;
1:cd6a4ff: import java.nio.ByteBuffer;
1:cd6a4ff: import java.util.ArrayList;
1:cd6a4ff: import java.util.List;
1:cd6a4ff: 
1:e710339: import org.apache.carbondata.core.datastore.blocklet.BlockletEncodedColumnPage;
1:e710339: import org.apache.carbondata.core.datastore.blocklet.EncodedBlocklet;
1:8f08c4a: import org.apache.carbondata.core.datastore.compression.CompressorFactory;
1:e710339: import org.apache.carbondata.core.datastore.page.encoding.EncodedColumnPage;
1:bc3e684: import org.apache.carbondata.core.datastore.page.statistics.TablePageStatistics;
1:ce09aaa: import org.apache.carbondata.core.metadata.ColumnarFormatVersion;
1:2cf1104: import org.apache.carbondata.core.metadata.datatype.DataType;
1:956833e: import org.apache.carbondata.core.metadata.datatype.DataTypes;
1:ce09aaa: import org.apache.carbondata.core.metadata.index.BlockIndexInfo;
1:2cf1104: import org.apache.carbondata.core.metadata.schema.table.column.CarbonMeasure;
1:cd6a4ff: import org.apache.carbondata.format.BlockIndex;
1:cd6a4ff: import org.apache.carbondata.format.BlockletBTreeIndex;
1:cd6a4ff: import org.apache.carbondata.format.BlockletIndex;
1:2cf1104: import org.apache.carbondata.format.BlockletInfo3;
1:cd6a4ff: import org.apache.carbondata.format.BlockletMinMaxIndex;
1:cd6a4ff: import org.apache.carbondata.format.ChunkCompressionMeta;
1:cd6a4ff: import org.apache.carbondata.format.ColumnSchema;
1:cd6a4ff: import org.apache.carbondata.format.CompressionCodec;
1:d54dc64: import org.apache.carbondata.format.DataChunk2;
1:2cf1104: import org.apache.carbondata.format.DataChunk3;
1:b41e48f: import org.apache.carbondata.format.FileFooter3;
1:b41e48f: import org.apache.carbondata.format.FileHeader;
1:cd6a4ff: import org.apache.carbondata.format.IndexHeader;
1:e710339: import org.apache.carbondata.format.LocalDictionaryChunk;
1:cd6a4ff: import org.apache.carbondata.format.SegmentInfo;
1:cd6a4ff: 
1:cd6a4ff: /**
1:cd6a4ff:  * Util class to convert to thrift metdata classes
1:cd6a4ff:  */
1:cd6a4ff: public class CarbonMetadataUtil {
1:cd6a4ff: 
1:f911403:   private CarbonMetadataUtil() {
1:9e064ee:   }
1:cd6a4ff: 
1:cd6a4ff:   /**
1:b41e48f:    * Below method prepares the file footer object for carbon data file version 3
1:cd6a4ff:    *
1:cd6a4ff:    * @param infoList
1:086b06d:    * @param blockletIndexs
1:cd6a4ff:    * @param cardinalities
1:086b06d:    * @param numberOfColumns
1:cd6a4ff:    * @return FileFooter
1:cd6a4ff:    */
1:b41e48f:   public static FileFooter3 convertFileFooterVersion3(List<BlockletInfo3> infoList,
1:dc83b2a:       List<BlockletIndex> blockletIndexs, int[] cardinalities, int numberOfColumns)
2:cd6a4ff:       throws IOException {
1:ebf13dc:     FileFooter3 footer = getFileFooter3(infoList, blockletIndexs, cardinalities, numberOfColumns);
1:2cf1104:     for (BlockletInfo3 info : infoList) {
1:2cf1104:       footer.addToBlocklet_info_list3(info);
1:9e064ee:     }
2:2cf1104:     return footer;
1:9e064ee:   }
1:cd6a4ff: 
1:21a72bf:   /**
1:2cf1104:    * Below method will be used to get the file footer object
1:cd6a4ff:    *
1:2cf1104:    * @param infoList         blocklet info
1:086b06d:    * @param blockletIndexs
1:2cf1104:    * @param cardinalities    cardinlaity of dimension columns
1:086b06d:    * @param numberOfColumns
1:2cf1104:    * @return file footer
1:cd6a4ff:    */
1:b41e48f:   private static FileFooter3 getFileFooter3(List<BlockletInfo3> infoList,
1:b41e48f:       List<BlockletIndex> blockletIndexs, int[] cardinalities, int numberOfColumns) {
1:cd6a4ff:     SegmentInfo segmentInfo = new SegmentInfo();
1:b41e48f:     segmentInfo.setNum_cols(numberOfColumns);
1:cd6a4ff:     segmentInfo.setColumn_cardinalities(CarbonUtil.convertToIntegerList(cardinalities));
1:b41e48f:     FileFooter3 footer = new FileFooter3();
1:2cf1104:     footer.setNum_rows(getNumberOfRowForFooter(infoList));
1:cd6a4ff:     footer.setSegment_info(segmentInfo);
1:2cf1104:     for (BlockletIndex info : blockletIndexs) {
1:2cf1104:       footer.addToBlocklet_index_list(info);
1:9e064ee:     }
1:cd6a4ff:     return footer;
10:cd6a4ff:   }
1:cd6a4ff: 
1:cd6a4ff:   /**
1:21a72bf:    * convert external thrift BlockletMinMaxIndex to BlockletMinMaxIndex of carbon metadata
1:21a72bf:    */
1:21a72bf:   public static org.apache.carbondata.core.metadata.blocklet.index.BlockletMinMaxIndex
1:21a72bf:       convertExternalMinMaxIndex(BlockletMinMaxIndex minMaxIndex) {
1:21a72bf:     if (minMaxIndex == null) {
1:21a72bf:       return null;
1:21a72bf:     }
1:21a72bf: 
1:21a72bf:     return new org.apache.carbondata.core.metadata.blocklet.index.BlockletMinMaxIndex(
1:21a72bf:             minMaxIndex.getMin_values(), minMaxIndex.getMax_values());
1:21a72bf:   }
1:21a72bf: 
1:21a72bf:   /**
1:21a72bf:    * convert BlockletMinMaxIndex of carbon metadata to external thrift BlockletMinMaxIndex
1:21a72bf:    */
1:21a72bf:   public static BlockletMinMaxIndex convertMinMaxIndex(
1:21a72bf:       org.apache.carbondata.core.metadata.blocklet.index.BlockletMinMaxIndex minMaxIndex) {
1:21a72bf:     if (minMaxIndex == null) {
1:21a72bf:       return null;
1:21a72bf:     }
1:21a72bf: 
1:cd6a4ff:     BlockletMinMaxIndex blockletMinMaxIndex = new BlockletMinMaxIndex();
1:9e064ee: 
1:21a72bf:     for (int i = 0; i < minMaxIndex.getMaxValues().length; i++) {
1:21a72bf:       blockletMinMaxIndex.addToMax_values(ByteBuffer.wrap(minMaxIndex.getMaxValues()[i]));
1:21a72bf:       blockletMinMaxIndex.addToMin_values(ByteBuffer.wrap(minMaxIndex.getMinValues()[i]));
1:9e064ee:     }
1:21a72bf: 
1:21a72bf:     return blockletMinMaxIndex;
1:21a72bf:   }
1:21a72bf: 
1:21a72bf:   public static BlockletIndex getBlockletIndex(
1:21a72bf:       org.apache.carbondata.core.metadata.blocklet.index.BlockletIndex info) {
1:21a72bf:     BlockletMinMaxIndex blockletMinMaxIndex = convertMinMaxIndex(info.getMinMaxIndex());
1:cd6a4ff:     BlockletBTreeIndex blockletBTreeIndex = new BlockletBTreeIndex();
1:cd6a4ff:     blockletBTreeIndex.setStart_key(info.getBtreeIndex().getStartKey());
1:cd6a4ff:     blockletBTreeIndex.setEnd_key(info.getBtreeIndex().getEndKey());
1:cd6a4ff:     BlockletIndex blockletIndex = new BlockletIndex();
1:cd6a4ff:     blockletIndex.setMin_max_index(blockletMinMaxIndex);
1:cd6a4ff:     blockletIndex.setB_tree_index(blockletBTreeIndex);
1:cd6a4ff:     return blockletIndex;
1:cd6a4ff:   }
1:cd6a4ff: 
1:cd6a4ff:   /**
1:cd6a4ff:    * Get total number of rows for the file.
1:cd6a4ff:    *
1:cd6a4ff:    * @param infoList
2:cd6a4ff:    * @return
1:cd6a4ff:    */
1:2cf1104:   private static long getNumberOfRowForFooter(List<BlockletInfo3> infoList) {
1:cd6a4ff:     long numberOfRows = 0;
1:2cf1104:     for (BlockletInfo3 info : infoList) {
1:2cf1104:       numberOfRows += info.num_rows;
1:cd6a4ff:     }
1:cd6a4ff:     return numberOfRows;
1:cd6a4ff:   }
1:cd6a4ff: 
1:e710339:   private static EncodedColumnPage[] getEncodedColumnPages(EncodedBlocklet encodedBlocklet,
1:e710339:       boolean isDimension, int pageIndex) {
1:e710339:     int size =
1:e710339:         isDimension ? encodedBlocklet.getNumberOfDimension() : encodedBlocklet.getNumberOfMeasure();
1:e710339:     EncodedColumnPage [] encodedPages = new EncodedColumnPage[size];
1:cd6a4ff: 
1:e710339:     for (int i = 0; i < size; i++) {
1:e710339:       if (isDimension) {
1:e710339:         encodedPages[i] =
1:e710339:             encodedBlocklet.getEncodedDimensionColumnPages().get(i).getEncodedColumnPageList()
1:e710339:                 .get(pageIndex);
1:cd6a4ff:       } else {
1:e710339:         encodedPages[i] =
1:e710339:             encodedBlocklet.getEncodedMeasureColumnPages().get(i).getEncodedColumnPageList()
1:e710339:                 .get(pageIndex);
1:cd6a4ff:       }
1:cd6a4ff:     }
1:e710339:     return encodedPages;
1:cd6a4ff:   }
1:e710339:   public static BlockletIndex getBlockletIndex(EncodedBlocklet encodedBlocklet,
1:2cf1104:       List<CarbonMeasure> carbonMeasureList) {
1:cd6a4ff:     BlockletMinMaxIndex blockletMinMaxIndex = new BlockletMinMaxIndex();
1:cd6a4ff: 
1:6b1ab2a:     // Calculating min/max for every each column.
1:e710339:     TablePageStatistics stats =
1:e710339:         new TablePageStatistics(getEncodedColumnPages(encodedBlocklet, true, 0),
1:e710339:             getEncodedColumnPages(encodedBlocklet, false, 0));
1:bc3e684:     byte[][] minCol = stats.getDimensionMinValue().clone();
1:bc3e684:     byte[][] maxCol = stats.getDimensionMaxValue().clone();
1:cd6a4ff: 
1:e710339:     for (int pageIndex = 0; pageIndex < encodedBlocklet.getNumberOfPages(); pageIndex++) {
1:e710339:       stats = new TablePageStatistics(getEncodedColumnPages(encodedBlocklet, true, pageIndex),
1:e710339:           getEncodedColumnPages(encodedBlocklet, false, pageIndex));
1:bc3e684:       byte[][] columnMaxData = stats.getDimensionMaxValue();
1:bc3e684:       byte[][] columnMinData = stats.getDimensionMinValue();
1:6b1ab2a:       for (int i = 0; i < maxCol.length; i++) {
1:6b1ab2a:         if (ByteUtil.UnsafeComparer.INSTANCE.compareTo(columnMaxData[i], maxCol[i]) > 0) {
1:6b1ab2a:           maxCol[i] = columnMaxData[i];
1:6b1ab2a:         }
1:6b1ab2a:         if (ByteUtil.UnsafeComparer.INSTANCE.compareTo(columnMinData[i], minCol[i]) < 0) {
1:6b1ab2a:           minCol[i] = columnMinData[i];
1:6b1ab2a:         }
1:6b1ab2a:       }
1:6b1ab2a:     }
1:6b1ab2a:     // Writing min/max to thrift file
1:6b1ab2a:     for (byte[] max : maxCol) {
1:cd6a4ff:       blockletMinMaxIndex.addToMax_values(ByteBuffer.wrap(max));
1:cd6a4ff:     }
1:6b1ab2a:     for (byte[] min : minCol) {
1:cd6a4ff:       blockletMinMaxIndex.addToMin_values(ByteBuffer.wrap(min));
1:cd6a4ff:     }
1:6b1ab2a: 
1:e710339:     stats = new TablePageStatistics(getEncodedColumnPages(encodedBlocklet, true, 0),
1:e710339:         getEncodedColumnPages(encodedBlocklet, false, 0));
1:bc3e684:     byte[][] measureMaxValue = stats.getMeasureMaxValue().clone();
1:bc3e684:     byte[][] measureMinValue = stats.getMeasureMinValue().clone();
1:2cf1104:     byte[] minVal = null;
1:2cf1104:     byte[] maxVal = null;
1:e710339:     for (int i = 1; i < encodedBlocklet.getNumberOfPages(); i++) {
1:2cf1104:       for (int j = 0; j < measureMinValue.length; j++) {
1:e710339:         stats = new TablePageStatistics(getEncodedColumnPages(encodedBlocklet, true, i),
1:e710339:             getEncodedColumnPages(encodedBlocklet, false, i));
1:bc3e684:         minVal = stats.getMeasureMinValue()[j];
1:bc3e684:         maxVal = stats.getMeasureMaxValue()[j];
1:2cf1104:         if (compareMeasureData(measureMaxValue[j], maxVal, carbonMeasureList.get(j).getDataType())
1:2cf1104:             < 0) {
1:2cf1104:           measureMaxValue[j] = maxVal.clone();
1:cd6a4ff:         }
1:2cf1104:         if (compareMeasureData(measureMinValue[j], minVal, carbonMeasureList.get(j).getDataType())
1:2cf1104:             > 0) {
1:2cf1104:           measureMinValue[j] = minVal.clone();
1:cd6a4ff:         }
1:cd6a4ff:       }
1:cd6a4ff:     }
1:cd6a4ff: 
1:2cf1104:     for (byte[] max : measureMaxValue) {
2:2cf1104:       blockletMinMaxIndex.addToMax_values(ByteBuffer.wrap(max));
1:cd6a4ff:     }
1:2cf1104:     for (byte[] min : measureMinValue) {
2:2cf1104:       blockletMinMaxIndex.addToMin_values(ByteBuffer.wrap(min));
1:cd6a4ff:     }
1:cd6a4ff:     BlockletBTreeIndex blockletBTreeIndex = new BlockletBTreeIndex();
1:e710339:     byte[] startKey = encodedBlocklet.getPageMetadataList().get(0).serializeStartKey();
1:bc3e684:     blockletBTreeIndex.setStart_key(startKey);
1:e710339:     byte[] endKey =
1:e710339:         encodedBlocklet.getPageMetadataList().get(encodedBlocklet.getPageMetadataList().size() - 1)
1:e710339:             .serializeEndKey();
1:bc3e684:     blockletBTreeIndex.setEnd_key(endKey);
1:cd6a4ff:     BlockletIndex blockletIndex = new BlockletIndex();
1:cd6a4ff:     blockletIndex.setMin_max_index(blockletMinMaxIndex);
1:cd6a4ff:     blockletIndex.setB_tree_index(blockletBTreeIndex);
1:cd6a4ff:     return blockletIndex;
1:cd6a4ff:   }
1:cd6a4ff: 
1:cd6a4ff:   /**
1:8f08c4a:    * set the compressor.
1:8f08c4a:    * before 1.5.0, we set a enum 'compression_codec';
1:8f08c4a:    * after 1.5.0, we use string 'compressor_name' instead
1:cd6a4ff:    */
1:8f08c4a:   public static ChunkCompressionMeta getChunkCompressorMeta(String compressorName) {
1:cd6a4ff:     ChunkCompressionMeta chunkCompressionMeta = new ChunkCompressionMeta();
1:8f08c4a:     // we will not use this field any longer and will use compressor_name instead,
1:8f08c4a:     // but in thrift definition, this field is required so we cannot set it to null, otherwise
1:8f08c4a:     // it will cause deserialization error in runtime (required field cannot be null).
1:8f08c4a:     chunkCompressionMeta.setCompression_codec(CompressionCodec.DEPRECATED);
1:8f08c4a:     chunkCompressionMeta.setCompressor_name(compressorName);
1:cd6a4ff:     chunkCompressionMeta.setTotal_compressed_size(0);
1:cd6a4ff:     chunkCompressionMeta.setTotal_uncompressed_size(0);
1:cd6a4ff:     return chunkCompressionMeta;
1:cd6a4ff:   }
1:cd6a4ff: 
1:cd6a4ff:   /**
1:8f08c4a:    * get the compressor name from chunk meta
1:8f08c4a:    * before 1.5.0, we only support snappy and do not have compressor_name field;
1:8f08c4a:    * after 1.5.0, we directly get the compressor from the compressor_name field
1:8f08c4a:    */
1:8f08c4a:   public static String getCompressorNameFromChunkMeta(ChunkCompressionMeta chunkCompressionMeta) {
1:8f08c4a:     if (chunkCompressionMeta.isSetCompressor_name()) {
1:8f08c4a:       return chunkCompressionMeta.getCompressor_name();
1:8f08c4a:     } else {
1:8f08c4a:       // this is for legacy store before 1.5.0
1:8f08c4a:       return CompressorFactory.SupportedCompressor.SNAPPY.getName();
1:8f08c4a:     }
1:8f08c4a:   }
1:8f08c4a:   /**
1:cd6a4ff:    * Below method will be used to get the index header
1:cd6a4ff:    *
1:cd6a4ff:    * @param columnCardinality cardinality of each column
1:cd6a4ff:    * @param columnSchemaList  list of column present in the table
1:7ed144c:    * @param bucketNumber
1:7ed144c:    * @param schemaTimeStamp current timestamp of schema
1:cd6a4ff:    * @return Index header object
1:cd6a4ff:    */
1:cd6a4ff:   public static IndexHeader getIndexHeader(int[] columnCardinality,
1:7ed144c:       List<ColumnSchema> columnSchemaList, int bucketNumber, long schemaTimeStamp) {
1:cd6a4ff:     // create segment info object
1:cd6a4ff:     SegmentInfo segmentInfo = new SegmentInfo();
1:cd6a4ff:     // set the number of columns
2:cd6a4ff:     segmentInfo.setNum_cols(columnSchemaList.size());
1:cd6a4ff:     // setting the column cardinality
1:cd6a4ff:     segmentInfo.setColumn_cardinalities(CarbonUtil.convertToIntegerList(columnCardinality));
1:cd6a4ff:     // create index header object
1:cd6a4ff:     IndexHeader indexHeader = new IndexHeader();
1:0ef3fb8:     ColumnarFormatVersion version = CarbonProperties.getInstance().getFormatVersion();
1:0ef3fb8:     indexHeader.setVersion(version.number());
1:cd6a4ff:     // set the segment info
1:cd6a4ff:     indexHeader.setSegment_info(segmentInfo);
1:cd6a4ff:     // set the column names
1:cd6a4ff:     indexHeader.setTable_columns(columnSchemaList);
1:cbf8797:     // set the bucket number
1:cbf8797:     indexHeader.setBucket_id(bucketNumber);
1:7ed144c:     // set the current schema time stamp which will used for deciding the restructured block
1:7ed144c:     indexHeader.setSchema_time_stamp(schemaTimeStamp);
1:cd6a4ff:     return indexHeader;
1:cd6a4ff:   }
1:cd6a4ff: 
1:cd6a4ff:   /**
1:2cf1104:    * Below method will be used to get the block index info thrift object for
1:2cf1104:    * each block present in the segment
1:cd6a4ff:    *
1:cd6a4ff:    * @param blockIndexInfoList block index info list
1:cd6a4ff:    * @return list of block index
1:cd6a4ff:    */
1:cd6a4ff:   public static List<BlockIndex> getBlockIndexInfo(List<BlockIndexInfo> blockIndexInfoList) {
1:cd6a4ff:     List<BlockIndex> thriftBlockIndexList = new ArrayList<BlockIndex>();
1:cd6a4ff:     BlockIndex blockIndex = null;
1:cd6a4ff:     // below code to create block index info object for each block
1:cd6a4ff:     for (BlockIndexInfo blockIndexInfo : blockIndexInfoList) {
1:cd6a4ff:       blockIndex = new BlockIndex();
1:cd6a4ff:       blockIndex.setNum_rows(blockIndexInfo.getNumberOfRows());
1:d54dc64:       blockIndex.setOffset(blockIndexInfo.getOffset());
1:cd6a4ff:       blockIndex.setFile_name(blockIndexInfo.getFileName());
1:cd6a4ff:       blockIndex.setBlock_index(getBlockletIndex(blockIndexInfo.getBlockletIndex()));
1:1e21cd1:       if (blockIndexInfo.getBlockletInfo() != null) {
1:1e21cd1:         blockIndex.setBlocklet_info(getBlocletInfo3(blockIndexInfo.getBlockletInfo()));
1:cd6a4ff:       }
1:cd6a4ff:       thriftBlockIndexList.add(blockIndex);
1:cd6a4ff:     }
1:cd6a4ff:     return thriftBlockIndexList;
1:cd6a4ff:   }
1:cd6a4ff: 
1:e820006:   public static BlockletInfo3 getBlocletInfo3(
1:1e21cd1:       org.apache.carbondata.core.metadata.blocklet.BlockletInfo blockletInfo) {
1:1e21cd1:     List<Long> dimensionChunkOffsets = blockletInfo.getDimensionChunkOffsets();
1:1e21cd1:     dimensionChunkOffsets.addAll(blockletInfo.getMeasureChunkOffsets());
1:1e21cd1:     List<Integer> dimensionChunksLength = blockletInfo.getDimensionChunksLength();
1:1e21cd1:     dimensionChunksLength.addAll(blockletInfo.getMeasureChunksLength());
1:1e21cd1:     return new BlockletInfo3(blockletInfo.getNumberOfRows(), dimensionChunkOffsets,
1:1e21cd1:         dimensionChunksLength, blockletInfo.getDimensionOffset(), blockletInfo.getMeasureOffsets(),
1:1e21cd1:         blockletInfo.getNumberOfPages());
1:cd6a4ff:   }
1:cd6a4ff: 
1:cd6a4ff:   /**
1:bc3e684:    * return DataChunk3 that contains the input DataChunk2 list
1:cd6a4ff:    */
1:e710339:   public static DataChunk3 getDataChunk3(List<DataChunk2> dataChunksList,
1:e710339:       LocalDictionaryChunk encodedDictionary) {
1:2cf1104:     int offset = 0;
1:2cf1104:     DataChunk3 dataChunk = new DataChunk3();
1:2cf1104:     List<Integer> pageOffsets = new ArrayList<>();
1:2cf1104:     List<Integer> pageLengths = new ArrayList<>();
1:2cf1104:     int length = 0;
1:bc3e684:     for (DataChunk2 dataChunk2 : dataChunksList) {
1:2cf1104:       pageOffsets.add(offset);
1:bc3e684:       length = dataChunk2.getData_page_length() + dataChunk2.getRle_page_length() +
1:bc3e684:           dataChunk2.getRowid_page_length();
1:2cf1104:       pageLengths.add(length);
1:2cf1104:       offset += length;
1:cd6a4ff:     }
1:e710339:     dataChunk.setLocal_dictionary(encodedDictionary);
1:2cf1104:     dataChunk.setData_chunk_list(dataChunksList);
1:2cf1104:     dataChunk.setPage_length(pageLengths);
1:2cf1104:     dataChunk.setPage_offset(pageOffsets);
1:2cf1104:     return dataChunk;
1:cd6a4ff:   }
1:cd6a4ff: 
1:cd6a4ff:   /**
1:bc3e684:    * return DataChunk3 for the dimension column (specifed by `columnIndex`)
1:bc3e684:    * in `encodedTablePageList`
1:cd6a4ff:    */
1:e710339:   public static DataChunk3 getDimensionDataChunk3(EncodedBlocklet encodedBlocklet,
1:e710339:       int columnIndex) {
1:e710339:     List<DataChunk2> dataChunksList = new ArrayList<>();
1:e710339:     BlockletEncodedColumnPage blockletEncodedColumnPage =
1:e710339:         encodedBlocklet.getEncodedDimensionColumnPages().get(columnIndex);
1:e710339:     for (EncodedColumnPage encodedColumnPage : blockletEncodedColumnPage
1:e710339:         .getEncodedColumnPageList()) {
1:e710339:       dataChunksList.add(encodedColumnPage.getPageMetadata());
1:cd6a4ff:     }
1:e710339:     return CarbonMetadataUtil
1:e710339:         .getDataChunk3(dataChunksList, blockletEncodedColumnPage.getEncodedDictionary());
1:cd6a4ff:   }
1:cd6a4ff: 
1:bc3e684:   /**
1:bc3e684:    * return DataChunk3 for the measure column (specifed by `columnIndex`)
1:bc3e684:    * in `encodedTablePageList`
1:bc3e684:    */
1:e710339:   public static DataChunk3 getMeasureDataChunk3(EncodedBlocklet encodedBlocklet, int columnIndex) {
1:e710339:     List<DataChunk2> dataChunksList = new ArrayList<>();
1:e710339:     BlockletEncodedColumnPage blockletEncodedColumnPage =
1:e710339:         encodedBlocklet.getEncodedMeasureColumnPages().get(columnIndex);
1:e710339:     for (EncodedColumnPage encodedColumnPage : blockletEncodedColumnPage
1:e710339:         .getEncodedColumnPageList()) {
1:e710339:       dataChunksList.add(encodedColumnPage.getPageMetadata());
1:cd6a4ff:     }
1:e710339:     return CarbonMetadataUtil.getDataChunk3(dataChunksList, null);
1:cd6a4ff:   }
1:cd6a4ff: 
1:956833e:   private static int compareMeasureData(byte[] first, byte[] second, DataType dataType) {
1:2cf1104:     ByteBuffer firstBuffer = null;
1:2cf1104:     ByteBuffer secondBuffer = null;
1:6abdd97:     if (dataType == DataTypes.BOOLEAN) {
1:6abdd97:       return first[0] - second[0];
1:6abdd97:     } else if (dataType == DataTypes.DOUBLE) {
1:956833e:       firstBuffer = ByteBuffer.allocate(8);
1:956833e:       firstBuffer.put(first);
1:956833e:       secondBuffer = ByteBuffer.allocate(8);
1:5172ec4:       secondBuffer.put(second);
1:956833e:       firstBuffer.flip();
1:956833e:       secondBuffer.flip();
1:8affab8:       double compare = firstBuffer.getDouble() - secondBuffer.getDouble();
1:8affab8:       if (compare > 0) {
1:8affab8:         compare = 1;
1:8affab8:       } else if (compare < 0) {
1:8affab8:         compare = -1;
1:8affab8:       }
1:8affab8:       return (int) compare;
1:956833e:     } else if (dataType == DataTypes.LONG || dataType == DataTypes.INT
1:956833e:         || dataType == DataTypes.SHORT) {
1:956833e:       firstBuffer = ByteBuffer.allocate(8);
1:956833e:       firstBuffer.put(first);
1:956833e:       secondBuffer = ByteBuffer.allocate(8);
1:5172ec4:       secondBuffer.put(second);
1:956833e:       firstBuffer.flip();
1:956833e:       secondBuffer.flip();
1:8affab8:       long compare = firstBuffer.getLong() - secondBuffer.getLong();
1:8affab8:       if (compare > 0) {
1:8affab8:         compare = 1;
1:8affab8:       } else if (compare < 0) {
1:8affab8:         compare = -1;
1:8affab8:       }
1:8affab8:       return (int) compare;
1:f209e8e:     } else if (DataTypes.isDecimal(dataType)) {
1:956833e:       return DataTypeUtil.byteToBigDecimal(first).compareTo(DataTypeUtil.byteToBigDecimal(second));
1:e710339:     } else {
1:7ef9164:       throw new IllegalArgumentException("Invalid data type:" + dataType);
1:cd6a4ff:     }
1:cd6a4ff:   }
1:cd6a4ff: 
2:bc3e684:   /**
1:b41e48f:    * Below method will be used to prepare the file header object for carbondata file
1:ebf13dc:    *
1:b41e48f:    * @param isFooterPresent  is footer present in carbon data file
1:b41e48f:    * @param columnSchemaList list of column schema
1:fc1af96:    * @param schemaUpdatedTimeStamp  schema updated time stamp to be used for restructure scenarios
1:b41e48f:    * @return file header thrift object
2:bc3e684:    */
1:b41e48f:   public static FileHeader getFileHeader(boolean isFooterPresent,
1:fc1af96:       List<ColumnSchema> columnSchemaList, long schemaUpdatedTimeStamp) {
1:b41e48f:     FileHeader fileHeader = new FileHeader();
1:0ef3fb8:     ColumnarFormatVersion version = CarbonProperties.getInstance().getFormatVersion();
1:b41e48f:     fileHeader.setIs_footer_present(isFooterPresent);
1:b41e48f:     fileHeader.setColumn_schema(columnSchemaList);
1:b41e48f:     fileHeader.setVersion(version.number());
1:fc1af96:     fileHeader.setTime_stamp(schemaUpdatedTimeStamp);
1:b41e48f:     return fileHeader;
1:cd6a4ff:   }
1:cd6a4ff: 
1:cd6a4ff: }
============================================================================
author:xuchuanyin
-------------------------------------------------------------------------------
commit:8f08c4a
/////////////////////////////////////////////////////////////////////////
1: import org.apache.carbondata.core.datastore.compression.CompressorFactory;
/////////////////////////////////////////////////////////////////////////
1:    * set the compressor.
1:    * before 1.5.0, we set a enum 'compression_codec';
1:    * after 1.5.0, we use string 'compressor_name' instead
1:   public static ChunkCompressionMeta getChunkCompressorMeta(String compressorName) {
1:     // we will not use this field any longer and will use compressor_name instead,
1:     // but in thrift definition, this field is required so we cannot set it to null, otherwise
1:     // it will cause deserialization error in runtime (required field cannot be null).
1:     chunkCompressionMeta.setCompression_codec(CompressionCodec.DEPRECATED);
1:     chunkCompressionMeta.setCompressor_name(compressorName);
1:    * get the compressor name from chunk meta
1:    * before 1.5.0, we only support snappy and do not have compressor_name field;
1:    * after 1.5.0, we directly get the compressor from the compressor_name field
1:    */
1:   public static String getCompressorNameFromChunkMeta(ChunkCompressionMeta chunkCompressionMeta) {
1:     if (chunkCompressionMeta.isSetCompressor_name()) {
1:       return chunkCompressionMeta.getCompressor_name();
1:     } else {
1:       // this is for legacy store before 1.5.0
1:       return CompressorFactory.SupportedCompressor.SNAPPY.getName();
1:     }
1:   }
1:   /**
author:QiangCai
-------------------------------------------------------------------------------
commit:21a72bf
/////////////////////////////////////////////////////////////////////////
1:   /**
1:    * convert external thrift BlockletMinMaxIndex to BlockletMinMaxIndex of carbon metadata
1:    */
1:   public static org.apache.carbondata.core.metadata.blocklet.index.BlockletMinMaxIndex
1:       convertExternalMinMaxIndex(BlockletMinMaxIndex minMaxIndex) {
1:     if (minMaxIndex == null) {
1:       return null;
1:     }
1: 
1:     return new org.apache.carbondata.core.metadata.blocklet.index.BlockletMinMaxIndex(
1:             minMaxIndex.getMin_values(), minMaxIndex.getMax_values());
1:   }
1: 
1:   /**
1:    * convert BlockletMinMaxIndex of carbon metadata to external thrift BlockletMinMaxIndex
1:    */
1:   public static BlockletMinMaxIndex convertMinMaxIndex(
1:       org.apache.carbondata.core.metadata.blocklet.index.BlockletMinMaxIndex minMaxIndex) {
1:     if (minMaxIndex == null) {
1:       return null;
1:     }
1: 
1:     for (int i = 0; i < minMaxIndex.getMaxValues().length; i++) {
1:       blockletMinMaxIndex.addToMax_values(ByteBuffer.wrap(minMaxIndex.getMaxValues()[i]));
1:       blockletMinMaxIndex.addToMin_values(ByteBuffer.wrap(minMaxIndex.getMinValues()[i]));
1: 
1:     return blockletMinMaxIndex;
1:   }
1: 
1:   public static BlockletIndex getBlockletIndex(
1:       org.apache.carbondata.core.metadata.blocklet.index.BlockletIndex info) {
1:     BlockletMinMaxIndex blockletMinMaxIndex = convertMinMaxIndex(info.getMinMaxIndex());
commit:086b06d
/////////////////////////////////////////////////////////////////////////
1:    * @param blockletIndexs
1:    * @param numberOfColumns
0:    * @param segmentProperties
/////////////////////////////////////////////////////////////////////////
1:    * @param blockletIndexs
1:    * @param numberOfColumns
/////////////////////////////////////////////////////////////////////////
0:    * @param nodeHolderList       blocklet info
commit:41347d8
/////////////////////////////////////////////////////////////////////////
1:  * Licensed to the Apache Software Foundation (ASF) under one or more
1:  * contributor license agreements.  See the NOTICE file distributed with
1:  * this work for additional information regarding copyright ownership.
1:  * The ASF licenses this file to You under the Apache License, Version 2.0
1:  * (the "License"); you may not use this file except in compliance with
1:  * the License.  You may obtain a copy of the License at
1:  * Unless required by applicable law or agreed to in writing, software
1:  * distributed under the License is distributed on an "AS IS" BASIS,
1:  * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
1:  * See the License for the specific language governing permissions and
1:  * limitations under the License.
author:ravipesala
-------------------------------------------------------------------------------
commit:8affab8
/////////////////////////////////////////////////////////////////////////
1:       double compare = firstBuffer.getDouble() - secondBuffer.getDouble();
1:       if (compare > 0) {
1:         compare = 1;
1:       } else if (compare < 0) {
1:         compare = -1;
1:       }
1:       return (int) compare;
/////////////////////////////////////////////////////////////////////////
1:       long compare = firstBuffer.getLong() - secondBuffer.getLong();
1:       if (compare > 0) {
1:         compare = 1;
1:       } else if (compare < 0) {
1:         compare = -1;
1:       }
1:       return (int) compare;
commit:9e064ee
/////////////////////////////////////////////////////////////////////////
0: import java.util.Map;
/////////////////////////////////////////////////////////////////////////
0:   private static ByteBuffer writeInfoIfDecimal(int blockIndex,
0:       SegmentProperties segmentProperties) {
0:     Map<Integer, Integer> blockMapping = segmentProperties.getMeasuresOrdinalToBlockMapping();
0:     List<CarbonMeasure> measures = segmentProperties.getMeasures();
0:     CarbonMeasure selectedMeasure = null;
0:     for (CarbonMeasure measure : measures) {
0:       Integer blockId = blockMapping.get(measure.getOrdinal());
0:       selectedMeasure = measure;
0:       if (blockId == blockIndex) {
0:         break;
1:       }
1:     }
0:     assert (selectedMeasure != null);
0:     if (selectedMeasure.getDataType() == DataType.DECIMAL) {
0:       ByteBuffer buffer = ByteBuffer.allocate(8);
0:       buffer.putInt(selectedMeasure.getScale());
0:       buffer.putInt(selectedMeasure.getPrecision());
0:       buffer.flip();
0:       return buffer;
1:     }
0:     return null;
1:   }
1: 
/////////////////////////////////////////////////////////////////////////
0:         ByteBuffer decimalMeta = writeInfoIfDecimal(index, segmentProperties);
0:         if (decimalMeta != null) {
0:           encoderMetaList.add(decimalMeta);
1:         }
commit:6b1ab2a
/////////////////////////////////////////////////////////////////////////
1:     // Calculating min/max for every each column.
0:     byte[][] minCol = nodeHolderList.get(0).getColumnMinData().clone();
0:     byte[][] maxCol = nodeHolderList.get(0).getColumnMaxData().clone();
0:     for (NodeHolder nodeHolder : nodeHolderList) {
0:       byte[][] columnMaxData = nodeHolder.getColumnMaxData();
0:       byte[][] columnMinData = nodeHolder.getColumnMinData();
1:       for (int i = 0; i < maxCol.length; i++) {
1:         if (ByteUtil.UnsafeComparer.INSTANCE.compareTo(columnMaxData[i], maxCol[i]) > 0) {
1:           maxCol[i] = columnMaxData[i];
1:         }
1:         if (ByteUtil.UnsafeComparer.INSTANCE.compareTo(columnMinData[i], minCol[i]) < 0) {
1:           minCol[i] = columnMinData[i];
1:         }
1:       }
1:     }
1:     // Writing min/max to thrift file
1:     for (byte[] max : maxCol) {
1:     for (byte[] min : minCol) {
1: 
commit:5172ec4
/////////////////////////////////////////////////////////////////////////
1:         secondBuffer.put(second);
/////////////////////////////////////////////////////////////////////////
1:         secondBuffer.put(second);
commit:cbf8797
/////////////////////////////////////////////////////////////////////////
0:       List<ColumnSchema> columnSchemaList, int bucketNumber) {
/////////////////////////////////////////////////////////////////////////
1:     // set the bucket number
1:     indexHeader.setBucket_id(bucketNumber);
commit:cd6a4ff
/////////////////////////////////////////////////////////////////////////
1: /*
0:  * Licensed to the Apache Software Foundation (ASF) under one
0:  * or more contributor license agreements.  See the NOTICE file
0:  * distributed with this work for additional information
0:  * regarding copyright ownership.  The ASF licenses this file
0:  * to you under the Apache License, Version 2.0 (the
0:  * "License"); you may not use this file except in compliance
0:  * with the License.  You may obtain a copy of the License at
1:  *
1:  *    http://www.apache.org/licenses/LICENSE-2.0
1:  *
0:  * Unless required by applicable law or agreed to in writing,
0:  * software distributed under the License is distributed on an
0:  * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
0:  * KIND, either express or implied.  See the License for the
0:  * specific language governing permissions and limitations
0:  * under the License.
1:  */
1: package org.apache.carbondata.core.util;
1: 
0: import java.io.ByteArrayInputStream;
0: import java.io.ByteArrayOutputStream;
1: import java.io.IOException;
0: import java.io.ObjectInputStream;
0: import java.io.ObjectOutputStream;
1: import java.nio.ByteBuffer;
1: import java.util.ArrayList;
1: import java.util.List;
0: import java.util.Set;
1: 
0: import org.apache.carbondata.common.logging.LogService;
0: import org.apache.carbondata.common.logging.LogServiceFactory;
0: import org.apache.carbondata.core.carbon.datastore.block.SegmentProperties;
0: import org.apache.carbondata.core.carbon.metadata.index.BlockIndexInfo;
0: import org.apache.carbondata.core.constants.CarbonCommonConstants;
0: import org.apache.carbondata.core.datastorage.store.compression.ValueCompressionModel;
0: import org.apache.carbondata.core.metadata.BlockletInfoColumnar;
0: import org.apache.carbondata.core.metadata.ValueEncoderMeta;
1: import org.apache.carbondata.format.BlockIndex;
1: import org.apache.carbondata.format.BlockletBTreeIndex;
1: import org.apache.carbondata.format.BlockletIndex;
0: import org.apache.carbondata.format.BlockletInfo;
1: import org.apache.carbondata.format.BlockletMinMaxIndex;
1: import org.apache.carbondata.format.ChunkCompressionMeta;
1: import org.apache.carbondata.format.ColumnSchema;
1: import org.apache.carbondata.format.CompressionCodec;
0: import org.apache.carbondata.format.DataChunk;
0: import org.apache.carbondata.format.Encoding;
0: import org.apache.carbondata.format.FileFooter;
1: import org.apache.carbondata.format.IndexHeader;
0: import org.apache.carbondata.format.PresenceMeta;
1: import org.apache.carbondata.format.SegmentInfo;
0: import org.apache.carbondata.format.SortState;
1: 
1: /**
1:  * Util class to convert to thrift metdata classes
1:  */
1: public class CarbonMetadataUtil {
1: 
1:   /**
0:    * Attribute for Carbon LOGGER
1:    */
0:   private static final LogService LOGGER =
0:       LogServiceFactory.getLogService(CarbonMetadataUtil.class.getName());
1: 
1:   /**
0:    * It converts list of BlockletInfoColumnar to FileFooter thrift objects
1:    *
1:    * @param infoList
0:    * @param numCols
1:    * @param cardinalities
1:    * @return FileFooter
1:    */
0:   public static FileFooter convertFileFooter(List<BlockletInfoColumnar> infoList, int numCols,
0:       int[] cardinalities, List<ColumnSchema> columnSchemaList,
0:       SegmentProperties segmentProperties) throws IOException {
1: 
1:     SegmentInfo segmentInfo = new SegmentInfo();
1:     segmentInfo.setNum_cols(columnSchemaList.size());
1:     segmentInfo.setColumn_cardinalities(CarbonUtil.convertToIntegerList(cardinalities));
1: 
0:     FileFooter footer = new FileFooter();
0:     footer.setNum_rows(getTotalNumberOfRows(infoList));
1:     footer.setSegment_info(segmentInfo);
0:     for (BlockletInfoColumnar info : infoList) {
0:       footer.addToBlocklet_index_list(getBlockletIndex(info));
1:     }
0:     footer.setTable_columns(columnSchemaList);
0:     for (BlockletInfoColumnar info : infoList) {
0:       footer.addToBlocklet_info_list(getBlockletInfo(info, columnSchemaList, segmentProperties));
1:     }
1:     return footer;
1:   }
1: 
0:   private static BlockletIndex getBlockletIndex(
0:       org.apache.carbondata.core.carbon.metadata.blocklet.index.BlockletIndex info) {
1:     BlockletMinMaxIndex blockletMinMaxIndex = new BlockletMinMaxIndex();
1: 
0:     for (int i = 0; i < info.getMinMaxIndex().getMaxValues().length; i++) {
0:       blockletMinMaxIndex.addToMax_values(ByteBuffer.wrap(info.getMinMaxIndex().getMaxValues()[i]));
0:       blockletMinMaxIndex.addToMin_values(ByteBuffer.wrap(info.getMinMaxIndex().getMinValues()[i]));
1:     }
1:     BlockletBTreeIndex blockletBTreeIndex = new BlockletBTreeIndex();
1:     blockletBTreeIndex.setStart_key(info.getBtreeIndex().getStartKey());
1:     blockletBTreeIndex.setEnd_key(info.getBtreeIndex().getEndKey());
1:     BlockletIndex blockletIndex = new BlockletIndex();
1:     blockletIndex.setMin_max_index(blockletMinMaxIndex);
1:     blockletIndex.setB_tree_index(blockletBTreeIndex);
1:     return blockletIndex;
1:   }
1: 
1:   /**
1:    * Get total number of rows for the file.
1:    *
1:    * @param infoList
1:    * @return
1:    */
0:   private static long getTotalNumberOfRows(List<BlockletInfoColumnar> infoList) {
1:     long numberOfRows = 0;
0:     for (BlockletInfoColumnar info : infoList) {
0:       numberOfRows += info.getNumberOfKeys();
1:     }
1:     return numberOfRows;
1:   }
1: 
0:   private static BlockletIndex getBlockletIndex(BlockletInfoColumnar info) {
1: 
1:     BlockletMinMaxIndex blockletMinMaxIndex = new BlockletMinMaxIndex();
0:     for (byte[] max : info.getColumnMaxData()) {
1:       blockletMinMaxIndex.addToMax_values(ByteBuffer.wrap(max));
1:     }
0:     for (byte[] min : info.getColumnMinData()) {
1:       blockletMinMaxIndex.addToMin_values(ByteBuffer.wrap(min));
1:     }
1:     BlockletBTreeIndex blockletBTreeIndex = new BlockletBTreeIndex();
0:     blockletBTreeIndex.setStart_key(info.getStartKey());
0:     blockletBTreeIndex.setEnd_key(info.getEndKey());
1: 
1:     BlockletIndex blockletIndex = new BlockletIndex();
1:     blockletIndex.setMin_max_index(blockletMinMaxIndex);
1:     blockletIndex.setB_tree_index(blockletBTreeIndex);
1:     return blockletIndex;
1:   }
1: 
0:   private static BlockletInfo getBlockletInfo(BlockletInfoColumnar blockletInfoColumnar,
0:       List<ColumnSchema> columnSchenma,
0:       SegmentProperties segmentProperties) throws IOException {
1: 
0:     BlockletInfo blockletInfo = new BlockletInfo();
0:     blockletInfo.setNum_rows(blockletInfoColumnar.getNumberOfKeys());
1: 
0:     List<DataChunk> colDataChunks = new ArrayList<DataChunk>();
0:     blockletInfoColumnar.getKeyLengths();
0:     int j = 0;
0:     int aggregateIndex = 0;
0:     boolean[] isSortedKeyColumn = blockletInfoColumnar.getIsSortedKeyColumn();
0:     boolean[] aggKeyBlock = blockletInfoColumnar.getAggKeyBlock();
0:     boolean[] colGrpblock = blockletInfoColumnar.getColGrpBlocks();
0:     for (int i = 0; i < blockletInfoColumnar.getKeyLengths().length; i++) {
0:       DataChunk dataChunk = new DataChunk();
0:       dataChunk.setChunk_meta(getChunkCompressionMeta());
0:       List<Encoding> encodings = new ArrayList<Encoding>();
0:       if (containsEncoding(i, Encoding.DICTIONARY, columnSchenma, segmentProperties)) {
0:         encodings.add(Encoding.DICTIONARY);
1:       }
0:       if (containsEncoding(i, Encoding.DIRECT_DICTIONARY, columnSchenma, segmentProperties)) {
0:         encodings.add(Encoding.DIRECT_DICTIONARY);
1:       }
0:       dataChunk.setRowMajor(colGrpblock[i]);
0:       //TODO : Once schema PR is merged and information needs to be passed here.
0:       dataChunk.setColumn_ids(new ArrayList<Integer>());
0:       dataChunk.setData_page_length(blockletInfoColumnar.getKeyLengths()[i]);
0:       dataChunk.setData_page_offset(blockletInfoColumnar.getKeyOffSets()[i]);
0:       if (aggKeyBlock[i]) {
0:         dataChunk.setRle_page_offset(blockletInfoColumnar.getDataIndexMapOffsets()[aggregateIndex]);
0:         dataChunk.setRle_page_length(blockletInfoColumnar.getDataIndexMapLength()[aggregateIndex]);
0:         encodings.add(Encoding.RLE);
0:         aggregateIndex++;
1:       }
0:       dataChunk
0:           .setSort_state(isSortedKeyColumn[i] ? SortState.SORT_EXPLICIT : SortState.SORT_NATIVE);
1: 
0:       if (!isSortedKeyColumn[i]) {
0:         dataChunk.setRowid_page_offset(blockletInfoColumnar.getKeyBlockIndexOffSets()[j]);
0:         dataChunk.setRowid_page_length(blockletInfoColumnar.getKeyBlockIndexLength()[j]);
0:         encodings.add(Encoding.INVERTED_INDEX);
0:         j++;
1:       }
1: 
0:       //TODO : Right now the encodings are happening at runtime. change as per this encoders.
0:       dataChunk.setEncoders(encodings);
1: 
0:       colDataChunks.add(dataChunk);
1:     }
1: 
0:     for (int i = 0; i < blockletInfoColumnar.getMeasureLength().length; i++) {
0:       DataChunk dataChunk = new DataChunk();
0:       dataChunk.setChunk_meta(getChunkCompressionMeta());
0:       dataChunk.setRowMajor(false);
0:       //TODO : Once schema PR is merged and information needs to be passed here.
0:       dataChunk.setColumn_ids(new ArrayList<Integer>());
0:       dataChunk.setData_page_length(blockletInfoColumnar.getMeasureLength()[i]);
0:       dataChunk.setData_page_offset(blockletInfoColumnar.getMeasureOffset()[i]);
0:       //TODO : Right now the encodings are happening at runtime. change as per this encoders.
0:       List<Encoding> encodings = new ArrayList<Encoding>();
0:       encodings.add(Encoding.DELTA);
0:       dataChunk.setEncoders(encodings);
0:       //TODO writing dummy presence meta need to set actual presence
0:       //meta
0:       PresenceMeta presenceMeta = new PresenceMeta();
0:       presenceMeta.setPresent_bit_streamIsSet(true);
0:       presenceMeta
0:           .setPresent_bit_stream(blockletInfoColumnar.getMeasureNullValueIndex()[i].toByteArray());
0:       dataChunk.setPresence(presenceMeta);
0:       //TODO : PresenceMeta needs to be implemented and set here
0:       // dataChunk.setPresence(new PresenceMeta());
0:       //TODO : Need to write ValueCompression meta here.
0:       List<ByteBuffer> encoderMetaList = new ArrayList<ByteBuffer>();
0:       encoderMetaList.add(ByteBuffer.wrap(serializeEncoderMeta(
0:           createValueEncoderMeta(blockletInfoColumnar.getCompressionModel(), i))));
0:       dataChunk.setEncoder_meta(encoderMetaList);
0:       colDataChunks.add(dataChunk);
1:     }
0:     blockletInfo.setColumn_data_chunks(colDataChunks);
1: 
0:     return blockletInfo;
1:   }
1: 
1:   /**
0:    * @param blockIndex
0:    * @param encoding
0:    * @param columnSchemas
0:    * @param segmentProperties
0:    * @return return true if given encoding is present in column
1:    */
0:   private static boolean containsEncoding(int blockIndex, Encoding encoding,
0:       List<ColumnSchema> columnSchemas, SegmentProperties segmentProperties) {
0:     Set<Integer> dimOrdinals = segmentProperties.getDimensionOrdinalForBlock(blockIndex);
0:     //column groups will always have dictionary encoding
0:     if (dimOrdinals.size() > 1 && Encoding.DICTIONARY == encoding) {
0:       return true;
1:     }
0:     for (Integer dimOrdinal : dimOrdinals) {
0:       if (columnSchemas.get(dimOrdinal).encoders.contains(encoding)) {
0:         return true;
1:       }
1:     }
0:     return false;
1:   }
1: 
0:   private static byte[] serializeEncoderMeta(ValueEncoderMeta encoderMeta) throws IOException {
0:     // TODO : should remove the unnecessary fields.
0:     ByteArrayOutputStream aos = new ByteArrayOutputStream();
0:     ObjectOutputStream objStream = new ObjectOutputStream(aos);
0:     objStream.writeObject(encoderMeta);
0:     objStream.close();
0:     return aos.toByteArray();
1:   }
1: 
0:   private static ValueEncoderMeta createValueEncoderMeta(ValueCompressionModel compressionModel,
0:       int index) {
0:     ValueEncoderMeta encoderMeta = new ValueEncoderMeta();
0:     encoderMeta.setMaxValue(compressionModel.getMaxValue()[index]);
0:     encoderMeta.setMinValue(compressionModel.getMinValue()[index]);
0:     encoderMeta.setDataTypeSelected(compressionModel.getDataTypeSelected()[index]);
0:     encoderMeta.setDecimal(compressionModel.getDecimal()[index]);
0:     encoderMeta.setType(compressionModel.getType()[index]);
0:     encoderMeta.setUniqueValue(compressionModel.getUniqueValue()[index]);
0:     return encoderMeta;
1:   }
1: 
1:   /**
0:    * Right now it is set to default values. We may use this in future
1:    */
0:   private static ChunkCompressionMeta getChunkCompressionMeta() {
1:     ChunkCompressionMeta chunkCompressionMeta = new ChunkCompressionMeta();
0:     chunkCompressionMeta.setCompression_codec(CompressionCodec.SNAPPY);
1:     chunkCompressionMeta.setTotal_compressed_size(0);
1:     chunkCompressionMeta.setTotal_uncompressed_size(0);
1:     return chunkCompressionMeta;
1:   }
1: 
1:   /**
0:    * It converts FileFooter thrift object to list of BlockletInfoColumnar objects
1:    *
0:    * @param footer
1:    * @return
1:    */
0:   public static List<BlockletInfoColumnar> convertBlockletInfo(FileFooter footer)
1:       throws IOException {
0:     List<BlockletInfoColumnar> listOfNodeInfo =
0:         new ArrayList<BlockletInfoColumnar>(CarbonCommonConstants.CONSTANT_SIZE_TEN);
0:     for (BlockletInfo blockletInfo : footer.getBlocklet_info_list()) {
0:       BlockletInfoColumnar blockletInfoColumnar = new BlockletInfoColumnar();
0:       blockletInfoColumnar.setNumberOfKeys(blockletInfo.getNum_rows());
0:       List<DataChunk> columnChunks = blockletInfo.getColumn_data_chunks();
0:       List<DataChunk> dictChunks = new ArrayList<DataChunk>();
0:       List<DataChunk> nonDictColChunks = new ArrayList<DataChunk>();
0:       for (DataChunk dataChunk : columnChunks) {
0:         if (dataChunk.getEncoders().get(0).equals(Encoding.DICTIONARY)) {
0:           dictChunks.add(dataChunk);
1:         } else {
0:           nonDictColChunks.add(dataChunk);
1:         }
1:       }
0:       int[] keyLengths = new int[dictChunks.size()];
0:       long[] keyOffSets = new long[dictChunks.size()];
0:       long[] keyBlockIndexOffsets = new long[dictChunks.size()];
0:       int[] keyBlockIndexLens = new int[dictChunks.size()];
0:       long[] indexMapOffsets = new long[dictChunks.size()];
0:       int[] indexMapLens = new int[dictChunks.size()];
0:       boolean[] sortState = new boolean[dictChunks.size()];
0:       int i = 0;
0:       for (DataChunk dataChunk : dictChunks) {
0:         keyLengths[i] = dataChunk.getData_page_length();
0:         keyOffSets[i] = dataChunk.getData_page_offset();
0:         keyBlockIndexOffsets[i] = dataChunk.getRowid_page_offset();
0:         keyBlockIndexLens[i] = dataChunk.getRowid_page_length();
0:         indexMapOffsets[i] = dataChunk.getRle_page_offset();
0:         indexMapLens[i] = dataChunk.getRle_page_length();
0:         sortState[i] = dataChunk.getSort_state().equals(SortState.SORT_EXPLICIT) ? true : false;
0:         i++;
1:       }
0:       blockletInfoColumnar.setKeyLengths(keyLengths);
0:       blockletInfoColumnar.setKeyOffSets(keyOffSets);
0:       blockletInfoColumnar.setKeyBlockIndexOffSets(keyBlockIndexOffsets);
0:       blockletInfoColumnar.setKeyBlockIndexLength(keyBlockIndexLens);
0:       blockletInfoColumnar.setDataIndexMapOffsets(indexMapOffsets);
0:       blockletInfoColumnar.setDataIndexMapLength(indexMapLens);
0:       blockletInfoColumnar.setIsSortedKeyColumn(sortState);
1: 
0:       int[] msrLens = new int[nonDictColChunks.size()];
0:       long[] msrOffsets = new long[nonDictColChunks.size()];
0:       ValueEncoderMeta[] encoderMetas = new ValueEncoderMeta[nonDictColChunks.size()];
0:       i = 0;
0:       for (DataChunk msrChunk : nonDictColChunks) {
0:         msrLens[i] = msrChunk.getData_page_length();
0:         msrOffsets[i] = msrChunk.getData_page_offset();
0:         encoderMetas[i] = deserializeValueEncoderMeta(msrChunk.getEncoder_meta().get(0));
0:         i++;
1:       }
0:       blockletInfoColumnar.setMeasureLength(msrLens);
0:       blockletInfoColumnar.setMeasureOffset(msrOffsets);
0:       blockletInfoColumnar.setCompressionModel(getValueCompressionModel(encoderMetas));
0:       listOfNodeInfo.add(blockletInfoColumnar);
1:     }
1: 
0:     setBlockletIndex(footer, listOfNodeInfo);
0:     return listOfNodeInfo;
1:   }
1: 
0:   private static ValueEncoderMeta deserializeValueEncoderMeta(ByteBuffer byteBuffer)
1:       throws IOException {
0:     ByteArrayInputStream bis = new ByteArrayInputStream(byteBuffer.array());
0:     ObjectInputStream objStream = new ObjectInputStream(bis);
0:     ValueEncoderMeta encoderMeta = null;
0:     try {
0:       encoderMeta = (ValueEncoderMeta) objStream.readObject();
0:     } catch (ClassNotFoundException e) {
0:       LOGGER.error("Error while reading ValueEncoderMeta");
1:     }
0:     return encoderMeta;
1: 
1:   }
1: 
0:   private static ValueCompressionModel getValueCompressionModel(ValueEncoderMeta[] encoderMetas) {
0:     Object[] maxValue = new Object[encoderMetas.length];
0:     Object[] minValue = new Object[encoderMetas.length];
0:     int[] decimalLength = new int[encoderMetas.length];
0:     Object[] uniqueValue = new Object[encoderMetas.length];
0:     char[] aggType = new char[encoderMetas.length];
0:     byte[] dataTypeSelected = new byte[encoderMetas.length];
0:     for (int i = 0; i < encoderMetas.length; i++) {
0:       maxValue[i] = encoderMetas[i].getMaxValue();
0:       minValue[i] = encoderMetas[i].getMinValue();
0:       decimalLength[i] = encoderMetas[i].getDecimal();
0:       uniqueValue[i] = encoderMetas[i].getUniqueValue();
0:       aggType[i] = encoderMetas[i].getType();
0:       dataTypeSelected[i] = encoderMetas[i].getDataTypeSelected();
1:     }
0:     return ValueCompressionUtil
0:         .getValueCompressionModel(maxValue, minValue, decimalLength, uniqueValue, aggType,
0:             dataTypeSelected);
1:   }
1: 
0:   private static void setBlockletIndex(FileFooter footer,
0:       List<BlockletInfoColumnar> listOfNodeInfo) {
0:     List<BlockletIndex> blockletIndexList = footer.getBlocklet_index_list();
0:     for (int i = 0; i < blockletIndexList.size(); i++) {
0:       BlockletBTreeIndex bTreeIndexList = blockletIndexList.get(i).getB_tree_index();
0:       BlockletMinMaxIndex minMaxIndexList = blockletIndexList.get(i).getMin_max_index();
1: 
0:       listOfNodeInfo.get(i).setStartKey(bTreeIndexList.getStart_key());
0:       listOfNodeInfo.get(i).setEndKey(bTreeIndexList.getEnd_key());
0:       byte[][] min = new byte[minMaxIndexList.getMin_values().size()][];
0:       byte[][] max = new byte[minMaxIndexList.getMax_values().size()][];
0:       for (int j = 0; j < minMaxIndexList.getMax_valuesSize(); j++) {
0:         min[j] = minMaxIndexList.getMin_values().get(j).array();
0:         max[j] = minMaxIndexList.getMax_values().get(j).array();
1:       }
0:       listOfNodeInfo.get(i).setColumnMaxData(max);
1:     }
1:   }
1: 
1:   /**
1:    * Below method will be used to get the index header
1:    *
1:    * @param columnCardinality cardinality of each column
1:    * @param columnSchemaList  list of column present in the table
1:    * @return Index header object
1:    */
1:   public static IndexHeader getIndexHeader(int[] columnCardinality,
0:       List<ColumnSchema> columnSchemaList) {
1:     // create segment info object
1:     SegmentInfo segmentInfo = new SegmentInfo();
1:     // set the number of columns
1:     segmentInfo.setNum_cols(columnSchemaList.size());
1:     // setting the column cardinality
1:     segmentInfo.setColumn_cardinalities(CarbonUtil.convertToIntegerList(columnCardinality));
1:     // create index header object
1:     IndexHeader indexHeader = new IndexHeader();
1:     // set the segment info
1:     indexHeader.setSegment_info(segmentInfo);
1:     // set the column names
1:     indexHeader.setTable_columns(columnSchemaList);
1:     return indexHeader;
1:   }
1: 
1:   /**
0:    * Below method will be used to get the block index info thrift object for each block
0:    * present in the segment
1:    *
1:    * @param blockIndexInfoList block index info list
1:    * @return list of block index
1:    */
1:   public static List<BlockIndex> getBlockIndexInfo(List<BlockIndexInfo> blockIndexInfoList) {
1:     List<BlockIndex> thriftBlockIndexList = new ArrayList<BlockIndex>();
1:     BlockIndex blockIndex = null;
1:     // below code to create block index info object for each block
1:     for (BlockIndexInfo blockIndexInfo : blockIndexInfoList) {
1:       blockIndex = new BlockIndex();
1:       blockIndex.setNum_rows(blockIndexInfo.getNumberOfRows());
0:       blockIndex.setOffset(blockIndexInfo.getNumberOfRows());
1:       blockIndex.setFile_name(blockIndexInfo.getFileName());
1:       blockIndex.setBlock_index(getBlockletIndex(blockIndexInfo.getBlockletIndex()));
1:       thriftBlockIndexList.add(blockIndex);
1:     }
1:     return thriftBlockIndexList;
1:   }
1: }
author:sraghunandan
-------------------------------------------------------------------------------
commit:f911403
/////////////////////////////////////////////////////////////////////////
/////////////////////////////////////////////////////////////////////////
/////////////////////////////////////////////////////////////////////////
1:   private CarbonMetadataUtil() {
0:   }
0: 
/////////////////////////////////////////////////////////////////////////
author:kumarvishal09
-------------------------------------------------------------------------------
commit:e710339
/////////////////////////////////////////////////////////////////////////
1: import org.apache.carbondata.core.datastore.blocklet.BlockletEncodedColumnPage;
1: import org.apache.carbondata.core.datastore.blocklet.EncodedBlocklet;
1: import org.apache.carbondata.core.datastore.page.encoding.EncodedColumnPage;
/////////////////////////////////////////////////////////////////////////
1: import org.apache.carbondata.format.LocalDictionaryChunk;
/////////////////////////////////////////////////////////////////////////
1:   private static EncodedColumnPage[] getEncodedColumnPages(EncodedBlocklet encodedBlocklet,
1:       boolean isDimension, int pageIndex) {
1:     int size =
1:         isDimension ? encodedBlocklet.getNumberOfDimension() : encodedBlocklet.getNumberOfMeasure();
1:     EncodedColumnPage [] encodedPages = new EncodedColumnPage[size];
0: 
1:     for (int i = 0; i < size; i++) {
1:       if (isDimension) {
1:         encodedPages[i] =
1:             encodedBlocklet.getEncodedDimensionColumnPages().get(i).getEncodedColumnPageList()
1:                 .get(pageIndex);
1:       } else {
1:         encodedPages[i] =
1:             encodedBlocklet.getEncodedMeasureColumnPages().get(i).getEncodedColumnPageList()
1:                 .get(pageIndex);
0:       }
0:     }
1:     return encodedPages;
0:   }
1:   public static BlockletIndex getBlockletIndex(EncodedBlocklet encodedBlocklet,
0: 
1:     TablePageStatistics stats =
1:         new TablePageStatistics(getEncodedColumnPages(encodedBlocklet, true, 0),
1:             getEncodedColumnPages(encodedBlocklet, false, 0));
1:     for (int pageIndex = 0; pageIndex < encodedBlocklet.getNumberOfPages(); pageIndex++) {
1:       stats = new TablePageStatistics(getEncodedColumnPages(encodedBlocklet, true, pageIndex),
1:           getEncodedColumnPages(encodedBlocklet, false, pageIndex));
/////////////////////////////////////////////////////////////////////////
1:     stats = new TablePageStatistics(getEncodedColumnPages(encodedBlocklet, true, 0),
1:         getEncodedColumnPages(encodedBlocklet, false, 0));
1:     for (int i = 1; i < encodedBlocklet.getNumberOfPages(); i++) {
1:         stats = new TablePageStatistics(getEncodedColumnPages(encodedBlocklet, true, i),
1:             getEncodedColumnPages(encodedBlocklet, false, i));
/////////////////////////////////////////////////////////////////////////
1:     byte[] startKey = encodedBlocklet.getPageMetadataList().get(0).serializeStartKey();
1:     byte[] endKey =
1:         encodedBlocklet.getPageMetadataList().get(encodedBlocklet.getPageMetadataList().size() - 1)
1:             .serializeEndKey();
/////////////////////////////////////////////////////////////////////////
1:   public static DataChunk3 getDataChunk3(List<DataChunk2> dataChunksList,
1:       LocalDictionaryChunk encodedDictionary) {
/////////////////////////////////////////////////////////////////////////
1:     dataChunk.setLocal_dictionary(encodedDictionary);
/////////////////////////////////////////////////////////////////////////
1:   public static DataChunk3 getDimensionDataChunk3(EncodedBlocklet encodedBlocklet,
1:       int columnIndex) {
1:     List<DataChunk2> dataChunksList = new ArrayList<>();
1:     BlockletEncodedColumnPage blockletEncodedColumnPage =
1:         encodedBlocklet.getEncodedDimensionColumnPages().get(columnIndex);
1:     for (EncodedColumnPage encodedColumnPage : blockletEncodedColumnPage
1:         .getEncodedColumnPageList()) {
1:       dataChunksList.add(encodedColumnPage.getPageMetadata());
1:     return CarbonMetadataUtil
1:         .getDataChunk3(dataChunksList, blockletEncodedColumnPage.getEncodedDictionary());
1:   public static DataChunk3 getMeasureDataChunk3(EncodedBlocklet encodedBlocklet, int columnIndex) {
1:     List<DataChunk2> dataChunksList = new ArrayList<>();
1:     BlockletEncodedColumnPage blockletEncodedColumnPage =
1:         encodedBlocklet.getEncodedMeasureColumnPages().get(columnIndex);
1:     for (EncodedColumnPage encodedColumnPage : blockletEncodedColumnPage
1:         .getEncodedColumnPageList()) {
1:       dataChunksList.add(encodedColumnPage.getPageMetadata());
1:     return CarbonMetadataUtil.getDataChunk3(dataChunksList, null);
author:Raghunandan S
-------------------------------------------------------------------------------
commit:7ef9164
/////////////////////////////////////////////////////////////////////////
1:       throw new IllegalArgumentException("Invalid data type:" + dataType);
author:manishgupta88
-------------------------------------------------------------------------------
commit:7ed144c
/////////////////////////////////////////////////////////////////////////
1:    * @param bucketNumber
1:    * @param schemaTimeStamp current timestamp of schema
1:       List<ColumnSchema> columnSchemaList, int bucketNumber, long schemaTimeStamp) {
/////////////////////////////////////////////////////////////////////////
1:     // set the current schema time stamp which will used for deciding the restructured block
1:     indexHeader.setSchema_time_stamp(schemaTimeStamp);
commit:e820006
/////////////////////////////////////////////////////////////////////////
0:   public static BlockletIndex getBlockletIndex(
/////////////////////////////////////////////////////////////////////////
1:   public static BlockletInfo3 getBlocletInfo3(
commit:fc1af96
/////////////////////////////////////////////////////////////////////////
1:    * @param schemaUpdatedTimeStamp  schema updated time stamp to be used for restructure scenarios
1:       List<ColumnSchema> columnSchemaList, long schemaUpdatedTimeStamp) {
1:     fileHeader.setTime_stamp(schemaUpdatedTimeStamp);
author:Jacky Li
-------------------------------------------------------------------------------
commit:f209e8e
/////////////////////////////////////////////////////////////////////////
1:     } else if (DataTypes.isDecimal(dataType)) {
commit:956833e
/////////////////////////////////////////////////////////////////////////
1: import org.apache.carbondata.core.metadata.datatype.DataTypes;
/////////////////////////////////////////////////////////////////////////
1:   private static int compareMeasureData(byte[] first, byte[] second, DataType dataType) {
0:     if (dataType == DataTypes.DOUBLE) {
1:       firstBuffer = ByteBuffer.allocate(8);
1:       firstBuffer.put(first);
1:       secondBuffer = ByteBuffer.allocate(8);
0:       secondBuffer.put(second);
1:       firstBuffer.flip();
1:       secondBuffer.flip();
0:       return (int) (firstBuffer.getDouble() - secondBuffer.getDouble());
1:     } else if (dataType == DataTypes.LONG || dataType == DataTypes.INT
1:         || dataType == DataTypes.SHORT) {
1:       firstBuffer = ByteBuffer.allocate(8);
1:       firstBuffer.put(first);
1:       secondBuffer = ByteBuffer.allocate(8);
0:       secondBuffer.put(second);
1:       firstBuffer.flip();
1:       secondBuffer.flip();
0:       return (int) (firstBuffer.getLong() - secondBuffer.getLong());
0:     } else if (dataType == DataTypes.DECIMAL) {
1:       return DataTypeUtil.byteToBigDecimal(first).compareTo(DataTypeUtil.byteToBigDecimal(second));
0:     } else {
0:       throw new IllegalArgumentException("Invalid data type");
commit:e6a4f64
/////////////////////////////////////////////////////////////////////////
/////////////////////////////////////////////////////////////////////////
/////////////////////////////////////////////////////////////////////////
/////////////////////////////////////////////////////////////////////////
/////////////////////////////////////////////////////////////////////////
/////////////////////////////////////////////////////////////////////////
0: 
/////////////////////////////////////////////////////////////////////////
/////////////////////////////////////////////////////////////////////////
/////////////////////////////////////////////////////////////////////////
/////////////////////////////////////////////////////////////////////////
/////////////////////////////////////////////////////////////////////////
0:       dataChunksList.add(encodedTablePage.getDimension(columnIndex).getPageMetadata());
/////////////////////////////////////////////////////////////////////////
0:       dataChunksList.add(encodedTablePage.getMeasure(columnIndex).getPageMetadata());
author:xubo245
-------------------------------------------------------------------------------
commit:6abdd97
/////////////////////////////////////////////////////////////////////////
1:     if (dataType == DataTypes.BOOLEAN) {
1:       return first[0] - second[0];
1:     } else if (dataType == DataTypes.DOUBLE) {
author:Ravindra Pesala
-------------------------------------------------------------------------------
commit:1e21cd1
/////////////////////////////////////////////////////////////////////////
1:       if (blockIndexInfo.getBlockletInfo() != null) {
1:         blockIndex.setBlocklet_info(getBlocletInfo3(blockIndexInfo.getBlockletInfo()));
0:       }
0:   private static BlockletInfo3 getBlocletInfo3(
1:       org.apache.carbondata.core.metadata.blocklet.BlockletInfo blockletInfo) {
1:     List<Long> dimensionChunkOffsets = blockletInfo.getDimensionChunkOffsets();
1:     dimensionChunkOffsets.addAll(blockletInfo.getMeasureChunkOffsets());
1:     List<Integer> dimensionChunksLength = blockletInfo.getDimensionChunksLength();
1:     dimensionChunksLength.addAll(blockletInfo.getMeasureChunksLength());
1:     return new BlockletInfo3(blockletInfo.getNumberOfRows(), dimensionChunkOffsets,
1:         dimensionChunksLength, blockletInfo.getDimensionOffset(), blockletInfo.getMeasureOffsets(),
1:         blockletInfo.getNumberOfPages());
0:   }
0: 
commit:4e83509
/////////////////////////////////////////////////////////////////////////
/////////////////////////////////////////////////////////////////////////
author:jackylk
-------------------------------------------------------------------------------
commit:bc3e684
/////////////////////////////////////////////////////////////////////////
/////////////////////////////////////////////////////////////////////////
0: import org.apache.carbondata.core.datastore.page.EncodedTablePage;
1: import org.apache.carbondata.core.datastore.page.statistics.TablePageStatistics;
/////////////////////////////////////////////////////////////////////////
0:   public static FileFooter convertFileFooter(List<BlockletInfoColumnar> infoList,
/////////////////////////////////////////////////////////////////////////
0:   public static BlockletIndex getBlockletIndex(List<EncodedTablePage> encodedTablePageList,
0:     TablePageStatistics stats = new TablePageStatistics(encodedTablePageList.get(0).getDimensions(),
0:         encodedTablePageList.get(0).getMeasures());
1:     byte[][] minCol = stats.getDimensionMinValue().clone();
1:     byte[][] maxCol = stats.getDimensionMaxValue().clone();
0:     for (EncodedTablePage encodedTablePage : encodedTablePageList) {
0:       stats = new TablePageStatistics(encodedTablePage.getDimensions(),
0:           encodedTablePage.getMeasures());
1:       byte[][] columnMaxData = stats.getDimensionMaxValue();
1:       byte[][] columnMinData = stats.getDimensionMinValue();
/////////////////////////////////////////////////////////////////////////
0:     stats = new TablePageStatistics(encodedTablePageList.get(0).getDimensions(),
0:         encodedTablePageList.get(0).getMeasures());
1:     byte[][] measureMaxValue = stats.getMeasureMaxValue().clone();
1:     byte[][] measureMinValue = stats.getMeasureMinValue().clone();
0:     for (int i = 1; i < encodedTablePageList.size(); i++) {
0:         stats = new TablePageStatistics(
0:             encodedTablePageList.get(i).getDimensions(), encodedTablePageList.get(i).getMeasures());
1:         minVal = stats.getMeasureMinValue()[j];
1:         maxVal = stats.getMeasureMaxValue()[j];
/////////////////////////////////////////////////////////////////////////
0:     byte[] startKey = encodedTablePageList.get(0).getPageKey().serializeStartKey();
1:     blockletBTreeIndex.setStart_key(startKey);
0:     byte[] endKey = encodedTablePageList.get(
0:         encodedTablePageList.size() - 1).getPageKey().serializeEndKey();
1:     blockletBTreeIndex.setEnd_key(endKey);
/////////////////////////////////////////////////////////////////////////
0:       dataChunk.setChunk_meta(getSnappyChunkCompressionMeta());
/////////////////////////////////////////////////////////////////////////
/////////////////////////////////////////////////////////////////////////
0:       dataChunk.setChunk_meta(getSnappyChunkCompressionMeta());
/////////////////////////////////////////////////////////////////////////
0:       encoderMetaList.add(
0:           ByteBuffer.wrap(
0:               serializeEncoderMeta(
0:                       blockletInfoColumnar.getEncodedTablePage().getMeasure(i).getMetaData())));
/////////////////////////////////////////////////////////////////////////
1:   /**
0:    * Right now it is set to default values. We may use this in future
1:    */
0:   public static ChunkCompressionMeta getSnappyChunkCompressionMeta() {
0:     ChunkCompressionMeta chunkCompressionMeta = new ChunkCompressionMeta();
0:     chunkCompressionMeta.setCompression_codec(CompressionCodec.SNAPPY);
0:     chunkCompressionMeta.setTotal_compressed_size(0);
0:     chunkCompressionMeta.setTotal_uncompressed_size(0);
0:     return chunkCompressionMeta;
/////////////////////////////////////////////////////////////////////////
/////////////////////////////////////////////////////////////////////////
/////////////////////////////////////////////////////////////////////////
/////////////////////////////////////////////////////////////////////////
0:       encoderMetaList.add(
0:           ByteBuffer.wrap(
0:               serializeEncoderMeta(
0:                       blockletInfoColumnar.getEncodedTablePage().getMeasure(i).getMetaData())));
/////////////////////////////////////////////////////////////////////////
1:    * return DataChunk3 that contains the input DataChunk2 list
0:   public static DataChunk3 getDataChunk3(List<DataChunk2> dataChunksList) {
1:     for (DataChunk2 dataChunk2 : dataChunksList) {
1:       length = dataChunk2.getData_page_length() + dataChunk2.getRle_page_length() +
1:           dataChunk2.getRowid_page_length();
/////////////////////////////////////////////////////////////////////////
1:   /**
1:    * return DataChunk3 for the dimension column (specifed by `columnIndex`)
1:    * in `encodedTablePageList`
1:    */
0:   public static DataChunk3 getDimensionDataChunk3(List<EncodedTablePage> encodedTablePageList,
0:       int columnIndex) throws IOException {
0:     List<DataChunk2> dataChunksList = new ArrayList<>(encodedTablePageList.size());
0:     for (EncodedTablePage encodedTablePage : encodedTablePageList) {
0:       dataChunksList.add(encodedTablePage.getDimension(columnIndex).getDataChunk2());
0:     return CarbonMetadataUtil.getDataChunk3(dataChunksList);
0:   }
0: 
1:   /**
1:    * return DataChunk3 for the measure column (specifed by `columnIndex`)
1:    * in `encodedTablePageList`
1:    */
0:   public static DataChunk3 getMeasureDataChunk3(List<EncodedTablePage> encodedTablePageList,
0:       int columnIndex) throws IOException {
0:     List<DataChunk2> dataChunksList = new ArrayList<>(encodedTablePageList.size());
0:     for (EncodedTablePage encodedTablePage : encodedTablePageList) {
0:       dataChunksList.add(encodedTablePage.getMeasure(columnIndex).getDataChunk2());
0:     }
0:     return CarbonMetadataUtil.getDataChunk3(dataChunksList);
/////////////////////////////////////////////////////////////////////////
commit:2340cc4
/////////////////////////////////////////////////////////////////////////
/////////////////////////////////////////////////////////////////////////
0:         buffer.putLong(0L);  // unique value, not used
/////////////////////////////////////////////////////////////////////////
0:         buffer.putDouble(0d); // unique value, not used
0:     buffer.putInt(0); // decimal point, not used
commit:dc83b2a
/////////////////////////////////////////////////////////////////////////
/////////////////////////////////////////////////////////////////////////
0: import org.apache.carbondata.core.datastore.page.statistics.MeasurePageStatsVO;
/////////////////////////////////////////////////////////////////////////
1:       List<BlockletIndex> blockletIndexs, int[] cardinalities, int numberOfColumns)
0:       throws IOException {
/////////////////////////////////////////////////////////////////////////
0:     byte[][] minCol = nodeHolderList.get(0).getDimensionColumnMinData().clone();
0:     byte[][] maxCol = nodeHolderList.get(0).getDimensionColumnMaxData().clone();
0:       byte[][] columnMaxData = nodeHolder.getDimensionColumnMaxData();
0:       byte[][] columnMinData = nodeHolder.getDimensionColumnMinData();
/////////////////////////////////////////////////////////////////////////
0:           createValueEncoderMeta(blockletInfoColumnar.getStats(), i))));
/////////////////////////////////////////////////////////////////////////
0:   private static ValueEncoderMeta createValueEncoderMeta(MeasurePageStatsVO stats,
0:     encoderMeta.setMaxValue(stats.getMax(index));
0:     encoderMeta.setMinValue(stats.getMin(index));
0:     encoderMeta.setDataTypeSelected(stats.getDataTypeSelected(index));
0:     encoderMeta.setDecimal(stats.getDecimal(index));
0:     encoderMeta.setType(getTypeInChar(stats.getDataType(index)));
0:     encoderMeta.setUniqueValue(stats.getNonExistValue(index));
0:   private static char getTypeInChar(DataType type) {
0:     switch (type) {
0:       case SHORT:
0:       case INT:
0:       case LONG:
0:         return CarbonCommonConstants.BIG_INT_MEASURE;
0:       case DOUBLE:
0:         return CarbonCommonConstants.DOUBLE_MEASURE;
0:       case DECIMAL:
0:         return CarbonCommonConstants.BIG_DECIMAL_MEASURE;
0:       default:
0:         throw new RuntimeException("unsupported type: " + type);
0:     }
0:   }
0: 
/////////////////////////////////////////////////////////////////////////
0:       blockletInfoColumnar.setStats(getMeasurePageStats(encoderMetas));
/////////////////////////////////////////////////////////////////////////
0:   private static MeasurePageStatsVO getMeasurePageStats(ValueEncoderMeta[] encoderMetas) {
0:     return MeasurePageStatsVO.build(encoderMetas);
/////////////////////////////////////////////////////////////////////////
0:           createValueEncoderMeta(blockletInfoColumnar.getStats(), i))));
/////////////////////////////////////////////////////////////////////////
0:         if (nodeHolder.getRleEncodingForDictDim()[index]) {
/////////////////////////////////////////////////////////////////////////
0:         dataChunk.min_max.addToMax_values(
0:             ByteBuffer.wrap(nodeHolder.getDimensionColumnMaxData()[index]));
0:         dataChunk.min_max.addToMin_values(
0:             ByteBuffer.wrap(nodeHolder.getDimensionColumnMinData()[index]));
/////////////////////////////////////////////////////////////////////////
0:             createValueEncoderMeta(nodeHolder.getStats(), index))));
/////////////////////////////////////////////////////////////////////////
/////////////////////////////////////////////////////////////////////////
0:         if (nodeHolder.getRleEncodingForDictDim()[i]) {
/////////////////////////////////////////////////////////////////////////
0:         dataChunk.min_max.addToMax_values(
0:             ByteBuffer.wrap(nodeHolder.getDimensionColumnMaxData()[i]));
0:         dataChunk.min_max.addToMin_values(
0:             ByteBuffer.wrap(nodeHolder.getDimensionColumnMinData()[i]));
/////////////////////////////////////////////////////////////////////////
0:             createValueEncoderMeta(nodeHolder.getStats(), i))));
commit:98df130
/////////////////////////////////////////////////////////////////////////
0:     DataType[] aggType = new DataType[encoderMetas.length];
/////////////////////////////////////////////////////////////////////////
0:     switch (valueEncoderMeta.getType()) {
0:       case LONG:
0:         buffer = ByteBuffer.allocate(
0:             (CarbonCommonConstants.LONG_SIZE_IN_BYTE * 3) + CarbonCommonConstants.INT_SIZE_IN_BYTE
0:                 + 3);
0:         buffer.putChar(valueEncoderMeta.getTypeInChar());
0:         buffer.putLong((Long) valueEncoderMeta.getMaxValue());
0:         buffer.putLong((Long) valueEncoderMeta.getMinValue());
0:         buffer.putLong((Long) valueEncoderMeta.getUniqueValue());
0:         break;
0:       case DOUBLE:
0:         buffer = ByteBuffer.allocate(
0:             (CarbonCommonConstants.DOUBLE_SIZE_IN_BYTE * 3) + CarbonCommonConstants.INT_SIZE_IN_BYTE
0:                 + 3);
0:         buffer.putChar(valueEncoderMeta.getTypeInChar());
0:         buffer.putDouble((Double) valueEncoderMeta.getMaxValue());
0:         buffer.putDouble((Double) valueEncoderMeta.getMinValue());
0:         buffer.putDouble((Double) valueEncoderMeta.getUniqueValue());
0:         break;
0:       case DECIMAL:
0:         buffer = ByteBuffer.allocate(CarbonCommonConstants.INT_SIZE_IN_BYTE + 3);
0:         buffer.putChar(valueEncoderMeta.getTypeInChar());
0:         break;
commit:8cca0af
/////////////////////////////////////////////////////////////////////////
0:     if (valueEncoderMeta.getType() == CarbonCommonConstants.DOUBLE_MEASURE) {
commit:ce09aaa
/////////////////////////////////////////////////////////////////////////
0: import org.apache.carbondata.core.datastore.block.SegmentProperties;
0: import org.apache.carbondata.core.datastore.compression.CompressorFactory;
0: import org.apache.carbondata.core.datastore.compression.WriterCompressModel;
1: import org.apache.carbondata.core.metadata.ColumnarFormatVersion;
1: import org.apache.carbondata.core.metadata.index.BlockIndexInfo;
/////////////////////////////////////////////////////////////////////////
0:       org.apache.carbondata.core.metadata.blocklet.index.BlockletIndex info) {
commit:eaadc88
/////////////////////////////////////////////////////////////////////////
0:         sortState[i] = dataChunk.getSort_state().equals(SortState.SORT_EXPLICIT);
commit:360edc8
/////////////////////////////////////////////////////////////////////////
0: import org.apache.carbondata.core.datastorage.store.compression.WriterCompressModel;
/////////////////////////////////////////////////////////////////////////
0:   private static ValueEncoderMeta createValueEncoderMeta(WriterCompressModel compressionModel,
0:     encoderMeta.setMantissa(compressionModel.getMantissa()[index]);
/////////////////////////////////////////////////////////////////////////
0:   private static WriterCompressModel getValueCompressionModel(ValueEncoderMeta[] encoderMetas) {
/////////////////////////////////////////////////////////////////////////
0:       decimalLength[i] = encoderMetas[i].getMantissa();
0:         .getWriterCompressModel(maxValue, minValue, decimalLength, uniqueValue, aggType,
commit:25b4ba2
/////////////////////////////////////////////////////////////////////////
0: import org.apache.carbondata.core.datastorage.store.compression.CompressorFactory;
/////////////////////////////////////////////////////////////////////////
0:       presenceMeta.setPresent_bit_stream(CompressorFactory.getInstance()
0:           .compressByte(blockletInfoColumnar.getMeasureNullValueIndex()[i].toByteArray()));
commit:0ef3fb8
/////////////////////////////////////////////////////////////////////////
0: import org.apache.carbondata.core.carbon.ColumnarFormatVersion;
/////////////////////////////////////////////////////////////////////////
1:     ColumnarFormatVersion version = CarbonProperties.getInstance().getFormatVersion();
0:     footer.setVersion(version.number());
/////////////////////////////////////////////////////////////////////////
1:     ColumnarFormatVersion version = CarbonProperties.getInstance().getFormatVersion();
1:     indexHeader.setVersion(version.number());
author:mayun
-------------------------------------------------------------------------------
commit:4a11fbd
/////////////////////////////////////////////////////////////////////////
0:       List<ColumnSchema> columnSchema, SegmentProperties segmentProperties) throws IOException {
/////////////////////////////////////////////////////////////////////////
0:       if (containsEncoding(i, Encoding.DICTIONARY, columnSchema, segmentProperties)) {
0:       if (containsEncoding(i, Encoding.DIRECT_DICTIONARY, columnSchema, segmentProperties)) {
/////////////////////////////////////////////////////////////////////////
0:    * @param columnSchema        list of columns
0:       List<ColumnSchema> columnSchema, SegmentProperties segmentProperties) throws IOException {
/////////////////////////////////////////////////////////////////////////
0:       if (containsEncoding(i, Encoding.DICTIONARY, columnSchema, segmentProperties)) {
0:       if (containsEncoding(i, Encoding.DIRECT_DICTIONARY, columnSchema, segmentProperties)) {
/////////////////////////////////////////////////////////////////////////
0:    * @param columnSchema        list of columns
0:       List<ColumnSchema> columnSchema, SegmentProperties segmentProperties, int index,
/////////////////////////////////////////////////////////////////////////
0:         if (containsEncoding(index, Encoding.DICTIONARY, columnSchema, segmentProperties)) {
0:         if (containsEncoding(index, Encoding.DIRECT_DICTIONARY, columnSchema, segmentProperties)) {
/////////////////////////////////////////////////////////////////////////
0:       List<ColumnSchema> columnSchema, SegmentProperties segmentProperties, int index,
0:         getDatachunk2(nodeHolderList, columnSchema, segmentProperties, index, isDimensionColumn);
/////////////////////////////////////////////////////////////////////////
0:    * @param columnSchema     table columns
0:   public static List<byte[]> getDataChunk2(NodeHolder nodeHolder, List<ColumnSchema> columnSchema,
/////////////////////////////////////////////////////////////////////////
0:         if (containsEncoding(i, Encoding.DICTIONARY, columnSchema, segmentProperties)) {
0:         if (containsEncoding(i, Encoding.DIRECT_DICTIONARY, columnSchema, segmentProperties)) {
author:kumarvishal
-------------------------------------------------------------------------------
commit:ebf13dc
/////////////////////////////////////////////////////////////////////////
1:     FileFooter3 footer = getFileFooter3(infoList, blockletIndexs, cardinalities, numberOfColumns);
/////////////////////////////////////////////////////////////////////////
0: 
0:   /**
0:    * Below method will be used to get the data chunk2 serialize object list
1:    *
0:    * @param nodeHolder        node holder
0:    * @param columnSchenma     table columns
0:    * @param segmentProperties segment properties
0:    * @param isDimensionColumn to get the list of dimension column or measure column
0:    * @return list of data chunk2
0:    * @throws IOException
0:    */
0:   public static List<byte[]> getDataChunk2(NodeHolder nodeHolder, List<ColumnSchema> columnSchenma,
0:       SegmentProperties segmentProperties, boolean isDimensionColumn) throws IOException {
0:     List<byte[]> dataChunkBuffer = new ArrayList<>();
0:     if (isDimensionColumn) {
0:       for (int i = 0; i < nodeHolder.getKeyArray().length; i++) {
0:         DataChunk2 dataChunk = new DataChunk2();
0:         dataChunk.min_max = new BlockletMinMaxIndex();
0:         dataChunk.setChunk_meta(getChunkCompressionMeta());
0:         dataChunk.setNumberOfRowsInpage(nodeHolder.getEntryCount());
0:         List<Encoding> encodings = new ArrayList<Encoding>();
0:         dataChunk.setData_page_length(nodeHolder.getKeyLengths()[i]);
0:         if (containsEncoding(i, Encoding.DICTIONARY, columnSchenma, segmentProperties)) {
0:           encodings.add(Encoding.DICTIONARY);
0:         }
0:         if (containsEncoding(i, Encoding.DIRECT_DICTIONARY, columnSchenma, segmentProperties)) {
0:           encodings.add(Encoding.DIRECT_DICTIONARY);
0:         }
0:         dataChunk.setRowMajor(nodeHolder.getColGrpBlocks()[i]);
0:         if (nodeHolder.getAggBlocks()[i]) {
0:           dataChunk.setRle_page_length(nodeHolder.getDataIndexMapLength()[i]);
0:           encodings.add(Encoding.RLE);
0:         }
0:         dataChunk.setSort_state(
0:             nodeHolder.getIsSortedKeyBlock()[i] ? SortState.SORT_EXPLICIT : SortState.SORT_NATIVE);
0:         if (!nodeHolder.getIsSortedKeyBlock()[i]) {
0:           dataChunk.setRowid_page_length(nodeHolder.getKeyBlockIndexLength()[i]);
0:           encodings.add(Encoding.INVERTED_INDEX);
0:         }
0:         dataChunk.min_max.addToMax_values(ByteBuffer.wrap(nodeHolder.getColumnMaxData()[i]));
0:         dataChunk.min_max.addToMin_values(ByteBuffer.wrap(nodeHolder.getColumnMinData()[i]));
0:         dataChunk.setEncoders(encodings);
0:         dataChunkBuffer.add(CarbonUtil.getByteArray(dataChunk));
0:       }
0:     } else {
0:       for (int i = 0; i < nodeHolder.getDataArray().length; i++) {
0:         DataChunk2 dataChunk = new DataChunk2();
0:         dataChunk.min_max = new BlockletMinMaxIndex();
0:         dataChunk.setChunk_meta(getChunkCompressionMeta());
0:         dataChunk.setNumberOfRowsInpage(nodeHolder.getEntryCount());
0:         dataChunk.setData_page_length(nodeHolder.getDataArray()[i].length);
0:         List<Encoding> encodings = new ArrayList<Encoding>();
0:         // TODO : Right now the encodings are happening at runtime. change as
0:         // per this encoders.
0:         dataChunk.setEncoders(encodings);
0:         dataChunk.setRowMajor(false);
0:         // TODO : Right now the encodings are happening at runtime. change as
0:         // per this encoders.
0:         encodings.add(Encoding.DELTA);
0:         dataChunk.setEncoders(encodings);
0:         // TODO writing dummy presence meta need to set actual presence
0:         // meta
0:         PresenceMeta presenceMeta = new PresenceMeta();
0:         presenceMeta.setPresent_bit_streamIsSet(true);
0:         presenceMeta.setPresent_bit_stream(CompressorFactory.getInstance().getCompressor()
0:             .compressByte(nodeHolder.getMeasureNullValueIndex()[i].toByteArray()));
0:         dataChunk.setPresence(presenceMeta);
0:         List<ByteBuffer> encoderMetaList = new ArrayList<ByteBuffer>();
0:         encoderMetaList.add(ByteBuffer.wrap(serializeEncodeMetaUsingByteBuffer(
0:             createValueEncoderMeta(nodeHolder.getCompressionModel(), i))));
0:         dataChunk.setEncoder_meta(encoderMetaList);
0:         dataChunk.min_max.addToMax_values(ByteBuffer.wrap(nodeHolder.getMeasureColumnMaxData()[i]));
0:         dataChunk.min_max.addToMin_values(ByteBuffer.wrap(nodeHolder.getMeasureColumnMinData()[i]));
0:         dataChunkBuffer.add(CarbonUtil.getByteArray(dataChunk));
0:       }
0:     }
0:     return dataChunkBuffer;
0:   }
commit:b41e48f
/////////////////////////////////////////////////////////////////////////
1: import org.apache.carbondata.format.FileFooter3;
1: import org.apache.carbondata.format.FileHeader;
/////////////////////////////////////////////////////////////////////////
1:    * Below method prepares the file footer object for carbon data file version 3
1:   public static FileFooter3 convertFileFooterVersion3(List<BlockletInfo3> infoList,
0:       List<BlockletIndex> blockletIndexs, int[] cardinalities, int numberOfColumns,
0:     FileFooter3 footer =
0:         getFileFooter3(infoList, blockletIndexs, cardinalities, numberOfColumns);
/////////////////////////////////////////////////////////////////////////
1:   private static FileFooter3 getFileFooter3(List<BlockletInfo3> infoList,
1:       List<BlockletIndex> blockletIndexs, int[] cardinalities, int numberOfColumns) {
1:     segmentInfo.setNum_cols(numberOfColumns);
1:     FileFooter3 footer = new FileFooter3();
/////////////////////////////////////////////////////////////////////////
0:         return DataTypeUtil.bigDecimalToByte((BigDecimal) data);
/////////////////////////////////////////////////////////////////////////
0:   /**
1:    * Below method will be used to prepare the file header object for carbondata file
0:    *
1:    * @param isFooterPresent  is footer present in carbon data file
1:    * @param columnSchemaList list of column schema
1:    * @return file header thrift object
0:    */
1:   public static FileHeader getFileHeader(boolean isFooterPresent,
0:       List<ColumnSchema> columnSchemaList) {
1:     FileHeader fileHeader = new FileHeader();
0:     ColumnarFormatVersion version = CarbonProperties.getInstance().getFormatVersion();
1:     fileHeader.setIs_footer_present(isFooterPresent);
1:     fileHeader.setColumn_schema(columnSchemaList);
1:     fileHeader.setVersion(version.number());
1:     return fileHeader;
0:   }
commit:2cf1104
/////////////////////////////////////////////////////////////////////////
0: import java.math.BigDecimal;
/////////////////////////////////////////////////////////////////////////
1: import org.apache.carbondata.core.metadata.datatype.DataType;
1: import org.apache.carbondata.core.metadata.schema.table.column.CarbonMeasure;
1: import org.apache.carbondata.format.BlockletInfo3;
1: import org.apache.carbondata.format.DataChunk3;
/////////////////////////////////////////////////////////////////////////
0:    * It converts list of BlockletInfoColumnar to FileFooter thrift objects
0:    *
0:    * @param infoList
0:    * @param numCols
0:    * @param cardinalities
0:    * @return FileFooter
0:    */
0:   public static FileFooter convertFileFooter3(List<BlockletInfo3> infoList,
0:       List<BlockletIndex> blockletIndexs, int[] cardinalities, List<ColumnSchema> columnSchemaList,
0:       SegmentProperties segmentProperties) throws IOException {
0:     FileFooter footer = getFileFooter3(infoList, blockletIndexs, cardinalities, columnSchemaList);
1:     for (BlockletInfo3 info : infoList) {
1:       footer.addToBlocklet_info_list3(info);
0:     }
1:     return footer;
0:   }
0: 
0:   /**
1:    * Below method will be used to get the file footer object
0:    *
1:    * @param infoList         blocklet info
1:    * @param cardinalities    cardinlaity of dimension columns
0:    * @param columnSchemaList column schema list
1:    * @return file footer
0:    */
0:   private static FileFooter getFileFooter3(List<BlockletInfo3> infoList,
0:       List<BlockletIndex> blockletIndexs, int[] cardinalities,
0:       List<ColumnSchema> columnSchemaList) {
0:     SegmentInfo segmentInfo = new SegmentInfo();
0:     segmentInfo.setNum_cols(columnSchemaList.size());
0:     segmentInfo.setColumn_cardinalities(CarbonUtil.convertToIntegerList(cardinalities));
0:     ColumnarFormatVersion version = CarbonProperties.getInstance().getFormatVersion();
0:     FileFooter footer = new FileFooter();
0:     footer.setVersion(version.number());
1:     footer.setNum_rows(getNumberOfRowForFooter(infoList));
0:     footer.setSegment_info(segmentInfo);
0:     footer.setTable_columns(columnSchemaList);
1:     for (BlockletIndex info : blockletIndexs) {
1:       footer.addToBlocklet_index_list(info);
0:     }
1:     return footer;
0:   }
0: 
0:   /**
/////////////////////////////////////////////////////////////////////////
0:   /**
0:    * Get total number of rows for the file.
0:    *
0:    * @param infoList
0:    * @return
0:    */
1:   private static long getNumberOfRowForFooter(List<BlockletInfo3> infoList) {
0:     long numberOfRows = 0;
1:     for (BlockletInfo3 info : infoList) {
1:       numberOfRows += info.num_rows;
0:     }
0:     return numberOfRows;
0:   }
0: 
/////////////////////////////////////////////////////////////////////////
0:   public static BlockletIndex getBlockletIndex(List<NodeHolder> nodeHolderList,
1:       List<CarbonMeasure> carbonMeasureList) {
0:     BlockletMinMaxIndex blockletMinMaxIndex = new BlockletMinMaxIndex();
0:     for (byte[] max : nodeHolderList.get(nodeHolderList.size() - 1).getColumnMaxData()) {
1:       blockletMinMaxIndex.addToMax_values(ByteBuffer.wrap(max));
0:     }
0:     for (byte[] min : nodeHolderList.get(0).getColumnMinData()) {
1:       blockletMinMaxIndex.addToMin_values(ByteBuffer.wrap(min));
0:     }
0:     byte[][] measureMaxValue = nodeHolderList.get(0).getMeasureColumnMaxData().clone();
0:     byte[][] measureMinValue = nodeHolderList.get(0).getMeasureColumnMinData().clone();
1:     byte[] minVal = null;
1:     byte[] maxVal = null;
0:     for (int i = 1; i < nodeHolderList.size(); i++) {
1:       for (int j = 0; j < measureMinValue.length; j++) {
0:         minVal = nodeHolderList.get(i).getMeasureColumnMinData()[j];
0:         maxVal = nodeHolderList.get(i).getMeasureColumnMaxData()[j];
1:         if (compareMeasureData(measureMaxValue[j], maxVal, carbonMeasureList.get(j).getDataType())
1:             < 0) {
1:           measureMaxValue[j] = maxVal.clone();
0:         }
1:         if (compareMeasureData(measureMinValue[j], minVal, carbonMeasureList.get(j).getDataType())
1:             > 0) {
1:           measureMinValue[j] = minVal.clone();
0:         }
0:       }
0:     }
0: 
1:     for (byte[] max : measureMaxValue) {
1:       blockletMinMaxIndex.addToMax_values(ByteBuffer.wrap(max));
0:     }
1:     for (byte[] min : measureMinValue) {
1:       blockletMinMaxIndex.addToMin_values(ByteBuffer.wrap(min));
0:     }
0:     BlockletBTreeIndex blockletBTreeIndex = new BlockletBTreeIndex();
0:     blockletBTreeIndex.setStart_key(nodeHolderList.get(0).getStartKey());
0:     blockletBTreeIndex.setEnd_key(nodeHolderList.get(nodeHolderList.size() - 1).getEndKey());
0:     BlockletIndex blockletIndex = new BlockletIndex();
0:     blockletIndex.setMin_max_index(blockletMinMaxIndex);
0:     blockletIndex.setB_tree_index(blockletBTreeIndex);
0:     return blockletIndex;
0:   }
0: 
0:    * Below method will be used to get the blocklet info object for data version
0:    * 2 file
/////////////////////////////////////////////////////////////////////////
0:       // TODO : Once schema PR is merged and information needs to be passed
0:       // here.
/////////////////////////////////////////////////////////////////////////
0:       // TODO : Right now the encodings are happening at runtime. change as per
0:       // this encoders.
/////////////////////////////////////////////////////////////////////////
0:       // TODO : Once schema PR is merged and information needs to be passed
0:       // here.
0:       // TODO : Right now the encodings are happening at runtime. change as per
0:       // this encoders.
0:       // TODO writing dummy presence meta need to set actual presence
0:       // meta
0:       // TODO : PresenceMeta needs to be implemented and set here
0:       // TODO : Need to write ValueCompression meta here.
/////////////////////////////////////////////////////////////////////////
0:     // column groups will always have dictionary encoding
/////////////////////////////////////////////////////////////////////////
0:    * It converts FileFooter thrift object to list of BlockletInfoColumnar
0:    * objects
/////////////////////////////////////////////////////////////////////////
1:    * Below method will be used to get the block index info thrift object for
1:    * each block present in the segment
/////////////////////////////////////////////////////////////////////////
0:    * Below method will be used to get the data chunk object for all the columns
/////////////////////////////////////////////////////////////////////////
0:       // TODO : Once schema PR is merged and information needs to be passed
0:       // here.
/////////////////////////////////////////////////////////////////////////
0:       // TODO : Right now the encodings are happening at runtime. change as per
0:       // this encoders.
/////////////////////////////////////////////////////////////////////////
0:       // TODO : Once schema PR is merged and information needs to be passed
0:       // here.
0:       // TODO : Right now the encodings are happening at runtime. change as per
0:       // this encoders.
0:       // TODO writing dummy presence meta need to set actual presence
0:       // meta
0:       // TODO : PresenceMeta needs to be implemented and set here
0:       // TODO : Need to write ValueCompression meta here.
/////////////////////////////////////////////////////////////////////////
0: 
0:   /**
0:    * Below method will be used to get the data chunk object for all the columns
0:    *
0:    * @param blockletInfoColumnar blocklet info
0:    * @param columnSchenma        list of columns
0:    * @param segmentProperties    segment properties
0:    * @return list of data chunks
0:    * @throws IOException
0:    */
0:   private static List<DataChunk2> getDatachunk2(List<NodeHolder> nodeHolderList,
0:       List<ColumnSchema> columnSchenma, SegmentProperties segmentProperties, int index,
0:       boolean isDimensionColumn) throws IOException {
0:     List<DataChunk2> colDataChunks = new ArrayList<DataChunk2>();
0:     DataChunk2 dataChunk = null;
0:     NodeHolder nodeHolder = null;
0:     for (int i = 0; i < nodeHolderList.size(); i++) {
0:       nodeHolder = nodeHolderList.get(i);
0:       dataChunk = new DataChunk2();
0:       dataChunk.min_max = new BlockletMinMaxIndex();
0:       dataChunk.setChunk_meta(getChunkCompressionMeta());
0:       dataChunk.setNumberOfRowsInpage(nodeHolder.getEntryCount());
0:       List<Encoding> encodings = new ArrayList<Encoding>();
0:       if (isDimensionColumn) {
0:         dataChunk.setData_page_length(nodeHolder.getKeyLengths()[index]);
0:         if (containsEncoding(index, Encoding.DICTIONARY, columnSchenma, segmentProperties)) {
0:           encodings.add(Encoding.DICTIONARY);
0:         }
0:         if (containsEncoding(index, Encoding.DIRECT_DICTIONARY, columnSchenma, segmentProperties)) {
0:           encodings.add(Encoding.DIRECT_DICTIONARY);
0:         }
0:         dataChunk.setRowMajor(nodeHolder.getColGrpBlocks()[index]);
0:         // TODO : Once schema PR is merged and information needs to be passed
0:         // here.
0:         if (nodeHolder.getAggBlocks()[index]) {
0:           dataChunk.setRle_page_length(nodeHolder.getDataIndexMapLength()[index]);
0:           encodings.add(Encoding.RLE);
0:         }
0:         dataChunk.setSort_state(nodeHolder.getIsSortedKeyBlock()[index] ?
0:             SortState.SORT_EXPLICIT :
0:             SortState.SORT_NATIVE);
0: 
0:         if (!nodeHolder.getIsSortedKeyBlock()[index]) {
0:           dataChunk.setRowid_page_length(nodeHolder.getKeyBlockIndexLength()[index]);
0:           encodings.add(Encoding.INVERTED_INDEX);
0:         }
0:         dataChunk.min_max.addToMax_values(ByteBuffer.wrap(nodeHolder.getColumnMaxData()[index]));
0:         dataChunk.min_max.addToMin_values(ByteBuffer.wrap(nodeHolder.getColumnMinData()[index]));
0:       } else {
0:         dataChunk.setData_page_length(nodeHolder.getDataArray()[index].length);
0:         // TODO : Right now the encodings are happening at runtime. change as
0:         // per this encoders.
0:         dataChunk.setEncoders(encodings);
0: 
0:         dataChunk.setRowMajor(false);
0:         // TODO : Right now the encodings are happening at runtime. change as
0:         // per this encoders.
0:         encodings.add(Encoding.DELTA);
0:         dataChunk.setEncoders(encodings);
0:         // TODO writing dummy presence meta need to set actual presence
0:         // meta
0:         PresenceMeta presenceMeta = new PresenceMeta();
0:         presenceMeta.setPresent_bit_streamIsSet(true);
0:         presenceMeta.setPresent_bit_stream(CompressorFactory.getInstance().getCompressor()
0:             .compressByte(nodeHolder.getMeasureNullValueIndex()[index].toByteArray()));
0:         dataChunk.setPresence(presenceMeta);
0:         List<ByteBuffer> encoderMetaList = new ArrayList<ByteBuffer>();
0:         encoderMetaList.add(ByteBuffer.wrap(serializeEncodeMetaUsingByteBuffer(
0:             createValueEncoderMeta(nodeHolder.getCompressionModel(), index))));
0:         dataChunk.setEncoder_meta(encoderMetaList);
0:         dataChunk.min_max
0:             .addToMax_values(ByteBuffer.wrap(nodeHolder.getMeasureColumnMaxData()[index]));
0:         dataChunk.min_max
0:             .addToMin_values(ByteBuffer.wrap(nodeHolder.getMeasureColumnMinData()[index]));
0:       }
0:       dataChunk.setEncoders(encodings);
0:       colDataChunks.add(dataChunk);
0:     }
0:     return colDataChunks;
0:   }
0: 
0:   public static DataChunk3 getDataChunk3(List<NodeHolder> nodeHolderList,
0:       List<ColumnSchema> columnSchenma, SegmentProperties segmentProperties, int index,
0:       boolean isDimensionColumn) throws IOException {
0:     List<DataChunk2> dataChunksList =
0:         getDatachunk2(nodeHolderList, columnSchenma, segmentProperties, index, isDimensionColumn);
1:     int offset = 0;
1:     DataChunk3 dataChunk = new DataChunk3();
1:     List<Integer> pageOffsets = new ArrayList<>();
1:     List<Integer> pageLengths = new ArrayList<>();
1:     int length = 0;
0:     for (int i = 0; i < dataChunksList.size(); i++) {
1:       pageOffsets.add(offset);
0:       length =
0:           dataChunksList.get(i).getData_page_length() + dataChunksList.get(i).getRle_page_length()
0:               + dataChunksList.get(i).getRowid_page_length();
1:       pageLengths.add(length);
1:       offset += length;
0:     }
1:     dataChunk.setData_chunk_list(dataChunksList);
1:     dataChunk.setPage_length(pageLengths);
1:     dataChunk.setPage_offset(pageOffsets);
1:     return dataChunk;
0:   }
0: 
0:   public static byte[] serializeEncodeMetaUsingByteBuffer(ValueEncoderMeta valueEncoderMeta) {
0:     ByteBuffer buffer = null;
0:     if (valueEncoderMeta.getType() == CarbonCommonConstants.SUM_COUNT_VALUE_MEASURE) {
0:       buffer = ByteBuffer.allocate(
0:           (CarbonCommonConstants.DOUBLE_SIZE_IN_BYTE * 3) + CarbonCommonConstants.INT_SIZE_IN_BYTE
0:               + 3);
0:       buffer.putChar(valueEncoderMeta.getType());
0:       buffer.putDouble((Double) valueEncoderMeta.getMaxValue());
0:       buffer.putDouble((Double) valueEncoderMeta.getMinValue());
0:       buffer.putDouble((Double) valueEncoderMeta.getUniqueValue());
0:     } else if (valueEncoderMeta.getType() == CarbonCommonConstants.BIG_INT_MEASURE) {
0:       buffer = ByteBuffer.allocate(
0:           (CarbonCommonConstants.LONG_SIZE_IN_BYTE * 3) + CarbonCommonConstants.INT_SIZE_IN_BYTE
0:               + 3);
0:       buffer.putChar(valueEncoderMeta.getType());
0:       buffer.putLong((Long) valueEncoderMeta.getMaxValue());
0:       buffer.putLong((Long) valueEncoderMeta.getMinValue());
0:       buffer.putLong((Long) valueEncoderMeta.getUniqueValue());
0:     } else {
0:       buffer = ByteBuffer.allocate(CarbonCommonConstants.INT_SIZE_IN_BYTE + 3);
0:       buffer.putChar(valueEncoderMeta.getType());
0:     }
0:     buffer.putInt(valueEncoderMeta.getDecimal());
0:     buffer.put(valueEncoderMeta.getDataTypeSelected());
0:     buffer.flip();
0:     return buffer.array();
0:   }
0: 
0:   public static byte[] getByteValueForMeasure(Object data, DataType dataType) {
0:     ByteBuffer b = null;
0:     switch (dataType) {
0:       case DOUBLE:
0:         b = ByteBuffer.allocate(8);
0:         b.putDouble((Double) data);
0:         b.flip();
0:         return b.array();
0:       case LONG:
0:       case INT:
0:       case SHORT:
0:         b = ByteBuffer.allocate(8);
0:         b.putLong((Long) data);
0:         b.flip();
0:         return b.array();
0:       case DECIMAL:
0:         return DataTypeUtil.bigDecimalToByte((BigDecimal)data);
0:       default:
0:         throw new IllegalArgumentException("Invalid data type");
0:     }
0:   }
0: 
0:   public static int compareMeasureData(byte[] first, byte[] second, DataType dataType) {
1:     ByteBuffer firstBuffer = null;
1:     ByteBuffer secondBuffer = null;
0:     switch (dataType) {
0:       case DOUBLE:
0:         firstBuffer = ByteBuffer.allocate(8);
0:         firstBuffer.put(first);
0:         secondBuffer = ByteBuffer.allocate(8);
0:         secondBuffer.put(first);
0:         firstBuffer.flip();
0:         secondBuffer.flip();
0:         return (int) (firstBuffer.getDouble() - secondBuffer.getDouble());
0:       case LONG:
0:       case INT:
0:       case SHORT:
0:         firstBuffer = ByteBuffer.allocate(8);
0:         firstBuffer.put(first);
0:         secondBuffer = ByteBuffer.allocate(8);
0:         secondBuffer.put(first);
0:         firstBuffer.flip();
0:         secondBuffer.flip();
0:         return (int) (firstBuffer.getLong() - secondBuffer.getLong());
0:       case DECIMAL:
0:         return DataTypeUtil.byteToBigDecimal(first)
0:             .compareTo(DataTypeUtil.byteToBigDecimal(second));
0:       default:
0:         throw new IllegalArgumentException("Invalid data type");
0:     }
0:   }
0: 
commit:f94bae5
/////////////////////////////////////////////////////////////////////////
0:     encoderMeta.setDecimal(compressionModel.getMantissa()[index]);
/////////////////////////////////////////////////////////////////////////
0:       decimalLength[i] = encoderMetas[i].getDecimal();
commit:8d9babe
/////////////////////////////////////////////////////////////////////////
0:       presenceMeta.setPresent_bit_stream(CompressorFactory.getInstance().getCompressor()
commit:d54dc64
/////////////////////////////////////////////////////////////////////////
0: import org.apache.carbondata.core.datastorage.store.compression.SnappyCompression.SnappyByteCompression;
/////////////////////////////////////////////////////////////////////////
0: import org.apache.carbondata.format.BlockletInfo2;
1: import org.apache.carbondata.format.DataChunk2;
/////////////////////////////////////////////////////////////////////////
0:       int[] cardinalities, List<ColumnSchema> columnSchemaList, SegmentProperties segmentProperties)
0:       throws IOException {
0:     FileFooter footer = getFileFooter(infoList, cardinalities, columnSchemaList);
0:     for (BlockletInfoColumnar info : infoList) {
0:       footer.addToBlocklet_info_list(getBlockletInfo(info, columnSchemaList, segmentProperties));
0:     }
0:     return footer;
0:   }
0:   /**
0:    * Below method will be used to get the file footer object
0:    *
0:    * @param infoList         blocklet info
0:    * @param cardinalities    cardinlaity of dimension columns
0:    * @param columnSchemaList column schema list
0:    * @return file footer
0:    */
0:   private static FileFooter getFileFooter(List<BlockletInfoColumnar> infoList, int[] cardinalities,
0:       List<ColumnSchema> columnSchemaList) {
0:     short version = Short.parseShort(
0:         CarbonProperties.getInstance().getProperty(CarbonCommonConstants.CARBON_DATA_FILE_VERSION));
0:     footer.setVersion(version);
0:     footer.setTable_columns(columnSchemaList);
0:     return footer;
0:   }
0: 
0:   /**
0:    * Below method will be used to get the file footer object for
0:    *
0:    * @param infoList         blocklet info
0:    * @param cardinalities    cardinality of each column
0:    * @param columnSchemaList column schema list
0:    * @param dataChunksOffset data chunks offsets
0:    * @param dataChunksLength data chunks length
0:    * @return filefooter thrift object
0:    */
0:   public static FileFooter convertFilterFooter2(List<BlockletInfoColumnar> infoList,
0:       int[] cardinalities, List<ColumnSchema> columnSchemaList, List<List<Long>> dataChunksOffset,
0:       List<List<Short>> dataChunksLength) {
0:     FileFooter footer = getFileFooter(infoList, cardinalities, columnSchemaList);
0:     int index = 0;
0:       footer.addToBlocklet_info_list2(
0:           getBlockletInfo2(info, dataChunksOffset.get(index), dataChunksLength.get(index)));
0:       index++;
/////////////////////////////////////////////////////////////////////////
0:   /**
0:    * Below method will be used to get the blocklet info object for
0:    * data version 2 file
0:    *
0:    * @param blockletInfoColumnar blocklet info
0:    * @param dataChunkOffsets     data chunks offsets
0:    * @param dataChunksLength     data chunks length
0:    * @return blocklet info version 2
0:    */
0:   private static BlockletInfo2 getBlockletInfo2(BlockletInfoColumnar blockletInfoColumnar,
0:       List<Long> dataChunkOffsets, List<Short> dataChunksLength) {
0:     BlockletInfo2 blockletInfo = new BlockletInfo2();
0:     blockletInfo.setNum_rows(blockletInfoColumnar.getNumberOfKeys());
0:     blockletInfo.setColumn_data_chunks_length(dataChunksLength);
0:     blockletInfo.setColumn_data_chunks_offsets(dataChunkOffsets);
0:     return blockletInfo;
0:   }
0: 
0:       List<ColumnSchema> columnSchenma, SegmentProperties segmentProperties) throws IOException {
/////////////////////////////////////////////////////////////////////////
0:     short version = Short.parseShort(
0:         CarbonProperties.getInstance().getProperty(CarbonCommonConstants.CARBON_DATA_FILE_VERSION));
0:     indexHeader.setVersion(version);
/////////////////////////////////////////////////////////////////////////
1:       blockIndex.setOffset(blockIndexInfo.getOffset());
0: 
0:   /**
0:    * Below method will be used to get the data chunk object for all the
0:    * columns
0:    *
0:    * @param blockletInfoColumnar blocklet info
0:    * @param columnSchenma        list of columns
0:    * @param segmentProperties    segment properties
0:    * @return list of data chunks
0:    * @throws IOException
0:    */
0:   public static List<DataChunk2> getDatachunk2(BlockletInfoColumnar blockletInfoColumnar,
0:       List<ColumnSchema> columnSchenma, SegmentProperties segmentProperties) throws IOException {
0:     List<DataChunk2> colDataChunks = new ArrayList<DataChunk2>();
0:     int rowIdIndex = 0;
0:     int aggregateIndex = 0;
0:     boolean[] isSortedKeyColumn = blockletInfoColumnar.getIsSortedKeyColumn();
0:     boolean[] aggKeyBlock = blockletInfoColumnar.getAggKeyBlock();
0:     boolean[] colGrpblock = blockletInfoColumnar.getColGrpBlocks();
0:     for (int i = 0; i < blockletInfoColumnar.getKeyLengths().length; i++) {
0:       DataChunk2 dataChunk = new DataChunk2();
0:       dataChunk.setChunk_meta(getChunkCompressionMeta());
0:       List<Encoding> encodings = new ArrayList<Encoding>();
0:       if (containsEncoding(i, Encoding.DICTIONARY, columnSchenma, segmentProperties)) {
0:         encodings.add(Encoding.DICTIONARY);
0:       }
0:       if (containsEncoding(i, Encoding.DIRECT_DICTIONARY, columnSchenma, segmentProperties)) {
0:         encodings.add(Encoding.DIRECT_DICTIONARY);
0:       }
0:       dataChunk.setRowMajor(colGrpblock[i]);
0:       //TODO : Once schema PR is merged and information needs to be passed here.
0:       dataChunk.setData_page_length(blockletInfoColumnar.getKeyLengths()[i]);
0:       if (aggKeyBlock[i]) {
0:         dataChunk.setRle_page_length(blockletInfoColumnar.getDataIndexMapLength()[aggregateIndex]);
0:         encodings.add(Encoding.RLE);
0:         aggregateIndex++;
0:       }
0:       dataChunk
0:           .setSort_state(isSortedKeyColumn[i] ? SortState.SORT_EXPLICIT : SortState.SORT_NATIVE);
0: 
0:       if (!isSortedKeyColumn[i]) {
0:         dataChunk.setRowid_page_length(blockletInfoColumnar.getKeyBlockIndexLength()[rowIdIndex]);
0:         encodings.add(Encoding.INVERTED_INDEX);
0:         rowIdIndex++;
0:       }
0: 
0:       //TODO : Right now the encodings are happening at runtime. change as per this encoders.
0:       dataChunk.setEncoders(encodings);
0: 
0:       colDataChunks.add(dataChunk);
0:     }
0: 
0:     for (int i = 0; i < blockletInfoColumnar.getMeasureLength().length; i++) {
0:       DataChunk2 dataChunk = new DataChunk2();
0:       dataChunk.setChunk_meta(getChunkCompressionMeta());
0:       dataChunk.setRowMajor(false);
0:       //TODO : Once schema PR is merged and information needs to be passed here.
0:       dataChunk.setData_page_length(blockletInfoColumnar.getMeasureLength()[i]);
0:       //TODO : Right now the encodings are happening at runtime. change as per this encoders.
0:       List<Encoding> encodings = new ArrayList<Encoding>();
0:       encodings.add(Encoding.DELTA);
0:       dataChunk.setEncoders(encodings);
0:       //TODO writing dummy presence meta need to set actual presence
0:       //meta
0:       PresenceMeta presenceMeta = new PresenceMeta();
0:       presenceMeta.setPresent_bit_streamIsSet(true);
0:       presenceMeta.setPresent_bit_stream(SnappyByteCompression.INSTANCE
0:           .compress(blockletInfoColumnar.getMeasureNullValueIndex()[i].toByteArray()));
0:       dataChunk.setPresence(presenceMeta);
0:       //TODO : PresenceMeta needs to be implemented and set here
0:       // dataChunk.setPresence(new PresenceMeta());
0:       //TODO : Need to write ValueCompression meta here.
0:       List<ByteBuffer> encoderMetaList = new ArrayList<ByteBuffer>();
0:       encoderMetaList.add(ByteBuffer.wrap(serializeEncoderMeta(
0:           createValueEncoderMeta(blockletInfoColumnar.getCompressionModel(), i))));
0:       dataChunk.setEncoder_meta(encoderMetaList);
0:       colDataChunks.add(dataChunk);
0:     }
0:     return colDataChunks;
0:   }
author:Zhangshunyu
-------------------------------------------------------------------------------
commit:96a75b3
/////////////////////////////////////////////////////////////////////////
0:         if (!encodings.contains(Encoding.INVERTED_INDEX)) {
0:           encodings.add(Encoding.INVERTED_INDEX);
0:         }
============================================================================