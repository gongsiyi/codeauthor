1:376d69f: /*
1:376d69f:  * Licensed to the Apache Software Foundation (ASF) under one or more
1:376d69f:  * contributor license agreements.  See the NOTICE file distributed with
1:376d69f:  * this work for additional information regarding copyright ownership.
1:376d69f:  * The ASF licenses this file to You under the Apache License, Version 2.0
1:376d69f:  * (the "License"); you may not use this file except in compliance with
1:376d69f:  * the License.  You may obtain a copy of the License at
1:376d69f:  *
1:376d69f:  *    http://www.apache.org/licenses/LICENSE-2.0
1:376d69f:  *
1:376d69f:  * Unless required by applicable law or agreed to in writing, software
1:376d69f:  * distributed under the License is distributed on an "AS IS" BASIS,
1:376d69f:  * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
1:376d69f:  * See the License for the specific language governing permissions and
1:376d69f:  * limitations under the License.
1:376d69f:  */
1:e5e74fc: 
1:376d69f: package org.apache.carbondata.spark.vectorreader;
1:376d69f: 
1:e5e74fc: import java.math.BigDecimal;
1:376d69f: 
1:e5e74fc: import org.apache.carbondata.core.metadata.datatype.DataType;
1:ce09aaa: import org.apache.carbondata.core.scan.result.vector.CarbonColumnVector;
1:3a4b881: import org.apache.carbondata.core.scan.result.vector.CarbonDictionary;
1:1f54c47: 
1:3a4b881: import org.apache.parquet.column.Encoding;
1:bcef656: import org.apache.spark.sql.CarbonVectorProxy;
1:347b8e1: import org.apache.spark.sql.carbondata.execution.datasources.CarbonSparkDataSourceUtil;
1:376d69f: import org.apache.spark.sql.types.Decimal;
1:376d69f: 
1:6fee993: class ColumnarVectorWrapper implements CarbonColumnVector {
1:376d69f: 
1:74c3eb1:   private CarbonVectorProxy sparkColumnVectorProxy;
1:376d69f: 
1:1c5b4a5:   private boolean[] filteredRows;
1:1c5b4a5: 
1:1c5b4a5:   private int counter;
1:1c5b4a5: 
1:bcef656:   private int ordinal;
1:bcef656: 
1:74c3eb1:   private boolean isDictionary;
1:74c3eb1: 
1:1c5b4a5:   private boolean filteredRowsExist;
1:1c5b4a5: 
1:1f54c47:   private DataType blockDataType;
1:3a4b881: 
1:74c3eb1:   private CarbonColumnVector dictionaryVector;
1:74c3eb1: 
1:bcef656:   ColumnarVectorWrapper(CarbonVectorProxy writableColumnVector,
1:74c3eb1:       boolean[] filteredRows, int ordinal) {
1:74c3eb1:     this.sparkColumnVectorProxy = writableColumnVector;
1:1c5b4a5:     this.filteredRows = filteredRows;
1:bcef656:     this.ordinal = ordinal;
1:376d69f:   }
1:376d69f: 
1:9f94529:   @Override public void putBoolean(int rowId, boolean value) {
1:1c5b4a5:     if (!filteredRows[rowId]) {
1:74c3eb1:       sparkColumnVectorProxy.putBoolean(counter++, value, ordinal);
1:1c5b4a5:     }
1:3a4b881:   }
1:e5e74fc: 
1:9f94529:   @Override public void putFloat(int rowId, float value) {
1:1c5b4a5:     if (!filteredRows[rowId]) {
1:74c3eb1:       sparkColumnVectorProxy.putFloat(counter++, value,ordinal);
1:1c5b4a5:     }
1:3a4b881:   }
1:9f94529: 
1:376d69f:   @Override public void putShort(int rowId, short value) {
1:1c5b4a5:     if (!filteredRows[rowId]) {
1:74c3eb1:       sparkColumnVectorProxy.putShort(counter++, value, ordinal);
1:1c5b4a5:     }
1:376d69f:   }
1:376d69f: 
1:6b3b16c:   @Override public void putShorts(int rowId, int count, short value) {
1:1c5b4a5:     if (filteredRowsExist) {
1:1c5b4a5:       for (int i = 0; i < count; i++) {
1:1c5b4a5:         if (!filteredRows[rowId]) {
1:74c3eb1:           sparkColumnVectorProxy.putShort(counter++, value, ordinal);
1:1c5b4a5:         }
1:1c5b4a5:         rowId++;
1:1c5b4a5:       }
1:1c5b4a5:     } else {
1:74c3eb1:       sparkColumnVectorProxy.putShorts(rowId, count, value, ordinal);
1:1c5b4a5:     }
1:6b3b16c:   }
1:6b3b16c: 
1:376d69f:   @Override public void putInt(int rowId, int value) {
1:1c5b4a5:     if (!filteredRows[rowId]) {
1:74c3eb1:       if (isDictionary) {
1:74c3eb1:         sparkColumnVectorProxy.putDictionaryInt(counter++, value, ordinal);
1:74c3eb1:       } else {
1:74c3eb1:         sparkColumnVectorProxy.putInt(counter++, value, ordinal);
1:74c3eb1:       }
1:1c5b4a5:     }
1:376d69f:   }
1:376d69f: 
1:6b3b16c:   @Override public void putInts(int rowId, int count, int value) {
1:1c5b4a5:     if (filteredRowsExist) {
1:1c5b4a5:       for (int i = 0; i < count; i++) {
1:1c5b4a5:         if (!filteredRows[rowId]) {
1:74c3eb1:           sparkColumnVectorProxy.putInt(counter++, value, ordinal);
1:1c5b4a5:         }
1:1c5b4a5:         rowId++;
1:1c5b4a5:       }
1:1c5b4a5:     } else {
1:74c3eb1:       sparkColumnVectorProxy.putInts(rowId, count, value, ordinal);
1:1c5b4a5:     }
1:6b3b16c:   }
1:6b3b16c: 
1:376d69f:   @Override public void putLong(int rowId, long value) {
1:1c5b4a5:     if (!filteredRows[rowId]) {
1:74c3eb1:       sparkColumnVectorProxy.putLong(counter++, value, ordinal);
1:1c5b4a5:     }
1:376d69f:   }
1:376d69f: 
1:6b3b16c:   @Override public void putLongs(int rowId, int count, long value) {
1:1c5b4a5:     if (filteredRowsExist) {
1:1c5b4a5:       for (int i = 0; i < count; i++) {
1:1c5b4a5:         if (!filteredRows[rowId]) {
1:74c3eb1:           sparkColumnVectorProxy.putLong(counter++, value, ordinal);
1:1c5b4a5:         }
1:1c5b4a5:         rowId++;
1:1c5b4a5:       }
1:1c5b4a5:     } else {
1:74c3eb1:       sparkColumnVectorProxy.putLongs(rowId, count, value, ordinal);
1:1c5b4a5:     }
1:1c5b4a5:   }
1:376d69f: 
1:e5e74fc:   @Override public void putDecimal(int rowId, BigDecimal value, int precision) {
1:1c5b4a5:     if (!filteredRows[rowId]) {
1:982d03f:       Decimal toDecimal = Decimal.apply(value);
1:74c3eb1:       sparkColumnVectorProxy.putDecimal(counter++, toDecimal, precision, ordinal);
1:1c5b4a5:     }
1:376d69f:   }
1:6b3b16c: 
1:e5e74fc:   @Override public void putDecimals(int rowId, int count, BigDecimal value, int precision) {
1:982d03f:     Decimal decimal = Decimal.apply(value);
1:6b3b16c:     for (int i = 0; i < count; i++) {
1:1c5b4a5:       if (!filteredRows[rowId]) {
1:74c3eb1:         sparkColumnVectorProxy.putDecimal(counter++, decimal, precision, ordinal);
1:6b3b16c:       }
1:1c5b4a5:       rowId++;
1:6b3b16c:     }
1:6b3b16c:   }
1:6b3b16c: 
1:376d69f:   @Override public void putDouble(int rowId, double value) {
1:1c5b4a5:     if (!filteredRows[rowId]) {
1:74c3eb1:       sparkColumnVectorProxy.putDouble(counter++, value, ordinal);
1:1c5b4a5:     }
1:376d69f:   }
1:376d69f: 
1:6b3b16c:   @Override public void putDoubles(int rowId, int count, double value) {
1:1c5b4a5:     if (filteredRowsExist) {
1:1c5b4a5:       for (int i = 0; i < count; i++) {
1:1c5b4a5:         if (!filteredRows[rowId]) {
1:74c3eb1:           sparkColumnVectorProxy.putDouble(counter++, value, ordinal);
1:1c5b4a5:         }
1:1c5b4a5:         rowId++;
1:1c5b4a5:       }
1:1c5b4a5:     } else {
1:74c3eb1:       sparkColumnVectorProxy.putDoubles(rowId, count, value, ordinal);
1:1c5b4a5:     }
1:6b3b16c:   }
1:6b3b16c: 
1:376d69f:   @Override public void putBytes(int rowId, byte[] value) {
1:1c5b4a5:     if (!filteredRows[rowId]) {
1:74c3eb1:       sparkColumnVectorProxy.putByteArray(counter++, value, ordinal);
1:1c5b4a5:     }
1:376d69f:   }
1:376d69f: 
1:6b3b16c:   @Override public void putBytes(int rowId, int count, byte[] value) {
1:6b3b16c:     for (int i = 0; i < count; i++) {
1:1c5b4a5:       if (!filteredRows[rowId]) {
1:74c3eb1:         sparkColumnVectorProxy.putByteArray(counter++, value, ordinal);
1:1c5b4a5:       }
1:1c5b4a5:       rowId++;
1:6b3b16c:     }
1:6b3b16c:   }
1:6b3b16c: 
1:376d69f:   @Override public void putBytes(int rowId, int offset, int length, byte[] value) {
1:1c5b4a5:     if (!filteredRows[rowId]) {
1:74c3eb1:       sparkColumnVectorProxy.putByteArray(counter++, value, offset, length, ordinal);
1:1c5b4a5:     }
1:376d69f:   }
1:376d69f: 
1:376d69f:   @Override public void putNull(int rowId) {
1:1c5b4a5:     if (!filteredRows[rowId]) {
1:74c3eb1:       sparkColumnVectorProxy.putNull(counter++, ordinal);
1:1c5b4a5:     }
1:376d69f:   }
1:376d69f: 
1:6b3b16c:   @Override public void putNulls(int rowId, int count) {
1:1c5b4a5:     if (filteredRowsExist) {
1:1c5b4a5:       for (int i = 0; i < count; i++) {
1:1c5b4a5:         if (!filteredRows[rowId]) {
1:74c3eb1:           sparkColumnVectorProxy.putNull(counter++, ordinal);
1:1c5b4a5:         }
1:1c5b4a5:         rowId++;
1:1c5b4a5:       }
1:1c5b4a5:     } else {
1:74c3eb1:       sparkColumnVectorProxy.putNulls(rowId, count,ordinal);
1:1c5b4a5:     }
1:6b3b16c:   }
1:6b3b16c: 
1:3a4b881:   @Override public void putNotNull(int rowId) {
1:3a4b881:     if (!filteredRows[rowId]) {
1:74c3eb1:       sparkColumnVectorProxy.putNotNull(counter++,ordinal);
1:3a4b881:     }
1:3a4b881:   }
1:3a4b881: 
1:3a4b881:   @Override public void putNotNull(int rowId, int count) {
1:3a4b881:     if (filteredRowsExist) {
1:3a4b881:       for (int i = 0; i < count; i++) {
1:3a4b881:         if (!filteredRows[rowId]) {
1:74c3eb1:           sparkColumnVectorProxy.putNotNull(counter++, ordinal);
1:3a4b881:         }
1:3a4b881:         rowId++;
1:3a4b881:       }
1:3a4b881:     } else {
1:74c3eb1:       sparkColumnVectorProxy.putNotNulls(rowId, count, ordinal);
1:3a4b881:     }
1:9f94529:   }
1:3a4b881: 
1:376d69f:   @Override public boolean isNull(int rowId) {
1:74c3eb1:     return sparkColumnVectorProxy.isNullAt(rowId,ordinal);
1:376d69f:   }
1:376d69f: 
1:376d69f:   @Override public void putObject(int rowId, Object obj) {
1:376d69f:     //TODO handle complex types
1:376d69f:   }
1:376d69f: 
1:376d69f:   @Override public Object getData(int rowId) {
1:376d69f:     //TODO handle complex types
1:376d69f:     return null;
1:376d69f:   }
1:376d69f: 
1:376d69f:   @Override public void reset() {
1:1c5b4a5:     counter = 0;
1:1c5b4a5:     filteredRowsExist = false;
1:3a4b881:     if (null != dictionaryVector) {
1:3a4b881:       dictionaryVector.reset();
1:376d69f:     }
1:376d69f:   }
1:9f94529: 
1:9f94529:   @Override public DataType getType() {
1:bcef656:     return CarbonSparkDataSourceUtil
1:74c3eb1:         .convertSparkToCarbonDataType(sparkColumnVectorProxy.dataType(ordinal));
1:3a4b881:   }
1:1c5b4a5: 
1:1f54c47:   @Override
1:1f54c47:   public DataType getBlockDataType() {
1:1f54c47:     return blockDataType;
1:1f54c47:   }
1:1f54c47: 
1:1f54c47:   @Override
1:1f54c47:   public void setBlockDataType(DataType blockDataType) {
1:1f54c47:     this.blockDataType = blockDataType;
1:1f54c47:   }
1:1f54c47: 
1:74c3eb1:   @Override
1:74c3eb1:   public void setFilteredRowsExist(boolean filteredRowsExist) {
1:1c5b4a5:     this.filteredRowsExist = filteredRowsExist;
1:1c5b4a5:   }
1:3a4b881: 
1:3a4b881:   @Override public void setDictionary(CarbonDictionary dictionary) {
1:3a4b881:     if (dictionary == null) {
1:74c3eb1:       sparkColumnVectorProxy.setDictionary(null, ordinal);
1:3a4b881:     } else {
1:74c3eb1:       sparkColumnVectorProxy
1:74c3eb1:           .setDictionary(new CarbonDictionaryWrapper(Encoding.PLAIN, dictionary),ordinal);
1:3a4b881:     }
1:3a4b881:   }
1:3a4b881: 
1:74c3eb1:   private void  setDictionaryType(boolean type) {
1:74c3eb1:     this.isDictionary = type;
1:74c3eb1:   }
1:74c3eb1: 
1:3a4b881:   @Override public boolean hasDictionary() {
1:74c3eb1:     return sparkColumnVectorProxy.hasDictionary(ordinal);
1:74c3eb1:   }
1:74c3eb1: 
1:74c3eb1:   public void reserveDictionaryIds() {
1:74c3eb1:     sparkColumnVectorProxy.reserveDictionaryIds(sparkColumnVectorProxy.numRows(), ordinal);
1:74c3eb1:     dictionaryVector = new ColumnarVectorWrapper(sparkColumnVectorProxy, filteredRows, ordinal);
1:74c3eb1:     ((ColumnarVectorWrapper) dictionaryVector).isDictionary = true;
1:3a4b881:   }
1:3a4b881: 
1:3a4b881:   @Override public CarbonColumnVector getDictionaryVector() {
1:3a4b881:     return dictionaryVector;
1:3a4b881:   }
1:9f94529: }
============================================================================
author:sandeep-katta
-------------------------------------------------------------------------------
commit:74c3eb1
/////////////////////////////////////////////////////////////////////////
1:   private CarbonVectorProxy sparkColumnVectorProxy;
/////////////////////////////////////////////////////////////////////////
1:   private boolean isDictionary;
1: 
1:   private CarbonColumnVector dictionaryVector;
1: 
1:       boolean[] filteredRows, int ordinal) {
1:     this.sparkColumnVectorProxy = writableColumnVector;
1:       sparkColumnVectorProxy.putBoolean(counter++, value, ordinal);
1:       sparkColumnVectorProxy.putFloat(counter++, value,ordinal);
1:       sparkColumnVectorProxy.putShort(counter++, value, ordinal);
/////////////////////////////////////////////////////////////////////////
1:           sparkColumnVectorProxy.putShort(counter++, value, ordinal);
1:       sparkColumnVectorProxy.putShorts(rowId, count, value, ordinal);
1:       if (isDictionary) {
1:         sparkColumnVectorProxy.putDictionaryInt(counter++, value, ordinal);
1:       } else {
1:         sparkColumnVectorProxy.putInt(counter++, value, ordinal);
1:       }
/////////////////////////////////////////////////////////////////////////
1:           sparkColumnVectorProxy.putInt(counter++, value, ordinal);
1:       sparkColumnVectorProxy.putInts(rowId, count, value, ordinal);
1:       sparkColumnVectorProxy.putLong(counter++, value, ordinal);
/////////////////////////////////////////////////////////////////////////
1:           sparkColumnVectorProxy.putLong(counter++, value, ordinal);
1:       sparkColumnVectorProxy.putLongs(rowId, count, value, ordinal);
1:       sparkColumnVectorProxy.putDecimal(counter++, toDecimal, precision, ordinal);
/////////////////////////////////////////////////////////////////////////
1:         sparkColumnVectorProxy.putDecimal(counter++, decimal, precision, ordinal);
/////////////////////////////////////////////////////////////////////////
1:       sparkColumnVectorProxy.putDouble(counter++, value, ordinal);
/////////////////////////////////////////////////////////////////////////
1:           sparkColumnVectorProxy.putDouble(counter++, value, ordinal);
1:       sparkColumnVectorProxy.putDoubles(rowId, count, value, ordinal);
1:       sparkColumnVectorProxy.putByteArray(counter++, value, ordinal);
1:         sparkColumnVectorProxy.putByteArray(counter++, value, ordinal);
/////////////////////////////////////////////////////////////////////////
1:       sparkColumnVectorProxy.putByteArray(counter++, value, offset, length, ordinal);
1:       sparkColumnVectorProxy.putNull(counter++, ordinal);
/////////////////////////////////////////////////////////////////////////
1:           sparkColumnVectorProxy.putNull(counter++, ordinal);
1:       sparkColumnVectorProxy.putNulls(rowId, count,ordinal);
1:       sparkColumnVectorProxy.putNotNull(counter++,ordinal);
/////////////////////////////////////////////////////////////////////////
1:           sparkColumnVectorProxy.putNotNull(counter++, ordinal);
1:       sparkColumnVectorProxy.putNotNulls(rowId, count, ordinal);
1:     return sparkColumnVectorProxy.isNullAt(rowId,ordinal);
/////////////////////////////////////////////////////////////////////////
1:         .convertSparkToCarbonDataType(sparkColumnVectorProxy.dataType(ordinal));
/////////////////////////////////////////////////////////////////////////
1:   @Override
1:   public void setFilteredRowsExist(boolean filteredRowsExist) {
1:       sparkColumnVectorProxy.setDictionary(null, ordinal);
1:       sparkColumnVectorProxy
1:           .setDictionary(new CarbonDictionaryWrapper(Encoding.PLAIN, dictionary),ordinal);
1:   private void  setDictionaryType(boolean type) {
1:     this.isDictionary = type;
1:   }
1: 
1:     return sparkColumnVectorProxy.hasDictionary(ordinal);
1:   }
1: 
1:   public void reserveDictionaryIds() {
1:     sparkColumnVectorProxy.reserveDictionaryIds(sparkColumnVectorProxy.numRows(), ordinal);
1:     dictionaryVector = new ColumnarVectorWrapper(sparkColumnVectorProxy, filteredRows, ordinal);
1:     ((ColumnarVectorWrapper) dictionaryVector).isDictionary = true;
author:sujith71955
-------------------------------------------------------------------------------
commit:bcef656
/////////////////////////////////////////////////////////////////////////
1: import org.apache.spark.sql.CarbonVectorProxy;
0:   private CarbonVectorProxy writableColumnVector;
1:   private int ordinal;
1: 
1:   ColumnarVectorWrapper(CarbonVectorProxy writableColumnVector,
0:                         boolean[] filteredRows, int ordinal) {
0:     this.writableColumnVector = writableColumnVector;
1:     this.ordinal = ordinal;
0:       writableColumnVector.putBoolean(counter++, value, ordinal);
0:       writableColumnVector.putFloat(counter++, value,ordinal);
0:       writableColumnVector.putShort(counter++, value, ordinal);
/////////////////////////////////////////////////////////////////////////
0:           writableColumnVector.putShort(counter++, value, ordinal);
0:       writableColumnVector.putShorts(rowId, count, value, ordinal);
0:       writableColumnVector.putInt(counter++, value, ordinal);
/////////////////////////////////////////////////////////////////////////
0:           writableColumnVector.putInt(counter++, value, ordinal);
0:       writableColumnVector.putInts(rowId, count, value, ordinal);
0:       writableColumnVector.putLong(counter++, value, ordinal);
/////////////////////////////////////////////////////////////////////////
0:           writableColumnVector.putLong(counter++, value, ordinal);
0:       writableColumnVector.putLongs(rowId, count, value, ordinal);
0:       writableColumnVector.putDecimal(counter++, toDecimal, precision, ordinal);
/////////////////////////////////////////////////////////////////////////
0:         writableColumnVector.putDecimal(counter++, decimal, precision, ordinal);
/////////////////////////////////////////////////////////////////////////
0:       writableColumnVector.putDouble(counter++, value, ordinal);
/////////////////////////////////////////////////////////////////////////
0:           writableColumnVector.putDouble(counter++, value, ordinal);
0:       writableColumnVector.putDoubles(rowId, count, value, ordinal);
0:       writableColumnVector.putByteArray(counter++, value, ordinal);
0:         writableColumnVector.putByteArray(counter++, value, ordinal);
/////////////////////////////////////////////////////////////////////////
0:       writableColumnVector.putByteArray(counter++, value, offset, length, ordinal);
0:       writableColumnVector.putNull(counter++, ordinal);
/////////////////////////////////////////////////////////////////////////
0:           writableColumnVector.putNull(counter++, ordinal);
0:       writableColumnVector.putNulls(rowId, count,ordinal);
/////////////////////////////////////////////////////////////////////////
0:     return writableColumnVector.isNullAt(rowId,ordinal);
/////////////////////////////////////////////////////////////////////////
1:     return CarbonSparkDataSourceUtil
0:         .convertSparkToCarbonDataType(writableColumnVector.dataType(ordinal));
author:ravipesala
-------------------------------------------------------------------------------
commit:347b8e1
/////////////////////////////////////////////////////////////////////////
1: import org.apache.spark.sql.carbondata.execution.datasources.CarbonSparkDataSourceUtil;
/////////////////////////////////////////////////////////////////////////
0:     return CarbonSparkDataSourceUtil.convertSparkToCarbonDataType(columnVector.dataType());
commit:9af6050
/////////////////////////////////////////////////////////////////////////
0:           columnVector.putShort(counter++, value);
/////////////////////////////////////////////////////////////////////////
0:           columnVector.putInt(counter++, value);
/////////////////////////////////////////////////////////////////////////
0:           columnVector.putLong(counter++, value);
/////////////////////////////////////////////////////////////////////////
0:         columnVector.putDecimal(counter++, value, precision);
/////////////////////////////////////////////////////////////////////////
0:           columnVector.putDouble(counter++, value);
/////////////////////////////////////////////////////////////////////////
0:         columnVector.putByteArray(counter++, value);
/////////////////////////////////////////////////////////////////////////
0:           columnVector.putNull(counter++);
commit:1c5b4a5
/////////////////////////////////////////////////////////////////////////
1:   private boolean[] filteredRows;
1: 
1:   private int counter;
1: 
1:   private boolean filteredRowsExist;
1: 
0:   public ColumnarVectorWrapper(ColumnVector columnVector, boolean[] filteredRows) {
1:     this.filteredRows = filteredRows;
1:     if (!filteredRows[rowId]) {
0:       columnVector.putBoolean(counter++, value);
1:     }
1:     if (!filteredRows[rowId]) {
0:       columnVector.putFloat(counter++, value);
1:     }
1:     if (!filteredRows[rowId]) {
0:       columnVector.putShort(counter++, value);
1:     }
1:     if (filteredRowsExist) {
1:       for (int i = 0; i < count; i++) {
1:         if (!filteredRows[rowId]) {
0:           putShort(counter++, value);
1:         }
1:         rowId++;
1:       }
1:     } else {
0:       columnVector.putShorts(rowId, count, value);
1:     }
1:     if (!filteredRows[rowId]) {
0:       columnVector.putInt(counter++, value);
1:     }
1:     if (filteredRowsExist) {
1:       for (int i = 0; i < count; i++) {
1:         if (!filteredRows[rowId]) {
0:           putInt(counter++, value);
1:         }
1:         rowId++;
1:       }
1:     } else {
0:       columnVector.putInts(rowId, count, value);
1:     }
1:     if (!filteredRows[rowId]) {
0:       columnVector.putLong(counter++, value);
1:     }
1:     if (filteredRowsExist) {
1:       for (int i = 0; i < count; i++) {
1:         if (!filteredRows[rowId]) {
0:           putLong(counter++, value);
1:         }
1:         rowId++;
1:       }
1:     } else {
0:       columnVector.putLongs(rowId, count, value);
1:     }
1:     if (!filteredRows[rowId]) {
0:       columnVector.putDecimal(counter++, value, precision);
1:     }
1:       if (!filteredRows[rowId]) {
0:         putDecimal(counter++, value, precision);
1:       }
1:       rowId++;
1:     if (!filteredRows[rowId]) {
0:       columnVector.putDouble(counter++, value);
1:     }
1:     if (filteredRowsExist) {
1:       for (int i = 0; i < count; i++) {
1:         if (!filteredRows[rowId]) {
0:           putDouble(counter++, value);
1:         }
1:         rowId++;
1:       }
1:     } else {
0:       columnVector.putDoubles(rowId, count, value);
1:     }
1:     if (!filteredRows[rowId]) {
0:       columnVector.putByteArray(counter++, value);
1:     }
1:       if (!filteredRows[rowId]) {
0:         putBytes(counter++, value);
1:       }
1:       rowId++;
1:     if (!filteredRows[rowId]) {
0:       columnVector.putByteArray(counter++, value, offset, length);
1:     }
1:     if (!filteredRows[rowId]) {
0:       columnVector.putNull(counter++);
1:     }
1:     if (filteredRowsExist) {
1:       for (int i = 0; i < count; i++) {
1:         if (!filteredRows[rowId]) {
0:           putNull(counter++);
1:         }
1:         rowId++;
1:       }
1:     } else {
0:       columnVector.putNulls(rowId, count);
1:     }
/////////////////////////////////////////////////////////////////////////
1:     counter = 0;
1:     filteredRowsExist = false;
1: 
0:   @Override public void setFilteredRowsExist(boolean filteredRowsExist) {
1:     this.filteredRowsExist = filteredRowsExist;
1:   }
commit:376d69f
/////////////////////////////////////////////////////////////////////////
1: /*
1:  * Licensed to the Apache Software Foundation (ASF) under one or more
1:  * contributor license agreements.  See the NOTICE file distributed with
1:  * this work for additional information regarding copyright ownership.
1:  * The ASF licenses this file to You under the Apache License, Version 2.0
1:  * (the "License"); you may not use this file except in compliance with
1:  * the License.  You may obtain a copy of the License at
1:  *
1:  *    http://www.apache.org/licenses/LICENSE-2.0
1:  *
1:  * Unless required by applicable law or agreed to in writing, software
1:  * distributed under the License is distributed on an "AS IS" BASIS,
1:  * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
1:  * See the License for the specific language governing permissions and
1:  * limitations under the License.
1:  */
1: package org.apache.carbondata.spark.vectorreader;
1: 
0: import org.apache.carbondata.scan.result.vector.CarbonColumnVector;
1: 
0: import org.apache.spark.sql.execution.vectorized.ColumnVector;
1: import org.apache.spark.sql.types.Decimal;
1: 
0: public class ColumnarVectorWrapper implements CarbonColumnVector {
1: 
0:   private ColumnVector columnVector;
1: 
0:   public ColumnarVectorWrapper(ColumnVector columnVector) {
0:     this.columnVector = columnVector;
1:   }
1: 
1:   @Override public void putShort(int rowId, short value) {
0:     columnVector.putShort(rowId, value);
1:   }
1: 
1:   @Override public void putInt(int rowId, int value) {
0:     columnVector.putInt(rowId, value);
1:   }
1: 
1:   @Override public void putLong(int rowId, long value) {
0:     columnVector.putLong(rowId, value);
1:   }
1: 
0:   @Override public void putDecimal(int rowId, Decimal value, int precision) {
0:     columnVector.putDecimal(rowId, value, precision);
1:   }
1: 
1:   @Override public void putDouble(int rowId, double value) {
0:     columnVector.putDouble(rowId, value);
1:   }
1: 
1:   @Override public void putBytes(int rowId, byte[] value) {
0:     columnVector.putByteArray(rowId, value);
1:   }
1: 
1:   @Override public void putBytes(int rowId, int offset, int length, byte[] value) {
0:     columnVector.putByteArray(rowId, value, offset, length);
1:   }
1: 
1:   @Override public void putNull(int rowId) {
0:     columnVector.putNull(rowId);
1:   }
1: 
1:   @Override public boolean isNull(int rowId) {
0:     return columnVector.isNullAt(rowId);
1:   }
1: 
1:   @Override public void putObject(int rowId, Object obj) {
1:     //TODO handle complex types
1:   }
1: 
1:   @Override public Object getData(int rowId) {
1:     //TODO handle complex types
1:     return null;
1:   }
1: 
1:   @Override public void reset() {
0: //    columnVector.reset();
1:   }
1: }
author:kumarvishal09
-------------------------------------------------------------------------------
commit:3a4b881
/////////////////////////////////////////////////////////////////////////
1: import org.apache.carbondata.core.scan.result.vector.CarbonDictionary;
1: import org.apache.parquet.column.Encoding;
/////////////////////////////////////////////////////////////////////////
0:   private CarbonColumnVector dictionaryVector;
1: 
0:     if (columnVector.getDictionaryIds() != null) {
0:       this.dictionaryVector =
0:           new ColumnarVectorWrapper(columnVector.getDictionaryIds(), filteredRows);
1:     }
/////////////////////////////////////////////////////////////////////////
1:   @Override public void putNotNull(int rowId) {
1:     if (!filteredRows[rowId]) {
0:       columnVector.putNotNull(counter++);
1:     }
1:   }
1: 
1:   @Override public void putNotNull(int rowId, int count) {
1:     if (filteredRowsExist) {
1:       for (int i = 0; i < count; i++) {
1:         if (!filteredRows[rowId]) {
0:           columnVector.putNotNull(counter++);
1:         }
1:         rowId++;
1:       }
1:     } else {
0:       columnVector.putNotNulls(rowId, count);
1:     }
1:   }
1: 
/////////////////////////////////////////////////////////////////////////
1:     if (null != dictionaryVector) {
1:       dictionaryVector.reset();
1:     }
/////////////////////////////////////////////////////////////////////////
1: 
1:   @Override public void setDictionary(CarbonDictionary dictionary) {
1:     if (dictionary == null) {
0:       columnVector.setDictionary(null);
1:     } else {
0:       columnVector.setDictionary(new CarbonDictionaryWrapper(Encoding.PLAIN, dictionary));
1:     }
1:   }
1: 
1:   @Override public boolean hasDictionary() {
0:     return columnVector.hasDictionary();
1:   }
1: 
1:   @Override public CarbonColumnVector getDictionaryVector() {
1:     return dictionaryVector;
1:   }
author:Jacky Li
-------------------------------------------------------------------------------
commit:982d03f
/////////////////////////////////////////////////////////////////////////
/////////////////////////////////////////////////////////////////////////
1:       Decimal toDecimal = Decimal.apply(value);
1:     Decimal decimal = Decimal.apply(value);
0:         columnVector.putDecimal(counter++, decimal, precision);
/////////////////////////////////////////////////////////////////////////
0:     return CarbonScalaUtil.convertSparkToCarbonDataType(columnVector.dataType());
commit:daa6465
/////////////////////////////////////////////////////////////////////////
0:   ColumnarVectorWrapper(ColumnVector columnVector, boolean[] filteredRows) {
author:manishgupta88
-------------------------------------------------------------------------------
commit:1f54c47
/////////////////////////////////////////////////////////////////////////
1:   private DataType blockDataType;
1: 
/////////////////////////////////////////////////////////////////////////
1:   @Override
1:   public DataType getBlockDataType() {
1:     return blockDataType;
1:   }
1: 
1:   @Override
1:   public void setBlockDataType(DataType blockDataType) {
1:     this.blockDataType = blockDataType;
1:   }
1: 
commit:6b3b16c
/////////////////////////////////////////////////////////////////////////
1:   @Override public void putShorts(int rowId, int count, short value) {
0:     columnVector.putShorts(rowId, count, value);
1:   }
1: 
1:   @Override public void putInts(int rowId, int count, int value) {
0:     columnVector.putInts(rowId, count, value);
1:   }
1: 
1:   @Override public void putLongs(int rowId, int count, long value) {
0:     columnVector.putLongs(rowId, count, value);
1:   }
1: 
0:   @Override public void putDecimals(int rowId, int count, Decimal value, int precision) {
1:     for (int i = 0; i < count; i++) {
0:       rowId += i;
0:       putDecimal(rowId, value, precision);
1:     }
1:   }
1: 
1:   @Override public void putDoubles(int rowId, int count, double value) {
0:     columnVector.putDoubles(rowId, count, value);
1:   }
1: 
1:   @Override public void putBytes(int rowId, int count, byte[] value) {
1:     for (int i = 0; i < count; i++) {
0:       rowId += i;
0:       putBytes(rowId, value);
1:     }
1:   }
1: 
/////////////////////////////////////////////////////////////////////////
1:   @Override public void putNulls(int rowId, int count) {
0:     columnVector.putNulls(rowId, count);
1:   }
1: 
author:Bhavya
-------------------------------------------------------------------------------
commit:e5e74fc
/////////////////////////////////////////////////////////////////////////
1: import java.math.BigDecimal;
1: 
1: import org.apache.carbondata.core.metadata.datatype.DataType;
0: import org.apache.carbondata.spark.util.CarbonScalaUtil;
/////////////////////////////////////////////////////////////////////////
0:   private DataType dataType;
1: 
0:     this.dataType = CarbonScalaUtil.convertSparkToCarbonDataType(columnVector.dataType());
/////////////////////////////////////////////////////////////////////////
1:   @Override public void putDecimal(int rowId, BigDecimal value, int precision) {
0:       Decimal toDecimal = org.apache.spark.sql.types.Decimal.apply(value);
0:       columnVector.putDecimal(counter++, toDecimal, precision);
1:   @Override public void putDecimals(int rowId, int count, BigDecimal value, int precision) {
0:         Decimal toDecimal = org.apache.spark.sql.types.Decimal.apply(value);
0:         columnVector.putDecimal(counter++, toDecimal, precision);
/////////////////////////////////////////////////////////////////////////
0:     return dataType;
author:QiangCai
-------------------------------------------------------------------------------
commit:9f94529
/////////////////////////////////////////////////////////////////////////
0: import org.apache.spark.sql.types.DataType;
/////////////////////////////////////////////////////////////////////////
1:   @Override public void putBoolean(int rowId, boolean value) {
0:     columnVector.putBoolean(rowId, value);
1:   }
1: 
1:   @Override public void putFloat(int rowId, float value) {
0:     columnVector.putFloat(rowId, value);
1:   }
1: 
/////////////////////////////////////////////////////////////////////////
0: 
1:   @Override public DataType getType() {
0:     return columnVector.dataType();
0:   }
commit:41347d8
/////////////////////////////////////////////////////////////////////////
0: 
author:kunal642
-------------------------------------------------------------------------------
commit:7e2e0f3
/////////////////////////////////////////////////////////////////////////
0:       putDecimal(rowId++, value, precision);
/////////////////////////////////////////////////////////////////////////
0:       putBytes(rowId++, value);
author:jackylk
-------------------------------------------------------------------------------
commit:ce09aaa
/////////////////////////////////////////////////////////////////////////
1: import org.apache.carbondata.core.scan.result.vector.CarbonColumnVector;
commit:6fee993
/////////////////////////////////////////////////////////////////////////
1: class ColumnarVectorWrapper implements CarbonColumnVector {
============================================================================