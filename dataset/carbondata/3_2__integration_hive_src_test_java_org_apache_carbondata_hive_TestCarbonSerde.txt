1:53267c8: ///*
1:53267c8: // * Licensed to the Apache Software Foundation (ASF) under one or more
1:53267c8: // * contributor license agreements.  See the NOTICE file distributed with
1:53267c8: // * this work for additional information regarding copyright ownership.
1:53267c8: // * The ASF licenses this file to You under the Apache License, Version 2.0
1:53267c8: // * (the "License"); you may not use this file except in compliance with
1:53267c8: // * the License.  You may obtain a copy of the License at
1:53267c8: // *
1:53267c8: // *    http://www.apache.org/licenses/LICENSE-2.0
1:53267c8: // *
1:53267c8: // * Unless required by applicable law or agreed to in writing, software
1:53267c8: // * distributed under the License is distributed on an "AS IS" BASIS,
1:53267c8: // * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
1:53267c8: // * See the License for the specific language governing permissions and
1:53267c8: // * limitations under the License.
1:53267c8: // */
1:53267c8: //package org.apache.carbondata.hive;
1:53267c8: //
1:53267c8: //import junit.framework.TestCase;
1:53267c8: //import org.apache.hadoop.conf.Configuration;
1:53267c8: //import org.apache.hadoop.hive.common.type.HiveDecimal;
1:53267c8: //import org.apache.hadoop.hive.serde2.SerDeException;
1:53267c8: //import org.apache.hadoop.hive.serde2.SerDeUtils;
1:53267c8: //import org.apache.hadoop.hive.serde2.io.DoubleWritable;
1:53267c8: //import org.apache.hadoop.hive.serde2.io.HiveDecimalWritable;
1:53267c8: //import org.apache.hadoop.hive.serde2.io.ShortWritable;
1:53267c8: //import org.apache.hadoop.hive.serde2.objectinspector.StructObjectInspector;
1:53267c8: //import org.apache.hadoop.io.*;
1:53267c8: //import org.junit.Test;
1:53267c8: //
1:53267c8: //import java.util.Properties;
1:53267c8: //
1:53267c8: //public class TestCarbonSerde extends TestCase {
1:53267c8: //  @Test
1:53267c8: //  public void testCarbonHiveSerDe() throws Throwable {
1:53267c8: //    try {
1:53267c8: //      // Create the SerDe
1:53267c8: //      System.out.println("test: testCarbonHiveSerDe");
1:53267c8: //
1:53267c8: //      final CarbonHiveSerDe serDe = new CarbonHiveSerDe();
1:53267c8: //      final Configuration conf = new Configuration();
1:53267c8: //      final Properties tbl = createProperties();
1:53267c8: //      SerDeUtils.initializeSerDe(serDe, conf, tbl, null);
1:53267c8: //
1:53267c8: //      // Data
1:53267c8: //      final Writable[] arr = new Writable[7];
1:53267c8: //
1:53267c8: //      //primitive types
1:53267c8: //      arr[0] = new ShortWritable((short) 456);
1:53267c8: //      arr[1] = new IntWritable(789);
1:53267c8: //      arr[2] = new LongWritable(1000l);
1:53267c8: //      arr[3] = new DoubleWritable((double) 5.3);
1:53267c8: //      arr[4] = new HiveDecimalWritable(HiveDecimal.create(1));
1:53267c8: //      arr[5] = new Text("carbonSerde binary".getBytes("UTF-8"));
1:53267c8: //
1:53267c8: //      final Writable[] arrayContainer = new Writable[1];
1:53267c8: //      final Writable[] array = new Writable[5];
1:53267c8: //      for (int i = 0; i < 5; ++i) {
1:53267c8: //        array[i] = new IntWritable(i);
1:53267c8: //      }
1:53267c8: //      arrayContainer[0] = new ArrayWritable(Writable.class, array);
1:53267c8: //      arr[6] = new ArrayWritable(Writable.class, arrayContainer);
1:53267c8: //
1:53267c8: //      final ArrayWritable arrWritable = new ArrayWritable(Writable.class, arr);
1:53267c8: //      // Test
1:53267c8: //      deserializeAndSerializeLazySimple(serDe, arrWritable);
1:53267c8: //      System.out.println("test: testCarbonHiveSerDe - OK");
1:53267c8: //
1:53267c8: //    } catch (final Throwable e) {
1:53267c8: //      e.printStackTrace();
1:53267c8: //      throw e;
1:53267c8: //    }
1:53267c8: //  }
1:53267c8: //
1:53267c8: //  private void deserializeAndSerializeLazySimple(final CarbonHiveSerDe serDe,
1:53267c8: //      final ArrayWritable t) throws SerDeException {
1:53267c8: //
1:53267c8: //    // Get the row structure
1:53267c8: //    final StructObjectInspector oi = (StructObjectInspector) serDe.getObjectInspector();
1:53267c8: //
1:53267c8: //    // Deserialize
1:53267c8: //    final Object row = serDe.deserialize(t);
1:53267c8: //    assertEquals("deserialization gives the wrong object class", row.getClass(),
1:53267c8: //        ArrayWritable.class);
1:53267c8: //    assertEquals("size correct after deserialization",
1:53267c8: //        serDe.getSerDeStats().getRawDataSize(), t.get().length);
1:53267c8: //    assertEquals("deserialization gives the wrong object", t, row);
1:53267c8: //
1:53267c8: //    // Serialize
1:bc3e684: //    final ArrayWritable serializedArr = (ArrayWritable) serDe.serializeStartKey(row, oi);
1:53267c8: //    assertEquals("size correct after serialization", serDe.getSerDeStats().getRawDataSize(),
1:53267c8: //        serializedArr.get().length);
1:53267c8: //    assertTrue("serialized object should be equal to starting object",
1:53267c8: //        arrayWritableEquals(t, serializedArr));
1:53267c8: //  }
1:53267c8: //
1:53267c8: //  private Properties createProperties() {
1:53267c8: //    final Properties tbl = new Properties();
1:53267c8: //
1:53267c8: //    // Set the configuration parameters
1:53267c8: //    tbl.setProperty("columns", "ashort,aint,along,adouble,adecimal,astring,alist");
1:53267c8: //    tbl.setProperty("columns.types",
1:53267c8: //        "smallint:int:bigint:double:decimal:string:array<int>");
1:53267c8: //    tbl.setProperty(org.apache.hadoop.hive.serde.serdeConstants.SERIALIZATION_NULL_FORMAT, "NULL");
1:53267c8: //    return tbl;
1:53267c8: //  }
1:53267c8: //
1:53267c8: //  public static boolean arrayWritableEquals(final ArrayWritable a1, final ArrayWritable a2) {
1:53267c8: //    final Writable[] a1Arr = a1.get();
1:53267c8: //    final Writable[] a2Arr = a2.get();
1:53267c8: //
1:53267c8: //    if (a1Arr.length != a2Arr.length) {
1:53267c8: //      return false;
1:53267c8: //    }
1:53267c8: //
1:53267c8: //    for (int i = 0; i < a1Arr.length; ++i) {
1:53267c8: //      if (a1Arr[i] instanceof ArrayWritable) {
1:53267c8: //        if (!(a2Arr[i] instanceof ArrayWritable)) {
1:53267c8: //          return false;
1:53267c8: //        }
1:53267c8: //        if (!arrayWritableEquals((ArrayWritable) a1Arr[i], (ArrayWritable) a2Arr[i])) {
1:53267c8: //          return false;
1:53267c8: //        }
1:53267c8: //      } else {
1:53267c8: //        if (!a1Arr[i].equals(a2Arr[i])) {
1:53267c8: //          return false;
1:53267c8: //        }
1:53267c8: //      }
1:53267c8: //
1:53267c8: //    }
1:53267c8: //    return true;
1:53267c8: //  }
1:53267c8: //}
============================================================================
author:jackylk
-------------------------------------------------------------------------------
commit:bc3e684
/////////////////////////////////////////////////////////////////////////
1: //    final ArrayWritable serializedArr = (ArrayWritable) serDe.serializeStartKey(row, oi);
author:chenliang613
-------------------------------------------------------------------------------
commit:53267c8
/////////////////////////////////////////////////////////////////////////
1: ///*
1: // * Licensed to the Apache Software Foundation (ASF) under one or more
1: // * contributor license agreements.  See the NOTICE file distributed with
1: // * this work for additional information regarding copyright ownership.
1: // * The ASF licenses this file to You under the Apache License, Version 2.0
1: // * (the "License"); you may not use this file except in compliance with
1: // * the License.  You may obtain a copy of the License at
1: // *
1: // *    http://www.apache.org/licenses/LICENSE-2.0
1: // *
1: // * Unless required by applicable law or agreed to in writing, software
1: // * distributed under the License is distributed on an "AS IS" BASIS,
1: // * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
1: // * See the License for the specific language governing permissions and
1: // * limitations under the License.
1: // */
1: //package org.apache.carbondata.hive;
1: //
1: //import junit.framework.TestCase;
1: //import org.apache.hadoop.conf.Configuration;
1: //import org.apache.hadoop.hive.common.type.HiveDecimal;
1: //import org.apache.hadoop.hive.serde2.SerDeException;
1: //import org.apache.hadoop.hive.serde2.SerDeUtils;
1: //import org.apache.hadoop.hive.serde2.io.DoubleWritable;
1: //import org.apache.hadoop.hive.serde2.io.HiveDecimalWritable;
1: //import org.apache.hadoop.hive.serde2.io.ShortWritable;
1: //import org.apache.hadoop.hive.serde2.objectinspector.StructObjectInspector;
1: //import org.apache.hadoop.io.*;
1: //import org.junit.Test;
1: //
1: //import java.util.Properties;
1: //
1: //public class TestCarbonSerde extends TestCase {
1: //  @Test
1: //  public void testCarbonHiveSerDe() throws Throwable {
1: //    try {
1: //      // Create the SerDe
1: //      System.out.println("test: testCarbonHiveSerDe");
1: //
1: //      final CarbonHiveSerDe serDe = new CarbonHiveSerDe();
1: //      final Configuration conf = new Configuration();
1: //      final Properties tbl = createProperties();
1: //      SerDeUtils.initializeSerDe(serDe, conf, tbl, null);
1: //
1: //      // Data
1: //      final Writable[] arr = new Writable[7];
1: //
1: //      //primitive types
1: //      arr[0] = new ShortWritable((short) 456);
1: //      arr[1] = new IntWritable(789);
1: //      arr[2] = new LongWritable(1000l);
1: //      arr[3] = new DoubleWritable((double) 5.3);
1: //      arr[4] = new HiveDecimalWritable(HiveDecimal.create(1));
1: //      arr[5] = new Text("carbonSerde binary".getBytes("UTF-8"));
1: //
1: //      final Writable[] arrayContainer = new Writable[1];
1: //      final Writable[] array = new Writable[5];
1: //      for (int i = 0; i < 5; ++i) {
1: //        array[i] = new IntWritable(i);
1: //      }
1: //      arrayContainer[0] = new ArrayWritable(Writable.class, array);
1: //      arr[6] = new ArrayWritable(Writable.class, arrayContainer);
1: //
1: //      final ArrayWritable arrWritable = new ArrayWritable(Writable.class, arr);
1: //      // Test
1: //      deserializeAndSerializeLazySimple(serDe, arrWritable);
1: //      System.out.println("test: testCarbonHiveSerDe - OK");
1: //
1: //    } catch (final Throwable e) {
1: //      e.printStackTrace();
1: //      throw e;
1: //    }
1: //  }
1: //
1: //  private void deserializeAndSerializeLazySimple(final CarbonHiveSerDe serDe,
1: //      final ArrayWritable t) throws SerDeException {
1: //
1: //    // Get the row structure
1: //    final StructObjectInspector oi = (StructObjectInspector) serDe.getObjectInspector();
1: //
1: //    // Deserialize
1: //    final Object row = serDe.deserialize(t);
1: //    assertEquals("deserialization gives the wrong object class", row.getClass(),
1: //        ArrayWritable.class);
1: //    assertEquals("size correct after deserialization",
1: //        serDe.getSerDeStats().getRawDataSize(), t.get().length);
1: //    assertEquals("deserialization gives the wrong object", t, row);
1: //
1: //    // Serialize
0: //    final ArrayWritable serializedArr = (ArrayWritable) serDe.serialize(row, oi);
1: //    assertEquals("size correct after serialization", serDe.getSerDeStats().getRawDataSize(),
1: //        serializedArr.get().length);
1: //    assertTrue("serialized object should be equal to starting object",
1: //        arrayWritableEquals(t, serializedArr));
1: //  }
1: //
1: //  private Properties createProperties() {
1: //    final Properties tbl = new Properties();
1: //
1: //    // Set the configuration parameters
1: //    tbl.setProperty("columns", "ashort,aint,along,adouble,adecimal,astring,alist");
1: //    tbl.setProperty("columns.types",
1: //        "smallint:int:bigint:double:decimal:string:array<int>");
1: //    tbl.setProperty(org.apache.hadoop.hive.serde.serdeConstants.SERIALIZATION_NULL_FORMAT, "NULL");
1: //    return tbl;
1: //  }
1: //
1: //  public static boolean arrayWritableEquals(final ArrayWritable a1, final ArrayWritable a2) {
1: //    final Writable[] a1Arr = a1.get();
1: //    final Writable[] a2Arr = a2.get();
1: //
1: //    if (a1Arr.length != a2Arr.length) {
1: //      return false;
1: //    }
1: //
1: //    for (int i = 0; i < a1Arr.length; ++i) {
1: //      if (a1Arr[i] instanceof ArrayWritable) {
1: //        if (!(a2Arr[i] instanceof ArrayWritable)) {
1: //          return false;
1: //        }
1: //        if (!arrayWritableEquals((ArrayWritable) a1Arr[i], (ArrayWritable) a2Arr[i])) {
1: //          return false;
1: //        }
1: //      } else {
1: //        if (!a1Arr[i].equals(a2Arr[i])) {
1: //          return false;
1: //        }
1: //      }
1: //
1: //    }
1: //    return true;
1: //  }
1: //}
author:cenyuhai
-------------------------------------------------------------------------------
commit:a700f83
/////////////////////////////////////////////////////////////////////////
0: /*
0:  * Licensed to the Apache Software Foundation (ASF) under one or more
0:  * contributor license agreements.  See the NOTICE file distributed with
0:  * this work for additional information regarding copyright ownership.
0:  * The ASF licenses this file to You under the Apache License, Version 2.0
0:  * (the "License"); you may not use this file except in compliance with
0:  * the License.  You may obtain a copy of the License at
0:  *
0:  *    http://www.apache.org/licenses/LICENSE-2.0
0:  *
0:  * Unless required by applicable law or agreed to in writing, software
0:  * distributed under the License is distributed on an "AS IS" BASIS,
0:  * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
0:  * See the License for the specific language governing permissions and
0:  * limitations under the License.
0:  */
0: package org.apache.carbondata.hive;
0: 
0: import junit.framework.TestCase;
0: import org.apache.hadoop.conf.Configuration;
0: import org.apache.hadoop.hive.common.type.HiveDecimal;
0: import org.apache.hadoop.hive.serde2.SerDeException;
0: import org.apache.hadoop.hive.serde2.SerDeUtils;
0: import org.apache.hadoop.hive.serde2.io.DoubleWritable;
0: import org.apache.hadoop.hive.serde2.io.HiveDecimalWritable;
0: import org.apache.hadoop.hive.serde2.io.ShortWritable;
0: import org.apache.hadoop.hive.serde2.objectinspector.StructObjectInspector;
0: import org.apache.hadoop.io.*;
0: import org.junit.Test;
0: 
0: import java.util.Properties;
0: 
0: public class TestCarbonSerde extends TestCase {
0:   @Test
0:   public void testCarbonHiveSerDe() throws Throwable {
0:     try {
0:       // Create the SerDe
0:       System.out.println("test: testCarbonHiveSerDe");
0: 
0:       final CarbonHiveSerDe serDe = new CarbonHiveSerDe();
0:       final Configuration conf = new Configuration();
0:       final Properties tbl = createProperties();
0:       SerDeUtils.initializeSerDe(serDe, conf, tbl, null);
0: 
0:       // Data
0:       final Writable[] arr = new Writable[7];
0: 
0:       //primitive types
0:       arr[0] = new ShortWritable((short) 456);
0:       arr[1] = new IntWritable(789);
0:       arr[2] = new LongWritable(1000l);
0:       arr[3] = new DoubleWritable((double) 5.3);
0:       arr[4] = new HiveDecimalWritable(HiveDecimal.create(1));
0:       arr[5] = new Text("carbonSerde binary".getBytes("UTF-8"));
0: 
0:       final Writable[] arrayContainer = new Writable[1];
0:       final Writable[] array = new Writable[5];
0:       for (int i = 0; i < 5; ++i) {
0:         array[i] = new IntWritable(i);
0:       }
0:       arrayContainer[0] = new ArrayWritable(Writable.class, array);
0:       arr[6] = new ArrayWritable(Writable.class, arrayContainer);
0: 
0:       final ArrayWritable arrWritable = new ArrayWritable(Writable.class, arr);
0:       // Test
0:       deserializeAndSerializeLazySimple(serDe, arrWritable);
0:       System.out.println("test: testCarbonHiveSerDe - OK");
0: 
0:     } catch (final Throwable e) {
0:       e.printStackTrace();
0:       throw e;
0:     }
0:   }
0: 
0:   private void deserializeAndSerializeLazySimple(final CarbonHiveSerDe serDe,
0:       final ArrayWritable t) throws SerDeException {
0: 
0:     // Get the row structure
0:     final StructObjectInspector oi = (StructObjectInspector) serDe.getObjectInspector();
0: 
0:     // Deserialize
0:     final Object row = serDe.deserialize(t);
0:     assertEquals("deserialization gives the wrong object class", row.getClass(),
0:         ArrayWritable.class);
0:     assertEquals("size correct after deserialization",
0:         serDe.getSerDeStats().getRawDataSize(), t.get().length);
0:     assertEquals("deserialization gives the wrong object", t, row);
0: 
0:     // Serialize
0:     final ArrayWritable serializedArr = (ArrayWritable) serDe.serialize(row, oi);
0:     assertEquals("size correct after serialization", serDe.getSerDeStats().getRawDataSize(),
0:         serializedArr.get().length);
0:     assertTrue("serialized object should be equal to starting object",
0:         arrayWritableEquals(t, serializedArr));
0:   }
0: 
0:   private Properties createProperties() {
0:     final Properties tbl = new Properties();
0: 
0:     // Set the configuration parameters
0:     tbl.setProperty("columns", "ashort,aint,along,adouble,adecimal,astring,alist");
0:     tbl.setProperty("columns.types",
0:         "smallint:int:bigint:double:decimal:string:array<int>");
0:     tbl.setProperty(org.apache.hadoop.hive.serde.serdeConstants.SERIALIZATION_NULL_FORMAT, "NULL");
0:     return tbl;
0:   }
0: 
0:   public static boolean arrayWritableEquals(final ArrayWritable a1, final ArrayWritable a2) {
0:     final Writable[] a1Arr = a1.get();
0:     final Writable[] a2Arr = a2.get();
0: 
0:     if (a1Arr.length != a2Arr.length) {
0:       return false;
0:     }
0: 
0:     for (int i = 0; i < a1Arr.length; ++i) {
0:       if (a1Arr[i] instanceof ArrayWritable) {
0:         if (!(a2Arr[i] instanceof ArrayWritable)) {
0:           return false;
0:         }
0:         if (!arrayWritableEquals((ArrayWritable) a1Arr[i], (ArrayWritable) a2Arr[i])) {
0:           return false;
0:         }
0:       } else {
0:         if (!a1Arr[i].equals(a2Arr[i])) {
0:           return false;
0:         }
0:       }
0: 
0:     }
0:     return true;
0:   }
0: }
============================================================================