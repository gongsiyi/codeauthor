1:2cf1104: /*
1:2cf1104:  * Licensed to the Apache Software Foundation (ASF) under one or more
1:2cf1104:  * contributor license agreements.  See the NOTICE file distributed with
1:2cf1104:  * this work for additional information regarding copyright ownership.
1:2cf1104:  * The ASF licenses this file to You under the Apache License, Version 2.0
1:2cf1104:  * (the "License"); you may not use this file except in compliance with
1:2cf1104:  * the License.  You may obtain a copy of the License at
1:2cf1104:  *
1:2cf1104:  *    http://www.apache.org/licenses/LICENSE-2.0
1:2cf1104:  *
1:2cf1104:  * Unless required by applicable law or agreed to in writing, software
1:2cf1104:  * distributed under the License is distributed on an "AS IS" BASIS,
1:2cf1104:  * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
1:2cf1104:  * See the License for the specific language governing permissions and
1:2cf1104:  * limitations under the License.
1:2cf1104:  */
1:2cf1104: package org.apache.carbondata.core.datastore.chunk.reader.measure.v3;
1:dc83b2a: 
1:2cf1104: import java.io.IOException;
1:2cf1104: import java.nio.ByteBuffer;
1:7422690: import java.util.List;
1:7422690: 
1:daa6465: import org.apache.carbondata.core.datastore.FileReader;
1:2cf1104: import org.apache.carbondata.core.datastore.chunk.impl.MeasureRawColumnChunk;
1:2cf1104: import org.apache.carbondata.core.datastore.chunk.reader.measure.AbstractMeasureChunkReaderV2V3Format;
1:8f08c4a: import org.apache.carbondata.core.datastore.compression.CompressorFactory;
1:edda248: import org.apache.carbondata.core.datastore.page.ColumnPage;
1:e6a4f64: import org.apache.carbondata.core.datastore.page.encoding.ColumnPageDecoder;
1:7359601: import org.apache.carbondata.core.memory.MemoryException;
1:2cf1104: import org.apache.carbondata.core.metadata.blocklet.BlockletInfo;
1:438b442: import org.apache.carbondata.core.scan.executor.util.QueryUtil;
1:8f08c4a: import org.apache.carbondata.core.util.CarbonMetadataUtil;
1:2cf1104: import org.apache.carbondata.core.util.CarbonUtil;
1:2cf1104: import org.apache.carbondata.format.DataChunk2;
1:2cf1104: import org.apache.carbondata.format.DataChunk3;
1:e6a4f64: import org.apache.carbondata.format.Encoding;
6:2cf1104: 
1:2cf1104: import org.apache.commons.lang.ArrayUtils;
1:2cf1104: 
1:2cf1104: /**
1:2cf1104:  * Measure column V3 Reader class which will be used to read and uncompress
1:2cf1104:  * V3 format data
1:2cf1104:  * data format
1:2cf1104:  * Data Format
1:b41e48f:  * <FileHeader>
1:2cf1104:  * <Column1 Data ChunkV3><Column1<Page1><Page2><Page3><Page4>>
1:2cf1104:  * <Column2 Data ChunkV3><Column2<Page1><Page2><Page3><Page4>>
1:2cf1104:  * <Column3 Data ChunkV3><Column3<Page1><Page2><Page3><Page4>>
1:2cf1104:  * <Column4 Data ChunkV3><Column4<Page1><Page2><Page3><Page4>>
1:b41e48f:  * <File Footer>
1:2cf1104:  */
1:2cf1104: public class CompressedMeasureChunkFileBasedReaderV3 extends AbstractMeasureChunkReaderV2V3Format {
1:2cf1104: 
1:2cf1104:   /**
1:2cf1104:    * end position of last measure in carbon data file
1:2cf1104:    */
1:2cf1104:   private long measureOffsets;
1:2cf1104: 
1:2cf1104:   public CompressedMeasureChunkFileBasedReaderV3(BlockletInfo blockletInfo, String filePath) {
1:2cf1104:     super(blockletInfo, filePath);
1:2cf1104:     measureOffsets = blockletInfo.getMeasureOffsets();
1:7422690:   }
1:bc3e684: 
1:2cf1104:   /**
1:2cf1104:    * Below method will be used to read the measure column data form carbon data file
1:2cf1104:    * 1. Get the length of the data to be read
1:2cf1104:    * 2. Allocate the direct buffer
1:2cf1104:    * 3. read the data from file
1:2cf1104:    * 4. Get the data chunk object from data read
1:2cf1104:    * 5. Create the raw chunk object and fill the details
1:2cf1104:    *
1:2cf1104:    * @param fileReader          reader for reading the column from carbon data file
1:8c1ddbf:    * @param columnIndex         column to be read
1:2cf1104:    * @return measure raw chunk
1:2cf1104:    */
1:daa6465:   @Override public MeasureRawColumnChunk readRawMeasureChunk(FileReader fileReader,
1:8c1ddbf:       int columnIndex) throws IOException {
1:2cf1104:     int dataLength = 0;
1:2cf1104:     // to calculate the length of the data to be read
1:2cf1104:     // column other than last column we can subtract the offset of current column with
1:2cf1104:     // next column and get the total length.
1:2cf1104:     // but for last column we need to use lastDimensionOffset which is the end position
1:e8da880:     // of the last dimension, we can subtract current dimension offset from lastDimensionOffset
1:8c1ddbf:     if (measureColumnChunkOffsets.size() - 1 == columnIndex) {
1:8c1ddbf:       dataLength = (int) (measureOffsets - measureColumnChunkOffsets.get(columnIndex));
1:2cf1104:     } else {
1:8f59a32:       dataLength =
1:8c1ddbf:           (int) (measureColumnChunkOffsets.get(columnIndex + 1) - measureColumnChunkOffsets
1:8c1ddbf:               .get(columnIndex));
1:bc3e684:     }
1:8f59a32:     ByteBuffer buffer = null;
1:2cf1104:     // read the data from carbon data file
1:2cf1104:     synchronized (fileReader) {
1:8f59a32:       buffer = fileReader
1:8c1ddbf:           .readByteBuffer(filePath, measureColumnChunkOffsets.get(columnIndex), dataLength);
1:dc83b2a:     }
1:2cf1104:     // get the data chunk which will have all the details about the data pages
1:2cf1104:     DataChunk3 dataChunk =
1:8c1ddbf:         CarbonUtil.readDataChunk3(buffer, 0, measureColumnChunkLength.get(columnIndex));
1:d509f17: 
1:d509f17:     return getMeasureRawColumnChunk(fileReader, columnIndex, 0,  dataLength, buffer,
1:d509f17:         dataChunk);
1:d509f17:   }
1:d509f17: 
1:daa6465:   MeasureRawColumnChunk getMeasureRawColumnChunk(FileReader fileReader, int columnIndex,
1:daa6465:       long offset, int dataLength, ByteBuffer buffer, DataChunk3 dataChunk) {
1:2cf1104:     // creating a raw chunks instance and filling all the details
1:2cf1104:     MeasureRawColumnChunk rawColumnChunk =
1:d509f17:         new MeasureRawColumnChunk(columnIndex, buffer, offset, dataLength, this);
2:2cf1104:     int numberOfPages = dataChunk.getPage_length().size();
2:2cf1104:     byte[][] maxValueOfEachPage = new byte[numberOfPages][];
2:2cf1104:     byte[][] minValueOfEachPage = new byte[numberOfPages][];
2:2cf1104:     int[] eachPageLength = new int[numberOfPages];
1:2cf1104:     for (int i = 0; i < minValueOfEachPage.length; i++) {
1:2cf1104:       maxValueOfEachPage[i] =
1:2cf1104:           dataChunk.getData_chunk_list().get(i).getMin_max().getMax_values().get(0).array();
1:2cf1104:       minValueOfEachPage[i] =
1:2cf1104:           dataChunk.getData_chunk_list().get(i).getMin_max().getMin_values().get(0).array();
1:2cf1104:       eachPageLength[i] = dataChunk.getData_chunk_list().get(i).getNumberOfRowsInpage();
5:2cf1104:     }
1:2cf1104:     rawColumnChunk.setDataChunkV3(dataChunk);
1:2cf1104:     rawColumnChunk.setFileReader(fileReader);
1:2cf1104:     rawColumnChunk.setPagesCount(dataChunk.getPage_length().size());
1:2cf1104:     rawColumnChunk.setMaxValues(maxValueOfEachPage);
1:2cf1104:     rawColumnChunk.setMinValues(minValueOfEachPage);
1:2cf1104:     rawColumnChunk.setRowCount(eachPageLength);
1:2cf1104:     rawColumnChunk.setOffsets(ArrayUtils
2:2cf1104:         .toPrimitive(dataChunk.page_offset.toArray(new Integer[dataChunk.page_offset.size()])));
1:2cf1104:     return rawColumnChunk;
1:2cf1104:   }
1:bc3e684: 
1:2cf1104:   /**
1:2cf1104:    * Below method will be used to read the multiple measure column data in group
1:2cf1104:    * and divide into measure raw chunk object
1:2cf1104:    * Steps for reading
1:2cf1104:    * 1. Get the length of the data to be read
1:2cf1104:    * 2. Allocate the direct buffer
1:2cf1104:    * 3. read the data from file
1:2cf1104:    * 4. Get the data chunk object from file for each column
1:2cf1104:    * 5. Create the raw chunk object and fill the details for each column
1:2cf1104:    * 6. increment the offset of the data
1:2cf1104:    *
1:2cf1104:    * @param fileReader
1:2cf1104:    *        reader which will be used to read the measure columns data from file
1:daa6465:    * @param startColumnIndex
1:daa6465:    *        column index of the first measure column
1:daa6465:    * @param endColumnIndex
1:daa6465:    *        column index of the last measure column
1:2cf1104:    * @return MeasureRawColumnChunk array
1:2cf1104:    */
1:daa6465:   protected MeasureRawColumnChunk[] readRawMeasureChunksInGroup(FileReader fileReader,
1:daa6465:       int startColumnIndex, int endColumnIndex) throws IOException {
1:2cf1104:     // to calculate the length of the data to be read
1:2cf1104:     // column we can subtract the offset of start column offset with
1:2cf1104:     // end column+1 offset and get the total length.
1:daa6465:     long currentMeasureOffset = measureColumnChunkOffsets.get(startColumnIndex);
1:8f59a32:     ByteBuffer buffer = null;
1:2cf1104:     // read the data from carbon data file
1:2cf1104:     synchronized (fileReader) {
1:8f59a32:       buffer = fileReader.readByteBuffer(filePath, currentMeasureOffset,
1:daa6465:           (int) (measureColumnChunkOffsets.get(endColumnIndex + 1) - currentMeasureOffset));
1:2cf1104:     }
1:2cf1104:     // create raw chunk for each measure column
1:2cf1104:     MeasureRawColumnChunk[] measureDataChunk =
1:daa6465:         new MeasureRawColumnChunk[endColumnIndex - startColumnIndex + 1];
1:2cf1104:     int runningLength = 0;
1:2cf1104:     int index = 0;
1:daa6465:     for (int i = startColumnIndex; i <= endColumnIndex; i++) {
1:2cf1104:       int currentLength =
1:2cf1104:           (int) (measureColumnChunkOffsets.get(i + 1) - measureColumnChunkOffsets.get(i));
1:2cf1104:       DataChunk3 dataChunk =
1:2cf1104:           CarbonUtil.readDataChunk3(buffer, runningLength, measureColumnChunkLength.get(i));
1:d509f17:       MeasureRawColumnChunk measureRawColumnChunk =
1:d509f17:           getMeasureRawColumnChunk(fileReader, i, runningLength, currentLength, buffer, dataChunk);
1:2cf1104:       measureDataChunk[index] = measureRawColumnChunk;
1:2cf1104:       runningLength += currentLength;
1:2cf1104:       index++;
1:2cf1104:     }
1:2cf1104:     return measureDataChunk;
1:2cf1104:   }
1:dc83b2a: 
1:2cf1104:   /**
1:2cf1104:    * Below method will be used to convert the compressed measure chunk raw data to actual data
1:2cf1104:    *
1:daa6465:    * @param rawColumnChunk measure raw chunk
1:086b06d:    * @param pageNumber            number
1:daa6465:    * @return DimensionColumnPage
1:2cf1104:    */
1:7359601:   @Override
1:daa6465:   public ColumnPage decodeColumnPage(
1:daa6465:       MeasureRawColumnChunk rawColumnChunk, int pageNumber)
1:7359601:       throws IOException, MemoryException {
1:2cf1104:     // data chunk of blocklet column
1:daa6465:     DataChunk3 dataChunk3 = rawColumnChunk.getDataChunkV3();
1:2cf1104:     // data chunk of page
1:e6a4f64:     DataChunk2 pageMetadata = dataChunk3.getData_chunk_list().get(pageNumber);
1:8f08c4a:     String compressorName = CarbonMetadataUtil.getCompressorNameFromChunkMeta(
1:8f08c4a:         pageMetadata.getChunk_meta());
1:8f08c4a:     this.compressor = CompressorFactory.getInstance().getCompressor(compressorName);
1:2cf1104:     // calculating the start point of data
1:2cf1104:     // as buffer can contain multiple column data, start point will be datachunkoffset +
1:2cf1104:     // data chunk length + page offset
1:daa6465:     int offset = (int) rawColumnChunk.getOffSet() +
1:daa6465:         measureColumnChunkLength.get(rawColumnChunk.getColumnIndex()) +
1:e6a4f64:         dataChunk3.getPage_offset().get(pageNumber);
1:daa6465:     ColumnPage decodedPage = decodeMeasure(pageMetadata, rawColumnChunk.getRawData(), offset);
1:8f08c4a:     decodedPage.setNullBits(QueryUtil.getNullBitSet(pageMetadata.presence, this.compressor));
1:e6a4f64:     return decodedPage;
1:2cf1104:   }
1:edda248: 
1:e6a4f64:   /**
1:e6a4f64:    * Decode measure column page with page header and raw data starting from offset
1:e6a4f64:    */
1:d509f17:   protected ColumnPage decodeMeasure(DataChunk2 pageMetadata, ByteBuffer pageData, int offset)
1:e6a4f64:       throws MemoryException, IOException {
1:e6a4f64:     List<Encoding> encodings = pageMetadata.getEncoders();
1:e6a4f64:     List<ByteBuffer> encoderMetas = pageMetadata.getEncoder_meta();
1:8f08c4a:     String compressorName = CarbonMetadataUtil.getCompressorNameFromChunkMeta(
1:8f08c4a:         pageMetadata.getChunk_meta());
1:8f08c4a:     ColumnPageDecoder codec = encodingFactory.createDecoder(encodings, encoderMetas,
1:8f08c4a:         compressorName);
1:e6a4f64:     return codec.decode(pageData.array(), offset, pageMetadata.data_page_length);
1:2cf1104:   }
1:2cf1104: 
1:2cf1104: }
============================================================================
author:xuchuanyin
-------------------------------------------------------------------------------
commit:8f08c4a
/////////////////////////////////////////////////////////////////////////
1: import org.apache.carbondata.core.datastore.compression.CompressorFactory;
1: import org.apache.carbondata.core.util.CarbonMetadataUtil;
/////////////////////////////////////////////////////////////////////////
1:     String compressorName = CarbonMetadataUtil.getCompressorNameFromChunkMeta(
1:         pageMetadata.getChunk_meta());
1:     this.compressor = CompressorFactory.getInstance().getCompressor(compressorName);
/////////////////////////////////////////////////////////////////////////
1:     decodedPage.setNullBits(QueryUtil.getNullBitSet(pageMetadata.presence, this.compressor));
/////////////////////////////////////////////////////////////////////////
1:     String compressorName = CarbonMetadataUtil.getCompressorNameFromChunkMeta(
1:         pageMetadata.getChunk_meta());
1:     ColumnPageDecoder codec = encodingFactory.createDecoder(encodings, encoderMetas,
1:         compressorName);
author:ajantha-bhat
-------------------------------------------------------------------------------
commit:438b442
/////////////////////////////////////////////////////////////////////////
1: import org.apache.carbondata.core.scan.executor.util.QueryUtil;
/////////////////////////////////////////////////////////////////////////
0:     decodedPage.setNullBits(QueryUtil.getNullBitSet(pageMetadata.presence));
author:xubo245
-------------------------------------------------------------------------------
commit:e8da880
/////////////////////////////////////////////////////////////////////////
1:     // of the last dimension, we can subtract current dimension offset from lastDimensionOffset
author:Jacky Li
-------------------------------------------------------------------------------
commit:daa6465
/////////////////////////////////////////////////////////////////////////
1: import org.apache.carbondata.core.datastore.FileReader;
/////////////////////////////////////////////////////////////////////////
1:   @Override public MeasureRawColumnChunk readRawMeasureChunk(FileReader fileReader,
/////////////////////////////////////////////////////////////////////////
1:   MeasureRawColumnChunk getMeasureRawColumnChunk(FileReader fileReader, int columnIndex,
1:       long offset, int dataLength, ByteBuffer buffer, DataChunk3 dataChunk) {
/////////////////////////////////////////////////////////////////////////
1:    * @param startColumnIndex
1:    *        column index of the first measure column
1:    * @param endColumnIndex
1:    *        column index of the last measure column
1:   protected MeasureRawColumnChunk[] readRawMeasureChunksInGroup(FileReader fileReader,
1:       int startColumnIndex, int endColumnIndex) throws IOException {
1:     long currentMeasureOffset = measureColumnChunkOffsets.get(startColumnIndex);
1:           (int) (measureColumnChunkOffsets.get(endColumnIndex + 1) - currentMeasureOffset));
1:         new MeasureRawColumnChunk[endColumnIndex - startColumnIndex + 1];
1:     for (int i = startColumnIndex; i <= endColumnIndex; i++) {
/////////////////////////////////////////////////////////////////////////
1:    * @param rawColumnChunk measure raw chunk
1:    * @return DimensionColumnPage
1:   public ColumnPage decodeColumnPage(
1:       MeasureRawColumnChunk rawColumnChunk, int pageNumber)
1:     DataChunk3 dataChunk3 = rawColumnChunk.getDataChunkV3();
1:     int offset = (int) rawColumnChunk.getOffSet() +
1:         measureColumnChunkLength.get(rawColumnChunk.getColumnIndex()) +
1:     ColumnPage decodedPage = decodeMeasure(pageMetadata, rawColumnChunk.getRawData(), offset);
commit:8c1ddbf
/////////////////////////////////////////////////////////////////////////
1:    * @param columnIndex         column to be read
1:       int columnIndex) throws IOException {
1:     if (measureColumnChunkOffsets.size() - 1 == columnIndex) {
1:       dataLength = (int) (measureOffsets - measureColumnChunkOffsets.get(columnIndex));
1:           (int) (measureColumnChunkOffsets.get(columnIndex + 1) - measureColumnChunkOffsets
1:               .get(columnIndex));
1:           .readByteBuffer(filePath, measureColumnChunkOffsets.get(columnIndex), dataLength);
1:         CarbonUtil.readDataChunk3(buffer, 0, measureColumnChunkLength.get(columnIndex));
0:         new MeasureRawColumnChunk(columnIndex, buffer, 0, dataLength, this);
/////////////////////////////////////////////////////////////////////////
0:         measureColumnChunkLength.get(rawColumnPage.getColumnIndex()) +
/////////////////////////////////////////////////////////////////////////
0:     ColumnPageDecoder codec = encodingFactory.createDecoder(encodings, encoderMetas);
commit:e6a4f64
/////////////////////////////////////////////////////////////////////////
1: import org.apache.carbondata.core.datastore.page.encoding.ColumnPageDecoder;
1: import org.apache.carbondata.format.Encoding;
/////////////////////////////////////////////////////////////////////////
/////////////////////////////////////////////////////////////////////////
/////////////////////////////////////////////////////////////////////////
0:    * @param rawColumnPage measure raw chunk
0:   public ColumnPage convertToColumnPage(
0:       MeasureRawColumnChunk rawColumnPage, int pageNumber)
0:     DataChunk3 dataChunk3 = rawColumnPage.getDataChunkV3();
1:     DataChunk2 pageMetadata = dataChunk3.getData_chunk_list().get(pageNumber);
0:     int offset = rawColumnPage.getOffSet() +
0:         measureColumnChunkLength.get(rawColumnPage.getBlockletId()) +
1:         dataChunk3.getPage_offset().get(pageNumber);
0:     ColumnPage decodedPage = decodeMeasure(pageMetadata, rawColumnPage.getRawData(), offset);
0:     decodedPage.setNullBits(getNullBitSet(pageMetadata.presence));
1:     return decodedPage;
1:   /**
1:    * Decode measure column page with page header and raw data starting from offset
1:    */
0:   private ColumnPage decodeMeasure(DataChunk2 pageMetadata, ByteBuffer pageData, int offset)
1:       throws MemoryException, IOException {
1:     List<Encoding> encodings = pageMetadata.getEncoders();
1:     List<ByteBuffer> encoderMetas = pageMetadata.getEncoder_meta();
0:     ColumnPageDecoder codec = strategy.createDecoder(encodings, encoderMetas);
1:     return codec.decode(pageData.array(), offset, pageMetadata.data_page_length);
commit:379d4f6
/////////////////////////////////////////////////////////////////////////
0:       DataChunk2 measureColumnChunk, int copyPoint) throws MemoryException, IOException {
author:ravipesala
-------------------------------------------------------------------------------
commit:d509f17
/////////////////////////////////////////////////////////////////////////
1: 
1:     return getMeasureRawColumnChunk(fileReader, columnIndex, 0,  dataLength, buffer,
1:         dataChunk);
1:   }
1: 
0:   protected MeasureRawColumnChunk getMeasureRawColumnChunk(FileHolder fileReader,
0:       int columnIndex, long offset, int dataLength, ByteBuffer buffer,
0:       DataChunk3 dataChunk) {
1:         new MeasureRawColumnChunk(columnIndex, buffer, offset, dataLength, this);
/////////////////////////////////////////////////////////////////////////
1:       MeasureRawColumnChunk measureRawColumnChunk =
1:           getMeasureRawColumnChunk(fileReader, i, runningLength, currentLength, buffer, dataChunk);
/////////////////////////////////////////////////////////////////////////
0:     int offset = (int) rawColumnPage.getOffSet() +
/////////////////////////////////////////////////////////////////////////
1:   protected ColumnPage decodeMeasure(DataChunk2 pageMetadata, ByteBuffer pageData, int offset)
commit:8f59a32
/////////////////////////////////////////////////////////////////////////
0:   @Override public MeasureRawColumnChunk readRawMeasureChunk(FileHolder fileReader,
0:       int blockletColumnIndex) throws IOException {
0:     if (measureColumnChunkOffsets.size() - 1 == blockletColumnIndex) {
0:       dataLength = (int) (measureOffsets - measureColumnChunkOffsets.get(blockletColumnIndex));
1:       dataLength =
0:           (int) (measureColumnChunkOffsets.get(blockletColumnIndex + 1) - measureColumnChunkOffsets
0:               .get(blockletColumnIndex));
1:     ByteBuffer buffer = null;
1:       buffer = fileReader
0:           .readByteBuffer(filePath, measureColumnChunkOffsets.get(blockletColumnIndex), dataLength);
0:         CarbonUtil.readDataChunk3(buffer, 0, measureColumnChunkLength.get(blockletColumnIndex));
0:         new MeasureRawColumnChunk(blockletColumnIndex, buffer, 0, dataLength, this);
/////////////////////////////////////////////////////////////////////////
1:     ByteBuffer buffer = null;
1:       buffer = fileReader.readByteBuffer(filePath, currentMeasureOffset,
/////////////////////////////////////////////////////////////////////////
0:     values.uncompress(compressionModel.getConvertedDataType()[0], rawData.array(), copyPoint,
author:Ravindra Pesala
-------------------------------------------------------------------------------
commit:4e83509
/////////////////////////////////////////////////////////////////////////
0:     ColumnPageCodec codec = strategy.newCodec(meta);
author:Raghunandan S
-------------------------------------------------------------------------------
commit:7422690
/////////////////////////////////////////////////////////////////////////
1: import java.util.List;
/////////////////////////////////////////////////////////////////////////
0:     List<ByteBuffer> encoder_meta = measureColumnChunk.getEncoder_meta();
0:     assert (encoder_meta.size() > 0);
0:     byte[] encodedMeta = encoder_meta.get(0).array();
1: 
0:     int scale = -1;
0:     int precision = -1;
0:     if (encoder_meta.size() > 1) {
0:       ByteBuffer decimalInfo = encoder_meta.get(1);
0:       scale = decimalInfo.getInt();
0:       precision = decimalInfo.getInt();
1:     }
0:     ColumnPageCodec codec = strategy.newCodec(meta, scale, precision);
author:jackylk
-------------------------------------------------------------------------------
commit:a5af0ff
/////////////////////////////////////////////////////////////////////////
0:     ColumnPageCodec codec = strategy.newCodec(meta);
commit:bc3e684
/////////////////////////////////////////////////////////////////////////
0: import org.apache.carbondata.core.datastore.page.encoding.ColumnPageCodec;
0: import org.apache.carbondata.core.metadata.ColumnPageCodecMeta;
/////////////////////////////////////////////////////////////////////////
0:   protected ColumnPage decodeMeasure(MeasureRawColumnChunk measureRawColumnChunk,
0:       DataChunk2 measureColumnChunk, int copyPoint) throws MemoryException {
0:     // for measure, it should have only one ValueEncoderMeta
0:     assert (measureColumnChunk.getEncoder_meta().size() == 1);
0:     byte[] encodedMeta = measureColumnChunk.getEncoder_meta().get(0).array();
1: 
0:     ColumnPageCodecMeta meta = new ColumnPageCodecMeta();
0:     meta.deserialize(encodedMeta);
0:     ColumnPageCodec codec = strategy.createCodec(meta);
0:     byte[] rawData = measureRawColumnChunk.getRawData().array();
0:     return codec.decode(rawData, copyPoint, measureColumnChunk.data_page_length);
1:   }
1: 
commit:7359601
/////////////////////////////////////////////////////////////////////////
1: import org.apache.carbondata.core.memory.MemoryException;
/////////////////////////////////////////////////////////////////////////
1:   @Override
0:   public MeasureColumnDataChunk convertToMeasureChunk(
0:       MeasureRawColumnChunk measureRawColumnChunk, int pageNumber)
1:       throws IOException, MemoryException {
commit:edda248
/////////////////////////////////////////////////////////////////////////
1: import org.apache.carbondata.core.datastore.page.ColumnPage;
/////////////////////////////////////////////////////////////////////////
0:     ColumnPage decodedPage = decodeMeasure(measureRawColumnChunk, measureColumnChunk, copyPoint);
0:     datChunk.setColumnPage(decodedPage);
1: 
commit:dc83b2a
/////////////////////////////////////////////////////////////////////////
0: import org.apache.carbondata.core.datastore.page.statistics.MeasurePageStatsVO;
0: import org.apache.carbondata.core.metadata.datatype.DataType;
0: import org.apache.carbondata.core.util.CompressionFinder;
0: import org.apache.carbondata.core.util.ValueCompressionUtil;
/////////////////////////////////////////////////////////////////////////
0:    * @param blockletColumnIndex          blocklet index of the column in carbon data file
/////////////////////////////////////////////////////////////////////////
0:       valueEncodeMeta.add(
0:           CarbonUtil.deserializeEncoderMetaV3(measureColumnChunk.getEncoder_meta().get(i).array()));
1: 
0:     MeasurePageStatsVO stats = CarbonUtil.getMeasurePageStats(valueEncodeMeta);
0:     int measureCount = valueEncodeMeta.size();
0:     CompressionFinder[] finders = new CompressionFinder[measureCount];
0:     DataType[] convertedType = new DataType[measureCount];
0:     for (int i = 0; i < measureCount; i++) {
0:       CompressionFinder compresssionFinder =
0:           ValueCompressionUtil.getCompressionFinder(stats.getMax(i), stats.getMin(i),
0:               stats.getDecimal(i), stats.getDataType(i), stats.getDataTypeSelected(i));
0:       finders[i] = compresssionFinder;
0:       convertedType[i] = compresssionFinder.getConvertedDataType();
1:     }
1: 
0:     ValueCompressionHolder values = ValueCompressionUtil.getValueCompressionHolder(finders)[0];
0:     values.uncompress(convertedType[0], rawData.array(), copyPoint,
0:         measureColumnChunk.data_page_length, stats.getDecimal(0),
0:         stats.getMax(0), measureRawColumnChunk.getRowCount()[pageNumber]);
author:QiangCai
-------------------------------------------------------------------------------
commit:086b06d
/////////////////////////////////////////////////////////////////////////
0:    * @param blockIndex          blocklet index of the column in carbon data file
/////////////////////////////////////////////////////////////////////////
0:    * @param startColumnBlockletIndex
0:    * @param endColumnBlockletIndex
/////////////////////////////////////////////////////////////////////////
1:    * @param pageNumber            number
author:kumarvishal
-------------------------------------------------------------------------------
commit:b41e48f
/////////////////////////////////////////////////////////////////////////
1:  * <FileHeader>
1:  * <File Footer>
commit:2cf1104
/////////////////////////////////////////////////////////////////////////
1: /*
1:  * Licensed to the Apache Software Foundation (ASF) under one or more
1:  * contributor license agreements.  See the NOTICE file distributed with
1:  * this work for additional information regarding copyright ownership.
1:  * The ASF licenses this file to You under the Apache License, Version 2.0
1:  * (the "License"); you may not use this file except in compliance with
1:  * the License.  You may obtain a copy of the License at
1:  *
1:  *    http://www.apache.org/licenses/LICENSE-2.0
1:  *
1:  * Unless required by applicable law or agreed to in writing, software
1:  * distributed under the License is distributed on an "AS IS" BASIS,
1:  * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
1:  * See the License for the specific language governing permissions and
1:  * limitations under the License.
1:  */
1: package org.apache.carbondata.core.datastore.chunk.reader.measure.v3;
1: 
1: import java.io.IOException;
1: import java.nio.ByteBuffer;
0: import java.util.ArrayList;
0: import java.util.List;
1: 
0: import org.apache.carbondata.core.datastore.FileHolder;
0: import org.apache.carbondata.core.datastore.chunk.MeasureColumnDataChunk;
1: import org.apache.carbondata.core.datastore.chunk.impl.MeasureRawColumnChunk;
1: import org.apache.carbondata.core.datastore.chunk.reader.measure.AbstractMeasureChunkReaderV2V3Format;
0: import org.apache.carbondata.core.datastore.compression.ValueCompressionHolder;
0: import org.apache.carbondata.core.datastore.compression.WriterCompressModel;
0: import org.apache.carbondata.core.datastore.dataholder.CarbonReadDataHolder;
0: import org.apache.carbondata.core.metadata.ValueEncoderMeta;
1: import org.apache.carbondata.core.metadata.blocklet.BlockletInfo;
1: import org.apache.carbondata.core.util.CarbonUtil;
1: import org.apache.carbondata.format.DataChunk2;
1: import org.apache.carbondata.format.DataChunk3;
1: 
1: import org.apache.commons.lang.ArrayUtils;
1: 
1: /**
1:  * Measure column V3 Reader class which will be used to read and uncompress
1:  * V3 format data
1:  * data format
1:  * Data Format
1:  * <Column1 Data ChunkV3><Column1<Page1><Page2><Page3><Page4>>
1:  * <Column2 Data ChunkV3><Column2<Page1><Page2><Page3><Page4>>
1:  * <Column3 Data ChunkV3><Column3<Page1><Page2><Page3><Page4>>
1:  * <Column4 Data ChunkV3><Column4<Page1><Page2><Page3><Page4>>
1:  */
1: public class CompressedMeasureChunkFileBasedReaderV3 extends AbstractMeasureChunkReaderV2V3Format {
1: 
1:   /**
1:    * end position of last measure in carbon data file
1:    */
1:   private long measureOffsets;
1: 
1:   public CompressedMeasureChunkFileBasedReaderV3(BlockletInfo blockletInfo, String filePath) {
1:     super(blockletInfo, filePath);
1:     measureOffsets = blockletInfo.getMeasureOffsets();
1:   }
1: 
1:   /**
1:    * Below method will be used to read the measure column data form carbon data file
1:    * 1. Get the length of the data to be read
1:    * 2. Allocate the direct buffer
1:    * 3. read the data from file
1:    * 4. Get the data chunk object from data read
1:    * 5. Create the raw chunk object and fill the details
1:    *
1:    * @param fileReader          reader for reading the column from carbon data file
0:    * @param blockletColumnIndex blocklet index of the column in carbon data file
1:    * @return measure raw chunk
1:    */
0:   @Override public MeasureRawColumnChunk readRawMeasureChunk(FileHolder fileReader, int blockIndex)
0:       throws IOException {
1:     int dataLength = 0;
1:     // to calculate the length of the data to be read
1:     // column other than last column we can subtract the offset of current column with
1:     // next column and get the total length.
1:     // but for last column we need to use lastDimensionOffset which is the end position
0:     // of the last dimension, we can subtract current dimension offset from lastDimesionOffset
0:     if (measureColumnChunkOffsets.size() - 1 == blockIndex) {
0:       dataLength = (int) (measureOffsets - measureColumnChunkOffsets.get(blockIndex));
1:     } else {
0:       dataLength = (int) (measureColumnChunkOffsets.get(blockIndex + 1) - measureColumnChunkOffsets
0:           .get(blockIndex));
1:     }
0:     // allocate the buffer
0:     ByteBuffer buffer = ByteBuffer.allocateDirect(dataLength);
1:     // read the data from carbon data file
1:     synchronized (fileReader) {
0:       fileReader
0:           .readByteBuffer(filePath, buffer, measureColumnChunkOffsets.get(blockIndex), dataLength);
1:     }
1:     // get the data chunk which will have all the details about the data pages
1:     DataChunk3 dataChunk =
0:         CarbonUtil.readDataChunk3(buffer, 0, measureColumnChunkLength.get(blockIndex));
1:     // creating a raw chunks instance and filling all the details
1:     MeasureRawColumnChunk rawColumnChunk =
0:         new MeasureRawColumnChunk(blockIndex, buffer, 0, dataLength, this);
1:     int numberOfPages = dataChunk.getPage_length().size();
1:     byte[][] maxValueOfEachPage = new byte[numberOfPages][];
1:     byte[][] minValueOfEachPage = new byte[numberOfPages][];
1:     int[] eachPageLength = new int[numberOfPages];
1:     for (int i = 0; i < minValueOfEachPage.length; i++) {
1:       maxValueOfEachPage[i] =
1:           dataChunk.getData_chunk_list().get(i).getMin_max().getMax_values().get(0).array();
1:       minValueOfEachPage[i] =
1:           dataChunk.getData_chunk_list().get(i).getMin_max().getMin_values().get(0).array();
1:       eachPageLength[i] = dataChunk.getData_chunk_list().get(i).getNumberOfRowsInpage();
1:     }
1:     rawColumnChunk.setDataChunkV3(dataChunk);
1:     rawColumnChunk.setFileReader(fileReader);
1:     rawColumnChunk.setPagesCount(dataChunk.getPage_length().size());
1:     rawColumnChunk.setMaxValues(maxValueOfEachPage);
1:     rawColumnChunk.setMinValues(minValueOfEachPage);
1:     rawColumnChunk.setRowCount(eachPageLength);
0:     rawColumnChunk.setLengths(ArrayUtils
0:         .toPrimitive(dataChunk.page_length.toArray(new Integer[dataChunk.page_length.size()])));
1:     rawColumnChunk.setOffsets(ArrayUtils
1:         .toPrimitive(dataChunk.page_offset.toArray(new Integer[dataChunk.page_offset.size()])));
1:     return rawColumnChunk;
1:   }
1: 
1:   /**
1:    * Below method will be used to read the multiple measure column data in group
1:    * and divide into measure raw chunk object
1:    * Steps for reading
1:    * 1. Get the length of the data to be read
1:    * 2. Allocate the direct buffer
1:    * 3. read the data from file
1:    * 4. Get the data chunk object from file for each column
1:    * 5. Create the raw chunk object and fill the details for each column
1:    * 6. increment the offset of the data
1:    *
1:    * @param fileReader
1:    *        reader which will be used to read the measure columns data from file
0:    * @param startBlockletColumnIndex
0:    *        blocklet index of the first measure column
0:    * @param endBlockletColumnIndex
0:    *        blocklet index of the last measure column
1:    * @return MeasureRawColumnChunk array
1:    */
0:   protected MeasureRawColumnChunk[] readRawMeasureChunksInGroup(FileHolder fileReader,
0:       int startColumnBlockletIndex, int endColumnBlockletIndex) throws IOException {
1:     // to calculate the length of the data to be read
1:     // column we can subtract the offset of start column offset with
1:     // end column+1 offset and get the total length.
0:     long currentMeasureOffset = measureColumnChunkOffsets.get(startColumnBlockletIndex);
0:     ByteBuffer buffer = ByteBuffer.allocateDirect(
0:         (int) (measureColumnChunkOffsets.get(endColumnBlockletIndex + 1) - currentMeasureOffset));
1:     // read the data from carbon data file
1:     synchronized (fileReader) {
0:       fileReader.readByteBuffer(filePath, buffer, currentMeasureOffset,
0:           (int) (measureColumnChunkOffsets.get(endColumnBlockletIndex + 1) - currentMeasureOffset));
1:     }
1:     // create raw chunk for each measure column
1:     MeasureRawColumnChunk[] measureDataChunk =
0:         new MeasureRawColumnChunk[endColumnBlockletIndex - startColumnBlockletIndex + 1];
1:     int runningLength = 0;
1:     int index = 0;
0:     for (int i = startColumnBlockletIndex; i <= endColumnBlockletIndex; i++) {
1:       int currentLength =
1:           (int) (measureColumnChunkOffsets.get(i + 1) - measureColumnChunkOffsets.get(i));
0:       MeasureRawColumnChunk measureRawColumnChunk =
0:           new MeasureRawColumnChunk(i, buffer, runningLength, currentLength, this);
1:       DataChunk3 dataChunk =
1:           CarbonUtil.readDataChunk3(buffer, runningLength, measureColumnChunkLength.get(i));
1: 
1:       int numberOfPages = dataChunk.getPage_length().size();
1:       byte[][] maxValueOfEachPage = new byte[numberOfPages][];
1:       byte[][] minValueOfEachPage = new byte[numberOfPages][];
1:       int[] eachPageLength = new int[numberOfPages];
0:       for (int j = 0; j < minValueOfEachPage.length; j++) {
0:         maxValueOfEachPage[j] =
0:             dataChunk.getData_chunk_list().get(j).getMin_max().getMax_values().get(0).array();
0:         minValueOfEachPage[j] =
0:             dataChunk.getData_chunk_list().get(j).getMin_max().getMin_values().get(0).array();
0:         eachPageLength[j] = dataChunk.getData_chunk_list().get(j).getNumberOfRowsInpage();
1:       }
0:       measureRawColumnChunk.setDataChunkV3(dataChunk);
0:       ;
0:       measureRawColumnChunk.setFileReader(fileReader);
0:       measureRawColumnChunk.setPagesCount(dataChunk.getPage_length().size());
0:       measureRawColumnChunk.setMaxValues(maxValueOfEachPage);
0:       measureRawColumnChunk.setMinValues(minValueOfEachPage);
0:       measureRawColumnChunk.setRowCount(eachPageLength);
0:       measureRawColumnChunk.setLengths(ArrayUtils
0:           .toPrimitive(dataChunk.page_length.toArray(new Integer[dataChunk.page_length.size()])));
0:       measureRawColumnChunk.setOffsets(ArrayUtils
1:           .toPrimitive(dataChunk.page_offset.toArray(new Integer[dataChunk.page_offset.size()])));
1:       measureDataChunk[index] = measureRawColumnChunk;
1:       runningLength += currentLength;
1:       index++;
1:     }
1:     return measureDataChunk;
1:   }
1: 
1:   /**
1:    * Below method will be used to convert the compressed measure chunk raw data to actual data
1:    *
0:    * @param measureRawColumnChunk measure raw chunk
0:    * @param page                  number
0:    * @return DimensionColumnDataChunk
1:    */
0:   @Override public MeasureColumnDataChunk convertToMeasureChunk(
0:       MeasureRawColumnChunk measureRawColumnChunk, int pageNumber) throws IOException {
0:     MeasureColumnDataChunk datChunk = new MeasureColumnDataChunk();
1:     // data chunk of blocklet column
0:     DataChunk3 dataChunk3 = measureRawColumnChunk.getDataChunkV3();
1:     // data chunk of page
0:     DataChunk2 measureColumnChunk = dataChunk3.getData_chunk_list().get(pageNumber);
1:     // calculating the start point of data
1:     // as buffer can contain multiple column data, start point will be datachunkoffset +
1:     // data chunk length + page offset
0:     int copyPoint = measureRawColumnChunk.getOffSet() + measureColumnChunkLength
0:         .get(measureRawColumnChunk.getBlockletId()) + dataChunk3.getPage_offset().get(pageNumber);
0:     List<ValueEncoderMeta> valueEncodeMeta = new ArrayList<>();
0:     for (int i = 0; i < measureColumnChunk.getEncoder_meta().size(); i++) {
0:       valueEncodeMeta.add(CarbonUtil
0:           .deserializeEncoderMetaNew(measureColumnChunk.getEncoder_meta().get(i).array()));
1:     }
0:     WriterCompressModel compressionModel = CarbonUtil.getValueCompressionModel(valueEncodeMeta);
0:     ValueCompressionHolder values = compressionModel.getValueCompressionHolder()[0];
0:     // uncompress
0:     byte[] data = new byte[measureColumnChunk.data_page_length];
0:     ByteBuffer rawData = measureRawColumnChunk.getRawData();
0:     rawData.position(copyPoint);
0:     rawData.get(data);
0:     values.uncompress(compressionModel.getConvertedDataType()[0], data, 0,
0:         measureColumnChunk.data_page_length, compressionModel.getMantissa()[0],
0:         compressionModel.getMaxValue()[0], measureRawColumnChunk.getRowCount()[pageNumber]);
0:     CarbonReadDataHolder measureDataHolder = new CarbonReadDataHolder(values);
0:     // set the data chunk
0:     datChunk.setMeasureDataHolder(measureDataHolder);
0:     // set the null value indexes
0:     datChunk.setNullValueIndexHolder(getPresenceMeta(measureColumnChunk.presence));
0:     return datChunk;
1:   }
1: }
============================================================================