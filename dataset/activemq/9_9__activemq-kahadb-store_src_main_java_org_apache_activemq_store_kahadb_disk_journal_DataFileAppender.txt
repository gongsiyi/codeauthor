2:456a2ba: /**
1:456a2ba:  * Licensed to the Apache Software Foundation (ASF) under one or more
1:456a2ba:  * contributor license agreements.  See the NOTICE file distributed with
1:456a2ba:  * this work for additional information regarding copyright ownership.
1:456a2ba:  * The ASF licenses this file to You under the Apache License, Version 2.0
1:456a2ba:  * (the "License"); you may not use this file except in compliance with
1:456a2ba:  * the License.  You may obtain a copy of the License at
4:456a2ba:  *
1:456a2ba:  *      http://www.apache.org/licenses/LICENSE-2.0
1:0bbc0ac:  *
1:456a2ba:  * Unless required by applicable law or agreed to in writing, software
1:456a2ba:  * distributed under the License is distributed on an "AS IS" BASIS,
1:456a2ba:  * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
1:456a2ba:  * See the License for the specific language governing permissions and
1:456a2ba:  * limitations under the License.
2:456a2ba:  */
1:1aab71b: package org.apache.activemq.store.kahadb.disk.journal;
23:456a2ba: 
1:456a2ba: import java.io.IOException;
1:456a2ba: import java.io.InterruptedIOException;
1:456a2ba: import java.util.Map;
1:456a2ba: import java.util.concurrent.CountDownLatch;
1:6a4e25c: import java.util.concurrent.atomic.AtomicReference;
1:f73b622: import java.util.zip.Adler32;
1:f73b622: import java.util.zip.Checksum;
1:456a2ba: 
1:dd0ed17: import org.apache.activemq.store.kahadb.disk.journal.Journal.JournalDiskSyncStrategy;
1:1aab71b: import org.apache.activemq.store.kahadb.disk.util.DataByteArrayOutputStream;
1:1aab71b: import org.apache.activemq.store.kahadb.disk.util.LinkedNodeList;
1:ef619b6: import org.apache.activemq.util.ByteSequence;
1:d53b8f8: import org.apache.activemq.util.IOExceptionSupport;
1:582af3e: import org.apache.activemq.util.RecoverableRandomAccessFile;
1:cdba931: import org.slf4j.Logger;
1:cdba931: import org.slf4j.LoggerFactory;
1:456a2ba: 
1:62bdbb0: import static org.apache.activemq.store.kahadb.disk.journal.Journal.EMPTY_BATCH_CONTROL_RECORD;
1:62bdbb0: import static org.apache.activemq.store.kahadb.disk.journal.Journal.RECORD_HEAD_SPACE;
1:62bdbb0: 
1:456a2ba: /**
1:456a2ba:  * An optimized writer to do batch appends to a data file. This object is thread
1:456a2ba:  * safe and gains throughput as you increase the number of concurrent writes it
1:456a2ba:  * does.
1:456a2ba:  */
1:bb4a2f7: class DataFileAppender implements FileAppender {
1:456a2ba: 
1:cdba931:     private static final Logger logger = LoggerFactory.getLogger(DataFileAppender.class);
1:456a2ba: 
1:f73b622:     protected final Journal journal;
1:bb4a2f7:     protected final Map<Journal.WriteKey, Journal.WriteCommand> inflightWrites;
1:cdba931:     protected final Object enqueueMutex = new Object();
1:456a2ba:     protected WriteBatch nextWriteBatch;
1:456a2ba: 
1:456a2ba:     protected boolean shutdown;
1:456a2ba:     protected IOException firstAsyncException;
1:456a2ba:     protected final CountDownLatch shutdownDone = new CountDownLatch(1);
1:561cda1:     protected int maxWriteBatchSize;
1:89f22da:     protected final boolean syncOnComplete;
1:dd0ed17:     protected final boolean periodicSync;
1:456a2ba: 
1:89f22da:     protected boolean running;
1:456a2ba:     private Thread thread;
1:456a2ba: 
1:456a2ba:     public class WriteBatch {
1:456a2ba: 
1:456a2ba:         public final DataFile dataFile;
1:456a2ba: 
1:bb4a2f7:         public final LinkedNodeList<Journal.WriteCommand> writes = new LinkedNodeList<Journal.WriteCommand>();
1:456a2ba:         public final CountDownLatch latch = new CountDownLatch(1);
1:89f22da:         protected final int offset;
1:f73b622:         public int size = Journal.BATCH_CONTROL_RECORD_SIZE;
1:6a4e25c:         public AtomicReference<IOException> exception = new AtomicReference<IOException>();
1:456a2ba: 
1:89f22da:         public WriteBatch(DataFile dataFile,int offset) {
1:456a2ba:             this.dataFile = dataFile;
1:f73b622:             this.offset = offset;
1:f73b622:             this.dataFile.incrementLength(Journal.BATCH_CONTROL_RECORD_SIZE);
1:f73b622:             this.size=Journal.BATCH_CONTROL_RECORD_SIZE;
1:f73b622:             journal.addToTotalLength(Journal.BATCH_CONTROL_RECORD_SIZE);
15:456a2ba:         }
1:45e59e6: 
1:89f22da:         public WriteBatch(DataFile dataFile, int offset, Journal.WriteCommand write) throws IOException {
1:89f22da:             this(dataFile, offset);
1:f73b622:             append(write);
1:45e59e6:         }
1:456a2ba: 
1:bb4a2f7:         public boolean canAppend(Journal.WriteCommand write) {
1:f73b622:             int newSize = size + write.location.getSize();
1:f73b622:             if (newSize >= maxWriteBatchSize || offset+newSize > journal.getMaxFileLength() ) {
3:456a2ba:                 return false;
1:456a2ba:             }
1:456a2ba:             return true;
1:456a2ba:         }
1:456a2ba: 
1:bb4a2f7:         public void append(Journal.WriteCommand write) throws IOException {
2:456a2ba:             this.writes.addLast(write);
1:f73b622:             write.location.setDataFileId(dataFile.getDataFileId());
1:f73b622:             write.location.setOffset(offset+size);
1:f73b622:             int s = write.location.getSize();
1:f73b622:             size += s;
1:f73b622:             dataFile.incrementLength(s);
1:f73b622:             journal.addToTotalLength(s);
1:456a2ba:         }
1:456a2ba:     }
1:456a2ba: 
1:456a2ba:     /**
1:456a2ba:      * Construct a Store writer
1:456a2ba:      */
1:456a2ba:     public DataFileAppender(Journal dataManager) {
1:f73b622:         this.journal = dataManager;
1:f73b622:         this.inflightWrites = this.journal.getInflightWrites();
1:561cda1:         this.maxWriteBatchSize = this.journal.getWriteBatchSize();
1:89f22da:         this.syncOnComplete = this.journal.isEnableAsyncDiskSync();
1:dd0ed17:         this.periodicSync = JournalDiskSyncStrategy.PERIODIC.equals(
1:dd0ed17:                 this.journal.getJournalDiskSyncStrategy());
1:456a2ba:     }
1:456a2ba: 
1:ef619b6:     @Override
1:456a2ba:     public Location storeItem(ByteSequence data, byte type, boolean sync) throws IOException {
1:456a2ba: 
1:456a2ba:         // Write the packet our internal buffer.
1:62bdbb0:         int size = data.getLength() + RECORD_HEAD_SPACE;
1:456a2ba: 
1:456a2ba:         final Location location = new Location();
1:456a2ba:         location.setSize(size);
1:456a2ba:         location.setType(type);
1:456a2ba: 
1:bb4a2f7:         Journal.WriteCommand write = new Journal.WriteCommand(location, data, sync);
1:456a2ba: 
1:958038e:         WriteBatch batch = enqueue(write);
1:8c3ef6c:         location.setBatch(batch);
1:456a2ba:         if (sync) {
3:456a2ba:             try {
1:456a2ba:                 batch.latch.await();
3:456a2ba:             } catch (InterruptedException e) {
2:456a2ba:                 throw new InterruptedIOException();
1:456a2ba:             }
1:cdba931:             IOException exception = batch.exception.get();
1:6a4e25c:             if (exception != null) {
1:cdba931:                 throw exception;
1:456a2ba:             }
1:456a2ba:         }
1:456a2ba: 
1:456a2ba:         return location;
1:456a2ba:     }
1:456a2ba: 
1:ef619b6:     @Override
1:456a2ba:     public Location storeItem(ByteSequence data, byte type, Runnable onComplete) throws IOException {
1:456a2ba:         // Write the packet our internal buffer.
1:62bdbb0:         int size = data.getLength() + RECORD_HEAD_SPACE;
1:456a2ba: 
1:456a2ba:         final Location location = new Location();
1:456a2ba:         location.setSize(size);
1:456a2ba:         location.setType(type);
1:456a2ba: 
1:bb4a2f7:         Journal.WriteCommand write = new Journal.WriteCommand(location, data, onComplete);
1:8c3ef6c:         location.setBatch(enqueue(write));
1:456a2ba: 
1:456a2ba:         return location;
1:456a2ba:     }
1:456a2ba: 
1:bb4a2f7:     private WriteBatch enqueue(Journal.WriteCommand write) throws IOException {
1:456a2ba:         synchronized (enqueueMutex) {
2:456a2ba:             if (shutdown) {
1:95f7262:                 throw new IOException("Async Writer Thread Shutdown");
1:456a2ba:             }
1:456a2ba: 
1:456a2ba:             if (!running) {
1:456a2ba:                 running = true;
1:456a2ba:                 thread = new Thread() {
1:ef619b6:                     @Override
1:456a2ba:                     public void run() {
1:456a2ba:                         processQueue();
1:456a2ba:                     }
2:456a2ba:                 };
1:456a2ba:                 thread.setPriority(Thread.MAX_PRIORITY);
1:456a2ba:                 thread.setDaemon(true);
1:456a2ba:                 thread.setName("ActiveMQ Data File Writer");
1:456a2ba:                 thread.start();
1:f73b622:             }
1:f73b622: 
1:f73b622:             while ( true ) {
1:f73b622:                 if (nextWriteBatch == null) {
1:62bdbb0:                     DataFile file = journal.getCurrentDataFile(write.location.getSize());
1:cdba931:                     nextWriteBatch = newWriteBatch(write, file);
1:cdba931:                     enqueueMutex.notifyAll();
1:cdba931:                     break;
1:f73b622:                 } else {
1:f73b622:                     // Append to current batch if possible..
1:f73b622:                     if (nextWriteBatch.canAppend(write)) {
1:f73b622:                         nextWriteBatch.append(write);
1:cdba931:                         break;
1:f73b622:                     } else {
1:f73b622:                         // Otherwise wait for the queuedCommand to be null
1:f73b622:                         try {
1:f73b622:                             while (nextWriteBatch != null) {
1:cdba931:                                 final long start = System.currentTimeMillis();
1:f73b622:                                 enqueueMutex.wait();
1:cdba931:                                 if (maxStat > 0) {
1:95f7262:                                     logger.info("Waiting for write to finish with full batch... millis: " +
1:cdba931:                                                 (System.currentTimeMillis() - start));
1:456a2ba:                                }
1:456a2ba:                             }
1:456a2ba:                         } catch (InterruptedException e) {
1:456a2ba:                             throw new InterruptedIOException();
1:456a2ba:                         }
1:456a2ba:                         if (shutdown) {
1:95f7262:                             throw new IOException("Async Writer Thread Shutdown");
1:456a2ba:                         }
1:456a2ba:                     }
1:456a2ba:                 }
1:456a2ba:             }
1:89f22da:             if (!write.sync) {
1:bb4a2f7:                 inflightWrites.put(new Journal.WriteKey(write.location), write);
1:456a2ba:             }
1:f73b622:             return nextWriteBatch;
1:f73b622:         }
1:f73b622:     }
1:456a2ba: 
1:89f22da:     protected WriteBatch newWriteBatch(Journal.WriteCommand write, DataFile file) throws IOException {
1:89f22da:         return new WriteBatch(file, file.getLength(), write);
1:f73b622:     }
1:456a2ba: 
1:ef619b6:     @Override
1:456a2ba:     public void close() throws IOException {
1:456a2ba:         synchronized (enqueueMutex) {
1:456a2ba:             if (!shutdown) {
1:456a2ba:                 shutdown = true;
1:456a2ba:                 if (running) {
1:456a2ba:                     enqueueMutex.notifyAll();
1:f73b622:                 } else {
1:456a2ba:                     shutdownDone.countDown();
1:f73b622:                 }
1:f73b622:             }
1:456a2ba:         }
1:456a2ba: 
1:456a2ba:         try {
1:456a2ba:             shutdownDone.await();
1:f73b622:         } catch (InterruptedException e) {
1:f73b622:             throw new InterruptedIOException();
1:456a2ba:         }
1:456a2ba: 
1:456a2ba:     }
1:456a2ba: 
1:76f842d:     int statIdx = 0;
1:76f842d:     int[] stats = new int[maxStat];
1:456a2ba:     /**
1:456a2ba:      * The async processing loop that writes to the data files and does the
1:456a2ba:      * force calls. Since the file sync() call is the slowest of all the
1:456a2ba:      * operations, this algorithm tries to 'batch' or group together several
1:456a2ba:      * file sync() requests into a single file sync() call. The batching is
1:456a2ba:      * accomplished attaching the same CountDownLatch instance to every force
1:456a2ba:      * request in a group.
1:456a2ba:      */
1:456a2ba:     protected void processQueue() {
1:456a2ba:         DataFile dataFile = null;
1:582af3e:         RecoverableRandomAccessFile file = null;
1:b1a9130:         WriteBatch wb = null;
1:946e62d:         try (DataByteArrayOutputStream buff = new DataByteArrayOutputStream(maxWriteBatchSize);) {
1:456a2ba: 
1:456a2ba:             while (true) {
1:456a2ba: 
1:456a2ba:                 // Block till we get a command.
1:456a2ba:                 synchronized (enqueueMutex) {
1:456a2ba:                     while (true) {
1:456a2ba:                         if (nextWriteBatch != null) {
1:cdba931:                             wb = nextWriteBatch;
1:456a2ba:                             nextWriteBatch = null;
1:456a2ba:                             break;
1:456a2ba:                         }
1:f73b622:                         if (shutdown) {
1:456a2ba:                             return;
1:456a2ba:                         }
2:456a2ba:                         enqueueMutex.wait();
1:456a2ba:                     }
1:4b49834:                     enqueueMutex.notifyAll();
1:456a2ba:                 }
1:456a2ba: 
1:456a2ba:                 if (dataFile != wb.dataFile) {
1:456a2ba:                     if (file != null) {
1:1a59827:                         if (periodicSync) {
1:1a59827:                             if (logger.isTraceEnabled()) {
1:d53b8f8:                                 logger.trace("Syncing file {} on rotate", dataFile.getFile().getName());
1:1a59827:                             }
1:1a59827:                             file.sync();
1:1a59827:                         }
1:456a2ba:                         dataFile.closeRandomAccessFile(file);
1:456a2ba:                     }
1:456a2ba:                     dataFile = wb.dataFile;
1:62bdbb0:                     file = dataFile.appendRandomAccessFile();
1:4a82118:                 }
1:456a2ba: 
1:bb4a2f7:                 Journal.WriteCommand write = wb.writes.getHead();
1:456a2ba: 
1:f73b622:                 // Write an empty batch control record.
1:f73b622:                 buff.reset();
1:62bdbb0:                 buff.write(EMPTY_BATCH_CONTROL_RECORD);
1:f73b622: 
1:456a2ba:                 boolean forceToDisk = false;
1:f73b622:                 while (write != null) {
1:89f22da:                     forceToDisk |= write.sync | (syncOnComplete && write.onComplete != null);
1:f73b622:                     buff.writeInt(write.location.getSize());
1:f73b622:                     buff.writeByte(write.location.getType());
1:f73b622:                     buff.write(write.data.getData(), write.data.getOffset(), write.data.getLength());
1:f73b622:                     write = write.getNext();
1:456a2ba:                 }
1:95f7262: 
1:62bdbb0:                 // append 'unset', zero length next batch so read can always find eof
1:62bdbb0:                 buff.write(Journal.EOF_RECORD);
1:456a2ba: 
1:f73b622:                 ByteSequence sequence = buff.toByteSequence();
1:f73b622: 
1:f73b622:                 // Now we can fill in the batch control record properly.
1:f73b622:                 buff.reset();
1:62bdbb0:                 buff.skip(RECORD_HEAD_SPACE + Journal.BATCH_CONTROL_RECORD_MAGIC.length);
1:62bdbb0:                 buff.writeInt(sequence.getLength() - Journal.BATCH_CONTROL_RECORD_SIZE - Journal.EOF_RECORD.length);
1:f73b622:                 if( journal.isChecksum() ) {
1:f73b622:                     Checksum checksum = new Adler32();
1:62bdbb0:                     checksum.update(sequence.getData(), sequence.getOffset()+Journal.BATCH_CONTROL_RECORD_SIZE, sequence.getLength()-Journal.BATCH_CONTROL_RECORD_SIZE-Journal.EOF_RECORD.length);
1:f73b622:                     buff.writeLong(checksum.getValue());
1:f73b622:                 }
1:f73b622: 
1:f73b622:                 // Now do the 1 big write.
1:f73b622:                 file.seek(wb.offset);
1:76f842d:                 if (maxStat > 0) {
1:76f842d:                     if (statIdx < maxStat) {
1:76f842d:                         stats[statIdx++] = sequence.getLength();
4:456a2ba:                     } else {
1:76f842d:                         long all = 0;
1:76f842d:                         for (;statIdx > 0;) {
1:76f842d:                             all+= stats[--statIdx];
1:456a2ba:                         }
1:cdba931:                         logger.info("Ave writeSize: " + all/maxStat);
1:456a2ba:                     }
1:456a2ba:                 }
1:f73b622:                 file.write(sequence.getData(), sequence.getOffset(), sequence.getLength());
1:f73b622: 
1:f73b622:                 ReplicationTarget replicationTarget = journal.getReplicationTarget();
1:f73b622:                 if( replicationTarget!=null ) {
1:f73b622:                     replicationTarget.replicate(wb.writes.getHead().location, sequence, forceToDisk);
1:f73b622:                 }
1:f73b622: 
1:456a2ba:                 if (forceToDisk) {
1:ef619b6:                     file.sync();
1:456a2ba:                 }
1:456a2ba: 
1:bb4a2f7:                 Journal.WriteCommand lastWrite = wb.writes.getTail();
1:f73b622:                 journal.setLastAppendLocation(lastWrite.location);
1:456a2ba: 
1:89f22da:                 signalDone(wb);
1:456a2ba:             }
1:d53b8f8:         } catch (Throwable error) {
1:d53b8f8:             logger.warn("Journal failed while writing at: " + wb.dataFile.getDataFileId() + ":" + wb.offset, error);
1:456a2ba:             synchronized (enqueueMutex) {
1:c5a8b2c:                 shutdown = true;
1:d53b8f8:                 running = false;
1:d53b8f8:                 signalError(wb, error);
1:21ae1ef:                 if (nextWriteBatch != null) {
1:21ae1ef:                     signalError(nextWriteBatch, error);
1:21ae1ef:                     nextWriteBatch = null;
1:21ae1ef:                     enqueueMutex.notifyAll();
1:21ae1ef:                 }
1:456a2ba:             }
1:456a2ba:         } finally {
1:456a2ba:             try {
1:456a2ba:                 if (file != null) {
1:1a59827:                     if (periodicSync) {
1:1a59827:                         if (logger.isTraceEnabled()) {
1:1a59827:                             logger.trace("Syning file {} on close", dataFile.getFile().getName());
1:1a59827:                         }
1:1a59827:                         file.sync();
1:1a59827:                     }
1:456a2ba:                     dataFile.closeRandomAccessFile(file);
1:456a2ba:                 }
1:456a2ba:             } catch (Throwable ignore) {
1:456a2ba:             }
1:456a2ba:             shutdownDone.countDown();
1:a19e27a:             running = false;
1:456a2ba:         }
1:456a2ba:     }
1:456a2ba: 
1:89f22da:     protected void signalDone(WriteBatch wb) {
1:456a2ba:         // Now that the data is on disk, remove the writes from the in
1:456a2ba:         // flight
1:456a2ba:         // cache.
1:89f22da:         Journal.WriteCommand write = wb.writes.getHead();
2:456a2ba:         while (write != null) {
1:456a2ba:             if (!write.sync) {
1:89f22da:                 inflightWrites.remove(new Journal.WriteKey(write.location));
1:456a2ba:             }
1:d53b8f8:             if (write.onComplete != null && wb.exception.get() == null) {
1:456a2ba:                 try {
1:456a2ba:                     write.onComplete.run();
1:456a2ba:                 } catch (Throwable e) {
1:cdba931:                     logger.info("Add exception was raised while executing the run command for onComplete", e);
1:456a2ba:                 }
1:456a2ba:             }
2:456a2ba:             write = write.getNext();
1:456a2ba:         }
1:456a2ba: 
1:456a2ba:         // Signal any waiting threads that the write is on disk.
1:456a2ba:         wb.latch.countDown();
1:456a2ba:     }
1:d53b8f8: 
1:d53b8f8:     protected void signalError(WriteBatch wb, Throwable t) {
1:d53b8f8:         if (wb != null) {
1:d53b8f8:             if (t instanceof IOException) {
1:d53b8f8:                 wb.exception.set((IOException) t);
1:21ae1ef:                 // revert sync batch increment such that next write is contiguous
1:21ae1ef:                 if (syncBatch(wb.writes)) {
1:21ae1ef:                     wb.dataFile.decrementLength(wb.size);
1:21ae1ef:                 }
1:d53b8f8:             } else {
1:d53b8f8:                 wb.exception.set(IOExceptionSupport.create(t));
1:d53b8f8:             }
1:d53b8f8:             signalDone(wb);
1:d53b8f8:         }
1:d53b8f8:     }
1:21ae1ef: 
1:21ae1ef:     // async writes will already be in the index so reuse is not an option
1:21ae1ef:     private boolean syncBatch(LinkedNodeList<Journal.WriteCommand> writes) {
1:21ae1ef:         Journal.WriteCommand write = writes.getHead();
1:21ae1ef:         while (write != null && write.sync) {
1:21ae1ef:             write = write.getNext();
1:21ae1ef:         }
1:21ae1ef:         return write == null;
1:21ae1ef:     }
1:456a2ba: }
============================================================================
author:gtully
-------------------------------------------------------------------------------
commit:8c3ef6c
/////////////////////////////////////////////////////////////////////////
1:         location.setBatch(batch);
/////////////////////////////////////////////////////////////////////////
1:         location.setBatch(enqueue(write));
commit:c5a8b2c
/////////////////////////////////////////////////////////////////////////
1:                 shutdown = true;
commit:21ae1ef
/////////////////////////////////////////////////////////////////////////
1:                 if (nextWriteBatch != null) {
1:                     signalError(nextWriteBatch, error);
1:                     nextWriteBatch = null;
1:                     enqueueMutex.notifyAll();
1:                 }
/////////////////////////////////////////////////////////////////////////
1:                 // revert sync batch increment such that next write is contiguous
1:                 if (syncBatch(wb.writes)) {
1:                     wb.dataFile.decrementLength(wb.size);
1:                 }
1: 
1:     // async writes will already be in the index so reuse is not an option
1:     private boolean syncBatch(LinkedNodeList<Journal.WriteCommand> writes) {
1:         Journal.WriteCommand write = writes.getHead();
1:         while (write != null && write.sync) {
1:             write = write.getNext();
1:         }
1:         return write == null;
1:     }
commit:d53b8f8
/////////////////////////////////////////////////////////////////////////
1: import org.apache.activemq.util.IOExceptionSupport;
/////////////////////////////////////////////////////////////////////////
/////////////////////////////////////////////////////////////////////////
/////////////////////////////////////////////////////////////////////////
1:                                 logger.trace("Syncing file {} on rotate", dataFile.getFile().getName());
/////////////////////////////////////////////////////////////////////////
1:         } catch (Throwable error) {
1:             logger.warn("Journal failed while writing at: " + wb.dataFile.getDataFileId() + ":" + wb.offset, error);
1:                 running = false;
1:                 signalError(wb, error);
0:                 signalError(nextWriteBatch, error);
/////////////////////////////////////////////////////////////////////////
1:             if (write.onComplete != null && wb.exception.get() == null) {
/////////////////////////////////////////////////////////////////////////
1: 
1:     protected void signalError(WriteBatch wb, Throwable t) {
1:         if (wb != null) {
1:             if (t instanceof IOException) {
1:                 wb.exception.set((IOException) t);
0:                 // revert batch increment such that next write is contiguous
0:                 wb.dataFile.decrementLength(wb.size);
1:             } else {
1:                 wb.exception.set(IOExceptionSupport.create(t));
1:             }
1:             signalDone(wb);
1:         }
1:     }
commit:538ed74
/////////////////////////////////////////////////////////////////////////
0:             logger.warn("Journal failed while writing at: " + wb.dataFile.getDataFileId() + ":" + wb.offset, e);
commit:62bdbb0
/////////////////////////////////////////////////////////////////////////
1: import static org.apache.activemq.store.kahadb.disk.journal.Journal.EMPTY_BATCH_CONTROL_RECORD;
1: import static org.apache.activemq.store.kahadb.disk.journal.Journal.RECORD_HEAD_SPACE;
1: 
/////////////////////////////////////////////////////////////////////////
1:         int size = data.getLength() + RECORD_HEAD_SPACE;
/////////////////////////////////////////////////////////////////////////
1:         int size = data.getLength() + RECORD_HEAD_SPACE;
/////////////////////////////////////////////////////////////////////////
1:                     DataFile file = journal.getCurrentDataFile(write.location.getSize());
/////////////////////////////////////////////////////////////////////////
1:                     file = dataFile.appendRandomAccessFile();
1:                 buff.write(EMPTY_BATCH_CONTROL_RECORD);
/////////////////////////////////////////////////////////////////////////
1:                 // append 'unset', zero length next batch so read can always find eof
1:                 buff.write(Journal.EOF_RECORD);
1:                 buff.skip(RECORD_HEAD_SPACE + Journal.BATCH_CONTROL_RECORD_MAGIC.length);
1:                 buff.writeInt(sequence.getLength() - Journal.BATCH_CONTROL_RECORD_SIZE - Journal.EOF_RECORD.length);
1:                     checksum.update(sequence.getData(), sequence.getOffset()+Journal.BATCH_CONTROL_RECORD_SIZE, sequence.getLength()-Journal.BATCH_CONTROL_RECORD_SIZE-Journal.EOF_RECORD.length);
commit:4a82118
/////////////////////////////////////////////////////////////////////////
0:                     // pre allocate on first open of new file (length==0)
0:                     // note dataFile.length cannot be used because it is updated in enqueue
0:                     if (file.length() == 0l) {
0:                         journal.preallocateEntireJournalDataFile(file);
1:                     }
commit:95f7262
/////////////////////////////////////////////////////////////////////////
1:                 throw new IOException("Async Writer Thread Shutdown");
/////////////////////////////////////////////////////////////////////////
0:                     if( file.getLength() + write.location.getSize() >= journal.getMaxFileLength() ) {
/////////////////////////////////////////////////////////////////////////
1:                                     logger.info("Waiting for write to finish with full batch... millis: " +
/////////////////////////////////////////////////////////////////////////
1:                             throw new IOException("Async Writer Thread Shutdown");
/////////////////////////////////////////////////////////////////////////
0:     final byte[] end = new byte[]{0};
/////////////////////////////////////////////////////////////////////////
0:                     // pre allocate on first open
0:                     file.seek(journal.maxFileLength-1);
0:                     file.write(end);
/////////////////////////////////////////////////////////////////////////
0:                 // append 'unset' next batch (5 bytes) so read can always find eof
0:                 buff.writeInt(0);
0:                 buff.writeByte(0);
1: 
0:                 buff.writeInt(sequence.getLength()-Journal.BATCH_CONTROL_RECORD_SIZE - 5);
0:                     checksum.update(sequence.getData(), sequence.getOffset()+Journal.BATCH_CONTROL_RECORD_SIZE, sequence.getLength()-Journal.BATCH_CONTROL_RECORD_SIZE - 5);
author:Christopher L. Shannon (cshannon)
-------------------------------------------------------------------------------
commit:1a59827
/////////////////////////////////////////////////////////////////////////
1:                         if (periodicSync) {
1:                             if (logger.isTraceEnabled()) {
0:                                 logger.trace("Syning file {} on rotate", dataFile.getFile().getName());
1:                             }
1:                             file.sync();
1:                         }
/////////////////////////////////////////////////////////////////////////
/////////////////////////////////////////////////////////////////////////
1:                     if (periodicSync) {
1:                         if (logger.isTraceEnabled()) {
1:                             logger.trace("Syning file {} on close", dataFile.getFile().getName());
1:                         }
1:                         file.sync();
1:                     }
commit:dd0ed17
/////////////////////////////////////////////////////////////////////////
1: import org.apache.activemq.store.kahadb.disk.journal.Journal.JournalDiskSyncStrategy;
/////////////////////////////////////////////////////////////////////////
1:     protected final boolean periodicSync;
/////////////////////////////////////////////////////////////////////////
1:         this.periodicSync = JournalDiskSyncStrategy.PERIODIC.equals(
1:                 this.journal.getJournalDiskSyncStrategy());
/////////////////////////////////////////////////////////////////////////
0:                 } else if (periodicSync) {
0:                     journal.currentFileNeedSync.set(true);
author:Timothy Bish
-------------------------------------------------------------------------------
commit:946e62d
/////////////////////////////////////////////////////////////////////////
/////////////////////////////////////////////////////////////////////////
0:                     DataFile file = journal.getOrCreateCurrentWriteFile();
/////////////////////////////////////////////////////////////////////////
1:         try (DataByteArrayOutputStream buff = new DataByteArrayOutputStream(maxWriteBatchSize);) {
commit:ef619b6
/////////////////////////////////////////////////////////////////////////
1: import org.apache.activemq.util.ByteSequence;
/////////////////////////////////////////////////////////////////////////
1:         @Override
1:         @Override
/////////////////////////////////////////////////////////////////////////
1:     @Override
/////////////////////////////////////////////////////////////////////////
1:     @Override
/////////////////////////////////////////////////////////////////////////
0:                     @Override
/////////////////////////////////////////////////////////////////////////
0:     @Override
/////////////////////////////////////////////////////////////////////////
1:                     file.sync();
commit:97b12c7
/////////////////////////////////////////////////////////////////////////
0:                     file.getFD().sync();
commit:c50b6c3
/////////////////////////////////////////////////////////////////////////
0:                 	file.getChannel().force(false);
author:Christian Posta
-------------------------------------------------------------------------------
commit:023b2ac
/////////////////////////////////////////////////////////////////////////
commit:45e59e6
/////////////////////////////////////////////////////////////////////////
0:                     // will do batch preallocation on the journal if configured
0:                     if (journal.preallocationScope == Journal.PreallocationScope.BATCH) {
0:                         file.preallocateJournalBatch(journal, write.location.getSize());
1:                     }
1: 
/////////////////////////////////////////////////////////////////////////
0:                     journal.preallocateEntireJournalDataFile(file);
author:Dejan Bosanac
-------------------------------------------------------------------------------
commit:582af3e
/////////////////////////////////////////////////////////////////////////
1: import org.apache.activemq.util.RecoverableRandomAccessFile;
/////////////////////////////////////////////////////////////////////////
1:         RecoverableRandomAccessFile file = null;
/////////////////////////////////////////////////////////////////////////
0:             logger.info("Journal failed while writing at: " + wb.offset);
author:Hiram R. Chirino
-------------------------------------------------------------------------------
commit:c5cf038
commit:1aab71b
/////////////////////////////////////////////////////////////////////////
1: package org.apache.activemq.store.kahadb.disk.journal;
/////////////////////////////////////////////////////////////////////////
0: import org.apache.activemq.util.ByteSequence;
1: import org.apache.activemq.store.kahadb.disk.util.DataByteArrayOutputStream;
1: import org.apache.activemq.store.kahadb.disk.util.LinkedNodeList;
commit:715010a
commit:0bbc0ac
/////////////////////////////////////////////////////////////////////////
1:  * 
commit:f73b622
/////////////////////////////////////////////////////////////////////////
1: import java.util.zip.Adler32;
1: import java.util.zip.Checksum;
/////////////////////////////////////////////////////////////////////////
1:     protected final Journal journal;
/////////////////////////////////////////////////////////////////////////
0: 		private final int offset;
1:         public int size = Journal.BATCH_CONTROL_RECORD_SIZE;
0:         public WriteBatch(DataFile dataFile, int offset, WriteCommand write) throws IOException {
1: 			this.offset = offset;
1:             this.dataFile.incrementLength(Journal.BATCH_CONTROL_RECORD_SIZE);
1:             this.size=Journal.BATCH_CONTROL_RECORD_SIZE;
1:             journal.addToTotalLength(Journal.BATCH_CONTROL_RECORD_SIZE);
1:             append(write);
0:         public boolean canAppend(WriteCommand write) {
1:             int newSize = size + write.location.getSize();
1: 			if (newSize >= maxWriteBatchSize || offset+newSize > journal.getMaxFileLength() ) {
/////////////////////////////////////////////////////////////////////////
1:             write.location.setDataFileId(dataFile.getDataFileId());
1:             write.location.setOffset(offset+size);
1:             int s = write.location.getSize();
1: 			size += s;
1:             dataFile.incrementLength(s);
1:             journal.addToTotalLength(s);
/////////////////////////////////////////////////////////////////////////
1:         this.journal = dataManager;
1:         this.inflightWrites = this.journal.getInflightWrites();
/////////////////////////////////////////////////////////////////////////
0:         int size = data.getLength() + Journal.RECORD_HEAD_SPACE;
/////////////////////////////////////////////////////////////////////////
0:             batch = enqueue(write);
/////////////////////////////////////////////////////////////////////////
1:         } else {
0:         	inflightWrites.put(new WriteKey(location), write);
/////////////////////////////////////////////////////////////////////////
0:         int size = data.getLength() + Journal.RECORD_HEAD_SPACE;
/////////////////////////////////////////////////////////////////////////
0:             batch = enqueue(write);
0:         inflightWrites.put(new WriteKey(location), write);
0:     private WriteBatch enqueue(WriteCommand write) throws IOException {
/////////////////////////////////////////////////////////////////////////
1:             while ( true ) {
1: 	            if (nextWriteBatch == null) {
0: 	            	DataFile file = journal.getCurrentWriteFile();
0: 	            	if( file.getLength() > journal.getMaxFileLength() ) {
0: 	            		file = journal.rotateWriteFile();
1: 	            	}
1: 	            	
0: 	                nextWriteBatch = new WriteBatch(file, file.getLength(), write);
0: 	                rc = nextWriteBatch;
0: 	                enqueueMutex.notify();
0: 	                return rc;
1: 	            } else {
1: 	                // Append to current batch if possible..
1: 	                if (nextWriteBatch.canAppend(write)) {
1: 	                    nextWriteBatch.append(write);
1: 	                    return nextWriteBatch;
1: 	                } else {
1: 	                    // Otherwise wait for the queuedCommand to be null
1: 	                    try {
1: 	                        while (nextWriteBatch != null) {
1: 	                            enqueueMutex.wait();
1: 	                        }
1: 	                    } catch (InterruptedException e) {
1: 	                        throw new InterruptedIOException();
1: 	                    }
1: 	                    if (shutdown) {
0: 	                        throw new IOException("Async Writter Thread Shutdown");
1: 	                    }
1: 	                }
1: 	            }
/////////////////////////////////////////////////////////////////////////
0:                     if( file.length() < journal.preferedFileLength ) {
0:                         file.setLength(journal.preferedFileLength);
1:                 // Write an empty batch control record.
1:                 buff.reset();
0:                 buff.writeInt(Journal.BATCH_CONTROL_RECORD_SIZE);
0:                 buff.writeByte(Journal.BATCH_CONTROL_RECORD_TYPE);
0:                 buff.write(Journal.BATCH_CONTROL_RECORD_MAGIC);
0:                 buff.writeInt(0);
0:                 buff.writeLong(0);
1:                 
1:                 while (write != null) {
0:                     forceToDisk |= write.sync | write.onComplete != null;
1:                     buff.writeInt(write.location.getSize());
1:                     buff.writeByte(write.location.getType());
1:                     buff.write(write.data.getData(), write.data.getOffset(), write.data.getLength());
1:                     write = write.getNext();
1:                 ByteSequence sequence = buff.toByteSequence();
1:                 
1:                 // Now we can fill in the batch control record properly. 
1:                 buff.reset();
0:                 buff.skip(5+Journal.BATCH_CONTROL_RECORD_MAGIC.length);
0:                 buff.writeInt(sequence.getLength()-Journal.BATCH_CONTROL_RECORD_SIZE);
1:                 if( journal.isChecksum() ) {
1: 	                Checksum checksum = new Adler32();
0: 	                checksum.update(sequence.getData(), sequence.getOffset()+Journal.BATCH_CONTROL_RECORD_SIZE, sequence.getLength()-Journal.BATCH_CONTROL_RECORD_SIZE);
1: 	                buff.writeLong(checksum.getValue());
1:                 }
1: 
1:                 // Now do the 1 big write.
1:                 file.seek(wb.offset);
1:                 file.write(sequence.getData(), sequence.getOffset(), sequence.getLength());
1:                 
1:                 ReplicationTarget replicationTarget = journal.getReplicationTarget();
1:                 if( replicationTarget!=null ) {
1:                 	replicationTarget.replicate(wb.writes.getHead().location, sequence, forceToDisk);
1:                 }
1:                 
1:                 journal.setLastAppendLocation(lastWrite.location);
commit:456a2ba
/////////////////////////////////////////////////////////////////////////
1: /**
1:  * Licensed to the Apache Software Foundation (ASF) under one or more
1:  * contributor license agreements.  See the NOTICE file distributed with
1:  * this work for additional information regarding copyright ownership.
1:  * The ASF licenses this file to You under the Apache License, Version 2.0
1:  * (the "License"); you may not use this file except in compliance with
1:  * the License.  You may obtain a copy of the License at
1:  *
1:  *      http://www.apache.org/licenses/LICENSE-2.0
1:  *
1:  * Unless required by applicable law or agreed to in writing, software
1:  * distributed under the License is distributed on an "AS IS" BASIS,
1:  * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
1:  * See the License for the specific language governing permissions and
1:  * limitations under the License.
1:  */
0: package org.apache.kahadb.journal;
1: 
1: import java.io.IOException;
1: import java.io.InterruptedIOException;
0: import java.io.RandomAccessFile;
1: import java.util.Map;
1: import java.util.concurrent.CountDownLatch;
1: 
0: import org.apache.kahadb.util.ByteSequence;
0: import org.apache.kahadb.util.DataByteArrayOutputStream;
0: import org.apache.kahadb.util.LinkedNode;
0: import org.apache.kahadb.util.LinkedNodeList;
1: 
1: /**
1:  * An optimized writer to do batch appends to a data file. This object is thread
1:  * safe and gains throughput as you increase the number of concurrent writes it
1:  * does.
1:  * 
0:  * @version $Revision$
1:  */
0: class DataFileAppender {
1: 
0:     protected static final byte[] RESERVED_SPACE = new byte[Journal.ITEM_HEAD_RESERVED_SPACE];
0:     protected static final int DEFAULT_MAX_BATCH_SIZE = 1024 * 1024 * 4;
1: 
0:     protected final Journal dataManager;
0:     protected final Map<WriteKey, WriteCommand> inflightWrites;
0:     protected final Object enqueueMutex = new Object() {
1:     };
1:     protected WriteBatch nextWriteBatch;
1: 
1:     protected boolean shutdown;
1:     protected IOException firstAsyncException;
1:     protected final CountDownLatch shutdownDone = new CountDownLatch(1);
0:     protected int maxWriteBatchSize = DEFAULT_MAX_BATCH_SIZE;
1: 
0:     private boolean running;
1:     private Thread thread;
1: 
0:     public static class WriteKey {
0:         private final int file;
0:         private final long offset;
0:         private final int hash;
1: 
0:         public WriteKey(Location item) {
0:             file = item.getDataFileId();
0:             offset = item.getOffset();
0:             // TODO: see if we can build a better hash
0:             hash = (int)(file ^ offset);
1:         }
1: 
0:         public int hashCode() {
0:             return hash;
1:         }
1: 
0:         public boolean equals(Object obj) {
0:             if (obj instanceof WriteKey) {
0:                 WriteKey di = (WriteKey)obj;
0:                 return di.file == file && di.offset == offset;
1:             }
1:             return false;
1:         }
1:     }
1: 
1:     public class WriteBatch {
1: 
1:         public final DataFile dataFile;
1: 
0:         public final LinkedNodeList<WriteCommand> writes = new LinkedNodeList<WriteCommand>();
1:         public final CountDownLatch latch = new CountDownLatch(1);
0:         public int size;
1: 
0:         public WriteBatch(DataFile dataFile, WriteCommand write) throws IOException {
1:             this.dataFile = dataFile;
1:             this.writes.addLast(write);
0:             size += write.location.getSize();
1:         }
1: 
0:         public boolean canAppend(DataFile dataFile, WriteCommand write) {
0:             if (dataFile != this.dataFile) {
1:                 return false;
1:             }
0:             if (size + write.location.getSize() >= maxWriteBatchSize) {
1:                 return false;
1:             }
1:             return true;
1:         }
1: 
0:         public void append(WriteCommand write) throws IOException {
1:             this.writes.addLast(write);
0:             size += write.location.getSize();
1:         }
1:     }
1: 
0:     public static class WriteCommand extends LinkedNode<WriteCommand> {
0:         public final Location location;
0:         public final ByteSequence data;
0:         final boolean sync;
0:         public final Runnable onComplete;
1: 
0:         public WriteCommand(Location location, ByteSequence data, boolean sync) {
0:             this.location = location;
0:             this.data = data;
0:             this.sync = sync;
0:             this.onComplete = null;
1:         }
1: 
0:         public WriteCommand(Location location, ByteSequence data, Runnable onComplete) {
0:             this.location = location;
0:             this.data = data;
0:             this.onComplete = onComplete;
0:             this.sync = false;
1:         }
1:     }
1: 
1:     /**
1:      * Construct a Store writer
1:      * 
0:      * @param fileId
1:      */
1:     public DataFileAppender(Journal dataManager) {
0:         this.dataManager = dataManager;
0:         this.inflightWrites = this.dataManager.getInflightWrites();
1:     }
1: 
1:     /**
0:      * @param type
0:      * @param marshaller
0:      * @param payload
0:      * @param type
0:      * @param sync
0:      * @return
0:      * @throws IOException
0:      * @throws
0:      * @throws
1:      */
1:     public Location storeItem(ByteSequence data, byte type, boolean sync) throws IOException {
1: 
1:         // Write the packet our internal buffer.
0:         int size = data.getLength() + Journal.ITEM_HEAD_FOOT_SPACE;
1: 
1:         final Location location = new Location();
1:         location.setSize(size);
1:         location.setType(type);
1: 
0:         WriteBatch batch;
0:         WriteCommand write = new WriteCommand(location, data, sync);
1: 
0:         // Locate datafile and enqueue into the executor in sychronized block so
0:         // that writes get equeued onto the executor in order that they were
0:         // assigned
0:         // by the data manager (which is basically just appending)
1: 
0:         synchronized (this) {
0:             // Find the position where this item will land at.
0:             DataFile dataFile = dataManager.allocateLocation(location);
0:             if (!sync) {
0:                 inflightWrites.put(new WriteKey(location), write);
1:             }
0:             batch = enqueue(dataFile, write);
1:         }
0:         location.setLatch(batch.latch);
1:         if (sync) {
1:             try {
1:                 batch.latch.await();
1:             } catch (InterruptedException e) {
1:                 throw new InterruptedIOException();
1:             }
1:         }
1: 
1:         return location;
1:     }
1: 
1:     public Location storeItem(ByteSequence data, byte type, Runnable onComplete) throws IOException {
1:         // Write the packet our internal buffer.
0:         int size = data.getLength() + Journal.ITEM_HEAD_FOOT_SPACE;
1: 
1:         final Location location = new Location();
1:         location.setSize(size);
1:         location.setType(type);
1: 
0:         WriteBatch batch;
0:         WriteCommand write = new WriteCommand(location, data, onComplete);
1: 
0:         // Locate datafile and enqueue into the executor in sychronized block so
0:         // that writes get equeued onto the executor in order that they were
0:         // assigned
0:         // by the data manager (which is basically just appending)
1: 
0:         synchronized (this) {
0:             // Find the position where this item will land at.
0:             DataFile dataFile = dataManager.allocateLocation(location);
0:             inflightWrites.put(new WriteKey(location), write);
0:             batch = enqueue(dataFile, write);
1:         }
0:         location.setLatch(batch.latch);
1: 
1:         return location;
1:     }
1: 
0:     private WriteBatch enqueue(DataFile dataFile, WriteCommand write) throws IOException {
1:         synchronized (enqueueMutex) {
0:             WriteBatch rc = null;
1:             if (shutdown) {
0:                 throw new IOException("Async Writter Thread Shutdown");
1:             }
0:             if (firstAsyncException != null) {
0:                 throw firstAsyncException;
1:             }
1: 
1:             if (!running) {
1:                 running = true;
1:                 thread = new Thread() {
1:                     public void run() {
1:                         processQueue();
1:                     }
1:                 };
1:                 thread.setPriority(Thread.MAX_PRIORITY);
1:                 thread.setDaemon(true);
1:                 thread.setName("ActiveMQ Data File Writer");
1:                 thread.start();
1:             }
1: 
0:             if (nextWriteBatch == null) {
0:                 nextWriteBatch = new WriteBatch(dataFile, write);
0:                 rc = nextWriteBatch;
0:                 enqueueMutex.notify();
1:             } else {
0:                 // Append to current batch if possible..
0:                 if (nextWriteBatch.canAppend(dataFile, write)) {
0:                     nextWriteBatch.append(write);
0:                     rc = nextWriteBatch;
1:                 } else {
0:                     // Otherwise wait for the queuedCommand to be null
1:                     try {
0:                         while (nextWriteBatch != null) {
1:                             enqueueMutex.wait();
1:                         }
1:                     } catch (InterruptedException e) {
1:                         throw new InterruptedIOException();
1:                     }
1:                     if (shutdown) {
0:                         throw new IOException("Async Writter Thread Shutdown");
1:                     }
1: 
0:                     // Start a new batch.
0:                     nextWriteBatch = new WriteBatch(dataFile, write);
0:                     rc = nextWriteBatch;
0:                     enqueueMutex.notify();
1:                 }
1:             }
0:             return rc;
1:         }
1:     }
1: 
1:     public void close() throws IOException {
1:         synchronized (enqueueMutex) {
1:             if (!shutdown) {
1:                 shutdown = true;
1:                 if (running) {
1:                     enqueueMutex.notifyAll();
1:                 } else {
1:                     shutdownDone.countDown();
1:                 }
1:             }
1:         }
1: 
1:         try {
1:             shutdownDone.await();
1:         } catch (InterruptedException e) {
1:             throw new InterruptedIOException();
1:         }
1: 
1:     }
1: 
1:     /**
1:      * The async processing loop that writes to the data files and does the
1:      * force calls. Since the file sync() call is the slowest of all the
1:      * operations, this algorithm tries to 'batch' or group together several
1:      * file sync() requests into a single file sync() call. The batching is
1:      * accomplished attaching the same CountDownLatch instance to every force
1:      * request in a group.
1:      */
1:     protected void processQueue() {
1:         DataFile dataFile = null;
0:         RandomAccessFile file = null;
1:         try {
1: 
0:             DataByteArrayOutputStream buff = new DataByteArrayOutputStream(maxWriteBatchSize);
1:             while (true) {
1: 
0:                 Object o = null;
1: 
1:                 // Block till we get a command.
1:                 synchronized (enqueueMutex) {
1:                     while (true) {
1:                         if (nextWriteBatch != null) {
0:                             o = nextWriteBatch;
1:                             nextWriteBatch = null;
1:                             break;
1:                         }
1:                         if (shutdown) {
1:                             return;
1:                         }
1:                         enqueueMutex.wait();
1:                     }
0:                     enqueueMutex.notify();
1:                 }
1: 
0:                 WriteBatch wb = (WriteBatch)o;
1:                 if (dataFile != wb.dataFile) {
1:                     if (file != null) {
0:                         file.setLength(dataFile.getLength());
1:                         dataFile.closeRandomAccessFile(file);
1:                     }
1:                     dataFile = wb.dataFile;
0:                     file = dataFile.openRandomAccessFile();
0:                     if( file.length() < dataManager.preferedFileLength ) {
0:                         file.setLength(dataManager.preferedFileLength);
1:                     }
1:                 }
1: 
0:                 WriteCommand write = wb.writes.getHead();
1: 
0:                 // Write all the data.
0:                 // Only need to seek to first location.. all others
0:                 // are in sequence.
0:                 file.seek(write.location.getOffset());
1: 
1:                 boolean forceToDisk = false;
1: 
0:                 // 
0:                 // is it just 1 big write?
0:                 ReplicationTarget replicationTarget = dataManager.getReplicationTarget();
0:                 if (wb.size == write.location.getSize() && replicationTarget==null) {
0:                     forceToDisk = write.sync | write.onComplete != null;
1: 
0:                     // Just write it directly..
0:                     file.writeInt(write.location.getSize());
0:                     file.writeByte(write.location.getType());
0:                     file.write(RESERVED_SPACE);
0:                     file.write(Journal.ITEM_HEAD_SOR);
0:                     file.write(write.data.getData(), write.data.getOffset(), write.data.getLength());
0:                     file.write(Journal.ITEM_HEAD_EOR);
1: 
1:                 } else {
1: 
0:                     // We are going to do 1 big write.
1:                     while (write != null) {
0:                         forceToDisk |= write.sync | write.onComplete != null;
1: 
0:                         buff.writeInt(write.location.getSize());
0:                         buff.writeByte(write.location.getType());
0:                         buff.write(RESERVED_SPACE);
0:                         buff.write(Journal.ITEM_HEAD_SOR);
0:                         buff.write(write.data.getData(), write.data.getOffset(), write.data.getLength());
0:                         buff.write(Journal.ITEM_HEAD_EOR);
1: 
1:                         write = write.getNext();
1:                     }
1: 
0:                     // Now do the 1 big write.
0:                     ByteSequence sequence = buff.toByteSequence();
0:                     file.write(sequence.getData(), sequence.getOffset(), sequence.getLength());
1:                     
0:                     if( replicationTarget!=null ) {
0:                     	replicationTarget.replicate(wb.writes.getHead().location, sequence, forceToDisk);
1:                     }
1:                     
0:                     buff.reset();
1:                 }
1: 
1:                 if (forceToDisk) {
0:                     file.getFD().sync();
1:                 }
1: 
0:                 WriteCommand lastWrite = wb.writes.getTail();
0:                 dataManager.setLastAppendLocation(lastWrite.location);
1: 
1:                 // Now that the data is on disk, remove the writes from the in
1:                 // flight
1:                 // cache.
0:                 write = wb.writes.getHead();
1:                 while (write != null) {
1:                     if (!write.sync) {
0:                         inflightWrites.remove(new WriteKey(write.location));
1:                     }
0:                     if (write.onComplete != null) {
1:                         try {
1:                             write.onComplete.run();
1:                         } catch (Throwable e) {
0:                             e.printStackTrace();
1:                         }
1:                     }
1:                     write = write.getNext();
1:                 }
1: 
1:                 // Signal any waiting threads that the write is on disk.
1:                 wb.latch.countDown();
1:             }
0:         } catch (IOException e) {
1:             synchronized (enqueueMutex) {
0:                 firstAsyncException = e;
1:             }
1:         } catch (InterruptedException e) {
1:         } finally {
1:             try {
1:                 if (file != null) {
1:                     dataFile.closeRandomAccessFile(file);
1:                 }
1:             } catch (Throwable ignore) {
1:             }
1:             shutdownDone.countDown();
1:         }
1:     }
1: 
1: }
author:Timothy A. Bish
-------------------------------------------------------------------------------
commit:cdba931
/////////////////////////////////////////////////////////////////////////
1: import org.slf4j.Logger;
1: import org.slf4j.LoggerFactory;
1:     private static final Logger logger = LoggerFactory.getLogger(DataFileAppender.class);
0: 
1:     protected final Object enqueueMutex = new Object();
/////////////////////////////////////////////////////////////////////////
0:             this.offset = offset;
/////////////////////////////////////////////////////////////////////////
0:             if (newSize >= maxWriteBatchSize || offset+newSize > journal.getMaxFileLength() ) {
/////////////////////////////////////////////////////////////////////////
0:             size += s;
/////////////////////////////////////////////////////////////////////////
0: 
/////////////////////////////////////////////////////////////////////////
1:             IOException exception = batch.exception.get();
1:                 throw exception;
0:         }
/////////////////////////////////////////////////////////////////////////
0: 
/////////////////////////////////////////////////////////////////////////
0: 
/////////////////////////////////////////////////////////////////////////
0: 
0:                 if (nextWriteBatch == null) {
0:                     DataFile file = journal.getCurrentWriteFile();
0:                     if( file.getLength() > journal.getMaxFileLength() ) {
0:                         file = journal.rotateWriteFile();
0:                     }
1:                     nextWriteBatch = newWriteBatch(write, file);
1:                     enqueueMutex.notifyAll();
1:                     break;
0:                 } else {
0:                     // Append to current batch if possible..
0:                     if (nextWriteBatch.canAppend(write)) {
0:                         nextWriteBatch.append(write);
1:                         break;
0:                     } else {
0:                         // Otherwise wait for the queuedCommand to be null
0:                         try {
0:                             while (nextWriteBatch != null) {
1:                                 final long start = System.currentTimeMillis();
0:                                 enqueueMutex.wait();
1:                                 if (maxStat > 0) {
0:                                     logger.info("Watiting for write to finish with full batch... millis: " +
1:                                                 (System.currentTimeMillis() - start));
0:                                }
0:                             }
0:                         } catch (InterruptedException e) {
0:                             throw new InterruptedIOException();
0:                         }
0:                         if (shutdown) {
0:                             throw new IOException("Async Writter Thread Shutdown");
0:                         }
0:                     }
0:                 }
/////////////////////////////////////////////////////////////////////////
1:                             wb = nextWriteBatch;
/////////////////////////////////////////////////////////////////////////
/////////////////////////////////////////////////////////////////////////
0: 
0:                 // Now we can fill in the batch control record properly.
0:                     Checksum checksum = new Adler32();
0:                     checksum.update(sequence.getData(), sequence.getOffset()+Journal.BATCH_CONTROL_RECORD_SIZE, sequence.getLength()-Journal.BATCH_CONTROL_RECORD_SIZE);
0:                     buff.writeLong(checksum.getValue());
/////////////////////////////////////////////////////////////////////////
1:                         logger.info("Ave writeSize: " + all/maxStat);
0: 
0:                     replicationTarget.replicate(wb.writes.getHead().location, sequence, forceToDisk);
0: 
/////////////////////////////////////////////////////////////////////////
1:                     logger.info("Add exception was raised while executing the run command for onComplete", e);
author:Gary Tully
-------------------------------------------------------------------------------
commit:89f22da
/////////////////////////////////////////////////////////////////////////
1:     protected final boolean syncOnComplete;
1:     protected boolean running;
/////////////////////////////////////////////////////////////////////////
1:         protected final int offset;
1:         public WriteBatch(DataFile dataFile,int offset) {
0:         }
0: 
1:         public WriteBatch(DataFile dataFile, int offset, Journal.WriteCommand write) throws IOException {
1:             this(dataFile, offset);
/////////////////////////////////////////////////////////////////////////
1:         this.syncOnComplete = this.journal.isEnableAsyncDiskSync();
/////////////////////////////////////////////////////////////////////////
0: 	                nextWriteBatch = newWriteBatch(write, file);
/////////////////////////////////////////////////////////////////////////
1:     protected WriteBatch newWriteBatch(Journal.WriteCommand write, DataFile file) throws IOException {
1:         return new WriteBatch(file, file.getLength(), write);
0:     }
0: 
/////////////////////////////////////////////////////////////////////////
/////////////////////////////////////////////////////////////////////////
1:                     forceToDisk |= write.sync | (syncOnComplete && write.onComplete != null);
/////////////////////////////////////////////////////////////////////////
1:                 signalDone(wb);
/////////////////////////////////////////////////////////////////////////
1:     protected void signalDone(WriteBatch wb) {
0:         // Now that the data is on disk, remove the writes from the in
0:         // flight
0:         // cache.
1:         Journal.WriteCommand write = wb.writes.getHead();
0:         while (write != null) {
1:             if (!write.sync) {
1:                 inflightWrites.remove(new Journal.WriteKey(write.location));
0:             }
0:             if (write.onComplete != null) {
0:                 try {
0:                     write.onComplete.run();
0:                 } catch (Throwable e) {
0:                     e.printStackTrace();
0:                 }
0:             }
0:             write = write.getNext();
0:         }
0: 
0:         // Signal any waiting threads that the write is on disk.
0:         wb.latch.countDown();
0:     }
commit:bb4a2f7
/////////////////////////////////////////////////////////////////////////
/////////////////////////////////////////////////////////////////////////
1: class DataFileAppender implements FileAppender {
1:     protected final Map<Journal.WriteKey, Journal.WriteCommand> inflightWrites;
/////////////////////////////////////////////////////////////////////////
1:         public final LinkedNodeList<Journal.WriteCommand> writes = new LinkedNodeList<Journal.WriteCommand>();
0:         public WriteBatch(DataFile dataFile, int offset, Journal.WriteCommand write) throws IOException {
/////////////////////////////////////////////////////////////////////////
1:         public boolean canAppend(Journal.WriteCommand write) {
/////////////////////////////////////////////////////////////////////////
1:         public void append(Journal.WriteCommand write) throws IOException {
/////////////////////////////////////////////////////////////////////////
/////////////////////////////////////////////////////////////////////////
1:         Journal.WriteCommand write = new Journal.WriteCommand(location, data, sync);
/////////////////////////////////////////////////////////////////////////
1:         Journal.WriteCommand write = new Journal.WriteCommand(location, data, onComplete);
/////////////////////////////////////////////////////////////////////////
1:     private WriteBatch enqueue(Journal.WriteCommand write) throws IOException {
/////////////////////////////////////////////////////////////////////////
0: 
/////////////////////////////////////////////////////////////////////////
1:                 inflightWrites.put(new Journal.WriteKey(write.location), write);
/////////////////////////////////////////////////////////////////////////
1:                 Journal.WriteCommand write = wb.writes.getHead();
/////////////////////////////////////////////////////////////////////////
0: 
/////////////////////////////////////////////////////////////////////////
1:                 Journal.WriteCommand lastWrite = wb.writes.getTail();
/////////////////////////////////////////////////////////////////////////
0:                         inflightWrites.remove(new Journal.WriteKey(write.location));
commit:9e40b91
/////////////////////////////////////////////////////////////////////////
/////////////////////////////////////////////////////////////////////////
/////////////////////////////////////////////////////////////////////////
0:                     wb.latch.countDown();
0:                     nextWriteBatch.exception.set(e);
0:                     nextWriteBatch.latch.countDown();
commit:958038e
/////////////////////////////////////////////////////////////////////////
1:         WriteBatch batch = enqueue(write);
/////////////////////////////////////////////////////////////////////////
0:         WriteBatch batch = enqueue(write);
commit:76f842d
/////////////////////////////////////////////////////////////////////////
0: 	                            final long start = System.currentTimeMillis();
1: 	                            if (maxStat > 0) { 
0: 	                                System.err.println("Watiting for write to finish with full batch... millis: " + (System.currentTimeMillis() - start));
0: 	                            }
/////////////////////////////////////////////////////////////////////////
0:     public static final String PROPERTY_LOG_WRITE_STAT_WINDOW = "org.apache.kahadb.journal.appender.WRITE_STAT_WINDOW";
0:     public static final int maxStat = Integer.parseInt(System.getProperty(PROPERTY_LOG_WRITE_STAT_WINDOW, "0"));
1:     int statIdx = 0;
1:     int[] stats = new int[maxStat];
/////////////////////////////////////////////////////////////////////////
0:                 if (maxStat > 0) {
1:                     if (statIdx < maxStat) {
1:                         stats[statIdx++] = sequence.getLength();
0:                     } else {
1:                         long all = 0;
1:                         for (;statIdx > 0;) {
1:                             all+= stats[--statIdx];
0:                         }
0:                         System.err.println("Ave writeSize: " + all/maxStat);
0:                     }
0:                 }
commit:4b49834
/////////////////////////////////////////////////////////////////////////
1: 	                enqueueMutex.notifyAll();
/////////////////////////////////////////////////////////////////////////
0:                     enqueueMutex.notifyAll();
commit:561cda1
/////////////////////////////////////////////////////////////////////////
/////////////////////////////////////////////////////////////////////////
1:     protected int maxWriteBatchSize;
/////////////////////////////////////////////////////////////////////////
1:         this.maxWriteBatchSize = this.journal.getWriteBatchSize();
commit:d458a5d
/////////////////////////////////////////////////////////////////////////
/////////////////////////////////////////////////////////////////////////
0:         }	
/////////////////////////////////////////////////////////////////////////
0:  
/////////////////////////////////////////////////////////////////////////
0: 	                break;
0: 	                    break;
/////////////////////////////////////////////////////////////////////////
0:             if (!write.sync) {
0:                 inflightWrites.put(new WriteKey(write.location), write);
0:             }
0:             return nextWriteBatch;
commit:d98ea37
/////////////////////////////////////////////////////////////////////////
0:         if (!sync) {
0:             inflightWrites.put(new WriteKey(location), write);
0:         }
/////////////////////////////////////////////////////////////////////////
0:         	
author:Bosanac Dejan
-------------------------------------------------------------------------------
commit:a19e27a
/////////////////////////////////////////////////////////////////////////
0:             
/////////////////////////////////////////////////////////////////////////
0:                 firstAsyncException = null;
0:             }
0:             
0:             if (firstAsyncException != null) {
0:                 throw firstAsyncException;
/////////////////////////////////////////////////////////////////////////
1:             running = false;
commit:6a4e25c
/////////////////////////////////////////////////////////////////////////
1: import java.util.concurrent.atomic.AtomicReference;
/////////////////////////////////////////////////////////////////////////
1:         public AtomicReference<IOException> exception = new AtomicReference<IOException>();
/////////////////////////////////////////////////////////////////////////
0:             IOException exception = batch.exception.get(); 
1:             if (exception != null) {
0:             	throw exception;
0:             }
/////////////////////////////////////////////////////////////////////////
0:                 if (wb != null) {
0:                     wb.latch.countDown();
0:                     wb.exception.set(e);
0:                 }
0:                 if (nextWriteBatch != null) {
0:             	    nextWriteBatch.latch.countDown();
0:             	    nextWriteBatch.exception.set(e);
0:                 }
commit:b1a9130
/////////////////////////////////////////////////////////////////////////
0:     	
/////////////////////////////////////////////////////////////////////////
1:         WriteBatch wb = null;
/////////////////////////////////////////////////////////////////////////
0:                 wb = (WriteBatch)o;
/////////////////////////////////////////////////////////////////////////
0:         	if (wb != null) {
0:         		wb.latch.countDown();
0:         	}
============================================================================