1:2518bde: /**
1:2518bde:  * Licensed to the Apache Software Foundation (ASF) under one or more
1:2518bde:  * contributor license agreements.  See the NOTICE file distributed with
1:2518bde:  * this work for additional information regarding copyright ownership.
1:2518bde:  * The ASF licenses this file to You under the Apache License, Version 2.0
1:2518bde:  * (the "License"); you may not use this file except in compliance with
1:2518bde:  * the License.  You may obtain a copy of the License at
1:2518bde:  *
1:2518bde:  *      http://www.apache.org/licenses/LICENSE-2.0
1:2518bde:  *
1:2518bde:  * Unless required by applicable law or agreed to in writing, software
1:2518bde:  * distributed under the License is distributed on an "AS IS" BASIS,
1:2518bde:  * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
1:2518bde:  * See the License for the specific language governing permissions and
1:2518bde:  * limitations under the License.
1:2518bde:  */
1:2518bde: package org.apache.activemq.store.kahadb;
1:2518bde: 
1:2518bde: import org.apache.activemq.ActiveMQConnectionFactory;
1:2518bde: import org.apache.activemq.broker.BrokerService;
1:2518bde: import org.apache.activemq.broker.region.policy.PolicyEntry;
1:2518bde: import org.apache.activemq.broker.region.policy.PolicyMap;
1:2518bde: import org.apache.activemq.command.ActiveMQQueue;
1:2518bde: import org.apache.activemq.store.kahadb.disk.journal.DataFile;
1:eb1a3eb: import org.apache.activemq.store.kahadb.disk.journal.Journal;
1:eb1a3eb: import org.apache.activemq.util.ByteSequence;
1:2518bde: import org.apache.activemq.util.RecoverableRandomAccessFile;
1:2518bde: import org.junit.After;
1:2518bde: import org.junit.Test;
1:2518bde: import org.junit.runner.RunWith;
1:2518bde: import org.junit.runners.Parameterized;
1:2518bde: import org.slf4j.Logger;
1:2518bde: import org.slf4j.LoggerFactory;
1:2518bde: 
1:2518bde: import javax.jms.Connection;
1:2518bde: import javax.jms.Destination;
1:2518bde: import javax.jms.Message;
1:2518bde: import javax.jms.MessageProducer;
1:2518bde: import javax.jms.Session;
1:2518bde: import java.io.IOException;
1:2518bde: import java.util.Arrays;
1:2518bde: import java.util.Collection;
1:2518bde: 
1:2518bde: import static org.apache.activemq.store.kahadb.JournalCorruptionEofIndexRecoveryTest.drain;
1:2518bde: import static org.junit.Assert.assertEquals;
1:2518bde: import static org.junit.Assert.assertTrue;
1:2518bde: 
1:2518bde: @RunWith(Parameterized.class)
1:2518bde: public class JournalCorruptionExceptionTest {
1:2518bde: 
1:2518bde:     private static final Logger LOG = LoggerFactory.getLogger(JournalCorruptionExceptionTest.class);
1:2518bde: 
1:2518bde:     private final String KAHADB_DIRECTORY = "target/activemq-data/";
1:2518bde:     private final String payload = new String(new byte[1024]);
1:2518bde: 
1:2518bde:     private ActiveMQConnectionFactory cf = null;
1:2518bde:     private BrokerService broker = null;
1:2518bde:     private final Destination destination = new ActiveMQQueue("Test");
1:2518bde:     private String connectionUri;
1:2518bde:     private KahaDBPersistenceAdapter adapter;
1:2518bde: 
1:2518bde:     @Parameterized.Parameter(0)
1:2518bde:     public byte fill = Byte.valueOf("3");
1:2518bde: 
1:2518bde:     @Parameterized.Parameter(1)
1:2518bde:     public int fillLength = 10;
1:2518bde: 
1:2518bde:     @Parameterized.Parameters(name = "fill=#{0},#{1}")
1:2518bde:     public static Iterable<Object[]> parameters() {
1:2518bde:         // corruption can be valid record type values
1:2518bde:         return Arrays.asList(new Object[][]{
1:eb1a3eb:                 {Byte.valueOf("0"), 6},
1:eb1a3eb:                 {Byte.valueOf("1"), 8},
1:eb1a3eb:                 {Byte.valueOf("-1"), 6},
1:2518bde:                 {Byte.valueOf("0"), 10},
1:2518bde:                 {Byte.valueOf("1"), 10},
1:eb1a3eb:                 {Byte.valueOf("2"), 10},
1:2518bde:                 {Byte.valueOf("-1"), 10}
1:2518bde:         });
1:2518bde:     }
1:2518bde: 
1:2518bde:     protected void startBroker() throws Exception {
1:2518bde:         doStartBroker(true);
1:2518bde:     }
1:2518bde: 
1:2518bde:     private void doStartBroker(boolean delete) throws Exception {
1:2518bde:         broker = new BrokerService();
1:2518bde:         broker.setDeleteAllMessagesOnStartup(delete);
1:2518bde:         broker.setPersistent(true);
1:2518bde:         broker.setUseJmx(true);
1:2518bde:         broker.setDataDirectory(KAHADB_DIRECTORY);
1:2518bde:         broker.addConnector("tcp://localhost:0");
1:2518bde: 
1:2518bde:         PolicyMap policyMap = new PolicyMap();
1:2518bde:         PolicyEntry defaultEntry = new PolicyEntry();
1:2518bde:         defaultEntry.setUseCache(false);
1:053c38f:         defaultEntry.setExpireMessagesPeriod(0);
1:2518bde:         policyMap.setDefaultEntry(defaultEntry);
1:2518bde:         broker.setDestinationPolicy(policyMap);
1:2518bde: 
1:2518bde:         configurePersistence(broker);
1:2518bde: 
1:2518bde:         connectionUri = "vm://localhost?create=false";
1:2518bde:         cf = new ActiveMQConnectionFactory(connectionUri);
1:2518bde: 
1:2518bde:         broker.start();
1:2518bde:         LOG.info("Starting broker..");
1:2518bde:     }
1:2518bde: 
1:2518bde:     protected void configurePersistence(BrokerService brokerService) throws Exception {
1:2518bde:         adapter = (KahaDBPersistenceAdapter) brokerService.getPersistenceAdapter();
1:2518bde: 
1:2518bde:         // ensure there are a bunch of data files but multiple entries in each
1:2518bde:         adapter.setJournalMaxFileLength(1024 * 20);
1:2518bde: 
1:2518bde:     }
1:2518bde: 
1:2518bde:     @After
1:2518bde:     public void tearDown() throws Exception {
1:2518bde:         if (broker != null) {
1:2518bde:             broker.stop();
1:2518bde:             broker.waitUntilStopped();
1:2518bde:         }
1:2518bde:     }
1:2518bde: 
1:2518bde:     @Test
1:2518bde:     public void testIOExceptionOnCorruptJournalLocationRead() throws Exception {
1:2518bde:         startBroker();
1:2518bde: 
1:2518bde:         produceMessagesToConsumeMultipleDataFiles(50);
1:2518bde: 
1:2518bde:         int numFiles = getNumberOfJournalFiles();
1:2518bde: 
1:2518bde:         assertTrue("more than x files: " + numFiles, numFiles > 4);
1:2518bde: 
1:2518bde:         corruptLocationAtDataFileIndex(3);
1:2518bde: 
1:2518bde:         assertEquals("missing one message", 50, broker.getAdminView().getTotalMessageCount());
1:2518bde:         assertEquals("Drain", 0, drainQueue(50));
1:2518bde:         assertTrue("broker stopping", broker.isStopping());
1:2518bde:     }
1:2518bde: 
1:2518bde: 
1:2518bde:     private void corruptLocationAtDataFileIndex(int id) throws IOException {
1:2518bde: 
1:2518bde:         Collection<DataFile> files = ((KahaDBPersistenceAdapter) broker.getPersistenceAdapter()).getStore().getJournal().getFileMap().values();
1:2518bde:         DataFile dataFile = (DataFile) files.toArray()[id];
1:2518bde: 
1:2518bde:         RecoverableRandomAccessFile randomAccessFile = dataFile.openRandomAccessFile();
1:2518bde: 
1:eb1a3eb:         final ByteSequence header = new ByteSequence(Journal.BATCH_CONTROL_RECORD_HEADER);
1:eb1a3eb:         byte data[] = new byte[1024 * 20];
1:eb1a3eb:         ByteSequence bs = new ByteSequence(data, 0, randomAccessFile.read(data, 0, data.length));
1:eb1a3eb: 
1:eb1a3eb:         int offset = bs.indexOf(header, 0);
1:eb1a3eb: 
1:eb1a3eb:         offset += Journal.BATCH_CONTROL_RECORD_SIZE;
1:eb1a3eb: 
1:eb1a3eb:         if (fillLength >= 10) {
1:eb1a3eb:             offset += 4; // location size
1:eb1a3eb:             offset += 1; // location type
1:2518bde:         }
1:2518bde: 
1:2518bde:         LOG.info("Whacking batch record in file:" + id + ", at offset: " + offset + " with fill:" + fill);
1:2518bde:         // whack that record
1:2518bde:         byte[] bla = new byte[fillLength];
1:2518bde:         Arrays.fill(bla, fill);
1:2518bde:         randomAccessFile.seek(offset);
1:2518bde:         randomAccessFile.write(bla, 0, bla.length);
1:2518bde:         randomAccessFile.getFD().sync();
1:2518bde:     }
1:2518bde: 
1:2518bde:     private int getNumberOfJournalFiles() throws IOException {
1:2518bde:         Collection<DataFile> files = ((KahaDBPersistenceAdapter) broker.getPersistenceAdapter()).getStore().getJournal().getFileMap().values();
1:2518bde:         int reality = 0;
1:2518bde:         for (DataFile file : files) {
1:2518bde:             if (file != null) {
1:2518bde:                 reality++;
1:2518bde:             }
1:2518bde:         }
1:2518bde:         return reality;
1:2518bde:     }
1:2518bde: 
1:2518bde:     private int produceMessages(Destination destination, int numToSend) throws Exception {
1:2518bde:         int sent = 0;
1:2518bde:         Connection connection = new ActiveMQConnectionFactory(broker.getTransportConnectors().get(0).getConnectUri()).createConnection();
1:2518bde:         connection.start();
1:2518bde:         try {
1:2518bde:             Session session = connection.createSession(false, Session.AUTO_ACKNOWLEDGE);
1:2518bde:             MessageProducer producer = session.createProducer(destination);
1:2518bde:             for (int i = 0; i < numToSend; i++) {
1:2518bde:                 producer.send(createMessage(session, i));
1:2518bde:                 sent++;
1:2518bde:             }
1:2518bde:         } finally {
1:2518bde:             connection.close();
1:2518bde:         }
1:2518bde: 
1:2518bde:         return sent;
1:2518bde:     }
1:2518bde: 
1:2518bde:     private int produceMessagesToConsumeMultipleDataFiles(int numToSend) throws Exception {
1:2518bde:         return produceMessages(destination, numToSend);
1:2518bde:     }
1:2518bde: 
1:2518bde:     private Message createMessage(Session session, int i) throws Exception {
1:2518bde:         return session.createTextMessage(payload + "::" + i);
1:2518bde:     }
1:2518bde: 
1:2518bde:     private int drainQueue(int max) throws Exception {
1:2518bde:         return drain(cf, destination, max);
1:2518bde:     }
1:2518bde: 
1:2518bde: }
============================================================================
author:gtully
-------------------------------------------------------------------------------
commit:eb1a3eb
/////////////////////////////////////////////////////////////////////////
1: import org.apache.activemq.store.kahadb.disk.journal.Journal;
1: import org.apache.activemq.util.ByteSequence;
/////////////////////////////////////////////////////////////////////////
1:                 {Byte.valueOf("0"), 6},
1:                 {Byte.valueOf("1"), 8},
1:                 {Byte.valueOf("-1"), 6},
1:                 {Byte.valueOf("2"), 10},
/////////////////////////////////////////////////////////////////////////
1:         final ByteSequence header = new ByteSequence(Journal.BATCH_CONTROL_RECORD_HEADER);
1:         byte data[] = new byte[1024 * 20];
1:         ByteSequence bs = new ByteSequence(data, 0, randomAccessFile.read(data, 0, data.length));
1: 
1:         int offset = bs.indexOf(header, 0);
1: 
1:         offset += Journal.BATCH_CONTROL_RECORD_SIZE;
1: 
1:         if (fillLength >= 10) {
1:             offset += 4; // location size
1:             offset += 1; // location type
commit:053c38f
/////////////////////////////////////////////////////////////////////////
1:         defaultEntry.setExpireMessagesPeriod(0);
commit:2518bde
/////////////////////////////////////////////////////////////////////////
1: /**
1:  * Licensed to the Apache Software Foundation (ASF) under one or more
1:  * contributor license agreements.  See the NOTICE file distributed with
1:  * this work for additional information regarding copyright ownership.
1:  * The ASF licenses this file to You under the Apache License, Version 2.0
1:  * (the "License"); you may not use this file except in compliance with
1:  * the License.  You may obtain a copy of the License at
1:  *
1:  *      http://www.apache.org/licenses/LICENSE-2.0
1:  *
1:  * Unless required by applicable law or agreed to in writing, software
1:  * distributed under the License is distributed on an "AS IS" BASIS,
1:  * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
1:  * See the License for the specific language governing permissions and
1:  * limitations under the License.
1:  */
1: package org.apache.activemq.store.kahadb;
1: 
1: import org.apache.activemq.ActiveMQConnectionFactory;
1: import org.apache.activemq.broker.BrokerService;
1: import org.apache.activemq.broker.region.policy.PolicyEntry;
1: import org.apache.activemq.broker.region.policy.PolicyMap;
1: import org.apache.activemq.command.ActiveMQQueue;
1: import org.apache.activemq.store.kahadb.disk.journal.DataFile;
1: import org.apache.activemq.util.RecoverableRandomAccessFile;
1: import org.junit.After;
1: import org.junit.Test;
1: import org.junit.runner.RunWith;
1: import org.junit.runners.Parameterized;
1: import org.slf4j.Logger;
1: import org.slf4j.LoggerFactory;
1: 
1: import javax.jms.Connection;
1: import javax.jms.Destination;
1: import javax.jms.Message;
1: import javax.jms.MessageProducer;
1: import javax.jms.Session;
1: import java.io.IOException;
1: import java.util.Arrays;
1: import java.util.Collection;
1: 
1: import static org.apache.activemq.store.kahadb.JournalCorruptionEofIndexRecoveryTest.drain;
1: import static org.junit.Assert.assertEquals;
1: import static org.junit.Assert.assertTrue;
1: 
1: @RunWith(Parameterized.class)
1: public class JournalCorruptionExceptionTest {
1: 
1:     private static final Logger LOG = LoggerFactory.getLogger(JournalCorruptionExceptionTest.class);
1: 
1:     private final String KAHADB_DIRECTORY = "target/activemq-data/";
1:     private final String payload = new String(new byte[1024]);
1: 
1:     private ActiveMQConnectionFactory cf = null;
1:     private BrokerService broker = null;
1:     private final Destination destination = new ActiveMQQueue("Test");
1:     private String connectionUri;
1:     private KahaDBPersistenceAdapter adapter;
1: 
1:     @Parameterized.Parameter(0)
1:     public byte fill = Byte.valueOf("3");
1: 
1:     @Parameterized.Parameter(1)
1:     public int fillLength = 10;
1: 
1:     @Parameterized.Parameters(name = "fill=#{0},#{1}")
1:     public static Iterable<Object[]> parameters() {
1:         // corruption can be valid record type values
1:         return Arrays.asList(new Object[][]{
1:                 {Byte.valueOf("0"), 10},
1:                 {Byte.valueOf("1"), 10},
0:                 {Byte.valueOf("1"), 9},
0:                 {Byte.valueOf("2"), 20},
1:                 {Byte.valueOf("-1"), 10}
1:         });
1:     }
1: 
1:     protected void startBroker() throws Exception {
1:         doStartBroker(true);
1:     }
1: 
1:     private void doStartBroker(boolean delete) throws Exception {
1:         broker = new BrokerService();
1:         broker.setDeleteAllMessagesOnStartup(delete);
1:         broker.setPersistent(true);
1:         broker.setUseJmx(true);
1:         broker.setDataDirectory(KAHADB_DIRECTORY);
1:         broker.addConnector("tcp://localhost:0");
1: 
1:         PolicyMap policyMap = new PolicyMap();
1:         PolicyEntry defaultEntry = new PolicyEntry();
1:         defaultEntry.setUseCache(false);
1:         policyMap.setDefaultEntry(defaultEntry);
1:         broker.setDestinationPolicy(policyMap);
1: 
1:         configurePersistence(broker);
1: 
1:         connectionUri = "vm://localhost?create=false";
1:         cf = new ActiveMQConnectionFactory(connectionUri);
1: 
1:         broker.start();
1:         LOG.info("Starting broker..");
1:     }
1: 
1:     protected void configurePersistence(BrokerService brokerService) throws Exception {
1:         adapter = (KahaDBPersistenceAdapter) brokerService.getPersistenceAdapter();
1: 
1:         // ensure there are a bunch of data files but multiple entries in each
1:         adapter.setJournalMaxFileLength(1024 * 20);
1: 
1:     }
1: 
1:     @After
1:     public void tearDown() throws Exception {
1:         if (broker != null) {
1:             broker.stop();
1:             broker.waitUntilStopped();
1:         }
1:     }
1: 
1:     @Test
1:     public void testIOExceptionOnCorruptJournalLocationRead() throws Exception {
1:         startBroker();
1: 
1:         produceMessagesToConsumeMultipleDataFiles(50);
1: 
1:         int numFiles = getNumberOfJournalFiles();
1: 
1:         assertTrue("more than x files: " + numFiles, numFiles > 4);
1: 
1:         corruptLocationAtDataFileIndex(3);
1: 
1:         assertEquals("missing one message", 50, broker.getAdminView().getTotalMessageCount());
1:         assertEquals("Drain", 0, drainQueue(50));
1:         assertTrue("broker stopping", broker.isStopping());
1:     }
1: 
1: 
1:     private void corruptLocationAtDataFileIndex(int id) throws IOException {
1: 
1:         Collection<DataFile> files = ((KahaDBPersistenceAdapter) broker.getPersistenceAdapter()).getStore().getJournal().getFileMap().values();
1:         DataFile dataFile = (DataFile) files.toArray()[id];
1: 
1:         RecoverableRandomAccessFile randomAccessFile = dataFile.openRandomAccessFile();
0:         int offset = 2415;
1: 
0:         if (fillLength < 10) {
0:             offset += (fillLength * 2);
1:         }
1: 
1:         LOG.info("Whacking batch record in file:" + id + ", at offset: " + offset + " with fill:" + fill);
1:         // whack that record
1:         byte[] bla = new byte[fillLength];
1:         Arrays.fill(bla, fill);
1:         randomAccessFile.seek(offset);
1:         randomAccessFile.write(bla, 0, bla.length);
1:         randomAccessFile.getFD().sync();
1:     }
1: 
1:     private int getNumberOfJournalFiles() throws IOException {
1:         Collection<DataFile> files = ((KahaDBPersistenceAdapter) broker.getPersistenceAdapter()).getStore().getJournal().getFileMap().values();
1:         int reality = 0;
1:         for (DataFile file : files) {
1:             if (file != null) {
1:                 reality++;
1:             }
1:         }
1:         return reality;
1:     }
1: 
1:     private int produceMessages(Destination destination, int numToSend) throws Exception {
1:         int sent = 0;
1:         Connection connection = new ActiveMQConnectionFactory(broker.getTransportConnectors().get(0).getConnectUri()).createConnection();
1:         connection.start();
1:         try {
1:             Session session = connection.createSession(false, Session.AUTO_ACKNOWLEDGE);
1:             MessageProducer producer = session.createProducer(destination);
1:             for (int i = 0; i < numToSend; i++) {
1:                 producer.send(createMessage(session, i));
1:                 sent++;
1:             }
1:         } finally {
1:             connection.close();
1:         }
1: 
1:         return sent;
1:     }
1: 
1:     private int produceMessagesToConsumeMultipleDataFiles(int numToSend) throws Exception {
1:         return produceMessages(destination, numToSend);
1:     }
1: 
1:     private Message createMessage(Session session, int i) throws Exception {
1:         return session.createTextMessage(payload + "::" + i);
1:     }
1: 
1:     private int drainQueue(int max) throws Exception {
1:         return drain(cf, destination, max);
1:     }
1: 
1: }
============================================================================