1:29a7f38: /**
1:29a7f38:  * Licensed to the Apache Software Foundation (ASF) under one or more
1:29a7f38:  * contributor license agreements.  See the NOTICE file distributed with
1:29a7f38:  * this work for additional information regarding copyright ownership.
1:29a7f38:  * The ASF licenses this file to You under the Apache License, Version 2.0
1:29a7f38:  * (the "License"); you may not use this file except in compliance with
1:29a7f38:  * the License.  You may obtain a copy of the License at
1:29a7f38:  *
1:29a7f38:  *     http://www.apache.org/licenses/LICENSE-2.0
1:29a7f38:  *
1:29a7f38:  * Unless required by applicable law or agreed to in writing, software
1:29a7f38:  * distributed under the License is distributed on an "AS IS" BASIS,
1:29a7f38:  * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
1:29a7f38:  * See the License for the specific language governing permissions and
1:29a7f38:  * limitations under the License.
1:29a7f38:  */
3:29a7f38: 
1:29a7f38: package org.apache.mahout.cf.taste.example.email;
1:29a7f38: 
1:29a7f38: import com.google.common.io.Closeables;
1:29a7f38: import org.apache.hadoop.conf.Configuration;
1:29a7f38: import org.apache.hadoop.filecache.DistributedCache;
1:29a7f38: import org.apache.hadoop.fs.FileStatus;
1:29a7f38: import org.apache.hadoop.fs.FileSystem;
1:29a7f38: import org.apache.hadoop.fs.FileUtil;
1:29a7f38: import org.apache.hadoop.fs.Path;
1:29a7f38: import org.apache.hadoop.io.IntWritable;
1:05cf634: import org.apache.hadoop.io.LongWritable;
1:29a7f38: import org.apache.hadoop.io.NullWritable;
1:29a7f38: import org.apache.hadoop.io.SequenceFile;
1:29a7f38: import org.apache.hadoop.io.Text;
1:29a7f38: import org.apache.hadoop.io.Writable;
1:29a7f38: import org.apache.hadoop.mapreduce.Job;
1:29a7f38: import org.apache.hadoop.mapreduce.lib.input.SequenceFileInputFormat;
1:29a7f38: import org.apache.hadoop.mapreduce.lib.output.SequenceFileOutputFormat;
1:29a7f38: import org.apache.hadoop.mapreduce.lib.output.TextOutputFormat;
1:29a7f38: import org.apache.hadoop.util.ToolRunner;
1:29a7f38: import org.apache.mahout.common.AbstractJob;
1:29a7f38: import org.apache.mahout.common.HadoopUtil;
1:29a7f38: import org.apache.mahout.common.Pair;
1:29a7f38: import org.apache.mahout.common.commandline.DefaultOptionCreator;
1:29a7f38: import org.apache.mahout.common.iterator.sequencefile.PathFilters;
1:29a7f38: import org.apache.mahout.common.iterator.sequencefile.PathType;
1:29a7f38: import org.apache.mahout.common.iterator.sequencefile.SequenceFileDirIterable;
1:29a7f38: import org.apache.mahout.math.VarIntWritable;
1:29a7f38: import org.slf4j.Logger;
1:29a7f38: import org.slf4j.LoggerFactory;
1:29a7f38: 
1:4ef9d31: import java.io.IOException;
1:4ef9d31: import java.net.URI;
1:4ef9d31: import java.util.ArrayList;
1:4ef9d31: import java.util.List;
1:4ef9d31: import java.util.Map;
1:4ef9d31: import java.util.concurrent.atomic.AtomicInteger;
1:4ef9d31: 
1:29a7f38: /**
1:29a7f38:  * Convert the Mail archives (see {@link org.apache.mahout.text.SequenceFilesFromMailArchives}) to a preference
1:87c15be:  * file that can be consumed by the {@link org.apache.mahout.cf.taste.hadoop.item.RecommenderJob}.
1:29a7f38:  * <p/>
1:3c22856:  * This assumes the input is a Sequence File, that the key is: filename/message id and the value is a list
1:3c22856:  * (separated by the user's choosing) containing the from email and any references
1:29a7f38:  * <p/>
1:3c22856:  * The output is a matrix where either the from or to are the rows (represented as longs) and the columns are the
1:3c22856:  * message ids that the user has interacted with (as a VectorWritable).  This class currently does not account for
1:3c22856:  * thread hijacking.
1:29a7f38:  * <p/>
1:29a7f38:  * It also outputs a side table mapping the row ids to their original and the message ids to the message thread id
1:29a7f38:  */
1:4194a28: public final class MailToPrefsDriver extends AbstractJob {
1:87d4b2e: 
1:29a7f38:   private static final Logger log = LoggerFactory.getLogger(MailToPrefsDriver.class);
1:29a7f38: 
1:29a7f38:   private static final String OUTPUT_FILES_PATTERN = "part-*";
1:29a7f38:   private static final int DICTIONARY_BYTE_OVERHEAD = 4;
1:29a7f38: 
1:29a7f38:   public static void main(String[] args) throws Exception {
1:29a7f38:     ToolRunner.run(new Configuration(), new MailToPrefsDriver(), args);
1:29a7f38:   }
1:29a7f38: 
1:29a7f38:   @Override
1:29a7f38:   public int run(String[] args) throws Exception {
1:29a7f38:     addInputOption();
1:29a7f38:     addOutputOption();
1:29a7f38:     addOption(DefaultOptionCreator.overwriteOption().create());
1:29a7f38:     addOption("chunkSize", "cs", "The size of chunks to write.  Default is 100 mb", "100");
1:3c22856:     addOption("separator", "sep", "The separator used in the input file to separate to, from, subject.  Default is \\n",
1:3c22856:         "\n");
1:6d16230:     addOption("from", "f", "The position in the input text (value) where the from email is located, starting from "
1:6d16230:         + "zero (0).", "0");
1:6d16230:     addOption("refs", "r", "The position in the input text (value) where the reference ids are located, "
1:6d16230:         + "starting from zero (0).", "1");
1:6d16230:     addOption(buildOption("useCounts", "u", "If set, then use the number of times the user has interacted with a "
1:6d16230:         + "thread as an indication of their preference.  Otherwise, use boolean preferences.", false, false,
1:3c22856:         String.valueOf(true)));
1:03a9492:     Map<String, List<String>> parsedArgs = parseArguments(args);
1:29a7f38: 
1:29a7f38:     Path input = getInputPath();
1:29a7f38:     Path output = getOutputPath();
1:03a9492:     int chunkSize = Integer.parseInt(getOption("chunkSize"));
1:03a9492:     String separator = getOption("separator");
1:29a7f38:     Configuration conf = getConf();
1:03a9492:     boolean useCounts = hasOption("useCounts");
1:29a7f38:     AtomicInteger currentPhase = new AtomicInteger();
1:29a7f38:     int[] msgDim = new int[1];
1:29a7f38:     //TODO: mod this to not do so many passes over the data.  Dictionary creation could probably be a chain mapper
1:4194a28:     List<Path> msgIdChunks = null;
1:29a7f38:     boolean overwrite = hasOption(DefaultOptionCreator.OVERWRITE_OPTION);
1:29a7f38:     // create the dictionary between message ids and longs
1:29a7f38:     if (shouldRunNextPhase(parsedArgs, currentPhase)) {
1:3c22856:       //TODO: there seems to be a pattern emerging for dictionary creation
1:3c22856:       // -- sparse vectors from seq files also has this.
1:29a7f38:       Path msgIdsPath = new Path(output, "msgIds");
1:29a7f38:       if (overwrite) {
1:29a7f38:         HadoopUtil.delete(conf, msgIdsPath);
1:29a7f38:       }
1:29a7f38:       log.info("Creating Msg Id Dictionary");
1:29a7f38:       Job createMsgIdDictionary = prepareJob(input,
1:29a7f38:               msgIdsPath,
1:29a7f38:               SequenceFileInputFormat.class,
1:29a7f38:               MsgIdToDictionaryMapper.class,
1:29a7f38:               Text.class,
1:29a7f38:               VarIntWritable.class,
1:29a7f38:               MailToDictionaryReducer.class,
1:29a7f38:               Text.class,
1:29a7f38:               VarIntWritable.class,
1:29a7f38:               SequenceFileOutputFormat.class);
1:29a7f38: 
1:7c2b664:       boolean succeeded = createMsgIdDictionary.waitForCompletion(true);
1:7c2b664:       if (!succeeded) {
1:7c2b664:         return -1;
1:29a7f38:       }
1:29a7f38:       //write out the dictionary at the top level
1:3c22856:       msgIdChunks = createDictionaryChunks(msgIdsPath, output, "msgIds-dictionary-",
1:3c22856:           createMsgIdDictionary.getConfiguration(), chunkSize, msgDim);
1:29a7f38:     }
1:29a7f38:     //create the dictionary between from email addresses and longs
1:4194a28:     List<Path> fromChunks = null;
1:29a7f38:     if (shouldRunNextPhase(parsedArgs, currentPhase)) {
1:29a7f38:       Path fromIdsPath = new Path(output, "fromIds");
1:29a7f38:       if (overwrite) {
1:29a7f38:         HadoopUtil.delete(conf, fromIdsPath);
1:29a7f38:       }
1:29a7f38:       log.info("Creating From Id Dictionary");
1:29a7f38:       Job createFromIdDictionary = prepareJob(input,
1:29a7f38:               fromIdsPath,
1:29a7f38:               SequenceFileInputFormat.class,
1:29a7f38:               FromEmailToDictionaryMapper.class,
1:29a7f38:               Text.class,
1:29a7f38:               VarIntWritable.class,
1:29a7f38:               MailToDictionaryReducer.class,
1:29a7f38:               Text.class,
1:29a7f38:               VarIntWritable.class,
1:29a7f38:               SequenceFileOutputFormat.class);
1:29a7f38:       createFromIdDictionary.getConfiguration().set(EmailUtility.SEPARATOR, separator);
1:7c2b664:       boolean succeeded = createFromIdDictionary.waitForCompletion(true);
1:7c2b664:       if (!succeeded) {
1:7c2b664:         return -1;
1:7c2b664:       }
1:29a7f38:       //write out the dictionary at the top level
1:29a7f38:       int[] fromDim = new int[1];
1:3c22856:       fromChunks = createDictionaryChunks(fromIdsPath, output, "fromIds-dictionary-",
1:3c22856:           createFromIdDictionary.getConfiguration(), chunkSize, fromDim);
1:29a7f38:     }
1:29a7f38:     //OK, we have our dictionaries, let's output the real thing we need: <from_id -> <msgId, msgId, msgId, ...>>
1:29a7f38:     if (shouldRunNextPhase(parsedArgs, currentPhase) && fromChunks != null && msgIdChunks != null) {
1:29a7f38:       //Job map
1:3c22856:       //may be a way to do this so that we can load the from ids in memory, if they are small enough so that
1:3c22856:       // we don't need the double loop
1:29a7f38:       log.info("Creating recommendation matrix");
1:29a7f38:       Path vecPath = new Path(output, "recInput");
1:29a7f38:       if (overwrite) {
1:29a7f38:         HadoopUtil.delete(conf, vecPath);
1:29a7f38:       }
1:29a7f38:       //conf.set(EmailUtility.FROM_DIMENSION, String.valueOf(fromDim[0]));
1:29a7f38:       conf.set(EmailUtility.MSG_ID_DIMENSION, String.valueOf(msgDim[0]));
1:29a7f38:       conf.set(EmailUtility.FROM_PREFIX, "fromIds-dictionary-");
1:29a7f38:       conf.set(EmailUtility.MSG_IDS_PREFIX, "msgIds-dictionary-");
1:03a9492:       conf.set(EmailUtility.FROM_INDEX, getOption("from"));
1:03a9492:       conf.set(EmailUtility.REFS_INDEX, getOption("refs"));
1:29a7f38:       conf.set(EmailUtility.SEPARATOR, separator);
1:05cf634:       conf.set(MailToRecReducer.USE_COUNTS_PREFERENCE, String.valueOf(useCounts));
1:4194a28:       int j = 0;
1:29a7f38:       int i = 0;
1:29a7f38:       for (Path fromChunk : fromChunks) {
1:29a7f38:         for (Path idChunk : msgIdChunks) {
1:4194a28:           Path out = new Path(vecPath, "tmp-" + i + '-' + j);
1:29a7f38:           DistributedCache.setCacheFiles(new URI[]{fromChunk.toUri(), idChunk.toUri()}, conf);
1:29a7f38:           Job createRecMatrix = prepareJob(input, out, SequenceFileInputFormat.class,
1:3c22856:                   MailToRecMapper.class, Text.class, LongWritable.class, MailToRecReducer.class, Text.class,
1:3c22856:                   NullWritable.class, TextOutputFormat.class);
1:29a7f38:           createRecMatrix.getConfiguration().set("mapred.output.compress", "false");
1:7c2b664:           boolean succeeded = createRecMatrix.waitForCompletion(true);
1:7c2b664:           if (!succeeded) {
1:7c2b664:             return -1;
1:7c2b664:           }
1:29a7f38:           //copy the results up a level
1:3c22856:           //HadoopUtil.copyMergeSeqFiles(out.getFileSystem(conf), out, vecPath.getFileSystem(conf), outPath, true,
1:3c22856:           // conf, "");
1:3c22856:           FileStatus[] fs = HadoopUtil.getFileStatus(new Path(out, "*"), PathType.GLOB, PathFilters.partFilter(), null,
1:3c22856:               conf);
1:29a7f38:           for (int k = 0; k < fs.length; k++) {
1:29a7f38:             FileStatus f = fs[k];
1:4194a28:             Path outPath = new Path(vecPath, "chunk-" + i + '-' + j + '-' + k);
1:3c22856:             FileUtil.copy(f.getPath().getFileSystem(conf), f.getPath(), outPath.getFileSystem(conf), outPath, true,
1:3c22856:                 overwrite, conf);
1:29a7f38:           }
1:29a7f38:           HadoopUtil.delete(conf, out);
1:29a7f38:           j++;
1:29a7f38:         }
1:29a7f38:         i++;
1:29a7f38:       }
1:29a7f38:       //concat the files together
1:29a7f38:       /*Path mergePath = new Path(output, "vectors.dat");
1:29a7f38:       if (overwrite) {
1:29a7f38:         HadoopUtil.delete(conf, mergePath);
1:29a7f38:       }
1:29a7f38:       log.info("Merging together output vectors to vectors.dat in {}", output);*/
1:3c22856:       //HadoopUtil.copyMergeSeqFiles(vecPath.getFileSystem(conf), vecPath, mergePath.getFileSystem(conf), mergePath,
1:3c22856:       // false, conf, "\n");
1:29a7f38:     }
1:29a7f38: 
1:4194a28:     return 0;
1:29a7f38:   }
1:29a7f38: 
1:29a7f38:   private static List<Path> createDictionaryChunks(Path inputPath,
1:29a7f38:                                                    Path dictionaryPathBase,
1:29a7f38:                                                    String name,
1:29a7f38:                                                    Configuration baseConf,
1:3c22856:                                                    int chunkSizeInMegabytes, int[] maxTermDimension)
1:6d16230:     throws IOException {
1:4ef9d31:     List<Path> chunkPaths = new ArrayList<>();
1:29a7f38: 
1:29a7f38:     Configuration conf = new Configuration(baseConf);
1:29a7f38: 
1:29a7f38:     FileSystem fs = FileSystem.get(inputPath.toUri(), conf);
1:29a7f38: 
1:29a7f38:     long chunkSizeLimit = chunkSizeInMegabytes * 1024L * 1024L;
1:29a7f38:     int chunkIndex = 0;
1:29a7f38:     Path chunkPath = new Path(dictionaryPathBase, name + chunkIndex);
1:29a7f38:     chunkPaths.add(chunkPath);
1:29a7f38: 
1:29a7f38:     SequenceFile.Writer dictWriter = new SequenceFile.Writer(fs, conf, chunkPath, Text.class, IntWritable.class);
1:29a7f38: 
1:29a7f38:     try {
1:29a7f38:       long currentChunkSize = 0;
1:29a7f38:       Path filesPattern = new Path(inputPath, OUTPUT_FILES_PATTERN);
1:44459bd:       int i = 1; //start at 1, since a miss in the OpenObjectIntHashMap returns a 0
1:29a7f38:       for (Pair<Writable, Writable> record
1:87c15be:               : new SequenceFileDirIterable<>(filesPattern, PathType.GLOB, null, null, true, conf)) {
1:29a7f38:         if (currentChunkSize > chunkSizeLimit) {
1:87d4b2e:           Closeables.close(dictWriter, false);
1:29a7f38:           chunkIndex++;
1:29a7f38: 
1:29a7f38:           chunkPath = new Path(dictionaryPathBase, name + chunkIndex);
1:29a7f38:           chunkPaths.add(chunkPath);
1:29a7f38: 
1:29a7f38:           dictWriter = new SequenceFile.Writer(fs, conf, chunkPath, Text.class, IntWritable.class);
1:29a7f38:           currentChunkSize = 0;
1:29a7f38:         }
1:29a7f38: 
1:29a7f38:         Writable key = record.getFirst();
1:29a7f38:         int fieldSize = DICTIONARY_BYTE_OVERHEAD + key.toString().length() * 2 + Integer.SIZE / 8;
1:29a7f38:         currentChunkSize += fieldSize;
1:29a7f38:         dictWriter.append(key, new IntWritable(i++));
1:29a7f38:       }
1:29a7f38:       maxTermDimension[0] = i;
1:29a7f38:     } finally {
1:87d4b2e:       Closeables.close(dictWriter, false);
1:29a7f38:     }
1:29a7f38: 
1:29a7f38:     return chunkPaths;
1:29a7f38:   }
1:29a7f38: 
1:29a7f38: }
============================================================================
author:Suneel Marthi
-------------------------------------------------------------------------------
commit:4ef9d31
/////////////////////////////////////////////////////////////////////////
/////////////////////////////////////////////////////////////////////////
1: import java.io.IOException;
1: import java.net.URI;
1: import java.util.ArrayList;
1: import java.util.List;
1: import java.util.Map;
1: import java.util.concurrent.atomic.AtomicInteger;
1: 
/////////////////////////////////////////////////////////////////////////
1:     List<Path> chunkPaths = new ArrayList<>();
commit:87c15be
/////////////////////////////////////////////////////////////////////////
1:  * file that can be consumed by the {@link org.apache.mahout.cf.taste.hadoop.item.RecommenderJob}.
/////////////////////////////////////////////////////////////////////////
1:               : new SequenceFileDirIterable<>(filesPattern, PathType.GLOB, null, null, true, conf)) {
author:Sebastian Schelter
-------------------------------------------------------------------------------
commit:58cc1ae
/////////////////////////////////////////////////////////////////////////
commit:6d16230
/////////////////////////////////////////////////////////////////////////
1:     addOption("from", "f", "The position in the input text (value) where the from email is located, starting from "
1:         + "zero (0).", "0");
1:     addOption("refs", "r", "The position in the input text (value) where the reference ids are located, "
1:         + "starting from zero (0).", "1");
1:     addOption(buildOption("useCounts", "u", "If set, then use the number of times the user has interacted with a "
1:         + "thread as an indication of their preference.  Otherwise, use boolean preferences.", false, false,
/////////////////////////////////////////////////////////////////////////
1:     throws IOException {
commit:3c22856
/////////////////////////////////////////////////////////////////////////
1:  * This assumes the input is a Sequence File, that the key is: filename/message id and the value is a list
1:  * (separated by the user's choosing) containing the from email and any references
1:  * The output is a matrix where either the from or to are the rows (represented as longs) and the columns are the
1:  * message ids that the user has interacted with (as a VectorWritable).  This class currently does not account for
1:  * thread hijacking.
/////////////////////////////////////////////////////////////////////////
1:     addOption("separator", "sep", "The separator used in the input file to separate to, from, subject.  Default is \\n",
1:         "\n");
0:     addOption("from", "f", "The position in the input text (value) where the from email is located, starting from " +
0:         "zero (0).", "0");
0:     addOption("refs", "r", "The position in the input text (value) where the reference ids are located, " +
0:         "starting from zero (0).", "1");
0:     addOption(buildOption("useCounts", "u", "If set, then use the number of times the user has interacted with a " +
0:         "thread as an indication of their preference.  Otherwise, use boolean preferences.", false, false,
1:         String.valueOf(true)));
/////////////////////////////////////////////////////////////////////////
1:       //TODO: there seems to be a pattern emerging for dictionary creation
1:       // -- sparse vectors from seq files also has this.
/////////////////////////////////////////////////////////////////////////
1:       msgIdChunks = createDictionaryChunks(msgIdsPath, output, "msgIds-dictionary-",
1:           createMsgIdDictionary.getConfiguration(), chunkSize, msgDim);
/////////////////////////////////////////////////////////////////////////
1:       fromChunks = createDictionaryChunks(fromIdsPath, output, "fromIds-dictionary-",
1:           createFromIdDictionary.getConfiguration(), chunkSize, fromDim);
1:       //may be a way to do this so that we can load the from ids in memory, if they are small enough so that
1:       // we don't need the double loop
/////////////////////////////////////////////////////////////////////////
1:                   MailToRecMapper.class, Text.class, LongWritable.class, MailToRecReducer.class, Text.class,
1:                   NullWritable.class, TextOutputFormat.class);
1:           //HadoopUtil.copyMergeSeqFiles(out.getFileSystem(conf), out, vecPath.getFileSystem(conf), outPath, true,
1:           // conf, "");
1:           FileStatus[] fs = HadoopUtil.getFileStatus(new Path(out, "*"), PathType.GLOB, PathFilters.partFilter(), null,
1:               conf);
1:             FileUtil.copy(f.getPath().getFileSystem(conf), f.getPath(), outPath.getFileSystem(conf), outPath, true,
1:                 overwrite, conf);
/////////////////////////////////////////////////////////////////////////
1:       //HadoopUtil.copyMergeSeqFiles(vecPath.getFileSystem(conf), vecPath, mergePath.getFileSystem(conf), mergePath,
1:       // false, conf, "\n");
/////////////////////////////////////////////////////////////////////////
1:                                                    int chunkSizeInMegabytes, int[] maxTermDimension)
0:       throws IOException {
author:dfilimon
-------------------------------------------------------------------------------
commit:87d4b2e
/////////////////////////////////////////////////////////////////////////
0: import java.io.IOException;
0: import java.net.URI;
0: import java.util.List;
0: import java.util.Map;
0: import java.util.concurrent.atomic.AtomicInteger;
1: 
/////////////////////////////////////////////////////////////////////////
/////////////////////////////////////////////////////////////////////////
1:           Closeables.close(dictWriter, false);
/////////////////////////////////////////////////////////////////////////
1:       Closeables.close(dictWriter, false);
author:Grant Ingersoll
-------------------------------------------------------------------------------
commit:31cb292
/////////////////////////////////////////////////////////////////////////
0:           Closeables.close(dictWriter, true);
/////////////////////////////////////////////////////////////////////////
0:       Closeables.close(dictWriter, true);
commit:03a9492
/////////////////////////////////////////////////////////////////////////
1:     Map<String, List<String>> parsedArgs = parseArguments(args);
1:     int chunkSize = Integer.parseInt(getOption("chunkSize"));
1:     String separator = getOption("separator");
1:     boolean useCounts = hasOption("useCounts");
/////////////////////////////////////////////////////////////////////////
1:       conf.set(EmailUtility.FROM_INDEX, getOption("from"));
1:       conf.set(EmailUtility.REFS_INDEX, getOption("refs"));
commit:05cf634
/////////////////////////////////////////////////////////////////////////
1: import org.apache.hadoop.io.LongWritable;
/////////////////////////////////////////////////////////////////////////
0:     addOption(buildOption("useCounts", "u", "If set, then use the number of times the user has interacted with a thread as an indication of their preference.  Otherwise, use boolean preferences.",
0:             false, false, "true"));
/////////////////////////////////////////////////////////////////////////
0:     boolean useCounts = hasOption("--useCounts");
/////////////////////////////////////////////////////////////////////////
1:       conf.set(MailToRecReducer.USE_COUNTS_PREFERENCE, String.valueOf(useCounts));
/////////////////////////////////////////////////////////////////////////
0:                   MailToRecMapper.class, Text.class, LongWritable.class, MailToRecReducer.class, Text.class, NullWritable.class,
/////////////////////////////////////////////////////////////////////////
0:       int i = 1;//start at 1, since a miss in the OpenObjectIntHashMap returns a 0
commit:3b9f635
/////////////////////////////////////////////////////////////////////////
0:  * user's choosing) containing the from email and any references
/////////////////////////////////////////////////////////////////////////
0:     addOption("from", "f", "The position in the input text (value) where the from email is located, starting from zero (0).", "0");
0:     addOption("refs", "r", "The position in the input text (value) where the reference ids are located, starting from zero (0).", "1");
/////////////////////////////////////////////////////////////////////////
0:       conf.set(EmailUtility.FROM_INDEX, parsedArgs.get("--from"));
0:       conf.set(EmailUtility.REFS_INDEX, parsedArgs.get("--refs"));
commit:29a7f38
/////////////////////////////////////////////////////////////////////////
1: package org.apache.mahout.cf.taste.example.email;
1: /**
1:  * Licensed to the Apache Software Foundation (ASF) under one or more
1:  * contributor license agreements.  See the NOTICE file distributed with
1:  * this work for additional information regarding copyright ownership.
1:  * The ASF licenses this file to You under the Apache License, Version 2.0
1:  * (the "License"); you may not use this file except in compliance with
1:  * the License.  You may obtain a copy of the License at
1:  *
1:  *     http://www.apache.org/licenses/LICENSE-2.0
1:  *
1:  * Unless required by applicable law or agreed to in writing, software
1:  * distributed under the License is distributed on an "AS IS" BASIS,
1:  * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
1:  * See the License for the specific language governing permissions and
1:  * limitations under the License.
1:  */
1: 
1: 
0: import com.google.common.collect.Lists;
1: import com.google.common.io.Closeables;
1: import org.apache.hadoop.conf.Configuration;
1: import org.apache.hadoop.filecache.DistributedCache;
1: import org.apache.hadoop.fs.FileStatus;
1: import org.apache.hadoop.fs.FileSystem;
1: import org.apache.hadoop.fs.FileUtil;
1: import org.apache.hadoop.fs.Path;
1: import org.apache.hadoop.io.IntWritable;
1: import org.apache.hadoop.io.NullWritable;
1: import org.apache.hadoop.io.SequenceFile;
1: import org.apache.hadoop.io.Text;
1: import org.apache.hadoop.io.Writable;
1: import org.apache.hadoop.mapreduce.Job;
1: import org.apache.hadoop.mapreduce.lib.input.SequenceFileInputFormat;
1: import org.apache.hadoop.mapreduce.lib.output.SequenceFileOutputFormat;
1: import org.apache.hadoop.mapreduce.lib.output.TextOutputFormat;
1: import org.apache.hadoop.util.ToolRunner;
1: import org.apache.mahout.common.AbstractJob;
1: import org.apache.mahout.common.HadoopUtil;
1: import org.apache.mahout.common.Pair;
1: import org.apache.mahout.common.commandline.DefaultOptionCreator;
1: import org.apache.mahout.common.iterator.sequencefile.PathFilters;
1: import org.apache.mahout.common.iterator.sequencefile.PathType;
1: import org.apache.mahout.common.iterator.sequencefile.SequenceFileDirIterable;
1: import org.apache.mahout.math.VarIntWritable;
1: import org.slf4j.Logger;
1: import org.slf4j.LoggerFactory;
1: 
0: import java.io.IOException;
0: import java.net.URI;
0: import java.util.List;
0: import java.util.Map;
0: import java.util.concurrent.atomic.AtomicInteger;
1: 
1: /**
1:  * Convert the Mail archives (see {@link org.apache.mahout.text.SequenceFilesFromMailArchives}) to a preference
0:  * file that can be consumed by the {@link org.apache.mahout.cf.taste.hadoop.pseudo.RecommenderJob}.
1:  * <p/>
0:  * This assumes the input is a Sequence File, that the key is: filename/message id and the value is a list (separated by the
0:  * user's choosing) of: from, to, subject
1:  * <p/>
0:  * The output is a matrix where either the from or to are the rows (represented as longs) and the columns are the message ids
0:  * that the user has interacted with (as a VectorWritable).  This class currently does not account for thread hijacking.
1:  * <p/>
1:  * It also outputs a side table mapping the row ids to their original and the message ids to the message thread id
1:  */
0: public class MailToPrefsDriver extends AbstractJob {
1:   private static final Logger log = LoggerFactory.getLogger(MailToPrefsDriver.class);
1: 
1:   private static final String OUTPUT_FILES_PATTERN = "part-*";
1:   private static final int DICTIONARY_BYTE_OVERHEAD = 4;
1: 
1: 
1:   public static void main(String[] args) throws Exception {
1:     ToolRunner.run(new Configuration(), new MailToPrefsDriver(), args);
1:   }
1: 
1:   @Override
1:   public int run(String[] args) throws Exception {
0:     int result = 0;
1:     addInputOption();
1:     addOutputOption();
1:     addOption(DefaultOptionCreator.overwriteOption().create());
1:     addOption("chunkSize", "cs", "The size of chunks to write.  Default is 100 mb", "100");
0:     addOption("separator", "sep", "The separator used in the input file to separate to, from, subject.  Default is \\n", "\n");
0:     Map<String, String> parsedArgs = parseArguments(args);
1: 
1:     Path input = getInputPath();
1:     Path output = getOutputPath();
0:     int chunkSize = Integer.parseInt(parsedArgs.get("--chunkSize"));
0:     String separator = parsedArgs.get("--separator");
1:     Configuration conf = getConf();
0:     if (conf == null) {
0:       setConf(new Configuration());
0:       conf = getConf();
1:     }
1: 
1:     AtomicInteger currentPhase = new AtomicInteger();
1:     int[] msgDim = new int[1];
1:     int[] fromDim = new int[1];
1:     //TODO: mod this to not do so many passes over the data.  Dictionary creation could probably be a chain mapper
0:     List<Path> msgIdChunks = null, fromChunks = null;
1:     boolean overwrite = hasOption(DefaultOptionCreator.OVERWRITE_OPTION);
1:     // create the dictionary between message ids and longs
1:     if (shouldRunNextPhase(parsedArgs, currentPhase)) {
0:       //TODO: there seems to be a pattern emerging for dictionary creation -- sparse vectors from seq files also has this.
1:       Path msgIdsPath = new Path(output, "msgIds");
1:       if (overwrite) {
1:         HadoopUtil.delete(conf, msgIdsPath);
1:       }
1:       log.info("Creating Msg Id Dictionary");
1:       Job createMsgIdDictionary = prepareJob(input,
1:               msgIdsPath,
1:               SequenceFileInputFormat.class,
1:               MsgIdToDictionaryMapper.class,
1:               Text.class,
1:               VarIntWritable.class,
1:               MailToDictionaryReducer.class,
1:               Text.class,
1:               VarIntWritable.class,
1:               SequenceFileOutputFormat.class);
0:       createMsgIdDictionary.waitForCompletion(true);
1:       //write out the dictionary at the top level
0:       msgIdChunks = createDictionaryChunks(msgIdsPath, output, "msgIds-dictionary-", createMsgIdDictionary.getConfiguration(), chunkSize, msgDim);
1:     }
1:     //create the dictionary between from email addresses and longs
1:     if (shouldRunNextPhase(parsedArgs, currentPhase)) {
1:       Path fromIdsPath = new Path(output, "fromIds");
1:       if (overwrite) {
1:         HadoopUtil.delete(conf, fromIdsPath);
1:       }
1:       log.info("Creating From Id Dictionary");
1:       Job createFromIdDictionary = prepareJob(input,
1:               fromIdsPath,
1:               SequenceFileInputFormat.class,
1:               FromEmailToDictionaryMapper.class,
1:               Text.class,
1:               VarIntWritable.class,
1:               MailToDictionaryReducer.class,
1:               Text.class,
1:               VarIntWritable.class,
1:               SequenceFileOutputFormat.class);
1:       createFromIdDictionary.getConfiguration().set(EmailUtility.SEPARATOR, separator);
0:       createFromIdDictionary.waitForCompletion(true);
1:       //write out the dictionary at the top level
0:       fromChunks = createDictionaryChunks(fromIdsPath, output, "fromIds-dictionary-", createFromIdDictionary.getConfiguration(), chunkSize, fromDim);
1:     }
1:     //OK, we have our dictionaries, let's output the real thing we need: <from_id -> <msgId, msgId, msgId, ...>>
1:     if (shouldRunNextPhase(parsedArgs, currentPhase) && fromChunks != null && msgIdChunks != null) {
1:       //Job map
0:       //may be a way to do this so that we can load the from ids in memory, if they are small enough so that we don't need the double loop
1:       log.info("Creating recommendation matrix");
0:       int i = 0, j = 0;
1:       Path vecPath = new Path(output, "recInput");
1:       if (overwrite) {
1:         HadoopUtil.delete(conf, vecPath);
1:       }
1:       //conf.set(EmailUtility.FROM_DIMENSION, String.valueOf(fromDim[0]));
1:       conf.set(EmailUtility.MSG_ID_DIMENSION, String.valueOf(msgDim[0]));
1:       conf.set(EmailUtility.FROM_PREFIX, "fromIds-dictionary-");
1:       conf.set(EmailUtility.MSG_IDS_PREFIX, "msgIds-dictionary-");
1:       conf.set(EmailUtility.SEPARATOR, separator);
1:       for (Path fromChunk : fromChunks) {
1:         for (Path idChunk : msgIdChunks) {
0:           Path out = new Path(vecPath, "tmp-" + i + "-" + j);
1:           DistributedCache.setCacheFiles(new URI[]{fromChunk.toUri(), idChunk.toUri()}, conf);
1:           Job createRecMatrix = prepareJob(input, out, SequenceFileInputFormat.class,
0:                   MailToRecMapper.class, NullWritable.class, Text.class,
0:                   TextOutputFormat.class);
1:           createRecMatrix.getConfiguration().set("mapred.output.compress", "false");
0:           createRecMatrix.waitForCompletion(true);
1:           //copy the results up a level
0:           //HadoopUtil.copyMergeSeqFiles(out.getFileSystem(conf), out, vecPath.getFileSystem(conf), outPath, true, conf, "");
0:           FileStatus fs[] = HadoopUtil.getFileStatus(new Path(out, "*"), PathType.GLOB, PathFilters.partFilter(), null, conf);
1:           for (int k = 0; k < fs.length; k++) {
1:             FileStatus f = fs[k];
0:             Path outPath = new Path(vecPath, "chunk-" + i + "-" + j + "-" + k);
0:             FileUtil.copy(f.getPath().getFileSystem(conf), f.getPath(), outPath.getFileSystem(conf), outPath, true, overwrite, conf);
1:           }
1:           HadoopUtil.delete(conf, out);
1:           j++;
1:         }
1:         i++;
1:       }
1:       //concat the files together
1:       /*Path mergePath = new Path(output, "vectors.dat");
1:       if (overwrite) {
1:         HadoopUtil.delete(conf, mergePath);
1:       }
1:       log.info("Merging together output vectors to vectors.dat in {}", output);*/
0:       //HadoopUtil.copyMergeSeqFiles(vecPath.getFileSystem(conf), vecPath, mergePath.getFileSystem(conf), mergePath, false, conf, "\n");
1:     }
1: 
0:     return result;
1:   }
1: 
1:   private static List<Path> createDictionaryChunks(Path inputPath,
1:                                                    Path dictionaryPathBase,
1:                                                    String name,
1:                                                    Configuration baseConf,
0:                                                    int chunkSizeInMegabytes, int[] maxTermDimension) throws IOException {
0:     List<Path> chunkPaths = Lists.newArrayList();
1: 
1:     Configuration conf = new Configuration(baseConf);
1: 
1:     FileSystem fs = FileSystem.get(inputPath.toUri(), conf);
1: 
1:     long chunkSizeLimit = chunkSizeInMegabytes * 1024L * 1024L;
1:     int chunkIndex = 0;
1:     Path chunkPath = new Path(dictionaryPathBase, name + chunkIndex);
1:     chunkPaths.add(chunkPath);
1: 
1:     SequenceFile.Writer dictWriter = new SequenceFile.Writer(fs, conf, chunkPath, Text.class, IntWritable.class);
1: 
1:     try {
1:       long currentChunkSize = 0;
1:       Path filesPattern = new Path(inputPath, OUTPUT_FILES_PATTERN);
1:       int i = 0;
1:       for (Pair<Writable, Writable> record
0:               : new SequenceFileDirIterable<Writable, Writable>(filesPattern, PathType.GLOB, null, null, true, conf)) {
1:         if (currentChunkSize > chunkSizeLimit) {
0:           Closeables.closeQuietly(dictWriter);
1:           chunkIndex++;
1: 
1:           chunkPath = new Path(dictionaryPathBase, name + chunkIndex);
1:           chunkPaths.add(chunkPath);
1: 
1:           dictWriter = new SequenceFile.Writer(fs, conf, chunkPath, Text.class, IntWritable.class);
1:           currentChunkSize = 0;
1:         }
1: 
1:         Writable key = record.getFirst();
1:         int fieldSize = DICTIONARY_BYTE_OVERHEAD + key.toString().length() * 2 + Integer.SIZE / 8;
1:         currentChunkSize += fieldSize;
1:         dictWriter.append(key, new IntWritable(i++));
1:       }
1:       maxTermDimension[0] = i;
1:     } finally {
0:       Closeables.closeQuietly(dictWriter);
1:     }
1: 
1:     return chunkPaths;
1:   }
1: 
1: }
author:tcp
-------------------------------------------------------------------------------
commit:44459bd
/////////////////////////////////////////////////////////////////////////
1:       int i = 1; //start at 1, since a miss in the OpenObjectIntHashMap returns a 0
author:Sean R. Owen
-------------------------------------------------------------------------------
commit:7c2b664
/////////////////////////////////////////////////////////////////////////
0: 
1:       boolean succeeded = createMsgIdDictionary.waitForCompletion(true);
1:       if (!succeeded) {
1:         return -1;
1:       }
/////////////////////////////////////////////////////////////////////////
1:       boolean succeeded = createFromIdDictionary.waitForCompletion(true);
1:       if (!succeeded) {
1:         return -1;
1:       }
/////////////////////////////////////////////////////////////////////////
1:           boolean succeeded = createRecMatrix.waitForCompletion(true);
1:           if (!succeeded) {
1:             return -1;
0:           }
commit:e0ec7c1
/////////////////////////////////////////////////////////////////////////
/////////////////////////////////////////////////////////////////////////
0:       int i = 0;
commit:4194a28
/////////////////////////////////////////////////////////////////////////
/////////////////////////////////////////////////////////////////////////
0: package org.apache.mahout.cf.taste.example.email;
/////////////////////////////////////////////////////////////////////////
1: public final class MailToPrefsDriver extends AbstractJob {
0: 
/////////////////////////////////////////////////////////////////////////
1:     List<Path> msgIdChunks = null;
/////////////////////////////////////////////////////////////////////////
1:     List<Path> fromChunks = null;
/////////////////////////////////////////////////////////////////////////
0:       int[] fromDim = new int[1];
/////////////////////////////////////////////////////////////////////////
0:       int i = 0;
/////////////////////////////////////////////////////////////////////////
1:       int j = 0;
1:           Path out = new Path(vecPath, "tmp-" + i + '-' + j);
/////////////////////////////////////////////////////////////////////////
0:           FileStatus[] fs = HadoopUtil.getFileStatus(new Path(out, "*"), PathType.GLOB, PathFilters.partFilter(), null, conf);
1:             Path outPath = new Path(vecPath, "chunk-" + i + '-' + j + '-' + k);
/////////////////////////////////////////////////////////////////////////
1:     return 0;
============================================================================