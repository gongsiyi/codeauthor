1:1d6dc49: /**
1:1d6dc49:  * Licensed to the Apache Software Foundation (ASF) under one or more
1:1d6dc49:  * contributor license agreements.  See the NOTICE file distributed with
1:1d6dc49:  * this work for additional information regarding copyright ownership.
1:1d6dc49:  * The ASF licenses this file to You under the Apache License, Version 2.0
1:1d6dc49:  * (the "License"); you may not use this file except in compliance with
1:1d6dc49:  * the License.  You may obtain a copy of the License at
1:1d6dc49:  *
1:1d6dc49:  *     http://www.apache.org/licenses/LICENSE-2.0
1:1d6dc49:  *
1:1d6dc49:  * Unless required by applicable law or agreed to in writing, software
1:1d6dc49:  * distributed under the License is distributed on an "AS IS" BASIS,
1:1d6dc49:  * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
1:1d6dc49:  * See the License for the specific language governing permissions and
1:1d6dc49:  * limitations under the License.
1:1d6dc49:  */
2:1d6dc49: 
1:1d6dc49: package org.apache.mahout.clustering.streaming.mapreduce;
1:1d6dc49: 
1:1d6dc49: import java.io.IOException;
1:1d6dc49: import java.util.List;
1:1d6dc49: 
1:1d6dc49: import com.google.common.base.Function;
1:1d6dc49: import com.google.common.base.Preconditions;
1:1d6dc49: import com.google.common.collect.Iterables;
1:1d6dc49: import com.google.common.collect.Lists;
1:1d6dc49: import org.apache.hadoop.conf.Configuration;
1:1d6dc49: import org.apache.hadoop.io.IntWritable;
1:1d6dc49: import org.apache.hadoop.mapreduce.Reducer;
1:1d6dc49: import org.apache.mahout.clustering.streaming.cluster.BallKMeans;
1:1d6dc49: import org.apache.mahout.common.commandline.DefaultOptionCreator;
1:1d6dc49: import org.apache.mahout.math.Centroid;
1:1d6dc49: import org.apache.mahout.math.Vector;
1:0c1b4a1: import org.slf4j.Logger;
1:0c1b4a1: import org.slf4j.LoggerFactory;
1:1d6dc49: 
1:1d6dc49: public class StreamingKMeansReducer extends Reducer<IntWritable, CentroidWritable, IntWritable, CentroidWritable> {
1:0c1b4a1: 
1:0c1b4a1:   private static final Logger log = LoggerFactory.getLogger(StreamingKMeansReducer.class);
1:0c1b4a1: 
1:1d6dc49:   /**
1:1d6dc49:    * Configuration for the MapReduce job.
1:1d6dc49:    */
1:1d6dc49:   private Configuration conf;
1:1d6dc49: 
2:1d6dc49:   @Override
1:1d6dc49:   public void setup(Context context) {
1:a330664:     // At this point the configuration received from the Driver is assumed to be valid.
1:a330664:     // No other checks are made.
1:1d6dc49:     conf = context.getConfiguration();
1:a330664:   }
1:a330664: 
1:1d6dc49:   @Override
1:1d6dc49:   public void reduce(IntWritable key, Iterable<CentroidWritable> centroids,
1:1d6dc49:                      Context context) throws IOException, InterruptedException {
1:a330664:     List<Centroid> intermediateCentroids;
1:a330664:     // There might be too many intermediate centroids to fit into memory, in which case, we run another pass
1:a330664:     // of StreamingKMeans to collapse the clusters further.
1:a330664:     if (conf.getBoolean(StreamingKMeansDriver.REDUCE_STREAMING_KMEANS, false)) {
1:6b6b8a0:       intermediateCentroids = Lists.newArrayList(
1:6b6b8a0:           new StreamingKMeansThread(Iterables.transform(centroids, new Function<CentroidWritable, Centroid>() {
1:7f431a0:             @Override
1:6b6b8a0:             public Centroid apply(CentroidWritable input) {
1:7f431a0:               Preconditions.checkNotNull(input);
1:0c1b4a1:               return input.getCentroid().clone();
1:7f431a0:             }
1:6b6b8a0:           }), conf).call());
1:a330664:     } else {
1:a330664:       intermediateCentroids = centroidWritablesToList(centroids);
1:a330664:     }
1:a330664: 
1:1d6dc49:     int index = 0;
1:a330664:     for (Vector centroid : getBestCentroids(intermediateCentroids, conf)) {
1:0c1b4a1:       context.write(new IntWritable(index), new CentroidWritable((Centroid) centroid));
1:1d6dc49:       ++index;
1:a330664:     }
3:1d6dc49:   }
1:1d6dc49: 
1:335a993:   public static List<Centroid> centroidWritablesToList(Iterable<CentroidWritable> centroids) {
1:1d6dc49:     // A new list must be created because Hadoop iterators mutate the contents of the Writable in
1:1d6dc49:     // place, without allocating new references when iterating through the centroids Iterable.
1:1d6dc49:     return Lists.newArrayList(Iterables.transform(centroids, new Function<CentroidWritable, Centroid>() {
1:a330664:       @Override
1:1d6dc49:       public Centroid apply(CentroidWritable input) {
1:a330664:         Preconditions.checkNotNull(input);
1:1d6dc49:         return input.getCentroid().clone();
1:1d6dc49:       }
1:a330664:     }));
1:1d6dc49:   }
1:1d6dc49: 
1:1d6dc49:   public static Iterable<Vector> getBestCentroids(List<Centroid> centroids, Configuration conf) {
1:0c1b4a1: 
1:0c1b4a1:     if (log.isInfoEnabled()) {
1:0c1b4a1:       log.info("Number of Centroids: {}", centroids.size());
1:0c1b4a1:     }
1:0c1b4a1: 
1:1d6dc49:     int numClusters = conf.getInt(DefaultOptionCreator.NUM_CLUSTERS_OPTION, 1);
1:1d6dc49:     int maxNumIterations = conf.getInt(StreamingKMeansDriver.MAX_NUM_ITERATIONS, 10);
1:1d6dc49:     float trimFraction = conf.getFloat(StreamingKMeansDriver.TRIM_FRACTION, 0.9f);
1:1d6dc49:     boolean kMeansPlusPlusInit = !conf.getBoolean(StreamingKMeansDriver.RANDOM_INIT, false);
1:1d6dc49:     boolean correctWeights = !conf.getBoolean(StreamingKMeansDriver.IGNORE_WEIGHTS, false);
1:1d6dc49:     float testProbability = conf.getFloat(StreamingKMeansDriver.TEST_PROBABILITY, 0.1f);
1:1d6dc49:     int numRuns = conf.getInt(StreamingKMeansDriver.NUM_BALLKMEANS_RUNS, 3);
1:1d6dc49: 
1:0c1b4a1:     BallKMeans ballKMeansCluster = new BallKMeans(StreamingKMeansUtilsMR.searcherFromConfiguration(conf),
1:1d6dc49:         numClusters, maxNumIterations, trimFraction, kMeansPlusPlusInit, correctWeights, testProbability, numRuns);
1:0c1b4a1:     return ballKMeansCluster.cluster(centroids);
1:1d6dc49:   }
1:1d6dc49: }
============================================================================
author:pferrel
-------------------------------------------------------------------------------
commit:b988c49
author:frankscholten
-------------------------------------------------------------------------------
commit:1a42d85
author:smarthi
-------------------------------------------------------------------------------
commit:0c1b4a1
/////////////////////////////////////////////////////////////////////////
1: import org.slf4j.Logger;
1: import org.slf4j.LoggerFactory;
1: 
1:   private static final Logger log = LoggerFactory.getLogger(StreamingKMeansReducer.class);
1: 
/////////////////////////////////////////////////////////////////////////
1:               return input.getCentroid().clone();
/////////////////////////////////////////////////////////////////////////
1:       context.write(new IntWritable(index), new CentroidWritable((Centroid) centroid));
/////////////////////////////////////////////////////////////////////////
1: 
1:     if (log.isInfoEnabled()) {
1:       log.info("Number of Centroids: {}", centroids.size());
1:     }
1: 
/////////////////////////////////////////////////////////////////////////
1:     BallKMeans ballKMeansCluster = new BallKMeans(StreamingKMeansUtilsMR.searcherFromConfiguration(conf),
1:     return ballKMeansCluster.cluster(centroids);
author:Sean R. Owen
-------------------------------------------------------------------------------
commit:335a993
/////////////////////////////////////////////////////////////////////////
1:   public static List<Centroid> centroidWritablesToList(Iterable<CentroidWritable> centroids) {
commit:4ca6b86
/////////////////////////////////////////////////////////////////////////
0:         conf.getFloat(StreamingKMeansDriver.ESTIMATED_DISTANCE_CUTOFF, 1.0e-4f));
author:dfilimon
-------------------------------------------------------------------------------
commit:7f431a0
/////////////////////////////////////////////////////////////////////////
1:             @Override
1:               Preconditions.checkNotNull(input);
0:               return input.getCentroid();
1:             }
commit:6b6b8a0
/////////////////////////////////////////////////////////////////////////
/////////////////////////////////////////////////////////////////////////
/////////////////////////////////////////////////////////////////////////
1:       intermediateCentroids = Lists.newArrayList(
1:           new StreamingKMeansThread(Iterables.transform(centroids, new Function<CentroidWritable, Centroid>() {
1:             public Centroid apply(CentroidWritable input) {
1:           }), conf).call());
commit:a330664
/////////////////////////////////////////////////////////////////////////
0: import org.apache.mahout.clustering.streaming.cluster.StreamingKMeans;
0: import org.apache.mahout.math.neighborhood.UpdatableSearcher;
/////////////////////////////////////////////////////////////////////////
0:   private StreamingKMeans getStreamingKMeans(int numClusters) {
1:     // At this point the configuration received from the Driver is assumed to be valid.
1:     // No other checks are made.
0:     UpdatableSearcher searcher = StreamingKMeansUtilsMR.searcherFromConfiguration(conf);
0:     // There is no way of estimating the distance cutoff unless we have some data.
0:     return new StreamingKMeans(searcher, numClusters,
0:         conf.getFloat(StreamingKMeansDriver.ESTIMATED_DISTANCE_CUTOFF, 1e-4f));
1:   }
1: 
1:     List<Centroid> intermediateCentroids;
1:     // There might be too many intermediate centroids to fit into memory, in which case, we run another pass
1:     // of StreamingKMeans to collapse the clusters further.
1:     if (conf.getBoolean(StreamingKMeansDriver.REDUCE_STREAMING_KMEANS, false)) {
0:       StreamingKMeans clusterer = getStreamingKMeans(conf.getInt(StreamingKMeansDriver.ESTIMATED_NUM_MAP_CLUSTERS, 1));
0:       clusterer.cluster(Iterables.transform(centroids, new Function<CentroidWritable, Centroid>() {
1:         @Override
0:         public Centroid apply(org.apache.mahout.clustering.streaming.mapreduce.CentroidWritable input) {
1:           Preconditions.checkNotNull(input);
0:           return input.getCentroid();
1:         }
1:       }));
0:       clusterer.reindexCentroids();
0:       intermediateCentroids = Lists.newArrayList(clusterer);
1:     } else {
1:       intermediateCentroids = centroidWritablesToList(centroids);
1:     }
1: 
1:     for (Vector centroid : getBestCentroids(intermediateCentroids, conf)) {
commit:1d6dc49
/////////////////////////////////////////////////////////////////////////
1: /**
1:  * Licensed to the Apache Software Foundation (ASF) under one or more
1:  * contributor license agreements.  See the NOTICE file distributed with
1:  * this work for additional information regarding copyright ownership.
1:  * The ASF licenses this file to You under the Apache License, Version 2.0
1:  * (the "License"); you may not use this file except in compliance with
1:  * the License.  You may obtain a copy of the License at
1:  *
1:  *     http://www.apache.org/licenses/LICENSE-2.0
1:  *
1:  * Unless required by applicable law or agreed to in writing, software
1:  * distributed under the License is distributed on an "AS IS" BASIS,
1:  * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
1:  * See the License for the specific language governing permissions and
1:  * limitations under the License.
1:  */
1: 
1: package org.apache.mahout.clustering.streaming.mapreduce;
1: 
1: import java.io.IOException;
1: import java.util.List;
1: 
1: import com.google.common.base.Function;
1: import com.google.common.base.Preconditions;
1: import com.google.common.collect.Iterables;
1: import com.google.common.collect.Lists;
1: import org.apache.hadoop.conf.Configuration;
1: import org.apache.hadoop.io.IntWritable;
1: import org.apache.hadoop.mapreduce.Reducer;
1: import org.apache.mahout.clustering.streaming.cluster.BallKMeans;
1: import org.apache.mahout.common.commandline.DefaultOptionCreator;
1: import org.apache.mahout.math.Centroid;
1: import org.apache.mahout.math.Vector;
1: 
1: public class StreamingKMeansReducer extends Reducer<IntWritable, CentroidWritable, IntWritable, CentroidWritable> {
1:   /**
1:    * Configuration for the MapReduce job.
1:    */
1:   private Configuration conf;
1: 
1:   @Override
1:   public void setup(Context context) {
0:     // At this point the configuration received from the Driver is assumed to be valid.
0:     // No other checks are made.
1:     conf = context.getConfiguration();
1:   }
1: 
1:   @Override
1:   public void reduce(IntWritable key, Iterable<CentroidWritable> centroids,
1:                      Context context) throws IOException, InterruptedException {
1:     int index = 0;
0:     for (Vector centroid : getBestCentroids(centroidWritablesToList(centroids), conf)) {
0:       context.write(new IntWritable(index), new CentroidWritable((Centroid)centroid));
1:       ++index;
1:     }
1:   }
1: 
0:   public List<Centroid> centroidWritablesToList(Iterable<CentroidWritable> centroids) {
1:     // A new list must be created because Hadoop iterators mutate the contents of the Writable in
1:     // place, without allocating new references when iterating through the centroids Iterable.
1:     return Lists.newArrayList(Iterables.transform(centroids, new Function<CentroidWritable, Centroid>() {
1:       @Override
1:       public Centroid apply(CentroidWritable input) {
0:         Preconditions.checkNotNull(input);
1:         return input.getCentroid().clone();
1:       }
0:     }));
1:   }
1: 
1:   public static Iterable<Vector> getBestCentroids(List<Centroid> centroids, Configuration conf) {
1:     int numClusters = conf.getInt(DefaultOptionCreator.NUM_CLUSTERS_OPTION, 1);
1:     int maxNumIterations = conf.getInt(StreamingKMeansDriver.MAX_NUM_ITERATIONS, 10);
1:     float trimFraction = conf.getFloat(StreamingKMeansDriver.TRIM_FRACTION, 0.9f);
1:     boolean kMeansPlusPlusInit = !conf.getBoolean(StreamingKMeansDriver.RANDOM_INIT, false);
1:     boolean correctWeights = !conf.getBoolean(StreamingKMeansDriver.IGNORE_WEIGHTS, false);
1:     float testProbability = conf.getFloat(StreamingKMeansDriver.TEST_PROBABILITY, 0.1f);
1:     int numRuns = conf.getInt(StreamingKMeansDriver.NUM_BALLKMEANS_RUNS, 3);
1: 
0:     BallKMeans clusterer = new BallKMeans(StreamingKMeansUtilsMR.searcherFromConfiguration(conf),
1:         numClusters, maxNumIterations, trimFraction, kMeansPlusPlusInit, correctWeights, testProbability, numRuns);
0:     return clusterer.cluster(centroids);
1:   }
1: }
============================================================================