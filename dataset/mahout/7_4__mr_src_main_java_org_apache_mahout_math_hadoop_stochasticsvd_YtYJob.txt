1:ffc7fab: /**
1:ffc7fab:  * Licensed to the Apache Software Foundation (ASF) under one or more
1:ffc7fab:  * contributor license agreements.  See the NOTICE file distributed with
1:ffc7fab:  * this work for additional information regarding copyright ownership.
1:ffc7fab:  * The ASF licenses this file to You under the Apache License, Version 2.0
1:ffc7fab:  * (the "License"); you may not use this file except in compliance with
1:ffc7fab:  * the License.  You may obtain a copy of the License at
2:ffc7fab:  *
1:ffc7fab:  *     http://www.apache.org/licenses/LICENSE-2.0
1:ffc7fab:  *
1:ffc7fab:  * Unless required by applicable law or agreed to in writing, software
1:ffc7fab:  * distributed under the License is distributed on an "AS IS" BASIS,
1:ffc7fab:  * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
1:ffc7fab:  * See the License for the specific language governing permissions and
1:ffc7fab:  * limitations under the License.
1:ffc7fab:  */
1:ffc7fab: package org.apache.mahout.math.hadoop.stochasticsvd;
1:ffc7fab: 
1:67a531e: import org.apache.commons.lang3.Validate;
1:ffc7fab: import org.apache.hadoop.conf.Configuration;
1:ffc7fab: import org.apache.hadoop.fs.Path;
1:ffc7fab: import org.apache.hadoop.io.IntWritable;
1:ffc7fab: import org.apache.hadoop.io.SequenceFile.CompressionType;
1:ffc7fab: import org.apache.hadoop.io.Writable;
1:ffc7fab: import org.apache.hadoop.mapreduce.Job;
1:ffc7fab: import org.apache.hadoop.mapreduce.Mapper;
1:ffc7fab: import org.apache.hadoop.mapreduce.Reducer;
1:ffc7fab: import org.apache.hadoop.mapreduce.lib.input.FileInputFormat;
1:ffc7fab: import org.apache.hadoop.mapreduce.lib.input.SequenceFileInputFormat;
1:ffc7fab: import org.apache.hadoop.mapreduce.lib.output.FileOutputFormat;
1:ffc7fab: import org.apache.hadoop.mapreduce.lib.output.SequenceFileOutputFormat;
1:ffc7fab: import org.apache.mahout.math.DenseVector;
1:f43adfe: import org.apache.mahout.math.UpperTriangular;
1:ffc7fab: import org.apache.mahout.math.Vector;
1:ffc7fab: import org.apache.mahout.math.VectorWritable;
1:ffc7fab: 
1:f43adfe: import java.io.IOException;
1:f43adfe: 
1:ffc7fab: /**
1:ffc7fab:  * Job that accumulates Y'Y output
1:ffc7fab:  */
1:229aeff: public final class YtYJob {
1:ffc7fab: 
1:ffc7fab:   public static final String PROP_OMEGA_SEED = "ssvd.omegaseed";
1:ffc7fab:   public static final String PROP_K = "ssvd.k";
1:ffc7fab:   public static final String PROP_P = "ssvd.p";
1:ffc7fab: 
1:ffc7fab:   // we have single output, so we use standard output
1:229aeff:   public static final String OUTPUT_YT_Y = "part-";
1:ffc7fab: 
1:4194a28:   private YtYJob() {
1:4194a28:   }
1:ffc7fab: 
1:ffc7fab:   public static class YtYMapper extends
1:f43adfe:     Mapper<Writable, VectorWritable, IntWritable, VectorWritable> {
1:ffc7fab: 
1:ffc7fab:     private int kp;
1:ffc7fab:     private Omega omega;
1:ffc7fab:     private UpperTriangular mYtY;
1:ffc7fab: 
1:5a2250c:     /*
1:5a2250c:      * we keep yRow in a dense form here but keep an eye not to dense up while
1:5a2250c:      * doing YtY products. I am not sure that sparse vector would create much
1:5a2250c:      * performance benefits since we must to assume that y would be more often
1:5a2250c:      * dense than sparse, so for bulk dense operations that would perform
1:5a2250c:      * somewhat better than a RandomAccessSparse vector frequent updates.
1:5a2250c:      */
1:ffc7fab:     private Vector yRow;
1:ffc7fab: 
1:ffc7fab:     @Override
1:ffc7fab:     protected void setup(Context context) throws IOException,
1:175701c:       InterruptedException {
1:ffc7fab:       int k = context.getConfiguration().getInt(PROP_K, -1);
1:ffc7fab:       int p = context.getConfiguration().getInt(PROP_P, -1);
1:ffc7fab: 
1:ffc7fab:       Validate.isTrue(k > 0, "invalid k parameter");
1:ffc7fab:       Validate.isTrue(p > 0, "invalid p parameter");
1:ffc7fab: 
1:ffc7fab:       kp = k + p;
1:175701c:       long omegaSeed =
1:175701c:         Long.parseLong(context.getConfiguration().get(PROP_OMEGA_SEED));
1:ffc7fab: 
1:175701c:       omega = new Omega(omegaSeed, k + p);
1:ffc7fab: 
1:ffc7fab:       mYtY = new UpperTriangular(kp);
1:ffc7fab: 
1:ffc7fab:       // see which one works better!
1:ffc7fab:       // yRow = new RandomAccessSparseVector(kp);
1:ffc7fab:       yRow = new DenseVector(kp);
1:ffc7fab:     }
1:ffc7fab: 
1:ffc7fab:     @Override
1:ffc7fab:     protected void map(Writable key, VectorWritable value, Context context)
1:175701c:       throws IOException, InterruptedException {
1:ffc7fab:       omega.computeYRow(value.get(), yRow);
1:ffc7fab:       // compute outer product update for YtY
1:ffc7fab: 
1:ffc7fab:       if (yRow.isDense()) {
1:ffc7fab:         for (int i = 0; i < kp; i++) {
1:ffc7fab:           double yi;
1:1499411:           if ((yi = yRow.getQuick(i)) == 0.0) {
1:ffc7fab:             continue; // avoid densing up here unnecessarily
1:1499411:           }
1:ffc7fab:           for (int j = i; j < kp; j++) {
1:ffc7fab:             double yj;
1:1499411:             if ((yj = yRow.getQuick(j)) != 0.0) {
1:ffc7fab:               mYtY.setQuick(i, j, mYtY.getQuick(i, j) + yi * yj);
1:ffc7fab:             }
1:ffc7fab:           }
1:1499411:         }
1:ffc7fab:       } else {
1:5a2250c:         /*
1:5a2250c:          * the disadvantage of using sparse vector (aside from the fact that we
1:5a2250c:          * are creating some short-lived references) here is that we obviously
1:5a2250c:          * do two times more iterations then necessary if y row is pretty dense.
1:5a2250c:          */
1:dc62944:         for (Vector.Element eli : yRow.nonZeroes()) {
1:ffc7fab:           int i = eli.index();
1:dc62944:           for (Vector.Element elj : yRow.nonZeroes()) {
1:ffc7fab:             int j = elj.index();
1:1499411:             if (j < i) {
1:ffc7fab:               continue;
1:1499411:             }
1:ffc7fab:             mYtY.setQuick(i, j, mYtY.getQuick(i, j) + eli.get() * elj.get());
1:ffc7fab:           }
1:ffc7fab:         }
1:ffc7fab:       }
1:ffc7fab:     }
1:ffc7fab: 
1:ffc7fab:     @Override
1:ffc7fab:     protected void cleanup(Context context) throws IOException,
1:175701c:       InterruptedException {
1:ffc7fab:       context.write(new IntWritable(context.getTaskAttemptID().getTaskID()
1:f43adfe:                                       .getId()),
1:175701c:                     new VectorWritable(new DenseVector(mYtY.getData())));
1:ffc7fab:     }
1:ffc7fab:   }
1:ffc7fab: 
1:ffc7fab:   public static class YtYReducer extends
1:f43adfe:     Reducer<IntWritable, VectorWritable, IntWritable, VectorWritable> {
1:ffc7fab:     private final VectorWritable accum = new VectorWritable();
1:ffc7fab:     private DenseVector acc;
1:ffc7fab: 
1:ffc7fab:     @Override
1:ffc7fab:     protected void setup(Context context) throws IOException,
1:175701c:       InterruptedException {
1:ffc7fab:       int k = context.getConfiguration().getInt(PROP_K, -1);
1:ffc7fab:       int p = context.getConfiguration().getInt(PROP_P, -1);
1:ffc7fab: 
1:ffc7fab:       Validate.isTrue(k > 0, "invalid k parameter");
1:ffc7fab:       Validate.isTrue(p > 0, "invalid p parameter");
1:ffc7fab:       accum.set(acc = new DenseVector(k + p));
1:ffc7fab:     }
1:ffc7fab: 
1:ffc7fab:     @Override
1:ffc7fab:     protected void cleanup(Context context) throws IOException,
1:175701c:       InterruptedException {
1:ffc7fab:       context.write(new IntWritable(), accum);
1:ffc7fab:     }
1:ffc7fab: 
1:ffc7fab:     @Override
1:175701c:     protected void reduce(IntWritable key,
1:175701c:                           Iterable<VectorWritable> values,
1:175701c:                           Context arg2) throws IOException,
1:175701c:       InterruptedException {
1:1499411:       for (VectorWritable vw : values) {
1:ffc7fab:         acc.addAll(vw.get());
1:ffc7fab:       }
1:ffc7fab:     }
1:1499411:   }
1:ffc7fab: 
1:175701c:   public static void run(Configuration conf,
1:175701c:                          Path[] inputPaths,
1:175701c:                          Path outputPath,
1:175701c:                          int k,
1:175701c:                          int p,
1:175701c:                          long seed) throws ClassNotFoundException,
1:175701c:     InterruptedException, IOException {
1:ffc7fab: 
1:ffc7fab:     Job job = new Job(conf);
1:ffc7fab:     job.setJobName("YtY-job");
1:ffc7fab:     job.setJarByClass(YtYJob.class);
1:ffc7fab: 
1:ffc7fab:     job.setInputFormatClass(SequenceFileInputFormat.class);
1:ffc7fab:     FileInputFormat.setInputPaths(job, inputPaths);
1:ffc7fab:     FileOutputFormat.setOutputPath(job, outputPath);
1:ffc7fab: 
1:ffc7fab:     SequenceFileOutputFormat.setOutputCompressionType(job,
1:ffc7fab:                                                       CompressionType.BLOCK);
1:ffc7fab: 
1:ffc7fab:     job.setMapOutputKeyClass(IntWritable.class);
1:ffc7fab:     job.setMapOutputValueClass(VectorWritable.class);
1:ffc7fab: 
1:ffc7fab:     job.setOutputKeyClass(IntWritable.class);
1:ffc7fab:     job.setOutputValueClass(VectorWritable.class);
1:ffc7fab: 
1:ffc7fab:     job.setMapperClass(YtYMapper.class);
1:ffc7fab: 
1:ffc7fab:     job.getConfiguration().setLong(PROP_OMEGA_SEED, seed);
1:ffc7fab:     job.getConfiguration().setInt(PROP_K, k);
1:ffc7fab:     job.getConfiguration().setInt(PROP_P, p);
1:ffc7fab: 
1:5a2250c:     /*
1:5a2250c:      * we must reduce to just one matrix which means we need only one reducer.
1:5a2250c:      * But it's ok since each mapper outputs only one vector (a packed
1:5a2250c:      * UpperTriangular) so even if there're thousands of mappers, one reducer
1:5a2250c:      * should cope just fine.
1:5a2250c:      */
1:ffc7fab:     job.setNumReduceTasks(1);
1:ffc7fab: 
1:ffc7fab:     job.submit();
1:ffc7fab:     job.waitForCompletion(false);
1:ffc7fab: 
1:ffc7fab:     if (!job.isSuccessful()) {
1:ffc7fab:       throw new IOException("YtY job unsuccessful.");
1:ffc7fab:     }
1:ffc7fab: 
1:ffc7fab:   }
1:ffc7fab: 
1:ffc7fab: }
============================================================================
author:pferrel
-------------------------------------------------------------------------------
commit:b988c49
author:frankscholten
-------------------------------------------------------------------------------
commit:1a42d85
author:Dmitriy Lyubimov
-------------------------------------------------------------------------------
commit:f43adfe
/////////////////////////////////////////////////////////////////////////
/////////////////////////////////////////////////////////////////////////
1: import org.apache.mahout.math.UpperTriangular;
1: import java.io.IOException;
1: 
/////////////////////////////////////////////////////////////////////////
1:     Mapper<Writable, VectorWritable, IntWritable, VectorWritable> {
/////////////////////////////////////////////////////////////////////////
1:                                       .getId()),
1:     Reducer<IntWritable, VectorWritable, IntWritable, VectorWritable> {
commit:175701c
/////////////////////////////////////////////////////////////////////////
1:       InterruptedException {
/////////////////////////////////////////////////////////////////////////
1:       long omegaSeed =
1:         Long.parseLong(context.getConfiguration().get(PROP_OMEGA_SEED));
1:       omega = new Omega(omegaSeed, k + p);
/////////////////////////////////////////////////////////////////////////
1:       throws IOException, InterruptedException {
/////////////////////////////////////////////////////////////////////////
0:         for (Iterator<Vector.Element> iterI = yRow.iterateNonZero(); iterI.hasNext();) {
0:           for (Iterator<Vector.Element> iterJ = yRow.iterateNonZero(); iterJ.hasNext();) {
/////////////////////////////////////////////////////////////////////////
1:       InterruptedException {
0:                                            .getId()),
1:                     new VectorWritable(new DenseVector(mYtY.getData())));
/////////////////////////////////////////////////////////////////////////
1:       InterruptedException {
/////////////////////////////////////////////////////////////////////////
1:       InterruptedException {
1:     protected void reduce(IntWritable key,
1:                           Iterable<VectorWritable> values,
1:                           Context arg2) throws IOException,
1:       InterruptedException {
1:   public static void run(Configuration conf,
1:                          Path[] inputPaths,
1:                          Path outputPath,
1:                          int k,
1:                          int p,
1:                          long seed) throws ClassNotFoundException,
1:     InterruptedException, IOException {
commit:5a2250c
/////////////////////////////////////////////////////////////////////////
1:     /*
1:      * we keep yRow in a dense form here but keep an eye not to dense up while
1:      * doing YtY products. I am not sure that sparse vector would create much
1:      * performance benefits since we must to assume that y would be more often
1:      * dense than sparse, so for bulk dense operations that would perform
1:      * somewhat better than a RandomAccessSparse vector frequent updates.
1:      */
/////////////////////////////////////////////////////////////////////////
1:         /*
1:          * the disadvantage of using sparse vector (aside from the fact that we
1:          * are creating some short-lived references) here is that we obviously
1:          * do two times more iterations then necessary if y row is pretty dense.
1:          */
/////////////////////////////////////////////////////////////////////////
1:     /*
1:      * we must reduce to just one matrix which means we need only one reducer.
1:      * But it's ok since each mapper outputs only one vector (a packed
1:      * UpperTriangular) so even if there're thousands of mappers, one reducer
1:      * should cope just fine.
1:      */
commit:ffc7fab
/////////////////////////////////////////////////////////////////////////
1: /**
1:  * Licensed to the Apache Software Foundation (ASF) under one or more
1:  * contributor license agreements.  See the NOTICE file distributed with
1:  * this work for additional information regarding copyright ownership.
1:  * The ASF licenses this file to You under the Apache License, Version 2.0
1:  * (the "License"); you may not use this file except in compliance with
1:  * the License.  You may obtain a copy of the License at
1:  *
1:  *     http://www.apache.org/licenses/LICENSE-2.0
1:  *
1:  * Unless required by applicable law or agreed to in writing, software
1:  * distributed under the License is distributed on an "AS IS" BASIS,
1:  * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
1:  * See the License for the specific language governing permissions and
1:  * limitations under the License.
1:  */
1: package org.apache.mahout.math.hadoop.stochasticsvd;
1: 
0: import java.io.IOException;
0: import java.util.Iterator;
1: 
0: import org.apache.commons.lang.Validate;
1: import org.apache.hadoop.conf.Configuration;
1: import org.apache.hadoop.fs.Path;
1: import org.apache.hadoop.io.IntWritable;
1: import org.apache.hadoop.io.SequenceFile.CompressionType;
1: import org.apache.hadoop.io.Writable;
1: import org.apache.hadoop.mapreduce.Job;
1: import org.apache.hadoop.mapreduce.Mapper;
1: import org.apache.hadoop.mapreduce.Reducer;
1: import org.apache.hadoop.mapreduce.lib.input.FileInputFormat;
1: import org.apache.hadoop.mapreduce.lib.input.SequenceFileInputFormat;
1: import org.apache.hadoop.mapreduce.lib.output.FileOutputFormat;
1: import org.apache.hadoop.mapreduce.lib.output.SequenceFileOutputFormat;
1: import org.apache.mahout.math.DenseVector;
1: import org.apache.mahout.math.Vector;
1: import org.apache.mahout.math.VectorWritable;
1: 
1: /**
1:  * Job that accumulates Y'Y output
1:  * 
1:  */
0: public class YtYJob {
1: 
1:   public static final String PROP_OMEGA_SEED = "ssvd.omegaseed";
1:   public static final String PROP_K = "ssvd.k";
1:   public static final String PROP_P = "ssvd.p";
1: 
1:   // we have single output, so we use standard output
0:   public static final String OUTPUT_YtY = "part-";
1: 
1:   public static class YtYMapper extends
0:       Mapper<Writable, VectorWritable, IntWritable, VectorWritable> {
1: 
1:     private int kp;
1:     private Omega omega;
1:     private UpperTriangular mYtY;
1: 
0:     // we keep yRow in a dense form here
0:     // but keep an eye not to dense up while doing YtY products.
0:     // I am not sure that sparse vector would create much performance
0:     // benefits since we must to assume that y would be more often
0:     // dense than sparse, so for bulk dense operations that would perform
0:     // somewhat better than a RandomAccessSparse vector frequent updates.
1:     private Vector yRow;
1: 
1:     @Override
1:     protected void setup(Context context) throws IOException,
0:         InterruptedException {
1:       int k = context.getConfiguration().getInt(PROP_K, -1);
1:       int p = context.getConfiguration().getInt(PROP_P, -1);
1: 
1:       Validate.isTrue(k > 0, "invalid k parameter");
1:       Validate.isTrue(p > 0, "invalid p parameter");
1: 
1:       kp = k + p;
0:       long omegaSeed = Long.parseLong(context.getConfiguration()
0:           .get(PROP_OMEGA_SEED));
1: 
0:       omega = new Omega(omegaSeed, k, p);
1: 
1:       mYtY = new UpperTriangular(kp);
1: 
1:       // see which one works better!
1:       // yRow = new RandomAccessSparseVector(kp);
1:       yRow = new DenseVector(kp);
1:     }
1: 
1:     @Override
1:     protected void map(Writable key, VectorWritable value, Context context)
0:         throws IOException, InterruptedException {
1:       omega.computeYRow(value.get(), yRow);
1:       // compute outer product update for YtY
1: 
1:       if (yRow.isDense()) {
1:         for (int i = 0; i < kp; i++) {
1:           double yi;
0:           if ((yi = yRow.getQuick(i)) == 0.0)
1:             continue; // avoid densing up here unnecessarily
1:           for (int j = i; j < kp; j++) {
1:             double yj;
0:             if ((yj = yRow.getQuick(j)) != 0.0)
1:               mYtY.setQuick(i, j, mYtY.getQuick(i, j) + yi * yj);
1:           }
1:         }
1:       } else {
0:         // the disadvantage of using sparse vector (aside from the fact that we
0:         // are creating some short-lived references) here is that we obviously
0:         // do two times more iterations then necessary if y row is pretty dense.
0:         for (Iterator<Vector.Element> iterI = yRow.iterateNonZero(); iterI
0:             .hasNext();) {
0:           Vector.Element eli = iterI.next();
1:           int i = eli.index();
0:           for (Iterator<Vector.Element> iterJ = yRow.iterateNonZero(); iterJ
0:               .hasNext();) {
0:             Vector.Element elj = iterJ.next();
1:             int j = elj.index();
0:             if (j < i)
1:               continue;
1:             mYtY.setQuick(i, j, mYtY.getQuick(i, j) + eli.get() * elj.get());
1:           }
1:         }
1:       }
1:     }
1: 
1:     @Override
1:     protected void cleanup(Context context) throws IOException,
0:         InterruptedException {
1:       context.write(new IntWritable(context.getTaskAttemptID().getTaskID()
0:           .getId()), new VectorWritable(new DenseVector(mYtY.getData())));
1:     }
1:   }
1: 
1:   public static class YtYReducer extends
0:       Reducer<IntWritable, VectorWritable, IntWritable, VectorWritable> {
1:     private final VectorWritable accum = new VectorWritable();
1:     private DenseVector acc;
1: 
1:     @Override
1:     protected void setup(Context context) throws IOException,
0:         InterruptedException {
1:       int k = context.getConfiguration().getInt(PROP_K, -1);
1:       int p = context.getConfiguration().getInt(PROP_P, -1);
1: 
1:       Validate.isTrue(k > 0, "invalid k parameter");
1:       Validate.isTrue(p > 0, "invalid p parameter");
1:       accum.set(acc = new DenseVector(k + p));
1:     }
1: 
1:     @Override
1:     protected void cleanup(Context context) throws IOException,
0:         InterruptedException {
1:       context.write(new IntWritable(), accum);
1:     }
1: 
1:     @Override
0:     protected void reduce(IntWritable key, Iterable<VectorWritable> values,
0:         Context arg2) throws IOException, InterruptedException {
0:       for (VectorWritable vw : values)
1:         acc.addAll(vw.get());
1:     }
1:   }
1: 
0:   public static void run(Configuration conf, Path[] inputPaths,
0:       Path outputPath, int k, int p, long seed, int numReduceTasks)
0:       throws ClassNotFoundException, InterruptedException, IOException {
1: 
1:     Job job = new Job(conf);
1:     job.setJobName("YtY-job");
1:     job.setJarByClass(YtYJob.class);
1: 
1:     job.setInputFormatClass(SequenceFileInputFormat.class);
1:     FileInputFormat.setInputPaths(job, inputPaths);
1:     FileOutputFormat.setOutputPath(job, outputPath);
1: 
1:     SequenceFileOutputFormat.setOutputCompressionType(job,
1:                                                       CompressionType.BLOCK);
1: 
1:     job.setMapOutputKeyClass(IntWritable.class);
1:     job.setMapOutputValueClass(VectorWritable.class);
1: 
1:     job.setOutputKeyClass(IntWritable.class);
1:     job.setOutputValueClass(VectorWritable.class);
1: 
1:     job.setMapperClass(YtYMapper.class);
1: 
1:     job.getConfiguration().setLong(PROP_OMEGA_SEED, seed);
1:     job.getConfiguration().setInt(PROP_K, k);
1:     job.getConfiguration().setInt(PROP_P, p);
1: 
0:     // we must reduce to just one matrix which means
0:     // we need only one reducer.
0:     // But it's ok since each mapper outputs only one
0:     // vector (a packed UpperTriangular) so even if
0:     // there're thousands of mappers, one reducer should cope just fine.
1:     job.setNumReduceTasks(1);
1: 
1:     job.submit();
1:     job.waitForCompletion(false);
1: 
1:     if (!job.isSuccessful()) {
1:       throw new IOException("YtY job unsuccessful.");
1:     }
1: 
1:   }
1: 
1: }
author:Sebastian Schelter
-------------------------------------------------------------------------------
commit:8b194c8
/////////////////////////////////////////////////////////////////////////
0:      * dense than sparse, so for bulk dense operations that would perform
commit:58cc1ae
/////////////////////////////////////////////////////////////////////////
0:      * dense than sparse, so for bulk dense OPERATIONS that would perform
author:Jacob Alexander Mannix
-------------------------------------------------------------------------------
commit:dc62944
/////////////////////////////////////////////////////////////////////////
/////////////////////////////////////////////////////////////////////////
1:         for (Vector.Element eli : yRow.nonZeroes()) {
1:           for (Vector.Element elj : yRow.nonZeroes()) {
author:smarthi
-------------------------------------------------------------------------------
commit:67a531e
/////////////////////////////////////////////////////////////////////////
1: import org.apache.commons.lang3.Validate;
author:Sean R. Owen
-------------------------------------------------------------------------------
commit:229aeff
/////////////////////////////////////////////////////////////////////////
1: public final class YtYJob {
1:   public static final String OUTPUT_YT_Y = "part-";
commit:4fbfbc6
/////////////////////////////////////////////////////////////////////////
0:       Path outputPath, int k, int p, long seed)
commit:4194a28
/////////////////////////////////////////////////////////////////////////
1:   private YtYJob() {
1:   }
0: 
commit:1499411
/////////////////////////////////////////////////////////////////////////
1:           if ((yi = yRow.getQuick(i)) == 0.0) {
1:           }
1:             if ((yj = yRow.getQuick(j)) != 0.0) {
1:             }
/////////////////////////////////////////////////////////////////////////
1:             if (j < i) {
1:             }
/////////////////////////////////////////////////////////////////////////
1:       for (VectorWritable vw : values) {
1:       }
============================================================================