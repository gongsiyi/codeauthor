1:1d6dc49: /**
1:1d6dc49:  * Licensed to the Apache Software Foundation (ASF) under one or more
1:1d6dc49:  * contributor license agreements.  See the NOTICE file distributed with
1:1d6dc49:  * this work for additional information regarding copyright ownership.
1:1d6dc49:  * The ASF licenses this file to You under the Apache License, Version 2.0
1:1d6dc49:  * (the "License"); you may not use this file except in compliance with
1:1d6dc49:  * the License.  You may obtain a copy of the License at
1:1d6dc49:  *
1:1d6dc49:  *     http://www.apache.org/licenses/LICENSE-2.0
1:1d6dc49:  *
1:1d6dc49:  * Unless required by applicable law or agreed to in writing, software
1:1d6dc49:  * distributed under the License is distributed on an "AS IS" BASIS,
1:1d6dc49:  * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
1:1d6dc49:  * See the License for the specific language governing permissions and
1:1d6dc49:  * limitations under the License.
1:1d6dc49:  */
1:1d6dc49: 
1:1d6dc49: package org.apache.mahout.clustering.streaming.mapreduce;
1:1d6dc49: 
1:1d6dc49: import java.io.IOException;
1:1d6dc49: import java.util.Arrays;
1:1d6dc49: import java.util.List;
1:1d6dc49: 
1:1d6dc49: import com.google.common.base.Function;
1:1d6dc49: import com.google.common.collect.Iterables;
1:1d6dc49: import com.google.common.collect.Lists;
1:1d6dc49: import org.apache.hadoop.conf.Configuration;
1:1d6dc49: import org.apache.hadoop.fs.Path;
1:1d6dc49: import org.apache.hadoop.io.IntWritable;
1:1d6dc49: import org.apache.hadoop.io.Writable;
1:1d6dc49: import org.apache.hadoop.mrunit.mapreduce.MapDriver;
1:1d6dc49: import org.apache.hadoop.mrunit.mapreduce.MapReduceDriver;
1:1d6dc49: import org.apache.hadoop.mrunit.mapreduce.ReduceDriver;
1:1d6dc49: import org.apache.mahout.clustering.ClusteringUtils;
1:1d6dc49: import org.apache.mahout.clustering.streaming.cluster.DataUtils;
1:1d6dc49: import org.apache.mahout.clustering.streaming.cluster.StreamingKMeans;
1:921e201: import org.apache.mahout.common.MahoutTestCase;
1:1d6dc49: import org.apache.mahout.common.Pair;
1:9abf755: import org.apache.mahout.common.RandomUtils;
1:1d6dc49: import org.apache.mahout.common.commandline.DefaultOptionCreator;
1:1d6dc49: import org.apache.mahout.common.distance.SquaredEuclideanDistanceMeasure;
1:1d6dc49: import org.apache.mahout.common.iterator.sequencefile.SequenceFileIterable;
1:1d6dc49: import org.apache.mahout.math.Centroid;
1:1d6dc49: import org.apache.mahout.math.Vector;
1:1d6dc49: import org.apache.mahout.math.VectorWritable;
1:1d6dc49: import org.apache.mahout.math.neighborhood.BruteSearch;
1:1d6dc49: import org.apache.mahout.math.neighborhood.FastProjectionSearch;
1:ec9035c: import org.apache.mahout.math.neighborhood.LocalitySensitiveHashSearch;
1:1d6dc49: import org.apache.mahout.math.neighborhood.ProjectionSearch;
1:1d6dc49: import org.apache.mahout.math.random.WeightedThing;
1:9abf755: import org.junit.Before;
1:1d6dc49: import org.junit.Test;
1:1d6dc49: import org.junit.runner.RunWith;
1:1d6dc49: import org.junit.runners.Parameterized;
1:1d6dc49: 
1:1d6dc49: import static org.junit.Assert.assertEquals;
1:1d6dc49: import static org.junit.Assert.assertTrue;
1:1d6dc49: 
1:4ca6b86: @RunWith(Parameterized.class)
1:921e201: public class StreamingKMeansTestMR extends MahoutTestCase {
1:1d6dc49:   private static final int NUM_DATA_POINTS = 1 << 15;
1:1d6dc49:   private static final int NUM_DIMENSIONS = 8;
1:1d6dc49:   private static final int NUM_PROJECTIONS = 3;
1:1d6dc49:   private static final int SEARCH_SIZE = 5;
1:1d6dc49:   private static final int MAX_NUM_ITERATIONS = 10;
1:4ca6b86:   private static final double DISTANCE_CUTOFF = 1.0e-6;
1:1d6dc49: 
1:9abf755:   private static Pair<List<Centroid>, List<Centroid>> syntheticData;
1:9abf755: 
1:9abf755:   @Before
1:9abf755:   public void setUp() {
1:9abf755:     RandomUtils.useTestSeed();
1:9abf755:     syntheticData =
1:4ca6b86:       DataUtils.sampleMultiNormalHypercube(NUM_DIMENSIONS, NUM_DATA_POINTS, 1.0e-4);
1:9abf755:   }
1:1d6dc49: 
1:4ca6b86:   private final String searcherClassName;
1:4ca6b86:   private final String distanceMeasureClassName;
1:1d6dc49: 
1:1d6dc49:   public StreamingKMeansTestMR(String searcherClassName, String distanceMeasureClassName) {
1:1d6dc49:     this.searcherClassName = searcherClassName;
1:1d6dc49:     this.distanceMeasureClassName = distanceMeasureClassName;
1:1d6dc49:   }
1:1d6dc49: 
1:1d6dc49:   private void configure(Configuration configuration) {
1:1d6dc49:     configuration.set(DefaultOptionCreator.DISTANCE_MEASURE_OPTION, distanceMeasureClassName);
1:1d6dc49:     configuration.setInt(StreamingKMeansDriver.SEARCH_SIZE_OPTION, SEARCH_SIZE);
1:1d6dc49:     configuration.setInt(StreamingKMeansDriver.NUM_PROJECTIONS_OPTION, NUM_PROJECTIONS);
1:1d6dc49:     configuration.set(StreamingKMeansDriver.SEARCHER_CLASS_OPTION, searcherClassName);
1:1d6dc49:     configuration.setInt(DefaultOptionCreator.NUM_CLUSTERS_OPTION, 1 << NUM_DIMENSIONS);
1:1d6dc49:     configuration.setInt(StreamingKMeansDriver.ESTIMATED_NUM_MAP_CLUSTERS,
1:1d6dc49:         (1 << NUM_DIMENSIONS) * (int)Math.log(NUM_DATA_POINTS));
1:1d6dc49:     configuration.setFloat(StreamingKMeansDriver.ESTIMATED_DISTANCE_CUTOFF, (float) DISTANCE_CUTOFF);
1:1d6dc49:     configuration.setInt(StreamingKMeansDriver.MAX_NUM_ITERATIONS, MAX_NUM_ITERATIONS);
1:a330664: 
1:a330664:     // Collapse the Centroids in the reducer.
1:a330664:     configuration.setBoolean(StreamingKMeansDriver.REDUCE_STREAMING_KMEANS, true);
1:1d6dc49:   }
1:1d6dc49: 
1:1d6dc49:   @Parameterized.Parameters
1:1d6dc49:   public static List<Object[]> generateData() {
1:1d6dc49:     return Arrays.asList(new Object[][]{
1:1d6dc49:         {ProjectionSearch.class.getName(), SquaredEuclideanDistanceMeasure.class.getName()},
1:1d6dc49:         {FastProjectionSearch.class.getName(), SquaredEuclideanDistanceMeasure.class.getName()},
1:ec9035c:         {LocalitySensitiveHashSearch.class.getName(), SquaredEuclideanDistanceMeasure.class.getName()},
1:1d6dc49:     });
1:1d6dc49:   }
1:1d6dc49: 
1:1d6dc49:   @Test
1:1d6dc49:   public void testHypercubeMapper() throws IOException {
1:1d6dc49:     MapDriver<Writable, VectorWritable, IntWritable, CentroidWritable> mapDriver =
1:1d6dc49:         MapDriver.newMapDriver(new StreamingKMeansMapper());
1:1d6dc49:     configure(mapDriver.getConfiguration());
1:1d6dc49:     System.out.printf("%s mapper test\n",
1:1d6dc49:         mapDriver.getConfiguration().get(StreamingKMeansDriver.SEARCHER_CLASS_OPTION));
1:1d6dc49:     for (Centroid datapoint : syntheticData.getFirst()) {
1:1d6dc49:       mapDriver.addInput(new IntWritable(0), new VectorWritable(datapoint));
1:1d6dc49:     }
1:1d6dc49:     List<org.apache.hadoop.mrunit.types.Pair<IntWritable,CentroidWritable>> results = mapDriver.run();
1:1d6dc49:     BruteSearch resultSearcher = new BruteSearch(new SquaredEuclideanDistanceMeasure());
1:1d6dc49:     for (org.apache.hadoop.mrunit.types.Pair<IntWritable, CentroidWritable> result : results) {
1:1d6dc49:       resultSearcher.add(result.getSecond().getCentroid());
1:1d6dc49:     }
1:1d6dc49:     System.out.printf("Clustered the data into %d clusters\n", results.size());
1:1d6dc49:     for (Vector mean : syntheticData.getSecond()) {
1:1d6dc49:       WeightedThing<Vector> closest = resultSearcher.search(mean, 1).get(0);
1:1d6dc49:       assertTrue("Weight " + closest.getWeight() + " not less than 0.5", closest.getWeight() < 0.5);
1:1d6dc49:     }
1:1d6dc49:   }
1:1d6dc49: 
1:1d6dc49:   @Test
1:1d6dc49:   public void testMapperVsLocal() throws IOException {
1:1d6dc49:     // Clusters the data using the StreamingKMeansMapper.
1:1d6dc49:     MapDriver<Writable, VectorWritable, IntWritable, CentroidWritable> mapDriver =
1:1d6dc49:         MapDriver.newMapDriver(new StreamingKMeansMapper());
1:1d6dc49:     Configuration configuration = mapDriver.getConfiguration();
1:1d6dc49:     configure(configuration);
1:1d6dc49:     System.out.printf("%s mapper vs local test\n",
1:1d6dc49:         mapDriver.getConfiguration().get(StreamingKMeansDriver.SEARCHER_CLASS_OPTION));
1:1d6dc49: 
1:1d6dc49:     for (Centroid datapoint : syntheticData.getFirst()) {
1:1d6dc49:       mapDriver.addInput(new IntWritable(0), new VectorWritable(datapoint));
1:1d6dc49:     }
1:1d6dc49:     List<Centroid> mapperCentroids = Lists.newArrayList();
1:1d6dc49:     for (org.apache.hadoop.mrunit.types.Pair<IntWritable, CentroidWritable> pair : mapDriver.run()) {
1:1d6dc49:       mapperCentroids.add(pair.getSecond().getCentroid());
1:1d6dc49:     }
1:1d6dc49: 
1:1d6dc49:     // Clusters the data using local batch StreamingKMeans.
1:1d6dc49:     StreamingKMeans batchClusterer =
1:1d6dc49:         new StreamingKMeans(StreamingKMeansUtilsMR.searcherFromConfiguration(configuration),
1:1d6dc49:             mapDriver.getConfiguration().getInt("estimatedNumMapClusters", -1), DISTANCE_CUTOFF);
1:1d6dc49:     batchClusterer.cluster(syntheticData.getFirst());
1:1d6dc49:     List<Centroid> batchCentroids = Lists.newArrayList();
1:1d6dc49:     for (Vector v : batchClusterer) {
1:1d6dc49:       batchCentroids.add((Centroid) v);
1:1d6dc49:     }
1:1d6dc49: 
1:1d6dc49:     // Clusters the data using point by point StreamingKMeans.
1:1d6dc49:     StreamingKMeans perPointClusterer =
1:1d6dc49:         new StreamingKMeans(StreamingKMeansUtilsMR.searcherFromConfiguration(configuration),
1:1d6dc49:             (1 << NUM_DIMENSIONS) * (int)Math.log(NUM_DATA_POINTS), DISTANCE_CUTOFF);
1:1d6dc49:     for (Centroid datapoint : syntheticData.getFirst()) {
1:1d6dc49:       perPointClusterer.cluster(datapoint);
1:1d6dc49:     }
1:1d6dc49:     List<Centroid> perPointCentroids = Lists.newArrayList();
1:1d6dc49:     for (Vector v : perPointClusterer) {
1:1d6dc49:       perPointCentroids.add((Centroid) v);
1:1d6dc49:     }
1:1d6dc49: 
1:1d6dc49:     // Computes the cost (total sum of distances) of these different clusterings.
1:1d6dc49:     double mapperCost = ClusteringUtils.totalClusterCost(syntheticData.getFirst(), mapperCentroids);
1:1d6dc49:     double localCost = ClusteringUtils.totalClusterCost(syntheticData.getFirst(), batchCentroids);
1:1d6dc49:     double perPointCost = ClusteringUtils.totalClusterCost(syntheticData.getFirst(), perPointCentroids);
1:1d6dc49:     System.out.printf("[Total cost] Mapper %f [%d] Local %f [%d] Perpoint local %f [%d];" +
1:1d6dc49:         "[ratio m-vs-l %f] [ratio pp-vs-l %f]\n", mapperCost, mapperCentroids.size(),
1:1d6dc49:         localCost, batchCentroids.size(), perPointCost, perPointCentroids.size(),
1:1d6dc49:         mapperCost / localCost, perPointCost / localCost);
1:1d6dc49: 
1:1d6dc49:     // These ratios should be close to 1.0 and have been observed to be go as low as 0.6 and as low as 1.5.
1:1d6dc49:     // A buffer of [0.2, 1.8] seems appropriate.
1:1d6dc49:     assertEquals("Mapper StreamingKMeans / Batch local StreamingKMeans total cost ratio too far from 1",
1:1d6dc49:         1.0, mapperCost / localCost, 0.8);
1:1d6dc49:     assertEquals("One by one local StreamingKMeans / Batch local StreamingKMeans total cost ratio too high",
1:1d6dc49:         1.0, perPointCost / localCost, 0.8);
1:1d6dc49:   }
1:1d6dc49: 
1:1d6dc49:   @Test
1:1d6dc49:   public void testHypercubeReducer() throws IOException {
1:1d6dc49:     ReduceDriver<IntWritable, CentroidWritable, IntWritable, CentroidWritable> reduceDriver =
1:1d6dc49:         ReduceDriver.newReduceDriver(new StreamingKMeansReducer());
1:1d6dc49:     Configuration configuration = reduceDriver.getConfiguration();
1:1d6dc49:     configure(configuration);
1:1d6dc49: 
1:1d6dc49:     System.out.printf("%s reducer test\n", configuration.get(StreamingKMeansDriver.SEARCHER_CLASS_OPTION));
1:1d6dc49:     StreamingKMeans clusterer =
1:1d6dc49:         new StreamingKMeans(StreamingKMeansUtilsMR .searcherFromConfiguration(configuration),
1:1d6dc49:             (1 << NUM_DIMENSIONS) * (int)Math.log(NUM_DATA_POINTS), DISTANCE_CUTOFF);
1:1d6dc49: 
1:1d6dc49:     long start = System.currentTimeMillis();
1:1d6dc49:     clusterer.cluster(syntheticData.getFirst());
1:1d6dc49:     long end = System.currentTimeMillis();
1:1d6dc49: 
1:1d6dc49:     System.out.printf("%f [s]\n", (end - start) / 1000.0);
1:1d6dc49:     List<CentroidWritable> reducerInputs = Lists.newArrayList();
1:1d6dc49:     int postMapperTotalWeight = 0;
1:1d6dc49:     for (Centroid intermediateCentroid : clusterer) {
1:1d6dc49:       reducerInputs.add(new CentroidWritable(intermediateCentroid));
1:1d6dc49:       postMapperTotalWeight += intermediateCentroid.getWeight();
1:1d6dc49:     }
1:1d6dc49: 
1:1d6dc49:     reduceDriver.addInput(new IntWritable(0), reducerInputs);
1:1d6dc49:     List<org.apache.hadoop.mrunit.types.Pair<IntWritable, CentroidWritable>> results =
1:1d6dc49:         reduceDriver.run();
1:1d6dc49:     testReducerResults(postMapperTotalWeight, results);
1:1d6dc49:   }
1:1d6dc49: 
1:1d6dc49:   @Test
1:1d6dc49:   public void testHypercubeMapReduce() throws IOException {
1:1d6dc49:     MapReduceDriver<Writable, VectorWritable, IntWritable, CentroidWritable, IntWritable, CentroidWritable>
1:02ff22f:         mapReduceDriver = new MapReduceDriver<>(new StreamingKMeansMapper(), new StreamingKMeansReducer());
1:1d6dc49:     Configuration configuration = mapReduceDriver.getConfiguration();
1:1d6dc49:     configure(configuration);
1:1d6dc49: 
1:1d6dc49:     System.out.printf("%s full test\n", configuration.get(StreamingKMeansDriver.SEARCHER_CLASS_OPTION));
1:1d6dc49:     for (Centroid datapoint : syntheticData.getFirst()) {
1:1d6dc49:       mapReduceDriver.addInput(new IntWritable(0), new VectorWritable(datapoint));
1:1d6dc49:     }
1:1d6dc49:     List<org.apache.hadoop.mrunit.types.Pair<IntWritable, CentroidWritable>> results = mapReduceDriver.run();
1:1d6dc49:     testReducerResults(syntheticData.getFirst().size(), results);
1:1d6dc49:   }
1:1d6dc49: 
1:1d6dc49:   @Test
1:4ca6b86:   public void testHypercubeMapReduceRunSequentially() throws Exception {
1:921e201:     Configuration configuration = getConfiguration();
1:1d6dc49:     configure(configuration);
1:1d6dc49:     configuration.set(DefaultOptionCreator.METHOD_OPTION, DefaultOptionCreator.SEQUENTIAL_METHOD);
1:1d6dc49: 
1:1d6dc49:     Path inputPath = new Path("testInput");
1:1d6dc49:     Path outputPath = new Path("testOutput");
1:1d6dc49:     StreamingKMeansUtilsMR.writeVectorsToSequenceFile(syntheticData.getFirst(), inputPath, configuration);
1:1d6dc49: 
1:1d6dc49:     StreamingKMeansDriver.run(configuration, inputPath, outputPath);
1:1d6dc49: 
1:1d6dc49:     testReducerResults(syntheticData.getFirst().size(),
1:1d6dc49:         Lists.newArrayList(Iterables.transform(
1:1d6dc49:             new SequenceFileIterable<IntWritable, CentroidWritable>(outputPath, configuration),
1:1d6dc49:             new Function<
1:1d6dc49:                 Pair<IntWritable, CentroidWritable>,
1:1d6dc49:                 org.apache.hadoop.mrunit.types.Pair<IntWritable, CentroidWritable>>() {
1:1d6dc49:               @Override
1:1d6dc49:               public org.apache.hadoop.mrunit.types.Pair<IntWritable, CentroidWritable> apply(
1:1d6dc49:                   org.apache.mahout.common.Pair<IntWritable, CentroidWritable> input) {
1:02ff22f:                 return new org.apache.hadoop.mrunit.types.Pair<>(
1:1d6dc49:                     input.getFirst(), input.getSecond());
1:1d6dc49:               }
1:1d6dc49:             })));
1:1d6dc49:   }
1:1d6dc49: 
1:4ca6b86:   private static void testReducerResults(int totalWeight, List<org.apache.hadoop.mrunit.types.Pair<IntWritable,
1:1d6dc49:       CentroidWritable>> results) {
1:1d6dc49:     int expectedNumClusters = 1 << NUM_DIMENSIONS;
1:4ca6b86:     double expectedWeight = (double) totalWeight / expectedNumClusters;
1:1d6dc49:     int numClusters = 0;
1:1d6dc49:     int numUnbalancedClusters = 0;
1:1d6dc49:     int totalReducerWeight = 0;
1:1d6dc49:     for (org.apache.hadoop.mrunit.types.Pair<IntWritable, CentroidWritable> result : results) {
1:1d6dc49:       if (result.getSecond().getCentroid().getWeight() != expectedWeight) {
1:1d6dc49:         System.out.printf("Unbalanced weight %f in centroid %d\n",  result.getSecond().getCentroid().getWeight(),
1:1d6dc49:             result.getSecond().getCentroid().getIndex());
1:1d6dc49:         ++numUnbalancedClusters;
1:1d6dc49:       }
1:1d6dc49:       assertEquals("Final centroid index is invalid", numClusters, result.getFirst().get());
1:1d6dc49:       totalReducerWeight += result.getSecond().getCentroid().getWeight();
1:1d6dc49:       ++numClusters;
1:1d6dc49:     }
1:1d6dc49:     System.out.printf("%d clusters are unbalanced\n", numUnbalancedClusters);
1:1d6dc49:     assertEquals("Invalid total weight", totalWeight, totalReducerWeight);
1:1d6dc49:     assertEquals("Invalid number of clusters", 1 << NUM_DIMENSIONS, numClusters);
1:1d6dc49:   }
1:1d6dc49: 
1:1d6dc49: }
============================================================================
author:Karl Richter
-------------------------------------------------------------------------------
commit:02ff22f
/////////////////////////////////////////////////////////////////////////
1:         mapReduceDriver = new MapReduceDriver<>(new StreamingKMeansMapper(), new StreamingKMeansReducer());
/////////////////////////////////////////////////////////////////////////
1:                 return new org.apache.hadoop.mrunit.types.Pair<>(
author:pferrel
-------------------------------------------------------------------------------
commit:b988c49
author:frankscholten
-------------------------------------------------------------------------------
commit:1a42d85
author:sslavic
-------------------------------------------------------------------------------
commit:921e201
/////////////////////////////////////////////////////////////////////////
1: import org.apache.mahout.common.MahoutTestCase;
/////////////////////////////////////////////////////////////////////////
1: public class StreamingKMeansTestMR extends MahoutTestCase {
/////////////////////////////////////////////////////////////////////////
1:     Configuration configuration = getConfiguration();
commit:9abf755
/////////////////////////////////////////////////////////////////////////
1: import org.apache.mahout.common.RandomUtils;
/////////////////////////////////////////////////////////////////////////
1: import org.junit.Before;
/////////////////////////////////////////////////////////////////////////
1:   private static Pair<List<Centroid>, List<Centroid>> syntheticData;
1: 
1:   @Before
1:   public void setUp() {
1:     RandomUtils.useTestSeed();
1:     syntheticData =
1:   }
author:Sebastian Schelter
-------------------------------------------------------------------------------
commit:6d9179e
/////////////////////////////////////////////////////////////////////////
author:Sean R. Owen
-------------------------------------------------------------------------------
commit:4ca6b86
/////////////////////////////////////////////////////////////////////////
1: @RunWith(Parameterized.class)
1:   private static final double DISTANCE_CUTOFF = 1.0e-6;
0:   private static final Pair<List<Centroid>, List<Centroid>> syntheticData =
1:       DataUtils.sampleMultiNormalHypercube(NUM_DIMENSIONS, NUM_DATA_POINTS, 1.0e-4);
1:   private final String searcherClassName;
1:   private final String distanceMeasureClassName;
/////////////////////////////////////////////////////////////////////////
1:   public void testHypercubeMapReduceRunSequentially() throws Exception {
/////////////////////////////////////////////////////////////////////////
1:   private static void testReducerResults(int totalWeight, List<org.apache.hadoop.mrunit.types.Pair<IntWritable,
1:     double expectedWeight = (double) totalWeight / expectedNumClusters;
author:dfilimon
-------------------------------------------------------------------------------
commit:a330664
/////////////////////////////////////////////////////////////////////////
1: 
1:     // Collapse the Centroids in the reducer.
1:     configuration.setBoolean(StreamingKMeansDriver.REDUCE_STREAMING_KMEANS, true);
commit:ec9035c
/////////////////////////////////////////////////////////////////////////
1: import org.apache.mahout.math.neighborhood.LocalitySensitiveHashSearch;
/////////////////////////////////////////////////////////////////////////
1:         {LocalitySensitiveHashSearch.class.getName(), SquaredEuclideanDistanceMeasure.class.getName()},
commit:1d6dc49
/////////////////////////////////////////////////////////////////////////
1: /**
1:  * Licensed to the Apache Software Foundation (ASF) under one or more
1:  * contributor license agreements.  See the NOTICE file distributed with
1:  * this work for additional information regarding copyright ownership.
1:  * The ASF licenses this file to You under the Apache License, Version 2.0
1:  * (the "License"); you may not use this file except in compliance with
1:  * the License.  You may obtain a copy of the License at
1:  *
1:  *     http://www.apache.org/licenses/LICENSE-2.0
1:  *
1:  * Unless required by applicable law or agreed to in writing, software
1:  * distributed under the License is distributed on an "AS IS" BASIS,
1:  * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
1:  * See the License for the specific language governing permissions and
1:  * limitations under the License.
1:  */
1: 
1: package org.apache.mahout.clustering.streaming.mapreduce;
1: 
1: import java.io.IOException;
1: import java.util.Arrays;
1: import java.util.List;
0: import java.util.concurrent.ExecutionException;
1: 
1: import com.google.common.base.Function;
1: import com.google.common.collect.Iterables;
1: import com.google.common.collect.Lists;
1: import org.apache.hadoop.conf.Configuration;
1: import org.apache.hadoop.fs.Path;
1: import org.apache.hadoop.io.IntWritable;
1: import org.apache.hadoop.io.Writable;
1: import org.apache.hadoop.mrunit.mapreduce.MapDriver;
1: import org.apache.hadoop.mrunit.mapreduce.MapReduceDriver;
1: import org.apache.hadoop.mrunit.mapreduce.ReduceDriver;
1: import org.apache.mahout.clustering.ClusteringUtils;
1: import org.apache.mahout.clustering.streaming.cluster.DataUtils;
1: import org.apache.mahout.clustering.streaming.cluster.StreamingKMeans;
1: import org.apache.mahout.common.Pair;
1: import org.apache.mahout.common.commandline.DefaultOptionCreator;
1: import org.apache.mahout.common.distance.SquaredEuclideanDistanceMeasure;
1: import org.apache.mahout.common.iterator.sequencefile.SequenceFileIterable;
1: import org.apache.mahout.math.Centroid;
1: import org.apache.mahout.math.Vector;
1: import org.apache.mahout.math.VectorWritable;
1: import org.apache.mahout.math.neighborhood.BruteSearch;
1: import org.apache.mahout.math.neighborhood.FastProjectionSearch;
1: import org.apache.mahout.math.neighborhood.ProjectionSearch;
1: import org.apache.mahout.math.random.WeightedThing;
1: import org.junit.Test;
1: import org.junit.runner.RunWith;
1: import org.junit.runners.Parameterized;
1: 
1: import static org.junit.Assert.assertEquals;
1: import static org.junit.Assert.assertTrue;
1: 
0: @RunWith(value = Parameterized.class)
0: public class StreamingKMeansTestMR {
1:   private static final int NUM_DATA_POINTS = 1 << 15;
1:   private static final int NUM_DIMENSIONS = 8;
1:   private static final int NUM_PROJECTIONS = 3;
1:   private static final int SEARCH_SIZE = 5;
1:   private static final int MAX_NUM_ITERATIONS = 10;
0:   private static final double DISTANCE_CUTOFF = 1e-6;
1: 
0:   private static Pair<List<Centroid>, List<Centroid>> syntheticData =
0:       DataUtils.sampleMultiNormalHypercube(NUM_DIMENSIONS, NUM_DATA_POINTS, 1e-4);
1: 
0:   private String searcherClassName;
0:   private String distanceMeasureClassName;
1: 
1:   public StreamingKMeansTestMR(String searcherClassName, String distanceMeasureClassName) {
1:     this.searcherClassName = searcherClassName;
1:     this.distanceMeasureClassName = distanceMeasureClassName;
1:   }
1: 
1:   private void configure(Configuration configuration) {
1:     configuration.set(DefaultOptionCreator.DISTANCE_MEASURE_OPTION, distanceMeasureClassName);
1:     configuration.setInt(StreamingKMeansDriver.SEARCH_SIZE_OPTION, SEARCH_SIZE);
1:     configuration.setInt(StreamingKMeansDriver.NUM_PROJECTIONS_OPTION, NUM_PROJECTIONS);
1:     configuration.set(StreamingKMeansDriver.SEARCHER_CLASS_OPTION, searcherClassName);
1:     configuration.setInt(DefaultOptionCreator.NUM_CLUSTERS_OPTION, 1 << NUM_DIMENSIONS);
1:     configuration.setInt(StreamingKMeansDriver.ESTIMATED_NUM_MAP_CLUSTERS,
1:         (1 << NUM_DIMENSIONS) * (int)Math.log(NUM_DATA_POINTS));
1:     configuration.setFloat(StreamingKMeansDriver.ESTIMATED_DISTANCE_CUTOFF, (float) DISTANCE_CUTOFF);
1:     configuration.setInt(StreamingKMeansDriver.MAX_NUM_ITERATIONS, MAX_NUM_ITERATIONS);
1:   }
1: 
1:   @Parameterized.Parameters
1:   public static List<Object[]> generateData() {
1:     return Arrays.asList(new Object[][]{
1:         {ProjectionSearch.class.getName(), SquaredEuclideanDistanceMeasure.class.getName()},
1:         {FastProjectionSearch.class.getName(), SquaredEuclideanDistanceMeasure.class.getName()},
1:     });
1:   }
1: 
1:   @Test
1:   public void testHypercubeMapper() throws IOException {
1:     MapDriver<Writable, VectorWritable, IntWritable, CentroidWritable> mapDriver =
1:         MapDriver.newMapDriver(new StreamingKMeansMapper());
1:     configure(mapDriver.getConfiguration());
1:     System.out.printf("%s mapper test\n",
1:         mapDriver.getConfiguration().get(StreamingKMeansDriver.SEARCHER_CLASS_OPTION));
1:     for (Centroid datapoint : syntheticData.getFirst()) {
1:       mapDriver.addInput(new IntWritable(0), new VectorWritable(datapoint));
1:     }
1:     List<org.apache.hadoop.mrunit.types.Pair<IntWritable,CentroidWritable>> results = mapDriver.run();
1:     BruteSearch resultSearcher = new BruteSearch(new SquaredEuclideanDistanceMeasure());
1:     for (org.apache.hadoop.mrunit.types.Pair<IntWritable, CentroidWritable> result : results) {
1:       resultSearcher.add(result.getSecond().getCentroid());
1:     }
1:     System.out.printf("Clustered the data into %d clusters\n", results.size());
1:     for (Vector mean : syntheticData.getSecond()) {
1:       WeightedThing<Vector> closest = resultSearcher.search(mean, 1).get(0);
1:       assertTrue("Weight " + closest.getWeight() + " not less than 0.5", closest.getWeight() < 0.5);
1:     }
1:   }
1: 
1:   @Test
1:   public void testMapperVsLocal() throws IOException {
1:     // Clusters the data using the StreamingKMeansMapper.
1:     MapDriver<Writable, VectorWritable, IntWritable, CentroidWritable> mapDriver =
1:         MapDriver.newMapDriver(new StreamingKMeansMapper());
1:     Configuration configuration = mapDriver.getConfiguration();
1:     configure(configuration);
1:     System.out.printf("%s mapper vs local test\n",
1:         mapDriver.getConfiguration().get(StreamingKMeansDriver.SEARCHER_CLASS_OPTION));
1: 
1:     for (Centroid datapoint : syntheticData.getFirst()) {
1:       mapDriver.addInput(new IntWritable(0), new VectorWritable(datapoint));
1:     }
1:     List<Centroid> mapperCentroids = Lists.newArrayList();
1:     for (org.apache.hadoop.mrunit.types.Pair<IntWritable, CentroidWritable> pair : mapDriver.run()) {
1:       mapperCentroids.add(pair.getSecond().getCentroid());
1:     }
1: 
1:     // Clusters the data using local batch StreamingKMeans.
1:     StreamingKMeans batchClusterer =
1:         new StreamingKMeans(StreamingKMeansUtilsMR.searcherFromConfiguration(configuration),
1:             mapDriver.getConfiguration().getInt("estimatedNumMapClusters", -1), DISTANCE_CUTOFF);
1:     batchClusterer.cluster(syntheticData.getFirst());
1:     List<Centroid> batchCentroids = Lists.newArrayList();
1:     for (Vector v : batchClusterer) {
1:       batchCentroids.add((Centroid) v);
1:     }
1: 
1:     // Clusters the data using point by point StreamingKMeans.
1:     StreamingKMeans perPointClusterer =
1:         new StreamingKMeans(StreamingKMeansUtilsMR.searcherFromConfiguration(configuration),
1:             (1 << NUM_DIMENSIONS) * (int)Math.log(NUM_DATA_POINTS), DISTANCE_CUTOFF);
1:     for (Centroid datapoint : syntheticData.getFirst()) {
1:       perPointClusterer.cluster(datapoint);
1:     }
1:     List<Centroid> perPointCentroids = Lists.newArrayList();
1:     for (Vector v : perPointClusterer) {
1:       perPointCentroids.add((Centroid) v);
1:     }
1: 
1:     // Computes the cost (total sum of distances) of these different clusterings.
1:     double mapperCost = ClusteringUtils.totalClusterCost(syntheticData.getFirst(), mapperCentroids);
1:     double localCost = ClusteringUtils.totalClusterCost(syntheticData.getFirst(), batchCentroids);
1:     double perPointCost = ClusteringUtils.totalClusterCost(syntheticData.getFirst(), perPointCentroids);
1:     System.out.printf("[Total cost] Mapper %f [%d] Local %f [%d] Perpoint local %f [%d];" +
1:         "[ratio m-vs-l %f] [ratio pp-vs-l %f]\n", mapperCost, mapperCentroids.size(),
1:         localCost, batchCentroids.size(), perPointCost, perPointCentroids.size(),
1:         mapperCost / localCost, perPointCost / localCost);
1: 
1:     // These ratios should be close to 1.0 and have been observed to be go as low as 0.6 and as low as 1.5.
1:     // A buffer of [0.2, 1.8] seems appropriate.
1:     assertEquals("Mapper StreamingKMeans / Batch local StreamingKMeans total cost ratio too far from 1",
1:         1.0, mapperCost / localCost, 0.8);
1:     assertEquals("One by one local StreamingKMeans / Batch local StreamingKMeans total cost ratio too high",
1:         1.0, perPointCost / localCost, 0.8);
1:   }
1: 
1:   @Test
1:   public void testHypercubeReducer() throws IOException {
1:     ReduceDriver<IntWritable, CentroidWritable, IntWritable, CentroidWritable> reduceDriver =
1:         ReduceDriver.newReduceDriver(new StreamingKMeansReducer());
1:     Configuration configuration = reduceDriver.getConfiguration();
1:     configure(configuration);
1: 
1:     System.out.printf("%s reducer test\n", configuration.get(StreamingKMeansDriver.SEARCHER_CLASS_OPTION));
1:     StreamingKMeans clusterer =
1:         new StreamingKMeans(StreamingKMeansUtilsMR .searcherFromConfiguration(configuration),
1:             (1 << NUM_DIMENSIONS) * (int)Math.log(NUM_DATA_POINTS), DISTANCE_CUTOFF);
1: 
1:     long start = System.currentTimeMillis();
1:     clusterer.cluster(syntheticData.getFirst());
1:     long end = System.currentTimeMillis();
1: 
1:     System.out.printf("%f [s]\n", (end - start) / 1000.0);
1:     List<CentroidWritable> reducerInputs = Lists.newArrayList();
1:     int postMapperTotalWeight = 0;
1:     for (Centroid intermediateCentroid : clusterer) {
1:       reducerInputs.add(new CentroidWritable(intermediateCentroid));
1:       postMapperTotalWeight += intermediateCentroid.getWeight();
1:     }
1: 
1:     reduceDriver.addInput(new IntWritable(0), reducerInputs);
1:     List<org.apache.hadoop.mrunit.types.Pair<IntWritable, CentroidWritable>> results =
1:         reduceDriver.run();
1:     testReducerResults(postMapperTotalWeight, results);
1:   }
1: 
1:   @Test
1:   public void testHypercubeMapReduce() throws IOException {
1:     MapReduceDriver<Writable, VectorWritable, IntWritable, CentroidWritable, IntWritable, CentroidWritable>
0:         mapReduceDriver = new MapReduceDriver<Writable, VectorWritable, IntWritable, CentroidWritable,
0:         IntWritable, CentroidWritable>(new StreamingKMeansMapper(), new StreamingKMeansReducer());
1:     Configuration configuration = mapReduceDriver.getConfiguration();
1:     configure(configuration);
1: 
1:     System.out.printf("%s full test\n", configuration.get(StreamingKMeansDriver.SEARCHER_CLASS_OPTION));
1:     for (Centroid datapoint : syntheticData.getFirst()) {
1:       mapReduceDriver.addInput(new IntWritable(0), new VectorWritable(datapoint));
1:     }
1:     List<org.apache.hadoop.mrunit.types.Pair<IntWritable, CentroidWritable>> results = mapReduceDriver.run();
1:     testReducerResults(syntheticData.getFirst().size(), results);
1:   }
1: 
1:   @Test
0:   public void testHypercubeMapReduceRunSequentially()
0:       throws IOException, InterruptedException, ExecutionException, ClassNotFoundException {
0:     Configuration configuration = new Configuration();
1:     configure(configuration);
1:     configuration.set(DefaultOptionCreator.METHOD_OPTION, DefaultOptionCreator.SEQUENTIAL_METHOD);
1: 
1:     Path inputPath = new Path("testInput");
1:     Path outputPath = new Path("testOutput");
1:     StreamingKMeansUtilsMR.writeVectorsToSequenceFile(syntheticData.getFirst(), inputPath, configuration);
1: 
1:     StreamingKMeansDriver.run(configuration, inputPath, outputPath);
1: 
1:     testReducerResults(syntheticData.getFirst().size(),
1:         Lists.newArrayList(Iterables.transform(
1:             new SequenceFileIterable<IntWritable, CentroidWritable>(outputPath, configuration),
1:             new Function<
1:                 Pair<IntWritable, CentroidWritable>,
1:                 org.apache.hadoop.mrunit.types.Pair<IntWritable, CentroidWritable>>() {
1:               @Override
1:               public org.apache.hadoop.mrunit.types.Pair<IntWritable, CentroidWritable> apply(
1:                   org.apache.mahout.common.Pair<IntWritable, CentroidWritable> input) {
0:                 return new org.apache.hadoop.mrunit.types.Pair<IntWritable, CentroidWritable>(
1:                     input.getFirst(), input.getSecond());
1:               }
1:             })));
1:   }
1: 
0:   private void testReducerResults(int totalWeight, List<org.apache.hadoop.mrunit.types.Pair<IntWritable,
1:       CentroidWritable>> results) {
1:     int expectedNumClusters = 1 << NUM_DIMENSIONS;
0:     double expectedWeight = totalWeight / expectedNumClusters;
1:     int numClusters = 0;
1:     int numUnbalancedClusters = 0;
1:     int totalReducerWeight = 0;
1:     for (org.apache.hadoop.mrunit.types.Pair<IntWritable, CentroidWritable> result : results) {
1:       if (result.getSecond().getCentroid().getWeight() != expectedWeight) {
1:         System.out.printf("Unbalanced weight %f in centroid %d\n",  result.getSecond().getCentroid().getWeight(),
1:             result.getSecond().getCentroid().getIndex());
1:         ++numUnbalancedClusters;
1:       }
1:       assertEquals("Final centroid index is invalid", numClusters, result.getFirst().get());
1:       totalReducerWeight += result.getSecond().getCentroid().getWeight();
1:       ++numClusters;
1:     }
1:     System.out.printf("%d clusters are unbalanced\n", numUnbalancedClusters);
1:     assertEquals("Invalid total weight", totalWeight, totalReducerWeight);
1:     assertEquals("Invalid number of clusters", 1 << NUM_DIMENSIONS, numClusters);
1:   }
1: 
1: }
============================================================================