1:6fca24e: /**
1:6fca24e:  * Licensed to the Apache Software Foundation (ASF) under one or more
1:6fca24e:  * contributor license agreements.  See the NOTICE file distributed with
1:6fca24e:  * this work for additional information regarding copyright ownership.
1:6fca24e:  * The ASF licenses this file to You under the Apache License, Version 2.0
1:6fca24e:  * (the "License"); you may not use this file except in compliance with
1:6fca24e:  * the License.  You may obtain a copy of the License at
1:6fca24e:  *
1:6fca24e:  *     http://www.apache.org/licenses/LICENSE-2.0
1:6fca24e:  *
1:6fca24e:  * Unless required by applicable law or agreed to in writing, software
1:6fca24e:  * distributed under the License is distributed on an "AS IS" BASIS,
1:6fca24e:  * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
1:6fca24e:  * See the License for the specific language governing permissions and
1:6fca24e:  * limitations under the License.
1:6fca24e:  */
3:4b25f31: 
1:4b25f31: package org.apache.mahout.classifier.sgd;
1:4b25f31: 
1:02f8694: import org.apache.hadoop.io.Writable;
1:4b25f31: import org.apache.mahout.classifier.OnlineLearner;
1:4b25f31: import org.apache.mahout.ep.EvolutionaryProcess;
1:4b25f31: import org.apache.mahout.ep.Mapping;
1:4b25f31: import org.apache.mahout.ep.Payload;
1:4b25f31: import org.apache.mahout.ep.State;
1:4b25f31: import org.apache.mahout.math.Vector;
1:02f8694: import org.apache.mahout.math.VectorWritable;
1:e6b4e35: import org.apache.mahout.math.stats.OnlineAuc;
1:4841efb: import org.slf4j.Logger;
1:4841efb: import org.slf4j.LoggerFactory;
1:4b25f31: 
1:02f8694: import java.io.DataInput;
1:02f8694: import java.io.DataOutput;
1:02f8694: import java.io.IOException;
1:85f9ece: import java.util.ArrayList;
1:4b25f31: import java.util.List;
1:6fca24e: import java.util.Locale;
1:4b25f31: import java.util.concurrent.ExecutionException;
1:4b25f31: 
1:4b25f31: /**
1:4841efb:  * This is a meta-learner that maintains a pool of ordinary
1:4841efb:  * {@link org.apache.mahout.classifier.sgd.OnlineLogisticRegression} learners. Each
1:4b25f31:  * member of the pool has different learning rates.  Whichever of the learners in the pool falls
1:4b25f31:  * behind in terms of average log-likelihood will be tossed out and replaced with variants of the
1:4b25f31:  * survivors.  This will let us automatically derive an annealing schedule that optimizes learning
1:4b25f31:  * speed.  Since on-line learners tend to be IO bound anyway, it doesn't cost as much as it might
1:4b25f31:  * seem that it would to maintain multiple learners in memory.  Doing this adaptation on-line as we
1:4b25f31:  * learn also decreases the number of learning rate parameters required and replaces the normal
1:4b25f31:  * hyper-parameter search.
1:4b25f31:  * <p/>
1:4841efb:  * One wrinkle is that the pool of learners that we maintain is actually a pool of
1:4841efb:  * {@link org.apache.mahout.classifier.sgd.CrossFoldLearner} which themselves contain several OnlineLogisticRegression
1:4841efb:  * objects.  These pools allow estimation
1:4b25f31:  * of performance on the fly even if we make many passes through the data.  This does, however,
1:4b25f31:  * increase the cost of training since if we are using 5-fold cross-validation, each vector is used
1:4b25f31:  * 4 times for training and once for classification.  If this becomes a problem, then we should
1:93ddcf8:  * probably use a 2-way unbalanced train/test split rather than full cross validation.  With the
1:02f8694:  * current default settings, we have 100 learners running.  This is better than the alternative of
1:02f8694:  * running hundreds of training passes to find good hyper-parameters because we only have to parse
1:02f8694:  * and feature-ize our inputs once.  If you already have good hyper-parameters, then you might
1:02f8694:  * prefer to just run one CrossFoldLearner with those settings.
1:93ddcf8:  * <p/>
1:02f8694:  * The fitness used here is AUC.  Another alternative would be to try log-likelihood, but it is much
1:02f8694:  * easier to get bogus values of log-likelihood than with AUC and the results seem to accord pretty
1:02f8694:  * well.  It would be nice to allow the fitness function to be pluggable. This use of AUC means that
1:02f8694:  * AdaptiveLogisticRegression is mostly suited for binary target variables. This will be fixed
1:02f8694:  * before long by extending OnlineAuc to handle non-binary cases or by using a different fitness
1:02f8694:  * value in non-binary cases.
1:4b25f31:  */
1:02f8694: public class AdaptiveLogisticRegression implements OnlineLearner, Writable {
1:69f324d:   public static final int DEFAULT_THREAD_COUNT = 20;
1:69f324d:   public static final int DEFAULT_POOL_SIZE = 20;
1:b553e02:   private static final int SURVIVORS = 2;
1:b553e02: 
1:61ae2e7:   private int record;
1:597db6a:   private int cutoff = 1000;
1:597db6a:   private int minInterval = 1000;
1:597db6a:   private int maxInterval = 1000;
1:597db6a:   private int currentStep = 1000;
1:597db6a:   private int bufferSize = 1000;
1:4b25f31: 
1:85f9ece:   private List<TrainingExample> buffer = new ArrayList<>();
1:02f8694:   private EvolutionaryProcess<Wrapper, CrossFoldLearner> ep;
1:02f8694:   private State<Wrapper, CrossFoldLearner> best;
1:69f324d:   private int threadCount = DEFAULT_THREAD_COUNT;
1:69f324d:   private int poolSize = DEFAULT_POOL_SIZE;
1:02f8694:   private State<Wrapper, CrossFoldLearner> seed;
1:913d44d:   private int numFeatures;
1:913d44d: 
1:9cfe290:   private boolean freezeSurvivors = true;
1:9cfe290: 
1:4841efb:   private static final Logger log = LoggerFactory.getLogger(AdaptiveLogisticRegression.class);
1:4b25f31: 
1:4841efb:   public AdaptiveLogisticRegression() {}
1:4b25f31: 
1:69f324d:   /**
1:69f324d:    * Uses {@link #DEFAULT_THREAD_COUNT} and {@link #DEFAULT_POOL_SIZE}
1:69f324d:    * @param numCategories The number of categories (labels) to train on
1:69f324d:    * @param numFeatures The number of features used in creating the vectors (i.e. the cardinality of the vector)
1:69f324d:    * @param prior The {@link org.apache.mahout.classifier.sgd.PriorFunction} to use
1:4b25f31:    *
1:d74edf4:    * @see #AdaptiveLogisticRegression(int, int, org.apache.mahout.classifier.sgd.PriorFunction, int, int)
1:69f324d:    */
1:d3ace90:   public AdaptiveLogisticRegression(int numCategories, int numFeatures, PriorFunction prior) {
1:69f324d:     this(numCategories, numFeatures, prior, DEFAULT_THREAD_COUNT, DEFAULT_POOL_SIZE);
1:913d44d:   }
1:b553e02: 
1:69f324d:   /**
2:69f324d:    *
1:69f324d:    * @param numCategories The number of categories (labels) to train on
1:69f324d:    * @param numFeatures The number of features used in creating the vectors (i.e. the cardinality of the vector)
1:69f324d:    * @param prior The {@link org.apache.mahout.classifier.sgd.PriorFunction} to use
1:69f324d:    * @param threadCount The number of threads to use for training
1:69f324d:    * @param poolSize The number of {@link org.apache.mahout.classifier.sgd.CrossFoldLearner} to use.
1:69f324d:    */
1:4841efb:   public AdaptiveLogisticRegression(int numCategories, int numFeatures, PriorFunction prior, int threadCount,
1:4841efb:       int poolSize) {
1:913d44d:     this.numFeatures = numFeatures;
1:69f324d:     this.threadCount = threadCount;
1:d3ace90:     this.poolSize = poolSize;
1:85f9ece:     seed = new State<>(new double[2], 10);
1:4b25f31:     Wrapper w = new Wrapper(numCategories, numFeatures, prior);
1:02f8694:     seed.setPayload(w);
1:02f8694: 
1:229aeff:     Wrapper.setMappings(seed);
1:d3ace90:     seed.setPayload(w);
1:69f324d:     setPoolSize(this.poolSize);
3:4b25f31:   }
1:4b25f31: 
1:4b25f31:   @Override
1:4b25f31:   public void train(int actual, Vector instance) {
1:58b5f28:     train(record, null, actual, instance);
1:4b25f31:   }
1:e6b4e35: 
1:4b25f31:   @Override
1:4b25f31:   public void train(long trackingKey, int actual, Vector instance) {
1:e6b4e35:     train(trackingKey, null, actual, instance);
1:e6b4e35:   }
1:e6b4e35: 
1:58b5f28:   @Override
1:e6b4e35:   public void train(long trackingKey, String groupKey, int actual, Vector instance) {
1:4b25f31:     record++;
1:4b25f31: 
1:e6b4e35:     buffer.add(new TrainingExample(trackingKey, groupKey, actual, instance));
1:8ad8d96:     //don't train until we have enough examples
1:597db6a:     if (buffer.size() > bufferSize) {
1:4b25f31:       trainWithBufferedExamples();
1:4b25f31:     }
1:4b25f31:   }
1:4b25f31: 
1:4b25f31:   private void trainWithBufferedExamples() {
1:4b25f31:     try {
1:02f8694:       this.best = ep.parallelDo(new EvolutionaryProcess.Function<Payload<CrossFoldLearner>>() {
1:6fca24e:         @Override
1:02f8694:         public double apply(Payload<CrossFoldLearner> z, double[] params) {
1:02f8694:           Wrapper x = (Wrapper) z;
1:4b25f31:           for (TrainingExample example : buffer) {
1:4b25f31:             x.train(example);
1:4b25f31:           }
1:6fca24e:           if (x.getLearner().validModel()) {
1:93ddcf8:             if (x.getLearner().numCategories() == 2) {
1:93ddcf8:               return x.wrapped.auc();
1:93ddcf8:             } else {
1:93ddcf8:               return x.wrapped.logLikelihood();
1:93ddcf8:             }
1:4b25f31:           } else {
1:4b25f31:             return Double.NaN;
1:4b25f31:           }
1:4b25f31:         }
1:4b25f31:       });
1:4b25f31:     } catch (InterruptedException e) {
1:4b25f31:       // ignore ... shouldn't happen
1:4841efb:       log.warn("Ignoring exception", e);
1:4b25f31:     } catch (ExecutionException e) {
1:f201912:       throw new IllegalStateException(e.getCause());
1:4b25f31:     }
1:597db6a:     buffer.clear();
1:4b25f31: 
1:597db6a:     if (record > cutoff) {
1:597db6a:       cutoff = nextStep(record);
1:4b25f31: 
1:597db6a:       // evolve based on new fitness
1:597db6a:       ep.mutatePopulation(SURVIVORS);
1:597db6a: 
1:597db6a:       if (freezeSurvivors) {
1:597db6a:         // now grossly hack the top survivors so they stick around.  Set their
1:597db6a:         // mutation rates small and also hack their learning rate to be small
1:597db6a:         // as well.
1:02f8694:         for (State<Wrapper, CrossFoldLearner> state : ep.getPopulation().subList(0, SURVIVORS)) {
1:229aeff:           Wrapper.freeze(state);
1:597db6a:         }
1:597db6a:       }
1:9cfe290:     }
1:597db6a: 
1:4b25f31:   }
1:597db6a: 
1:597db6a:   public int nextStep(int recordNumber) {
1:597db6a:     int stepSize = stepSize(recordNumber, 2.6);
1:597db6a:     if (stepSize < minInterval) {
1:597db6a:       stepSize = minInterval;
1:597db6a:     }
1:597db6a: 
1:597db6a:     if (stepSize > maxInterval) {
1:597db6a:       stepSize = maxInterval;
1:597db6a:     }
1:597db6a: 
1:597db6a:     int newCutoff = stepSize * (recordNumber / stepSize + 1);
1:597db6a:     if (newCutoff < cutoff + currentStep) {
1:597db6a:       newCutoff = cutoff + currentStep;
1:597db6a:     } else {
1:597db6a:       this.currentStep = stepSize;
1:597db6a:     }
1:597db6a:     return newCutoff;
1:597db6a:   }
1:597db6a: 
1:597db6a:   public static int stepSize(int recordNumber, double multiplier) {
1:049e7dc:     int[] bumps = {1, 2, 5};
1:597db6a:     double log = Math.floor(multiplier * Math.log10(recordNumber));
1:597db6a:     int bump = bumps[(int) log % bumps.length];
1:597db6a:     int scale = (int) Math.pow(10, Math.floor(log / bumps.length));
1:597db6a: 
1:597db6a:     return bump * scale;
1:4b25f31:   }
1:4b25f31: 
1:4b25f31:   @Override
1:4b25f31:   public void close() {
1:4b25f31:     trainWithBufferedExamples();
1:4b25f31:     try {
1:02f8694:       ep.parallelDo(new EvolutionaryProcess.Function<Payload<CrossFoldLearner>>() {
1:4b25f31:         @Override
1:02f8694:         public double apply(Payload<CrossFoldLearner> payload, double[] params) {
1:02f8694:           CrossFoldLearner learner = ((Wrapper) payload).getLearner();
1:02f8694:           learner.close();
1:02f8694:           return learner.logLikelihood();
1:4b25f31:         }
1:4b25f31:       });
1:4b25f31:     } catch (InterruptedException e) {
1:4841efb:       log.warn("Ignoring exception", e);
1:4b25f31:     } catch (ExecutionException e) {
1:049e7dc:       throw new IllegalStateException(e);
1:c88c240:     } finally {
1:42b989b:       ep.close();
1:b553e02:     }
1:4b25f31:   }
1:4b25f31: 
1:4b25f31:   /**
1:4b25f31:    * How often should the evolutionary optimization of learning parameters occur?
1:02f8694:    *
1:02f8694:    * @param interval Number of training examples to use in each epoch of optimization.
1:4b25f31:    */
1:4b25f31:   public void setInterval(int interval) {
1:d0dc388:     setInterval(interval, interval);
1:597db6a:   }
1:597db6a: 
1:597db6a:   /**
1:597db6a:    * Starts optimization using the shorter interval and progresses to the longer using the specified
1:597db6a:    * number of steps per decade.  Note that values < 200 are not accepted.  Values even that small
1:597db6a:    * are unlikely to be useful.
1:597db6a:    *
1:02f8694:    * @param minInterval The minimum epoch length for the evolutionary optimization
1:02f8694:    * @param maxInterval The maximum epoch length
1:597db6a:    */
1:597db6a:   public void setInterval(int minInterval, int maxInterval) {
1:597db6a:     this.minInterval = Math.max(200, minInterval);
1:597db6a:     this.maxInterval = Math.max(200, maxInterval);
1:597db6a:     this.cutoff = minInterval * (record / minInterval + 1);
1:d0dc388:     this.currentStep = minInterval;
1:d0dc388:     bufferSize = Math.min(minInterval, bufferSize);
1:4b25f31:   }
1:4b25f31: 
1:0844e69:   public final void setPoolSize(int poolSize) {
1:69f324d:     this.poolSize = poolSize;
1:d3ace90:     setupOptimizer(poolSize);
1:d3ace90:   }
1:d3ace90: 
1:d3ace90:   public void setThreadCount(int threadCount) {
1:d3ace90:     this.threadCount = threadCount;
1:d3ace90:     setupOptimizer(poolSize);
1:d3ace90:   }
1:d3ace90: 
1:e6b4e35:   public void setAucEvaluator(OnlineAuc auc) {
1:e6b4e35:     seed.getPayload().setAucEvaluator(auc);
1:e6b4e35:     setupOptimizer(poolSize);
1:e6b4e35:   }
1:e6b4e35: 
1:d3ace90:   private void setupOptimizer(int poolSize) {
1:85f9ece:     ep = new EvolutionaryProcess<>(threadCount, poolSize, seed);
1:d3ace90:   }
1:d3ace90: 
1:d3ace90:   /**
1:02f8694:    * Returns the size of the internal feature vector.  Note that this is not the same as the number
1:02f8694:    * of distinct features, especially if feature hashing is being used.
1:02f8694:    *
1:d3ace90:    * @return The internal feature vector size.
1:d3ace90:    */
1:d3ace90:   public int numFeatures() {
1:d3ace90:     return numFeatures;
1:d3ace90:   }
1:d3ace90: 
1:4b25f31:   /**
1:02f8694:    * What is the AUC for the current best member of the population.  If no member is best, usually
1:02f8694:    * because we haven't done any training yet, then the result is set to NaN.
1:02f8694:    *
1:02f8694:    * @return The AUC of the best member of the population or NaN if we can't figure that out.
1:4b25f31:    */
1:4b25f31:   public double auc() {
1:6fca24e:     if (best == null) {
1:4b25f31:       return Double.NaN;
1:4b25f31:     } else {
1:02f8694:       Wrapper payload = best.getPayload();
1:02f8694:       return payload.getLearner().auc();
1:4b25f31:     }
1:4b25f31:   }
1:4b25f31: 
1:02f8694:   public State<Wrapper, CrossFoldLearner> getBest() {
1:4b25f31:     return best;
1:913d44d:   }
1:913d44d: 
1:02f8694:   public void setBest(State<Wrapper, CrossFoldLearner> best) {
1:913d44d:     this.best = best;
1:913d44d:   }
1:913d44d: 
1:913d44d:   public int getRecord() {
1:913d44d:     return record;
1:913d44d:   }
1:913d44d: 
1:913d44d:   public void setRecord(int record) {
1:913d44d:     this.record = record;
1:913d44d:   }
1:913d44d: 
1:597db6a:   public int getMinInterval() {
1:597db6a:     return minInterval;
1:597db6a:   }
1:597db6a: 
1:597db6a:   public int getMaxInterval() {
1:597db6a:     return maxInterval;
1:913d44d:   }
1:913d44d: 
1:913d44d:   public int getNumCategories() {
1:913d44d:     return seed.getPayload().getLearner().numCategories();
1:913d44d:   }
1:913d44d: 
1:913d44d:   public PriorFunction getPrior() {
1:913d44d:     return seed.getPayload().getLearner().getPrior();
1:913d44d:   }
1:913d44d: 
1:913d44d:   public void setBuffer(List<TrainingExample> buffer) {
1:913d44d:     this.buffer = buffer;
1:913d44d:   }
1:913d44d: 
1:913d44d:   public List<TrainingExample> getBuffer() {
1:913d44d:     return buffer;
1:913d44d:   }
1:913d44d: 
1:02f8694:   public EvolutionaryProcess<Wrapper, CrossFoldLearner> getEp() {
1:913d44d:     return ep;
1:913d44d:   }
1:913d44d: 
1:02f8694:   public void setEp(EvolutionaryProcess<Wrapper, CrossFoldLearner> ep) {
1:913d44d:     this.ep = ep;
1:913d44d:   }
1:913d44d: 
1:02f8694:   public State<Wrapper, CrossFoldLearner> getSeed() {
1:913d44d:     return seed;
1:913d44d:   }
1:913d44d: 
1:02f8694:   public void setSeed(State<Wrapper, CrossFoldLearner> seed) {
1:913d44d:     this.seed = seed;
1:913d44d:   }
1:913d44d: 
1:913d44d:   public int getNumFeatures() {
1:913d44d:     return numFeatures;
1:913d44d:   }
1:913d44d: 
1:ef7b768:   public void setAveragingWindow(int averagingWindow) {
1:ef7b768:     seed.getPayload().getLearner().setWindowSize(averagingWindow);
1:e6b4e35:     setupOptimizer(poolSize);
1:ef7b768:   }
1:ef7b768: 
1:9cfe290:   public void setFreezeSurvivors(boolean freezeSurvivors) {
1:9cfe290:     this.freezeSurvivors = freezeSurvivors;
1:9cfe290:   }
1:9cfe290: 
1:4b25f31:   /**
1:4b25f31:    * Provides a shim between the EP optimization stuff and the CrossFoldLearner.  The most important
1:4b25f31:    * interface has to do with the parameters of the optimization.  These are taken from the double[]
1:4b25f31:    * params in the following order <ul> <li> regularization constant lambda <li> learningRate </ul>.
1:4b25f31:    * All other parameters are set in such a way so as to defeat annealing to the extent possible.
1:4b25f31:    * This lets the evolutionary algorithm handle the annealing.
1:02f8694:    * <p/>
1:4b25f31:    * Note that per coefficient annealing is still done and no optimization of the per coefficient
1:4b25f31:    * offset is done.
1:4b25f31:    */
1:02f8694:   public static class Wrapper implements Payload<CrossFoldLearner> {
1:4b25f31:     private CrossFoldLearner wrapped;
1:4b25f31: 
1:02f8694:     public Wrapper() {
1:913d44d:     }
1:4b25f31: 
1:4b25f31:     public Wrapper(int numCategories, int numFeatures, PriorFunction prior) {
1:4b25f31:       wrapped = new CrossFoldLearner(5, numCategories, numFeatures, prior);
1:4b25f31:     }
1:4b25f31: 
1:4b25f31:     @Override
1:4b25f31:     public Wrapper copy() {
1:4b25f31:       Wrapper r = new Wrapper();
1:4b25f31:       r.wrapped = wrapped.copy();
1:4b25f31:       return r;
1:4b25f31:     }
1:4b25f31: 
1:4b25f31:     @Override
1:4b25f31:     public void update(double[] params) {
1:4b25f31:       int i = 0;
1:4b25f31:       wrapped.lambda(params[i++]);
1:913d44d:       wrapped.learningRate(params[i]);
1:4b25f31: 
1:4b25f31:       wrapped.stepOffset(1);
1:4b25f31:       wrapped.alpha(1);
1:4b25f31:       wrapped.decayExponent(0);
1:4b25f31:     }
1:4b25f31: 
1:229aeff:     public static void freeze(State<Wrapper, CrossFoldLearner> s) {
1:b553e02:       // radically decrease learning rate
1:6d16230:       double[] params = s.getParams();
1:6d16230:       params[1] -= 10;
1:b553e02: 
1:b553e02:       // and cause evolution to hold (almost)
1:a87bed8:       s.setOmni(s.getOmni() / 20);
1:b553e02:       double[] step = s.getStep();
1:b553e02:       for (int i = 0; i < step.length; i++) {
1:a87bed8:         step[i] /= 20;
1:b553e02:       }
1:b553e02:     }
1:b553e02: 
1:229aeff:     public static void setMappings(State<Wrapper, CrossFoldLearner> x) {
1:4b25f31:       int i = 0;
1:3686970:       // set the range for regularization (lambda)
1:6fca24e:       x.setMap(i++, Mapping.logLimit(1.0e-8, 0.1));
1:3686970:       // set the range for learning rate (mu)
1:049e7dc:       x.setMap(i, Mapping.logLimit(1.0e-8, 1));
1:4b25f31:     }
1:4b25f31: 
1:4b25f31:     public void train(TrainingExample example) {
1:e6b4e35:       wrapped.train(example.getKey(), example.getGroupKey(), example.getActual(), example.getInstance());
1:4b25f31:     }
1:4b25f31: 
1:4b25f31:     public CrossFoldLearner getLearner() {
1:4b25f31:       return wrapped;
1:4b25f31:     }
1:4b25f31: 
1:4b25f31:     @Override
1:4b25f31:     public String toString() {
1:6fca24e:       return String.format(Locale.ENGLISH, "auc=%.2f", wrapped.auc());
1:4b25f31:     }
1:e6b4e35: 
1:e6b4e35:     public void setAucEvaluator(OnlineAuc auc) {
1:e6b4e35:       wrapped.setAucEvaluator(auc);
1:e6b4e35:     }
1:02f8694: 
1:02f8694:     @Override
1:02f8694:     public void write(DataOutput out) throws IOException {
1:02f8694:       wrapped.write(out);
1:02f8694:     }
1:02f8694: 
1:02f8694:     @Override
1:02f8694:     public void readFields(DataInput input) throws IOException {
1:02f8694:       wrapped = new CrossFoldLearner();
1:02f8694:       wrapped.readFields(input);
1:02f8694:     }
1:4b25f31:   }
1:4b25f31: 
1:02f8694:   public static class TrainingExample implements Writable {
1:913d44d:     private long key;
1:e6b4e35:     private String groupKey;
1:913d44d:     private int actual;
1:913d44d:     private Vector instance;
1:02f8694: 
1:913d44d:     private TrainingExample() {
1:913d44d:     }
1:913d44d: 
1:e6b4e35:     public TrainingExample(long key, String groupKey, int actual, Vector instance) {
1:4b25f31:       this.key = key;
1:e6b4e35:       this.groupKey = groupKey;
1:4b25f31:       this.actual = actual;
1:4b25f31:       this.instance = instance;
1:4b25f31:     }
1:4b25f31: 
1:4b25f31:     public long getKey() {
1:4b25f31:       return key;
1:4b25f31:     }
1:4b25f31: 
1:4b25f31:     public int getActual() {
1:4b25f31:       return actual;
1:4b25f31:     }
1:4b25f31: 
1:4b25f31:     public Vector getInstance() {
1:4b25f31:       return instance;
1:4b25f31:     }
1:e6b4e35: 
1:e6b4e35:     public String getGroupKey() {
1:e6b4e35:       return groupKey;
1:e6b4e35:     }
1:02f8694: 
1:02f8694:     @Override
1:02f8694:     public void write(DataOutput out) throws IOException {
1:02f8694:       out.writeLong(key);
1:02f8694:       if (groupKey != null) {
1:02f8694:         out.writeBoolean(true);
1:02f8694:         out.writeUTF(groupKey);
1:02f8694:       } else {
1:02f8694:         out.writeBoolean(false);
1:02f8694:       }
1:02f8694:       out.writeInt(actual);
1:02f8694:       VectorWritable.writeVector(out, instance, true);
1:02f8694:     }
1:02f8694: 
1:02f8694:     @Override
1:02f8694:     public void readFields(DataInput in) throws IOException {
1:02f8694:       key = in.readLong();
1:02f8694:       if (in.readBoolean()) {
1:02f8694:         groupKey = in.readUTF();
1:02f8694:       }
1:02f8694:       actual = in.readInt();
1:02f8694:       instance = VectorWritable.readVector(in);
1:02f8694:     }
1:02f8694:   }
1:02f8694: 
1:02f8694:   @Override
1:02f8694:   public void write(DataOutput out) throws IOException {
1:02f8694:     out.writeInt(record);
1:02f8694:     out.writeInt(cutoff);
1:02f8694:     out.writeInt(minInterval);
1:02f8694:     out.writeInt(maxInterval);
1:02f8694:     out.writeInt(currentStep);
1:02f8694:     out.writeInt(bufferSize);
1:02f8694: 
1:02f8694:     out.writeInt(buffer.size());
1:02f8694:     for (TrainingExample example : buffer) {
1:02f8694:       example.write(out);
1:02f8694:     }
1:02f8694: 
1:02f8694:     ep.write(out);
1:02f8694: 
1:02f8694:     best.write(out);
1:02f8694: 
1:02f8694:     out.writeInt(threadCount);
1:02f8694:     out.writeInt(poolSize);
1:02f8694:     seed.write(out);
1:02f8694:     out.writeInt(numFeatures);
1:02f8694: 
1:02f8694:     out.writeBoolean(freezeSurvivors);
1:02f8694:   }
1:02f8694: 
1:02f8694:   @Override
1:02f8694:   public void readFields(DataInput in) throws IOException {
1:02f8694:     record = in.readInt();
1:02f8694:     cutoff = in.readInt();
1:02f8694:     minInterval = in.readInt();
1:02f8694:     maxInterval = in.readInt();
1:02f8694:     currentStep = in.readInt();
1:02f8694:     bufferSize = in.readInt();
1:02f8694: 
1:02f8694:     int n = in.readInt();
1:85f9ece:     buffer = new ArrayList<>();
1:02f8694:     for (int i = 0; i < n; i++) {
1:02f8694:       TrainingExample example = new TrainingExample();
1:02f8694:       example.readFields(in);
1:02f8694:       buffer.add(example);
1:02f8694:     }
1:02f8694: 
1:85f9ece:     ep = new EvolutionaryProcess<>();
1:02f8694:     ep.readFields(in);
1:02f8694: 
1:85f9ece:     best = new State<>();
1:02f8694:     best.readFields(in);
1:02f8694: 
1:02f8694:     threadCount = in.readInt();
1:02f8694:     poolSize = in.readInt();
1:85f9ece:     seed = new State<>();
1:02f8694:     seed.readFields(in);
1:02f8694: 
1:02f8694:     numFeatures = in.readInt();
1:02f8694:     freezeSurvivors = in.readBoolean();
1:4b25f31:   }
1:4b25f31: }
1:4b25f31: 
============================================================================
author:Suneel Marthi
-------------------------------------------------------------------------------
commit:85f9ece
/////////////////////////////////////////////////////////////////////////
/////////////////////////////////////////////////////////////////////////
1: import java.util.ArrayList;
/////////////////////////////////////////////////////////////////////////
1:   private List<TrainingExample> buffer = new ArrayList<>();
/////////////////////////////////////////////////////////////////////////
1:     seed = new State<>(new double[2], 10);
/////////////////////////////////////////////////////////////////////////
1:     ep = new EvolutionaryProcess<>(threadCount, poolSize, seed);
/////////////////////////////////////////////////////////////////////////
1:     buffer = new ArrayList<>();
1:     ep = new EvolutionaryProcess<>();
1:     best = new State<>();
1:     seed = new State<>();
author:pferrel
-------------------------------------------------------------------------------
commit:b988c49
author:Ted Dunning
-------------------------------------------------------------------------------
commit:0844e69
/////////////////////////////////////////////////////////////////////////
1:   public final void setPoolSize(int poolSize) {
commit:02f8694
/////////////////////////////////////////////////////////////////////////
1: import org.apache.hadoop.io.Writable;
1: import org.apache.mahout.math.VectorWritable;
1: import java.io.DataInput;
1: import java.io.DataOutput;
1: import java.io.IOException;
/////////////////////////////////////////////////////////////////////////
1:  * current default settings, we have 100 learners running.  This is better than the alternative of
1:  * running hundreds of training passes to find good hyper-parameters because we only have to parse
1:  * and feature-ize our inputs once.  If you already have good hyper-parameters, then you might
1:  * prefer to just run one CrossFoldLearner with those settings.
1:  * The fitness used here is AUC.  Another alternative would be to try log-likelihood, but it is much
1:  * easier to get bogus values of log-likelihood than with AUC and the results seem to accord pretty
1:  * well.  It would be nice to allow the fitness function to be pluggable. This use of AUC means that
1:  * AdaptiveLogisticRegression is mostly suited for binary target variables. This will be fixed
1:  * before long by extending OnlineAuc to handle non-binary cases or by using a different fitness
1:  * value in non-binary cases.
1: public class AdaptiveLogisticRegression implements OnlineLearner, Writable {
/////////////////////////////////////////////////////////////////////////
1:   private EvolutionaryProcess<Wrapper, CrossFoldLearner> ep;
1:   private State<Wrapper, CrossFoldLearner> best;
1:   private State<Wrapper, CrossFoldLearner> seed;
0:   public AdaptiveLogisticRegression() {
0:     seed = new State<Wrapper, CrossFoldLearner>(new double[2], 10);
1:     seed.setPayload(w);
1: 
/////////////////////////////////////////////////////////////////////////
1:       this.best = ep.parallelDo(new EvolutionaryProcess.Function<Payload<CrossFoldLearner>>() {
1:         public double apply(Payload<CrossFoldLearner> z, double[] params) {
1:           Wrapper x = (Wrapper) z;
/////////////////////////////////////////////////////////////////////////
1:         for (State<Wrapper, CrossFoldLearner> state : ep.getPopulation().subList(0, SURVIVORS)) {
/////////////////////////////////////////////////////////////////////////
1:       ep.parallelDo(new EvolutionaryProcess.Function<Payload<CrossFoldLearner>>() {
1:         public double apply(Payload<CrossFoldLearner> payload, double[] params) {
1:           CrossFoldLearner learner = ((Wrapper) payload).getLearner();
1:           learner.close();
1:           return learner.logLikelihood();
/////////////////////////////////////////////////////////////////////////
1:    *
1:    * @param interval Number of training examples to use in each epoch of optimization.
/////////////////////////////////////////////////////////////////////////
1:    * @param minInterval The minimum epoch length for the evolutionary optimization
1:    * @param maxInterval The maximum epoch length
/////////////////////////////////////////////////////////////////////////
0:     ep = new EvolutionaryProcess<Wrapper, CrossFoldLearner>(threadCount, poolSize, seed);
1:    * Returns the size of the internal feature vector.  Note that this is not the same as the number
1:    * of distinct features, especially if feature hashing is being used.
1:    *
/////////////////////////////////////////////////////////////////////////
1:    * What is the AUC for the current best member of the population.  If no member is best, usually
1:    * because we haven't done any training yet, then the result is set to NaN.
1:    *
1:    * @return The AUC of the best member of the population or NaN if we can't figure that out.
1:       Wrapper payload = best.getPayload();
1:       return payload.getLearner().auc();
1:   public State<Wrapper, CrossFoldLearner> getBest() {
1:   public void setBest(State<Wrapper, CrossFoldLearner> best) {
/////////////////////////////////////////////////////////////////////////
1:   public EvolutionaryProcess<Wrapper, CrossFoldLearner> getEp() {
1:   public void setEp(EvolutionaryProcess<Wrapper, CrossFoldLearner> ep) {
1:   public State<Wrapper, CrossFoldLearner> getSeed() {
1:   public void setSeed(State<Wrapper, CrossFoldLearner> seed) {
/////////////////////////////////////////////////////////////////////////
1:    * <p/>
1:   public static class Wrapper implements Payload<CrossFoldLearner> {
1:     public Wrapper() {
/////////////////////////////////////////////////////////////////////////
0:     public void freeze(State<Wrapper, CrossFoldLearner> s) {
/////////////////////////////////////////////////////////////////////////
0:     public void setMappings(State<Wrapper, CrossFoldLearner> x) {
/////////////////////////////////////////////////////////////////////////
1: 
1:     @Override
1:     public void write(DataOutput out) throws IOException {
1:       wrapped.write(out);
1:     }
1: 
1:     @Override
1:     public void readFields(DataInput input) throws IOException {
1:       wrapped = new CrossFoldLearner();
1:       wrapped.readFields(input);
1:     }
1:   public static class TrainingExample implements Writable {
1: 
/////////////////////////////////////////////////////////////////////////
1: 
1:     @Override
1:     public void write(DataOutput out) throws IOException {
1:       out.writeLong(key);
1:       if (groupKey != null) {
1:         out.writeBoolean(true);
1:         out.writeUTF(groupKey);
1:       } else {
1:         out.writeBoolean(false);
1:       }
1:       out.writeInt(actual);
1:       VectorWritable.writeVector(out, instance, true);
1:     }
1: 
1:     @Override
1:     public void readFields(DataInput in) throws IOException {
1:       key = in.readLong();
1:       if (in.readBoolean()) {
1:         groupKey = in.readUTF();
1:       }
1:       actual = in.readInt();
1:       instance = VectorWritable.readVector(in);
1:     }
1:   }
1: 
1:   @Override
1:   public void write(DataOutput out) throws IOException {
1:     out.writeInt(record);
1:     out.writeInt(cutoff);
1:     out.writeInt(minInterval);
1:     out.writeInt(maxInterval);
1:     out.writeInt(currentStep);
1:     out.writeInt(bufferSize);
1: 
1:     out.writeInt(buffer.size());
1:     for (TrainingExample example : buffer) {
1:       example.write(out);
1:     }
1: 
1:     ep.write(out);
1: 
1:     best.write(out);
1: 
1:     out.writeInt(threadCount);
1:     out.writeInt(poolSize);
1:     seed.write(out);
1:     out.writeInt(numFeatures);
1: 
1:     out.writeBoolean(freezeSurvivors);
1:   }
1: 
1:   @Override
1:   public void readFields(DataInput in) throws IOException {
1:     record = in.readInt();
1:     cutoff = in.readInt();
1:     minInterval = in.readInt();
1:     maxInterval = in.readInt();
1:     currentStep = in.readInt();
1:     bufferSize = in.readInt();
1: 
1:     int n = in.readInt();
0:     buffer = Lists.newArrayList();
1:     for (int i = 0; i < n; i++) {
1:       TrainingExample example = new TrainingExample();
1:       example.readFields(in);
1:       buffer.add(example);
1:     }
1: 
0:     ep = new EvolutionaryProcess<Wrapper, CrossFoldLearner>();
1:     ep.readFields(in);
1: 
0:     best = new State<Wrapper, CrossFoldLearner>();
1:     best.readFields(in);
1: 
1:     threadCount = in.readInt();
1:     poolSize = in.readInt();
0:     seed = new State<Wrapper, CrossFoldLearner>();
1:     seed.readFields(in);
1: 
1:     numFeatures = in.readInt();
1:     freezeSurvivors = in.readBoolean();
commit:58b5f28
/////////////////////////////////////////////////////////////////////////
0:   private List<TrainingExample> buffer = Lists.newArrayList();
0:   @SuppressWarnings({"UnusedDeclaration"})
/////////////////////////////////////////////////////////////////////////
1:     train(record, null, actual, instance);
/////////////////////////////////////////////////////////////////////////
1:   @Override
/////////////////////////////////////////////////////////////////////////
0:     @SuppressWarnings({"UnusedDeclaration"})
commit:d0dc388
/////////////////////////////////////////////////////////////////////////
1:     setInterval(interval, interval);
/////////////////////////////////////////////////////////////////////////
1:     this.currentStep = minInterval;
1:     bufferSize = Math.min(minInterval, bufferSize);
commit:597db6a
/////////////////////////////////////////////////////////////////////////
1:   private int cutoff = 1000;
1:   private int minInterval = 1000;
1:   private int maxInterval = 1000;
1:   private int currentStep = 1000;
1:   private int bufferSize = 1000;
/////////////////////////////////////////////////////////////////////////
1:     if (buffer.size() > bufferSize) {
/////////////////////////////////////////////////////////////////////////
1:     buffer.clear();
1:     if (record > cutoff) {
1:       cutoff = nextStep(record);
1:       // evolve based on new fitness
1:       ep.mutatePopulation(SURVIVORS);
1: 
1:       if (freezeSurvivors) {
1:         // now grossly hack the top survivors so they stick around.  Set their
1:         // mutation rates small and also hack their learning rate to be small
1:         // as well.
0:         for (State<Wrapper> state : ep.getPopulation().subList(0, SURVIVORS)) {
0:           state.getPayload().freeze(state);
1:         }
1: 
1:   }
1: 
1:   public int nextStep(int recordNumber) {
1:     int stepSize = stepSize(recordNumber, 2.6);
1:     if (stepSize < minInterval) {
1:       stepSize = minInterval;
1:     }
1: 
1:     if (stepSize > maxInterval) {
1:       stepSize = maxInterval;
1:     }
1: 
1:     int newCutoff = stepSize * (recordNumber / stepSize + 1);
1:     if (newCutoff < cutoff + currentStep) {
1:       newCutoff = cutoff + currentStep;
1:     } else {
1:       this.currentStep = stepSize;
1:     }
1:     return newCutoff;
1:   }
1: 
1:   public static int stepSize(int recordNumber, double multiplier) {
0:     final int[] bumps = new int[]{1, 2, 5};
1:     double log = Math.floor(multiplier * Math.log10(recordNumber));
1:     int bump = bumps[(int) log % bumps.length];
1:     int scale = (int) Math.pow(10, Math.floor(log / bumps.length));
1: 
1:     return bump * scale;
/////////////////////////////////////////////////////////////////////////
0:     this.minInterval = interval;
0:     this.maxInterval = interval;
0:     this.cutoff = interval * (record / interval + 1);
1:   }
1: 
1:   /**
1:    * Starts optimization using the shorter interval and progresses to the longer using the specified
1:    * number of steps per decade.  Note that values < 200 are not accepted.  Values even that small
1:    * are unlikely to be useful.
1:    *
0:    * @param minInterval  The minimum epoch length for the evolutionary optimization
0:    * @param maxInterval  The maximum epoch length
1:    */
1:   public void setInterval(int minInterval, int maxInterval) {
1:     this.minInterval = Math.max(200, minInterval);
1:     this.maxInterval = Math.max(200, maxInterval);
1:     this.cutoff = minInterval * (record / minInterval + 1);
/////////////////////////////////////////////////////////////////////////
1:   public int getMinInterval() {
1:     return minInterval;
1:   }
1: 
1:   public int getMaxInterval() {
1:     return maxInterval;
/////////////////////////////////////////////////////////////////////////
commit:e6b4e35
/////////////////////////////////////////////////////////////////////////
1: import org.apache.mahout.math.stats.OnlineAuc;
/////////////////////////////////////////////////////////////////////////
0:   // transient here is a signal to GSON not to serialize pending records
0:   private transient List<TrainingExample> buffer = Lists.newArrayList();
/////////////////////////////////////////////////////////////////////////
1:     train(trackingKey, null, actual, instance);
1:   }
1: 
1: 
1:   public void train(long trackingKey, String groupKey, int actual, Vector instance) {
1:     buffer.add(new TrainingExample(trackingKey, groupKey, actual, instance));
/////////////////////////////////////////////////////////////////////////
1:   public void setAucEvaluator(OnlineAuc auc) {
1:     seed.getPayload().setAucEvaluator(auc);
1:     setupOptimizer(poolSize);
1:   }
1: 
/////////////////////////////////////////////////////////////////////////
1:     setupOptimizer(poolSize);
/////////////////////////////////////////////////////////////////////////
0:       x.setMap(i, Mapping.logLimit(1e-8, 1));
1:       wrapped.train(example.getKey(), example.getGroupKey(), example.getActual(), example.getInstance());
/////////////////////////////////////////////////////////////////////////
1: 
1:     public void setAucEvaluator(OnlineAuc auc) {
1:       wrapped.setAucEvaluator(auc);
1:     }
1:     private String groupKey;
/////////////////////////////////////////////////////////////////////////
1:     public TrainingExample(long key, String groupKey, int actual, Vector instance) {
1:       this.groupKey = groupKey;
/////////////////////////////////////////////////////////////////////////
1: 
1:     public String getGroupKey() {
1:       return groupKey;
1:     }
commit:9cfe290
/////////////////////////////////////////////////////////////////////////
1:   private boolean freezeSurvivors = true;
1: 
/////////////////////////////////////////////////////////////////////////
0:     if (freezeSurvivors) {
0:       // now grossly hack the top survivors so they stick around.  Set their
0:       // mutation rates small and also hack their learning rate to be small
0:       // as well.
0:       for (State<Wrapper> state : ep.getPopulation().subList(0, SURVIVORS)) {
0:         state.getPayload().freeze(state);
1:       }
/////////////////////////////////////////////////////////////////////////
1:   public void setFreezeSurvivors(boolean freezeSurvivors) {
1:     this.freezeSurvivors = freezeSurvivors;
1:   }
1: 
commit:ef7b768
/////////////////////////////////////////////////////////////////////////
0:   private double averagingWindow;
/////////////////////////////////////////////////////////////////////////
1:   public void setAveragingWindow(int averagingWindow) {
1:     seed.getPayload().getLearner().setWindowSize(averagingWindow);
1:   }
1: 
commit:a87bed8
/////////////////////////////////////////////////////////////////////////
0:       s.getParams()[1] -= 10;
1:       s.setOmni(s.getOmni() / 20);
1:         step[i] /= 20;
commit:b553e02
/////////////////////////////////////////////////////////////////////////
1:   private static final int SURVIVORS = 2;
1: 
/////////////////////////////////////////////////////////////////////////
0:     // evolve based on new fitness
0:     ep.mutatePopulation(SURVIVORS);
1: 
0:     // now grossly hack the top survivors so they stick around.  Set their
0:     // mutation rates small and also hack their learning rate to be small
0:     // as well.
0:     for (State<Wrapper> state : ep.getPopulation().subList(0, SURVIVORS)) {
0:       state.getPayload().freeze(state);
1:     }
/////////////////////////////////////////////////////////////////////////
0:     public void freeze(State<Wrapper> s) {
1:       // radically decrease learning rate
0:       s.getParams()[1] -= 5;
1: 
1:       // and cause evolution to hold (almost)
0:       s.setOmni(s.getOmni() / 10);
1:       double[] step = s.getStep();
1:       for (int i = 0; i < step.length; i++) {
0:         step[i] /= 10;
1:       }
1:     }
1: 
commit:42b989b
/////////////////////////////////////////////////////////////////////////
0:           return payload.getLearner().logLikelihood();
1:       ep.close();
commit:3686970
/////////////////////////////////////////////////////////////////////////
1:       // set the range for regularization (lambda)
1:       // set the range for learning rate (mu)
commit:93ddcf8
/////////////////////////////////////////////////////////////////////////
1:  * probably use a 2-way unbalanced train/test split rather than full cross validation.  With the
0:  * current default settings, we have 100 learners running.  This is better than the alternative
0:  * of running hundreds of training passes to find good hyper-parameters because we only have to
0:  * parse and feature-ize our inputs once.  If you already have good hyper-parameters, then you
0:  * might prefer to just run one CrossFoldLearner with those settings.
1:  * <p/>
0:  * accord pretty well.  It would be nice to allow the fitness function to be pluggable. This
0:  * use of AUC means that AdaptiveLogisticRegression is mostly suited for binary target variables.
0:  * This will be fixed before long by extending OnlineAuc to handle non-binary cases or by using
0:  * a different fitness value in non-binary cases.
/////////////////////////////////////////////////////////////////////////
1:             if (x.getLearner().numCategories() == 2) {
1:               return x.wrapped.auc();
1:             } else {
1:               return x.wrapped.logLikelihood();
1:             }
commit:913d44d
/////////////////////////////////////////////////////////////////////////
0:   private List<TrainingExample> buffer = Lists.newArrayList();
0:   private State<Wrapper> seed;
1:   private int numFeatures;
1: 
0:   // for GSON
0:   @SuppressWarnings({"UnusedDeclaration"})
0:   private AdaptiveLogisticRegression() {
1:   }
/////////////////////////////////////////////////////////////////////////
0:   public void setBest(State<Wrapper> best) {
1:     this.best = best;
1:   }
1: 
1:   public int getRecord() {
1:     return record;
1:   }
1: 
1:   public void setRecord(int record) {
1:     this.record = record;
1:   }
1: 
0:   public int getEvaluationInterval() {
0:     return evaluationInterval;
1:   }
1: 
1:   public int getNumCategories() {
1:     return seed.getPayload().getLearner().numCategories();
1:   }
1: 
1:   public PriorFunction getPrior() {
1:     return seed.getPayload().getLearner().getPrior();
1:   }
1: 
0:   public void setEvaluationInterval(int evaluationInterval) {
0:     this.evaluationInterval = evaluationInterval;
1:   }
1: 
1:   public void setBuffer(List<TrainingExample> buffer) {
1:     this.buffer = buffer;
1:   }
1: 
1:   public List<TrainingExample> getBuffer() {
1:     return buffer;
1:   }
1: 
0:   public EvolutionaryProcess<Wrapper> getEp() {
1:     return ep;
1:   }
1: 
0:   public void setEp(EvolutionaryProcess<Wrapper> ep) {
1:     this.ep = ep;
1:   }
1: 
0:   public State<Wrapper> getSeed() {
1:     return seed;
1:   }
1: 
0:   public void setSeed(State<Wrapper> seed) {
1:     this.seed = seed;
1:   }
1: 
1:   public int getNumFeatures() {
1:     return numFeatures;
1:   }
1: 
0:   public void setNumFeatures(int numFeatures) {
1:     this.numFeatures = numFeatures;
1:   }
/////////////////////////////////////////////////////////////////////////
/////////////////////////////////////////////////////////////////////////
1:       wrapped.learningRate(params[i]);
/////////////////////////////////////////////////////////////////////////
0:       x.setMap(i, Mapping.softLimit(0.001, 10));
/////////////////////////////////////////////////////////////////////////
1:     private long key;
1:     private int actual;
1:     private Vector instance;
1: 
0:     // for GSON
0:     @SuppressWarnings({"UnusedDeclaration"})
1:     private TrainingExample() {
1:     }
commit:d3ace90
/////////////////////////////////////////////////////////////////////////
0:   private int threadCount = 20;
0:   private int poolSize = 20;
0:   private State<Wrapper> seed;
0:   private int numFeatures;
1:   public AdaptiveLogisticRegression(int numCategories, int numFeatures, PriorFunction prior) {
0:     this.numFeatures = numFeatures;
0:     seed = new State<Wrapper>(new double[2], 10);
0:     w.setMappings(seed);
1:     seed.setPayload(w);
0:     setPoolSize(poolSize);
/////////////////////////////////////////////////////////////////////////
0:   public void setPoolSize(int poolSize) {
1:     this.poolSize = poolSize;
1:     setupOptimizer(poolSize);
1:   }
1: 
1:   public void setThreadCount(int threadCount) {
1:     this.threadCount = threadCount;
1:     setupOptimizer(poolSize);
1:   }
1: 
1:   private void setupOptimizer(int poolSize) {
0:     ep = new EvolutionaryProcess<Wrapper>(threadCount, poolSize, seed);
1:   }
1: 
1:   /**
0:    * Returns the size of the internal feature vector.  Note that this is not the
0:    * same as the number of distinct features, especially if feature hashing is
0:    * being used.
1:    * @return The internal feature vector size.
1:    */
1:   public int numFeatures() {
1:     return numFeatures;
1:   }
1: 
commit:4b25f31
/////////////////////////////////////////////////////////////////////////
1: package org.apache.mahout.classifier.sgd;
1: 
0: import com.google.common.collect.Lists;
1: import org.apache.mahout.classifier.OnlineLearner;
1: import org.apache.mahout.ep.Payload;
1: import org.apache.mahout.ep.EvolutionaryProcess;
1: import org.apache.mahout.ep.Mapping;
1: import org.apache.mahout.ep.State;
1: import org.apache.mahout.math.Vector;
1: 
1: import java.util.List;
1: import java.util.concurrent.ExecutionException;
1: 
1: /**
0:  * This is a meta-learner that maintains a pool of ordinary OnlineLogisticRegression learners. Each
1:  * member of the pool has different learning rates.  Whichever of the learners in the pool falls
1:  * behind in terms of average log-likelihood will be tossed out and replaced with variants of the
1:  * survivors.  This will let us automatically derive an annealing schedule that optimizes learning
1:  * speed.  Since on-line learners tend to be IO bound anyway, it doesn't cost as much as it might
1:  * seem that it would to maintain multiple learners in memory.  Doing this adaptation on-line as we
1:  * learn also decreases the number of learning rate parameters required and replaces the normal
1:  * hyper-parameter search.
1:  * <p/>
0:  * One wrinkle is that the pool of learners that we maintain is actually a pool of CrossFoldLearners
0:  * which themselves contain several OnlineLogisticRegression objects.  These pools allow estimation
1:  * of performance on the fly even if we make many passes through the data.  This does, however,
1:  * increase the cost of training since if we are using 5-fold cross-validation, each vector is used
1:  * 4 times for training and once for classification.  If this becomes a problem, then we should
0:  * probably use a 2-way unbalanced train/test split rather than full cross validation.
1:  *
0:  * The fitness used here is AUC.  Another alternative would be to try log-likelihood, but it is
0:  * much easier to get bogus values of log-likelihood than with AUC and the results seem to
0:  * accord pretty well.  It would be nice to allow the fitness function to be pluggable.
1:  */
0: public class AdaptiveLogisticRegression implements OnlineLearner {
0:   private int record = 0;
0:   private int evaluationInterval = 1000;
1: 
0:   List<TrainingExample> buffer = Lists.newArrayList();
0:   private EvolutionaryProcess<Wrapper> ep;
0:   private State<Wrapper> best;
1: 
0:   public AdaptiveLogisticRegression(int poolSize, int numCategories, int numFeatures, PriorFunction prior) {
0:     State<Wrapper> s0 = new State<Wrapper>(new double[2], 10);
1:     Wrapper w = new Wrapper(numCategories, numFeatures, prior);
0:     s0.setPayload(w);
0:     w.setMappings(s0);
0:     ep = new EvolutionaryProcess<Wrapper>(20, poolSize, s0);
1:   }
1: 
1:   @Override
1:   public void train(int actual, Vector instance) {
0:     train(record, actual, instance);
1:   }
1: 
1:   @Override
1:   public void train(long trackingKey, int actual, Vector instance) {
1:     record++;
1: 
0:     buffer.add(new TrainingExample(trackingKey, actual, instance));
0:     if (buffer.size() > evaluationInterval) {
1:       trainWithBufferedExamples();
1:     }
1:   }
1: 
1:   private void trainWithBufferedExamples() {
1:     try {
0:       this.best = ep.parallelDo(new EvolutionaryProcess.Function<Wrapper>() {
0:         public double apply(Wrapper x, double[] params) {
1:           for (TrainingExample example : buffer) {
1:             x.train(example);
1:           }
0:           if (!x.getLearner().validModel()) {
1:             return Double.NaN;
1:           } else {
0:             return x.wrapped.auc();
1:           }
1:         }
1:       });
1:     } catch (InterruptedException e) {
1:       // ignore ... shouldn't happen
1:     } catch (ExecutionException e) {
0:       throw new RuntimeException(e);
1:     }
1: 
0:     ep.mutatePopulation(2);
0:     buffer.clear();
1:   }
1: 
1:   @Override
1:   public void close() {
1:     trainWithBufferedExamples();
1:     try {
0:       ep.parallelDo(new EvolutionaryProcess.Function<Wrapper>() {
1:         @Override
0:         public double apply(Wrapper payload, double[] params) {
0:           payload.getLearner().close();
0:           return payload.getLearner().auc();
1:         }
1:       });
1:     } catch (InterruptedException e) {
0:       // ignore
1:     } catch (ExecutionException e) {
0:       throw new RuntimeException(e);
1:     }
1:   }
1: 
1:   /**
1:    * How often should the evolutionary optimization of learning parameters occur?
0:    * @param interval  Number of training examples to use in each epoch of optimization.
1:    */
1:   public void setInterval(int interval) {
0:     this.evaluationInterval = interval;
1:   }
1: 
1:   /**
0:    * What is the AUC for the current best member of the population.  If no member is best,
0:    * usually because we haven't done any training yet, then the result is set to NaN.
0:    * @return  The AUC of the best member of the population or NaN if we can't figure that out.
1:    */
1:   public double auc() {
0:     if (best != null) {
0:       return best.getPayload().getLearner().auc();
1:     } else {
1:       return Double.NaN;
1:     }
1:   }
1: 
0:   public State<Wrapper> getBest() {
1:     return best;
1:   }
1: 
1: 
1:   /**
1:    * Provides a shim between the EP optimization stuff and the CrossFoldLearner.  The most important
1:    * interface has to do with the parameters of the optimization.  These are taken from the double[]
1:    * params in the following order <ul> <li> regularization constant lambda <li> learningRate </ul>.
1:    * All other parameters are set in such a way so as to defeat annealing to the extent possible.
1:    * This lets the evolutionary algorithm handle the annealing.
0:    * <p>
1:    * Note that per coefficient annealing is still done and no optimization of the per coefficient
1:    * offset is done.
1:    */
0:   public static class Wrapper implements Payload<Wrapper> {
0:     private static volatile int counter = 0;
1: 
0:     private volatile int id = counter++;
1:     private CrossFoldLearner wrapped;
1: 
0:     private Wrapper() {
0:       // just here to help copy
1:     }
1: 
1:     public Wrapper(int numCategories, int numFeatures, PriorFunction prior) {
1:       wrapped = new CrossFoldLearner(5, numCategories, numFeatures, prior);
1:     }
1: 
1:     @Override
1:     public Wrapper copy() {
1:       Wrapper r = new Wrapper();
1:       r.wrapped = wrapped.copy();
1:       return r;
1:     }
1: 
1:     @Override
1:     public void update(double[] params) {
1:       int i = 0;
1:       wrapped.lambda(params[i++]);
0:       wrapped.learningRate(params[i++]);
1: 
1:       wrapped.stepOffset(1);
1:       wrapped.alpha(1);
1:       wrapped.decayExponent(0);
1:     }
1: 
0:     public void setMappings(State<Wrapper> x) {
1:       int i = 0;
0:       x.setMap(i++, Mapping.logLimit(1e-8, 0.1));
0:       x.setMap(i++, Mapping.softLimit(0.001, 10));
1:     }
1: 
1:     public void train(TrainingExample example) {
0:       wrapped.train(example.getKey(), example.getActual(), example.getInstance());
1:     }
1: 
1:     public CrossFoldLearner getLearner() {
1:       return wrapped;
1:     }
1: 
1:     @Override
1:     public String toString() {
0:       return String.format("auc=%.2f", wrapped.auc());
1:     }
1:   }
1: 
0:   public static class TrainingExample {
0:     private long key;
0:     private int actual;
0:     private Vector instance;
1: 
0:     public TrainingExample(long key, int actual, Vector instance) {
1:       this.key = key;
1:       this.actual = actual;
1:       this.instance = instance;
1:     }
1: 
1:     public long getKey() {
1:       return key;
1:     }
1: 
1:     public int getActual() {
1:       return actual;
1:     }
1: 
1:     public Vector getInstance() {
1:       return instance;
1:     }
1:   }
1: }
1: 
author:frankscholten
-------------------------------------------------------------------------------
commit:1a42d85
author:smarthi
-------------------------------------------------------------------------------
commit:d74edf4
/////////////////////////////////////////////////////////////////////////
1:    * @see #AdaptiveLogisticRegression(int, int, org.apache.mahout.classifier.sgd.PriorFunction, int, int)
commit:c88c240
/////////////////////////////////////////////////////////////////////////
1:     } finally {
0:       ep.close();
author:Sebastian Schelter
-------------------------------------------------------------------------------
commit:6d16230
/////////////////////////////////////////////////////////////////////////
1:       double[] params = s.getParams();
1:       params[1] -= 10;
commit:4841efb
/////////////////////////////////////////////////////////////////////////
1: import org.slf4j.Logger;
1: import org.slf4j.LoggerFactory;
/////////////////////////////////////////////////////////////////////////
1:  * This is a meta-learner that maintains a pool of ordinary
1:  * {@link org.apache.mahout.classifier.sgd.OnlineLogisticRegression} learners. Each
/////////////////////////////////////////////////////////////////////////
1:  * One wrinkle is that the pool of learners that we maintain is actually a pool of
1:  * {@link org.apache.mahout.classifier.sgd.CrossFoldLearner} which themselves contain several OnlineLogisticRegression
1:  * objects.  These pools allow estimation
/////////////////////////////////////////////////////////////////////////
1:   private static final Logger log = LoggerFactory.getLogger(AdaptiveLogisticRegression.class);
0: 
1:   public AdaptiveLogisticRegression() {}
/////////////////////////////////////////////////////////////////////////
1:   public AdaptiveLogisticRegression(int numCategories, int numFeatures, PriorFunction prior, int threadCount,
1:       int poolSize) {
/////////////////////////////////////////////////////////////////////////
1:       log.warn("Ignoring exception", e);
/////////////////////////////////////////////////////////////////////////
1:       log.warn("Ignoring exception", e);
author:Sean R. Owen
-------------------------------------------------------------------------------
commit:229aeff
/////////////////////////////////////////////////////////////////////////
1:     Wrapper.setMappings(seed);
/////////////////////////////////////////////////////////////////////////
1:           Wrapper.freeze(state);
/////////////////////////////////////////////////////////////////////////
1:     public static void freeze(State<Wrapper, CrossFoldLearner> s) {
/////////////////////////////////////////////////////////////////////////
1:     public static void setMappings(State<Wrapper, CrossFoldLearner> x) {
commit:35032b8
/////////////////////////////////////////////////////////////////////////
commit:f201912
/////////////////////////////////////////////////////////////////////////
1:       throw new IllegalStateException(e.getCause());
commit:049e7dc
/////////////////////////////////////////////////////////////////////////
/////////////////////////////////////////////////////////////////////////
1:     int[] bumps = {1, 2, 5};
/////////////////////////////////////////////////////////////////////////
1:       throw new IllegalStateException(e);
/////////////////////////////////////////////////////////////////////////
1:       x.setMap(i, Mapping.logLimit(1.0e-8, 1));
/////////////////////////////////////////////////////////////////////////
commit:61ae2e7
/////////////////////////////////////////////////////////////////////////
0: 
1:   private int record;
/////////////////////////////////////////////////////////////////////////
0:   //private double averagingWindow;
commit:5ce5992
/////////////////////////////////////////////////////////////////////////
/////////////////////////////////////////////////////////////////////////
commit:6fca24e
/////////////////////////////////////////////////////////////////////////
1: /**
1:  * Licensed to the Apache Software Foundation (ASF) under one or more
1:  * contributor license agreements.  See the NOTICE file distributed with
1:  * this work for additional information regarding copyright ownership.
1:  * The ASF licenses this file to You under the Apache License, Version 2.0
1:  * (the "License"); you may not use this file except in compliance with
1:  * the License.  You may obtain a copy of the License at
1:  *
1:  *     http://www.apache.org/licenses/LICENSE-2.0
1:  *
1:  * Unless required by applicable law or agreed to in writing, software
1:  * distributed under the License is distributed on an "AS IS" BASIS,
1:  * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
1:  * See the License for the specific language governing permissions and
1:  * limitations under the License.
1:  */
0: 
/////////////////////////////////////////////////////////////////////////
1: import java.util.Locale;
/////////////////////////////////////////////////////////////////////////
0:   private final List<TrainingExample> buffer = Lists.newArrayList();
0:   private final State<Wrapper> seed;
0:   private final int numFeatures;
/////////////////////////////////////////////////////////////////////////
1:         @Override
1:           if (x.getLearner().validModel()) {
0:           } else {
0:             return Double.NaN;
0:       throw new IllegalStateException(e);
/////////////////////////////////////////////////////////////////////////
1:     if (best == null) {
0:     } else {
0:       return best.getPayload().getLearner().auc();
/////////////////////////////////////////////////////////////////////////
0:     //private static volatile int counter = 0;
0:     //private volatile int id = counter++;
/////////////////////////////////////////////////////////////////////////
1:       x.setMap(i++, Mapping.logLimit(1.0e-8, 0.1));
/////////////////////////////////////////////////////////////////////////
1:       return String.format(Locale.ENGLISH, "auc=%.2f", wrapped.auc());
0:     private final long key;
0:     private final int actual;
0:     private final Vector instance;
author:Grant Ingersoll
-------------------------------------------------------------------------------
commit:69f324d
/////////////////////////////////////////////////////////////////////////
1:   public static final int DEFAULT_THREAD_COUNT = 20;
1:   public static final int DEFAULT_POOL_SIZE = 20;
/////////////////////////////////////////////////////////////////////////
1:   private int threadCount = DEFAULT_THREAD_COUNT;
1:   private int poolSize = DEFAULT_POOL_SIZE;
/////////////////////////////////////////////////////////////////////////
1:   /**
1:    * Uses {@link #DEFAULT_THREAD_COUNT} and {@link #DEFAULT_POOL_SIZE}
1:    * @param numCategories The number of categories (labels) to train on
1:    * @param numFeatures The number of features used in creating the vectors (i.e. the cardinality of the vector)
1:    * @param prior The {@link org.apache.mahout.classifier.sgd.PriorFunction} to use
1:    *
0:    * @see {@link #AdaptiveLogisticRegression(int, int, org.apache.mahout.classifier.sgd.PriorFunction, int, int)}
1:    */
1:     this(numCategories, numFeatures, prior, DEFAULT_THREAD_COUNT, DEFAULT_POOL_SIZE);
0:   }
0: 
1:   /**
1:    *
1:    * @param numCategories The number of categories (labels) to train on
1:    * @param numFeatures The number of features used in creating the vectors (i.e. the cardinality of the vector)
1:    * @param prior The {@link org.apache.mahout.classifier.sgd.PriorFunction} to use
1:    * @param threadCount The number of threads to use for training
1:    * @param poolSize The number of {@link org.apache.mahout.classifier.sgd.CrossFoldLearner} to use.
1:    */
0:   public AdaptiveLogisticRegression(int numCategories, int numFeatures, PriorFunction prior, int threadCount, int poolSize) {
1:     this.threadCount = threadCount;
1:     this.poolSize = poolSize;
1:     setPoolSize(this.poolSize);
commit:8ad8d96
/////////////////////////////////////////////////////////////////////////
0: import org.apache.mahout.ep.Payload;
/////////////////////////////////////////////////////////////////////////
0:  * This is a meta-learner that maintains a pool of ordinary {@link org.apache.mahout.classifier.sgd.OnlineLogisticRegression} learners. Each
/////////////////////////////////////////////////////////////////////////
0:  * One wrinkle is that the pool of learners that we maintain is actually a pool of {@link org.apache.mahout.classifier.sgd.CrossFoldLearner}
/////////////////////////////////////////////////////////////////////////
1:     //don't train until we have enough examples
============================================================================