1:131eb4a: /**
1:131eb4a:  * Licensed to the Apache Software Foundation (ASF) under one or more
1:131eb4a:  * contributor license agreements.  See the NOTICE file distributed with
1:131eb4a:  * this work for additional information regarding copyright ownership.
1:131eb4a:  * The ASF licenses this file to You under the Apache License, Version 2.0
1:131eb4a:  * (the "License"); you may not use this file except in compliance with
1:131eb4a:  * the License.  You may obtain a copy of the License at
1:131eb4a:  *
1:131eb4a:  *     http://www.apache.org/licenses/LICENSE-2.0
1:131eb4a:  *
1:131eb4a:  * Unless required by applicable law or agreed to in writing, software
1:131eb4a:  * distributed under the License is distributed on an "AS IS" BASIS,
1:131eb4a:  * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
1:131eb4a:  * See the License for the specific language governing permissions and
1:131eb4a:  * limitations under the License.
1:131eb4a:  */
1:131eb4a: package org.apache.mahout.clustering.lda.cvb;
3:131eb4a: 
1:85f9ece: import java.io.IOException;
1:85f9ece: import java.net.URI;
1:85f9ece: import java.util.ArrayList;
1:85f9ece: import java.util.List;
1:85f9ece: 
1:131eb4a: import com.google.common.base.Joiner;
1:131eb4a: import com.google.common.base.Preconditions;
1:131eb4a: import org.apache.hadoop.conf.Configuration;
1:131eb4a: import org.apache.hadoop.filecache.DistributedCache;
1:131eb4a: import org.apache.hadoop.fs.FileStatus;
1:131eb4a: import org.apache.hadoop.fs.FileSystem;
1:131eb4a: import org.apache.hadoop.fs.Path;
1:131eb4a: import org.apache.hadoop.io.DoubleWritable;
1:131eb4a: import org.apache.hadoop.io.IntWritable;
1:131eb4a: import org.apache.hadoop.io.SequenceFile;
1:131eb4a: import org.apache.hadoop.io.Text;
1:131eb4a: import org.apache.hadoop.mapreduce.Job;
1:131eb4a: import org.apache.hadoop.mapreduce.Reducer;
1:131eb4a: import org.apache.hadoop.mapreduce.lib.input.SequenceFileInputFormat;
1:131eb4a: import org.apache.hadoop.mapreduce.lib.output.SequenceFileOutputFormat;
1:131eb4a: import org.apache.hadoop.util.ToolRunner;
1:131eb4a: import org.apache.mahout.common.AbstractJob;
1:131eb4a: import org.apache.mahout.common.HadoopUtil;
1:131eb4a: import org.apache.mahout.common.Pair;
1:131eb4a: import org.apache.mahout.common.commandline.DefaultOptionCreator;
1:131eb4a: import org.apache.mahout.common.iterator.sequencefile.PathFilters;
1:131eb4a: import org.apache.mahout.common.iterator.sequencefile.PathType;
1:131eb4a: import org.apache.mahout.common.iterator.sequencefile.SequenceFileDirIterable;
1:131eb4a: import org.apache.mahout.common.mapreduce.VectorSumReducer;
1:131eb4a: import org.apache.mahout.math.VectorWritable;
1:131eb4a: import org.slf4j.Logger;
1:131eb4a: import org.slf4j.LoggerFactory;
1:131eb4a: 
1:131eb4a: /**
1:131eb4a:  * See {@link CachingCVB0Mapper} for more details on scalability and room for improvement.
1:131eb4a:  * To try out this LDA implementation without using Hadoop, check out
1:131eb4a:  * {@link InMemoryCollapsedVariationalBayes0}.  If you want to do training directly in java code
1:131eb4a:  * with your own main(), then look to {@link ModelTrainer} and {@link TopicModel}.
1:131eb4a:  *
1:4fbfbc6:  * Usage: {@code ./bin/mahout cvb <i>options</i>}
1:131eb4a:  * <p>
1:131eb4a:  * Valid options include:
1:131eb4a:  * <dl>
1:131eb4a:  * <dt>{@code --input path}</td>
1:131eb4a:  * <dd>Input path for {@code SequenceFile<IntWritable, VectorWritable>} document vectors. See
1:229aeff:  * {@link org.apache.mahout.vectorizer.SparseVectorsFromSequenceFiles}
1:229aeff:  *  for details on how to generate this input format.</dd>
1:131eb4a:  * <dt>{@code --dictionary path}</dt>
1:131eb4a:  * <dd>Path to dictionary file(s) generated during construction of input document vectors (glob
1:131eb4a:  * expression supported). If set, this data is scanned to determine an appropriate value for option
1:131eb4a:  * {@code --num_terms}.</dd>
1:131eb4a:  * <dt>{@code --output path}</dt>
1:131eb4a:  * <dd>Output path for topic-term distributions.</dd>
1:131eb4a:  * <dt>{@code --doc_topic_output path}</dt>
1:131eb4a:  * <dd>Output path for doc-topic distributions.</dd>
1:131eb4a:  * <dt>{@code --num_topics k}</dt>
1:131eb4a:  * <dd>Number of latent topics.</dd>
1:131eb4a:  * <dt>{@code --num_terms nt}</dt>
1:131eb4a:  * <dd>Number of unique features defined by input document vectors. If option {@code --dictionary}
1:131eb4a:  * is defined and this option is unspecified, term count is calculated from dictionary.</dd>
1:131eb4a:  * <dt>{@code --topic_model_temp_dir path}</dt>
1:131eb4a:  * <dd>Path in which to store model state after each iteration.</dd>
1:131eb4a:  * <dt>{@code --maxIter i}</dt>
1:131eb4a:  * <dd>Maximum number of iterations to perform. If this value is less than or equal to the number of
1:131eb4a:  * iteration states found beneath the path specified by option {@code --topic_model_temp_dir}, no
1:131eb4a:  * further iterations are performed. Instead, output topic-term and doc-topic distributions are
1:131eb4a:  * generated using data from the specified iteration.</dd>
1:131eb4a:  * <dt>{@code --max_doc_topic_iters i}</dt>
1:131eb4a:  * <dd>Maximum number of iterations per doc for p(topic|doc) learning. Defaults to {@code 10}.</dd>
1:131eb4a:  * <dt>{@code --doc_topic_smoothing a}</dt>
1:131eb4a:  * <dd>Smoothing for doc-topic distribution. Defaults to {@code 0.0001}.</dd>
1:131eb4a:  * <dt>{@code --term_topic_smoothing e}</dt>
1:131eb4a:  * <dd>Smoothing for topic-term distribution. Defaults to {@code 0.0001}.</dd>
1:131eb4a:  * <dt>{@code --random_seed seed}</dt>
1:131eb4a:  * <dd>Integer seed for random number generation.</dd>
1:131eb4a:  * <dt>{@code --test_set_percentage p}</dt>
1:131eb4a:  * <dd>Fraction of data to hold out for testing. Defaults to {@code 0.0}.</dd>
1:131eb4a:  * <dt>{@code --iteration_block_size block}</dt>
1:131eb4a:  * <dd>Number of iterations between perplexity checks. Defaults to {@code 10}. This option is
1:131eb4a:  * ignored unless option {@code --test_set_percentage} is greater than zero.</dd>
1:131eb4a:  * </dl>
1:131eb4a:  */
1:131eb4a: public class CVB0Driver extends AbstractJob {
1:131eb4a:   private static final Logger log = LoggerFactory.getLogger(CVB0Driver.class);
1:131eb4a: 
1:131eb4a:   public static final String NUM_TOPICS = "num_topics";
1:131eb4a:   public static final String NUM_TERMS = "num_terms";
1:131eb4a:   public static final String DOC_TOPIC_SMOOTHING = "doc_topic_smoothing";
1:131eb4a:   public static final String TERM_TOPIC_SMOOTHING = "term_topic_smoothing";
1:131eb4a:   public static final String DICTIONARY = "dictionary";
1:131eb4a:   public static final String DOC_TOPIC_OUTPUT = "doc_topic_output";
1:131eb4a:   public static final String MODEL_TEMP_DIR = "topic_model_temp_dir";
1:131eb4a:   public static final String ITERATION_BLOCK_SIZE = "iteration_block_size";
1:131eb4a:   public static final String RANDOM_SEED = "random_seed";
1:131eb4a:   public static final String TEST_SET_FRACTION = "test_set_fraction";
1:131eb4a:   public static final String NUM_TRAIN_THREADS = "num_train_threads";
1:131eb4a:   public static final String NUM_UPDATE_THREADS = "num_update_threads";
1:131eb4a:   public static final String MAX_ITERATIONS_PER_DOC = "max_doc_topic_iters";
1:131eb4a:   public static final String MODEL_WEIGHT = "prev_iter_mult";
1:131eb4a:   public static final String NUM_REDUCE_TASKS = "num_reduce_tasks";
1:131eb4a:   public static final String BACKFILL_PERPLEXITY = "backfill_perplexity";
1:131eb4a:   private static final String MODEL_PATHS = "mahout.lda.cvb.modelPath";
1:131eb4a: 
1:4aed7cc:   private static final double DEFAULT_CONVERGENCE_DELTA = 0;
1:4aed7cc:   private static final double DEFAULT_DOC_TOPIC_SMOOTHING = 0.0001;
1:4aed7cc:   private static final double DEFAULT_TERM_TOPIC_SMOOTHING = 0.0001;
1:4aed7cc:   private static final int DEFAULT_ITERATION_BLOCK_SIZE = 10;
1:4aed7cc:   private static final double DEFAULT_TEST_SET_FRACTION = 0;
1:4aed7cc:   private static final int DEFAULT_NUM_TRAIN_THREADS = 4;
1:4aed7cc:   private static final int DEFAULT_NUM_UPDATE_THREADS = 1;
1:4aed7cc:   private static final int DEFAULT_MAX_ITERATIONS_PER_DOC = 10;
1:4aed7cc:   private static final int DEFAULT_NUM_REDUCE_TASKS = 10;
1:4aed7cc: 
1:131eb4a:   @Override
1:131eb4a:   public int run(String[] args) throws Exception {
1:131eb4a:     addInputOption();
1:131eb4a:     addOutputOption();
1:131eb4a:     addOption(DefaultOptionCreator.maxIterationsOption().create());
1:4aed7cc:     addOption(DefaultOptionCreator.CONVERGENCE_DELTA_OPTION, "cd", "The convergence delta value",
1:4aed7cc:               String.valueOf(DEFAULT_CONVERGENCE_DELTA));
1:131eb4a:     addOption(DefaultOptionCreator.overwriteOption().create());
1:131eb4a: 
1:131eb4a:     addOption(NUM_TOPICS, "k", "Number of topics to learn", true);
1:131eb4a:     addOption(NUM_TERMS, "nt", "Vocabulary size", false);
1:4aed7cc:     addOption(DOC_TOPIC_SMOOTHING, "a", "Smoothing for document/topic distribution",
1:4aed7cc:               String.valueOf(DEFAULT_DOC_TOPIC_SMOOTHING));
1:4aed7cc:     addOption(TERM_TOPIC_SMOOTHING, "e", "Smoothing for topic/term distribution",
1:4aed7cc:               String.valueOf(DEFAULT_TERM_TOPIC_SMOOTHING));
1:4aed7cc:     addOption(DICTIONARY, "dict", "Path to term-dictionary file(s) (glob expression supported)", false);
1:4aed7cc:     addOption(DOC_TOPIC_OUTPUT, "dt", "Output path for the training doc/topic distribution", false);
1:4aed7cc:     addOption(MODEL_TEMP_DIR, "mt", "Path to intermediate model path (useful for restarting)", false);
1:4aed7cc:     addOption(ITERATION_BLOCK_SIZE, "block", "Number of iterations per perplexity check",
1:4aed7cc:               String.valueOf(DEFAULT_ITERATION_BLOCK_SIZE));
1:131eb4a:     addOption(RANDOM_SEED, "seed", "Random seed", false);
1:4aed7cc:     addOption(TEST_SET_FRACTION, "tf", "Fraction of data to hold out for testing",
1:4aed7cc:               String.valueOf(DEFAULT_TEST_SET_FRACTION));
1:4aed7cc:     addOption(NUM_TRAIN_THREADS, "ntt", "number of threads per mapper to train with",
1:4aed7cc:               String.valueOf(DEFAULT_NUM_TRAIN_THREADS));
1:131eb4a:     addOption(NUM_UPDATE_THREADS, "nut", "number of threads per mapper to update the model with",
1:4aed7cc:               String.valueOf(DEFAULT_NUM_UPDATE_THREADS));
1:4aed7cc:     addOption(MAX_ITERATIONS_PER_DOC, "mipd", "max number of iterations per doc for p(topic|doc) learning",
1:4aed7cc:               String.valueOf(DEFAULT_MAX_ITERATIONS_PER_DOC));
1:4aed7cc:     addOption(NUM_REDUCE_TASKS, null, "number of reducers to use during model estimation",
1:4aed7cc:               String.valueOf(DEFAULT_NUM_REDUCE_TASKS));
1:4aed7cc:     addOption(buildOption(BACKFILL_PERPLEXITY, null, "enable backfilling of missing perplexity values", false, false,
1:4aed7cc:               null));
1:131eb4a: 
1:e64dd36:     if (parseArguments(args) == null) {
1:131eb4a:       return -1;
2:131eb4a:     }
1:131eb4a: 
1:131eb4a:     int numTopics = Integer.parseInt(getOption(NUM_TOPICS));
1:131eb4a:     Path inputPath = getInputPath();
1:131eb4a:     Path topicModelOutputPath = getOutputPath();
1:131eb4a:     int maxIterations = Integer.parseInt(getOption(DefaultOptionCreator.MAX_ITERATIONS_OPTION));
1:131eb4a:     int iterationBlockSize = Integer.parseInt(getOption(ITERATION_BLOCK_SIZE));
1:131eb4a:     double convergenceDelta = Double.parseDouble(getOption(DefaultOptionCreator.CONVERGENCE_DELTA_OPTION));
1:131eb4a:     double alpha = Double.parseDouble(getOption(DOC_TOPIC_SMOOTHING));
1:131eb4a:     double eta = Double.parseDouble(getOption(TERM_TOPIC_SMOOTHING));
1:131eb4a:     int numTrainThreads = Integer.parseInt(getOption(NUM_TRAIN_THREADS));
1:131eb4a:     int numUpdateThreads = Integer.parseInt(getOption(NUM_UPDATE_THREADS));
1:131eb4a:     int maxItersPerDoc = Integer.parseInt(getOption(MAX_ITERATIONS_PER_DOC));
1:131eb4a:     Path dictionaryPath = hasOption(DICTIONARY) ? new Path(getOption(DICTIONARY)) : null;
1:131eb4a:     int numTerms = hasOption(NUM_TERMS)
1:131eb4a:                  ? Integer.parseInt(getOption(NUM_TERMS))
1:131eb4a:                  : getNumTerms(getConf(), dictionaryPath);
1:131eb4a:     Path docTopicOutputPath = hasOption(DOC_TOPIC_OUTPUT) ? new Path(getOption(DOC_TOPIC_OUTPUT)) : null;
1:131eb4a:     Path modelTempPath = hasOption(MODEL_TEMP_DIR)
1:131eb4a:                        ? new Path(getOption(MODEL_TEMP_DIR))
1:131eb4a:                        : getTempPath("topicModelState");
1:131eb4a:     long seed = hasOption(RANDOM_SEED)
1:131eb4a:               ? Long.parseLong(getOption(RANDOM_SEED))
1:131eb4a:               : System.nanoTime() % 10000;
1:131eb4a:     float testFraction = hasOption(TEST_SET_FRACTION)
1:131eb4a:                        ? Float.parseFloat(getOption(TEST_SET_FRACTION))
1:4fbfbc6:                        : 0.0f;
1:131eb4a:     int numReduceTasks = Integer.parseInt(getOption(NUM_REDUCE_TASKS));
1:131eb4a:     boolean backfillPerplexity = hasOption(BACKFILL_PERPLEXITY);
1:131eb4a: 
1:131eb4a:     return run(getConf(), inputPath, topicModelOutputPath, numTopics, numTerms, alpha, eta,
1:131eb4a:         maxIterations, iterationBlockSize, convergenceDelta, dictionaryPath, docTopicOutputPath,
1:131eb4a:         modelTempPath, seed, testFraction, numTrainThreads, numUpdateThreads, maxItersPerDoc,
1:131eb4a:         numReduceTasks, backfillPerplexity);
1:131eb4a:   }
1:131eb4a: 
1:4fbfbc6:   private static int getNumTerms(Configuration conf, Path dictionaryPath) throws IOException {
1:131eb4a:     FileSystem fs = dictionaryPath.getFileSystem(conf);
1:131eb4a:     Text key = new Text();
1:131eb4a:     IntWritable value = new IntWritable();
1:131eb4a:     int maxTermId = -1;
1:131eb4a:     for (FileStatus stat : fs.globStatus(dictionaryPath)) {
1:131eb4a:       SequenceFile.Reader reader = new SequenceFile.Reader(fs, stat.getPath(), conf);
1:131eb4a:       while (reader.next(key, value)) {
1:131eb4a:         maxTermId = Math.max(maxTermId, value.get());
1:131eb4a:       }
1:131eb4a:     }
1:131eb4a:     return maxTermId + 1;
1:131eb4a:   }
1:131eb4a: 
1:4aed7cc:   public int run(Configuration conf,
1:4aed7cc:                  Path inputPath,
1:4aed7cc:                  Path topicModelOutputPath,
1:4aed7cc:                  int numTopics,
1:4aed7cc:                  int numTerms,
1:4aed7cc:                  double alpha,
1:4aed7cc:                  double eta,
1:4aed7cc:                  int maxIterations,
1:4aed7cc:                  int iterationBlockSize,
1:4aed7cc:                  double convergenceDelta,
1:4aed7cc:                  Path dictionaryPath,
1:4aed7cc:                  Path docTopicOutputPath,
1:4aed7cc:                  Path topicModelStateTempPath,
1:4aed7cc:                  long randomSeed,
1:4aed7cc:                  float testFraction,
1:4aed7cc:                  int numTrainThreads,
1:4aed7cc:                  int numUpdateThreads,
1:4aed7cc:                  int maxItersPerDoc,
1:4aed7cc:                  int numReduceTasks,
1:4aed7cc:                  boolean backfillPerplexity)
1:131eb4a:     throws ClassNotFoundException, IOException, InterruptedException {
1:4aed7cc: 
1:4aed7cc:     setConf(conf);
1:4aed7cc: 
1:131eb4a:     // verify arguments
1:4fbfbc6:     Preconditions.checkArgument(testFraction >= 0.0 && testFraction <= 1.0,
1:131eb4a:         "Expected 'testFraction' value in range [0, 1] but found value '%s'", testFraction);
1:4fbfbc6:     Preconditions.checkArgument(!backfillPerplexity || testFraction > 0.0,
1:131eb4a:         "Expected 'testFraction' value in range (0, 1] but found value '%s'", testFraction);
1:131eb4a: 
1:44459bd:     String infoString = "Will run Collapsed Variational Bayes (0th-derivative approximation) " 
1:44459bd:       + "learning for LDA on {} (numTerms: {}), finding {}-topics, with document/topic prior {}, " 
1:44459bd:       + "topic/term prior {}.  Maximum iterations to run will be {}, unless the change in " 
1:44459bd:       + "perplexity is less than {}.  Topic model output (p(term|topic) for each topic) will be " 
1:44459bd:       + "stored {}.  Random initialization seed is {}, holding out {} of the data for perplexity " 
1:44459bd:       + "check\n";
1:8396a27:     log.info(infoString, inputPath, numTerms, numTopics, alpha, eta, maxIterations,
1:8396a27:              convergenceDelta, topicModelOutputPath, randomSeed, testFraction);
1:131eb4a:     infoString = dictionaryPath == null
1:4fbfbc6:                ? "" : "Dictionary to be used located " + dictionaryPath.toString() + '\n';
1:131eb4a:     infoString += docTopicOutputPath == null
1:4fbfbc6:                ? "" : "p(topic|docId) will be stored " + docTopicOutputPath.toString() + '\n';
1:131eb4a:     log.info(infoString);
1:131eb4a: 
1:1de8cec:     FileSystem fs = FileSystem.get(topicModelStateTempPath.toUri(), conf);
1:131eb4a:     int iterationNumber = getCurrentIterationNumber(conf, topicModelStateTempPath, maxIterations);
1:131eb4a:     log.info("Current iteration number: {}", iterationNumber);
1:131eb4a: 
1:131eb4a:     conf.set(NUM_TOPICS, String.valueOf(numTopics));
1:131eb4a:     conf.set(NUM_TERMS, String.valueOf(numTerms));
1:131eb4a:     conf.set(DOC_TOPIC_SMOOTHING, String.valueOf(alpha));
1:131eb4a:     conf.set(TERM_TOPIC_SMOOTHING, String.valueOf(eta));
1:131eb4a:     conf.set(RANDOM_SEED, String.valueOf(randomSeed));
1:131eb4a:     conf.set(NUM_TRAIN_THREADS, String.valueOf(numTrainThreads));
1:131eb4a:     conf.set(NUM_UPDATE_THREADS, String.valueOf(numUpdateThreads));
1:131eb4a:     conf.set(MAX_ITERATIONS_PER_DOC, String.valueOf(maxItersPerDoc));
1:4fbfbc6:     conf.set(MODEL_WEIGHT, "1"); // TODO
1:131eb4a:     conf.set(TEST_SET_FRACTION, String.valueOf(testFraction));
1:131eb4a: 
1:85f9ece:     List<Double> perplexities = new ArrayList<>();
1:131eb4a:     for (int i = 1; i <= iterationNumber; i++) {
1:131eb4a:       // form path to model
1:131eb4a:       Path modelPath = modelPath(topicModelStateTempPath, i);
1:131eb4a: 
1:131eb4a:       // read perplexity
1:131eb4a:       double perplexity = readPerplexity(conf, topicModelStateTempPath, i);
1:131eb4a:       if (Double.isNaN(perplexity)) {
1:131eb4a:         if (!(backfillPerplexity && i % iterationBlockSize == 0)) {
1:131eb4a:           continue;
1:131eb4a:         }
1:131eb4a:         log.info("Backfilling perplexity at iteration {}", i);
1:131eb4a:         if (!fs.exists(modelPath)) {
1:4841efb:           log.error("Model path '{}' does not exist; Skipping iteration {} perplexity calculation",
1:4841efb:               modelPath.toString(), i);
1:131eb4a:           continue;
1:131eb4a:         }
1:131eb4a:         perplexity = calculatePerplexity(conf, inputPath, modelPath, i);
1:131eb4a:       }
1:131eb4a: 
1:131eb4a:       // register and log perplexity
1:131eb4a:       perplexities.add(perplexity);
1:131eb4a:       log.info("Perplexity at iteration {} = {}", i, perplexity);
1:131eb4a:     }
1:131eb4a: 
1:131eb4a:     long startTime = System.currentTimeMillis();
1:e64dd36:     while (iterationNumber < maxIterations) {
1:131eb4a:       // test convergence
1:4fbfbc6:       if (convergenceDelta > 0.0) {
1:4fbfbc6:         double delta = rateOfChange(perplexities);
1:131eb4a:         if (delta < convergenceDelta) {
1:131eb4a:           log.info("Convergence achieved at iteration {} with perplexity {} and delta {}",
1:8396a27:                    iterationNumber, perplexities.get(perplexities.size() - 1), delta);
1:131eb4a:           break;
1:131eb4a:         }
1:131eb4a:       }
1:131eb4a: 
1:131eb4a:       // update model
1:131eb4a:       iterationNumber++;
1:131eb4a:       log.info("About to run iteration {} of {}", iterationNumber, maxIterations);
1:131eb4a:       Path modelInputPath = modelPath(topicModelStateTempPath, iterationNumber - 1);
1:131eb4a:       Path modelOutputPath = modelPath(topicModelStateTempPath, iterationNumber);
1:131eb4a:       runIteration(conf, inputPath, modelInputPath, modelOutputPath, iterationNumber,
1:131eb4a:           maxIterations, numReduceTasks);
1:131eb4a: 
1:131eb4a:       // calculate perplexity
1:e64dd36:       if (testFraction > 0 && iterationNumber % iterationBlockSize == 0) {
1:131eb4a:         perplexities.add(calculatePerplexity(conf, inputPath, modelOutputPath, iterationNumber));
1:131eb4a:         log.info("Current perplexity = {}", perplexities.get(perplexities.size() - 1));
1:4841efb:         log.info("(p_{} - p_{}) / p_0 = {}; target = {}", iterationNumber, iterationNumber - iterationBlockSize,
1:4841efb:             rateOfChange(perplexities), convergenceDelta);
1:131eb4a:       }
1:131eb4a:     }
1:131eb4a:     log.info("Completed {} iterations in {} seconds", iterationNumber,
1:229aeff:         (System.currentTimeMillis() - startTime) / 1000);
1:131eb4a:     log.info("Perplexities: ({})", Joiner.on(", ").join(perplexities));
1:131eb4a: 
1:131eb4a:     // write final topic-term and doc-topic distributions
1:131eb4a:     Path finalIterationData = modelPath(topicModelStateTempPath, iterationNumber);
1:131eb4a:     Job topicModelOutputJob = topicModelOutputPath != null
1:131eb4a:         ? writeTopicModel(conf, finalIterationData, topicModelOutputPath)
1:131eb4a:         : null;
1:131eb4a:     Job docInferenceJob = docTopicOutputPath != null
1:131eb4a:         ? writeDocTopicInference(conf, inputPath, finalIterationData, docTopicOutputPath)
1:131eb4a:         : null;
1:e64dd36:     if (topicModelOutputJob != null && !topicModelOutputJob.waitForCompletion(true)) {
1:131eb4a:       return -1;
1:131eb4a:     }
1:e64dd36:     if (docInferenceJob != null && !docInferenceJob.waitForCompletion(true)) {
1:131eb4a:       return -1;
1:131eb4a:     }
1:131eb4a:     return 0;
1:131eb4a:   }
1:131eb4a: 
1:4fbfbc6:   private static double rateOfChange(List<Double> perplexities) {
1:131eb4a:     int sz = perplexities.size();
1:e64dd36:     if (sz < 2) {
1:131eb4a:       return Double.MAX_VALUE;
1:131eb4a:     }
1:131eb4a:     return Math.abs(perplexities.get(sz - 1) - perplexities.get(sz - 2)) / perplexities.get(0);
1:131eb4a:   }
1:131eb4a: 
1:4aed7cc:   private double calculatePerplexity(Configuration conf, Path corpusPath, Path modelPath, int iteration)
1:229aeff:     throws IOException, ClassNotFoundException, InterruptedException {
1:131eb4a:     String jobName = "Calculating perplexity for " + modelPath;
1:10c535c:     log.info("About to run: {}", jobName);
1:4aed7cc: 
1:131eb4a:     Path outputPath = perplexityPath(modelPath.getParent(), iteration);
1:4aed7cc:     Job job = prepareJob(corpusPath, outputPath, CachingCVB0PerplexityMapper.class, DoubleWritable.class,
1:4aed7cc:         DoubleWritable.class, DualDoubleSumReducer.class, DoubleWritable.class, DoubleWritable.class);
1:4aed7cc: 
1:4aed7cc:     job.setJobName(jobName);
1:4aed7cc:     job.setCombinerClass(DualDoubleSumReducer.class);
1:4aed7cc:     job.setNumReduceTasks(1);
1:131eb4a:     setModelPaths(job, modelPath);
1:131eb4a:     HadoopUtil.delete(conf, outputPath);
1:e64dd36:     if (!job.waitForCompletion(true)) {
1:131eb4a:       throw new InterruptedException("Failed to calculate perplexity for: " + modelPath);
1:131eb4a:     }
1:131eb4a:     return readPerplexity(conf, modelPath.getParent(), iteration);
1:131eb4a:   }
1:131eb4a: 
1:131eb4a:   /**
1:131eb4a:    * Sums keys and values independently.
1:131eb4a:    */
1:131eb4a:   public static class DualDoubleSumReducer extends
1:131eb4a:     Reducer<DoubleWritable, DoubleWritable, DoubleWritable, DoubleWritable> {
1:131eb4a:     private final DoubleWritable outKey = new DoubleWritable();
1:131eb4a:     private final DoubleWritable outValue = new DoubleWritable();
1:131eb4a: 
1:131eb4a:     @Override
1:131eb4a:     public void run(Context context) throws IOException,
1:131eb4a:         InterruptedException {
1:4fbfbc6:       double keySum = 0.0;
1:4fbfbc6:       double valueSum = 0.0;
1:131eb4a:       while (context.nextKey()) {
1:131eb4a:         keySum += context.getCurrentKey().get();
1:131eb4a:         for (DoubleWritable value : context.getValues()) {
1:131eb4a:           valueSum += value.get();
1:131eb4a:         }
1:131eb4a:       }
1:131eb4a:       outKey.set(keySum);
1:131eb4a:       outValue.set(valueSum);
1:131eb4a:       context.write(outKey, outValue);
1:131eb4a:     }
1:131eb4a:   }
1:131eb4a: 
1:131eb4a:   /**
1:131eb4a:    * @param topicModelStateTemp
1:131eb4a:    * @param iteration
1:131eb4a:    * @return {@code double[2]} where first value is perplexity and second is model weight of those
1:131eb4a:    *         documents sampled during perplexity computation, or {@code null} if no perplexity data
1:131eb4a:    *         exists for the given iteration.
1:131eb4a:    * @throws IOException
1:131eb4a:    */
1:131eb4a:   public static double readPerplexity(Configuration conf, Path topicModelStateTemp, int iteration)
1:131eb4a:     throws IOException {
1:131eb4a:     Path perplexityPath = perplexityPath(topicModelStateTemp, iteration);
1:1de8cec:     FileSystem fs = FileSystem.get(perplexityPath.toUri(), conf);
1:131eb4a:     if (!fs.exists(perplexityPath)) {
1:131eb4a:       log.warn("Perplexity path {} does not exist, returning NaN", perplexityPath);
1:131eb4a:       return Double.NaN;
1:131eb4a:     }
1:131eb4a:     double perplexity = 0;
1:131eb4a:     double modelWeight = 0;
1:131eb4a:     long n = 0;
1:131eb4a:     for (Pair<DoubleWritable, DoubleWritable> pair : new SequenceFileDirIterable<DoubleWritable, DoubleWritable>(
1:131eb4a:         perplexityPath, PathType.LIST, PathFilters.partFilter(), null, true, conf)) {
1:131eb4a:       modelWeight += pair.getFirst().get();
1:131eb4a:       perplexity += pair.getSecond().get();
1:131eb4a:       n++;
1:131eb4a:     }
1:8396a27:     log.info("Read {} entries with total perplexity {} and model weight {}", n,
1:8396a27:              perplexity, modelWeight);
1:131eb4a:     return perplexity / modelWeight;
1:131eb4a:   }
1:131eb4a: 
1:4aed7cc:   private Job writeTopicModel(Configuration conf, Path modelInput, Path output)
1:229aeff:     throws IOException, InterruptedException, ClassNotFoundException {
1:229aeff:     String jobName = String.format("Writing final topic/term distributions from %s to %s", modelInput, output);
1:10c535c:     log.info("About to run: {}", jobName);
1:4aed7cc: 
1:4aed7cc:     Job job = prepareJob(modelInput, output, SequenceFileInputFormat.class, CVB0TopicTermVectorNormalizerMapper.class,
1:4aed7cc:         IntWritable.class, VectorWritable.class, SequenceFileOutputFormat.class, jobName);
1:131eb4a:     job.submit();
1:131eb4a:     return job;
1:131eb4a:   }
1:131eb4a: 
1:4aed7cc:   private Job writeDocTopicInference(Configuration conf, Path corpus, Path modelInput, Path output)
1:131eb4a:     throws IOException, ClassNotFoundException, InterruptedException {
1:229aeff:     String jobName = String.format("Writing final document/topic inference from %s to %s", corpus, output);
1:10c535c:     log.info("About to run: {}", jobName);
1:4aed7cc: 
1:7ee9154:     Job job = prepareJob(corpus, output, SequenceFileInputFormat.class, CVB0DocInferenceMapper.class,
1:4aed7cc:         IntWritable.class, VectorWritable.class, SequenceFileOutputFormat.class, jobName);
1:4aed7cc: 
1:1de8cec:     FileSystem fs = FileSystem.get(corpus.toUri(), conf);
1:1de8cec:     if (modelInput != null && fs.exists(modelInput)) {
1:1de8cec:       FileStatus[] statuses = fs.listStatus(modelInput, PathFilters.partFilter());
1:131eb4a:       URI[] modelUris = new URI[statuses.length];
1:e64dd36:       for (int i = 0; i < statuses.length; i++) {
1:131eb4a:         modelUris[i] = statuses[i].getPath().toUri();
1:131eb4a:       }
1:131eb4a:       DistributedCache.setCacheFiles(modelUris, conf);
1:1c16298:       setModelPaths(job, modelInput);
1:131eb4a:     }
1:131eb4a:     job.submit();
1:131eb4a:     return job;
1:131eb4a:   }
1:131eb4a: 
1:131eb4a:   public static Path modelPath(Path topicModelStateTempPath, int iterationNumber) {
1:131eb4a:     return new Path(topicModelStateTempPath, "model-" + iterationNumber);
1:131eb4a:   }
1:131eb4a: 
1:131eb4a:   public static Path perplexityPath(Path topicModelStateTempPath, int iterationNumber) {
1:131eb4a:     return new Path(topicModelStateTempPath, "perplexity-" + iterationNumber);
1:131eb4a:   }
1:131eb4a: 
1:4fbfbc6:   private static int getCurrentIterationNumber(Configuration config, Path modelTempDir, int maxIterations)
1:131eb4a:     throws IOException {
1:1de8cec:     FileSystem fs = FileSystem.get(modelTempDir.toUri(), config);
1:131eb4a:     int iterationNumber = 1;
1:131eb4a:     Path iterationPath = modelPath(modelTempDir, iterationNumber);
1:e64dd36:     while (fs.exists(iterationPath) && iterationNumber <= maxIterations) {
1:10c535c:       log.info("Found previous state: {}", iterationPath);
1:131eb4a:       iterationNumber++;
1:131eb4a:       iterationPath = modelPath(modelTempDir, iterationNumber);
1:131eb4a:     }
1:131eb4a:     return iterationNumber - 1;
1:131eb4a:   }
1:131eb4a: 
1:4aed7cc:   public void runIteration(Configuration conf, Path corpusInput, Path modelInput, Path modelOutput,
1:4aed7cc:                            int iterationNumber, int maxIterations, int numReduceTasks)
1:229aeff:     throws IOException, ClassNotFoundException, InterruptedException {
1:131eb4a:     String jobName = String.format("Iteration %d of %d, input path: %s",
1:131eb4a:         iterationNumber, maxIterations, modelInput);
1:10c535c:     log.info("About to run: {}", jobName);
1:4aed7cc:     Job job = prepareJob(corpusInput, modelOutput, CachingCVB0Mapper.class, IntWritable.class, VectorWritable.class,
1:4aed7cc:         VectorSumReducer.class, IntWritable.class, VectorWritable.class);
1:131eb4a:     job.setCombinerClass(VectorSumReducer.class);
1:131eb4a:     job.setNumReduceTasks(numReduceTasks);
1:4aed7cc:     job.setJobName(jobName);
1:131eb4a:     setModelPaths(job, modelInput);
1:131eb4a:     HadoopUtil.delete(conf, modelOutput);
1:e64dd36:     if (!job.waitForCompletion(true)) {
1:131eb4a:       throw new InterruptedException(String.format("Failed to complete iteration %d stage 1",
1:131eb4a:           iterationNumber));
1:131eb4a:     }
1:131eb4a:   }
1:131eb4a: 
1:4fbfbc6:   private static void setModelPaths(Job job, Path modelPath) throws IOException {
1:131eb4a:     Configuration conf = job.getConfiguration();
1:1de8cec:     if (modelPath == null || !FileSystem.get(modelPath.toUri(), conf).exists(modelPath)) {
1:131eb4a:       return;
1:131eb4a:     }
1:1de8cec:     FileStatus[] statuses = FileSystem.get(modelPath.toUri(), conf).listStatus(modelPath, PathFilters.partFilter());
1:131eb4a:     Preconditions.checkState(statuses.length > 0, "No part files found in model path '%s'", modelPath.toString());
1:131eb4a:     String[] modelPaths = new String[statuses.length];
1:131eb4a:     for (int i = 0; i < statuses.length; i++) {
1:131eb4a:       modelPaths[i] = statuses[i].getPath().toUri().toString();
1:131eb4a:     }
1:131eb4a:     conf.setStrings(MODEL_PATHS, modelPaths);
1:131eb4a:   }
1:131eb4a: 
1:131eb4a:   public static Path[] getModelPaths(Configuration conf) {
1:131eb4a:     String[] modelPathNames = conf.getStrings(MODEL_PATHS);
1:131eb4a:     if (modelPathNames == null || modelPathNames.length == 0) {
1:131eb4a:       return null;
1:131eb4a:     }
1:131eb4a:     Path[] modelPaths = new Path[modelPathNames.length];
1:131eb4a:     for (int i = 0; i < modelPathNames.length; i++) {
1:131eb4a:       modelPaths[i] = new Path(modelPathNames[i]);
1:131eb4a:     }
1:131eb4a:     return modelPaths;
1:131eb4a:   }
1:131eb4a: 
1:131eb4a:   public static void main(String[] args) throws Exception {
1:131eb4a:     ToolRunner.run(new Configuration(), new CVB0Driver(), args);
1:131eb4a:   }
1:131eb4a: }
============================================================================
author:Suneel Marthi
-------------------------------------------------------------------------------
commit:85f9ece
/////////////////////////////////////////////////////////////////////////
1: import java.io.IOException;
1: import java.net.URI;
1: import java.util.ArrayList;
1: import java.util.List;
1: 
/////////////////////////////////////////////////////////////////////////
/////////////////////////////////////////////////////////////////////////
1:     List<Double> perplexities = new ArrayList<>();
author:pferrel
-------------------------------------------------------------------------------
commit:b988c49
author:frankscholten
-------------------------------------------------------------------------------
commit:1a42d85
author:Sebastian Schelter
-------------------------------------------------------------------------------
commit:7ee9154
/////////////////////////////////////////////////////////////////////////
1:     Job job = prepareJob(corpus, output, SequenceFileInputFormat.class, CVB0DocInferenceMapper.class,
commit:4aed7cc
/////////////////////////////////////////////////////////////////////////
/////////////////////////////////////////////////////////////////////////
1:   private static final double DEFAULT_CONVERGENCE_DELTA = 0;
1:   private static final double DEFAULT_DOC_TOPIC_SMOOTHING = 0.0001;
1:   private static final double DEFAULT_TERM_TOPIC_SMOOTHING = 0.0001;
1:   private static final int DEFAULT_ITERATION_BLOCK_SIZE = 10;
1:   private static final double DEFAULT_TEST_SET_FRACTION = 0;
1:   private static final int DEFAULT_NUM_TRAIN_THREADS = 4;
1:   private static final int DEFAULT_NUM_UPDATE_THREADS = 1;
1:   private static final int DEFAULT_MAX_ITERATIONS_PER_DOC = 10;
1:   private static final int DEFAULT_NUM_REDUCE_TASKS = 10;
1: 
1:     addOption(DefaultOptionCreator.CONVERGENCE_DELTA_OPTION, "cd", "The convergence delta value",
1:               String.valueOf(DEFAULT_CONVERGENCE_DELTA));
1:     addOption(DOC_TOPIC_SMOOTHING, "a", "Smoothing for document/topic distribution",
1:               String.valueOf(DEFAULT_DOC_TOPIC_SMOOTHING));
1:     addOption(TERM_TOPIC_SMOOTHING, "e", "Smoothing for topic/term distribution",
1:               String.valueOf(DEFAULT_TERM_TOPIC_SMOOTHING));
1:     addOption(DICTIONARY, "dict", "Path to term-dictionary file(s) (glob expression supported)", false);
1:     addOption(DOC_TOPIC_OUTPUT, "dt", "Output path for the training doc/topic distribution", false);
1:     addOption(MODEL_TEMP_DIR, "mt", "Path to intermediate model path (useful for restarting)", false);
1:     addOption(ITERATION_BLOCK_SIZE, "block", "Number of iterations per perplexity check",
1:               String.valueOf(DEFAULT_ITERATION_BLOCK_SIZE));
1:     addOption(TEST_SET_FRACTION, "tf", "Fraction of data to hold out for testing",
1:               String.valueOf(DEFAULT_TEST_SET_FRACTION));
1:     addOption(NUM_TRAIN_THREADS, "ntt", "number of threads per mapper to train with",
1:               String.valueOf(DEFAULT_NUM_TRAIN_THREADS));
1:               String.valueOf(DEFAULT_NUM_UPDATE_THREADS));
1:     addOption(MAX_ITERATIONS_PER_DOC, "mipd", "max number of iterations per doc for p(topic|doc) learning",
1:               String.valueOf(DEFAULT_MAX_ITERATIONS_PER_DOC));
1:     addOption(NUM_REDUCE_TASKS, null, "number of reducers to use during model estimation",
1:               String.valueOf(DEFAULT_NUM_REDUCE_TASKS));
1:     addOption(buildOption(BACKFILL_PERPLEXITY, null, "enable backfilling of missing perplexity values", false, false,
1:               null));
/////////////////////////////////////////////////////////////////////////
1:   public int run(Configuration conf,
1:                  Path inputPath,
1:                  Path topicModelOutputPath,
1:                  int numTopics,
1:                  int numTerms,
1:                  double alpha,
1:                  double eta,
1:                  int maxIterations,
1:                  int iterationBlockSize,
1:                  double convergenceDelta,
1:                  Path dictionaryPath,
1:                  Path docTopicOutputPath,
1:                  Path topicModelStateTempPath,
1:                  long randomSeed,
1:                  float testFraction,
1:                  int numTrainThreads,
1:                  int numUpdateThreads,
1:                  int maxItersPerDoc,
1:                  int numReduceTasks,
1:                  boolean backfillPerplexity)
1: 
1:     setConf(conf);
1: 
/////////////////////////////////////////////////////////////////////////
1:   private double calculatePerplexity(Configuration conf, Path corpusPath, Path modelPath, int iteration)
1: 
1:     Job job = prepareJob(corpusPath, outputPath, CachingCVB0PerplexityMapper.class, DoubleWritable.class,
1:         DoubleWritable.class, DualDoubleSumReducer.class, DoubleWritable.class, DoubleWritable.class);
1: 
1:     job.setJobName(jobName);
1:     job.setCombinerClass(DualDoubleSumReducer.class);
1:     job.setNumReduceTasks(1);
/////////////////////////////////////////////////////////////////////////
1:   private Job writeTopicModel(Configuration conf, Path modelInput, Path output)
1: 
1:     Job job = prepareJob(modelInput, output, SequenceFileInputFormat.class, CVB0TopicTermVectorNormalizerMapper.class,
1:         IntWritable.class, VectorWritable.class, SequenceFileOutputFormat.class, jobName);
1:   private Job writeDocTopicInference(Configuration conf, Path corpus, Path modelInput, Path output)
1: 
0:     Job job = prepareJob(corpus, outputPath, SequenceFileInputFormat.class, CVB0DocInferenceMapper.class,
1:         IntWritable.class, VectorWritable.class, SequenceFileOutputFormat.class, jobName);
1: 
/////////////////////////////////////////////////////////////////////////
/////////////////////////////////////////////////////////////////////////
/////////////////////////////////////////////////////////////////////////
1:   public void runIteration(Configuration conf, Path corpusInput, Path modelInput, Path modelOutput,
1:                            int iterationNumber, int maxIterations, int numReduceTasks)
1:     Job job = prepareJob(corpusInput, modelOutput, CachingCVB0Mapper.class, IntWritable.class, VectorWritable.class,
1:         VectorSumReducer.class, IntWritable.class, VectorWritable.class);
1:     job.setJobName(jobName);
commit:4841efb
/////////////////////////////////////////////////////////////////////////
1:           log.error("Model path '{}' does not exist; Skipping iteration {} perplexity calculation",
1:               modelPath.toString(), i);
/////////////////////////////////////////////////////////////////////////
1:         log.info("(p_{} - p_{}) / p_0 = {}; target = {}", iterationNumber, iterationNumber - iterationBlockSize,
1:             rateOfChange(perplexities), convergenceDelta);
author:Jacob Alexander Mannix
-------------------------------------------------------------------------------
commit:1c16298
/////////////////////////////////////////////////////////////////////////
1:       setModelPaths(job, modelInput);
commit:131eb4a
/////////////////////////////////////////////////////////////////////////
1: /**
1:  * Licensed to the Apache Software Foundation (ASF) under one or more
1:  * contributor license agreements.  See the NOTICE file distributed with
1:  * this work for additional information regarding copyright ownership.
1:  * The ASF licenses this file to You under the Apache License, Version 2.0
1:  * (the "License"); you may not use this file except in compliance with
1:  * the License.  You may obtain a copy of the License at
1:  *
1:  *     http://www.apache.org/licenses/LICENSE-2.0
1:  *
1:  * Unless required by applicable law or agreed to in writing, software
1:  * distributed under the License is distributed on an "AS IS" BASIS,
1:  * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
1:  * See the License for the specific language governing permissions and
1:  * limitations under the License.
1:  */
1: package org.apache.mahout.clustering.lda.cvb;
1: 
1: import com.google.common.base.Joiner;
1: import com.google.common.base.Preconditions;
0: import com.google.common.collect.Lists;
1: import org.apache.hadoop.conf.Configuration;
1: import org.apache.hadoop.filecache.DistributedCache;
1: import org.apache.hadoop.fs.FileStatus;
1: import org.apache.hadoop.fs.FileSystem;
1: import org.apache.hadoop.fs.Path;
1: import org.apache.hadoop.io.DoubleWritable;
1: import org.apache.hadoop.io.IntWritable;
1: import org.apache.hadoop.io.SequenceFile;
1: import org.apache.hadoop.io.Text;
1: import org.apache.hadoop.mapreduce.Job;
1: import org.apache.hadoop.mapreduce.Reducer;
0: import org.apache.hadoop.mapreduce.lib.input.FileInputFormat;
1: import org.apache.hadoop.mapreduce.lib.input.SequenceFileInputFormat;
0: import org.apache.hadoop.mapreduce.lib.output.FileOutputFormat;
1: import org.apache.hadoop.mapreduce.lib.output.SequenceFileOutputFormat;
1: import org.apache.hadoop.util.ToolRunner;
1: import org.apache.mahout.common.AbstractJob;
1: import org.apache.mahout.common.HadoopUtil;
1: import org.apache.mahout.common.Pair;
1: import org.apache.mahout.common.commandline.DefaultOptionCreator;
1: import org.apache.mahout.common.iterator.sequencefile.PathFilters;
1: import org.apache.mahout.common.iterator.sequencefile.PathType;
1: import org.apache.mahout.common.iterator.sequencefile.SequenceFileDirIterable;
1: import org.apache.mahout.common.mapreduce.VectorSumReducer;
1: import org.apache.mahout.math.VectorWritable;
0: import org.apache.mahout.vectorizer.SparseVectorsFromSequenceFiles;
1: import org.slf4j.Logger;
1: import org.slf4j.LoggerFactory;
1: 
0: import java.io.IOException;
0: import java.net.URI;
0: import java.util.List;
1: 
1: /**
1:  * See {@link CachingCVB0Mapper} for more details on scalability and room for improvement.
1:  * To try out this LDA implementation without using Hadoop, check out
1:  * {@link InMemoryCollapsedVariationalBayes0}.  If you want to do training directly in java code
1:  * with your own main(), then look to {@link ModelTrainer} and {@link TopicModel}.
1:  *
0:  * Usage: <code>./bin/mahout cvb <i>options</i></code>
1:  * <p>
1:  * Valid options include:
1:  * <dl>
1:  * <dt>{@code --input path}</td>
1:  * <dd>Input path for {@code SequenceFile<IntWritable, VectorWritable>} document vectors. See
0:  * {@link SparseVectorsFromSequenceFiles} for details on how to generate this input format.</dd>
1:  * <dt>{@code --dictionary path}</dt>
1:  * <dd>Path to dictionary file(s) generated during construction of input document vectors (glob
1:  * expression supported). If set, this data is scanned to determine an appropriate value for option
1:  * {@code --num_terms}.</dd>
1:  * <dt>{@code --output path}</dt>
1:  * <dd>Output path for topic-term distributions.</dd>
1:  * <dt>{@code --doc_topic_output path}</dt>
1:  * <dd>Output path for doc-topic distributions.</dd>
1:  * <dt>{@code --num_topics k}</dt>
1:  * <dd>Number of latent topics.</dd>
1:  * <dt>{@code --num_terms nt}</dt>
1:  * <dd>Number of unique features defined by input document vectors. If option {@code --dictionary}
1:  * is defined and this option is unspecified, term count is calculated from dictionary.</dd>
1:  * <dt>{@code --topic_model_temp_dir path}</dt>
1:  * <dd>Path in which to store model state after each iteration.</dd>
1:  * <dt>{@code --maxIter i}</dt>
1:  * <dd>Maximum number of iterations to perform. If this value is less than or equal to the number of
1:  * iteration states found beneath the path specified by option {@code --topic_model_temp_dir}, no
1:  * further iterations are performed. Instead, output topic-term and doc-topic distributions are
1:  * generated using data from the specified iteration.</dd>
1:  * <dt>{@code --max_doc_topic_iters i}</dt>
1:  * <dd>Maximum number of iterations per doc for p(topic|doc) learning. Defaults to {@code 10}.</dd>
1:  * <dt>{@code --doc_topic_smoothing a}</dt>
1:  * <dd>Smoothing for doc-topic distribution. Defaults to {@code 0.0001}.</dd>
1:  * <dt>{@code --term_topic_smoothing e}</dt>
1:  * <dd>Smoothing for topic-term distribution. Defaults to {@code 0.0001}.</dd>
1:  * <dt>{@code --random_seed seed}</dt>
1:  * <dd>Integer seed for random number generation.</dd>
1:  * <dt>{@code --test_set_percentage p}</dt>
1:  * <dd>Fraction of data to hold out for testing. Defaults to {@code 0.0}.</dd>
1:  * <dt>{@code --iteration_block_size block}</dt>
1:  * <dd>Number of iterations between perplexity checks. Defaults to {@code 10}. This option is
1:  * ignored unless option {@code --test_set_percentage} is greater than zero.</dd>
1:  * </dl>
1:  */
1: public class CVB0Driver extends AbstractJob {
1:   private static final Logger log = LoggerFactory.getLogger(CVB0Driver.class);
1: 
1:   public static final String NUM_TOPICS = "num_topics";
1:   public static final String NUM_TERMS = "num_terms";
1:   public static final String DOC_TOPIC_SMOOTHING = "doc_topic_smoothing";
1:   public static final String TERM_TOPIC_SMOOTHING = "term_topic_smoothing";
1:   public static final String DICTIONARY = "dictionary";
1:   public static final String DOC_TOPIC_OUTPUT = "doc_topic_output";
1:   public static final String MODEL_TEMP_DIR = "topic_model_temp_dir";
1:   public static final String ITERATION_BLOCK_SIZE = "iteration_block_size";
1:   public static final String RANDOM_SEED = "random_seed";
1:   public static final String TEST_SET_FRACTION = "test_set_fraction";
1:   public static final String NUM_TRAIN_THREADS = "num_train_threads";
1:   public static final String NUM_UPDATE_THREADS = "num_update_threads";
1:   public static final String MAX_ITERATIONS_PER_DOC = "max_doc_topic_iters";
1:   public static final String MODEL_WEIGHT = "prev_iter_mult";
1:   public static final String NUM_REDUCE_TASKS = "num_reduce_tasks";
1:   public static final String BACKFILL_PERPLEXITY = "backfill_perplexity";
1:   private static final String MODEL_PATHS = "mahout.lda.cvb.modelPath";
1: 
1:   @Override
1:   public int run(String[] args) throws Exception {
1:     addInputOption();
1:     addOutputOption();
1:     addOption(DefaultOptionCreator.maxIterationsOption().create());
0:     addOption(DefaultOptionCreator.CONVERGENCE_DELTA_OPTION, "cd", "The convergence delta value", "0");
1:     addOption(DefaultOptionCreator.overwriteOption().create());
1: 
1:     addOption(NUM_TOPICS, "k", "Number of topics to learn", true);
1:     addOption(NUM_TERMS, "nt", "Vocabulary size", false);
0:     addOption(DOC_TOPIC_SMOOTHING, "a", "Smoothing for document/topic distribution", "0.0001");
0:     addOption(TERM_TOPIC_SMOOTHING, "e", "Smoothing for topic/term distribution", "0.0001");
0:     addOption(DICTIONARY, "dict", "Path to term-dictionary file(s) (glob expression supported)",
0:         false);
0:     addOption(DOC_TOPIC_OUTPUT, "dt", "Output path for the training doc/topic distribution",
0:         false);
0:     addOption(MODEL_TEMP_DIR, "mt", "Path to intermediate model path (useful for restarting)",
0:         false);
0:     addOption(ITERATION_BLOCK_SIZE, "block", "Number of iterations per perplexity check", "10");
1:     addOption(RANDOM_SEED, "seed", "Random seed", false);
0:     addOption(TEST_SET_FRACTION, "tf", "Fraction of data to hold out for testing", "0");
0:     addOption(NUM_TRAIN_THREADS, "ntt", "number of threads per mapper to train with", "4");
1:     addOption(NUM_UPDATE_THREADS, "nut", "number of threads per mapper to update the model with",
0:         "1");
0:     addOption(MAX_ITERATIONS_PER_DOC, "mipd",
0:         "max number of iterations per doc for p(topic|doc) learning", "10");
0:     addOption(NUM_REDUCE_TASKS, null,
0:         "number of reducers to use during model estimation", "10");
0:     addOption(buildOption(BACKFILL_PERPLEXITY, null,
0:         "enable backfilling of missing perplexity values", false, false, null));
1: 
0:     if(parseArguments(args) == null) {
1:       return -1;
1:     }
1: 
1:     int numTopics = Integer.parseInt(getOption(NUM_TOPICS));
1:     Path inputPath = getInputPath();
1:     Path topicModelOutputPath = getOutputPath();
1:     int maxIterations = Integer.parseInt(getOption(DefaultOptionCreator.MAX_ITERATIONS_OPTION));
1:     int iterationBlockSize = Integer.parseInt(getOption(ITERATION_BLOCK_SIZE));
1:     double convergenceDelta = Double.parseDouble(getOption(DefaultOptionCreator.CONVERGENCE_DELTA_OPTION));
1:     double alpha = Double.parseDouble(getOption(DOC_TOPIC_SMOOTHING));
1:     double eta = Double.parseDouble(getOption(TERM_TOPIC_SMOOTHING));
1:     int numTrainThreads = Integer.parseInt(getOption(NUM_TRAIN_THREADS));
1:     int numUpdateThreads = Integer.parseInt(getOption(NUM_UPDATE_THREADS));
1:     int maxItersPerDoc = Integer.parseInt(getOption(MAX_ITERATIONS_PER_DOC));
1:     Path dictionaryPath = hasOption(DICTIONARY) ? new Path(getOption(DICTIONARY)) : null;
1:     int numTerms = hasOption(NUM_TERMS)
1:                  ? Integer.parseInt(getOption(NUM_TERMS))
1:                  : getNumTerms(getConf(), dictionaryPath);
1:     Path docTopicOutputPath = hasOption(DOC_TOPIC_OUTPUT) ? new Path(getOption(DOC_TOPIC_OUTPUT)) : null;
1:     Path modelTempPath = hasOption(MODEL_TEMP_DIR)
1:                        ? new Path(getOption(MODEL_TEMP_DIR))
1:                        : getTempPath("topicModelState");
1:     long seed = hasOption(RANDOM_SEED)
1:               ? Long.parseLong(getOption(RANDOM_SEED))
1:               : System.nanoTime() % 10000;
1:     float testFraction = hasOption(TEST_SET_FRACTION)
1:                        ? Float.parseFloat(getOption(TEST_SET_FRACTION))
0:                        : 0f;
1:     int numReduceTasks = Integer.parseInt(getOption(NUM_REDUCE_TASKS));
1:     boolean backfillPerplexity = hasOption(BACKFILL_PERPLEXITY);
1: 
1:     return run(getConf(), inputPath, topicModelOutputPath, numTopics, numTerms, alpha, eta,
1:         maxIterations, iterationBlockSize, convergenceDelta, dictionaryPath, docTopicOutputPath,
1:         modelTempPath, seed, testFraction, numTrainThreads, numUpdateThreads, maxItersPerDoc,
1:         numReduceTasks, backfillPerplexity);
1:   }
1: 
0:   private int getNumTerms(Configuration conf, Path dictionaryPath) throws IOException {
1:     FileSystem fs = dictionaryPath.getFileSystem(conf);
1:     Text key = new Text();
1:     IntWritable value = new IntWritable();
1:     int maxTermId = -1;
1:     for (FileStatus stat : fs.globStatus(dictionaryPath)) {
1:       SequenceFile.Reader reader = new SequenceFile.Reader(fs, stat.getPath(), conf);
1:       while (reader.next(key, value)) {
1:         maxTermId = Math.max(maxTermId, value.get());
1:       }
1:     }
1:     return maxTermId + 1;
1:   }
1: 
0:   public int run(Configuration conf, Path inputPath, Path topicModelOutputPath, int numTopics,
0:       int numTerms, double alpha, double eta, int maxIterations, int iterationBlockSize,
0:       double convergenceDelta, Path dictionaryPath, Path docTopicOutputPath,
0:       Path topicModelStateTempPath, long randomSeed, float testFraction, int numTrainThreads,
0:       int numUpdateThreads, int maxItersPerDoc, int numReduceTasks, boolean backfillPerplexity)
1:       throws ClassNotFoundException, IOException, InterruptedException {
1:     // verify arguments
0:     Preconditions.checkArgument(0 <= testFraction && 1.0 >= testFraction,
1:         "Expected 'testFraction' value in range [0, 1] but found value '%s'", testFraction);
0:     Preconditions.checkArgument(!backfillPerplexity || 0 < testFraction,
1:         "Expected 'testFraction' value in range (0, 1] but found value '%s'", testFraction);
1: 
0:     String infoString = "Will run Collapsed Variational Bayes (0th-derivative approximation) " +
0:       "learning for LDA on {} (numTerms: {}), finding {}-topics, with document/topic prior {}, " +
0:       "topic/term prior {}.  Maximum iterations to run will be {}, unless the change in " +
0:       "perplexity is less than {}.  Topic model output (p(term|topic) for each topic) will be " +
0:       "stored {}.  Random initialization seed is {}, holding out {} of the data for perplexity " +
0:       "check\n";
0:     log.info(infoString, new Object[] {inputPath, numTerms, numTopics, alpha, eta, maxIterations,
0:         convergenceDelta, topicModelOutputPath, randomSeed, testFraction});
1:     infoString = dictionaryPath == null
0:                ? "" : "Dictionary to be used located " + dictionaryPath.toString() + "\n";
1:     infoString += docTopicOutputPath == null
0:                ? "" : "p(topic|docId) will be stored " + docTopicOutputPath.toString() + "\n";
1:     log.info(infoString);
1: 
0:     FileSystem fs = FileSystem.get(conf);
1:     int iterationNumber = getCurrentIterationNumber(conf, topicModelStateTempPath, maxIterations);
1:     log.info("Current iteration number: {}", iterationNumber);
1: 
1:     conf.set(NUM_TOPICS, String.valueOf(numTopics));
1:     conf.set(NUM_TERMS, String.valueOf(numTerms));
1:     conf.set(DOC_TOPIC_SMOOTHING, String.valueOf(alpha));
1:     conf.set(TERM_TOPIC_SMOOTHING, String.valueOf(eta));
1:     conf.set(RANDOM_SEED, String.valueOf(randomSeed));
1:     conf.set(NUM_TRAIN_THREADS, String.valueOf(numTrainThreads));
1:     conf.set(NUM_UPDATE_THREADS, String.valueOf(numUpdateThreads));
1:     conf.set(MAX_ITERATIONS_PER_DOC, String.valueOf(maxItersPerDoc));
0:     conf.set(MODEL_WEIGHT, String.valueOf(1)); // TODO:
1:     conf.set(TEST_SET_FRACTION, String.valueOf(testFraction));
1: 
0:     List<Double> perplexities = Lists.newArrayList();
1:     for (int i = 1; i <= iterationNumber; i++) {
1:       // form path to model
1:       Path modelPath = modelPath(topicModelStateTempPath, i);
1: 
1:       // read perplexity
1:       double perplexity = readPerplexity(conf, topicModelStateTempPath, i);
1:       if (Double.isNaN(perplexity)) {
1:         if (!(backfillPerplexity && i % iterationBlockSize == 0)) {
1:           continue;
1:         }
1:         log.info("Backfilling perplexity at iteration {}", i);
1:         if (!fs.exists(modelPath)) {
0:           log.error("Model path '{}' does not exist; Skipping iteration {} perplexity calculation", modelPath.toString(), i);
1:           continue;
1:         }
1:         perplexity = calculatePerplexity(conf, inputPath, modelPath, i);
1:       }
1: 
1:       // register and log perplexity
1:       perplexities.add(perplexity);
1:       log.info("Perplexity at iteration {} = {}", i, perplexity);
1:     }
1: 
1:     long startTime = System.currentTimeMillis();
0:     while(iterationNumber < maxIterations) {
1:       // test convergence
0:       if (0 < convergenceDelta) {
0:         final double delta = rateOfChange(perplexities);
1:         if (delta < convergenceDelta) {
1:           log.info("Convergence achieved at iteration {} with perplexity {} and delta {}",
0:               new Object[]{iterationNumber, perplexities.get(perplexities.size() - 1), delta});
1:           break;
1:         }
1:       }
1: 
1:       // update model
1:       iterationNumber++;
1:       log.info("About to run iteration {} of {}", iterationNumber, maxIterations);
1:       Path modelInputPath = modelPath(topicModelStateTempPath, iterationNumber - 1);
1:       Path modelOutputPath = modelPath(topicModelStateTempPath, iterationNumber);
1:       runIteration(conf, inputPath, modelInputPath, modelOutputPath, iterationNumber,
1:           maxIterations, numReduceTasks);
1: 
1:       // calculate perplexity
0:       if(testFraction > 0 && iterationNumber % iterationBlockSize == 0) {
1:         perplexities.add(calculatePerplexity(conf, inputPath, modelOutputPath, iterationNumber));
1:         log.info("Current perplexity = {}", perplexities.get(perplexities.size() - 1));
0:         log.info("(p_{} - p_{}) / p_0 = {}; target = {}", new Object[]{
0:             iterationNumber , iterationNumber - iterationBlockSize, rateOfChange(perplexities), convergenceDelta
0:         });
1:       }
1:     }
1:     log.info("Completed {} iterations in {} seconds", iterationNumber,
0:         (System.currentTimeMillis() - startTime)/1000);
1:     log.info("Perplexities: ({})", Joiner.on(", ").join(perplexities));
1: 
1:     // write final topic-term and doc-topic distributions
1:     Path finalIterationData = modelPath(topicModelStateTempPath, iterationNumber);
1:     Job topicModelOutputJob = topicModelOutputPath != null
1:         ? writeTopicModel(conf, finalIterationData, topicModelOutputPath)
1:         : null;
1:     Job docInferenceJob = docTopicOutputPath != null
1:         ? writeDocTopicInference(conf, inputPath, finalIterationData, docTopicOutputPath)
1:         : null;
0:     if(topicModelOutputJob != null && !topicModelOutputJob.waitForCompletion(true)) {
1:       return -1;
1:     }
0:     if(docInferenceJob != null && !docInferenceJob.waitForCompletion(true)) {
1:       return -1;
1:     }
1:     return 0;
1:   }
1: 
0:   private double rateOfChange(List<Double> perplexities) {
1:     int sz = perplexities.size();
0:     if(sz < 2) {
1:       return Double.MAX_VALUE;
1:     }
1:     return Math.abs(perplexities.get(sz - 1) - perplexities.get(sz - 2)) / perplexities.get(0);
1:   }
1: 
0:   private double calculatePerplexity(Configuration conf, Path corpusPath, Path modelPath, int iteration)
0:       throws IOException,
0:       ClassNotFoundException, InterruptedException {
1:     String jobName = "Calculating perplexity for " + modelPath;
0:     log.info("About to run: " + jobName);
0:     Job job = new Job(conf, jobName);
0:     job.setJarByClass(CachingCVB0PerplexityMapper.class);
0:     job.setMapperClass(CachingCVB0PerplexityMapper.class);
0:     job.setCombinerClass(DualDoubleSumReducer.class);
0:     job.setReducerClass(DualDoubleSumReducer.class);
0:     job.setNumReduceTasks(1);
0:     job.setOutputKeyClass(DoubleWritable.class);
0:     job.setOutputValueClass(DoubleWritable.class);
0:     job.setInputFormatClass(SequenceFileInputFormat.class);
0:     job.setOutputFormatClass(SequenceFileOutputFormat.class);
0:     FileInputFormat.addInputPath(job, corpusPath);
1:     Path outputPath = perplexityPath(modelPath.getParent(), iteration);
0:     FileOutputFormat.setOutputPath(job, outputPath);
1:     setModelPaths(job, modelPath);
1:     HadoopUtil.delete(conf, outputPath);
0:     if(!job.waitForCompletion(true)) {
1:       throw new InterruptedException("Failed to calculate perplexity for: " + modelPath);
1:     }
1:     return readPerplexity(conf, modelPath.getParent(), iteration);
1:   }
1: 
1:   /**
1:    * Sums keys and values independently.
1:    */
1:   public static class DualDoubleSumReducer extends
1:     Reducer<DoubleWritable, DoubleWritable, DoubleWritable, DoubleWritable> {
1:     private final DoubleWritable outKey = new DoubleWritable();
1:     private final DoubleWritable outValue = new DoubleWritable();
1: 
1:     @Override
1:     public void run(Context context) throws IOException,
1:         InterruptedException {
0:       double keySum = 0, valueSum = 0;
1:       while (context.nextKey()) {
1:         keySum += context.getCurrentKey().get();
1:         for (DoubleWritable value : context.getValues()) {
1:           valueSum += value.get();
1:         }
1:       }
1:       outKey.set(keySum);
1:       outValue.set(valueSum);
1:       context.write(outKey, outValue);
1:     }
1:   }
1: 
1:   /**
1:    * @param topicModelStateTemp
1:    * @param iteration
1:    * @return {@code double[2]} where first value is perplexity and second is model weight of those
1:    *         documents sampled during perplexity computation, or {@code null} if no perplexity data
1:    *         exists for the given iteration.
1:    * @throws IOException
1:    */
1:   public static double readPerplexity(Configuration conf, Path topicModelStateTemp, int iteration)
1:       throws IOException {
1:     Path perplexityPath = perplexityPath(topicModelStateTemp, iteration);
0:     FileSystem fs = FileSystem.get(conf);
1:     if (!fs.exists(perplexityPath)) {
1:       log.warn("Perplexity path {} does not exist, returning NaN", perplexityPath);
1:       return Double.NaN;
1:     }
1:     double perplexity = 0;
1:     double modelWeight = 0;
1:     long n = 0;
1:     for (Pair<DoubleWritable, DoubleWritable> pair : new SequenceFileDirIterable<DoubleWritable, DoubleWritable>(
1:         perplexityPath, PathType.LIST, PathFilters.partFilter(), null, true, conf)) {
1:       modelWeight += pair.getFirst().get();
1:       perplexity += pair.getSecond().get();
1:       n++;
1:     }
0:     log.info("Read {} entries with total perplexity {} and model weight {}", new Object[] { n,
0:             perplexity, modelWeight });
1:     return perplexity / modelWeight;
1:   }
1: 
0:   private Job writeTopicModel(Configuration conf, Path modelInput, Path output) throws IOException,
0:       InterruptedException, ClassNotFoundException {
0:     String jobName = String.format("Writing final topic/term distributions from %s to %s", modelInput,
0:         output);
0:     log.info("About to run: " + jobName);
0:     Job job = new Job(conf, jobName);
0:     job.setJarByClass(CVB0Driver.class);
0:     job.setInputFormatClass(SequenceFileInputFormat.class);
0:     job.setMapperClass(CVB0TopicTermVectorNormalizerMapper.class);
0:     job.setNumReduceTasks(0);
0:     job.setOutputKeyClass(IntWritable.class);
0:     job.setOutputValueClass(VectorWritable.class);
0:     job.setOutputFormatClass(SequenceFileOutputFormat.class);
0:     FileInputFormat.addInputPath(job, modelInput);
0:     FileOutputFormat.setOutputPath(job, output);
1:     job.submit();
1:     return job;
1:   }
1: 
0:   private Job writeDocTopicInference(Configuration conf, Path corpus, Path modelInput, Path output)
1:       throws IOException, ClassNotFoundException, InterruptedException {
0:     String jobName = String.format("Writing final document/topic inference from %s to %s", corpus,
0:         output);
0:     log.info("About to run: " + jobName);
0:     Job job = new Job(conf, jobName);
0:     job.setMapperClass(CVB0DocInferenceMapper.class);
0:     job.setNumReduceTasks(0);
0:     job.setInputFormatClass(SequenceFileInputFormat.class);
0:     job.setOutputFormatClass(SequenceFileOutputFormat.class);
0:     job.setOutputKeyClass(IntWritable.class);
0:     job.setOutputValueClass(VectorWritable.class);
0:     FileSystem fs = FileSystem.get(conf);
0:     if(modelInput != null && fs.exists(modelInput)) {
0:       FileStatus[] statuses = FileSystem.get(conf).listStatus(modelInput, PathFilters.partFilter());
1:       URI[] modelUris = new URI[statuses.length];
0:       for(int i = 0; i < statuses.length; i++) {
1:         modelUris[i] = statuses[i].getPath().toUri();
1:       }
1:       DistributedCache.setCacheFiles(modelUris, conf);
1:     }
0:     FileInputFormat.addInputPath(job, corpus);
0:     FileOutputFormat.setOutputPath(job, output);
0:     job.setJarByClass(CVB0Driver.class);
1:     job.submit();
1:     return job;
1:   }
1: 
1:   public static Path modelPath(Path topicModelStateTempPath, int iterationNumber) {
1:     return new Path(topicModelStateTempPath, "model-" + iterationNumber);
1:   }
1: 
0:   public static Path stage1OutputPath(Path topicModelStateTempPath, int iterationNumber) {
0:     return new Path(topicModelStateTempPath, "tmp-" + iterationNumber);
1:   }
1: 
1:   public static Path perplexityPath(Path topicModelStateTempPath, int iterationNumber) {
1:     return new Path(topicModelStateTempPath, "perplexity-" + iterationNumber);
1:   }
1: 
0:   private int getCurrentIterationNumber(Configuration config, Path modelTempDir, int maxIterations)
1:       throws IOException {
0:     FileSystem fs = FileSystem.get(config);
1:     int iterationNumber = 1;
1:     Path iterationPath = modelPath(modelTempDir, iterationNumber);
0:     while(fs.exists(iterationPath) && iterationNumber <= maxIterations) {
0:       log.info("Found previous state: " + iterationPath);
1:       iterationNumber++;
1:       iterationPath = modelPath(modelTempDir, iterationNumber);
1:     }
1:     return iterationNumber - 1;
1:   }
1: 
0:   public void runIteration(Configuration conf, Path corpusInput, Path modelInput, Path modelOutput,
0:       int iterationNumber, int maxIterations, int numReduceTasks) throws IOException, ClassNotFoundException, InterruptedException {
1:     String jobName = String.format("Iteration %d of %d, input path: %s",
1:         iterationNumber, maxIterations, modelInput);
0:     log.info("About to run: " + jobName);
0:     Job job = new Job(conf, jobName);
0:     job.setJarByClass(CVB0Driver.class);
0:     job.setMapperClass(CachingCVB0Mapper.class);
1:     job.setCombinerClass(VectorSumReducer.class);
0:     job.setReducerClass(VectorSumReducer.class);
1:     job.setNumReduceTasks(numReduceTasks);
0:     job.setOutputKeyClass(IntWritable.class);
0:     job.setOutputValueClass(VectorWritable.class);
0:     job.setInputFormatClass(SequenceFileInputFormat.class);
0:     job.setOutputFormatClass(SequenceFileOutputFormat.class);
0:     FileInputFormat.addInputPath(job, corpusInput);
0:     FileOutputFormat.setOutputPath(job, modelOutput);
1:     setModelPaths(job, modelInput);
1:     HadoopUtil.delete(conf, modelOutput);
0:     if(!job.waitForCompletion(true)) {
1:       throw new InterruptedException(String.format("Failed to complete iteration %d stage 1",
1:           iterationNumber));
1:     }
1:   }
1: 
0:   private void setModelPaths(Job job, Path modelPath) throws IOException {
1:     Configuration conf = job.getConfiguration();
0:     FileSystem fs = FileSystem.get(conf);
0:     if (modelPath == null || !fs.exists(modelPath)) {
1:       return;
1:     }
0:     FileStatus[] statuses = FileSystem.get(conf).listStatus(modelPath, PathFilters.partFilter());
1:     Preconditions.checkState(statuses.length > 0, "No part files found in model path '%s'", modelPath.toString());
1:     String[] modelPaths = new String[statuses.length];
1:     for (int i = 0; i < statuses.length; i++) {
1:       modelPaths[i] = statuses[i].getPath().toUri().toString();
1:     }
1:     conf.setStrings(MODEL_PATHS, modelPaths);
1:   }
1: 
1:   public static Path[] getModelPaths(Configuration conf) {
1:     String[] modelPathNames = conf.getStrings(MODEL_PATHS);
1:     if (modelPathNames == null || modelPathNames.length == 0) {
1:       return null;
1:     }
1:     Path[] modelPaths = new Path[modelPathNames.length];
1:     for (int i = 0; i < modelPathNames.length; i++) {
1:       modelPaths[i] = new Path(modelPathNames[i]);
1:     }
1:     return modelPaths;
1:   }
1: 
1:   public static void main(String[] args) throws Exception {
1:     ToolRunner.run(new Configuration(), new CVB0Driver(), args);
1:   }
1: }
author:Sean R. Owen
-------------------------------------------------------------------------------
commit:10c535c
/////////////////////////////////////////////////////////////////////////
1:     log.info("About to run: {}", jobName);
/////////////////////////////////////////////////////////////////////////
1:     log.info("About to run: {}", jobName);
/////////////////////////////////////////////////////////////////////////
1:     log.info("About to run: {}", jobName);
/////////////////////////////////////////////////////////////////////////
1:       log.info("Found previous state: {}", iterationPath);
/////////////////////////////////////////////////////////////////////////
1:     log.info("About to run: {}", jobName);
commit:8396a27
/////////////////////////////////////////////////////////////////////////
1:     log.info(infoString, inputPath, numTerms, numTopics, alpha, eta, maxIterations,
1:              convergenceDelta, topicModelOutputPath, randomSeed, testFraction);
/////////////////////////////////////////////////////////////////////////
1:                    iterationNumber, perplexities.get(perplexities.size() - 1), delta);
/////////////////////////////////////////////////////////////////////////
0:         log.info("(p_{} - p_{}) / p_0 = {}; target = {}", iterationNumber, iterationNumber - iterationBlockSize, rateOfChange(perplexities), convergenceDelta);
/////////////////////////////////////////////////////////////////////////
1:     log.info("Read {} entries with total perplexity {} and model weight {}", n,
1:              perplexity, modelWeight);
commit:229aeff
/////////////////////////////////////////////////////////////////////////
/////////////////////////////////////////////////////////////////////////
1:  * {@link org.apache.mahout.vectorizer.SparseVectorsFromSequenceFiles}
1:  *  for details on how to generate this input format.</dd>
/////////////////////////////////////////////////////////////////////////
0:     throws ClassNotFoundException, IOException, InterruptedException {
/////////////////////////////////////////////////////////////////////////
1:         (System.currentTimeMillis() - startTime) / 1000);
/////////////////////////////////////////////////////////////////////////
1:     throws IOException, ClassNotFoundException, InterruptedException {
/////////////////////////////////////////////////////////////////////////
0:     throws IOException {
/////////////////////////////////////////////////////////////////////////
0:   private static Job writeTopicModel(Configuration conf, Path modelInput, Path output)
1:     throws IOException, InterruptedException, ClassNotFoundException {
1:     String jobName = String.format("Writing final topic/term distributions from %s to %s", modelInput, output);
/////////////////////////////////////////////////////////////////////////
1:     throws IOException, ClassNotFoundException, InterruptedException {
1:     String jobName = String.format("Writing final document/topic inference from %s to %s", corpus, output);
/////////////////////////////////////////////////////////////////////////
0:     throws IOException {
/////////////////////////////////////////////////////////////////////////
0:                                   int iterationNumber, int maxIterations, int numReduceTasks)
0:     throws IOException, ClassNotFoundException, InterruptedException {
commit:1de8cec
/////////////////////////////////////////////////////////////////////////
1:     FileSystem fs = FileSystem.get(topicModelStateTempPath.toUri(), conf);
/////////////////////////////////////////////////////////////////////////
1:     FileSystem fs = FileSystem.get(perplexityPath.toUri(), conf);
/////////////////////////////////////////////////////////////////////////
1:     FileSystem fs = FileSystem.get(corpus.toUri(), conf);
1:     if (modelInput != null && fs.exists(modelInput)) {
1:       FileStatus[] statuses = fs.listStatus(modelInput, PathFilters.partFilter());
/////////////////////////////////////////////////////////////////////////
1:     FileSystem fs = FileSystem.get(modelTempDir.toUri(), config);
/////////////////////////////////////////////////////////////////////////
1:     if (modelPath == null || !FileSystem.get(modelPath.toUri(), conf).exists(modelPath)) {
1:     FileStatus[] statuses = FileSystem.get(modelPath.toUri(), conf).listStatus(modelPath, PathFilters.partFilter());
commit:4fbfbc6
/////////////////////////////////////////////////////////////////////////
1:  * Usage: {@code ./bin/mahout cvb <i>options</i>}
/////////////////////////////////////////////////////////////////////////
1:                        : 0.0f;
/////////////////////////////////////////////////////////////////////////
1:   private static int getNumTerms(Configuration conf, Path dictionaryPath) throws IOException {
/////////////////////////////////////////////////////////////////////////
0:   public static int run(Configuration conf,
0:                         Path inputPath,
0:                         Path topicModelOutputPath,
0:                         int numTopics,
0:                         int numTerms,
0:                         double alpha,
0:                         double eta,
0:                         int maxIterations,
0:                         int iterationBlockSize,
0:                         double convergenceDelta,
0:                         Path dictionaryPath,
0:                         Path docTopicOutputPath,
0:                         Path topicModelStateTempPath,
0:                         long randomSeed,
0:                         float testFraction,
0:                         int numTrainThreads,
0:                         int numUpdateThreads,
0:                         int maxItersPerDoc,
0:                         int numReduceTasks,
0:                         boolean backfillPerplexity)
1:     Preconditions.checkArgument(testFraction >= 0.0 && testFraction <= 1.0,
1:     Preconditions.checkArgument(!backfillPerplexity || testFraction > 0.0,
/////////////////////////////////////////////////////////////////////////
1:                ? "" : "Dictionary to be used located " + dictionaryPath.toString() + '\n';
1:                ? "" : "p(topic|docId) will be stored " + docTopicOutputPath.toString() + '\n';
/////////////////////////////////////////////////////////////////////////
1:     conf.set(MODEL_WEIGHT, "1"); // TODO
/////////////////////////////////////////////////////////////////////////
1:       if (convergenceDelta > 0.0) {
1:         double delta = rateOfChange(perplexities);
/////////////////////////////////////////////////////////////////////////
1:   private static double rateOfChange(List<Double> perplexities) {
/////////////////////////////////////////////////////////////////////////
0:   private static double calculatePerplexity(Configuration conf, Path corpusPath, Path modelPath, int iteration)
/////////////////////////////////////////////////////////////////////////
1:       double keySum = 0.0;
1:       double valueSum = 0.0;
/////////////////////////////////////////////////////////////////////////
0:   private static Job writeTopicModel(Configuration conf, Path modelInput, Path output) throws IOException,
/////////////////////////////////////////////////////////////////////////
0:   private static Job writeDocTopicInference(Configuration conf, Path corpus, Path modelInput, Path output)
/////////////////////////////////////////////////////////////////////////
1:   private static int getCurrentIterationNumber(Configuration config, Path modelTempDir, int maxIterations)
/////////////////////////////////////////////////////////////////////////
0:   public static void runIteration(Configuration conf, Path corpusInput, Path modelInput, Path modelOutput,
0:                                   int iterationNumber, int maxIterations, int numReduceTasks) throws IOException, ClassNotFoundException, InterruptedException {
/////////////////////////////////////////////////////////////////////////
1:   private static void setModelPaths(Job job, Path modelPath) throws IOException {
author:tcp
-------------------------------------------------------------------------------
commit:44459bd
/////////////////////////////////////////////////////////////////////////
1:     String infoString = "Will run Collapsed Variational Bayes (0th-derivative approximation) " 
1:       + "learning for LDA on {} (numTerms: {}), finding {}-topics, with document/topic prior {}, " 
1:       + "topic/term prior {}.  Maximum iterations to run will be {}, unless the change in " 
1:       + "perplexity is less than {}.  Topic model output (p(term|topic) for each topic) will be " 
1:       + "stored {}.  Random initialization seed is {}, holding out {} of the data for perplexity " 
1:       + "check\n";
commit:e64dd36
/////////////////////////////////////////////////////////////////////////
1:     if (parseArguments(args) == null) {
/////////////////////////////////////////////////////////////////////////
1:     while (iterationNumber < maxIterations) {
/////////////////////////////////////////////////////////////////////////
1:       if (testFraction > 0 && iterationNumber % iterationBlockSize == 0) {
/////////////////////////////////////////////////////////////////////////
1:     if (topicModelOutputJob != null && !topicModelOutputJob.waitForCompletion(true)) {
1:     if (docInferenceJob != null && !docInferenceJob.waitForCompletion(true)) {
/////////////////////////////////////////////////////////////////////////
1:     if (sz < 2) {
/////////////////////////////////////////////////////////////////////////
1:     if (!job.waitForCompletion(true)) {
/////////////////////////////////////////////////////////////////////////
1:       for (int i = 0; i < statuses.length; i++) {
/////////////////////////////////////////////////////////////////////////
1:     while (fs.exists(iterationPath) && iterationNumber <= maxIterations) {
/////////////////////////////////////////////////////////////////////////
1:     if (!job.waitForCompletion(true)) {
============================================================================