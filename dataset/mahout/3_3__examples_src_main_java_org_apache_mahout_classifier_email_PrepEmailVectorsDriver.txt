1:29a7f38: /**
1:29a7f38:  * Licensed to the Apache Software Foundation (ASF) under one or more
1:29a7f38:  * contributor license agreements.  See the NOTICE file distributed with
1:29a7f38:  * this work for additional information regarding copyright ownership.
1:29a7f38:  * The ASF licenses this file to You under the Apache License, Version 2.0
1:29a7f38:  * (the "License"); you may not use this file except in compliance with
1:29a7f38:  * the License.  You may obtain a copy of the License at
1:29a7f38:  *
1:29a7f38:  *     http://www.apache.org/licenses/LICENSE-2.0
1:29a7f38:  *
1:29a7f38:  * Unless required by applicable law or agreed to in writing, software
1:29a7f38:  * distributed under the License is distributed on an "AS IS" BASIS,
1:29a7f38:  * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
1:29a7f38:  * See the License for the specific language governing permissions and
1:29a7f38:  * limitations under the License.
1:29a7f38:  */
1:29a7f38: 
1:29a7f38: package org.apache.mahout.classifier.email;
1:29a7f38: 
1:29a7f38: import org.apache.hadoop.conf.Configuration;
1:29a7f38: import org.apache.hadoop.fs.Path;
1:29a7f38: import org.apache.hadoop.io.Text;
1:29a7f38: import org.apache.hadoop.mapreduce.Job;
1:29a7f38: import org.apache.hadoop.mapreduce.lib.input.SequenceFileInputFormat;
1:29a7f38: import org.apache.hadoop.mapreduce.lib.output.SequenceFileOutputFormat;
1:29a7f38: import org.apache.hadoop.util.ToolRunner;
1:29a7f38: import org.apache.mahout.common.AbstractJob;
1:29a7f38: import org.apache.mahout.common.HadoopUtil;
1:29a7f38: import org.apache.mahout.common.commandline.DefaultOptionCreator;
1:29a7f38: import org.apache.mahout.math.VectorWritable;
1:29a7f38: 
1:03a9492: import java.util.List;
1:29a7f38: import java.util.Map;
1:29a7f38: 
1:29a7f38: /**
1:29a7f38:  * Convert the labels generated by {@link org.apache.mahout.text.SequenceFilesFromMailArchives} and
1:3c22856:  * {@link org.apache.mahout.vectorizer.SparseVectorsFromSequenceFiles} to ones consumable by the classifiers. We do this
1:29a7f38:  * here b/c if it is done in the creation of sparse vectors, the Reducer collapses all the vectors.
1:29a7f38:  */
1:29a7f38: public class PrepEmailVectorsDriver extends AbstractJob {
1:29a7f38: 
1:29a7f38:   public static final String ITEMS_PER_CLASS = "itemsPerClass";
1:29a7f38:   public static final String USE_LIST_NAME = "USE_LIST_NAME";
1:29a7f38: 
1:29a7f38:   public static void main(String[] args) throws Exception {
1:29a7f38:     ToolRunner.run(new Configuration(), new PrepEmailVectorsDriver(), args);
1:29a7f38:   }
1:29a7f38: 
1:29a7f38:   @Override
1:29a7f38:   public int run(String[] args) throws Exception {
1:29a7f38:     addInputOption();
1:29a7f38:     addOutputOption();
1:29a7f38:     addOption(DefaultOptionCreator.overwriteOption().create());
1:6d16230:     addOption("maxItemsPerLabel", "mipl", "The maximum number of items per label.  Can be useful for making the "
1:6d16230:         + "training sets the same size", String.valueOf(100000));
1:6d16230:     addOption(buildOption("useListName", "ul", "Use the name of the list as part of the label.  If not set, then "
1:6d16230:         + "just use the project name", false, false, "false"));
1:03a9492:     Map<String,List<String>> parsedArgs = parseArguments(args);
1:29a7f38:     if (parsedArgs == null) {
1:29a7f38:       return -1;
1:29a7f38:     }
1:29a7f38: 
1:29a7f38:     Path input = getInputPath();
1:29a7f38:     Path output = getOutputPath();
1:29a7f38:     if (hasOption(DefaultOptionCreator.OVERWRITE_OPTION)) {
1:29a7f38:       HadoopUtil.delete(getConf(), output);
1:29a7f38:     }
1:3c22856:     Job convertJob = prepareJob(input, output, SequenceFileInputFormat.class, PrepEmailMapper.class, Text.class,
1:3c22856:         VectorWritable.class, PrepEmailReducer.class, Text.class, VectorWritable.class, SequenceFileOutputFormat.class);
1:03a9492:     convertJob.getConfiguration().set(ITEMS_PER_CLASS, getOption("maxItemsPerLabel"));
1:03a9492:     convertJob.getConfiguration().set(USE_LIST_NAME, String.valueOf(hasOption("useListName")));
1:7c2b664: 
1:7c2b664:     boolean succeeded = convertJob.waitForCompletion(true);
1:7c2b664:     return succeeded ? 0 : -1;
1:29a7f38:   }
1:29a7f38: }
============================================================================
author:Sebastian Schelter
-------------------------------------------------------------------------------
commit:6d16230
/////////////////////////////////////////////////////////////////////////
1:     addOption("maxItemsPerLabel", "mipl", "The maximum number of items per label.  Can be useful for making the "
1:         + "training sets the same size", String.valueOf(100000));
1:     addOption(buildOption("useListName", "ul", "Use the name of the list as part of the label.  If not set, then "
1:         + "just use the project name", false, false, "false"));
commit:3c22856
/////////////////////////////////////////////////////////////////////////
1:  * {@link org.apache.mahout.vectorizer.SparseVectorsFromSequenceFiles} to ones consumable by the classifiers. We do this
/////////////////////////////////////////////////////////////////////////
0:     addOption("maxItemsPerLabel", "mipl", "The maximum number of items per label.  Can be useful for making the " +
0:         "training sets the same size", String.valueOf(100000));
0:     addOption(buildOption("useListName", "ul", "Use the name of the list as part of the label.  If not set, then " +
0:         "just use the project name", false, false, "false"));
/////////////////////////////////////////////////////////////////////////
1:     Job convertJob = prepareJob(input, output, SequenceFileInputFormat.class, PrepEmailMapper.class, Text.class,
1:         VectorWritable.class, PrepEmailReducer.class, Text.class, VectorWritable.class, SequenceFileOutputFormat.class);
author:Grant Ingersoll
-------------------------------------------------------------------------------
commit:03a9492
/////////////////////////////////////////////////////////////////////////
1: import java.util.List;
/////////////////////////////////////////////////////////////////////////
1:     Map<String,List<String>> parsedArgs = parseArguments(args);
/////////////////////////////////////////////////////////////////////////
1:     convertJob.getConfiguration().set(ITEMS_PER_CLASS, getOption("maxItemsPerLabel"));
1:     convertJob.getConfiguration().set(USE_LIST_NAME, String.valueOf(hasOption("useListName")));
commit:29a7f38
/////////////////////////////////////////////////////////////////////////
1: package org.apache.mahout.classifier.email;
1: /**
1:  * Licensed to the Apache Software Foundation (ASF) under one or more
1:  * contributor license agreements.  See the NOTICE file distributed with
1:  * this work for additional information regarding copyright ownership.
1:  * The ASF licenses this file to You under the Apache License, Version 2.0
1:  * (the "License"); you may not use this file except in compliance with
1:  * the License.  You may obtain a copy of the License at
1:  *
1:  *     http://www.apache.org/licenses/LICENSE-2.0
1:  *
1:  * Unless required by applicable law or agreed to in writing, software
1:  * distributed under the License is distributed on an "AS IS" BASIS,
1:  * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
1:  * See the License for the specific language governing permissions and
1:  * limitations under the License.
1:  */
1: 
1: 
1: import org.apache.hadoop.conf.Configuration;
1: import org.apache.hadoop.fs.Path;
1: import org.apache.hadoop.io.Text;
1: import org.apache.hadoop.mapreduce.Job;
1: import org.apache.hadoop.mapreduce.lib.input.SequenceFileInputFormat;
1: import org.apache.hadoop.mapreduce.lib.output.SequenceFileOutputFormat;
1: import org.apache.hadoop.util.ToolRunner;
1: import org.apache.mahout.common.AbstractJob;
1: import org.apache.mahout.common.HadoopUtil;
1: import org.apache.mahout.common.commandline.DefaultOptionCreator;
1: import org.apache.mahout.math.VectorWritable;
1: 
1: import java.util.Map;
1: 
1: /**
1:  * Convert the labels generated by {@link org.apache.mahout.text.SequenceFilesFromMailArchives} and
0:  * {@link org.apache.mahout.vectorizer.SparseVectorsFromSequenceFiles} to ones consumable by the classifiers.  We do this
1:  * here b/c if it is done in the creation of sparse vectors, the Reducer collapses all the vectors.
1:  */
1: public class PrepEmailVectorsDriver extends AbstractJob {
1: 
1:   public static final String ITEMS_PER_CLASS = "itemsPerClass";
1:   public static final String USE_LIST_NAME = "USE_LIST_NAME";
1: 
1:   public static void main(String[] args) throws Exception {
1:     ToolRunner.run(new Configuration(), new PrepEmailVectorsDriver(), args);
1:   }
1: 
1:   @Override
1:   public int run(String[] args) throws Exception {
0:     int result = 0;
1:     addInputOption();
1:     addOutputOption();
1:     addOption(DefaultOptionCreator.overwriteOption().create());
0:     addOption("maxItemsPerLabel", "mipl", "The maximum number of items per label.  Can be useful for making the training sets the same size", String.valueOf(100000));
0:     addOption(buildOption("useListName", "ul", "Use the name of the list as part of the label.  If not set, then just use the project name", false, false, "false"));
0:     Map<String,String> parsedArgs = parseArguments(args);
1:     if (parsedArgs == null) {
1:       return -1;
1:     }
1: 
1:     Path input = getInputPath();
1:     Path output = getOutputPath();
1:     if (hasOption(DefaultOptionCreator.OVERWRITE_OPTION)) {
1:       HadoopUtil.delete(getConf(), output);
1:     }
0:     Job convertJob = prepareJob(input, output, SequenceFileInputFormat.class, PrepEmailMapper.class,
0:             Text.class, VectorWritable.class, PrepEmailReducer.class, Text.class, VectorWritable.class, SequenceFileOutputFormat.class);
0:     convertJob.getConfiguration().set(ITEMS_PER_CLASS, parsedArgs.get("--maxItemsPerLabel"));
0:     convertJob.getConfiguration().set(USE_LIST_NAME, String.valueOf(parsedArgs.containsKey("--useListName")));
0:     convertJob.waitForCompletion(true);
0:     return result;
1:   }
1: }
author:Sean R. Owen
-------------------------------------------------------------------------------
commit:7c2b664
/////////////////////////////////////////////////////////////////////////
1: 
1:     boolean succeeded = convertJob.waitForCompletion(true);
1:     return succeeded ? 0 : -1;
commit:e0ec7c1
/////////////////////////////////////////////////////////////////////////
/////////////////////////////////////////////////////////////////////////
0: package org.apache.mahout.classifier.email;
/////////////////////////////////////////////////////////////////////////
/////////////////////////////////////////////////////////////////////////
0:     return 0;
============================================================================