1:48989a4: /**
1:48989a4:  * Licensed to the Apache Software Foundation (ASF) under one or more
1:48989a4:  * contributor license agreements.  See the NOTICE file distributed with
1:48989a4:  * this work for additional information regarding copyright ownership.
1:48989a4:  * The ASF licenses this file to You under the Apache License, Version 2.0
1:48989a4:  * (the "License"); you may not use this file except in compliance with
1:48989a4:  * the License.  You may obtain a copy of the License at
3:48989a4:  *
1:48989a4:  *     http://www.apache.org/licenses/LICENSE-2.0
1:48989a4:  *
1:48989a4:  * Unless required by applicable law or agreed to in writing, software
1:48989a4:  * distributed under the License is distributed on an "AS IS" BASIS,
1:48989a4:  * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
1:48989a4:  * See the License for the specific language governing permissions and
1:48989a4:  * limitations under the License.
1:48989a4:  */
10:e8b1acf: 
1:48989a4: package org.apache.mahout.text;
1:d608a88: 
1:d711ac1: import java.io.IOException;
1:7c0da90: import java.nio.charset.Charset;
1:85f9ece: import java.util.HashMap;
1:466f319: import java.util.Map;
1:e8b1acf: 
1:c87196e: import org.apache.commons.lang3.StringUtils;
1:48989a4: import org.apache.hadoop.conf.Configuration;
1:d711ac1: import org.apache.hadoop.fs.FileStatus;
1:48989a4: import org.apache.hadoop.fs.FileSystem;
1:48989a4: import org.apache.hadoop.fs.Path;
1:c87196e: import org.apache.hadoop.fs.PathFilter;
1:d711ac1: import org.apache.hadoop.io.Text;
1:d711ac1: import org.apache.hadoop.mapreduce.Job;
1:d711ac1: import org.apache.hadoop.mapreduce.lib.input.FileInputFormat;
1:d711ac1: import org.apache.hadoop.mapreduce.lib.output.FileOutputFormat;
1:d711ac1: import org.apache.hadoop.mapreduce.lib.output.SequenceFileOutputFormat;
1:d29a980: import org.apache.hadoop.util.ToolRunner;
1:d29a980: import org.apache.mahout.common.AbstractJob;
1:d711ac1: import org.apache.mahout.common.ClassUtils;
1:e8b1acf: import org.apache.mahout.common.HadoopUtil;
1:e8b1acf: import org.apache.mahout.common.commandline.DefaultOptionCreator;
1:29a7f38: import org.apache.mahout.utils.io.ChunkedWriter;
1:e8b1acf: 
1:48989a4: /**
1:42ae840:  * Converts a directory of text documents into SequenceFiles of Specified chunkSize. This class takes in a
1:42ae840:  * parent directory containing sub folders of text documents and recursively reads the files and creates the
1:3c22856:  * {@link org.apache.hadoop.io.SequenceFile}s of docid => content. The docid is set as the relative path of the
1:3c22856:  * document from the parent directory prepended with a specified prefix. You can also specify the input encoding
1:3c22856:  * of the text files. The content of the output SequenceFiles are encoded as UTF-8 text.
1:48989a4:  */
1:466f319: public class SequenceFilesFromDirectory extends AbstractJob {
1:e8b1acf: 
1:e8b1acf:   private static final String PREFIX_ADDITION_FILTER = PrefixAdditionFilter.class.getName();
1:d711ac1: 
1:1499411:   private static final String[] CHUNK_SIZE_OPTION = {"chunkSize", "chunk"};
1:c87196e:   public static final String[] FILE_FILTER_CLASS_OPTION = {"fileFilterClass", "filter"};
1:7c0da90:   private static final String[] CHARSET_OPTION = {"charset", "c"};
1:e8b1acf: 
1:74078b6:   private static final int MAX_JOB_SPLIT_LOCATIONS = 1000000;
1:74078b6: 
1:74078b6:   public static final String[] KEY_PREFIX_OPTION = {"keyPrefix", "prefix"};
1:74078b6:   public static final String BASE_INPUT_PATH = "baseinputpath";
1:74078b6: 
1:48989a4:   public static void main(String[] args) throws Exception {
1:d29a980:     ToolRunner.run(new SequenceFilesFromDirectory(), args);
16:48989a4:   }
1:d711ac1: 
1:e8b1acf:   /*
1:d711ac1:   * callback main after processing MapReduce parameters
1:d711ac1:   */
2:48989a4:   @Override
1:7c0da90:   public int run(String[] args) throws Exception {
1:d711ac1:     addOptions();
1:d711ac1:     addOption(DefaultOptionCreator.methodOption().create());
1:d711ac1:     addOption(DefaultOptionCreator.overwriteOption().create());
1:d711ac1: 
1:e8b1acf:     if (parseArguments(args) == null) {
1:e8b1acf:       return -1;
1:e8b1acf:     }
1:d711ac1: 
1:466f319:     Map<String, String> options = parseOptions();
1:e8b1acf:     Path output = getOutputPath();
1:e8b1acf:     if (hasOption(DefaultOptionCreator.OVERWRITE_OPTION)) {
1:d711ac1:       HadoopUtil.delete(getConf(), output);
1:48989a4:     }
1:e8b1acf: 
1:d711ac1:     if (getOption(DefaultOptionCreator.METHOD_OPTION,
1:d711ac1:       DefaultOptionCreator.MAPREDUCE_METHOD).equals(DefaultOptionCreator.SEQUENTIAL_METHOD)) {
1:d711ac1:       runSequential(getConf(), getInputPath(), output, options);
1:d711ac1:     } else {
1:d711ac1:       runMapReduce(getInputPath(), output);
1:d711ac1:     }
1:d711ac1: 
1:d711ac1:     return 0;
1:d711ac1:   }
1:d711ac1: 
1:d711ac1:   private int runSequential(Configuration conf, Path input, Path output, Map<String, String> options)
1:d711ac1:     throws IOException, InterruptedException, NoSuchMethodException {
1:d711ac1:     // Running sequentially
1:7c0da90:     Charset charset = Charset.forName(getOption(CHARSET_OPTION[0]));
1:d711ac1:     String keyPrefix = getOption(KEY_PREFIX_OPTION[0]);
1:7c0da90:     FileSystem fs = FileSystem.get(input.toUri(), conf);
1:7c0da90: 
1:85f9ece:     try (ChunkedWriter writer = new ChunkedWriter(conf, Integer.parseInt(options.get(CHUNK_SIZE_OPTION[0])), output)) {
1:d608a88:       SequenceFilesFromDirectoryFilter pathFilter;
1:d608a88:       String fileFilterClassName = options.get(FILE_FILTER_CLASS_OPTION[0]);
1:d608a88:       if (PrefixAdditionFilter.class.getName().equals(fileFilterClassName)) {
1:7c0da90:         pathFilter = new PrefixAdditionFilter(conf, keyPrefix, options, writer, charset, fs);
1:d608a88:       } else {
1:d711ac1:         pathFilter = ClassUtils.instantiateAs(fileFilterClassName, SequenceFilesFromDirectoryFilter.class,
1:c87196e:           new Class[] {Configuration.class, String.class, Map.class, ChunkedWriter.class, Charset.class, FileSystem.class},
1:c87196e:           new Object[] {conf, keyPrefix, options, writer, charset, fs});
1:d608a88:       }
1:d608a88:       fs.listStatus(input, pathFilter);
1:7c0da90:     }
1:466f319:     return 0;
1:7c0da90:   }
1:e8b1acf: 
1:d711ac1:   private int runMapReduce(Path input, Path output) throws IOException, ClassNotFoundException, InterruptedException {
1:d711ac1: 
1:d711ac1:     int chunkSizeInMB = 64;
1:d711ac1:     if (hasOption(CHUNK_SIZE_OPTION[0])) {
1:d711ac1:       chunkSizeInMB = Integer.parseInt(getOption(CHUNK_SIZE_OPTION[0]));
1:d711ac1:     }
1:d711ac1: 
1:d711ac1:     String keyPrefix = null;
1:d711ac1:     if (hasOption(KEY_PREFIX_OPTION[0])) {
1:d711ac1:       keyPrefix = getOption(KEY_PREFIX_OPTION[0]);
1:d711ac1:     }
1:d711ac1: 
1:c87196e:     String fileFilterClassName = null;
1:c87196e:     if (hasOption(FILE_FILTER_CLASS_OPTION[0])) {
1:c87196e:       fileFilterClassName = getOption(FILE_FILTER_CLASS_OPTION[0]);
1:c87196e:     }
1:c87196e: 
1:c87196e:     PathFilter pathFilter = null;
1:c87196e:     // Prefix Addition is presently handled in the Mapper and unlike runsequential()
1:c87196e:     // need not be done via a pathFilter
1:c87196e:     if (!StringUtils.isBlank(fileFilterClassName) && !PrefixAdditionFilter.class.getName().equals(fileFilterClassName)) {
1:c87196e:       try {
1:c87196e:         pathFilter = (PathFilter) Class.forName(fileFilterClassName).newInstance();
1:87c15be:       } catch (InstantiationException | IllegalAccessException e) {
2:c87196e:         throw new IllegalStateException(e);
1:c87196e:       }
1:c87196e:     }
1:c87196e: 
1:d711ac1:     // Prepare Job for submission.
1:d711ac1:     Job job = prepareJob(input, output, MultipleTextFileInputFormat.class,
1:d711ac1:       SequenceFilesFromDirectoryMapper.class, Text.class, Text.class,
1:d711ac1:       SequenceFileOutputFormat.class, "SequenceFilesFromDirectory");
1:d711ac1: 
1:d711ac1:     Configuration jobConfig = job.getConfiguration();
1:74078b6:     jobConfig.set(KEY_PREFIX_OPTION[0], keyPrefix);
1:c87196e:     jobConfig.set(FILE_FILTER_CLASS_OPTION[0], fileFilterClassName);
1:c87196e: 
1:d711ac1:     FileSystem fs = FileSystem.get(jobConfig);
1:53fe357:     FileStatus fsFileStatus = fs.getFileStatus(input);
1:c87196e: 
1:c87196e:     String inputDirList;
1:c87196e:     if (pathFilter != null) {
1:c87196e:       inputDirList = HadoopUtil.buildDirList(fs, fsFileStatus, pathFilter);
1:c87196e:     } else {
1:c87196e:       inputDirList = HadoopUtil.buildDirList(fs, fsFileStatus);
1:c87196e:     }
1:c87196e: 
1:74078b6:     jobConfig.set(BASE_INPUT_PATH, input.toString());
1:d711ac1: 
1:d711ac1:     long chunkSizeInBytes = chunkSizeInMB * 1024 * 1024;
1:d711ac1: 
1:d711ac1:     // set the max split locations, otherwise we get nasty debug stuff
1:74078b6:     jobConfig.set("mapreduce.job.max.split.locations", String.valueOf(MAX_JOB_SPLIT_LOCATIONS));
1:d711ac1: 
1:d711ac1:     FileInputFormat.setInputPaths(job, inputDirList);
1:d711ac1:     // need to set this to a multiple of the block size, or no split happens
1:d711ac1:     FileInputFormat.setMaxInputSplitSize(job, chunkSizeInBytes);
1:d711ac1:     FileOutputFormat.setCompressOutput(job, true);
1:d711ac1: 
1:d711ac1:     boolean succeeded = job.waitForCompletion(true);
1:d711ac1:     if (!succeeded) {
1:d711ac1:       return -1;
1:d711ac1:     }
1:d711ac1:     return 0;
1:d711ac1:   }
1:d711ac1: 
1:466f319:   /**
1:466f319:    * Override this method in order to add additional options to the command line of the SequenceFileFromDirectory job.
1:466f319:    * Do not forget to call super() otherwise all standard options (input/output dirs etc) will not be available.
1:d711ac1:    */
1:466f319:   protected void addOptions() {
1:e8b1acf:     addInputOption();
1:e8b1acf:     addOutputOption();
1:e8b1acf:     addOption(DefaultOptionCreator.overwriteOption().create());
1:d711ac1:     addOption(DefaultOptionCreator.methodOption().create());
1:e8b1acf:     addOption(CHUNK_SIZE_OPTION[0], CHUNK_SIZE_OPTION[1], "The chunkSize in MegaBytes. Defaults to 64", "64");
1:e8b1acf:     addOption(FILE_FILTER_CLASS_OPTION[0], FILE_FILTER_CLASS_OPTION[1],
1:d711ac1:       "The name of the class to use for file parsing. Default: " + PREFIX_ADDITION_FILTER, PREFIX_ADDITION_FILTER);
1:e8b1acf:     addOption(KEY_PREFIX_OPTION[0], KEY_PREFIX_OPTION[1], "The prefix to be prepended to the key", "");
1:e8b1acf:     addOption(CHARSET_OPTION[0], CHARSET_OPTION[1],
1:d711ac1:       "The name of the character encoding of the input files. Default to UTF-8", "UTF-8");
1:e8b1acf:   }
1:e8b1acf: 
1:466f319:   /**
1:466f319:    * Override this method in order to parse your additional options from the command line. Do not forget to call
1:466f319:    * super() otherwise standard options (input/output dirs etc) will not be available.
1:d711ac1:    *
1:d711ac1:    * @return Map of options
1:e8b1acf:    */
1:4194a28:   protected Map<String, String> parseOptions() {
1:85f9ece:     Map<String, String> options = new HashMap<>();
1:466f319:     options.put(CHUNK_SIZE_OPTION[0], getOption(CHUNK_SIZE_OPTION[0]));
1:466f319:     options.put(FILE_FILTER_CLASS_OPTION[0], getOption(FILE_FILTER_CLASS_OPTION[0]));
1:466f319:     options.put(CHARSET_OPTION[0], getOption(CHARSET_OPTION[0]));
1:466f319:     return options;
1:48989a4:   }
1:48989a4: }
============================================================================
author:Suneel Marthi
-------------------------------------------------------------------------------
commit:85f9ece
/////////////////////////////////////////////////////////////////////////
1: import java.util.HashMap;
/////////////////////////////////////////////////////////////////////////
1:     try (ChunkedWriter writer = new ChunkedWriter(conf, Integer.parseInt(options.get(CHUNK_SIZE_OPTION[0])), output)) {
/////////////////////////////////////////////////////////////////////////
/////////////////////////////////////////////////////////////////////////
1:     Map<String, String> options = new HashMap<>();
commit:87c15be
/////////////////////////////////////////////////////////////////////////
1:       } catch (InstantiationException | IllegalAccessException e) {
author:smarthi
-------------------------------------------------------------------------------
commit:c87196e
/////////////////////////////////////////////////////////////////////////
1: import org.apache.commons.lang3.StringUtils;
1: import org.apache.hadoop.fs.PathFilter;
/////////////////////////////////////////////////////////////////////////
1:   public static final String[] FILE_FILTER_CLASS_OPTION = {"fileFilterClass", "filter"};
/////////////////////////////////////////////////////////////////////////
1:           new Class[] {Configuration.class, String.class, Map.class, ChunkedWriter.class, Charset.class, FileSystem.class},
1:           new Object[] {conf, keyPrefix, options, writer, charset, fs});
/////////////////////////////////////////////////////////////////////////
1:     String fileFilterClassName = null;
1:     if (hasOption(FILE_FILTER_CLASS_OPTION[0])) {
1:       fileFilterClassName = getOption(FILE_FILTER_CLASS_OPTION[0]);
1:     }
1: 
1:     PathFilter pathFilter = null;
1:     // Prefix Addition is presently handled in the Mapper and unlike runsequential()
1:     // need not be done via a pathFilter
1:     if (!StringUtils.isBlank(fileFilterClassName) && !PrefixAdditionFilter.class.getName().equals(fileFilterClassName)) {
1:       try {
1:         pathFilter = (PathFilter) Class.forName(fileFilterClassName).newInstance();
0:       } catch (InstantiationException e) {
1:         throw new IllegalStateException(e);
0:       } catch (IllegalAccessException e) {
1:         throw new IllegalStateException(e);
1:       }
1:     }
1: 
/////////////////////////////////////////////////////////////////////////
1:     jobConfig.set(FILE_FILTER_CLASS_OPTION[0], fileFilterClassName);
1: 
1: 
1:     String inputDirList;
1:     if (pathFilter != null) {
1:       inputDirList = HadoopUtil.buildDirList(fs, fsFileStatus, pathFilter);
1:     } else {
1:       inputDirList = HadoopUtil.buildDirList(fs, fsFileStatus);
1:     }
1: 
commit:74078b6
/////////////////////////////////////////////////////////////////////////
1:   private static final int MAX_JOB_SPLIT_LOCATIONS = 1000000;
1: 
1:   public static final String[] KEY_PREFIX_OPTION = {"keyPrefix", "prefix"};
1:   public static final String BASE_INPUT_PATH = "baseinputpath";
1: 
/////////////////////////////////////////////////////////////////////////
1:     jobConfig.set(KEY_PREFIX_OPTION[0], keyPrefix);
1:     jobConfig.set(BASE_INPUT_PATH, input.toString());
1:     jobConfig.set("mapreduce.job.max.split.locations", String.valueOf(MAX_JOB_SPLIT_LOCATIONS));
commit:53fe357
/////////////////////////////////////////////////////////////////////////
1:     FileStatus fsFileStatus = fs.getFileStatus(input);
commit:22533ae
/////////////////////////////////////////////////////////////////////////
0:     FileStatus fsFileStatus = HadoopUtil.listStatus(fs, input)[0];
0:     String inputDirList = HadoopUtil.buildDirList(fs, fsFileStatus);
commit:d711ac1
/////////////////////////////////////////////////////////////////////////
1: import java.io.IOException;
1: import org.apache.hadoop.fs.FileStatus;
1: import org.apache.hadoop.io.Text;
1: import org.apache.hadoop.mapreduce.Job;
1: import org.apache.hadoop.mapreduce.lib.input.FileInputFormat;
1: import org.apache.hadoop.mapreduce.lib.output.FileOutputFormat;
1: import org.apache.hadoop.mapreduce.lib.output.SequenceFileOutputFormat;
1: import org.apache.mahout.common.ClassUtils;
/////////////////////////////////////////////////////////////////////////
1: 
0:   private static final String[] FILE_FILTER_CLASS_OPTION = {"fileFilterClass", "filter"};
1: 
1:   * callback main after processing MapReduce parameters
1:   */
1:     addOptions();
1:     addOption(DefaultOptionCreator.methodOption().create());
1:     addOption(DefaultOptionCreator.overwriteOption().create());
1: 
1: 
1:       HadoopUtil.delete(getConf(), output);
1:     if (getOption(DefaultOptionCreator.METHOD_OPTION,
1:       DefaultOptionCreator.MAPREDUCE_METHOD).equals(DefaultOptionCreator.SEQUENTIAL_METHOD)) {
1:       runSequential(getConf(), getInputPath(), output, options);
1:     } else {
1:       runMapReduce(getInputPath(), output);
1:     }
1: 
1:     return 0;
1:   }
1: 
1:   private int runSequential(Configuration conf, Path input, Path output, Map<String, String> options)
1:     throws IOException, InterruptedException, NoSuchMethodException {
1:     // Running sequentially
1:     String keyPrefix = getOption(KEY_PREFIX_OPTION[0]);
/////////////////////////////////////////////////////////////////////////
1:         pathFilter = ClassUtils.instantiateAs(fileFilterClassName, SequenceFilesFromDirectoryFilter.class,
0:           new Class[]{Configuration.class, String.class, Map.class, ChunkedWriter.class, Charset.class, FileSystem.class},
0:           new Object[]{conf, keyPrefix, options, writer, charset, fs});
/////////////////////////////////////////////////////////////////////////
1:   private int runMapReduce(Path input, Path output) throws IOException, ClassNotFoundException, InterruptedException {
1: 
1:     int chunkSizeInMB = 64;
1:     if (hasOption(CHUNK_SIZE_OPTION[0])) {
1:       chunkSizeInMB = Integer.parseInt(getOption(CHUNK_SIZE_OPTION[0]));
1:     }
1: 
1:     String keyPrefix = null;
1:     if (hasOption(KEY_PREFIX_OPTION[0])) {
1:       keyPrefix = getOption(KEY_PREFIX_OPTION[0]);
1:     }
1: 
1:     // Prepare Job for submission.
1:     Job job = prepareJob(input, output, MultipleTextFileInputFormat.class,
1:       SequenceFilesFromDirectoryMapper.class, Text.class, Text.class,
1:       SequenceFileOutputFormat.class, "SequenceFilesFromDirectory");
1: 
1:     Configuration jobConfig = job.getConfiguration();
0:     jobConfig.set("keyPrefix", keyPrefix);
1:     FileSystem fs = FileSystem.get(jobConfig);
0:     FileStatus fsFileStatus = fs.getFileStatus(input);
0:     String inputDirList = buildDirList(fs, fsFileStatus);
0:     jobConfig.set("baseinputpath", input.toString());
1: 
1:     long chunkSizeInBytes = chunkSizeInMB * 1024 * 1024;
1: 
1:     // set the max split locations, otherwise we get nasty debug stuff
0:     jobConfig.set("mapreduce.job.max.split.locations", "1000000");
1: 
1:     FileInputFormat.setInputPaths(job, inputDirList);
1:     // need to set this to a multiple of the block size, or no split happens
1:     FileInputFormat.setMaxInputSplitSize(job, chunkSizeInBytes);
1:     FileOutputFormat.setCompressOutput(job, true);
1: 
1:     boolean succeeded = job.waitForCompletion(true);
1:     if (!succeeded) {
1:       return -1;
1:     }
1:     return 0;
1:   }
1: 
1:    */
1:     addOption(DefaultOptionCreator.methodOption().create());
1:       "The name of the class to use for file parsing. Default: " + PREFIX_ADDITION_FILTER, PREFIX_ADDITION_FILTER);
1:       "The name of the character encoding of the input files. Default to UTF-8", "UTF-8");
1:    *
1:    * @return Map of options
commit:54a6657
/////////////////////////////////////////////////////////////////////////
0:       Closeables.close(writer, false);
author:Sebastian Schelter
-------------------------------------------------------------------------------
commit:3c22856
/////////////////////////////////////////////////////////////////////////
/////////////////////////////////////////////////////////////////////////
1:  * {@link org.apache.hadoop.io.SequenceFile}s of docid => content. The docid is set as the relative path of the
1:  * document from the parent directory prepended with a specified prefix. You can also specify the input encoding
1:  * of the text files. The content of the output SequenceFiles are encoded as UTF-8 text.
commit:6a8de49
/////////////////////////////////////////////////////////////////////////
0:         pathFilter = constructor.newInstance(conf, keyPrefix, options, writer, charset, fs);
commit:74f849b
/////////////////////////////////////////////////////////////////////////
0: import com.google.common.collect.Maps;
/////////////////////////////////////////////////////////////////////////
0:     Map<String, String> options = Maps.newHashMap();
commit:d608a88
/////////////////////////////////////////////////////////////////////////
0: import com.google.common.io.Closeables;
/////////////////////////////////////////////////////////////////////////
0:     try {
1:       SequenceFilesFromDirectoryFilter pathFilter;
1: 
1:       String fileFilterClassName = options.get(FILE_FILTER_CLASS_OPTION[0]);
1:       if (PrefixAdditionFilter.class.getName().equals(fileFilterClassName)) {
0:         pathFilter = new PrefixAdditionFilter(conf, keyPrefix, options, writer);
1:       } else {
0:         Class<? extends SequenceFilesFromDirectoryFilter> pathFilterClass =
0:             Class.forName(fileFilterClassName).asSubclass(SequenceFilesFromDirectoryFilter.class);
0:         Constructor<? extends SequenceFilesFromDirectoryFilter> constructor =
0:             pathFilterClass.getConstructor(Configuration.class, String.class, Map.class, ChunkedWriter.class);
0:         pathFilter = constructor.newInstance(conf, keyPrefix, options, writer);
1:       }
1:       fs.listStatus(input, pathFilter);
0:     } finally {
0:       Closeables.closeQuietly(writer);
author:Sean R. Owen
-------------------------------------------------------------------------------
commit:4194a28
/////////////////////////////////////////////////////////////////////////
/////////////////////////////////////////////////////////////////////////
1:   protected Map<String, String> parseOptions() {
commit:7c0da90
/////////////////////////////////////////////////////////////////////////
1: import java.nio.charset.Charset;
/////////////////////////////////////////////////////////////////////////
0:   private static final String[] FILE_FILTER_CLASS_OPTION = {"fileFilterClass","filter"};
1:   private static final String[] CHARSET_OPTION = {"charset", "c"};
/////////////////////////////////////////////////////////////////////////
1:   public int run(String[] args) throws Exception {
/////////////////////////////////////////////////////////////////////////
1:     Charset charset = Charset.forName(getOption(CHARSET_OPTION[0]));
0:     Configuration conf = getConf();
1:     FileSystem fs = FileSystem.get(input.toUri(), conf);
0:     ChunkedWriter writer = new ChunkedWriter(conf, Integer.parseInt(options.get(CHUNK_SIZE_OPTION[0])), output);
1: 
0:     try {
0:       SequenceFilesFromDirectoryFilter pathFilter;
0:       String fileFilterClassName = options.get(FILE_FILTER_CLASS_OPTION[0]);
0:       if (PrefixAdditionFilter.class.getName().equals(fileFilterClassName)) {
1:         pathFilter = new PrefixAdditionFilter(conf, keyPrefix, options, writer, charset, fs);
0:       } else {
0:         Class<? extends SequenceFilesFromDirectoryFilter> pathFilterClass =
0:             Class.forName(fileFilterClassName).asSubclass(SequenceFilesFromDirectoryFilter.class);
0:         Constructor<? extends SequenceFilesFromDirectoryFilter> constructor =
0:             pathFilterClass.getConstructor(Configuration.class,
0:                                            String.class,
0:                                            Map.class,
0:                                            ChunkedWriter.class,
0:                                            Charset.class,
0:                                            FileSystem.class);
0:         pathFilter = constructor.newInstance(conf, keyPrefix, options, writer, fs);
1:       }
0:       fs.listStatus(input, pathFilter);
0:     } finally {
0:       Closeables.closeQuietly(writer);
1:     }
commit:1499411
/////////////////////////////////////////////////////////////////////////
1:   private static final String[] CHUNK_SIZE_OPTION = {"chunkSize", "chunk"};
0:   static final String[] FILE_FILTER_CLASS_OPTION = {"fileFilterClass","filter"};
0:   private static final String[] KEY_PREFIX_OPTION = {"keyPrefix", "prefix"};
0:   static final String[] CHARSET_OPTION = {"charset", "c"};
commit:3d44c1e
/////////////////////////////////////////////////////////////////////////
0:   public static void run(Configuration conf,
0:                          String keyPrefix,
0:                          Map<String, String> options,
0:                          Path input,
0:                          Path output)
commit:f5e7732
/////////////////////////////////////////////////////////////////////////
/////////////////////////////////////////////////////////////////////////
/////////////////////////////////////////////////////////////////////////
0:     FileSystem fs = FileSystem.get(input.toUri(), conf);
0:         pathFilter = new PrefixAdditionFilter(conf, keyPrefix, options, writer, fs);
0:             pathFilterClass.getConstructor(Configuration.class,
0:                                            String.class,
0:                                            Map.class,
0:                                            ChunkedWriter.class,
0:                                            FileSystem.class);
0:         pathFilter = constructor.newInstance(conf, keyPrefix, options, writer, fs);
commit:50fd693
commit:a13b4b7
/////////////////////////////////////////////////////////////////////////
/////////////////////////////////////////////////////////////////////////
0:       Configuration conf = new Configuration();
0:       HadoopUtil.delete(conf, output);
commit:d61a0ee
/////////////////////////////////////////////////////////////////////////
0:     throws InstantiationException, IllegalAccessException, InvocationTargetException, IOException,
0:            NoSuchMethodException, ClassNotFoundException {
commit:3a1adb7
/////////////////////////////////////////////////////////////////////////
0:       } catch (IOException e) {
commit:e8b1acf
/////////////////////////////////////////////////////////////////////////
0: import java.io.InputStream;
0: import org.apache.hadoop.fs.FileStatus;
0: import org.apache.hadoop.fs.PathFilter;
1: import org.apache.mahout.common.HadoopUtil;
1: import org.apache.mahout.common.commandline.DefaultOptionCreator;
/////////////////////////////////////////////////////////////////////////
1:   private static final String PREFIX_ADDITION_FILTER = PrefixAdditionFilter.class.getName();
1:   
0:   public static final String[] CHUNK_SIZE_OPTION = {"chunkSize", "chunk"};
0:   public static final String[] FILE_FILTER_CLASS_OPTION = {"fileFilterClass","filter"};
0:   public static final String[] KEY_PREFIX_OPTION = {"keyPrefix", "prefix"};
0:   public static final String[] CHARSET_OPTION = {"charset", "c"};
0:   public void run(Configuration conf,
0:                   Path input,
0:                   Path output,
0:                   String prefix,
0:                   int chunkSizeInMB,
0:                   Charset charset,
0:                   String fileFilterClassName)
0:     throws IllegalArgumentException, InstantiationException, IllegalAccessException, InvocationTargetException,
0:            IOException, SecurityException, NoSuchMethodException, ClassNotFoundException {
0:     FileSystem fs = FileSystem.get(conf);
0:     ChunkedWriter writer = new ChunkedWriter(conf, chunkSizeInMB, output);
1:     
0:     PathFilter pathFilter;
1:     
0:     if (PrefixAdditionFilter.class.getName().equals(fileFilterClassName)) {
0:       pathFilter = new PrefixAdditionFilter(conf, prefix, writer, charset);
0:       Class<? extends PathFilter> pathFilterClass = Class.forName(fileFilterClassName).asSubclass(PathFilter.class);
0:       Constructor<? extends PathFilter> constructor =
0:           pathFilterClass.getConstructor(Configuration.class, String.class, ChunkedWriter.class, Charset.class);
0:       pathFilter = constructor.newInstance(conf, prefix, writer, charset);
0:     fs.listStatus(input, pathFilter);
1:   
0:   private static final class ChunkedWriter implements Closeable {
0:     private final Path output;
0:     private final Configuration conf;
1:     
0:     private ChunkedWriter(Configuration conf, int chunkSizeInMB, Path output) throws IOException {
0:       this.output = output;
0:       this.conf = conf;
1:     
0:       return new Path(output, "chunk-" + chunkID);
1:     
1:       
1:     
1:   
0:   private final class PrefixAdditionFilter implements PathFilter {
0:     private final Configuration conf;
0:     private final FileSystem fs;
1:     
0:     private PrefixAdditionFilter(Configuration conf, String prefix, ChunkedWriter writer, Charset charset)
0:       throws IOException {
0:       this.conf = conf;
0:       this.fs = FileSystem.get(conf);
1:     
0:     public boolean accept(Path current) {
0:       log.debug("CURRENT: {}", current.getName());
0:       try {
0:         FileStatus[] fstatus = fs.listStatus(current);
0:         for (FileStatus fst : fstatus) {
0:           log.debug("CHILD: {}", fst.getPath().getName());
0:           if (fst.isDir()) {
0:             fs.listStatus(fst.getPath(),
0:                           new PrefixAdditionFilter(conf, prefix + Path.SEPARATOR + current.getName(), writer, charset));
0:           } else {
0:             StringBuilder file = new StringBuilder();
0:             InputStream in = fs.open(fst.getPath());
0:             for (String aFit : new FileLineIterable(in, charset, false)) {
0:               file.append(aFit).append('\n');
1:             }
0:             String name = current.getName().equals(fst.getPath().getName())
0:                 ? current.getName()
0:                 : current.getName() + Path.SEPARATOR + fst.getPath().getName();
0:             writer.write(prefix + Path.SEPARATOR + name, file.toString());
0:       } catch (Exception e) {
0:         throw new IllegalStateException(e);
1:   
1:   /*
0:    * callback main after processing hadoop parameters
1:    */
0:   public int run(String[] args)
0:     throws IOException, ClassNotFoundException, InstantiationException, IllegalAccessException, NoSuchMethodException,
0:            InvocationTargetException {
1:     
1:     addInputOption();
1:     addOutputOption();
1:     addOption(DefaultOptionCreator.overwriteOption().create());
1:     addOption(CHUNK_SIZE_OPTION[0], CHUNK_SIZE_OPTION[1], "The chunkSize in MegaBytes. Defaults to 64", "64");
1:     addOption(FILE_FILTER_CLASS_OPTION[0], FILE_FILTER_CLASS_OPTION[1],
0:         "The name of the class to use for file parsing. Default: " + PREFIX_ADDITION_FILTER, PREFIX_ADDITION_FILTER);
1:     addOption(KEY_PREFIX_OPTION[0], KEY_PREFIX_OPTION[1], "The prefix to be prepended to the key", "");
1:     addOption(CHARSET_OPTION[0], CHARSET_OPTION[1],
0:         "The name of the character encoding of the input files. Default to UTF-8", "UTF-8");
1:     
1:     if (parseArguments(args) == null) {
1:       return -1;
1:     
0:     Path input = getInputPath();
1:     Path output = getOutputPath();
1:     if (hasOption(DefaultOptionCreator.OVERWRITE_OPTION)) {
0:       HadoopUtil.overwriteOutput(output);
1:     }
0:     int chunkSize = Integer.parseInt(getOption(CHUNK_SIZE_OPTION[0]));
0:     String fileFilterClassName = getOption(FILE_FILTER_CLASS_OPTION[0]);
0:     String keyPrefix = getOption(KEY_PREFIX_OPTION[0]);
0:     Charset charset = Charset.forName(getOption(CHARSET_OPTION[0]));
1:     
0:     run(getConf(), input, output, keyPrefix, chunkSize, charset, fileFilterClassName);
commit:210fac3
commit:2e77bf8
/////////////////////////////////////////////////////////////////////////
/////////////////////////////////////////////////////////////////////////
0:   private static ChunkedWriter createNewChunkedWriter(int chunkSizeInMB,
commit:5a677d4
/////////////////////////////////////////////////////////////////////////
0:   public static class ChunkedWriter implements Closeable {
0:     private final int maxChunkSizeInBytes;
0:     private final String outputDir;
0:     private SequenceFile.Writer writer;
0:     private int currentChunkID;
0:     private int currentChunkSize;
0:     private final Configuration conf = new Configuration();
0:     private final FileSystem fs;
/////////////////////////////////////////////////////////////////////////
0:     private final String prefix;
0:     private final ChunkedWriter writer;
0:     private final Charset charset;
/////////////////////////////////////////////////////////////////////////
0:           for (String aFit : new FileLineIterable(current, charset, false)) {
0:             file.append(aFit).append('\n');
/////////////////////////////////////////////////////////////////////////
0:       chunkSize = Integer.parseInt((String) cmdLine.getValue(chunkSizeOpt));
commit:48989a4
/////////////////////////////////////////////////////////////////////////
1: /**
1:  * Licensed to the Apache Software Foundation (ASF) under one or more
1:  * contributor license agreements.  See the NOTICE file distributed with
1:  * this work for additional information regarding copyright ownership.
1:  * The ASF licenses this file to You under the Apache License, Version 2.0
1:  * (the "License"); you may not use this file except in compliance with
1:  * the License.  You may obtain a copy of the License at
1:  *
1:  *     http://www.apache.org/licenses/LICENSE-2.0
1:  *
1:  * Unless required by applicable law or agreed to in writing, software
1:  * distributed under the License is distributed on an "AS IS" BASIS,
1:  * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
1:  * See the License for the specific language governing permissions and
1:  * limitations under the License.
1:  */
0: 
1: package org.apache.mahout.text;
0: 
0: import java.io.Closeable;
0: import java.io.File;
0: import java.io.FileFilter;
0: import java.io.FileNotFoundException;
0: import java.io.IOException;
0: import java.nio.charset.Charset;
0: import java.util.Iterator;
0: 
0: import org.apache.commons.cli2.CommandLine;
0: import org.apache.commons.cli2.Group;
0: import org.apache.commons.cli2.Option;
0: import org.apache.commons.cli2.builder.ArgumentBuilder;
0: import org.apache.commons.cli2.builder.DefaultOptionBuilder;
0: import org.apache.commons.cli2.builder.GroupBuilder;
0: import org.apache.commons.cli2.commandline.Parser;
1: import org.apache.hadoop.conf.Configuration;
1: import org.apache.hadoop.fs.FileSystem;
1: import org.apache.hadoop.fs.Path;
0: import org.apache.hadoop.io.SequenceFile;
0: import org.apache.hadoop.io.Text;
0: import org.apache.mahout.common.FileLineIterable;
0: 
1: /**
0:  * Converts a directory of text documents into SequenceFiles of Specified
0:  * chunkSize. This class takes in a parent directory containing sub folders of
0:  * text documents and recursively reads the files and creates the
0:  * {@link SequenceFile}s of docid => content. The docid is set as the relative
0:  * path of the document from the parent directory prepended with a specified
0:  * prefix. You can also specify the input encoding of the text files. The
0:  * content of the output SequenceFiles are encoded as UTF-8 text.
1:  * 
1:  * 
1:  */
0: public final class SequenceFilesFromDirectory {
0:   
0:   private ChunkedWriter createNewChunkedWriter(int chunkSizeInMB,
0:                                                String outputDir) throws IOException {
0:     return new ChunkedWriter(chunkSizeInMB, outputDir);
1:   }
0:   
0:   public void createSequenceFiles(File parentDir,
0:                                   String outputDir,
0:                                   String prefix,
0:                                   int chunkSizeInMB,
0:                                   Charset charset) throws IOException {
0:     ChunkedWriter writer = createNewChunkedWriter(chunkSizeInMB, outputDir);
0:     parentDir.listFiles(new PrefixAdditionFilter(prefix, writer, charset));
0:     writer.close();
1:   }
0:   
0:   public class ChunkedWriter implements Closeable {
0:     int maxChunkSizeInBytes;
0:     String outputDir;
0:     SequenceFile.Writer writer;
0:     int currentChunkID;
0:     int currentChunkSize;
0:     Configuration conf = new Configuration();
0:     FileSystem fs;
0:     
0:     public ChunkedWriter(int chunkSizeInMB, String outputDir) throws IOException {
0:       if (chunkSizeInMB < 64) {
0:         chunkSizeInMB = 64;
0:       } else if (chunkSizeInMB > 1984) {
0:         chunkSizeInMB = 1984;
1:       }
0:       maxChunkSizeInBytes = chunkSizeInMB * 1024 * 1024;
0:       this.outputDir = outputDir;
0:       fs = FileSystem.get(conf);
0:       writer =
0:           new SequenceFile.Writer(fs, conf, getPath(currentChunkID),
0:               Text.class, Text.class);
1:     }
0:     
0:     private Path getPath(int chunkID) {
0:       return new Path(outputDir + "/chunk-" + chunkID);
1:     }
0:     
0:     public void write(String key, String value) throws IOException {
0:       if (currentChunkSize > maxChunkSizeInBytes) {
0:         writer.close();
0:         writer =
0:             new SequenceFile.Writer(fs, conf, getPath(currentChunkID++),
0:                 Text.class, Text.class);
0:         currentChunkSize = 0;
0:         
1:       }
0:       
0:       Text keyT = new Text(key);
0:       Text valueT = new Text(value);
0:       currentChunkSize += keyT.getBytes().length + valueT.getBytes().length; // Overhead
0:       writer.append(keyT, valueT);
1:     }
0:     
1:     @Override
0:     public void close() throws IOException {
0:       writer.close();
1:     }
1:   }
0:   
0:   public class PrefixAdditionFilter implements FileFilter {
0:     String prefix;
0:     ChunkedWriter writer;
0:     Charset charset;
0:     
0:     public PrefixAdditionFilter(String prefix,
0:                                 ChunkedWriter writer,
0:                                 Charset charset) {
0:       this.prefix = prefix;
0:       this.writer = writer;
0:       this.charset = charset;
1:     }
0:     
1:     @Override
0:     public boolean accept(File current) {
0:       if (current.isDirectory()) {
0:         current.listFiles(new PrefixAdditionFilter(prefix
0:             + File.separator
0:             + current.getName(), writer, charset));
0:       } else {
0:         try {
0:           FileLineIterable fit = new FileLineIterable(current, charset, false);
0:           StringBuilder file = new StringBuilder();
0:           Iterator<String> it = fit.iterator();
0:           while (it.hasNext()) {
0:             file.append(it.next()).append("\n");
1:           }
0:           writer.write(prefix + File.separator + current.getName(), file
0:               .toString());
0:           
0:         } catch (FileNotFoundException e) {
0:           // Skip file.
0:         } catch (IOException e) {
0:           // TODO: report exceptions and continue;
0:           throw new IllegalStateException(e);
1:         }
1:       }
0:       return false;
1:     }
0:     
1:   }
0:   
1:   public static void main(String[] args) throws Exception {
0:     DefaultOptionBuilder obuilder = new DefaultOptionBuilder();
0:     ArgumentBuilder abuilder = new ArgumentBuilder();
0:     GroupBuilder gbuilder = new GroupBuilder();
0:     
0:     Option parentOpt =
0:         obuilder.withLongName("parent").withRequired(true).withArgument(
0:             abuilder.withName("parent").withMinimum(1).withMaximum(1).create())
0:             .withDescription("Parent dir containing the documents")
0:             .withShortName("p").create();
0:     
0:     Option outputDirOpt =
0:         obuilder.withLongName("outputDir").withRequired(true).withArgument(
0:             abuilder.withName("outputDir").withMinimum(1).withMaximum(1)
0:                 .create()).withDescription("The output directory")
0:             .withShortName("o").create();
0:     
0:     Option chunkSizeOpt =
0:         obuilder.withLongName("chunkSize").withArgument(
0:             abuilder.withName("chunkSize").withMinimum(1).withMaximum(1)
0:                 .create()).withDescription(
0:             "The chunkSize in MegaBytes. Defaults to 64")
0:             .withShortName("chunk").create();
0:     
0:     Option keyPrefixOpt =
0:         obuilder.withLongName("keyPrefix").withArgument(
0:             abuilder.withName("keyPrefix").withMinimum(1).withMaximum(1)
0:                 .create()).withDescription(
0:             "The prefix to be prepended to the key").withShortName("prefix")
0:             .create();
0:     
0:     Option charsetOpt =
0:         obuilder.withLongName("charset").withRequired(true)
0:             .withArgument(
0:                 abuilder.withName("charset").withMinimum(1).withMaximum(1)
0:                     .create()).withDescription(
0:                 "The name of the character encoding of the input files")
0:             .withShortName("c").create();
0:     
0:     Group group =
0:         gbuilder.withName("Options").withOption(keyPrefixOpt).withOption(
0:             chunkSizeOpt).withOption(charsetOpt).withOption(outputDirOpt)
0:             .withOption(parentOpt).create();
0:     
0:     Parser parser = new Parser();
0:     parser.setGroup(group);
0:     CommandLine cmdLine = parser.parse(args);
0:     
0:     File parentDir = new File((String) cmdLine.getValue(parentOpt));
0:     String outputDir = (String) cmdLine.getValue(outputDirOpt);
0:     
0:     int chunkSize = 64;
0:     if (cmdLine.hasOption(chunkSizeOpt)) {
0:       chunkSize =
0:           Integer.valueOf((String) cmdLine.getValue(chunkSizeOpt)).intValue();
1:     }
0:     
0:     String prefix = "";
0:     if (cmdLine.hasOption(keyPrefixOpt)) {
0:       prefix = (String) cmdLine.getValue(keyPrefixOpt);
1:     }
0:     Charset charset = Charset.forName((String) cmdLine.getValue(charsetOpt));
0:     SequenceFilesFromDirectory dir = new SequenceFilesFromDirectory();
0:     
0:     dir.createSequenceFiles(parentDir, outputDir, prefix, chunkSize, charset);
1:   }
1: }
author:Grant Ingersoll
-------------------------------------------------------------------------------
commit:29a7f38
/////////////////////////////////////////////////////////////////////////
1: import org.apache.mahout.utils.io.ChunkedWriter;
commit:e970c0b
commit:833d96f
/////////////////////////////////////////////////////////////////////////
0:             .withShortName("i").create();
commit:905bee3
/////////////////////////////////////////////////////////////////////////
0:         obuilder.withLongName("input").withRequired(true).withArgument(
0:             abuilder.withName("input").withMinimum(1).withMaximum(1).create())
0:             .withDescription("The input dir containing the documents")
0:         obuilder.withLongName("output").withRequired(true).withArgument(
0:             abuilder.withName("output").withMinimum(1).withMaximum(1)
commit:9ebf912
/////////////////////////////////////////////////////////////////////////
0: import org.apache.commons.cli2.OptionException;
/////////////////////////////////////////////////////////////////////////
0: import org.apache.mahout.common.CommandLineUtil;
0: import org.slf4j.LoggerFactory;
0: import org.slf4j.Logger;
/////////////////////////////////////////////////////////////////////////
0: 
0:   private transient static Logger log = LoggerFactory.getLogger(SequenceFilesFromDirectory.class);
/////////////////////////////////////////////////////////////////////////
0: 
0:     Option helpOpt = obuilder.withLongName("help").withDescription("Print out help").withShortName("h").create();
0: 
0:             chunkSizeOpt).withOption(charsetOpt).withOption(outputDirOpt).withOption(helpOpt)
0: 
0:     try {
0:       Parser parser = new Parser();
0:       parser.setGroup(group);
0:       CommandLine cmdLine = parser.parse(args);
0:       if (cmdLine.hasOption(helpOpt)) {
0:         CommandLineUtil.printHelp(group);
0:         return;
0:       }
0:       File parentDir = new File((String) cmdLine.getValue(parentOpt));
0:       String outputDir = (String) cmdLine.getValue(outputDirOpt);
0: 
0:       int chunkSize = 64;
0:       if (cmdLine.hasOption(chunkSizeOpt)) {
0:         chunkSize = Integer.parseInt((String) cmdLine.getValue(chunkSizeOpt));
0:       }
0: 
0:       String prefix = "";
0:       if (cmdLine.hasOption(keyPrefixOpt)) {
0:         prefix = (String) cmdLine.getValue(keyPrefixOpt);
0:       }
0:       Charset charset = Charset.forName((String) cmdLine.getValue(charsetOpt));
0:       SequenceFilesFromDirectory dir = new SequenceFilesFromDirectory();
0: 
0:       dir.createSequenceFiles(parentDir, outputDir, prefix, chunkSize, charset);
0:     } catch (OptionException e) {
0:       log.error("Exception", e);
0:       CommandLineUtil.printHelp(group);
0:     } 
author:Isabel Drost
-------------------------------------------------------------------------------
commit:466f319
/////////////////////////////////////////////////////////////////////////
0: import java.util.HashMap;
1: import java.util.Map;
/////////////////////////////////////////////////////////////////////////
1: public class SequenceFilesFromDirectory extends AbstractJob {
/////////////////////////////////////////////////////////////////////////
0:                   String keyPrefix,
0:                   Map<String, String> options,
0:                   Path output)
0:     ChunkedWriter writer = new ChunkedWriter(conf, Integer.parseInt(options.get(CHUNK_SIZE_OPTION[0])), output);
0:     SequenceFilesFromDirectoryFilter pathFilter;
0: 
0:     String fileFilterClassName = options.get(FILE_FILTER_CLASS_OPTION[0]);
0:       pathFilter = new PrefixAdditionFilter(conf, keyPrefix, options, writer);
0:       Class<? extends SequenceFilesFromDirectoryFilter> pathFilterClass = Class.forName(fileFilterClassName).asSubclass(SequenceFilesFromDirectoryFilter.class);
0:       Constructor<? extends SequenceFilesFromDirectoryFilter> constructor =
0:           pathFilterClass.getConstructor(Configuration.class, String.class, Map.class, ChunkedWriter.class);
0:       pathFilter = constructor.newInstance(conf, keyPrefix, options, writer);
/////////////////////////////////////////////////////////////////////////
0:     addOptions();    
0:     if (parseArguments(args) == null) {
0:       return -1;
0:     }
0:    
1:     Map<String, String> options = parseOptions();
0:     Path input = getInputPath();
0:     Path output = getOutputPath();
0:     if (hasOption(DefaultOptionCreator.OVERWRITE_OPTION)) {
0:       HadoopUtil.overwriteOutput(output);
0:     }
0:     String keyPrefix = getOption(KEY_PREFIX_OPTION[0]);
0: 
0:     run(getConf(), keyPrefix, options, input, output);
1:     return 0;
0:   }
0: 
1:   /**
1:    * Override this method in order to add additional options to the command line of the SequenceFileFromDirectory job.
1:    * Do not forget to call super() otherwise all standard options (input/output dirs etc) will not be available.
0:    * */
1:   protected void addOptions() {
/////////////////////////////////////////////////////////////////////////
0:   }
0: 
1:   /**
1:    * Override this method in order to parse your additional options from the command line. Do not forget to call
1:    * super() otherwise standard options (input/output dirs etc) will not be available.
0:    */
0:   protected Map<String, String> parseOptions() throws IOException {
0:     Map<String, String> options = new HashMap<String, String>();
1:     options.put(CHUNK_SIZE_OPTION[0], getOption(CHUNK_SIZE_OPTION[0]));
1:     options.put(FILE_FILTER_CLASS_OPTION[0], getOption(FILE_FILTER_CLASS_OPTION[0]));
1:     options.put(CHARSET_OPTION[0], getOption(CHARSET_OPTION[0]));
1:     return options;
commit:614fc76
/////////////////////////////////////////////////////////////////////////
0: import java.lang.reflect.Constructor;
0: import java.lang.reflect.InvocationTargetException;
/////////////////////////////////////////////////////////////////////////
0:                                   Charset charset,
0:                                   String filter) throws IOException, ClassNotFoundException, NoSuchMethodException, InvocationTargetException, IllegalAccessException, InstantiationException {
0:     if ("PrefixAdditionFilter".equals(filter)) {
0:       parentDir.listFiles(new PrefixAdditionFilter(prefix, writer, charset));
0:     } else {
0:       Class filterClass = Class.forName(filter);
0:       Constructor<FileFilter> constructor = filterClass.getConstructor(String.class, ChunkedWriter.class, Charset.class);
0:       FileFilter fileFilter = constructor.newInstance(prefix, writer, charset);
0:       parentDir.listFiles(fileFilter);
0:     }
/////////////////////////////////////////////////////////////////////////
0: 
0:     Option fileFilterOpt = obuilder.withLongName("fileFilterClass").withArgument(
0:       abuilder.withName("fileFilterClass").withMinimum(1).withMaximum(1).create()).withDescription(
0:       "The name of the class to use for file parsing. Default: PrefixAdditionFilter").withShortName("filter").create();
0:       charsetOpt).withOption(outputDirOpt).withOption(fileFilterOpt).withOption(helpOpt).withOption(parentOpt).create();
/////////////////////////////////////////////////////////////////////////
0: 
0:       String filter = "PrefixAdditionFilter";
0:       if (cmdLine.hasOption(fileFilterOpt)) {
0:         filter = (String) cmdLine.getValue(fileFilterOpt);
0:       }
0: 
0:       dir.createSequenceFiles(parentDir, outputDir, prefix, chunkSize, charset, filter);
author:Jeff Eastman
-------------------------------------------------------------------------------
commit:d29a980
/////////////////////////////////////////////////////////////////////////
1: import org.apache.hadoop.util.ToolRunner;
1: import org.apache.mahout.common.AbstractJob;
/////////////////////////////////////////////////////////////////////////
0: public final class SequenceFilesFromDirectory extends AbstractJob {
0: 
0: 
0: 
0:   public void createSequenceFiles(File parentDir, String outputDir, String prefix, int chunkSizeInMB, Charset charset, String filter)
0:       throws IOException, ClassNotFoundException, NoSuchMethodException, InvocationTargetException, IllegalAccessException,
0:       InstantiationException {
/////////////////////////////////////////////////////////////////////////
0: 
0: 
0: 
0: 
0: 
0: 
0: 
0: 
/////////////////////////////////////////////////////////////////////////
0: 
0: 
0: 
0: 
0: 
0: 
0: 
0: 
0: 
0: 
0:         current.listFiles(new PrefixAdditionFilter(prefix + File.separator + current.getName(), writer, charset));
/////////////////////////////////////////////////////////////////////////
0: 
/////////////////////////////////////////////////////////////////////////
0: 
0: 
1:     ToolRunner.run(new SequenceFilesFromDirectory(), args);
0:   }
0: 
0:   @Override
0:   public int run(String[] args) throws Exception {
0:     Option parentOpt = obuilder.withLongName("input").withRequired(true).withArgument(abuilder.withName("input").withMinimum(1)
0:         .withMaximum(1).create()).withDescription("The input dir containing the documents").withShortName("i").create();
0: 
0:     Option outputDirOpt = obuilder.withLongName("output").withRequired(true).withArgument(abuilder.withName("output")
0:         .withMinimum(1).withMaximum(1).create()).withDescription("The output directory").withShortName("o").create();
0: 
0:     Option chunkSizeOpt = obuilder.withLongName("chunkSize").withArgument(abuilder.withName("chunkSize").withMinimum(1)
0:         .withMaximum(1).create()).withDescription("The chunkSize in MegaBytes. Defaults to 64").withShortName("chunk").create();
0: 
0:     Option keyPrefixOpt = obuilder.withLongName("keyPrefix").withArgument(abuilder.withName("keyPrefix").withMinimum(1)
0:         .withMaximum(1).create()).withDescription("The prefix to be prepended to the key").withShortName("prefix").create();
0: 
0:     Option charsetOpt = obuilder.withLongName("charset").withRequired(true).withArgument(abuilder.withName("charset")
0:         .withMinimum(1).withMaximum(1).create()).withDescription("The name of the character encoding of the input files")
0:         .withShortName("c").create();
0: 
0:     Option fileFilterOpt = obuilder.withLongName("fileFilterClass").withArgument(abuilder.withName("fileFilterClass")
0:         .withMinimum(1).withMaximum(1).create())
0:         .withDescription("The name of the class to use for file parsing. Default: PrefixAdditionFilter").withShortName("filter")
0: 
0:     Option helpOpt = obuilder.withLongName("help").withDescription("Print out help").withShortName("h").create();
0: 
0:     Group group = gbuilder.withName("Options").withOption(keyPrefixOpt).withOption(chunkSizeOpt).withOption(charsetOpt)
0:         .withOption(outputDirOpt).withOption(fileFilterOpt).withOption(helpOpt).withOption(parentOpt).create();
0: 
/////////////////////////////////////////////////////////////////////////
0:         return -1;
0: 
0: 
/////////////////////////////////////////////////////////////////////////
0: 
0:     return 0;
commit:96d5647
author:Andrew L. Farris
-------------------------------------------------------------------------------
commit:db12089
author:Robin Anil
-------------------------------------------------------------------------------
commit:42ae840
/////////////////////////////////////////////////////////////////////////
0: import org.apache.mahout.common.FileLineIterable;
0: import org.slf4j.LoggerFactory;
1:  * Converts a directory of text documents into SequenceFiles of Specified chunkSize. This class takes in a
1:  * parent directory containing sub folders of text documents and recursively reads the files and creates the
0:  * {@link SequenceFile}s of docid => content. The docid is set as the relative path of the document from the
0:  * parent directory prepended with a specified prefix. You can also specify the input encoding of the text
0:  * files. The content of the output SequenceFiles are encoded as UTF-8 text.
0:   
0:   private static ChunkedWriter createNewChunkedWriter(int chunkSizeInMB, String outputDir) throws IOException {
/////////////////////////////////////////////////////////////////////////
0:       writer = new SequenceFile.Writer(fs, conf, getPath(currentChunkID), Text.class, Text.class);
/////////////////////////////////////////////////////////////////////////
0:         writer = new SequenceFile.Writer(fs, conf, getPath(currentChunkID++), Text.class, Text.class);
/////////////////////////////////////////////////////////////////////////
0:     public PrefixAdditionFilter(String prefix, ChunkedWriter writer, Charset charset) {
/////////////////////////////////////////////////////////////////////////
0:         current.listFiles(new PrefixAdditionFilter(prefix + File.separator + current.getName(), writer,
0:             charset));
0:           writer.write(prefix + File.separator + current.getName(), file.toString());
/////////////////////////////////////////////////////////////////////////
0:     Option parentOpt = obuilder.withLongName("input").withRequired(true).withArgument(
0:       abuilder.withName("input").withMinimum(1).withMaximum(1).create()).withDescription(
0:       "The input dir containing the documents").withShortName("i").create();
0:     Option outputDirOpt = obuilder.withLongName("output").withRequired(true).withArgument(
0:       abuilder.withName("output").withMinimum(1).withMaximum(1).create()).withDescription(
0:       "The output directory").withShortName("o").create();
0:     Option chunkSizeOpt = obuilder.withLongName("chunkSize").withArgument(
0:       abuilder.withName("chunkSize").withMinimum(1).withMaximum(1).create()).withDescription(
0:       "The chunkSize in MegaBytes. Defaults to 64").withShortName("chunk").create();
0:     Option keyPrefixOpt = obuilder.withLongName("keyPrefix").withArgument(
0:       abuilder.withName("keyPrefix").withMinimum(1).withMaximum(1).create()).withDescription(
0:       "The prefix to be prepended to the key").withShortName("prefix").create();
0:     Option charsetOpt = obuilder.withLongName("charset").withRequired(true).withArgument(
0:       abuilder.withName("charset").withMinimum(1).withMaximum(1).create()).withDescription(
0:       "The name of the character encoding of the input files").withShortName("c").create();
0:     
0:     Option helpOpt = obuilder.withLongName("help").withDescription("Print out help").withShortName("h")
0:         .create();
0:     
0:     Group group = gbuilder.withName("Options").withOption(keyPrefixOpt).withOption(chunkSizeOpt).withOption(
0:       charsetOpt).withOption(outputDirOpt).withOption(helpOpt).withOption(parentOpt).create();
0:     
/////////////////////////////////////////////////////////////////////////
0:       
0:       
0:       
0:     }
commit:b38e824
/////////////////////////////////////////////////////////////////////////
0:       currentChunkID = 0;
============================================================================