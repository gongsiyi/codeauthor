1:ffc7fab: /**
1:ffc7fab:  * Licensed to the Apache Software Foundation (ASF) under one or more
1:ffc7fab:  * contributor license agreements.  See the NOTICE file distributed with
1:ffc7fab:  * this work for additional information regarding copyright ownership.
1:ffc7fab:  * The ASF licenses this file to You under the Apache License, Version 2.0
1:ffc7fab:  * (the "License"); you may not use this file except in compliance with
1:ffc7fab:  * the License.  You may obtain a copy of the License at
2:ffc7fab:  *
1:ffc7fab:  *     http://www.apache.org/licenses/LICENSE-2.0
1:ffc7fab:  *
1:ffc7fab:  * Unless required by applicable law or agreed to in writing, software
1:ffc7fab:  * distributed under the License is distributed on an "AS IS" BASIS,
1:ffc7fab:  * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
1:ffc7fab:  * See the License for the specific language governing permissions and
1:ffc7fab:  * limitations under the License.
2:ffc7fab:  */
1:1499411: 
1:ffc7fab: package org.apache.mahout.math.hadoop.stochasticsvd;
1:5214b1d: 
1:ffc7fab: import java.io.Closeable;
1:ffc7fab: import java.io.IOException;
1:ffc7fab: import java.text.NumberFormat;
1:ffc7fab: import java.util.ArrayDeque;
1:ffc7fab: import java.util.Arrays;
1:ffc7fab: import java.util.Deque;
1:ffc7fab: import java.util.regex.Matcher;
1:5214b1d: 
1:58cc1ae: import com.google.common.collect.Lists;
1:67a531e: import org.apache.commons.lang3.Validate;
1:ffc7fab: import org.apache.hadoop.conf.Configuration;
1:8bac914: import org.apache.hadoop.filecache.DistributedCache;
1:8bac914: import org.apache.hadoop.fs.FileStatus;
1:ffc7fab: import org.apache.hadoop.fs.FileSystem;
1:ffc7fab: import org.apache.hadoop.fs.Path;
1:ffc7fab: import org.apache.hadoop.io.IntWritable;
1:ffc7fab: import org.apache.hadoop.io.LongWritable;
1:ffc7fab: import org.apache.hadoop.io.SequenceFile;
1:ffc7fab: import org.apache.hadoop.io.SequenceFile.CompressionType;
1:ffc7fab: import org.apache.hadoop.io.Writable;
1:ffc7fab: import org.apache.hadoop.mapred.JobConf;
1:ffc7fab: import org.apache.hadoop.mapred.OutputCollector;
1:ffc7fab: import org.apache.hadoop.mapreduce.Job;
1:ffc7fab: import org.apache.hadoop.mapreduce.Mapper;
1:ffc7fab: import org.apache.hadoop.mapreduce.Reducer;
1:ffc7fab: import org.apache.hadoop.mapreduce.lib.input.FileInputFormat;
1:ffc7fab: import org.apache.hadoop.mapreduce.lib.input.SequenceFileInputFormat;
1:ffc7fab: import org.apache.hadoop.mapreduce.lib.output.FileOutputFormat;
1:ffc7fab: import org.apache.hadoop.mapreduce.lib.output.SequenceFileOutputFormat;
1:6d9179e: import org.apache.mahout.common.HadoopUtil;
1:ffc7fab: import org.apache.mahout.common.IOUtils;
1:ffc7fab: import org.apache.mahout.common.Pair;
1:ffc7fab: import org.apache.mahout.common.iterator.sequencefile.PathType;
1:ffc7fab: import org.apache.mahout.common.iterator.sequencefile.SequenceFileDirIterator;
1:ffc7fab: import org.apache.mahout.math.SequentialAccessSparseVector;
1:ffc7fab: import org.apache.mahout.math.Vector;
1:ffc7fab: import org.apache.mahout.math.VectorWritable;
1:ffc7fab: import org.apache.mahout.math.hadoop.stochasticsvd.qr.QRFirstStep;
1:5214b1d: 
1:ffc7fab: /**
1:ffc7fab:  * Computes ABt products, then first step of QR which is pushed down to the
1:ffc7fab:  * reducer.
1:ffc7fab:  * 
1:5214b1d:  */
1:ffc7fab: @SuppressWarnings("deprecation")
1:229aeff: public final class ABtJob {
11:ffc7fab: 
1:ffc7fab:   public static final String PROP_BT_PATH = "ssvd.Bt.path";
1:8bac914:   public static final String PROP_BT_BROADCAST = "ssvd.Bt.broadcast";
1:ffc7fab: 
1:4194a28:   private ABtJob() {
1:4194a28:   }
1:4194a28: 
1:ffc7fab:   /**
1:ffc7fab:    * So, here, i preload A block into memory.
1:ffc7fab:    * <P>
1:ffc7fab:    * 
1:ffc7fab:    * A sparse matrix seems to be ideal for that but there are two reasons why i
1:ffc7fab:    * am not using it:
1:ffc7fab:    * <UL>
1:ffc7fab:    * <LI>1) I don't know the full block height. so i may need to reallocate it
1:ffc7fab:    * from time to time. Although this probably not a showstopper.
1:ffc7fab:    * <LI>2) I found that RandomAccessSparseVectors seem to take much more memory
1:ffc7fab:    * than the SequentialAccessSparseVectors.
1:ffc7fab:    * </UL>
1:ffc7fab:    * <P>
1:ffc7fab:    * 
1:ffc7fab:    */
1:ffc7fab:   public static class ABtMapper
1:ffc7fab:       extends
1:ffc7fab:       Mapper<Writable, VectorWritable, SplitPartitionedWritable, SparseRowBlockWritable> {
1:5214b1d: 
1:ffc7fab:     private SplitPartitionedWritable outKey;
1:87c15be:     private final Deque<Closeable> closeables = new ArrayDeque<>();
1:ffc7fab:     private SequenceFileDirIterator<IntWritable, VectorWritable> btInput;
1:ffc7fab:     private Vector[] aCols;
1:ffc7fab:     // private Vector[] yiRows;
1:ffc7fab:     // private VectorWritable outValue = new VectorWritable();
1:ffc7fab:     private int aRowCount;
1:ffc7fab:     private int kp;
1:ffc7fab:     private int blockHeight;
1:ffc7fab:     private SparseRowBlockAccumulator yiCollector;
1:ffc7fab: 
1:ffc7fab:     @Override
1:ffc7fab:     protected void map(Writable key, VectorWritable value, Context context)
1:ffc7fab:       throws IOException, InterruptedException {
1:ffc7fab: 
1:ffc7fab:       Vector vec = value.get();
1:ffc7fab: 
1:ffc7fab:       int vecSize = vec.size();
1:1499411:       if (aCols == null) {
1:ffc7fab:         aCols = new Vector[vecSize];
1:1499411:       } else if (aCols.length < vecSize) {
1:ffc7fab:         aCols = Arrays.copyOf(aCols, vecSize);
1:1499411:       }
1:ffc7fab: 
1:ffc7fab:       if (vec.isDense()) {
1:ffc7fab:         for (int i = 0; i < vecSize; i++) {
1:ffc7fab:           extendAColIfNeeded(i, aRowCount + 1);
1:ffc7fab:           aCols[i].setQuick(aRowCount, vec.getQuick(i));
1:5214b1d:         }
1:5214b1d:       } else {
1:dc62944:         for (Vector.Element vecEl : vec.nonZeroes()) {
1:ffc7fab:           int i = vecEl.index();
1:ffc7fab:           extendAColIfNeeded(i, aRowCount + 1);
1:ffc7fab:           aCols[i].setQuick(aRowCount, vecEl.get());
6:ffc7fab:         }
1:ffc7fab:       }
1:ffc7fab:       aRowCount++;
1:ffc7fab:     }
1:ffc7fab: 
1:ffc7fab:     private void extendAColIfNeeded(int col, int rowCount) {
1:1499411:       if (aCols[col] == null) {
1:ffc7fab:         aCols[col] =
1:1499411:           new SequentialAccessSparseVector(rowCount < 10000 ? 10000 : rowCount,
1:8bac914:                                            1);
1:1499411:       } else if (aCols[col].size() < rowCount) {
1:1499411:         Vector newVec =
1:ffc7fab:           new SequentialAccessSparseVector(rowCount << 1,
1:8bac914:                                            aCols[col].getNumNondefaultElements() << 1);
1:ffc7fab:         newVec.viewPart(0, aCols[col].size()).assign(aCols[col]);
1:ffc7fab:         aCols[col] = newVec;
1:ffc7fab:       }
1:ffc7fab:     }
1:ffc7fab: 
1:ffc7fab:     @Override
2:ffc7fab:     protected void cleanup(Context context) throws IOException,
3:ffc7fab:       InterruptedException {
1:ffc7fab:       try {
1:ffc7fab:         // yiRows= new Vector[aRowCount];
1:ffc7fab: 
1:ffc7fab:         int lastRowIndex = -1;
1:ffc7fab: 
1:1499411:         while (btInput.hasNext()) {
1:ffc7fab:           Pair<IntWritable, VectorWritable> btRec = btInput.next();
1:ffc7fab:           int btIndex = btRec.getFirst().get();
1:ffc7fab:           Vector btVec = btRec.getSecond().get();
1:ffc7fab:           Vector aCol;
1:1499411:           if (btIndex > aCols.length || (aCol = aCols[btIndex]) == null) {
1:ffc7fab:             continue;
1:1499411:           }
1:ffc7fab:           int j = -1;
1:dc62944:           for (Vector.Element aEl : aCol.nonZeroes()) {
1:ffc7fab:             j = aEl.index();
1:ffc7fab: 
1:ffc7fab:             // outKey.setTaskItemOrdinal(j);
1:ffc7fab:             // outValue.set(btVec.times(aEl.get())); // assign might work better
1:ffc7fab:             // // with memory after all.
1:ffc7fab:             // context.write(outKey, outValue);
1:ffc7fab:             yiCollector.collect((long) j, btVec.times(aEl.get()));
1:ffc7fab:           }
1:1499411:           if (lastRowIndex < j) {
1:ffc7fab:             lastRowIndex = j;
1:1499411:           }
1:ffc7fab:         }
1:ffc7fab:         aCols = null;
1:ffc7fab: 
1:ffc7fab:         // output empty rows if we never output partial products for them
1:ffc7fab:         // this happens in sparse matrices when last rows are all zeros
1:ffc7fab:         // and is subsequently causing shorter Q matrix row count which we
1:ffc7fab:         // probably don't want to repair there but rather here.
1:8bac914:         Vector yDummy = new SequentialAccessSparseVector(kp);
1:ffc7fab:         // outValue.set(yDummy);
1:ffc7fab:         for (lastRowIndex += 1; lastRowIndex < aRowCount; lastRowIndex++) {
1:ffc7fab:           // outKey.setTaskItemOrdinal(lastRowIndex);
1:ffc7fab:           // context.write(outKey, outValue);
1:ffc7fab: 
1:ffc7fab:           yiCollector.collect((long) lastRowIndex, yDummy);
1:ffc7fab:         }
1:ffc7fab: 
1:ffc7fab:       } finally {
1:ffc7fab:         IOUtils.close(closeables);
1:ffc7fab:       }
1:ffc7fab:     }
1:ffc7fab: 
1:ffc7fab:     @Override
1:ffc7fab:     protected void setup(final Context context) throws IOException,
1:ffc7fab:       InterruptedException {
1:ffc7fab: 
1:ffc7fab:       int k =
1:ffc7fab:         Integer.parseInt(context.getConfiguration().get(QRFirstStep.PROP_K));
1:ffc7fab:       int p =
1:ffc7fab:         Integer.parseInt(context.getConfiguration().get(QRFirstStep.PROP_P));
1:ffc7fab:       kp = k + p;
1:ffc7fab: 
1:ffc7fab:       outKey = new SplitPartitionedWritable(context);
1:ffc7fab:       String propBtPathStr = context.getConfiguration().get(PROP_BT_PATH);
1:ffc7fab:       Validate.notNull(propBtPathStr, "Bt input is not set");
1:ffc7fab:       Path btPath = new Path(propBtPathStr);
1:5214b1d: 
1:8bac914:       boolean distributedBt =
1:8bac914:         context.getConfiguration().get(PROP_BT_BROADCAST) != null;
1:8bac914: 
1:8bac914:       if (distributedBt) {
1:8bac914: 
1:6d9179e:         Path[] btFiles = HadoopUtil.getCachedFiles(context.getConfiguration());
1:8bac914: 
1:8bac914:         // DEBUG: stdout
1:564c3e1:         //System.out.printf("list of files: " + btFiles);
1:8bac914: 
1:229aeff:         StringBuilder btLocalPath = new StringBuilder();
1:8bac914:         for (Path btFile : btFiles) {
1:229aeff:           if (btLocalPath.length() > 0) {
1:229aeff:             btLocalPath.append(Path.SEPARATOR_CHAR);
1:564c3e1:           }
1:229aeff:           btLocalPath.append(btFile);
1:8bac914:         }
1:8bac914: 
1:8bac914:         btInput =
1:87c15be:           new SequenceFileDirIterator<>(new Path(btLocalPath.toString()),
1:8bac914:                                                                    PathType.LIST,
3:8bac914:                                                                    null,
1:8bac914:                                                                    null,
2:8bac914:                                                                    true,
2:8bac914:                                                                    context.getConfiguration());
1:8bac914: 
1:8bac914:       } else {
1:8bac914: 
1:8bac914:         btInput =
1:87c15be:           new SequenceFileDirIterator<>(btPath, PathType.GLOB, null, null, true, context.getConfiguration());
1:8bac914:       }
1:ffc7fab:       // TODO: how do i release all that stuff??
1:ffc7fab:       closeables.addFirst(btInput);
1:ffc7fab:       OutputCollector<LongWritable, SparseRowBlockWritable> yiBlockCollector =
1:ffc7fab:         new OutputCollector<LongWritable, SparseRowBlockWritable>() {
1:5214b1d: 
1:ffc7fab:           @Override
1:ffc7fab:           public void collect(LongWritable blockKey,
1:ffc7fab:                               SparseRowBlockWritable block) throws IOException {
1:ffc7fab:             outKey.setTaskItemOrdinal((int) blockKey.get());
1:ffc7fab:             try {
1:ffc7fab:               context.write(outKey, block);
1:ffc7fab:             } catch (InterruptedException exc) {
1:ffc7fab:               throw new IOException("Interrupted", exc);
1:5214b1d:             }
1:ffc7fab:           }
1:ffc7fab:         };
1:ffc7fab:       blockHeight =
1:ffc7fab:         context.getConfiguration().getInt(BtJob.PROP_OUTER_PROD_BLOCK_HEIGHT,
1:ffc7fab:                                           -1);
1:ffc7fab:       yiCollector =
1:ffc7fab:         new SparseRowBlockAccumulator(blockHeight, yiBlockCollector);
1:ffc7fab:       closeables.addFirst(yiCollector);
1:ffc7fab:     }
1:5214b1d: 
1:ffc7fab:   }
1:ffc7fab: 
1:ffc7fab:   /**
1:ffc7fab:    * QR first step pushed down to reducer.
1:ffc7fab:    * 
1:ffc7fab:    */
1:ffc7fab:   public static class QRReducer
1:ffc7fab:       extends
1:ffc7fab:       Reducer<SplitPartitionedWritable, SparseRowBlockWritable, SplitPartitionedWritable, VectorWritable> {
1:ffc7fab: 
1:ffc7fab:     // hack: partition number formats in hadoop, copied. this may stop working
1:ffc7fab:     // if it gets
1:ffc7fab:     // out of sync with newer hadoop version. But unfortunately rules of forming
1:ffc7fab:     // output file names are not sufficiently exposed so we need to hack it
1:ffc7fab:     // if we write the same split output from either mapper or reducer.
1:ffc7fab:     // alternatively, we probably can replace it by our own output file namnig
1:ffc7fab:     // management
1:ffc7fab:     // completely and bypass MultipleOutputs entirely.
1:ffc7fab: 
1:8bac914:     private static final NumberFormat NUMBER_FORMAT =
1:8bac914:       NumberFormat.getInstance();
1:ffc7fab:     static {
1:ffc7fab:       NUMBER_FORMAT.setMinimumIntegerDigits(5);
1:ffc7fab:       NUMBER_FORMAT.setGroupingUsed(false);
1:ffc7fab:     }
1:ffc7fab: 
1:58cc1ae:     private final Deque<Closeable> closeables = Lists.newLinkedList();
1:ffc7fab:     protected final SparseRowBlockWritable accum = new SparseRowBlockWritable();
1:ffc7fab: 
1:ffc7fab:     protected int blockHeight;
1:ffc7fab: 
1:ffc7fab:     protected int lastTaskId = -1;
1:ffc7fab: 
1:ffc7fab:     protected OutputCollector<Writable, DenseBlockWritable> qhatCollector;
1:ffc7fab:     protected OutputCollector<Writable, VectorWritable> rhatCollector;
1:ffc7fab:     protected QRFirstStep qr;
1:ffc7fab: 
1:ffc7fab:     @Override
1:ffc7fab:     protected void setup(Context context) throws IOException,
1:ffc7fab:       InterruptedException {
1:ffc7fab:       blockHeight =
1:ffc7fab:         context.getConfiguration().getInt(BtJob.PROP_OUTER_PROD_BLOCK_HEIGHT,
1:ffc7fab:                                           -1);
1:ffc7fab: 
1:ffc7fab:     }
1:ffc7fab: 
1:ffc7fab:     protected void setupBlock(Context context, SplitPartitionedWritable spw)
1:ffc7fab:       throws InterruptedException, IOException {
1:ffc7fab:       IOUtils.close(closeables);
1:ffc7fab:       qhatCollector =
1:ffc7fab:         createOutputCollector(QJob.OUTPUT_QHAT,
1:ffc7fab:                               spw,
1:ffc7fab:                               context,
1:ffc7fab:                               DenseBlockWritable.class);
1:ffc7fab:       rhatCollector =
1:ffc7fab:         createOutputCollector(QJob.OUTPUT_RHAT,
1:ffc7fab:                               spw,
1:ffc7fab:                               context,
1:ffc7fab:                               VectorWritable.class);
1:ffc7fab:       qr =
1:ffc7fab:         new QRFirstStep(context.getConfiguration(),
1:ffc7fab:                         qhatCollector,
1:ffc7fab:                         rhatCollector);
1:ffc7fab:       closeables.addFirst(qr);
1:ffc7fab:       lastTaskId = spw.getTaskId();
1:ffc7fab: 
1:ffc7fab:     }
1:ffc7fab: 
1:ffc7fab:     @Override
1:ffc7fab:     protected void reduce(SplitPartitionedWritable key,
1:ffc7fab:                           Iterable<SparseRowBlockWritable> values,
1:ffc7fab:                           Context context) throws IOException,
1:ffc7fab:       InterruptedException {
1:ffc7fab: 
1:ffc7fab:       accum.clear();
1:1499411:       for (SparseRowBlockWritable bw : values) {
1:ffc7fab:         accum.plusBlock(bw);
1:1499411:       }
1:ffc7fab: 
1:ffc7fab:       if (key.getTaskId() != lastTaskId) {
1:ffc7fab:         setupBlock(context, key);
1:ffc7fab:       }
1:ffc7fab: 
1:ffc7fab:       long blockBase = key.getTaskItemOrdinal() * blockHeight;
1:ffc7fab:       for (int k = 0; k < accum.getNumRows(); k++) {
1:ffc7fab:         Vector yiRow = accum.getRows()[k];
1:ffc7fab:         key.setTaskItemOrdinal(blockBase + accum.getRowIndices()[k]);
1:ffc7fab:         qr.collect(key, yiRow);
1:ffc7fab:       }
1:ffc7fab: 
1:ffc7fab:     }
1:ffc7fab: 
1:ffc7fab:     private Path getSplitFilePath(String name,
1:ffc7fab:                                   SplitPartitionedWritable spw,
1:ffc7fab:                                   Context context) throws InterruptedException,
1:ffc7fab:       IOException {
1:ffc7fab:       String uniqueFileName = FileOutputFormat.getUniqueFile(context, name, "");
1:ffc7fab:       uniqueFileName = uniqueFileName.replaceFirst("-r-", "-m-");
1:ffc7fab:       uniqueFileName =
1:8bac914:         uniqueFileName.replaceFirst("\\d+$",
1:8bac914:                                     Matcher.quoteReplacement(NUMBER_FORMAT.format(spw.getTaskId())));
1:ffc7fab:       return new Path(FileOutputFormat.getWorkOutputPath(context),
1:ffc7fab:                       uniqueFileName);
1:ffc7fab:     }
1:ffc7fab: 
1:ffc7fab:     /**
1:ffc7fab:      * key doesn't matter here, only value does. key always gets substituted by
1:ffc7fab:      * SPW.
1:ffc7fab:      */
1:229aeff:     private <K,V> OutputCollector<K,V> createOutputCollector(String name,
1:229aeff:                                                              final SplitPartitionedWritable spw,
1:229aeff:                                                              Context ctx,
1:229aeff:                                                              Class<V> valueClass)
1:229aeff:       throws IOException, InterruptedException {
1:1499411:       Path outputPath = getSplitFilePath(name, spw, ctx);
1:ffc7fab:       final SequenceFile.Writer w =
1:1de8cec:         SequenceFile.createWriter(FileSystem.get(outputPath.toUri(), ctx.getConfiguration()),
1:ffc7fab:                                   ctx.getConfiguration(),
1:ffc7fab:                                   outputPath,
1:ffc7fab:                                   SplitPartitionedWritable.class,
1:ffc7fab:                                   valueClass);
1:ffc7fab:       closeables.addFirst(w);
1:ffc7fab:       return new OutputCollector<K, V>() {
1:ffc7fab:         @Override
1:ffc7fab:         public void collect(K key, V val) throws IOException {
1:ffc7fab:           w.append(spw, val);
1:ffc7fab:         }
1:ffc7fab:       };
1:ffc7fab:     }
1:ffc7fab: 
1:ffc7fab:     @Override
1:229aeff:     protected void cleanup(Context context) throws IOException, InterruptedException {
1:ffc7fab:       IOUtils.close(closeables);
1:ffc7fab:     }
1:ffc7fab: 
1:ffc7fab:   }
1:ffc7fab: 
1:ffc7fab:   public static void run(Configuration conf,
1:ffc7fab:                          Path[] inputAPaths,
1:ffc7fab:                          Path inputBtGlob,
1:ffc7fab:                          Path outputPath,
1:ffc7fab:                          int aBlockRows,
1:ffc7fab:                          int minSplitSize,
1:ffc7fab:                          int k,
1:ffc7fab:                          int p,
1:ffc7fab:                          int outerProdBlockHeight,
1:8bac914:                          int numReduceTasks,
1:8bac914:                          boolean broadcastBInput)
1:8bac914:     throws ClassNotFoundException, InterruptedException, IOException {
1:ffc7fab: 
1:ffc7fab:     JobConf oldApiJob = new JobConf(conf);
1:ffc7fab: 
1:ffc7fab:     // MultipleOutputs
1:ffc7fab:     // .addNamedOutput(oldApiJob,
1:ffc7fab:     // QJob.OUTPUT_QHAT,
1:ffc7fab:     // org.apache.hadoop.mapred.SequenceFileOutputFormat.class,
1:ffc7fab:     // SplitPartitionedWritable.class,
1:ffc7fab:     // DenseBlockWritable.class);
1:ffc7fab:     //
1:ffc7fab:     // MultipleOutputs
1:ffc7fab:     // .addNamedOutput(oldApiJob,
1:ffc7fab:     // QJob.OUTPUT_RHAT,
1:ffc7fab:     // org.apache.hadoop.mapred.SequenceFileOutputFormat.class,
1:ffc7fab:     // SplitPartitionedWritable.class,
1:ffc7fab:     // VectorWritable.class);
1:ffc7fab: 
1:ffc7fab:     Job job = new Job(oldApiJob);
1:ffc7fab:     job.setJobName("ABt-job");
1:ffc7fab:     job.setJarByClass(ABtJob.class);
1:ffc7fab: 
1:ffc7fab:     job.setInputFormatClass(SequenceFileInputFormat.class);
1:ffc7fab:     FileInputFormat.setInputPaths(job, inputAPaths);
1:ffc7fab:     if (minSplitSize > 0) {
1:ffc7fab:       FileInputFormat.setMinInputSplitSize(job, minSplitSize);
1:ffc7fab:     }
1:ffc7fab: 
1:ffc7fab:     FileOutputFormat.setOutputPath(job, outputPath);
1:ffc7fab: 
1:ffc7fab:     SequenceFileOutputFormat.setOutputCompressionType(job,
1:ffc7fab:                                                       CompressionType.BLOCK);
1:ffc7fab: 
1:ffc7fab:     job.setMapOutputKeyClass(SplitPartitionedWritable.class);
1:ffc7fab:     job.setMapOutputValueClass(SparseRowBlockWritable.class);
1:ffc7fab: 
1:ffc7fab:     job.setOutputKeyClass(SplitPartitionedWritable.class);
1:ffc7fab:     job.setOutputValueClass(VectorWritable.class);
1:ffc7fab: 
1:ffc7fab:     job.setMapperClass(ABtMapper.class);
1:ffc7fab:     job.setCombinerClass(BtJob.OuterProductCombiner.class);
1:ffc7fab:     job.setReducerClass(QRReducer.class);
1:ffc7fab: 
1:ffc7fab:     job.getConfiguration().setInt(QJob.PROP_AROWBLOCK_SIZE, aBlockRows);
1:ffc7fab:     job.getConfiguration().setInt(BtJob.PROP_OUTER_PROD_BLOCK_HEIGHT,
1:ffc7fab:                                   outerProdBlockHeight);
1:ffc7fab:     job.getConfiguration().setInt(QRFirstStep.PROP_K, k);
1:ffc7fab:     job.getConfiguration().setInt(QRFirstStep.PROP_P, p);
1:ffc7fab:     job.getConfiguration().set(PROP_BT_PATH, inputBtGlob.toString());
1:ffc7fab: 
1:ffc7fab:     // number of reduce tasks doesn't matter. we don't actually
1:ffc7fab:     // send anything to reducers.
1:ffc7fab: 
1:ffc7fab:     job.setNumReduceTasks(numReduceTasks);
1:5214b1d: 
1:8bac914:     // broadcast Bt files if required.
1:8bac914:     if (broadcastBInput) {
1:8bac914:       job.getConfiguration().set(PROP_BT_BROADCAST, "y");
1:8bac914: 
1:1de8cec:       FileSystem fs = FileSystem.get(inputBtGlob.toUri(), conf);
1:8bac914:       FileStatus[] fstats = fs.globStatus(inputBtGlob);
1:8bac914:       if (fstats != null) {
1:8bac914:         for (FileStatus fstat : fstats) {
1:8bac914:           /*
1:8bac914:            * new api is not enabled yet in our dependencies at this time, still
1:8bac914:            * using deprecated one
1:8bac914:            */
1:8bac914:           DistributedCache.addCacheFile(fstat.getPath().toUri(), conf);
1:8bac914:         }
1:8bac914:       }
1:8bac914:     }
1:8bac914: 
1:ffc7fab:     job.submit();
1:ffc7fab:     job.waitForCompletion(false);
1:5214b1d: 
1:ffc7fab:     if (!job.isSuccessful()) {
1:ffc7fab:       throw new IOException("ABt job unsuccessful.");
1:5214b1d:     }
1:ffc7fab: 
1:5214b1d:   }
1:ffc7fab: 
1:5214b1d: }
============================================================================
author:pferrel
-------------------------------------------------------------------------------
commit:b988c49
author:Suneel Marthi
-------------------------------------------------------------------------------
commit:87c15be
/////////////////////////////////////////////////////////////////////////
1:     private final Deque<Closeable> closeables = new ArrayDeque<>();
/////////////////////////////////////////////////////////////////////////
1:           new SequenceFileDirIterator<>(new Path(btLocalPath.toString()),
/////////////////////////////////////////////////////////////////////////
1:           new SequenceFileDirIterator<>(btPath, PathType.GLOB, null, null, true, context.getConfiguration());
/////////////////////////////////////////////////////////////////////////
author:frankscholten
-------------------------------------------------------------------------------
commit:1a42d85
author:Sean R. Owen
-------------------------------------------------------------------------------
commit:335a993
/////////////////////////////////////////////////////////////////////////
commit:229aeff
/////////////////////////////////////////////////////////////////////////
1: public final class ABtJob {
/////////////////////////////////////////////////////////////////////////
1:         StringBuilder btLocalPath = new StringBuilder();
1:           if (btLocalPath.length() > 0) {
1:             btLocalPath.append(Path.SEPARATOR_CHAR);
1:           btLocalPath.append(btFile);
0:           new SequenceFileDirIterator<IntWritable, VectorWritable>(new Path(btLocalPath.toString()),
/////////////////////////////////////////////////////////////////////////
1:     private <K,V> OutputCollector<K,V> createOutputCollector(String name,
1:                                                              final SplitPartitionedWritable spw,
1:                                                              Context ctx,
1:                                                              Class<V> valueClass)
1:       throws IOException, InterruptedException {
/////////////////////////////////////////////////////////////////////////
1:     protected void cleanup(Context context) throws IOException, InterruptedException {
commit:1de8cec
/////////////////////////////////////////////////////////////////////////
/////////////////////////////////////////////////////////////////////////
1:         SequenceFile.createWriter(FileSystem.get(outputPath.toUri(), ctx.getConfiguration()),
/////////////////////////////////////////////////////////////////////////
1:       FileSystem fs = FileSystem.get(inputBtGlob.toUri(), conf);
commit:564c3e1
/////////////////////////////////////////////////////////////////////////
1:         //System.out.printf("list of files: " + btFiles);
0:           if (!btLocalPath.isEmpty()) {
1:           }
commit:4194a28
/////////////////////////////////////////////////////////////////////////
1:   private ABtJob() {
1:   }
1: 
commit:1499411
/////////////////////////////////////////////////////////////////////////
1: 
/////////////////////////////////////////////////////////////////////////
0:     private final Deque<Closeable> closeables = new ArrayDeque<Closeable>();
/////////////////////////////////////////////////////////////////////////
1:       if (aCols == null) {
1:       } else if (aCols.length < vecSize) {
1:       }
/////////////////////////////////////////////////////////////////////////
1:       if (aCols[col] == null) {
1:             new SequentialAccessSparseVector(rowCount < 10000 ? 10000 : rowCount,
0:                                              16);
1:       } else if (aCols[col].size() < rowCount) {
1:         Vector newVec =
/////////////////////////////////////////////////////////////////////////
1:         while (btInput.hasNext()) {
1:           if (btIndex > aCols.length || (aCol = aCols[btIndex]) == null) {
1:           }
0:               .hasNext(); ) {
/////////////////////////////////////////////////////////////////////////
1:           if (lastRowIndex < j) {
1:           }
/////////////////////////////////////////////////////////////////////////
0:         Vector yDummy =
/////////////////////////////////////////////////////////////////////////
1:       for (SparseRowBlockWritable bw : values) {
1:       }
/////////////////////////////////////////////////////////////////////////
1:       Path outputPath = getSplitFilePath(name, spw, ctx);
author:Sebastian Schelter
-------------------------------------------------------------------------------
commit:58cc1ae
/////////////////////////////////////////////////////////////////////////
1: import com.google.common.collect.Lists;
/////////////////////////////////////////////////////////////////////////
1:     private final Deque<Closeable> closeables = Lists.newLinkedList();
commit:6d9179e
/////////////////////////////////////////////////////////////////////////
1: import org.apache.mahout.common.HadoopUtil;
/////////////////////////////////////////////////////////////////////////
1:         Path[] btFiles = HadoopUtil.getCachedFiles(context.getConfiguration());
author:Jacob Alexander Mannix
-------------------------------------------------------------------------------
commit:dc62944
/////////////////////////////////////////////////////////////////////////
/////////////////////////////////////////////////////////////////////////
1:         for (Vector.Element vecEl : vec.nonZeroes()) {
/////////////////////////////////////////////////////////////////////////
1:           for (Vector.Element aEl : aCol.nonZeroes()) {
author:smarthi
-------------------------------------------------------------------------------
commit:67a531e
/////////////////////////////////////////////////////////////////////////
1: import org.apache.commons.lang3.Validate;
author:Dmitriy Lyubimov
-------------------------------------------------------------------------------
commit:8bac914
/////////////////////////////////////////////////////////////////////////
1: import org.apache.hadoop.filecache.DistributedCache;
1: import org.apache.hadoop.fs.FileStatus;
/////////////////////////////////////////////////////////////////////////
1:   public static final String PROP_BT_BROADCAST = "ssvd.Bt.broadcast";
/////////////////////////////////////////////////////////////////////////
0:         for (Iterator<Vector.Element> vecIter = vec.iterateNonZero(); vecIter.hasNext();) {
/////////////////////////////////////////////////////////////////////////
0:           new SequentialAccessSparseVector(rowCount < 10000 ? 10000 : rowCount,
1:                                            1);
1:                                            aCols[col].getNumNondefaultElements() << 1);
/////////////////////////////////////////////////////////////////////////
0:           for (Iterator<Vector.Element> aColIter = aCol.iterateNonZero(); aColIter.hasNext();) {
/////////////////////////////////////////////////////////////////////////
1:         Vector yDummy = new SequentialAccessSparseVector(kp);
/////////////////////////////////////////////////////////////////////////
1:       boolean distributedBt =
1:         context.getConfiguration().get(PROP_BT_BROADCAST) != null;
1: 
1:       if (distributedBt) {
1: 
0:         Path[] btFiles =
0:           DistributedCache.getLocalCacheFiles(context.getConfiguration());
1: 
1:         // DEBUG: stdout
0:         System.out.printf("list of files: " + btFiles);
1: 
0:         String btLocalPath = "";
1:         for (Path btFile : btFiles) {
0:           if (btLocalPath.length() > 0)
0:             btLocalPath += Path.SEPARATOR_CHAR;
0:           btLocalPath += btFile;
1:         }
1: 
1:         btInput =
0:           new SequenceFileDirIterator<IntWritable, VectorWritable>(new Path(btLocalPath),
1:                                                                    PathType.LIST,
1:                                                                    null,
1:                                                                    null,
1:                                                                    true,
1:                                                                    context.getConfiguration());
1: 
1:       } else {
1: 
1:         btInput =
0:           new SequenceFileDirIterator<IntWritable, VectorWritable>(btPath,
0:                                                                    PathType.GLOB,
1:                                                                    null,
1:                                                                    null,
1:                                                                    true,
1:                                                                    context.getConfiguration());
1:       }
/////////////////////////////////////////////////////////////////////////
1:     private static final NumberFormat NUMBER_FORMAT =
1:       NumberFormat.getInstance();
/////////////////////////////////////////////////////////////////////////
1:         uniqueFileName.replaceFirst("\\d+$",
1:                                     Matcher.quoteReplacement(NUMBER_FORMAT.format(spw.getTaskId())));
/////////////////////////////////////////////////////////////////////////
1:                          int numReduceTasks,
1:                          boolean broadcastBInput)
1:     throws ClassNotFoundException, InterruptedException, IOException {
/////////////////////////////////////////////////////////////////////////
1:     // broadcast Bt files if required.
1:     if (broadcastBInput) {
1:       job.getConfiguration().set(PROP_BT_BROADCAST, "y");
1: 
0:       FileSystem fs = FileSystem.get(conf);
1:       FileStatus[] fstats = fs.globStatus(inputBtGlob);
1:       if (fstats != null) {
1:         for (FileStatus fstat : fstats) {
1:           /*
1:            * new api is not enabled yet in our dependencies at this time, still
1:            * using deprecated one
1:            */
1:           DistributedCache.addCacheFile(fstat.getPath().toUri(), conf);
1:         }
1:       }
1:     }
1: 
commit:ebeade9
/////////////////////////////////////////////////////////////////////////
/////////////////////////////////////////////////////////////////////////
/////////////////////////////////////////////////////////////////////////
0:         for (Iterator<Vector.Element> vecIter = vec.iterateNonZero(); vecIter
0:           .hasNext();) {
/////////////////////////////////////////////////////////////////////////
0:             new SequentialAccessSparseVector(rowCount < 10000 ? 10000 : rowCount,
0:                                              16);
0:                                            aCols[col]
0:                                              .getNumNondefaultElements() << 1);
/////////////////////////////////////////////////////////////////////////
0:           for (Iterator<Vector.Element> aColIter = aCol.iterateNonZero(); aColIter
0:               .hasNext(); ) {
/////////////////////////////////////////////////////////////////////////
0:         Vector yDummy =
0:           new SequentialAccessSparseVector(kp);
/////////////////////////////////////////////////////////////////////////
0:       btInput =
0:         new SequenceFileDirIterator<IntWritable, VectorWritable>(btPath,
0:                                                                  PathType.GLOB,
0:                                                                  null,
0:                                                                  null,
0:                                                                  true,
0:                                                                  context
0:                                                                    .getConfiguration());
/////////////////////////////////////////////////////////////////////////
0:     private static final NumberFormat NUMBER_FORMAT = NumberFormat
0:       .getInstance();
/////////////////////////////////////////////////////////////////////////
0:         uniqueFileName.replaceFirst("\\d+$", Matcher
0:           .quoteReplacement(NUMBER_FORMAT.format(spw.getTaskId())));
/////////////////////////////////////////////////////////////////////////
0:                          int numReduceTasks) throws ClassNotFoundException,
0:     InterruptedException, IOException {
/////////////////////////////////////////////////////////////////////////
commit:5214b1d
/////////////////////////////////////////////////////////////////////////
0: import org.apache.hadoop.filecache.DistributedCache;
0: import org.apache.hadoop.fs.FileStatus;
/////////////////////////////////////////////////////////////////////////
0:   public static final String PROP_BT_BROADCAST = "ssvd.Bt.broadcast";
/////////////////////////////////////////////////////////////////////////
0:         for (Iterator<Vector.Element> vecIter = vec.iterateNonZero(); vecIter.hasNext();) {
/////////////////////////////////////////////////////////////////////////
0:           new SequentialAccessSparseVector(rowCount < 10000 ? 10000 : rowCount,
0:                                            1);
0:                                            aCols[col].getNumNondefaultElements() << 1);
/////////////////////////////////////////////////////////////////////////
0:           for (Iterator<Vector.Element> aColIter = aCol.iterateNonZero(); aColIter.hasNext();) {
/////////////////////////////////////////////////////////////////////////
0:         Vector yDummy = new SequentialAccessSparseVector(kp);
/////////////////////////////////////////////////////////////////////////
0:       boolean distributedBt =
0:         context.getConfiguration().get(PROP_BT_BROADCAST) != null;
1: 
0:       if (distributedBt) {
1: 
0:         Path[] btFiles =
0:           DistributedCache.getLocalCacheFiles(context.getConfiguration());
1: 
0:         // DEBUG: stdout
0:         System.out.printf("list of files: " + btFiles);
1: 
0:         String btLocalPath = "";
0:         for (Path btFile : btFiles) {
0:           if (btLocalPath.length() > 0)
0:             btLocalPath += Path.SEPARATOR_CHAR;
0:           btLocalPath += btFile;
1:         }
1: 
0:         btInput =
0:           new SequenceFileDirIterator<IntWritable, VectorWritable>(new Path(btLocalPath),
0:                                                                    PathType.LIST,
0:                                                                    null,
0:                                                                    null,
0:                                                                    true,
0:                                                                    context.getConfiguration());
1: 
1:       } else {
1: 
0:         btInput =
0:           new SequenceFileDirIterator<IntWritable, VectorWritable>(btPath,
0:                                                                    PathType.GLOB,
0:                                                                    null,
0:                                                                    null,
0:                                                                    true,
0:                                                                    context.getConfiguration());
1:       }
/////////////////////////////////////////////////////////////////////////
0:     private static final NumberFormat NUMBER_FORMAT =
0:       NumberFormat.getInstance();
/////////////////////////////////////////////////////////////////////////
0:         uniqueFileName.replaceFirst("\\d+$",
0:                                     Matcher.quoteReplacement(NUMBER_FORMAT.format(spw.getTaskId())));
/////////////////////////////////////////////////////////////////////////
0:                          int numReduceTasks,
0:                          boolean broadcastBInput)
0:     throws ClassNotFoundException, InterruptedException, IOException {
/////////////////////////////////////////////////////////////////////////
0:     // broadcast Bt files if required.
0:     if (broadcastBInput) {
0:       job.getConfiguration().set(PROP_BT_BROADCAST, "y");
1: 
0:       FileSystem fs = FileSystem.get(conf);
0:       FileStatus[] fstats = fs.globStatus(inputBtGlob);
0:       if (fstats != null) {
0:         for (FileStatus fstat : fstats) {
0:           /*
0:            * new api is not enabled yet in our dependencies at this time, still
0:            * using deprecated one
1:            */
0:           DistributedCache.addCacheFile(fstat.getPath().toUri(), conf);
1:         }
1:       }
1:     }
1: 
commit:ffc7fab
/////////////////////////////////////////////////////////////////////////
1: /**
1:  * Licensed to the Apache Software Foundation (ASF) under one or more
1:  * contributor license agreements.  See the NOTICE file distributed with
1:  * this work for additional information regarding copyright ownership.
1:  * The ASF licenses this file to You under the Apache License, Version 2.0
1:  * (the "License"); you may not use this file except in compliance with
1:  * the License.  You may obtain a copy of the License at
1:  *
1:  *     http://www.apache.org/licenses/LICENSE-2.0
1:  *
1:  * Unless required by applicable law or agreed to in writing, software
1:  * distributed under the License is distributed on an "AS IS" BASIS,
1:  * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
1:  * See the License for the specific language governing permissions and
1:  * limitations under the License.
1:  */
1: package org.apache.mahout.math.hadoop.stochasticsvd;
1: 
1: import java.io.Closeable;
1: import java.io.IOException;
1: import java.text.NumberFormat;
1: import java.util.ArrayDeque;
1: import java.util.Arrays;
1: import java.util.Deque;
0: import java.util.Iterator;
0: import java.util.LinkedList;
1: import java.util.regex.Matcher;
1: 
0: import org.apache.commons.lang.Validate;
1: import org.apache.hadoop.conf.Configuration;
1: import org.apache.hadoop.fs.FileSystem;
1: import org.apache.hadoop.fs.Path;
1: import org.apache.hadoop.io.IntWritable;
1: import org.apache.hadoop.io.LongWritable;
1: import org.apache.hadoop.io.SequenceFile;
1: import org.apache.hadoop.io.SequenceFile.CompressionType;
1: import org.apache.hadoop.io.Writable;
1: import org.apache.hadoop.mapred.JobConf;
1: import org.apache.hadoop.mapred.OutputCollector;
1: import org.apache.hadoop.mapreduce.Job;
1: import org.apache.hadoop.mapreduce.Mapper;
1: import org.apache.hadoop.mapreduce.Reducer;
1: import org.apache.hadoop.mapreduce.lib.input.FileInputFormat;
1: import org.apache.hadoop.mapreduce.lib.input.SequenceFileInputFormat;
1: import org.apache.hadoop.mapreduce.lib.output.FileOutputFormat;
1: import org.apache.hadoop.mapreduce.lib.output.SequenceFileOutputFormat;
1: import org.apache.mahout.common.IOUtils;
1: import org.apache.mahout.common.Pair;
1: import org.apache.mahout.common.iterator.sequencefile.PathType;
1: import org.apache.mahout.common.iterator.sequencefile.SequenceFileDirIterator;
1: import org.apache.mahout.math.SequentialAccessSparseVector;
1: import org.apache.mahout.math.Vector;
1: import org.apache.mahout.math.VectorWritable;
1: import org.apache.mahout.math.hadoop.stochasticsvd.qr.QRFirstStep;
1: 
1: /**
1:  * Computes ABt products, then first step of QR which is pushed down to the
1:  * reducer.
1:  * 
1:  */
1: @SuppressWarnings("deprecation")
0: public class ABtJob {
1: 
1:   public static final String PROP_BT_PATH = "ssvd.Bt.path";
1: 
1:   /**
1:    * So, here, i preload A block into memory.
1:    * <P>
1:    * 
1:    * A sparse matrix seems to be ideal for that but there are two reasons why i
1:    * am not using it:
1:    * <UL>
1:    * <LI>1) I don't know the full block height. so i may need to reallocate it
1:    * from time to time. Although this probably not a showstopper.
1:    * <LI>2) I found that RandomAccessSparseVectors seem to take much more memory
1:    * than the SequentialAccessSparseVectors.
1:    * </UL>
1:    * <P>
1:    * 
1:    */
1:   public static class ABtMapper
1:       extends
1:       Mapper<Writable, VectorWritable, SplitPartitionedWritable, SparseRowBlockWritable> {
1: 
1:     private SplitPartitionedWritable outKey;
0:     private Deque<Closeable> closeables = new ArrayDeque<Closeable>();
1:     private SequenceFileDirIterator<IntWritable, VectorWritable> btInput;
1:     private Vector[] aCols;
1:     // private Vector[] yiRows;
1:     // private VectorWritable outValue = new VectorWritable();
1:     private int aRowCount;
1:     private int kp;
1:     private int blockHeight;
1:     private SparseRowBlockAccumulator yiCollector;
1: 
1:     @Override
1:     protected void map(Writable key, VectorWritable value, Context context)
1:       throws IOException, InterruptedException {
1: 
1:       Vector vec = value.get();
1: 
1:       int vecSize = vec.size();
0:       if (aCols == null)
1:         aCols = new Vector[vecSize];
0:       else if (aCols.length < vecSize)
1:         aCols = Arrays.copyOf(aCols, vecSize);
1: 
1:       if (vec.isDense()) {
1:         for (int i = 0; i < vecSize; i++) {
1:           extendAColIfNeeded(i, aRowCount + 1);
1:           aCols[i].setQuick(aRowCount, vec.getQuick(i));
1:         }
0:       } else {
0:         for (Iterator<Vector.Element> vecIter = vec.iterateNonZero(); vecIter
0:           .hasNext();) {
0:           Vector.Element vecEl = vecIter.next();
1:           int i = vecEl.index();
1:           extendAColIfNeeded(i, aRowCount + 1);
1:           aCols[i].setQuick(aRowCount, vecEl.get());
1:         }
1:       }
1:       aRowCount++;
1:     }
1: 
1:     private void extendAColIfNeeded(int col, int rowCount) {
0:       if (aCols[col] == null)
1:         aCols[col] =
0:           new SequentialAccessSparseVector(rowCount < 10000 ? 10000 : rowCount,
0:                                            16);
0:       else if (aCols[col].size() < rowCount) {
0:         SequentialAccessSparseVector newVec =
1:           new SequentialAccessSparseVector(rowCount << 1,
0:                                            aCols[col]
0:                                              .getNumNondefaultElements() << 1);
1:         newVec.viewPart(0, aCols[col].size()).assign(aCols[col]);
1:         aCols[col] = newVec;
1:       }
1:     }
1: 
1:     @Override
1:     protected void cleanup(Context context) throws IOException,
1:       InterruptedException {
1:       try {
1:         // yiRows= new Vector[aRowCount];
1: 
1:         int lastRowIndex = -1;
1: 
0:         for (; btInput.hasNext();) {
1:           Pair<IntWritable, VectorWritable> btRec = btInput.next();
1:           int btIndex = btRec.getFirst().get();
1:           Vector btVec = btRec.getSecond().get();
1:           Vector aCol;
0:           if (btIndex > aCols.length || null == (aCol = aCols[btIndex]))
1:             continue;
1:           int j = -1;
0:           for (Iterator<Vector.Element> aColIter = aCol.iterateNonZero(); aColIter
0:             .hasNext();) {
0:             Vector.Element aEl = aColIter.next();
1:             j = aEl.index();
1: 
1:             // outKey.setTaskItemOrdinal(j);
1:             // outValue.set(btVec.times(aEl.get())); // assign might work better
1:             // // with memory after all.
1:             // context.write(outKey, outValue);
1:             yiCollector.collect((long) j, btVec.times(aEl.get()));
1:           }
0:           if (lastRowIndex < j)
1:             lastRowIndex = j;
1:         }
1:         aCols = null;
1: 
1:         // output empty rows if we never output partial products for them
1:         // this happens in sparse matrices when last rows are all zeros
1:         // and is subsequently causing shorter Q matrix row count which we
1:         // probably don't want to repair there but rather here.
0:         SequentialAccessSparseVector yDummy =
0:           new SequentialAccessSparseVector(kp);
1:         // outValue.set(yDummy);
1:         for (lastRowIndex += 1; lastRowIndex < aRowCount; lastRowIndex++) {
1:           // outKey.setTaskItemOrdinal(lastRowIndex);
1:           // context.write(outKey, outValue);
1: 
1:           yiCollector.collect((long) lastRowIndex, yDummy);
1:         }
1: 
1:       } finally {
1:         IOUtils.close(closeables);
1:       }
1:     }
1: 
1:     @Override
1:     protected void setup(final Context context) throws IOException,
1:       InterruptedException {
1: 
1:       int k =
1:         Integer.parseInt(context.getConfiguration().get(QRFirstStep.PROP_K));
1:       int p =
1:         Integer.parseInt(context.getConfiguration().get(QRFirstStep.PROP_P));
1:       kp = k + p;
1: 
1:       outKey = new SplitPartitionedWritable(context);
1:       String propBtPathStr = context.getConfiguration().get(PROP_BT_PATH);
1:       Validate.notNull(propBtPathStr, "Bt input is not set");
1:       Path btPath = new Path(propBtPathStr);
1: 
0:       btInput =
0:         new SequenceFileDirIterator<IntWritable, VectorWritable>(btPath,
0:                                                                  PathType.GLOB,
0:                                                                  null,
0:                                                                  null,
0:                                                                  true,
0:                                                                  context
0:                                                                    .getConfiguration());
1:       // TODO: how do i release all that stuff??
1:       closeables.addFirst(btInput);
1:       OutputCollector<LongWritable, SparseRowBlockWritable> yiBlockCollector =
1:         new OutputCollector<LongWritable, SparseRowBlockWritable>() {
1: 
1:           @Override
1:           public void collect(LongWritable blockKey,
1:                               SparseRowBlockWritable block) throws IOException {
1:             outKey.setTaskItemOrdinal((int) blockKey.get());
1:             try {
1:               context.write(outKey, block);
1:             } catch (InterruptedException exc) {
1:               throw new IOException("Interrupted", exc);
1:             }
1:           }
1:         };
1:       blockHeight =
1:         context.getConfiguration().getInt(BtJob.PROP_OUTER_PROD_BLOCK_HEIGHT,
1:                                           -1);
1:       yiCollector =
1:         new SparseRowBlockAccumulator(blockHeight, yiBlockCollector);
1:       closeables.addFirst(yiCollector);
1:     }
1: 
1:   }
1: 
1:   /**
1:    * QR first step pushed down to reducer.
1:    * 
1:    */
1:   public static class QRReducer
1:       extends
1:       Reducer<SplitPartitionedWritable, SparseRowBlockWritable, SplitPartitionedWritable, VectorWritable> {
1: 
1:     // hack: partition number formats in hadoop, copied. this may stop working
1:     // if it gets
1:     // out of sync with newer hadoop version. But unfortunately rules of forming
1:     // output file names are not sufficiently exposed so we need to hack it
1:     // if we write the same split output from either mapper or reducer.
1:     // alternatively, we probably can replace it by our own output file namnig
1:     // management
1:     // completely and bypass MultipleOutputs entirely.
1: 
0:     private static final NumberFormat NUMBER_FORMAT = NumberFormat
0:       .getInstance();
1:     static {
1:       NUMBER_FORMAT.setMinimumIntegerDigits(5);
1:       NUMBER_FORMAT.setGroupingUsed(false);
1:     }
1: 
0:     private final Deque<Closeable> closeables = new LinkedList<Closeable>();
1:     protected final SparseRowBlockWritable accum = new SparseRowBlockWritable();
1: 
1:     protected int blockHeight;
1: 
0:     protected int accumSize;
1:     protected int lastTaskId = -1;
1: 
1:     protected OutputCollector<Writable, DenseBlockWritable> qhatCollector;
1:     protected OutputCollector<Writable, VectorWritable> rhatCollector;
1:     protected QRFirstStep qr;
1: 
1:     @Override
1:     protected void setup(Context context) throws IOException,
1:       InterruptedException {
1:       blockHeight =
1:         context.getConfiguration().getInt(BtJob.PROP_OUTER_PROD_BLOCK_HEIGHT,
1:                                           -1);
1: 
1:     }
1: 
1:     protected void setupBlock(Context context, SplitPartitionedWritable spw)
1:       throws InterruptedException, IOException {
1:       IOUtils.close(closeables);
1:       qhatCollector =
1:         createOutputCollector(QJob.OUTPUT_QHAT,
1:                               spw,
1:                               context,
1:                               DenseBlockWritable.class);
1:       rhatCollector =
1:         createOutputCollector(QJob.OUTPUT_RHAT,
1:                               spw,
1:                               context,
1:                               VectorWritable.class);
1:       qr =
1:         new QRFirstStep(context.getConfiguration(),
1:                         qhatCollector,
1:                         rhatCollector);
1:       closeables.addFirst(qr);
1:       lastTaskId = spw.getTaskId();
1: 
1:     }
1: 
1:     @Override
1:     protected void reduce(SplitPartitionedWritable key,
1:                           Iterable<SparseRowBlockWritable> values,
1:                           Context context) throws IOException,
1:       InterruptedException {
1: 
1:       accum.clear();
0:       for (SparseRowBlockWritable bw : values)
1:         accum.plusBlock(bw);
1: 
1:       if (key.getTaskId() != lastTaskId) {
1:         setupBlock(context, key);
1:       }
1: 
1:       long blockBase = key.getTaskItemOrdinal() * blockHeight;
1:       for (int k = 0; k < accum.getNumRows(); k++) {
1:         Vector yiRow = accum.getRows()[k];
1:         key.setTaskItemOrdinal(blockBase + accum.getRowIndices()[k]);
1:         qr.collect(key, yiRow);
1:       }
1: 
1:     }
1: 
1:     private Path getSplitFilePath(String name,
1:                                   SplitPartitionedWritable spw,
1:                                   Context context) throws InterruptedException,
1:       IOException {
1:       String uniqueFileName = FileOutputFormat.getUniqueFile(context, name, "");
1:       uniqueFileName = uniqueFileName.replaceFirst("-r-", "-m-");
1:       uniqueFileName =
0:         uniqueFileName.replaceFirst("\\d+$", Matcher
0:           .quoteReplacement(NUMBER_FORMAT.format(spw.getTaskId())));
1:       return new Path(FileOutputFormat.getWorkOutputPath(context),
1:                       uniqueFileName);
1:     }
1: 
1:     /**
1:      * key doesn't matter here, only value does. key always gets substituted by
1:      * SPW.
1:      * 
0:      * @param <K>
0:      *          bogus
0:      * @param <V>
0:      * @param name
0:      * @param spw
0:      * @param ctx
0:      * @param valueClass
0:      * @return
0:      * @throws IOException
0:      * @throws InterruptedException
1:      */
0:     private <K, V> OutputCollector<K, V>
0:         createOutputCollector(String name,
0:                               final SplitPartitionedWritable spw,
0:                               Context ctx,
0:                               Class<V> valueClass) throws IOException,
1:           InterruptedException {
0:       final Path outputPath = getSplitFilePath(name, spw, ctx);
1:       final SequenceFile.Writer w =
0:         SequenceFile.createWriter(FileSystem.get(ctx.getConfiguration()),
1:                                   ctx.getConfiguration(),
1:                                   outputPath,
1:                                   SplitPartitionedWritable.class,
1:                                   valueClass);
1:       closeables.addFirst(w);
1:       return new OutputCollector<K, V>() {
1:         @Override
1:         public void collect(K key, V val) throws IOException {
1:           w.append(spw, val);
1:         }
1:       };
1:     }
1: 
1:     @Override
1:     protected void cleanup(Context context) throws IOException,
1:       InterruptedException {
1: 
1:       IOUtils.close(closeables);
1:     }
1: 
1:   }
1: 
1:   public static void run(Configuration conf,
1:                          Path[] inputAPaths,
1:                          Path inputBtGlob,
1:                          Path outputPath,
1:                          int aBlockRows,
1:                          int minSplitSize,
1:                          int k,
1:                          int p,
1:                          int outerProdBlockHeight,
0:                          int numReduceTasks) throws ClassNotFoundException,
0:     InterruptedException, IOException {
1: 
1:     JobConf oldApiJob = new JobConf(conf);
1: 
1:     // MultipleOutputs
1:     // .addNamedOutput(oldApiJob,
1:     // QJob.OUTPUT_QHAT,
1:     // org.apache.hadoop.mapred.SequenceFileOutputFormat.class,
1:     // SplitPartitionedWritable.class,
1:     // DenseBlockWritable.class);
1:     //
1:     // MultipleOutputs
1:     // .addNamedOutput(oldApiJob,
1:     // QJob.OUTPUT_RHAT,
1:     // org.apache.hadoop.mapred.SequenceFileOutputFormat.class,
1:     // SplitPartitionedWritable.class,
1:     // VectorWritable.class);
1: 
1:     Job job = new Job(oldApiJob);
1:     job.setJobName("ABt-job");
1:     job.setJarByClass(ABtJob.class);
1: 
1:     job.setInputFormatClass(SequenceFileInputFormat.class);
1:     FileInputFormat.setInputPaths(job, inputAPaths);
1:     if (minSplitSize > 0) {
1:       FileInputFormat.setMinInputSplitSize(job, minSplitSize);
1:     }
1: 
1:     FileOutputFormat.setOutputPath(job, outputPath);
1: 
1:     SequenceFileOutputFormat.setOutputCompressionType(job,
1:                                                       CompressionType.BLOCK);
1: 
1:     job.setMapOutputKeyClass(SplitPartitionedWritable.class);
1:     job.setMapOutputValueClass(SparseRowBlockWritable.class);
1: 
1:     job.setOutputKeyClass(SplitPartitionedWritable.class);
1:     job.setOutputValueClass(VectorWritable.class);
1: 
1:     job.setMapperClass(ABtMapper.class);
1:     job.setCombinerClass(BtJob.OuterProductCombiner.class);
1:     job.setReducerClass(QRReducer.class);
1: 
1:     job.getConfiguration().setInt(QJob.PROP_AROWBLOCK_SIZE, aBlockRows);
1:     job.getConfiguration().setInt(BtJob.PROP_OUTER_PROD_BLOCK_HEIGHT,
1:                                   outerProdBlockHeight);
1:     job.getConfiguration().setInt(QRFirstStep.PROP_K, k);
1:     job.getConfiguration().setInt(QRFirstStep.PROP_P, p);
1:     job.getConfiguration().set(PROP_BT_PATH, inputBtGlob.toString());
1: 
1:     // number of reduce tasks doesn't matter. we don't actually
1:     // send anything to reducers.
1: 
1:     job.setNumReduceTasks(numReduceTasks);
1: 
1:     job.submit();
1:     job.waitForCompletion(false);
1: 
1:     if (!job.isSuccessful()) {
1:       throw new IOException("ABt job unsuccessful.");
1:     }
1: 
1:   }
1: 
1: }
============================================================================