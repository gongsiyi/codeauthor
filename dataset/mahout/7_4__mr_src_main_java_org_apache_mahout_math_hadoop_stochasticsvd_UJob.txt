1:151de0d: /**
1:151de0d:  * Licensed to the Apache Software Foundation (ASF) under one or more
1:151de0d:  * contributor license agreements.  See the NOTICE file distributed with
1:151de0d:  * this work for additional information regarding copyright ownership.
1:151de0d:  * The ASF licenses this file to You under the Apache License, Version 2.0
1:151de0d:  * (the "License"); you may not use this file except in compliance with
1:151de0d:  * the License.  You may obtain a copy of the License at
2:151de0d:  *
1:151de0d:  *     http://www.apache.org/licenses/LICENSE-2.0
1:151de0d:  *
1:151de0d:  * Unless required by applicable law or agreed to in writing, software
1:151de0d:  * distributed under the License is distributed on an "AS IS" BASIS,
1:151de0d:  * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
1:151de0d:  * See the License for the specific language governing permissions and
1:151de0d:  * limitations under the License.
1:151de0d:  */
2:151de0d: 
1:151de0d: package org.apache.mahout.math.hadoop.stochasticsvd;
1:151de0d: 
1:151de0d: import java.io.IOException;
1:151de0d: 
1:151de0d: import org.apache.hadoop.conf.Configuration;
1:151de0d: import org.apache.hadoop.fs.FileSystem;
1:151de0d: import org.apache.hadoop.fs.Path;
1:151de0d: import org.apache.hadoop.io.IntWritable;
1:151de0d: import org.apache.hadoop.io.SequenceFile.CompressionType;
1:151de0d: import org.apache.hadoop.io.Writable;
1:151de0d: import org.apache.hadoop.io.compress.DefaultCodec;
1:151de0d: import org.apache.hadoop.mapreduce.Job;
1:151de0d: import org.apache.hadoop.mapreduce.Mapper;
1:151de0d: import org.apache.hadoop.mapreduce.lib.input.FileInputFormat;
1:151de0d: import org.apache.hadoop.mapreduce.lib.input.SequenceFileInputFormat;
1:151de0d: import org.apache.hadoop.mapreduce.lib.output.FileOutputFormat;
1:151de0d: import org.apache.hadoop.mapreduce.lib.output.SequenceFileOutputFormat;
1:151de0d: import org.apache.mahout.math.DenseVector;
1:151de0d: import org.apache.mahout.math.Matrix;
1:478cad9: import org.apache.mahout.math.NamedVector;
1:151de0d: import org.apache.mahout.math.Vector;
1:151de0d: import org.apache.mahout.math.VectorWritable;
1:175701c: import org.apache.mahout.math.function.Functions;
1:151de0d: 
1:151de0d: /**
1:151de0d:  * Computes U=Q*Uhat of SSVD (optionally adding x pow(Sigma, 0.5) )
1:151de0d:  * 
1:151de0d:  */
1:151de0d: public class UJob {
1:151de0d:   private static final String OUTPUT_U = "u";
1:151de0d:   private static final String PROP_UHAT_PATH = "ssvd.uhat.path";
1:151de0d:   private static final String PROP_SIGMA_PATH = "ssvd.sigma.path";
1:478cad9:   private static final String PROP_OUTPUT_SCALING = "ssvd.u.output.scaling";
1:151de0d:   private static final String PROP_K = "ssvd.k";
1:151de0d: 
1:151de0d:   private Job job;
1:151de0d: 
1:175701c:   public void run(Configuration conf, Path inputPathQ, Path inputUHatPath,
1:151de0d:       Path sigmaPath, Path outputPath, int k, int numReduceTasks,
1:478cad9:       Class<? extends Writable> labelClass, SSVDSolver.OutputScalingEnum outputScaling)
1:151de0d:     throws ClassNotFoundException, InterruptedException, IOException {
1:151de0d: 
1:151de0d:     job = new Job(conf);
1:151de0d:     job.setJobName("U-job");
1:151de0d:     job.setJarByClass(UJob.class);
1:151de0d: 
1:151de0d:     job.setInputFormatClass(SequenceFileInputFormat.class);
1:151de0d:     job.setOutputFormatClass(SequenceFileOutputFormat.class);
1:151de0d:     FileInputFormat.setInputPaths(job, inputPathQ);
1:151de0d:     FileOutputFormat.setOutputPath(job, outputPath);
1:151de0d: 
1:5a2250c:     // WARN: tight hadoop integration here:
1:151de0d:     job.getConfiguration().set("mapreduce.output.basename", OUTPUT_U);
1:b16c260:     FileOutputFormat.setCompressOutput(job, true);
1:b16c260:     FileOutputFormat.setOutputCompressorClass(job, DefaultCodec.class);
1:b16c260:     SequenceFileOutputFormat.setOutputCompressionType(job, CompressionType.BLOCK);
1:151de0d: 
1:151de0d:     job.setMapperClass(UMapper.class);
1:151de0d:     job.setMapOutputKeyClass(IntWritable.class);
1:151de0d:     job.setMapOutputValueClass(VectorWritable.class);
1:151de0d: 
1:151de0d:     job.setOutputKeyClass(labelClass);
1:151de0d:     job.setOutputValueClass(VectorWritable.class);
1:151de0d: 
1:151de0d:     job.getConfiguration().set(PROP_UHAT_PATH, inputUHatPath.toString());
1:151de0d:     job.getConfiguration().set(PROP_SIGMA_PATH, sigmaPath.toString());
1:478cad9:     job.getConfiguration().set(PROP_OUTPUT_SCALING, outputScaling.name());
1:151de0d:     job.getConfiguration().setInt(PROP_K, k);
1:151de0d:     job.setNumReduceTasks(0);
1:151de0d:     job.submit();
1:151de0d: 
1:151de0d:   }
1:151de0d: 
1:151de0d:   public void waitForCompletion() throws IOException, ClassNotFoundException,
1:151de0d:       InterruptedException {
1:151de0d:     job.waitForCompletion(false);
1:151de0d: 
1:b16c260:     if (!job.isSuccessful()) {
1:151de0d:       throw new IOException("U job unsuccessful.");
1:151de0d:     }
1:151de0d: 
1:b16c260:   }
1:151de0d: 
1:151de0d:   public static final class UMapper extends
1:151de0d:       Mapper<Writable, VectorWritable, Writable, VectorWritable> {
1:151de0d: 
1:151de0d:     private Matrix uHat;
1:151de0d:     private DenseVector uRow;
1:151de0d:     private VectorWritable uRowWritable;
1:151de0d:     private int kp;
1:151de0d:     private int k;
1:151de0d:     private Vector sValues;
1:151de0d: 
1:151de0d:     @Override
1:151de0d:     protected void map(Writable key, VectorWritable value, Context context)
1:151de0d:       throws IOException, InterruptedException {
1:151de0d:       Vector qRow = value.get();
1:c4550a1:       if (sValues != null) {
1:b16c260:         for (int i = 0; i < k; i++) {
1:151de0d:           uRow.setQuick(i,
1:528ffcd:                         qRow.dot(uHat.viewColumn(i)) * sValues.getQuick(i));
1:b16c260:         }
1:c4550a1:       } else {
2:c4550a1:         for (int i = 0; i < k; i++) {
1:528ffcd:           uRow.setQuick(i, qRow.dot(uHat.viewColumn(i)));
1:b16c260:         }
3:c4550a1:       }
1:151de0d: 
1:478cad9:       /*
1:478cad9:        * MAHOUT-1067: inherit A names too.
1:478cad9:        */
1:478cad9:       if (qRow instanceof NamedVector) {
1:478cad9:         uRowWritable.set(new NamedVector(uRow, ((NamedVector) qRow).getName()));
1:8b6a26a:       } else {
1:478cad9:         uRowWritable.set(uRow);
1:8b6a26a:       }
1:478cad9: 
1:151de0d:       context.write(key, uRowWritable); // U inherits original A row labels.
1:151de0d:     }
1:151de0d: 
1:151de0d:     @Override
1:151de0d:     protected void setup(Context context) throws IOException,
1:151de0d:         InterruptedException {
1:151de0d:       super.setup(context);
1:151de0d:       Path uHatPath = new Path(context.getConfiguration().get(PROP_UHAT_PATH));
1:151de0d:       Path sigmaPath = new Path(context.getConfiguration().get(PROP_SIGMA_PATH));
1:1de8cec:       FileSystem fs = FileSystem.get(uHatPath.toUri(), context.getConfiguration());
1:151de0d: 
1:b717cfc:       uHat = SSVDHelper.drmLoadAsDense(fs, uHatPath, context.getConfiguration());
1:151de0d:       // since uHat is (k+p) x (k+p)
1:151de0d:       kp = uHat.columnSize();
1:151de0d:       k = context.getConfiguration().getInt(PROP_K, kp);
1:151de0d:       uRow = new DenseVector(k);
1:151de0d:       uRowWritable = new VectorWritable(uRow);
1:151de0d: 
1:478cad9:       SSVDSolver.OutputScalingEnum outputScaling =
1:478cad9:         SSVDSolver.OutputScalingEnum.valueOf(context.getConfiguration()
1:478cad9:                                                     .get(PROP_OUTPUT_SCALING));
1:478cad9:       switch (outputScaling) {
1:478cad9:         case SIGMA:
1:478cad9:           sValues = SSVDHelper.loadVector(sigmaPath, context.getConfiguration());
1:478cad9:           break;
1:478cad9:         case HALFSIGMA:
1:175701c:           sValues = SSVDHelper.loadVector(sigmaPath, context.getConfiguration());
1:175701c:           sValues.assign(Functions.SQRT);
1:478cad9:           break;
1:4841efb:         default:
1:151de0d:       }
1:151de0d:     }
1:151de0d: 
1:151de0d:   }
1:151de0d: 
1:151de0d: }
============================================================================
author:pferrel
-------------------------------------------------------------------------------
commit:b988c49
author:Suneel Marthi
-------------------------------------------------------------------------------
commit:87c15be
/////////////////////////////////////////////////////////////////////////
author:frankscholten
-------------------------------------------------------------------------------
commit:1a42d85
author:Dmitriy Lyubimov
-------------------------------------------------------------------------------
commit:b717cfc
/////////////////////////////////////////////////////////////////////////
1:       uHat = SSVDHelper.drmLoadAsDense(fs, uHatPath, context.getConfiguration());
commit:478cad9
/////////////////////////////////////////////////////////////////////////
1: import org.apache.mahout.math.NamedVector;
/////////////////////////////////////////////////////////////////////////
1:   private static final String PROP_OUTPUT_SCALING = "ssvd.u.output.scaling";
1:       Class<? extends Writable> labelClass, SSVDSolver.OutputScalingEnum outputScaling)
/////////////////////////////////////////////////////////////////////////
1:     job.getConfiguration().set(PROP_OUTPUT_SCALING, outputScaling.name());
/////////////////////////////////////////////////////////////////////////
1:       /*
1:        * MAHOUT-1067: inherit A names too.
1:        */
1:       if (qRow instanceof NamedVector) {
1:         uRowWritable.set(new NamedVector(uRow, ((NamedVector) qRow).getName()));
0:       } else
1:         uRowWritable.set(uRow);
1: 
/////////////////////////////////////////////////////////////////////////
1:       SSVDSolver.OutputScalingEnum outputScaling =
1:         SSVDSolver.OutputScalingEnum.valueOf(context.getConfiguration()
1:                                                     .get(PROP_OUTPUT_SCALING));
1:       switch (outputScaling) {
1:       case SIGMA:
1:         sValues = SSVDHelper.loadVector(sigmaPath, context.getConfiguration());
1:         break;
1:       case HALFSIGMA:
1:         break;
commit:175701c
/////////////////////////////////////////////////////////////////////////
1: import org.apache.mahout.math.function.Functions;
/////////////////////////////////////////////////////////////////////////
1:   public void run(Configuration conf, Path inputPathQ, Path inputUHatPath,
/////////////////////////////////////////////////////////////////////////
0:       uHat = new DenseMatrix(SSVDHelper.loadDistributedRowMatrix(fs,
/////////////////////////////////////////////////////////////////////////
1:         sValues = SSVDHelper.loadVector(sigmaPath, context.getConfiguration());
1:         sValues.assign(Functions.SQRT);
commit:5a2250c
/////////////////////////////////////////////////////////////////////////
1:     // WARN: tight hadoop integration here:
commit:151de0d
/////////////////////////////////////////////////////////////////////////
1: /**
1:  * Licensed to the Apache Software Foundation (ASF) under one or more
1:  * contributor license agreements.  See the NOTICE file distributed with
1:  * this work for additional information regarding copyright ownership.
1:  * The ASF licenses this file to You under the Apache License, Version 2.0
1:  * (the "License"); you may not use this file except in compliance with
1:  * the License.  You may obtain a copy of the License at
1:  *
1:  *     http://www.apache.org/licenses/LICENSE-2.0
1:  *
1:  * Unless required by applicable law or agreed to in writing, software
1:  * distributed under the License is distributed on an "AS IS" BASIS,
1:  * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
1:  * See the License for the specific language governing permissions and
1:  * limitations under the License.
1:  */
1: 
1: package org.apache.mahout.math.hadoop.stochasticsvd;
1: 
1: import java.io.IOException;
1: 
1: import org.apache.hadoop.conf.Configuration;
1: import org.apache.hadoop.fs.FileSystem;
1: import org.apache.hadoop.fs.Path;
1: import org.apache.hadoop.io.IntWritable;
1: import org.apache.hadoop.io.SequenceFile.CompressionType;
1: import org.apache.hadoop.io.Writable;
1: import org.apache.hadoop.io.compress.DefaultCodec;
1: import org.apache.hadoop.mapreduce.Job;
1: import org.apache.hadoop.mapreduce.Mapper;
1: import org.apache.hadoop.mapreduce.lib.input.FileInputFormat;
1: import org.apache.hadoop.mapreduce.lib.input.SequenceFileInputFormat;
1: import org.apache.hadoop.mapreduce.lib.output.FileOutputFormat;
1: import org.apache.hadoop.mapreduce.lib.output.SequenceFileOutputFormat;
0: import org.apache.mahout.math.DenseMatrix;
1: import org.apache.mahout.math.DenseVector;
1: import org.apache.mahout.math.Matrix;
1: import org.apache.mahout.math.Vector;
1: import org.apache.mahout.math.VectorWritable;
1: 
1: /**
1:  * Computes U=Q*Uhat of SSVD (optionally adding x pow(Sigma, 0.5) )
1:  * 
0:  * @author Dmitriy
1:  * 
1:  */
1: public class UJob {
1:   private static final String OUTPUT_U = "u";
1:   private static final String PROP_UHAT_PATH = "ssvd.uhat.path";
1:   private static final String PROP_SIGMA_PATH = "ssvd.sigma.path";
0:   private static final String PROP_U_HALFSIGMA = "ssvd.u.halfsigma";
1:   private static final String PROP_K = "ssvd.k";
1: 
1:   private Job job;
1: 
0:   public void start(Configuration conf, Path inputPathQ, Path inputUHatPath,
1:       Path sigmaPath, Path outputPath, int k, int numReduceTasks,
0:       Class<? extends Writable> labelClass, boolean uHalfSigma)
1:     throws ClassNotFoundException, InterruptedException, IOException {
1: 
1:     job = new Job(conf);
1:     job.setJobName("U-job");
1:     job.setJarByClass(UJob.class);
1: 
1:     job.setInputFormatClass(SequenceFileInputFormat.class);
1:     job.setOutputFormatClass(SequenceFileOutputFormat.class);
1:     FileInputFormat.setInputPaths(job, inputPathQ);
1:     FileOutputFormat.setOutputPath(job, outputPath);
1: 
0:     // Warn: tight hadoop integration here:
1:     job.getConfiguration().set("mapreduce.output.basename", OUTPUT_U);
0:     SequenceFileOutputFormat.setCompressOutput(job, true);
0:     SequenceFileOutputFormat
0:         .setOutputCompressorClass(job, DefaultCodec.class);
0:     SequenceFileOutputFormat.setOutputCompressionType(job,
0:         CompressionType.BLOCK);
1: 
1:     job.setMapperClass(UMapper.class);
1:     job.setMapOutputKeyClass(IntWritable.class);
1:     job.setMapOutputValueClass(VectorWritable.class);
1: 
1:     job.setOutputKeyClass(labelClass);
1:     job.setOutputValueClass(VectorWritable.class);
1: 
1:     job.getConfiguration().set(PROP_UHAT_PATH, inputUHatPath.toString());
1:     job.getConfiguration().set(PROP_SIGMA_PATH, sigmaPath.toString());
0:     if (uHalfSigma)
0:       job.getConfiguration().set(PROP_U_HALFSIGMA, "y");
1:     job.getConfiguration().setInt(PROP_K, k);
1:     job.setNumReduceTasks(0);
1:     job.submit();
1: 
1:   }
1: 
1:   public void waitForCompletion() throws IOException, ClassNotFoundException,
1:       InterruptedException {
1:     job.waitForCompletion(false);
1: 
0:     if (!job.isSuccessful())
1:       throw new IOException("U job unsuccessful.");
1: 
1:   }
1: 
1:   public static final class UMapper extends
1:       Mapper<Writable, VectorWritable, Writable, VectorWritable> {
1: 
1:     private Matrix uHat;
1:     private DenseVector uRow;
1:     private VectorWritable uRowWritable;
1:     private int kp;
1:     private int k;
1:     private Vector sValues;
1: 
1:     @Override
1:     protected void map(Writable key, VectorWritable value, Context context)
1:       throws IOException, InterruptedException {
1:       Vector qRow = value.get();
0:       if (sValues != null)
0:         for (int i = 0; i < k; i++)
1:           uRow.setQuick(i,
0:               qRow.dot(uHat.getColumn(i)) * sValues.getQuick(i));
0:       else
0:         for (int i = 0; i < k; i++)
0:           uRow.setQuick(i, qRow.dot(uHat.getColumn(i)));
1: 
1:       context.write(key, uRowWritable); // U inherits original A row labels.
1:     }
1: 
1:     @Override
1:     protected void setup(Context context) throws IOException,
1:         InterruptedException {
1:       super.setup(context);
0:       FileSystem fs = FileSystem.get(context.getConfiguration());
1:       Path uHatPath = new Path(context.getConfiguration().get(PROP_UHAT_PATH));
1:       Path sigmaPath = new Path(context.getConfiguration().get(PROP_SIGMA_PATH));
1: 
0:       uHat = new DenseMatrix(SSVDSolver.loadDistributedRowMatrix(fs,
0:           uHatPath, context.getConfiguration()));
1:       // since uHat is (k+p) x (k+p)
1:       kp = uHat.columnSize();
1:       k = context.getConfiguration().getInt(PROP_K, kp);
1:       uRow = new DenseVector(k);
1:       uRowWritable = new VectorWritable(uRow);
1: 
0:       if (context.getConfiguration().get(PROP_U_HALFSIGMA) != null) {
0:         sValues = new DenseVector(SSVDSolver.loadDistributedRowMatrix(fs,
0:             sigmaPath, context.getConfiguration())[0], true);
0:         for (int i = 0; i < k; i++)
0:           sValues.setQuick(i, Math.sqrt(sValues.getQuick(i)));
1:       }
1: 
1:     }
1: 
1:   }
1: 
1: }
author:Sebastian Schelter
-------------------------------------------------------------------------------
commit:4841efb
/////////////////////////////////////////////////////////////////////////
0:         case SIGMA:
0:           sValues = SSVDHelper.loadVector(sigmaPath, context.getConfiguration());
0:           break;
0:         case HALFSIGMA:
0:           sValues = SSVDHelper.loadVector(sigmaPath, context.getConfiguration());
0:           sValues.assign(Functions.SQRT);
0:           break;
1:         default:
author:Sean R. Owen
-------------------------------------------------------------------------------
commit:8b6a26a
/////////////////////////////////////////////////////////////////////////
1:       } else {
1:       }
commit:1de8cec
/////////////////////////////////////////////////////////////////////////
1:       FileSystem fs = FileSystem.get(uHatPath.toUri(), context.getConfiguration());
commit:b16c260
/////////////////////////////////////////////////////////////////////////
1:     FileOutputFormat.setCompressOutput(job, true);
1:     FileOutputFormat.setOutputCompressorClass(job, DefaultCodec.class);
1:     SequenceFileOutputFormat.setOutputCompressionType(job, CompressionType.BLOCK);
/////////////////////////////////////////////////////////////////////////
0:     if (uHalfSigma) {
1:     }
/////////////////////////////////////////////////////////////////////////
1:     if (!job.isSuccessful()) {
1:     }
/////////////////////////////////////////////////////////////////////////
1:         for (int i = 0; i < k; i++) {
1:         }
commit:c4550a1
/////////////////////////////////////////////////////////////////////////
/////////////////////////////////////////////////////////////////////////
1:       if (sValues != null) {
1:         for (int i = 0; i < k; i++) {
0:                         qRow.dot(uHat.getColumn(i)) * sValues.getQuick(i));
1:         }
1:       } else {
1:         for (int i = 0; i < k; i++) {
1:         }
1:       }
author:Ted Dunning
-------------------------------------------------------------------------------
commit:528ffcd
/////////////////////////////////////////////////////////////////////////
1:                         qRow.dot(uHat.viewColumn(i)) * sValues.getQuick(i));
1:           uRow.setQuick(i, qRow.dot(uHat.viewColumn(i)));
============================================================================