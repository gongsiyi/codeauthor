1:48d069f: /**
1:48d069f:  * Licensed to the Apache Software Foundation (ASF) under one or more
1:48d069f:  * contributor license agreements.  See the NOTICE file distributed with
1:48d069f:  * this work for additional information regarding copyright ownership.
1:48d069f:  * The ASF licenses this file to You under the Apache License, Version 2.0
1:48d069f:  * (the "License"); you may not use this file except in compliance with
1:48d069f:  * the License.  You may obtain a copy of the License at
1:48d069f:  *
1:48d069f:  *     http://www.apache.org/licenses/LICENSE-2.0
1:48d069f:  *
1:48d069f:  * Unless required by applicable law or agreed to in writing, software
1:48d069f:  * distributed under the License is distributed on an "AS IS" BASIS,
1:48d069f:  * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
1:48d069f:  * See the License for the specific language governing permissions and
1:48d069f:  * limitations under the License.
1:48d069f:  */
2:48d069f: 
1:b60c909: package org.apache.mahout.clustering.spectral;
1:48d069f: 
1:48d069f: import java.io.IOException;
1:48d069f: 
1:48d069f: import org.apache.hadoop.conf.Configuration;
1:48d069f: import org.apache.hadoop.fs.Path;
1:48d069f: import org.apache.hadoop.io.IntWritable;
1:48d069f: import org.apache.hadoop.io.NullWritable;
1:48d069f: import org.apache.hadoop.mapreduce.Job;
1:48d069f: import org.apache.hadoop.mapreduce.Mapper;
1:48d069f: import org.apache.hadoop.mapreduce.Reducer;
1:48d069f: import org.apache.hadoop.mapreduce.lib.input.FileInputFormat;
1:48d069f: import org.apache.hadoop.mapreduce.lib.input.SequenceFileInputFormat;
1:48d069f: import org.apache.hadoop.mapreduce.lib.output.FileOutputFormat;
1:48d069f: import org.apache.hadoop.mapreduce.lib.output.SequenceFileOutputFormat;
1:48d069f: import org.apache.mahout.common.HadoopUtil;
1:48d069f: import org.apache.mahout.math.DenseVector;
1:48d069f: import org.apache.mahout.math.Vector;
1:48d069f: import org.apache.mahout.math.VectorWritable;
1:48d069f: 
1:48d069f: /**
1:48d069f:  * Given a matrix, this job returns a vector whose i_th element is the 
1:48d069f:  * sum of all the elements in the i_th row of the original matrix.
1:48d069f:  */
1:48d069f: public final class MatrixDiagonalizeJob {
1:48d069f: 
1:049e7dc:   private MatrixDiagonalizeJob() {
1:049e7dc:   }
1:049e7dc: 
1:049e7dc:   public static Vector runJob(Path affInput, int dimensions)
1:48d069f:     throws IOException, ClassNotFoundException, InterruptedException {
1:48d069f:     
1:48d069f:     // set up all the job tasks
1:48d069f:     Configuration conf = new Configuration();
1:48d069f:     Path diagOutput = new Path(affInput.getParent(), "diagonal");
1:a13b4b7:     HadoopUtil.delete(conf, diagOutput);
1:b60c909:     conf.setInt(Keys.AFFINITY_DIMENSIONS, dimensions);
1:48d069f:     Job job = new Job(conf, "MatrixDiagonalizeJob");
1:48d069f:     
1:48d069f:     job.setInputFormatClass(SequenceFileInputFormat.class);
1:48d069f:     job.setMapOutputKeyClass(NullWritable.class);
1:48d069f:     job.setMapOutputValueClass(IntDoublePairWritable.class);
1:48d069f:     job.setOutputKeyClass(NullWritable.class);
1:48d069f:     job.setOutputValueClass(VectorWritable.class);
1:48d069f:     job.setOutputFormatClass(SequenceFileOutputFormat.class);
1:48d069f:     job.setMapperClass(MatrixDiagonalizeMapper.class);
1:48d069f:     job.setReducerClass(MatrixDiagonalizeReducer.class);
1:48d069f:     
1:48d069f:     FileInputFormat.addInputPath(job, affInput);
1:48d069f:     FileOutputFormat.setOutputPath(job, diagOutput);
1:48d069f:     
1:765834c:     job.setJarByClass(MatrixDiagonalizeJob.class);
1:765834c: 
1:7c2b664:     boolean succeeded = job.waitForCompletion(true);
1:7c2b664:     if (!succeeded) {
1:7c2b664:       throw new IllegalStateException("Job failed!");
1:7c2b664:     }
1:7c2b664: 
1:48d069f:     // read the results back from the path
1:a13b4b7:     return VectorCache.load(conf, new Path(diagOutput, "part-r-00000"));
1:48d069f:   }
1:48d069f:   
1:48d069f:   public static class MatrixDiagonalizeMapper
1:48d069f:     extends Mapper<IntWritable, VectorWritable, NullWritable, IntDoublePairWritable> {
1:48d069f:     
1:48d069f:     @Override
1:48d069f:     protected void map(IntWritable key, VectorWritable row, Context context) 
1:48d069f:       throws IOException, InterruptedException {
1:48d069f:       // store the sum
1:48d069f:       IntDoublePairWritable store = new IntDoublePairWritable(key.get(), row.get().zSum());
1:48d069f:       context.write(NullWritable.get(), store);
1:48d069f:     }
1:48d069f:   }
1:48d069f:   
1:48d069f:   public static class MatrixDiagonalizeReducer
1:48d069f:     extends Reducer<NullWritable, IntDoublePairWritable, NullWritable, VectorWritable> {
1:48d069f:     
1:049e7dc:     @Override
1:049e7dc:     protected void reduce(NullWritable key, Iterable<IntDoublePairWritable> values,
1:48d069f:       Context context) throws IOException, InterruptedException {
1:48d069f:       // create the return vector
1:b60c909:       Vector retval = new DenseVector(context.getConfiguration().getInt(Keys.AFFINITY_DIMENSIONS, Integer.MAX_VALUE));
1:48d069f:       // put everything in its correct spot
1:48d069f:       for (IntDoublePairWritable e : values) {
1:48d069f:         retval.setQuick(e.getKey(), e.getValue());
1:48d069f:       }
1:48d069f:       // write it out
1:48d069f:       context.write(key, new VectorWritable(retval));
1:48d069f:     }
1:48d069f:   }
1:48d069f: }
============================================================================
author:pferrel
-------------------------------------------------------------------------------
commit:b988c49
author:frankscholten
-------------------------------------------------------------------------------
commit:1a42d85
author:Sebastian Schelter
-------------------------------------------------------------------------------
commit:b60c909
/////////////////////////////////////////////////////////////////////////
1: package org.apache.mahout.clustering.spectral;
/////////////////////////////////////////////////////////////////////////
/////////////////////////////////////////////////////////////////////////
1:     conf.setInt(Keys.AFFINITY_DIMENSIONS, dimensions);
/////////////////////////////////////////////////////////////////////////
1:       Vector retval = new DenseVector(context.getConfiguration().getInt(Keys.AFFINITY_DIMENSIONS, Integer.MAX_VALUE));
author:Sean R. Owen
-------------------------------------------------------------------------------
commit:7c2b664
/////////////////////////////////////////////////////////////////////////
1:     boolean succeeded = job.waitForCompletion(true);
1:     if (!succeeded) {
1:       throw new IllegalStateException("Job failed!");
1:     }
1: 
commit:765834c
/////////////////////////////////////////////////////////////////////////
1:     job.setJarByClass(MatrixDiagonalizeJob.class);
1: 
commit:a13b4b7
/////////////////////////////////////////////////////////////////////////
1:     HadoopUtil.delete(conf, diagOutput);
/////////////////////////////////////////////////////////////////////////
1:     return VectorCache.load(conf, new Path(diagOutput, "part-r-00000"));
commit:049e7dc
/////////////////////////////////////////////////////////////////////////
1:   private MatrixDiagonalizeJob() {
1:   }
1: 
1:   public static Vector runJob(Path affInput, int dimensions)
/////////////////////////////////////////////////////////////////////////
1:     @Override
1:     protected void reduce(NullWritable key, Iterable<IntDoublePairWritable> values,
author:Jeff Eastman
-------------------------------------------------------------------------------
commit:48d069f
/////////////////////////////////////////////////////////////////////////
1: /**
1:  * Licensed to the Apache Software Foundation (ASF) under one or more
1:  * contributor license agreements.  See the NOTICE file distributed with
1:  * this work for additional information regarding copyright ownership.
1:  * The ASF licenses this file to You under the Apache License, Version 2.0
1:  * (the "License"); you may not use this file except in compliance with
1:  * the License.  You may obtain a copy of the License at
1:  *
1:  *     http://www.apache.org/licenses/LICENSE-2.0
1:  *
1:  * Unless required by applicable law or agreed to in writing, software
1:  * distributed under the License is distributed on an "AS IS" BASIS,
1:  * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
1:  * See the License for the specific language governing permissions and
1:  * limitations under the License.
1:  */
1: 
0: package org.apache.mahout.clustering.spectral.common;
1: 
1: import java.io.IOException;
1: 
1: import org.apache.hadoop.conf.Configuration;
1: import org.apache.hadoop.fs.Path;
1: import org.apache.hadoop.io.IntWritable;
1: import org.apache.hadoop.io.NullWritable;
1: import org.apache.hadoop.mapreduce.Job;
1: import org.apache.hadoop.mapreduce.Mapper;
1: import org.apache.hadoop.mapreduce.Reducer;
1: import org.apache.hadoop.mapreduce.lib.input.FileInputFormat;
1: import org.apache.hadoop.mapreduce.lib.input.SequenceFileInputFormat;
1: import org.apache.hadoop.mapreduce.lib.output.FileOutputFormat;
1: import org.apache.hadoop.mapreduce.lib.output.SequenceFileOutputFormat;
0: import org.apache.mahout.clustering.spectral.eigencuts.EigencutsKeys;
1: import org.apache.mahout.common.HadoopUtil;
1: import org.apache.mahout.math.DenseVector;
1: import org.apache.mahout.math.Vector;
1: import org.apache.mahout.math.VectorWritable;
1: 
1: /**
1:  * Given a matrix, this job returns a vector whose i_th element is the 
1:  * sum of all the elements in the i_th row of the original matrix.
1:  */
1: public final class MatrixDiagonalizeJob {
1: 
0:   public static Vector runJob(Path affInput, int dimensions) 
1:     throws IOException, ClassNotFoundException, InterruptedException {
1:     
1:     // set up all the job tasks
1:     Configuration conf = new Configuration();
1:     Path diagOutput = new Path(affInput.getParent(), "diagonal");
0:     HadoopUtil.overwriteOutput(diagOutput);
0:     conf.setInt(EigencutsKeys.AFFINITY_DIMENSIONS, dimensions);
1:     Job job = new Job(conf, "MatrixDiagonalizeJob");
1:     
1:     job.setInputFormatClass(SequenceFileInputFormat.class);
1:     job.setMapOutputKeyClass(NullWritable.class);
1:     job.setMapOutputValueClass(IntDoublePairWritable.class);
1:     job.setOutputKeyClass(NullWritable.class);
1:     job.setOutputValueClass(VectorWritable.class);
1:     job.setOutputFormatClass(SequenceFileOutputFormat.class);
1:     job.setMapperClass(MatrixDiagonalizeMapper.class);
1:     job.setReducerClass(MatrixDiagonalizeReducer.class);
1:     
1:     FileInputFormat.addInputPath(job, affInput);
1:     FileOutputFormat.setOutputPath(job, diagOutput);
1:     
0:     job.waitForCompletion(true);
1:     
1:     // read the results back from the path
0:     return VectorCache.load(NullWritable.get(), conf, 
0:         new Path(diagOutput, "part-r-00000"));
1:   }
1:   
1:   public static class MatrixDiagonalizeMapper
1:     extends Mapper<IntWritable, VectorWritable, NullWritable, IntDoublePairWritable> {
1:     
1:     @Override
1:     protected void map(IntWritable key, VectorWritable row, Context context) 
1:       throws IOException, InterruptedException {
1:       // store the sum
1:       IntDoublePairWritable store = new IntDoublePairWritable(key.get(), row.get().zSum());
1:       context.write(NullWritable.get(), store);
1:     }
1:   }
1:   
1:   public static class MatrixDiagonalizeReducer
1:     extends Reducer<NullWritable, IntDoublePairWritable, NullWritable, VectorWritable> {
1:     
0:     protected void reduce(NullWritable key, Iterable<IntDoublePairWritable> values, 
1:       Context context) throws IOException, InterruptedException {
1:       // create the return vector
0:       Vector retval = new DenseVector(context.getConfiguration().getInt(
0:           EigencutsKeys.AFFINITY_DIMENSIONS, Integer.MAX_VALUE));
1:       // put everything in its correct spot
1:       for (IntDoublePairWritable e : values) {
1:         retval.setQuick(e.getKey(), e.getValue());
1:       }
1:       // write it out
1:       context.write(key, new VectorWritable(retval));
1:     }
1:   }
1: }
============================================================================