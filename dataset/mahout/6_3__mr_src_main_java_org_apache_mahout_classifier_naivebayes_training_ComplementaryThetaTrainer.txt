1:e3fb0c4: /**
1:e3fb0c4:  * Licensed to the Apache Software Foundation (ASF) under one or more
1:e3fb0c4:  * contributor license agreements.  See the NOTICE file distributed with
1:e3fb0c4:  * this work for additional information regarding copyright ownership.
1:e3fb0c4:  * The ASF licenses this file to You under the Apache License, Version 2.0
1:e3fb0c4:  * (the "License"); you may not use this file except in compliance with
1:e3fb0c4:  * the License.  You may obtain a copy of the License at
1:e3fb0c4:  *
1:e3fb0c4:  *     http://www.apache.org/licenses/LICENSE-2.0
1:e3fb0c4:  *
1:e3fb0c4:  * Unless required by applicable law or agreed to in writing, software
1:e3fb0c4:  * distributed under the License is distributed on an "AS IS" BASIS,
1:e3fb0c4:  * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
1:e3fb0c4:  * See the License for the specific language governing permissions and
1:e3fb0c4:  * limitations under the License.
1:e3fb0c4:  */
2:e3fb0c4: 
1:e3fb0c4: package org.apache.mahout.classifier.naivebayes.training;
1:e3fb0c4: 
1:9a5bab5: import com.google.common.base.Preconditions;
1:d94eb39: import org.apache.mahout.classifier.naivebayes.ComplementaryNaiveBayesClassifier;
1:e3fb0c4: import org.apache.mahout.math.Vector;
1:e3fb0c4: 
1:9a5bab5: public class ComplementaryThetaTrainer {
1:9a5bab5: 
1:9a5bab5:   private final Vector weightsPerFeature;
1:9a5bab5:   private final Vector weightsPerLabel;
1:9a5bab5:   private final Vector perLabelThetaNormalizer;
1:9a5bab5:   private final double alphaI;
1:9a5bab5:   private final double totalWeightSum;
1:9a5bab5:   private final double numFeatures;
1:e3fb0c4: 
1:e3fb0c4:   public ComplementaryThetaTrainer(Vector weightsPerFeature, Vector weightsPerLabel, double alphaI) {
1:9a5bab5:     Preconditions.checkNotNull(weightsPerFeature);
1:9a5bab5:     Preconditions.checkNotNull(weightsPerLabel);
1:9a5bab5:     this.weightsPerFeature = weightsPerFeature;
1:9a5bab5:     this.weightsPerLabel = weightsPerLabel;
1:9a5bab5:     this.alphaI = alphaI;
1:9a5bab5:     perLabelThetaNormalizer = weightsPerLabel.like();
1:9a5bab5:     totalWeightSum = weightsPerLabel.zSum();
1:9a5bab5:     numFeatures = weightsPerFeature.getNumNondefaultElements();
1:e3fb0c4:   }
1:e3fb0c4: 
1:1bdf3ab:   public void train(int label, Vector perLabelWeight) {
1:d94eb39:     double labelWeight = labelWeight(label);
1:fa29726:     // sum weights for each label including those with zero word counts
1:9a5bab5:     for(int i = 0; i < perLabelWeight.size(); i++){
1:fa29726:       Vector.Element perLabelWeightElement = perLabelWeight.getElement(i);
1:d94eb39:       updatePerLabelThetaNormalizer(label,
1:9a5bab5:           ComplementaryNaiveBayesClassifier.computeWeight(featureWeight(perLabelWeightElement.index()),
1:9a5bab5:               perLabelWeightElement.get(), totalWeightSum(), labelWeight, alphaI(), numFeatures()));
1:e3fb0c4:     }
1:e3fb0c4:   }
1:9a5bab5: 
1:9a5bab5:   protected double alphaI() {
1:9a5bab5:     return alphaI;
1:9a5bab5:   }
1:9a5bab5: 
1:9a5bab5:   protected double numFeatures() {
1:9a5bab5:     return numFeatures;
1:9a5bab5:   }
1:9a5bab5: 
1:9a5bab5:   protected double labelWeight(int label) {
1:9a5bab5:     return weightsPerLabel.get(label);
1:9a5bab5:   }
1:9a5bab5: 
1:9a5bab5:   protected double totalWeightSum() {
1:9a5bab5:     return totalWeightSum;
1:9a5bab5:   }
1:9a5bab5: 
1:9a5bab5:   protected double featureWeight(int feature) {
1:9a5bab5:     return weightsPerFeature.get(feature);
1:9a5bab5:   }
1:9a5bab5: 
1:9a5bab5:   // http://people.csail.mit.edu/jrennie/papers/icml03-nb.pdf - Section 3.2, Weight Magnitude Errors
1:9a5bab5:   protected void updatePerLabelThetaNormalizer(int label, double weight) {
1:9a5bab5:     perLabelThetaNormalizer.set(label, perLabelThetaNormalizer.get(label) + Math.abs(weight));
1:9a5bab5:   }
1:9a5bab5: 
1:9a5bab5:   public Vector retrievePerLabelThetaNormalizer() {
1:9a5bab5:     return perLabelThetaNormalizer.clone();
1:9a5bab5:   }
1:e3fb0c4: }
============================================================================
author:pferrel
-------------------------------------------------------------------------------
commit:b988c49
author:Sebastian Schelter
-------------------------------------------------------------------------------
commit:9a5bab5
/////////////////////////////////////////////////////////////////////////
1: import com.google.common.base.Preconditions;
1: public class ComplementaryThetaTrainer {
1: 
1:   private final Vector weightsPerFeature;
1:   private final Vector weightsPerLabel;
1:   private final Vector perLabelThetaNormalizer;
1:   private final double alphaI;
1:   private final double totalWeightSum;
1:   private final double numFeatures;
1:     Preconditions.checkNotNull(weightsPerFeature);
1:     Preconditions.checkNotNull(weightsPerLabel);
1:     this.weightsPerFeature = weightsPerFeature;
1:     this.weightsPerLabel = weightsPerLabel;
1:     this.alphaI = alphaI;
1:     perLabelThetaNormalizer = weightsPerLabel.like();
1:     totalWeightSum = weightsPerLabel.zSum();
1:     numFeatures = weightsPerFeature.getNumNondefaultElements();
1:     for(int i = 0; i < perLabelWeight.size(); i++){
1:           ComplementaryNaiveBayesClassifier.computeWeight(featureWeight(perLabelWeightElement.index()),
1:               perLabelWeightElement.get(), totalWeightSum(), labelWeight, alphaI(), numFeatures()));
1: 
1:   protected double alphaI() {
1:     return alphaI;
1:   }
1: 
1:   protected double numFeatures() {
1:     return numFeatures;
1:   }
1: 
1:   protected double labelWeight(int label) {
1:     return weightsPerLabel.get(label);
1:   }
1: 
1:   protected double totalWeightSum() {
1:     return totalWeightSum;
1:   }
1: 
1:   protected double featureWeight(int feature) {
1:     return weightsPerFeature.get(feature);
1:   }
1: 
1:   // http://people.csail.mit.edu/jrennie/papers/icml03-nb.pdf - Section 3.2, Weight Magnitude Errors
1:   protected void updatePerLabelThetaNormalizer(int label, double weight) {
1:     perLabelThetaNormalizer.set(label, perLabelThetaNormalizer.get(label) + Math.abs(weight));
1:   }
1: 
1:   public Vector retrievePerLabelThetaNormalizer() {
1:     return perLabelThetaNormalizer.clone();
1:   }
commit:e3fb0c4
/////////////////////////////////////////////////////////////////////////
1: /**
1:  * Licensed to the Apache Software Foundation (ASF) under one or more
1:  * contributor license agreements.  See the NOTICE file distributed with
1:  * this work for additional information regarding copyright ownership.
1:  * The ASF licenses this file to You under the Apache License, Version 2.0
1:  * (the "License"); you may not use this file except in compliance with
1:  * the License.  You may obtain a copy of the License at
1:  *
1:  *     http://www.apache.org/licenses/LICENSE-2.0
1:  *
1:  * Unless required by applicable law or agreed to in writing, software
1:  * distributed under the License is distributed on an "AS IS" BASIS,
1:  * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
1:  * See the License for the specific language governing permissions and
1:  * limitations under the License.
1:  */
1: 
1: package org.apache.mahout.classifier.naivebayes.training;
1: 
1: import org.apache.mahout.math.Vector;
1: 
0: import java.util.Iterator;
1: 
0: public class ComplementaryThetaTrainer extends AbstractThetaTrainer {
1: 
1:   public ComplementaryThetaTrainer(Vector weightsPerFeature, Vector weightsPerLabel, double alphaI) {
0:     super(weightsPerFeature, weightsPerLabel, alphaI);
1:   }
1: 
0:   @Override
0:   public void train(int label, Vector instance) {
0:     double sigmaK = labelWeight(label);
0:     Iterator<Vector.Element> it = instance.iterateNonZero();
0:     while (it.hasNext()) {
0:       Vector.Element e = it.next();
0:       double numerator = featureWeight(e.index()) - e.get() + alphaI();
0:       double denominator = totalWeightSum() - sigmaK + alphaI() * numFeatures();
0:       double weight = Math.log(numerator / denominator);
0:       updatePerLabelThetaNormalizer(label, weight);
1:     }
1:   }
1: }
author:frankscholten
-------------------------------------------------------------------------------
commit:1a42d85
author:smarthi
-------------------------------------------------------------------------------
commit:fa29726
/////////////////////////////////////////////////////////////////////////
1:     // sum weights for each label including those with zero word counts
0:     for(int i=0; i < perLabelWeight.size(); i++){
1:       Vector.Element perLabelWeightElement = perLabelWeight.getElement(i);
0:           ComplementaryNaiveBayesClassifier.computeWeight(featureWeight(perLabelWeightElement.index()), perLabelWeightElement.get(),
author:Jacob Alexander Mannix
-------------------------------------------------------------------------------
commit:dc62944
/////////////////////////////////////////////////////////////////////////
/////////////////////////////////////////////////////////////////////////
0:     for (Vector.Element e : perLabelWeight.nonZeroes()) {
author:Robin Anil
-------------------------------------------------------------------------------
commit:d94eb39
/////////////////////////////////////////////////////////////////////////
1: import org.apache.mahout.classifier.naivebayes.ComplementaryNaiveBayesClassifier;
0: import org.apache.mahout.math.Vector;
0: 
/////////////////////////////////////////////////////////////////////////
1:     double labelWeight = labelWeight(label);
1:       updatePerLabelThetaNormalizer(label,
0:           ComplementaryNaiveBayesClassifier.computeWeight(featureWeight(e.index()), e.get(),
0:               totalWeightSum(), labelWeight, alphaI(), numFeatures()));
commit:1bdf3ab
/////////////////////////////////////////////////////////////////////////
1:   public void train(int label, Vector perLabelWeight) {
0:     Iterator<Vector.Element> it = perLabelWeight.iterateNonZero();
0:       double denominator = totalWeightSum() - sigmaK + numFeatures() ;
============================================================================