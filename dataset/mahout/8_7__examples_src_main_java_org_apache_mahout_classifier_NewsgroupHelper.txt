1:f533aef: /**
1:f533aef:  * Licensed to the Apache Software Foundation (ASF) under one or more
1:f533aef:  * contributor license agreements.  See the NOTICE file distributed with
1:f533aef:  * this work for additional information regarding copyright ownership.
1:f533aef:  * The ASF licenses this file to You under the Apache License, Version 2.0
1:f533aef:  * (the "License"); you may not use this file except in compliance with
1:f533aef:  * the License.  You may obtain a copy of the License at
1:f533aef:  *
1:f533aef:  *     http://www.apache.org/licenses/LICENSE-2.0
1:f533aef:  *
1:f533aef:  * Unless required by applicable law or agreed to in writing, software
1:f533aef:  * distributed under the License is distributed on an "AS IS" BASIS,
1:f533aef:  * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
1:f533aef:  * See the License for the specific language governing permissions and
1:f533aef:  * limitations under the License.
1:f533aef:  */
1:95736dd: 
1:5257bc9: package org.apache.mahout.classifier;
1:95736dd: 
1:a551b15: import com.google.common.collect.ConcurrentHashMultiset;
1:a551b15: import com.google.common.collect.Multiset;
1:a551b15: import com.google.common.io.Closeables;
1:a551b15: import com.google.common.io.Files;
1:a551b15: import org.apache.commons.io.Charsets;
1:a551b15: import org.apache.lucene.analysis.Analyzer;
1:a551b15: import org.apache.lucene.analysis.TokenStream;
1:a551b15: import org.apache.lucene.analysis.standard.StandardAnalyzer;
1:a551b15: import org.apache.lucene.analysis.tokenattributes.CharTermAttribute;
1:a551b15: import org.apache.mahout.common.RandomUtils;
1:a551b15: import org.apache.mahout.math.RandomAccessSparseVector;
1:a551b15: import org.apache.mahout.math.Vector;
1:a551b15: import org.apache.mahout.vectorizer.encoders.ConstantValueEncoder;
1:a551b15: import org.apache.mahout.vectorizer.encoders.FeatureVectorEncoder;
1:a551b15: import org.apache.mahout.vectorizer.encoders.StaticWordValueEncoder;
1:a551b15: 
1:95736dd: import java.io.BufferedReader;
1:95736dd: import java.io.File;
1:95736dd: import java.io.IOException;
1:95736dd: import java.io.Reader;
1:95736dd: import java.io.StringReader;
1:95736dd: import java.text.SimpleDateFormat;
1:95736dd: import java.util.Collection;
1:95736dd: import java.util.Date;
1:95736dd: import java.util.Locale;
1:95736dd: import java.util.Random;
1:4cff542: 
1:5257bc9: public final class NewsgroupHelper {
1:95736dd:   
1:e0ec7c1:   private static final SimpleDateFormat[] DATE_FORMATS = {
1:f533aef:     new SimpleDateFormat("", Locale.ENGLISH),
1:f533aef:     new SimpleDateFormat("MMM-yyyy", Locale.ENGLISH),
1:f533aef:     new SimpleDateFormat("dd-MMM-yyyy HH:mm:ss", Locale.ENGLISH)
1:95736dd:   };
1:95736dd: 
1:95736dd:   public static final int FEATURES = 10000;
1:95736dd:   // 1997-01-15 00:01:00 GMT
1:e0ec7c1:   private static final long DATE_REFERENCE = 853286460;
1:e0ec7c1:   private static final long MONTH = 30 * 24 * 3600;
1:e0ec7c1:   private static final long WEEK = 7 * 24 * 3600;
1:95736dd:   
1:e0ec7c1:   private final Random rand = RandomUtils.getRandom();  
1:4d0cd66:   private final Analyzer analyzer = new StandardAnalyzer();
1:e0ec7c1:   private final FeatureVectorEncoder encoder = new StaticWordValueEncoder("body");
1:e0ec7c1:   private final FeatureVectorEncoder bias = new ConstantValueEncoder("Intercept");
1:95736dd:   
1:5257bc9:   public FeatureVectorEncoder getEncoder() {
1:e0ec7c1:     return encoder;
1:e0ec7c1:   }
1:6d16230:   
1:5257bc9:   public FeatureVectorEncoder getBias() {
1:e0ec7c1:     return bias;
1:e0ec7c1:   }
1:3beecc0:   
1:5257bc9:   public Random getRandom() {
1:e0ec7c1:     return rand;
1:e0ec7c1:   }
4:e0ec7c1: 
1:5257bc9:   public Vector encodeFeatureVector(File file, int actual, int leakType, Multiset<String> overallCounts)
1:e0ec7c1:     throws IOException {
1:95736dd:     long date = (long) (1000 * (DATE_REFERENCE + actual * MONTH + 1 * WEEK * rand.nextDouble()));
1:95736dd:     Multiset<String> words = ConcurrentHashMultiset.create();
1:95736dd: 
1:4ef9d31:     try (BufferedReader reader = Files.newReader(file, Charsets.UTF_8)) {
1:95736dd:       String line = reader.readLine();
1:95736dd:       Reader dateString = new StringReader(DATE_FORMATS[leakType % 3].format(new Date(date)));
1:95736dd:       countWords(analyzer, words, dateString, overallCounts);
1:4fbfbc6:       while (line != null && !line.isEmpty()) {
1:95736dd:         boolean countHeader = (
1:6d16230:                 line.startsWith("From:") || line.startsWith("Subject:")
1:6d16230:                         || line.startsWith("Keywords:") || line.startsWith("Summary:")) && leakType < 6;
1:95736dd:         do {
1:95736dd:           Reader in = new StringReader(line);
1:95736dd:           if (countHeader) {
1:95736dd:             countWords(analyzer, words, in, overallCounts);
1:95736dd:           }
1:95736dd:           line = reader.readLine();
1:95736dd:         } while (line != null && line.startsWith(" "));
1:95736dd:       }
1:95736dd:       if (leakType < 3) {
1:95736dd:         countWords(analyzer, words, reader, overallCounts);
1:95736dd:       }
1:95736dd:     }
1:95736dd: 
1:95736dd:     Vector v = new RandomAccessSparseVector(FEATURES);
1:95736dd:     bias.addToVector("", 1, v);
1:95736dd:     for (String word : words.elementSet()) {
1:95736dd:       encoder.addToVector(word, Math.log1p(words.count(word)), v);
1:95736dd:     }
1:95736dd: 
1:95736dd:     return v;
1:95736dd:   }
1:95736dd: 
1:5257bc9:   public static void countWords(Analyzer analyzer,
1:051cbcf:                                  Collection<String> words,
1:051cbcf:                                  Reader in,
1:051cbcf:                                  Multiset<String> overallCounts) throws IOException {
1:6a4942c:     TokenStream ts = analyzer.tokenStream("text", in);
1:95736dd:     ts.addAttribute(CharTermAttribute.class);
1:95736dd:     ts.reset();
1:95736dd:     while (ts.incrementToken()) {
1:95736dd:       String s = ts.getAttribute(CharTermAttribute.class).toString();
1:95736dd:       words.add(s);
1:95736dd:     }
1:95736dd:     overallCounts.addAll(words);
1:6a4942c:     ts.end();
1:31cb292:     Closeables.close(ts, true);
1:95736dd:   }
1:95736dd: }
============================================================================
author:smarthi
-------------------------------------------------------------------------------
commit:4d0cd66
/////////////////////////////////////////////////////////////////////////
/////////////////////////////////////////////////////////////////////////
1:   private final Analyzer analyzer = new StandardAnalyzer();
commit:c88c240
/////////////////////////////////////////////////////////////////////////
0:   private final Analyzer analyzer = new StandardAnalyzer(Version.LUCENE_45);
commit:c36dc71
/////////////////////////////////////////////////////////////////////////
0:   private final Analyzer analyzer = new StandardAnalyzer(Version.LUCENE_42);
author:Andrew Musselman
-------------------------------------------------------------------------------
commit:864ba1a
/////////////////////////////////////////////////////////////////////////
0:   private final Analyzer analyzer = new StandardAnalyzer(Version.LUCENE_46);
commit:a551b15
/////////////////////////////////////////////////////////////////////////
1: import com.google.common.collect.ConcurrentHashMultiset;
1: import com.google.common.collect.Multiset;
1: import com.google.common.io.Closeables;
1: import com.google.common.io.Files;
1: import org.apache.commons.io.Charsets;
1: import org.apache.lucene.analysis.Analyzer;
1: import org.apache.lucene.analysis.TokenStream;
1: import org.apache.lucene.analysis.standard.StandardAnalyzer;
1: import org.apache.lucene.analysis.tokenattributes.CharTermAttribute;
0: import org.apache.lucene.util.Version;
1: import org.apache.mahout.common.RandomUtils;
1: import org.apache.mahout.math.RandomAccessSparseVector;
1: import org.apache.mahout.math.Vector;
1: import org.apache.mahout.vectorizer.encoders.ConstantValueEncoder;
1: import org.apache.mahout.vectorizer.encoders.FeatureVectorEncoder;
1: import org.apache.mahout.vectorizer.encoders.StaticWordValueEncoder;
1: 
/////////////////////////////////////////////////////////////////////////
/////////////////////////////////////////////////////////////////////////
0:   private final Analyzer analyzer = new StandardAnalyzer(Version.LUCENE_46);
author:Suneel Marthi
-------------------------------------------------------------------------------
commit:670a7d2
/////////////////////////////////////////////////////////////////////////
0:   private final Analyzer analyzer = new StandardAnalyzer();
commit:4cff542
/////////////////////////////////////////////////////////////////////////
/////////////////////////////////////////////////////////////////////////
0: import com.google.common.collect.ConcurrentHashMultiset;
0: import com.google.common.collect.Multiset;
0: import com.google.common.io.Closeables;
0: import com.google.common.io.Files;
0: import org.apache.commons.io.Charsets;
0: import org.apache.lucene.analysis.Analyzer;
0: import org.apache.lucene.analysis.TokenStream;
0: import org.apache.lucene.analysis.standard.StandardAnalyzer;
0: import org.apache.lucene.analysis.tokenattributes.CharTermAttribute;
0: import org.apache.mahout.common.RandomUtils;
0: import org.apache.mahout.math.RandomAccessSparseVector;
0: import org.apache.mahout.math.Vector;
0: import org.apache.mahout.vectorizer.encoders.ConstantValueEncoder;
0: import org.apache.mahout.vectorizer.encoders.FeatureVectorEncoder;
0: import org.apache.mahout.vectorizer.encoders.StaticWordValueEncoder;
1: 
/////////////////////////////////////////////////////////////////////////
0:   private final Analyzer analyzer = new StandardAnalyzer();
commit:4ef9d31
/////////////////////////////////////////////////////////////////////////
0: import org.apache.commons.io.Charsets;
/////////////////////////////////////////////////////////////////////////
1:     try (BufferedReader reader = Files.newReader(file, Charsets.UTF_8)) {
/////////////////////////////////////////////////////////////////////////
author:frankscholten
-------------------------------------------------------------------------------
commit:2e5449f
/////////////////////////////////////////////////////////////////////////
0:   private final Analyzer analyzer = new StandardAnalyzer(Version.LUCENE_46);
author:Grant Ingersoll
-------------------------------------------------------------------------------
commit:31cb292
/////////////////////////////////////////////////////////////////////////
0:       Closeables.close(reader, true);
/////////////////////////////////////////////////////////////////////////
1:     Closeables.close(ts, true);
commit:e48eb4c
/////////////////////////////////////////////////////////////////////////
0:   private final Analyzer analyzer = new StandardAnalyzer(Version.LUCENE_43);
commit:6a4942c
/////////////////////////////////////////////////////////////////////////
/////////////////////////////////////////////////////////////////////////
0:   private final Analyzer analyzer = new StandardAnalyzer(Version.LUCENE_41);
/////////////////////////////////////////////////////////////////////////
1:     TokenStream ts = analyzer.tokenStream("text", in);
/////////////////////////////////////////////////////////////////////////
1:     ts.end();
0:     Closeables.closeQuietly(ts);
commit:f533aef
/////////////////////////////////////////////////////////////////////////
1: /**
1:  * Licensed to the Apache Software Foundation (ASF) under one or more
1:  * contributor license agreements.  See the NOTICE file distributed with
1:  * this work for additional information regarding copyright ownership.
1:  * The ASF licenses this file to You under the Apache License, Version 2.0
1:  * (the "License"); you may not use this file except in compliance with
1:  * the License.  You may obtain a copy of the License at
1:  *
1:  *     http://www.apache.org/licenses/LICENSE-2.0
1:  *
1:  * Unless required by applicable law or agreed to in writing, software
1:  * distributed under the License is distributed on an "AS IS" BASIS,
1:  * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
1:  * See the License for the specific language governing permissions and
1:  * limitations under the License.
1:  */
/////////////////////////////////////////////////////////////////////////
1:           new SimpleDateFormat("", Locale.ENGLISH),
1:           new SimpleDateFormat("MMM-yyyy", Locale.ENGLISH),
1:           new SimpleDateFormat("dd-MMM-yyyy HH:mm:ss", Locale.ENGLISH)
/////////////////////////////////////////////////////////////////////////
0:                 line.startsWith("From:") || line.startsWith("Subject:") ||
0:                         line.startsWith("Keywords:") || line.startsWith("Summary:")) && leakType < 6;
commit:95736dd
/////////////////////////////////////////////////////////////////////////
0: package org.apache.mahout.classifier.sgd;
1: 
1: 
0: import com.google.common.base.Charsets;
0: import com.google.common.collect.ConcurrentHashMultiset;
0: import com.google.common.collect.Multiset;
0: import com.google.common.io.Closeables;
0: import com.google.common.io.Files;
0: import org.apache.lucene.analysis.Analyzer;
0: import org.apache.lucene.analysis.TokenStream;
0: import org.apache.lucene.analysis.standard.StandardAnalyzer;
0: import org.apache.lucene.analysis.tokenattributes.CharTermAttribute;
0: import org.apache.lucene.util.Version;
0: import org.apache.mahout.common.RandomUtils;
0: import org.apache.mahout.math.RandomAccessSparseVector;
0: import org.apache.mahout.math.Vector;
0: import org.apache.mahout.vectorizer.encoders.ConstantValueEncoder;
0: import org.apache.mahout.vectorizer.encoders.FeatureVectorEncoder;
0: import org.apache.mahout.vectorizer.encoders.StaticWordValueEncoder;
1: 
1: import java.io.BufferedReader;
1: import java.io.File;
1: import java.io.IOException;
1: import java.io.Reader;
1: import java.io.StringReader;
1: import java.text.SimpleDateFormat;
1: import java.util.Collection;
1: import java.util.Date;
1: import java.util.Locale;
1: import java.util.Random;
1: 
0: /**
0:  *
0:  *
0:  **/
0: public class NewsgroupHelper {
1: 
0:   static final Random rand = RandomUtils.getRandom();
0:   static final SimpleDateFormat[] DATE_FORMATS = {
0:     new SimpleDateFormat("", Locale.ENGLISH),
0:     new SimpleDateFormat("MMM-yyyy", Locale.ENGLISH),
0:     new SimpleDateFormat("dd-MMM-yyyy HH:mm:ss", Locale.ENGLISH)
1:   };
0:   static final Analyzer analyzer = new StandardAnalyzer(Version.LUCENE_31);
0:   static final FeatureVectorEncoder encoder = new StaticWordValueEncoder("body");
0:   static final FeatureVectorEncoder bias = new ConstantValueEncoder("Intercept");
1:   public static final int FEATURES = 10000;
1:   // 1997-01-15 00:01:00 GMT
0:   static final long DATE_REFERENCE = 853286460;
0:   static final long MONTH = 30 * 24 * 3600;
0:   static final long WEEK = 7 * 24 * 3600;
1: 
0:   static Vector encodeFeatureVector(File file, int actual, int leakType, Multiset<String> overallCounts) throws IOException {
1:     long date = (long) (1000 * (DATE_REFERENCE + actual * MONTH + 1 * WEEK * rand.nextDouble()));
1:     Multiset<String> words = ConcurrentHashMultiset.create();
1: 
0:     BufferedReader reader = Files.newReader(file, Charsets.UTF_8);
0:     try {
1:       String line = reader.readLine();
1:       Reader dateString = new StringReader(DATE_FORMATS[leakType % 3].format(new Date(date)));
1:       countWords(analyzer, words, dateString, overallCounts);
0:       while (line != null && line.length() > 0) {
1:         boolean countHeader = (
0:           line.startsWith("From:") || line.startsWith("Subject:") ||
0:             line.startsWith("Keywords:") || line.startsWith("Summary:")) && leakType < 6;
1:         do {
1:           Reader in = new StringReader(line);
1:           if (countHeader) {
1:             countWords(analyzer, words, in, overallCounts);
1:           }
1:           line = reader.readLine();
1:         } while (line != null && line.startsWith(" "));
1:       }
1:       if (leakType < 3) {
1:         countWords(analyzer, words, reader, overallCounts);
1:       }
0:     } finally {
0:       Closeables.closeQuietly(reader);
1:     }
1: 
1:     Vector v = new RandomAccessSparseVector(FEATURES);
1:     bias.addToVector("", 1, v);
1:     for (String word : words.elementSet()) {
1:       encoder.addToVector(word, Math.log1p(words.count(word)), v);
1:     }
1: 
1:     return v;
1:   }
1: 
0:   static void countWords(Analyzer analyzer, Collection<String> words, Reader in, Multiset<String> overallCounts) throws IOException {
0:     TokenStream ts = analyzer.reusableTokenStream("text", in);
1:     ts.addAttribute(CharTermAttribute.class);
1:     ts.reset();
1:     while (ts.incrementToken()) {
1:       String s = ts.getAttribute(CharTermAttribute.class).toString();
1:       words.add(s);
1:     }
1:     overallCounts.addAll(words);
1:   }
1: }
author:Sebastian Schelter
-------------------------------------------------------------------------------
commit:6d16230
/////////////////////////////////////////////////////////////////////////
0:     new SimpleDateFormat("", Locale.ENGLISH),
0:     new SimpleDateFormat("MMM-yyyy", Locale.ENGLISH),
0:     new SimpleDateFormat("dd-MMM-yyyy HH:mm:ss", Locale.ENGLISH)
1: 
/////////////////////////////////////////////////////////////////////////
1:                 line.startsWith("From:") || line.startsWith("Subject:")
1:                         || line.startsWith("Keywords:") || line.startsWith("Summary:")) && leakType < 6;
author:Robin Anil
-------------------------------------------------------------------------------
commit:3beecc0
/////////////////////////////////////////////////////////////////////////
0:   private static final Version LUCENE_VERSION = Version.LUCENE_36;
1:   
/////////////////////////////////////////////////////////////////////////
0:   private final Analyzer analyzer = new StandardAnalyzer(LUCENE_VERSION);
commit:5257bc9
/////////////////////////////////////////////////////////////////////////
1: package org.apache.mahout.classifier;
/////////////////////////////////////////////////////////////////////////
1: public final class NewsgroupHelper {
/////////////////////////////////////////////////////////////////////////
1:   public FeatureVectorEncoder getEncoder() {
1:   public FeatureVectorEncoder getBias() {
1:   public Random getRandom() {
1:   public Vector encodeFeatureVector(File file, int actual, int leakType, Multiset<String> overallCounts)
/////////////////////////////////////////////////////////////////////////
1:   public static void countWords(Analyzer analyzer,
author:Sean R. Owen
-------------------------------------------------------------------------------
commit:051cbcf
/////////////////////////////////////////////////////////////////////////
0:   private static void countWords(Analyzer analyzer,
1:                                  Collection<String> words,
1:                                  Reader in,
1:                                  Multiset<String> overallCounts) throws IOException {
commit:4fbfbc6
/////////////////////////////////////////////////////////////////////////
1:       while (line != null && !line.isEmpty()) {
commit:e0ec7c1
/////////////////////////////////////////////////////////////////////////
/////////////////////////////////////////////////////////////////////////
0: package org.apache.mahout.classifier.sgd;
/////////////////////////////////////////////////////////////////////////
0: final class NewsgroupHelper {
1:   private static final SimpleDateFormat[] DATE_FORMATS = {
1:   private static final long DATE_REFERENCE = 853286460;
1:   private static final long MONTH = 30 * 24 * 3600;
1:   private static final long WEEK = 7 * 24 * 3600;
1:   
1:   private final Random rand = RandomUtils.getRandom();  
0:   private final Analyzer analyzer = new StandardAnalyzer(Version.LUCENE_31);
1:   private final FeatureVectorEncoder encoder = new StaticWordValueEncoder("body");
1:   private final FeatureVectorEncoder bias = new ConstantValueEncoder("Intercept");
1:   
0:   FeatureVectorEncoder getEncoder() {
1:     return encoder;
1:   }
1:   
0:   FeatureVectorEncoder getBias() {
1:     return bias;
1:   }
1:   
0:   Random getRandom() {
1:     return rand;
1:   }
0:   Vector encodeFeatureVector(File file, int actual, int leakType, Multiset<String> overallCounts)
1:     throws IOException {
/////////////////////////////////////////////////////////////////////////
0:   static void countWords(Analyzer analyzer, Collection<String> words, Reader in, Multiset<String> overallCounts) 
0:     throws IOException {
============================================================================