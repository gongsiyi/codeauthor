1:b717cfc: /**
1:b717cfc:  * Licensed to the Apache Software Foundation (ASF) under one or more
1:b717cfc:  * contributor license agreements.  See the NOTICE file distributed with
1:b717cfc:  * this work for additional information regarding copyright ownership.
1:b717cfc:  * The ASF licenses this file to You under the Apache License, Version 2.0
1:b717cfc:  * (the "License"); you may not use this file except in compliance with
1:b717cfc:  * the License.  You may obtain a copy of the License at
1:b717cfc:  *
1:b717cfc:  *     http://www.apache.org/licenses/LICENSE-2.0
1:b717cfc:  *
1:b717cfc:  * Unless required by applicable law or agreed to in writing, software
1:b717cfc:  * distributed under the License is distributed on an "AS IS" BASIS,
1:b717cfc:  * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
1:b717cfc:  * See the License for the specific language governing permissions and
1:b717cfc:  * limitations under the License.
1:b717cfc:  */
1:b717cfc: 
1:b717cfc: package org.apache.mahout.math.hadoop.stochasticsvd;
1:b717cfc: 
1:b717cfc: import com.google.common.collect.Lists;
1:b717cfc: import com.google.common.io.Closeables;
1:b717cfc: import org.apache.hadoop.conf.Configuration;
1:b717cfc: import org.apache.hadoop.fs.FileSystem;
1:b717cfc: import org.apache.hadoop.fs.Path;
1:b717cfc: import org.apache.hadoop.io.SequenceFile;
1:b717cfc: import org.apache.hadoop.io.SequenceFile.CompressionType;
1:b717cfc: import org.apache.hadoop.io.Text;
1:b717cfc: import org.apache.hadoop.io.Writable;
1:b717cfc: import org.apache.hadoop.io.compress.DefaultCodec;
1:b717cfc: import org.apache.mahout.common.IOUtils;
1:b717cfc: import org.apache.mahout.common.MahoutTestCase;
1:b717cfc: import org.apache.mahout.common.Pair;
1:b717cfc: import org.apache.mahout.common.RandomUtils;
1:b717cfc: import org.apache.mahout.math.*;
1:b717cfc: import org.apache.mahout.math.function.DoubleFunction;
1:b717cfc: import org.apache.mahout.math.function.Functions;
1:b717cfc: import org.apache.mahout.math.function.VectorFunction;
1:b717cfc: import org.junit.Test;
1:b717cfc: 
1:b717cfc: import java.io.Closeable;
1:b717cfc: import java.io.File;
1:b717cfc: import java.io.IOException;
1:b717cfc: import java.util.Deque;
1:b717cfc: import java.util.Iterator;
1:b717cfc: import java.util.Random;
1:b717cfc: 
1:b717cfc: public class LocalSSVDPCASparseTest extends MahoutTestCase {
1:b717cfc: 
1:b717cfc:   private static final double s_epsilon = 1.0E-10d;
1:b717cfc: 
1:b717cfc:   @Test
1:b717cfc:   public void testOmegaTRightMultiply() {
1:b717cfc:     final Random rnd = RandomUtils.getRandom();
1:b717cfc:     final long seed = rnd.nextLong();
1:b717cfc:     final int n = 2000;
1:b717cfc: 
1:b717cfc:     final int kp = 100;
1:b717cfc: 
1:b717cfc:     final Omega omega = new Omega(seed, kp);
1:b717cfc:     final Matrix materializedOmega = new DenseMatrix(n, kp);
1:b717cfc:     for (int i = 0; i < n; i++)
1:b717cfc:       for (int j = 0; j < kp; j++)
1:b717cfc:         materializedOmega.setQuick(i, j, omega.getQuick(i, j));
1:b717cfc:     Vector xi = new DenseVector(n);
1:b717cfc:     xi.assign(new DoubleFunction() {
1:b717cfc:       @Override
1:b717cfc:       public double apply(double x) {
1:b717cfc:         return rnd.nextDouble() * 100;
1:b717cfc:       }
1:b717cfc:     });
1:b717cfc: 
1:b717cfc:     Vector s_o = omega.mutlithreadedTRightMultiply(xi);
1:b717cfc: 
1:b717cfc:     Matrix xiVector = new DenseMatrix(n, 1);
1:b717cfc:     xiVector.assignColumn(0, xi);
1:b717cfc: 
1:b717cfc:     Vector s_o_control = materializedOmega.transpose().times(xiVector).viewColumn(0);
1:b717cfc: 
1:b717cfc:     assertEquals(0, s_o.minus(s_o_control).aggregate(Functions.PLUS, Functions.ABS), 1e-10);
1:b717cfc: 
1:b717cfc:     System.out.printf("s_omega=\n%s\n", s_o);
1:b717cfc:     System.out.printf("s_omega_control=\n%s\n", s_o_control);
1:b717cfc:   }
1:b717cfc: 
1:b717cfc:   @Test
1:b717cfc:   public void runPCATest1() throws IOException {
1:b717cfc:     runSSVDSolver(1);
1:b717cfc:   }
1:b717cfc: 
1:b717cfc: //  @Test
1:b717cfc:   public void runPCATest0() throws IOException {
1:b717cfc:     runSSVDSolver(0);
1:b717cfc:   }
1:b717cfc: 
1:b717cfc: 
1:b717cfc:   public void runSSVDSolver(int q) throws IOException {
1:b717cfc: 
1:b717cfc:     Configuration conf = new Configuration();
1:b717cfc:     conf.set("mapred.job.tracker", "local");
1:b717cfc:     conf.set("fs.default.name", "file:///");
1:b717cfc: 
1:b717cfc:     // conf.set("mapred.job.tracker","localhost:11011");
1:b717cfc:     // conf.set("fs.default.name","hdfs://localhost:11010/");
1:b717cfc: 
1:b717cfc:     Deque<Closeable> closeables = Lists.newLinkedList();
1:b717cfc:     try {
1:b717cfc:       Random rnd = RandomUtils.getRandom();
1:b717cfc: 
1:b717cfc:       File tmpDir = getTestTempDir("svdtmp");
1:b717cfc:       conf.set("hadoop.tmp.dir", tmpDir.getAbsolutePath());
1:b717cfc: 
1:b717cfc:       Path aLocPath = new Path(getTestTempDirPath("svdtmp/A"), "A.seq");
1:b717cfc: 
1:b717cfc:       // create distributed row matrix-like struct
1:b717cfc:       SequenceFile.Writer w =
1:b717cfc:         SequenceFile.createWriter(FileSystem.getLocal(conf),
1:b717cfc:                                   conf,
1:b717cfc:                                   aLocPath,
1:b717cfc:                                   Text.class,
1:b717cfc:                                   VectorWritable.class,
1:b717cfc:                                   CompressionType.BLOCK,
1:b717cfc:                                   new DefaultCodec());
1:b717cfc:       closeables.addFirst(w);
1:b717cfc: 
1:b717cfc:       int n = 100;
1:b717cfc:       int m = 2000;
1:b717cfc:       double percent = 5;
1:b717cfc: 
1:b717cfc:       VectorWritable vw = new VectorWritable();
1:b717cfc:       Text rkey = new Text();
1:b717cfc: 
1:b717cfc:       Vector xi = new DenseVector(n);
1:b717cfc: 
1:b717cfc:       double muAmplitude = 50.0;
1:b717cfc:       for (int i = 0; i < m; i++) {
1:b717cfc:         Vector dv = new SequentialAccessSparseVector(n);
1:b717cfc:         String rowname = "row-"+i;
1:b717cfc:         NamedVector namedRow = new NamedVector(dv, rowname);
1:b717cfc:         for (int j = 0; j < n * percent / 100; j++) {
1:b717cfc:           dv.setQuick(rnd.nextInt(n), muAmplitude * (rnd.nextDouble() - 0.25));
1:b717cfc:         }
1:b717cfc:         rkey.set("row-i"+i);
1:b717cfc:         vw.set(namedRow);
1:b717cfc:         w.append(rkey, vw);
1:b717cfc:         xi.assign(dv, Functions.PLUS);
1:b717cfc:       }
1:b717cfc:       closeables.remove(w);
1:b717cfc:       Closeables.close(w, false);
1:b717cfc: 
1:b717cfc:       xi.assign(Functions.mult(1.0 / m));
1:b717cfc: 
1:b717cfc:       FileSystem fs = FileSystem.get(conf);
1:b717cfc: 
1:b717cfc:       Path tempDirPath = getTestTempDirPath("svd-proc");
1:b717cfc:       Path aPath = new Path(tempDirPath, "A/A.seq");
1:b717cfc:       fs.copyFromLocalFile(aLocPath, aPath);
1:b717cfc:       Path xiPath = new Path(tempDirPath, "xi/xi.seq");
1:b717cfc:       SSVDHelper.saveVector(xi, xiPath, conf);
1:b717cfc: 
1:b717cfc:       Path svdOutPath = new Path(tempDirPath, "SSVD-out");
1:b717cfc: 
1:b717cfc:       // make sure we wipe out previous test results, just a convenience
1:b717cfc:       fs.delete(svdOutPath, true);
1:b717cfc: 
1:b717cfc:       // Solver starts here:
1:b717cfc:       System.out.println("Input prepared, starting solver...");
1:b717cfc: 
1:b717cfc:       int ablockRows = 867;
1:b717cfc:       int p = 60;
1:b717cfc:       int k = 40;
1:b717cfc:       SSVDSolver ssvd =
1:b717cfc:         new SSVDSolver(conf,
1:b717cfc:                        new Path[]{aPath},
1:b717cfc:                        svdOutPath,
1:b717cfc:                        ablockRows,
1:b717cfc:                        k,
1:b717cfc:                        p,
1:b717cfc:                        3);
1:b717cfc:       ssvd.setOuterBlockHeight(500);
1:b717cfc:       ssvd.setAbtBlockHeight(251);
1:b717cfc:       ssvd.setPcaMeanPath(xiPath);
1:b717cfc: 
1:b717cfc:     /*
1:b717cfc:      * Removing V,U jobs from this test to reduce running time. i will keep them
1:b717cfc:      * put in the dense test though.
1:b717cfc:      *
1:b717cfc:      * For PCA test, we also want to request U*Sigma output and check it for named
1:b717cfc:      * vector propagation.
1:b717cfc:      */
1:b717cfc:       ssvd.setComputeU(false);
1:b717cfc:       ssvd.setComputeV(false);
1:b717cfc:       ssvd.setcUSigma(true);
1:b717cfc: 
1:b717cfc:       ssvd.setOverwrite(true);
1:b717cfc:       ssvd.setQ(q);
1:b717cfc:       ssvd.setBroadcast(true);
1:b717cfc:       ssvd.run();
1:b717cfc: 
1:b717cfc:       Vector stochasticSValues = ssvd.getSingularValues();
1:b717cfc: 
1:b717cfc:       // try to run the same thing without stochastic algo
1:b717cfc:       Matrix a = SSVDHelper.drmLoadAsDense(fs, aPath, conf);
1:b717cfc: 
1:b717cfc:       verifyInternals(svdOutPath, a, new Omega(ssvd.getOmegaSeed(), k + p), k + p, q);
1:b717cfc: 
1:b717cfc:       // subtract pseudo pca mean
1:b717cfc:       for (int i = 0; i < m; i++) {
1:b717cfc:         a.viewRow(i).assign(xi, Functions.MINUS);
1:b717cfc:       }
1:b717cfc: 
1:b717cfc:       SingularValueDecomposition svd2 =
1:b717cfc:         new SingularValueDecomposition(a);
1:b717cfc: 
1:b717cfc:       Vector svalues2 = new DenseVector(svd2.getSingularValues());
1:b717cfc: 
1:b717cfc:       System.out.println("--SSVD solver singular values:");
1:b717cfc:       LocalSSVDSolverSparseSequentialTest.dumpSv(stochasticSValues);
1:b717cfc:       System.out.println("--SVD solver singular values:");
1:b717cfc:       LocalSSVDSolverSparseSequentialTest.dumpSv(svalues2);
1:b717cfc: 
1:b717cfc:       for (int i = 0; i < k + p; i++) {
1:b717cfc:         assertTrue(Math.abs(svalues2.getQuick(i) - stochasticSValues.getQuick(i)) <= s_epsilon);
1:b717cfc:       }
1:b717cfc: 
1:b717cfc:       DenseMatrix mQ =
1:b717cfc:         SSVDHelper.drmLoadAsDense(fs, new Path(svdOutPath, "Bt-job/"
1:b717cfc:           + BtJob.OUTPUT_Q + "-*"), conf);
1:b717cfc: 
1:b717cfc:       SSVDCommonTest.assertOrthonormality(mQ,
1:b717cfc:                                           false,
1:b717cfc:                                           s_epsilon);
1:b717cfc: 
1:b717cfc:       // assert name propagation
1:b717cfc:       for (Iterator<Pair<Writable, Vector>> iter = SSVDHelper.drmIterator(fs,
1:b717cfc:                                                                           new Path(ssvd.getuSigmaPath()+"/*"),
1:b717cfc:                                                                           conf,
1:b717cfc:                                                                           closeables); iter.hasNext(); ) {
1:b717cfc:         Pair<Writable, Vector> pair = iter.next();
1:b717cfc:         Writable key = pair.getFirst();
1:b717cfc:         Vector v = pair.getSecond();
1:b717cfc: 
1:b717cfc:         assertTrue(v instanceof NamedVector);
1:b717cfc:         assertTrue(key instanceof Text);
1:b717cfc:       }
1:b717cfc: 
1:b717cfc:     } finally {
1:b717cfc:       IOUtils.close(closeables);
1:b717cfc:     }
1:b717cfc:   }
1:b717cfc: 
1:b717cfc:   private void verifyInternals(Path tempDir, Matrix a, Omega omega, int kp, int q) {
1:b717cfc:     int m = a.numRows();
1:b717cfc:     int n = a.numCols();
1:b717cfc: 
1:b717cfc:     Vector xi = a.aggregateColumns(new VectorFunction() {
1:b717cfc:       @Override
1:b717cfc:       public double apply(Vector v) {
1:b717cfc:         return v.zSum() / v.size();
1:b717cfc:       }
1:b717cfc:     });
1:b717cfc: 
1:b717cfc:     // materialize omega
1:b717cfc:     Matrix momega = new DenseMatrix(n, kp);
1:b717cfc:     for (int i = 0; i < n; i++)
1:b717cfc:       for (int j = 0; j < kp; j++)
1:b717cfc:         momega.setQuick(i, j, omega.getQuick(i, j));
1:b717cfc: 
1:b717cfc:     Vector s_o = omega.mutlithreadedTRightMultiply(xi);
1:b717cfc: 
1:b717cfc:     System.out.printf("s_omega=\n%s\n", s_o);
1:b717cfc: 
1:b717cfc:     Matrix y = a.times(momega);
1:b717cfc:     for (int i = 0; i < n; i++) y.viewRow(i).assign(s_o, Functions.MINUS);
1:b717cfc: 
1:b717cfc:     QRDecomposition qr = new QRDecomposition(y);
1:b717cfc:     Matrix qm = qr.getQ();
1:b717cfc: 
1:b717cfc:     Vector s_q = qm.aggregateColumns(new VectorFunction() {
1:b717cfc:       @Override
1:b717cfc:       public double apply(Vector v) {
1:b717cfc:         return v.zSum();
1:b717cfc:       }
1:b717cfc:     });
1:b717cfc: 
1:b717cfc:     System.out.printf("s_q=\n%s\n", s_q);
1:b717cfc: 
1:b717cfc:     Matrix b = qm.transpose().times(a);
1:b717cfc: 
1:b717cfc:     Vector s_b = b.times(xi);
1:b717cfc: 
1:b717cfc:     System.out.printf("s_b=\n%s\n", s_b);
1:b717cfc: 
1:b717cfc: 
1:b717cfc:   }
1:b717cfc: 
1:b717cfc: }
============================================================================
author:pferrel
-------------------------------------------------------------------------------
commit:b988c49
author:frankscholten
-------------------------------------------------------------------------------
commit:1a42d85
author:sslavic
-------------------------------------------------------------------------------
commit:9d58fc8
/////////////////////////////////////////////////////////////////////////
author:Dmitriy Lyubimov
-------------------------------------------------------------------------------
commit:b717cfc
/////////////////////////////////////////////////////////////////////////
1: /**
1:  * Licensed to the Apache Software Foundation (ASF) under one or more
1:  * contributor license agreements.  See the NOTICE file distributed with
1:  * this work for additional information regarding copyright ownership.
1:  * The ASF licenses this file to You under the Apache License, Version 2.0
1:  * (the "License"); you may not use this file except in compliance with
1:  * the License.  You may obtain a copy of the License at
1:  *
1:  *     http://www.apache.org/licenses/LICENSE-2.0
1:  *
1:  * Unless required by applicable law or agreed to in writing, software
1:  * distributed under the License is distributed on an "AS IS" BASIS,
1:  * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
1:  * See the License for the specific language governing permissions and
1:  * limitations under the License.
1:  */
1: 
1: package org.apache.mahout.math.hadoop.stochasticsvd;
1: 
1: import com.google.common.collect.Lists;
1: import com.google.common.io.Closeables;
1: import org.apache.hadoop.conf.Configuration;
1: import org.apache.hadoop.fs.FileSystem;
1: import org.apache.hadoop.fs.Path;
0: import org.apache.hadoop.io.IntWritable;
1: import org.apache.hadoop.io.SequenceFile;
1: import org.apache.hadoop.io.SequenceFile.CompressionType;
1: import org.apache.hadoop.io.Text;
1: import org.apache.hadoop.io.Writable;
1: import org.apache.hadoop.io.compress.DefaultCodec;
1: import org.apache.mahout.common.IOUtils;
1: import org.apache.mahout.common.MahoutTestCase;
1: import org.apache.mahout.common.Pair;
1: import org.apache.mahout.common.RandomUtils;
1: import org.apache.mahout.math.*;
1: import org.apache.mahout.math.function.DoubleFunction;
1: import org.apache.mahout.math.function.Functions;
1: import org.apache.mahout.math.function.VectorFunction;
1: import org.junit.Test;
1: 
1: import java.io.Closeable;
1: import java.io.File;
1: import java.io.IOException;
1: import java.util.Deque;
1: import java.util.Iterator;
1: import java.util.Random;
1: 
1: public class LocalSSVDPCASparseTest extends MahoutTestCase {
1: 
1:   private static final double s_epsilon = 1.0E-10d;
1: 
1:   @Test
1:   public void testOmegaTRightMultiply() {
1:     final Random rnd = RandomUtils.getRandom();
1:     final long seed = rnd.nextLong();
1:     final int n = 2000;
1: 
1:     final int kp = 100;
1: 
1:     final Omega omega = new Omega(seed, kp);
1:     final Matrix materializedOmega = new DenseMatrix(n, kp);
1:     for (int i = 0; i < n; i++)
1:       for (int j = 0; j < kp; j++)
1:         materializedOmega.setQuick(i, j, omega.getQuick(i, j));
1:     Vector xi = new DenseVector(n);
1:     xi.assign(new DoubleFunction() {
1:       @Override
1:       public double apply(double x) {
1:         return rnd.nextDouble() * 100;
1:       }
1:     });
1: 
1:     Vector s_o = omega.mutlithreadedTRightMultiply(xi);
1: 
1:     Matrix xiVector = new DenseMatrix(n, 1);
1:     xiVector.assignColumn(0, xi);
1: 
1:     Vector s_o_control = materializedOmega.transpose().times(xiVector).viewColumn(0);
1: 
1:     assertEquals(0, s_o.minus(s_o_control).aggregate(Functions.PLUS, Functions.ABS), 1e-10);
1: 
1:     System.out.printf("s_omega=\n%s\n", s_o);
1:     System.out.printf("s_omega_control=\n%s\n", s_o_control);
1:   }
1: 
1:   @Test
1:   public void runPCATest1() throws IOException {
1:     runSSVDSolver(1);
1:   }
1: 
1: //  @Test
1:   public void runPCATest0() throws IOException {
1:     runSSVDSolver(0);
1:   }
1: 
1: 
1:   public void runSSVDSolver(int q) throws IOException {
1: 
1:     Configuration conf = new Configuration();
1:     conf.set("mapred.job.tracker", "local");
1:     conf.set("fs.default.name", "file:///");
1: 
1:     // conf.set("mapred.job.tracker","localhost:11011");
1:     // conf.set("fs.default.name","hdfs://localhost:11010/");
1: 
1:     Deque<Closeable> closeables = Lists.newLinkedList();
1:     try {
1:       Random rnd = RandomUtils.getRandom();
1: 
1:       File tmpDir = getTestTempDir("svdtmp");
1:       conf.set("hadoop.tmp.dir", tmpDir.getAbsolutePath());
1: 
1:       Path aLocPath = new Path(getTestTempDirPath("svdtmp/A"), "A.seq");
1: 
1:       // create distributed row matrix-like struct
1:       SequenceFile.Writer w =
1:         SequenceFile.createWriter(FileSystem.getLocal(conf),
1:                                   conf,
1:                                   aLocPath,
1:                                   Text.class,
1:                                   VectorWritable.class,
1:                                   CompressionType.BLOCK,
1:                                   new DefaultCodec());
1:       closeables.addFirst(w);
1: 
1:       int n = 100;
1:       int m = 2000;
1:       double percent = 5;
1: 
1:       VectorWritable vw = new VectorWritable();
1:       Text rkey = new Text();
1: 
1:       Vector xi = new DenseVector(n);
1: 
1:       double muAmplitude = 50.0;
1:       for (int i = 0; i < m; i++) {
1:         Vector dv = new SequentialAccessSparseVector(n);
1:         String rowname = "row-"+i;
1:         NamedVector namedRow = new NamedVector(dv, rowname);
1:         for (int j = 0; j < n * percent / 100; j++) {
1:           dv.setQuick(rnd.nextInt(n), muAmplitude * (rnd.nextDouble() - 0.25));
1:         }
1:         rkey.set("row-i"+i);
1:         vw.set(namedRow);
1:         w.append(rkey, vw);
1:         xi.assign(dv, Functions.PLUS);
1:       }
1:       closeables.remove(w);
1:       Closeables.close(w, false);
1: 
1:       xi.assign(Functions.mult(1.0 / m));
1: 
1:       FileSystem fs = FileSystem.get(conf);
1: 
1:       Path tempDirPath = getTestTempDirPath("svd-proc");
1:       Path aPath = new Path(tempDirPath, "A/A.seq");
1:       fs.copyFromLocalFile(aLocPath, aPath);
1:       Path xiPath = new Path(tempDirPath, "xi/xi.seq");
1:       SSVDHelper.saveVector(xi, xiPath, conf);
1: 
1:       Path svdOutPath = new Path(tempDirPath, "SSVD-out");
1: 
1:       // make sure we wipe out previous test results, just a convenience
1:       fs.delete(svdOutPath, true);
1: 
1:       // Solver starts here:
1:       System.out.println("Input prepared, starting solver...");
1: 
1:       int ablockRows = 867;
1:       int p = 60;
1:       int k = 40;
1:       SSVDSolver ssvd =
1:         new SSVDSolver(conf,
1:                        new Path[]{aPath},
1:                        svdOutPath,
1:                        ablockRows,
1:                        k,
1:                        p,
1:                        3);
1:       ssvd.setOuterBlockHeight(500);
1:       ssvd.setAbtBlockHeight(251);
1:       ssvd.setPcaMeanPath(xiPath);
1: 
1:     /*
1:      * Removing V,U jobs from this test to reduce running time. i will keep them
1:      * put in the dense test though.
1:      *
1:      * For PCA test, we also want to request U*Sigma output and check it for named
1:      * vector propagation.
1:      */
1:       ssvd.setComputeU(false);
1:       ssvd.setComputeV(false);
1:       ssvd.setcUSigma(true);
1: 
1:       ssvd.setOverwrite(true);
1:       ssvd.setQ(q);
1:       ssvd.setBroadcast(true);
1:       ssvd.run();
1: 
1:       Vector stochasticSValues = ssvd.getSingularValues();
1: 
1:       // try to run the same thing without stochastic algo
1:       Matrix a = SSVDHelper.drmLoadAsDense(fs, aPath, conf);
1: 
1:       verifyInternals(svdOutPath, a, new Omega(ssvd.getOmegaSeed(), k + p), k + p, q);
1: 
1:       // subtract pseudo pca mean
1:       for (int i = 0; i < m; i++) {
1:         a.viewRow(i).assign(xi, Functions.MINUS);
1:       }
1: 
1:       SingularValueDecomposition svd2 =
1:         new SingularValueDecomposition(a);
1: 
1:       Vector svalues2 = new DenseVector(svd2.getSingularValues());
1: 
1:       System.out.println("--SSVD solver singular values:");
1:       LocalSSVDSolverSparseSequentialTest.dumpSv(stochasticSValues);
1:       System.out.println("--SVD solver singular values:");
1:       LocalSSVDSolverSparseSequentialTest.dumpSv(svalues2);
1: 
1:       for (int i = 0; i < k + p; i++) {
1:         assertTrue(Math.abs(svalues2.getQuick(i) - stochasticSValues.getQuick(i)) <= s_epsilon);
1:       }
1: 
1:       DenseMatrix mQ =
1:         SSVDHelper.drmLoadAsDense(fs, new Path(svdOutPath, "Bt-job/"
1:           + BtJob.OUTPUT_Q + "-*"), conf);
1: 
1:       SSVDCommonTest.assertOrthonormality(mQ,
1:                                           false,
1:                                           s_epsilon);
1: 
1:       // assert name propagation
1:       for (Iterator<Pair<Writable, Vector>> iter = SSVDHelper.drmIterator(fs,
1:                                                                           new Path(ssvd.getuSigmaPath()+"/*"),
1:                                                                           conf,
1:                                                                           closeables); iter.hasNext(); ) {
1:         Pair<Writable, Vector> pair = iter.next();
1:         Writable key = pair.getFirst();
1:         Vector v = pair.getSecond();
1: 
1:         assertTrue(v instanceof NamedVector);
1:         assertTrue(key instanceof Text);
1:       }
1: 
1:     } finally {
1:       IOUtils.close(closeables);
1:     }
1:   }
1: 
1:   private void verifyInternals(Path tempDir, Matrix a, Omega omega, int kp, int q) {
1:     int m = a.numRows();
1:     int n = a.numCols();
1: 
1:     Vector xi = a.aggregateColumns(new VectorFunction() {
1:       @Override
1:       public double apply(Vector v) {
1:         return v.zSum() / v.size();
1:       }
1:     });
1: 
1:     // materialize omega
1:     Matrix momega = new DenseMatrix(n, kp);
1:     for (int i = 0; i < n; i++)
1:       for (int j = 0; j < kp; j++)
1:         momega.setQuick(i, j, omega.getQuick(i, j));
1: 
1:     Vector s_o = omega.mutlithreadedTRightMultiply(xi);
1: 
1:     System.out.printf("s_omega=\n%s\n", s_o);
1: 
1:     Matrix y = a.times(momega);
1:     for (int i = 0; i < n; i++) y.viewRow(i).assign(s_o, Functions.MINUS);
1: 
1:     QRDecomposition qr = new QRDecomposition(y);
1:     Matrix qm = qr.getQ();
1: 
1:     Vector s_q = qm.aggregateColumns(new VectorFunction() {
1:       @Override
1:       public double apply(Vector v) {
1:         return v.zSum();
1:       }
1:     });
1: 
1:     System.out.printf("s_q=\n%s\n", s_q);
1: 
1:     Matrix b = qm.transpose().times(a);
1: 
1:     Vector s_b = b.times(xi);
1: 
1:     System.out.printf("s_b=\n%s\n", s_b);
1: 
1: 
1:   }
1: 
1: }
============================================================================