1:6a3f566: /**
1:6a3f566:  * Licensed to the Apache Software Foundation (ASF) under one or more
1:6a3f566:  * contributor license agreements.  See the NOTICE file distributed with
1:6a3f566:  * this work for additional information regarding copyright ownership.
1:6a3f566:  * The ASF licenses this file to You under the Apache License, Version 2.0
1:6a3f566:  * (the "License"); you may not use this file except in compliance with
1:6a3f566:  * the License.  You may obtain a copy of the License at
1:6a3f566:  *
1:6a3f566:  *     http://www.apache.org/licenses/LICENSE-2.0
1:6a3f566:  *
1:6a3f566:  * Unless required by applicable law or agreed to in writing, software
1:6a3f566:  * distributed under the License is distributed on an "AS IS" BASIS,
1:6a3f566:  * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
1:6a3f566:  * See the License for the specific language governing permissions and
1:6a3f566:  * limitations under the License.
1:6a3f566:  */
6:6a3f566: 
1:6a3f566: package org.apache.mahout.clustering.classify;
1:6a3f566: 
1:6a3f566: import java.io.IOException;
1:85f9ece: import java.util.ArrayList;
1:85f9ece: import java.util.HashMap;
1:6a3f566: import java.util.Iterator;
1:6a3f566: import java.util.List;
1:8253491: import java.util.Map;
1:6a3f566: 
1:6a3f566: import org.apache.hadoop.conf.Configuration;
1:6a3f566: import org.apache.hadoop.fs.FileStatus;
1:6a3f566: import org.apache.hadoop.fs.FileSystem;
1:6a3f566: import org.apache.hadoop.fs.Path;
1:6a3f566: import org.apache.hadoop.io.IntWritable;
1:8253491: import org.apache.hadoop.io.Text;
1:6a3f566: import org.apache.hadoop.io.Writable;
1:73ff886: import org.apache.hadoop.io.WritableComparable;
1:6a3f566: import org.apache.hadoop.mapreduce.Mapper;
1:6a3f566: import org.apache.mahout.clustering.Cluster;
1:51f58b8: import org.apache.mahout.clustering.iterator.ClusterWritable;
1:8d102ea: import org.apache.mahout.clustering.iterator.ClusteringPolicy;
1:4d4efb0: import org.apache.mahout.clustering.iterator.DistanceMeasureCluster;
1:4d4efb0: import org.apache.mahout.common.distance.DistanceMeasure;
1:6a3f566: import org.apache.mahout.common.iterator.sequencefile.PathFilters;
1:6a3f566: import org.apache.mahout.common.iterator.sequencefile.PathType;
1:6a3f566: import org.apache.mahout.common.iterator.sequencefile.SequenceFileDirValueIterator;
1:e93f05d: import org.apache.mahout.math.NamedVector;
1:6a3f566: import org.apache.mahout.math.Vector;
1:1371326: import org.apache.mahout.math.Vector.Element;
1:6a3f566: import org.apache.mahout.math.VectorWritable;
1:6a3f566: 
1:6a3f566: /**
1:6a3f566:  * Mapper for classifying vectors into clusters.
1:6a3f566:  */
1:1371326: public class ClusterClassificationMapper extends
1:73ff886:     Mapper<WritableComparable<?>,VectorWritable,IntWritable,WeightedVectorWritable> {
1:6a3f566:   
1:229aeff:   private double threshold;
1:6a3f566:   private List<Cluster> clusterModels;
1:6a3f566:   private ClusterClassifier clusterClassifier;
1:6a3f566:   private IntWritable clusterId;
1:1371326:   private boolean emitMostLikely;
1:6a3f566:   
2:6a3f566:   @Override
1:229aeff:   protected void setup(Context context) throws IOException, InterruptedException {
1:457db94:     super.setup(context);
1:457db94:     
1:457db94:     Configuration conf = context.getConfiguration();
1:229aeff:     String clustersIn = conf.get(ClusterClassificationConfigKeys.CLUSTERS_IN);
1:229aeff:     threshold = conf.getFloat(ClusterClassificationConfigKeys.OUTLIER_REMOVAL_THRESHOLD, 0.0f);
1:229aeff:     emitMostLikely = conf.getBoolean(ClusterClassificationConfigKeys.EMIT_MOST_LIKELY, false);
1:457db94:     
1:85f9ece:     clusterModels = new ArrayList<>();
1:457db94:     
1:457db94:     if (clustersIn != null && !clustersIn.isEmpty()) {
1:1371326:       Path clustersInPath = new Path(clustersIn);
1:ba81a93:       clusterModels = populateClusterModels(clustersInPath, conf);
1:1371326:       ClusteringPolicy policy = ClusterClassifier
1:1371326:           .readPolicy(finalClustersPath(clustersInPath));
1:457db94:       clusterClassifier = new ClusterClassifier(clusterModels, policy);
1:457db94:     }
1:457db94:     clusterId = new IntWritable();
1:457db94:   }
1:457db94:   
1:1371326:   /**
1:1371326:    * Mapper which classifies the vectors to respective clusters.
1:1371326:    */
1:457db94:   @Override
1:73ff886:   protected void map(WritableComparable<?> key, VectorWritable vw, Context context)
1:6d16230:     throws IOException, InterruptedException {
1:457db94:     if (!clusterModels.isEmpty()) {
1:08da368:       // Converting to NamedVectors to preserve the vectorId else its not obvious as to which point
1:08da368:       // belongs to which cluster - fix for MAHOUT-1410
1:ab6216f:       Class<? extends Vector> vectorClass = vw.get().getClass();
1:e93f05d:       Vector vector = vw.get();
1:ab6216f:       if (!vectorClass.equals(NamedVector.class)) {
1:ab6216f:         if (key.getClass().equals(Text.class)) {
1:e93f05d:           vector = new NamedVector(vector, key.toString());
1:ab6216f:         } else if (key.getClass().equals(IntWritable.class)) {
1:e93f05d:           vector = new NamedVector(vector, Integer.toString(((IntWritable) key).get()));
1:e93f05d:         }
1:e93f05d:       }
1:e93f05d:       Vector pdfPerCluster = clusterClassifier.classify(vector);
1:457db94:       if (shouldClassify(pdfPerCluster)) {
1:1371326:         if (emitMostLikely) {
1:1371326:           int maxValueIndex = pdfPerCluster.maxValueIndex();
1:e93f05d:           write(new VectorWritable(vector), context, maxValueIndex, 1.0);
1:1371326:         } else {
1:e93f05d:           writeAllAboveThreshold(new VectorWritable(vector), context, pdfPerCluster);
1:1371326:         }
4:6a3f566:       }
1:6a3f566:     }
1:6a3f566:   }
1:6a3f566:   
1:1371326:   private void writeAllAboveThreshold(VectorWritable vw, Context context,
1:1371326:       Vector pdfPerCluster) throws IOException, InterruptedException {
1:dc62944:     for (Element pdf : pdfPerCluster.nonZeroes()) {
1:1371326:       if (pdf.get() >= threshold) {
1:1371326:         int clusterIndex = pdf.index();
1:0ab9b15:         write(vw, context, clusterIndex, pdf.get());
1:1371326:       }
1:1371326:     }
1:1371326:   }
1:1371326:   
1:0ab9b15:   private void write(VectorWritable vw, Context context, int clusterIndex, double weight)
1:6d16230:     throws IOException, InterruptedException {
1:1371326:     Cluster cluster = clusterModels.get(clusterIndex);
1:1371326:     clusterId.set(cluster.getId());
1:4d4efb0: 
1:4d4efb0:     DistanceMeasureCluster distanceMeasureCluster = (DistanceMeasureCluster) cluster;
1:4d4efb0:     DistanceMeasure distanceMeasure = distanceMeasureCluster.getMeasure();
1:4d4efb0:     double distance = distanceMeasure.distance(cluster.getCenter(), vw.get());
1:4d4efb0: 
1:85f9ece:     Map<Text, Text> props = new HashMap<>();
1:4d4efb0:     props.put(new Text("distance"), new Text(Double.toString(distance)));
1:8253491:     context.write(clusterId, new WeightedPropertyVectorWritable(weight, vw.get(), props));
1:1371326:   }
1:1371326:   
1:229aeff:   public static List<Cluster> populateClusterModels(Path clusterOutputPath, Configuration conf) throws IOException {
1:85f9ece:     List<Cluster> clusters = new ArrayList<>();
1:6a3f566:     FileSystem fileSystem = clusterOutputPath.getFileSystem(conf);
1:229aeff:     FileStatus[] clusterFiles = fileSystem.listStatus(clusterOutputPath, PathFilters.finalPartFilter());
1:02ff22f:     Iterator<?> it = new SequenceFileDirValueIterator<>(
1:1371326:         clusterFiles[0].getPath(), PathType.LIST, PathFilters.partFilter(),
1:1371326:         null, false, conf);
1:6a3f566:     while (it.hasNext()) {
1:51f58b8:       ClusterWritable next = (ClusterWritable) it.next();
1:229aeff:       Cluster cluster = next.getValue();
1:590ffed:       cluster.configure(conf);
1:1371326:       clusters.add(cluster);
1:ba81a93:     }
1:1371326:     return clusters;
1:1371326:   }
1:6a3f566:   
1:229aeff:   private boolean shouldClassify(Vector pdfPerCluster) {
1:229aeff:     return pdfPerCluster.maxValue() >= threshold;
1:6a3f566:   }
1:6a3f566:   
1:229aeff:   private static Path finalClustersPath(Path clusterOutputPath) throws IOException {
1:229aeff:     FileSystem fileSystem = clusterOutputPath.getFileSystem(new Configuration());
1:229aeff:     FileStatus[] clusterFiles = fileSystem.listStatus(clusterOutputPath, PathFilters.finalPartFilter());
1:229aeff:     return clusterFiles[0].getPath();
1:6a3f566:   }
1:6a3f566: }
============================================================================
author:Karl Richter
-------------------------------------------------------------------------------
commit:02ff22f
/////////////////////////////////////////////////////////////////////////
1:     Iterator<?> it = new SequenceFileDirValueIterator<>(
author:Suneel Marthi
-------------------------------------------------------------------------------
commit:85f9ece
/////////////////////////////////////////////////////////////////////////
1: import java.util.ArrayList;
1: import java.util.HashMap;
/////////////////////////////////////////////////////////////////////////
1:     clusterModels = new ArrayList<>();
/////////////////////////////////////////////////////////////////////////
1:     Map<Text, Text> props = new HashMap<>();
1:     List<Cluster> clusters = new ArrayList<>();
author:pferrel
-------------------------------------------------------------------------------
commit:b988c49
author:frankscholten
-------------------------------------------------------------------------------
commit:1a42d85
author:smarthi
-------------------------------------------------------------------------------
commit:4d4efb0
/////////////////////////////////////////////////////////////////////////
1: import org.apache.mahout.clustering.iterator.DistanceMeasureCluster;
1: import org.apache.mahout.common.distance.DistanceMeasure;
/////////////////////////////////////////////////////////////////////////
1: 
1:     DistanceMeasureCluster distanceMeasureCluster = (DistanceMeasureCluster) cluster;
1:     DistanceMeasure distanceMeasure = distanceMeasureCluster.getMeasure();
1:     double distance = distanceMeasure.distance(cluster.getCenter(), vw.get());
1: 
1:     props.put(new Text("distance"), new Text(Double.toString(distance)));
commit:ab6216f
/////////////////////////////////////////////////////////////////////////
1:       Class<? extends Vector> vectorClass = vw.get().getClass();
1:       if (!vectorClass.equals(NamedVector.class)) {
1:         if (key.getClass().equals(Text.class)) {
1:         } else if (key.getClass().equals(IntWritable.class)) {
commit:08da368
/////////////////////////////////////////////////////////////////////////
1:       // Converting to NamedVectors to preserve the vectorId else its not obvious as to which point
1:       // belongs to which cluster - fix for MAHOUT-1410
commit:e93f05d
/////////////////////////////////////////////////////////////////////////
1: import org.apache.mahout.math.NamedVector;
/////////////////////////////////////////////////////////////////////////
1:       Vector vector = vw.get();
0:       if (!(vector instanceof NamedVector)) {
0:         if (key instanceof Text) {
1:           vector = new NamedVector(vector, key.toString());
0:         } else if (key instanceof IntWritable) {
1:           vector = new NamedVector(vector, Integer.toString(((IntWritable) key).get()));
1:         }
1:       }
1:       Vector pdfPerCluster = clusterClassifier.classify(vector);
1:           write(new VectorWritable(vector), context, maxValueIndex, 1.0);
1:           writeAllAboveThreshold(new VectorWritable(vector), context, pdfPerCluster);
/////////////////////////////////////////////////////////////////////////
0:     double d = Math.sqrt(cluster.getCenter().getDistanceSquared(vw.get()));
0:     props.put(new Text("distance"), new Text(Double.toString(d)));
commit:8253491
/////////////////////////////////////////////////////////////////////////
1: import java.util.Map;
0: import com.google.common.collect.Maps;
1: import org.apache.hadoop.io.Text;
/////////////////////////////////////////////////////////////////////////
0:     double d = cluster.getCenter().getDistanceSquared(vw.get());
0:     Map<Text, Text> props = Maps.newHashMap();
0:     props.put(new Text("distance-squared"), new Text(Double.toString(d)));
1:     context.write(clusterId, new WeightedPropertyVectorWritable(weight, vw.get(), props));
author:Jacob Alexander Mannix
-------------------------------------------------------------------------------
commit:dc62944
/////////////////////////////////////////////////////////////////////////
1:     for (Element pdf : pdfPerCluster.nonZeroes()) {
author:Sebastian Schelter
-------------------------------------------------------------------------------
commit:6d16230
/////////////////////////////////////////////////////////////////////////
1:     throws IOException, InterruptedException {
/////////////////////////////////////////////////////////////////////////
1:     throws IOException, InterruptedException {
commit:210b265
/////////////////////////////////////////////////////////////////////////
0: import com.google.common.collect.Lists;
/////////////////////////////////////////////////////////////////////////
0:     clusterModels = Lists.newArrayList();
/////////////////////////////////////////////////////////////////////////
0:     List<Cluster> clusters = Lists.newArrayList();
author:Ted Dunning
-------------------------------------------------------------------------------
commit:402e296
/////////////////////////////////////////////////////////////////////////
author:pranjan
-------------------------------------------------------------------------------
commit:0ab9b15
/////////////////////////////////////////////////////////////////////////
/////////////////////////////////////////////////////////////////////////
/////////////////////////////////////////////////////////////////////////
0:           write(vw, context, maxValueIndex, 1.0);
/////////////////////////////////////////////////////////////////////////
1:         write(vw, context, clusterIndex, pdf.get());
1:   private void write(VectorWritable vw, Context context, int clusterIndex, double weight)
0:     context.write(clusterId, new WeightedVectorWritable(weight, vw.get()));
commit:51f58b8
/////////////////////////////////////////////////////////////////////////
1: import org.apache.mahout.clustering.iterator.ClusterWritable;
/////////////////////////////////////////////////////////////////////////
1:       ClusterWritable next = (ClusterWritable) it.next();
0:       cluster = next.getValue();
commit:73ff886
/////////////////////////////////////////////////////////////////////////
1: import org.apache.hadoop.io.WritableComparable;
/////////////////////////////////////////////////////////////////////////
1:     Mapper<WritableComparable<?>,VectorWritable,IntWritable,WeightedVectorWritable> {
/////////////////////////////////////////////////////////////////////////
1:   protected void map(WritableComparable<?> key, VectorWritable vw, Context context)
commit:ba81a93
/////////////////////////////////////////////////////////////////////////
0: import org.apache.mahout.clustering.dirichlet.DirichletCluster;
/////////////////////////////////////////////////////////////////////////
0:     Mapper<IntWritable,VectorWritable,IntWritable,WeightedVectorWritable> {
/////////////////////////////////////////////////////////////////////////
1:       clusterModels = populateClusterModels(clustersInPath, conf);
/////////////////////////////////////////////////////////////////////////
0:   protected void map(IntWritable key, VectorWritable vw, Context context)
/////////////////////////////////////////////////////////////////////////
0:   public static List<Cluster> populateClusterModels(Path clusterOutputPath, Configuration conf)
/////////////////////////////////////////////////////////////////////////
0:       if(cluster instanceof DirichletCluster){
0:         ((DirichletCluster) cluster).getModel().configure(conf);
1:       }
commit:1371326
/////////////////////////////////////////////////////////////////////////
0: import static org.apache.mahout.clustering.classify.ClusterClassificationConfigKeys.CLUSTERS_IN;
0: import static org.apache.mahout.clustering.classify.ClusterClassificationConfigKeys.EMIT_MOST_LIKELY;
/////////////////////////////////////////////////////////////////////////
0: import org.apache.hadoop.io.LongWritable;
/////////////////////////////////////////////////////////////////////////
1: import org.apache.mahout.math.Vector.Element;
1: public class ClusterClassificationMapper extends
0:     Mapper<LongWritable,VectorWritable,IntWritable,WeightedVectorWritable> {
1:   private boolean emitMostLikely;
0:   protected void setup(Context context) throws IOException,
0:       InterruptedException {
0:     String clustersIn = conf.get(CLUSTERS_IN);
0:     threshold = conf.getFloat(OUTLIER_REMOVAL_THRESHOLD, 0.0f);
0:     emitMostLikely = conf.getBoolean(EMIT_MOST_LIKELY, false);
1:       Path clustersInPath = new Path(clustersIn);
0:       clusterModels = populateClusterModels(clustersInPath);
1:       ClusteringPolicy policy = ClusterClassifier
1:           .readPolicy(finalClustersPath(clustersInPath));
1:   /**
1:    * Mapper which classifies the vectors to respective clusters.
1:    */
0:   protected void map(LongWritable key, VectorWritable vw, Context context)
0:       throws IOException, InterruptedException {
1:         if (emitMostLikely) {
1:           int maxValueIndex = pdfPerCluster.maxValueIndex();
0:           write(vw, context, maxValueIndex);
1:         } else {
0:           writeAllAboveThreshold(vw, context, pdfPerCluster);
1:         }
1:   private void writeAllAboveThreshold(VectorWritable vw, Context context,
1:       Vector pdfPerCluster) throws IOException, InterruptedException {
0:     Iterator<Element> iterateNonZero = pdfPerCluster.iterateNonZero();
0:     while (iterateNonZero.hasNext()) {
0:       Element pdf = iterateNonZero.next();
1:       if (pdf.get() >= threshold) {
1:         int clusterIndex = pdf.index();
0:         write(vw, context, clusterIndex);
1:       }
1:     }
1:   }
1:   
0:   private void write(VectorWritable vw, Context context, int clusterIndex)
0:       throws IOException, InterruptedException {
1:     Cluster cluster = clusterModels.get(clusterIndex);
1:     clusterId.set(cluster.getId());
0:     weightedVW.setVector(vw.get());
0:     context.write(clusterId, weightedVW);
1:   }
1:   
0:   public static List<Cluster> populateClusterModels(Path clusterOutputPath)
0:       throws IOException {
0:     List<Cluster> clusters = new ArrayList<Cluster>();
0:     FileStatus[] clusterFiles = fileSystem.listStatus(clusterOutputPath,
0:         PathFilters.finalPartFilter());
0:     Iterator<?> it = new SequenceFileDirValueIterator<Writable>(
1:         clusterFiles[0].getPath(), PathType.LIST, PathFilters.partFilter(),
1:         null, false, conf);
1:       clusters.add(cluster);
1:     return clusters;
0:     boolean isMaxPDFGreatherThanThreshold = pdfPerCluster.maxValue() >= threshold;
0:     return isMaxPDFGreatherThanThreshold;
0:   private static Path finalClustersPath(Path clusterOutputPath)
0:       throws IOException {
0:     FileSystem fileSystem = clusterOutputPath
0:         .getFileSystem(new Configuration());
0:     FileStatus[] clusterFiles = fileSystem.listStatus(clusterOutputPath,
0:         PathFilters.finalPartFilter());
0:     Path finalClustersPath = clusterFiles[0].getPath();
0:     return finalClustersPath;
1:   }
author:Sean R. Owen
-------------------------------------------------------------------------------
commit:229aeff
/////////////////////////////////////////////////////////////////////////
/////////////////////////////////////////////////////////////////////////
1:   private double threshold;
/////////////////////////////////////////////////////////////////////////
1:   protected void setup(Context context) throws IOException, InterruptedException {
1:     String clustersIn = conf.get(ClusterClassificationConfigKeys.CLUSTERS_IN);
1:     threshold = conf.getFloat(ClusterClassificationConfigKeys.OUTLIER_REMOVAL_THRESHOLD, 0.0f);
1:     emitMostLikely = conf.getBoolean(ClusterClassificationConfigKeys.EMIT_MOST_LIKELY, false);
/////////////////////////////////////////////////////////////////////////
1:   public static List<Cluster> populateClusterModels(Path clusterOutputPath, Configuration conf) throws IOException {
1:     FileStatus[] clusterFiles = fileSystem.listStatus(clusterOutputPath, PathFilters.finalPartFilter());
1:       Cluster cluster = next.getValue();
1:   private boolean shouldClassify(Vector pdfPerCluster) {
1:     return pdfPerCluster.maxValue() >= threshold;
1:   private static Path finalClustersPath(Path clusterOutputPath) throws IOException {
1:     FileSystem fileSystem = clusterOutputPath.getFileSystem(new Configuration());
1:     FileStatus[] clusterFiles = fileSystem.listStatus(clusterOutputPath, PathFilters.finalPartFilter());
1:     return clusterFiles[0].getPath();
author:Jeff Eastman
-------------------------------------------------------------------------------
commit:590ffed
/////////////////////////////////////////////////////////////////////////
/////////////////////////////////////////////////////////////////////////
1:       cluster.configure(conf);
commit:8d102ea
/////////////////////////////////////////////////////////////////////////
1: import org.apache.mahout.clustering.iterator.ClusteringPolicy;
commit:457db94
/////////////////////////////////////////////////////////////////////////
0: import org.apache.mahout.clustering.ClusteringPolicy;
/////////////////////////////////////////////////////////////////////////
0: public class ClusterClassificationMapper extends Mapper<IntWritable,VectorWritable,IntWritable,WeightedVectorWritable> {
0:   protected void setup(Context context) throws IOException, InterruptedException {
1:     super.setup(context);
1:     
1:     Configuration conf = context.getConfiguration();
0:     String clustersIn = conf.get(ClusterClassificationConfigKeys.CLUSTERS_IN);
1:     
0:     clusterModels = new ArrayList<Cluster>();
1:     
1:     if (clustersIn != null && !clustersIn.isEmpty()) {
0:       Path clustersInPath = new Path(clustersIn, "*");
0:       populateClusterModels(clustersInPath);
0:       ClusteringPolicy policy = ClusterClassifier.readPolicy(clustersInPath);
1:       clusterClassifier = new ClusterClassifier(clusterModels, policy);
1:     }
0:     threshold = conf.getFloat(OUTLIER_REMOVAL_THRESHOLD, 0.0f);
1:     clusterId = new IntWritable();
0:     weightedVW = new WeightedVectorWritable(1, null);
1:   }
1:   
1:   @Override
0:   protected void map(IntWritable key, VectorWritable vw, Context context) throws IOException, InterruptedException {
1:     if (!clusterModels.isEmpty()) {
1:       if (shouldClassify(pdfPerCluster)) {
/////////////////////////////////////////////////////////////////////////
0:     Iterator<?> it = new SequenceFileDirValueIterator<Writable>(clusterFiles[0].getPath(), PathType.LIST,
0:         PathFilters.partFilter(), null, false, conf);
commit:76e80dc
/////////////////////////////////////////////////////////////////////////
0:         clusterClassifier = new ClusterClassifier(clusterModels, null);
commit:6a3f566
/////////////////////////////////////////////////////////////////////////
1: /**
1:  * Licensed to the Apache Software Foundation (ASF) under one or more
1:  * contributor license agreements.  See the NOTICE file distributed with
1:  * this work for additional information regarding copyright ownership.
1:  * The ASF licenses this file to You under the Apache License, Version 2.0
1:  * (the "License"); you may not use this file except in compliance with
1:  * the License.  You may obtain a copy of the License at
1:  *
1:  *     http://www.apache.org/licenses/LICENSE-2.0
1:  *
1:  * Unless required by applicable law or agreed to in writing, software
1:  * distributed under the License is distributed on an "AS IS" BASIS,
1:  * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
1:  * See the License for the specific language governing permissions and
1:  * limitations under the License.
1:  */
1: 
1: package org.apache.mahout.clustering.classify;
1: 
0: import static org.apache.mahout.clustering.classify.ClusterClassificationConfigKeys.OUTLIER_REMOVAL_THRESHOLD;
1: 
1: import java.io.IOException;
0: import java.util.ArrayList;
1: import java.util.Iterator;
1: import java.util.List;
1: 
1: import org.apache.hadoop.conf.Configuration;
1: import org.apache.hadoop.fs.FileStatus;
1: import org.apache.hadoop.fs.FileSystem;
1: import org.apache.hadoop.fs.Path;
1: import org.apache.hadoop.io.IntWritable;
1: import org.apache.hadoop.io.Writable;
1: import org.apache.hadoop.mapreduce.Mapper;
1: import org.apache.mahout.clustering.Cluster;
0: import org.apache.mahout.clustering.ClusterClassifier;
0: import org.apache.mahout.clustering.WeightedVectorWritable;
1: import org.apache.mahout.common.iterator.sequencefile.PathFilters;
1: import org.apache.mahout.common.iterator.sequencefile.PathType;
1: import org.apache.mahout.common.iterator.sequencefile.SequenceFileDirValueIterator;
1: import org.apache.mahout.math.Vector;
1: import org.apache.mahout.math.VectorWritable;
1: 
1: /**
1:  * Mapper for classifying vectors into clusters.
1:  */
0: public class ClusterClassificationMapper extends
0:     Mapper<IntWritable,VectorWritable,IntWritable,WeightedVectorWritable> {
1:   
0:   private static double threshold;
1:   private List<Cluster> clusterModels;
1:   private ClusterClassifier clusterClassifier;
1:   private IntWritable clusterId;
0:   private WeightedVectorWritable weightedVW;
1: 
1:   @Override
0:   protected void setup(Context context) throws IOException, InterruptedException {
0:       super.setup(context);
1: 
0:       Configuration conf = context.getConfiguration();
0:       String clustersIn = conf.get(ClusterClassificationConfigKeys.CLUSTERS_IN);
1:       
0:       clusterModels = new ArrayList<Cluster>();
1:       
0:       if (clustersIn != null && !clustersIn.isEmpty()) {
0:         Path clustersInPath = new Path(clustersIn, "*");
0:         populateClusterModels(clustersInPath);
0:         clusterClassifier = new ClusterClassifier(clusterModels);
1:       }
0:       threshold = conf.getFloat(OUTLIER_REMOVAL_THRESHOLD, 0.0f);
0:       clusterId = new IntWritable();
0:       weightedVW = new WeightedVectorWritable(1, null);
1:     }
1:   
1:   @Override
0:   protected void map(IntWritable key, VectorWritable vw, Context context) throws IOException,
0:                                                                                      InterruptedException {
0:     if(!clusterModels.isEmpty()) {
0:       Vector pdfPerCluster = clusterClassifier.classify(vw.get());
0:       if(shouldClassify(pdfPerCluster)) {
0:         int maxValueIndex = pdfPerCluster.maxValueIndex();
0:         Cluster cluster = clusterModels.get(maxValueIndex);
0:         clusterId.set(cluster.getId());
0:         weightedVW.setVector(vw.get());
0:         context.write(clusterId, weightedVW);
1:       }
1:     }
1:   }
1:   
0:   public static List<Cluster> populateClusterModels(Path clusterOutputPath) throws IOException {
0:     List<Cluster> clusterModels = new ArrayList<Cluster>();
0:     Configuration conf = new Configuration();
0:     Cluster cluster = null;
1:     FileSystem fileSystem = clusterOutputPath.getFileSystem(conf);
0:     FileStatus[] clusterFiles = fileSystem.listStatus(clusterOutputPath, PathFilters.finalPartFilter());
0:     Iterator<?> it = new SequenceFileDirValueIterator<Writable>(clusterFiles[0].getPath(),
0:                                                                 PathType.LIST,
0:                                                                 PathFilters.partFilter(),
0:                                                                 null,
0:                                                                 false,
0:                                                                 conf);
1:     while (it.hasNext()) {
0:       cluster = (Cluster) it.next();
0:       clusterModels.add(cluster);
1:     }
0:     return clusterModels;
1:   }
1:   
0:   private static boolean shouldClassify(Vector pdfPerCluster) {
0:     return pdfPerCluster.maxValue() >= threshold;
1:   }
1:   
1: }
============================================================================