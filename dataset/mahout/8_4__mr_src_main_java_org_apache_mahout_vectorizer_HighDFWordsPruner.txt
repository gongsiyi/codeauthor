1:229aeff: /*
1:d8e91f9:  * Licensed to the Apache Software Foundation (ASF) under one or more
1:d8e91f9:  * contributor license agreements.  See the NOTICE file distributed with
1:d8e91f9:  * this work for additional information regarding copyright ownership.
1:d8e91f9:  * The ASF licenses this file to You under the Apache License, Version 2.0
1:d8e91f9:  * (the "License"); you may not use this file except in compliance with
1:d8e91f9:  * the License.  You may obtain a copy of the License at
1:d8e91f9:  *
1:d8e91f9:  *     http://www.apache.org/licenses/LICENSE-2.0
1:d8e91f9:  *
1:d8e91f9:  * Unless required by applicable law or agreed to in writing, software
1:d8e91f9:  * distributed under the License is distributed on an "AS IS" BASIS,
1:d8e91f9:  * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
1:d8e91f9:  * See the License for the specific language governing permissions and
1:d8e91f9:  * limitations under the License.
1:d8e91f9:  */
2:d8e91f9: 
1:229aeff: package org.apache.mahout.vectorizer;
1:229aeff: 
1:d8e91f9: import org.apache.hadoop.conf.Configuration;
1:d8e91f9: import org.apache.hadoop.filecache.DistributedCache;
1:d8e91f9: import org.apache.hadoop.fs.Path;
1:d8e91f9: import org.apache.hadoop.io.Text;
1:d8e91f9: import org.apache.hadoop.mapreduce.Job;
1:d8e91f9: import org.apache.hadoop.mapreduce.Mapper;
1:d8e91f9: import org.apache.hadoop.mapreduce.lib.input.FileInputFormat;
1:d8e91f9: import org.apache.hadoop.mapreduce.lib.input.SequenceFileInputFormat;
1:d8e91f9: import org.apache.hadoop.mapreduce.lib.output.FileOutputFormat;
1:d8e91f9: import org.apache.hadoop.mapreduce.lib.output.SequenceFileOutputFormat;
1:d8e91f9: import org.apache.mahout.common.HadoopUtil;
1:d8e91f9: import org.apache.mahout.common.Pair;
1:d8e91f9: import org.apache.mahout.math.VectorWritable;
1:d8e91f9: import org.apache.mahout.vectorizer.common.PartialVectorMerger;
1:d8e91f9: import org.apache.mahout.vectorizer.pruner.PrunedPartialVectorMergeReducer;
1:d8e91f9: import org.apache.mahout.vectorizer.pruner.WordsPrunerReducer;
1:d8e91f9: 
1:d8e91f9: import java.io.IOException;
1:5624a96: import java.util.ArrayList;
1:d8e91f9: import java.util.List;
1:d8e91f9: 
1:229aeff: public final class HighDFWordsPruner {
1:d8e91f9: 
1:d8e91f9:   public static final String STD_CALC_DIR = "stdcalc";
1:d8e91f9:   public static final String MAX_DF = "max.df";
1:d1e5295:   public static final String MIN_DF = "min.df";
1:d8e91f9: 
1:564c3e1:   private HighDFWordsPruner() {
1:564c3e1:   }
1:d8e91f9: 
1:d8e91f9:   public static void pruneVectors(Path tfDir, Path prunedTFDir, Path prunedPartialTFDir, long maxDF,
1:d1e5295:                                   long minDF, Configuration baseConf,
1:d8e91f9:                                   Pair<Long[], List<Path>> docFrequenciesFeatures,
1:d8e91f9:                                   float normPower,
1:d8e91f9:                                   boolean logNormalize,
1:229aeff:                                   int numReducers) throws IOException, InterruptedException, ClassNotFoundException {
1:d8e91f9: 
1:d8e91f9:     int partialVectorIndex = 0;
1:5624a96:     List<Path> partialVectorPaths = new ArrayList<>();
1:d8e91f9:     for (Path path : docFrequenciesFeatures.getSecond()) {
1:d8e91f9:       Path partialVectorOutputPath = new Path(prunedPartialTFDir, "partial-" + partialVectorIndex++);
1:d8e91f9:       partialVectorPaths.add(partialVectorOutputPath);
1:d1e5295:       pruneVectorsPartial(tfDir, partialVectorOutputPath, path, maxDF, minDF, baseConf);
1:d8e91f9:     }
1:d8e91f9: 
1:229aeff:     mergePartialVectors(partialVectorPaths, prunedTFDir, baseConf, normPower, logNormalize, numReducers);
1:d8e91f9:     HadoopUtil.delete(new Configuration(baseConf), prunedPartialTFDir);
1:d8e91f9:   }
1:d8e91f9: 
1:d8e91f9:   private static void pruneVectorsPartial(Path input, Path output, Path dictionaryFilePath, long maxDF,
1:d1e5295:                                           long minDF, Configuration baseConf) throws IOException, InterruptedException,
2:d8e91f9:           ClassNotFoundException {
1:d8e91f9: 
1:d8e91f9:     Configuration conf = new Configuration(baseConf);
1:d8e91f9:     // this conf parameter needs to be set enable serialisation of conf
1:d8e91f9:     // values
1:d8e91f9:     conf.set("io.serializations",
1:d8e91f9:             "org.apache.hadoop.io.serializer.JavaSerialization,"
1:d8e91f9:                     + "org.apache.hadoop.io.serializer.WritableSerialization");
1:d8e91f9:     conf.setLong(MAX_DF, maxDF);
1:d1e5295:     conf.setLong(MIN_DF, minDF);
1:5624a96:     DistributedCache.addCacheFile(dictionaryFilePath.toUri(), conf);
1:d8e91f9: 
1:d8e91f9:     Job job = HadoopUtil.prepareJob(input, output, SequenceFileInputFormat.class,
1:d8e91f9:             Mapper.class, null, null, WordsPrunerReducer.class,
1:d8e91f9:             Text.class, VectorWritable.class, SequenceFileOutputFormat.class,
1:d8e91f9:             conf);
1:d8e91f9:     job.setJobName(": Prune Vectors: input-folder: " + input
1:d8e91f9:             + ", dictionary-file: " + dictionaryFilePath.toString());
1:d8e91f9: 
1:d8e91f9:     HadoopUtil.delete(conf, output);
1:d8e91f9: 
1:7c2b664:     boolean succeeded = job.waitForCompletion(true);
1:229aeff:     if (!succeeded) {
1:7c2b664:       throw new IllegalStateException("Job failed!");
1:229aeff:     }
1:229aeff:   }
1:d8e91f9: 
1:d8e91f9:   public static void mergePartialVectors(Iterable<Path> partialVectorPaths,
1:d8e91f9:                                          Path output,
2:d8e91f9:                                          Configuration baseConf,
1:d8e91f9:                                          float normPower,
1:d8e91f9:                                          boolean logNormalize,
1:d8e91f9:                                          int numReducers)
1:229aeff:     throws IOException, InterruptedException, ClassNotFoundException {
1:d8e91f9: 
1:d8e91f9:     Configuration conf = new Configuration(baseConf);
1:d8e91f9:     // this conf parameter needs to be set enable serialisation of conf values
1:d8e91f9:     conf.set("io.serializations", "org.apache.hadoop.io.serializer.JavaSerialization,"
1:d8e91f9:             + "org.apache.hadoop.io.serializer.WritableSerialization");
1:d8e91f9:     conf.setFloat(PartialVectorMerger.NORMALIZATION_POWER, normPower);
1:d8e91f9:     conf.setBoolean(PartialVectorMerger.LOG_NORMALIZE, logNormalize);
1:d8e91f9: 
1:d8e91f9:     Job job = new Job(conf);
1:d8e91f9:     job.setJobName("PrunerPartialVectorMerger::MergePartialVectors");
1:d8e91f9:     job.setJarByClass(PartialVectorMerger.class);
1:d8e91f9: 
1:d8e91f9:     job.setOutputKeyClass(Text.class);
1:d8e91f9:     job.setOutputValueClass(VectorWritable.class);
1:d8e91f9: 
1:d8e91f9:     FileInputFormat.setInputPaths(job, getCommaSeparatedPaths(partialVectorPaths));
1:d8e91f9: 
1:d8e91f9:     FileOutputFormat.setOutputPath(job, output);
1:d8e91f9: 
1:d8e91f9:     job.setMapperClass(Mapper.class);
1:d8e91f9:     job.setInputFormatClass(SequenceFileInputFormat.class);
1:d8e91f9:     job.setReducerClass(PrunedPartialVectorMergeReducer.class);
1:d8e91f9:     job.setOutputFormatClass(SequenceFileOutputFormat.class);
1:d8e91f9:     job.setNumReduceTasks(numReducers);
1:d8e91f9: 
1:d8e91f9:     HadoopUtil.delete(conf, output);
1:d8e91f9: 
1:7c2b664:     boolean succeeded = job.waitForCompletion(true);
1:229aeff:     if (!succeeded) {
1:7c2b664:       throw new IllegalStateException("Job failed!");
1:d8e91f9:     }
1:d8e91f9:   }
1:d8e91f9: 
1:d8e91f9:   private static String getCommaSeparatedPaths(Iterable<Path> paths) {
1:d8e91f9:     StringBuilder commaSeparatedPaths = new StringBuilder(100);
1:d8e91f9:     String sep = "";
1:d8e91f9:     for (Path path : paths) {
1:d8e91f9:       commaSeparatedPaths.append(sep).append(path.toString());
1:d8e91f9:       sep = ",";
1:d8e91f9:     }
1:d8e91f9:     return commaSeparatedPaths.toString();
1:d8e91f9:   }
1:d8e91f9: }
============================================================================
author:pferrel
-------------------------------------------------------------------------------
commit:b988c49
author:Suneel Marthi
-------------------------------------------------------------------------------
commit:5624a96
/////////////////////////////////////////////////////////////////////////
/////////////////////////////////////////////////////////////////////////
1: import java.util.ArrayList;
/////////////////////////////////////////////////////////////////////////
1:     List<Path> partialVectorPaths = new ArrayList<>();
/////////////////////////////////////////////////////////////////////////
1:     DistributedCache.addCacheFile(dictionaryFilePath.toUri(), conf);
author:frankscholten
-------------------------------------------------------------------------------
commit:1a42d85
author:Robin Anil
-------------------------------------------------------------------------------
commit:d1e5295
/////////////////////////////////////////////////////////////////////////
1:   public static final String MIN_DF = "min.df";
1:                                   long minDF, Configuration baseConf,
/////////////////////////////////////////////////////////////////////////
1:       pruneVectorsPartial(tfDir, partialVectorOutputPath, path, maxDF, minDF, baseConf);
/////////////////////////////////////////////////////////////////////////
1:                                           long minDF, Configuration baseConf) throws IOException, InterruptedException,
/////////////////////////////////////////////////////////////////////////
1:     conf.setLong(MIN_DF, minDF);
author:Ted Dunning
-------------------------------------------------------------------------------
commit:402e296
/////////////////////////////////////////////////////////////////////////
author:Sebastian Schelter
-------------------------------------------------------------------------------
commit:210b265
/////////////////////////////////////////////////////////////////////////
0: import com.google.common.collect.Lists;
/////////////////////////////////////////////////////////////////////////
/////////////////////////////////////////////////////////////////////////
0:     List<Path> partialVectorPaths = Lists.newArrayList();
author:Sean R. Owen
-------------------------------------------------------------------------------
commit:229aeff
/////////////////////////////////////////////////////////////////////////
1: /*
/////////////////////////////////////////////////////////////////////////
1: package org.apache.mahout.vectorizer;
1: 
/////////////////////////////////////////////////////////////////////////
1: public final class HighDFWordsPruner {
/////////////////////////////////////////////////////////////////////////
1:                                   int numReducers) throws IOException, InterruptedException, ClassNotFoundException {
/////////////////////////////////////////////////////////////////////////
1:     mergePartialVectors(partialVectorPaths, prunedTFDir, baseConf, normPower, logNormalize, numReducers);
/////////////////////////////////////////////////////////////////////////
1:     if (!succeeded) {
1:     }
/////////////////////////////////////////////////////////////////////////
1:     throws IOException, InterruptedException, ClassNotFoundException {
/////////////////////////////////////////////////////////////////////////
1:     if (!succeeded) {
1:     }
commit:7c2b664
/////////////////////////////////////////////////////////////////////////
1:     boolean succeeded = job.waitForCompletion(true);
0:     if (!succeeded) 
1:       throw new IllegalStateException("Job failed!");
/////////////////////////////////////////////////////////////////////////
1:     boolean succeeded = job.waitForCompletion(true);
0:     if (!succeeded) 
1:       throw new IllegalStateException("Job failed!");
commit:564c3e1
/////////////////////////////////////////////////////////////////////////
/////////////////////////////////////////////////////////////////////////
/////////////////////////////////////////////////////////////////////////
1:   private HighDFWordsPruner() {
1:   }
author:Grant Ingersoll
-------------------------------------------------------------------------------
commit:d8e91f9
/////////////////////////////////////////////////////////////////////////
0: package org.apache.mahout.vectorizer;
0: /**
1:  * Licensed to the Apache Software Foundation (ASF) under one or more
1:  * contributor license agreements.  See the NOTICE file distributed with
1:  * this work for additional information regarding copyright ownership.
1:  * The ASF licenses this file to You under the Apache License, Version 2.0
1:  * (the "License"); you may not use this file except in compliance with
1:  * the License.  You may obtain a copy of the License at
1:  *
1:  *     http://www.apache.org/licenses/LICENSE-2.0
1:  *
1:  * Unless required by applicable law or agreed to in writing, software
1:  * distributed under the License is distributed on an "AS IS" BASIS,
1:  * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
1:  * See the License for the specific language governing permissions and
1:  * limitations under the License.
1:  */
1: 
1: import org.apache.hadoop.conf.Configuration;
1: import org.apache.hadoop.filecache.DistributedCache;
1: import org.apache.hadoop.fs.Path;
1: import org.apache.hadoop.io.Text;
0: import org.apache.hadoop.io.Writable;
1: import org.apache.hadoop.mapreduce.Job;
1: import org.apache.hadoop.mapreduce.Mapper;
1: import org.apache.hadoop.mapreduce.lib.input.FileInputFormat;
1: import org.apache.hadoop.mapreduce.lib.input.SequenceFileInputFormat;
1: import org.apache.hadoop.mapreduce.lib.output.FileOutputFormat;
1: import org.apache.hadoop.mapreduce.lib.output.SequenceFileOutputFormat;
1: import org.apache.mahout.common.HadoopUtil;
1: import org.apache.mahout.common.Pair;
1: import org.apache.mahout.math.VectorWritable;
1: import org.apache.mahout.vectorizer.common.PartialVectorMerger;
1: import org.apache.mahout.vectorizer.pruner.PrunedPartialVectorMergeReducer;
1: import org.apache.mahout.vectorizer.pruner.WordsPrunerReducer;
0: import org.slf4j.Logger;
0: import org.slf4j.LoggerFactory;
1: 
1: import java.io.IOException;
0: import java.net.URI;
0: import java.util.ArrayList;
1: import java.util.List;
1: 
0: public class HighDFWordsPruner {
1: 
0:   public static final String OUT_DIR_SUFFIX = "-pruned";
1:   public static final String STD_CALC_DIR = "stdcalc";
1:   public static final String MAX_DF = "max.df";
1: 
0:   private static final Logger log = LoggerFactory
0:           .getLogger(HighDFWordsPruner.class);
1: 
1: 
1:   public static void pruneVectors(Path tfDir, Path prunedTFDir, Path prunedPartialTFDir, long maxDF,
1:                                   Configuration baseConf,
1:                                   Pair<Long[], List<Path>> docFrequenciesFeatures,
1:                                   float normPower,
1:                                   boolean logNormalize,
0:                                   int numReducers) throws IOException, InterruptedException,
1:           ClassNotFoundException {
1: 
1:     int partialVectorIndex = 0;
0:     List<Path> partialVectorPaths = new ArrayList<Path>();
1:     for (Path path : docFrequenciesFeatures.getSecond()) {
1:       Path partialVectorOutputPath = new Path(prunedPartialTFDir, "partial-" + partialVectorIndex++);
1:       partialVectorPaths.add(partialVectorOutputPath);
0:       pruneVectorsPartial(tfDir, partialVectorOutputPath, path, maxDF, baseConf);
1:     }
1: 
0:     mergePartialVectors(partialVectorPaths, prunedTFDir, baseConf, normPower, logNormalize,
0:             numReducers);
1:     HadoopUtil.delete(new Configuration(baseConf), prunedPartialTFDir);
1:   }
1: 
1:   private static void pruneVectorsPartial(Path input, Path output, Path dictionaryFilePath, long maxDF,
0:                                           Configuration baseConf) throws IOException, InterruptedException,
1:           ClassNotFoundException {
1: 
1:     Configuration conf = new Configuration(baseConf);
1:     // this conf parameter needs to be set enable serialisation of conf
1:     // values
1:     conf.set("io.serializations",
1:             "org.apache.hadoop.io.serializer.JavaSerialization,"
1:                     + "org.apache.hadoop.io.serializer.WritableSerialization");
1:     conf.setLong(MAX_DF, maxDF);
0:     DistributedCache.setCacheFiles(
0:             new URI[]{dictionaryFilePath.toUri()}, conf);
1: 
1:     Job job = HadoopUtil.prepareJob(input, output, SequenceFileInputFormat.class,
1:             Mapper.class, null, null, WordsPrunerReducer.class,
1:             Text.class, VectorWritable.class, SequenceFileOutputFormat.class,
1:             conf);
1:     job.setJobName(": Prune Vectors: input-folder: " + input
1:             + ", dictionary-file: " + dictionaryFilePath.toString());
1: 
1:     HadoopUtil.delete(conf, output);
1: 
0:     job.waitForCompletion(true);
1:   }
1: 
1:   public static void mergePartialVectors(Iterable<Path> partialVectorPaths,
1:                                          Path output,
1:                                          Configuration baseConf,
1:                                          float normPower,
1:                                          boolean logNormalize,
1:                                          int numReducers)
0:           throws IOException, InterruptedException, ClassNotFoundException {
1: 
1:     Configuration conf = new Configuration(baseConf);
1:     // this conf parameter needs to be set enable serialisation of conf values
1:     conf.set("io.serializations", "org.apache.hadoop.io.serializer.JavaSerialization,"
1:             + "org.apache.hadoop.io.serializer.WritableSerialization");
1:     conf.setFloat(PartialVectorMerger.NORMALIZATION_POWER, normPower);
1:     conf.setBoolean(PartialVectorMerger.LOG_NORMALIZE, logNormalize);
1: 
1:     Job job = new Job(conf);
1:     job.setJobName("PrunerPartialVectorMerger::MergePartialVectors");
1:     job.setJarByClass(PartialVectorMerger.class);
1: 
1:     job.setOutputKeyClass(Text.class);
1:     job.setOutputValueClass(VectorWritable.class);
1: 
1:     FileInputFormat.setInputPaths(job, getCommaSeparatedPaths(partialVectorPaths));
1: 
1:     FileOutputFormat.setOutputPath(job, output);
1: 
1:     job.setMapperClass(Mapper.class);
1:     job.setInputFormatClass(SequenceFileInputFormat.class);
1:     job.setReducerClass(PrunedPartialVectorMergeReducer.class);
1:     job.setOutputFormatClass(SequenceFileOutputFormat.class);
1:     job.setNumReduceTasks(numReducers);
1: 
1:     HadoopUtil.delete(conf, output);
1: 
0:     job.waitForCompletion(true);
1:   }
1: 
1:   private static String getCommaSeparatedPaths(Iterable<Path> paths) {
1:     StringBuilder commaSeparatedPaths = new StringBuilder(100);
1:     String sep = "";
1:     for (Path path : paths) {
1:       commaSeparatedPaths.append(sep).append(path.toString());
1:       sep = ",";
1:     }
1:     return commaSeparatedPaths.toString();
1:   }
1: }
============================================================================