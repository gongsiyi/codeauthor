1:51e2486: /*
1:51e2486:  * Licensed to the Apache Software Foundation (ASF) under one or more
1:51e2486:  * contributor license agreements.  See the NOTICE file distributed with
1:51e2486:  * this work for additional information regarding copyright ownership.
1:51e2486:  * The ASF licenses this file to You under the Apache License, Version 2.0
1:51e2486:  * (the "License"); you may not use this file except in compliance with
1:51e2486:  * the License.  You may obtain a copy of the License at
1:51e2486:  *
1:51e2486:  *     http://www.apache.org/licenses/LICENSE-2.0
1:51e2486:  *
1:51e2486:  * Unless required by applicable law or agreed to in writing, software
1:51e2486:  * distributed under the License is distributed on an "AS IS" BASIS,
1:51e2486:  * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
1:51e2486:  * See the License for the specific language governing permissions and
1:51e2486:  * limitations under the License.
1:51e2486:  */
1:a4778a4: 
1:8b6f5d7: package org.apache.mahout.classifier.sgd;
1:d53cf4a: 
1:4ef9d31: import com.google.common.collect.HashMultiset;
1:4ef9d31: import com.google.common.collect.Multiset;
1:4ef9d31: import com.google.common.collect.Ordering;
1:5257bc9: import org.apache.mahout.classifier.NewsgroupHelper;
1:5257bc9: import org.apache.mahout.ep.State;
1:5257bc9: import org.apache.mahout.math.Vector;
1:5257bc9: import org.apache.mahout.vectorizer.encoders.Dictionary;
1:319fd82: 
1:4ef9d31: import java.io.File;
1:4ef9d31: import java.io.IOException;
1:4ef9d31: import java.util.ArrayList;
1:4ef9d31: import java.util.Arrays;
1:4ef9d31: import java.util.Collections;
1:4ef9d31: import java.util.List;
1:5257bc9: 
1:8b6f5d7: /**
1:8b6f5d7:  * Reads and trains an adaptive logistic regression model on the 20 newsgroups data.
1:8b6f5d7:  * The first command line argument gives the path of the directory holding the training
1:8b6f5d7:  * data.  The optional second argument, leakType, defines which classes of features to use.
1:8b6f5d7:  * Importantly, leakType controls whether a synthetic date is injected into the data as
1:8b6f5d7:  * a target leak and if so, how.
1:8a2c0f3:  * <p/>
1:8b6f5d7:  * The value of leakType % 3 determines whether the target leak is injected according to
1:8b6f5d7:  * the following table:
1:8a2c0f3:  * <p/>
1:8b6f5d7:  * <table>
1:8b6f5d7:  * <tr><td valign='top'>0</td><td>No leak injected</td></tr>
1:8b6f5d7:  * <tr><td valign='top'>1</td><td>Synthetic date injected in MMM-yyyy format. This will be a single token and
1:8b6f5d7:  * is a perfect target leak since each newsgroup is given a different month</td></tr>
1:8b6f5d7:  * <tr><td valign='top'>2</td><td>Synthetic date injected in dd-MMM-yyyy HH:mm:ss format.  The day varies
1:8b6f5d7:  * and thus there are more leak symbols that need to be learned.  Ultimately this is just
1:8b6f5d7:  * as big a leak as case 1.</td></tr>
1:8b6f5d7:  * </table>
1:8a2c0f3:  * <p/>
1:8b6f5d7:  * Leaktype also determines what other text will be indexed.  If leakType is greater
1:8b6f5d7:  * than or equal to 6, then neither headers nor text body will be used for features and the leak is the only
1:8b6f5d7:  * source of data.  If leakType is greater than or equal to 3, then subject words will be used as features.
1:8b6f5d7:  * If leakType is less than 3, then both subject and body text will be used as features.
1:8a2c0f3:  * <p/>
1:8b6f5d7:  * A leakType of 0 gives no leak and all textual features.
1:8a2c0f3:  * <p/>
1:8b6f5d7:  * See the following table for a summary of commonly used values for leakType
1:8a2c0f3:  * <p/>
1:8b6f5d7:  * <table>
1:8b6f5d7:  * <tr><td><b>leakType</b></td><td><b>Leak?</b></td><td><b>Subject?</b></td><td><b>Body?</b></td></tr>
1:8b6f5d7:  * <tr><td colspan=4><hr></td></tr>
1:8b6f5d7:  * <tr><td>0</td><td>no</td><td>yes</td><td>yes</td></tr>
1:8b6f5d7:  * <tr><td>1</td><td>mmm-yyyy</td><td>yes</td><td>yes</td></tr>
1:8b6f5d7:  * <tr><td>2</td><td>dd-mmm-yyyy</td><td>yes</td><td>yes</td></tr>
1:8b6f5d7:  * <tr><td colspan=4><hr></td></tr>
1:8b6f5d7:  * <tr><td>3</td><td>no</td><td>yes</td><td>no</td></tr>
1:8b6f5d7:  * <tr><td>4</td><td>mmm-yyyy</td><td>yes</td><td>no</td></tr>
1:8b6f5d7:  * <tr><td>5</td><td>dd-mmm-yyyy</td><td>yes</td><td>no</td></tr>
1:8b6f5d7:  * <tr><td colspan=4><hr></td></tr>
1:8b6f5d7:  * <tr><td>6</td><td>no</td><td>no</td><td>no</td></tr>
1:8b6f5d7:  * <tr><td>7</td><td>mmm-yyyy</td><td>no</td><td>no</td></tr>
1:8b6f5d7:  * <tr><td>8</td><td>dd-mmm-yyyy</td><td>no</td><td>no</td></tr>
1:8b6f5d7:  * <tr><td colspan=4><hr></td></tr>
1:8b6f5d7:  * </table>
1:8b6f5d7:  */
1:d53cf4a: public final class TrainNewsGroups {
1:d53cf4a: 
1:58fd277:   private TrainNewsGroups() {
1:58fd277:   }
1:58fd277: 
1:8b6f5d7:   public static void main(String[] args) throws IOException {
1:8b6f5d7:     File base = new File(args[0]);
1:5257bc9: 
1:4fbfbc6:     Multiset<String> overallCounts = HashMultiset.create();
1:319fd82: 
1:8b6f5d7:     int leakType = 0;
1:8b6f5d7:     if (args.length > 1) {
1:8b6f5d7:       leakType = Integer.parseInt(args[1]);
1:35032b8:     }
1:f029d24: 
1:8b6f5d7:     Dictionary newsGroups = new Dictionary();
1:c93818e: 
1:e0ec7c1:     NewsgroupHelper helper = new NewsgroupHelper();
1:e0ec7c1:     helper.getEncoder().setProbes(2);
1:229aeff:     AdaptiveLogisticRegression learningAlgorithm =
1:229aeff:         new AdaptiveLogisticRegression(20, NewsgroupHelper.FEATURES, new L1());
1:c93818e:     learningAlgorithm.setInterval(800);
1:51e2486:     learningAlgorithm.setAveragingWindow(500);
1:c93818e: 
1:4ef9d31:     List<File> files = new ArrayList<>();
1:8b6f5d7:     for (File newsgroup : base.listFiles()) {
1:f201912:       if (newsgroup.isDirectory()) {
1:f201912:         newsGroups.intern(newsgroup.getName());
1:f201912:         files.addAll(Arrays.asList(newsgroup.listFiles()));
1:f201912:       }
1:d53cf4a:     }
1:c93818e:     Collections.shuffle(files);
1:229aeff:     System.out.println(files.size() + " training files");
1:8a2c0f3:     SGDInfo info = new SGDInfo();
1:8a2c0f3: 
2:8b6f5d7:     int k = 0;
1:8a2c0f3: 
1:95736dd:     for (File file : files) {
2:8b6f5d7:       String ng = file.getParentFile().getName();
2:8b6f5d7:       int actual = newsGroups.intern(ng);
1:24a75db: 
1:e0ec7c1:       Vector v = helper.encodeFeatureVector(file, actual, leakType, overallCounts);
1:8b6f5d7:       learningAlgorithm.train(actual, v);
1:24a75db: 
1:9a5b461:       k++;
1:02f8694:       State<AdaptiveLogisticRegression.Wrapper, CrossFoldLearner> best = learningAlgorithm.getBest();
1:51e2486: 
1:8a2c0f3:       SGDHelper.analyzeState(info, leakType, k, best);
1:d53cf4a:     }
1:8b6f5d7:     learningAlgorithm.close();
1:8a2c0f3:     SGDHelper.dissect(leakType, newsGroups, learningAlgorithm, files, overallCounts);
1:8b6f5d7:     System.out.println("exiting main");
1:319fd82: 
1:3a284f5:     File modelFile = new File(System.getProperty("java.io.tmpdir"), "news-group.model");
1:3a284f5:     ModelSerializer.writeBinary(modelFile.getAbsolutePath(),
1:3a284f5:         learningAlgorithm.getBest().getPayload().getLearner().getModels().get(0));
1:f029d24: 
1:4ef9d31:     List<Integer> counts = new ArrayList<>();
1:229aeff:     System.out.println("Word counts");
1:f029d24:     for (String count : overallCounts.elementSet()) {
1:f029d24:       counts.add(overallCounts.count(count));
1:d53cf4a:     }
1:f029d24:     Collections.sort(counts, Ordering.natural().reverse());
1:f029d24:     k = 0;
1:f029d24:     for (Integer count : counts) {
1:229aeff:       System.out.println(k + "\t" + count);
1:f029d24:       k++;
1:319fd82:       if (k > 1000) {
1:319fd82:         break;
1:d53cf4a:       }
1:319fd82:     }
1:319fd82:   }
32:8b6f5d7: 
1:8b6f5d7: 
2:f029d24: }
============================================================================
author:Suneel Marthi
-------------------------------------------------------------------------------
commit:4ef9d31
/////////////////////////////////////////////////////////////////////////
1: import com.google.common.collect.HashMultiset;
1: import com.google.common.collect.Multiset;
1: import com.google.common.collect.Ordering;
1: import java.io.File;
1: import java.io.IOException;
1: import java.util.ArrayList;
1: import java.util.Arrays;
1: import java.util.Collections;
1: import java.util.List;
/////////////////////////////////////////////////////////////////////////
1:     List<File> files = new ArrayList<>();
/////////////////////////////////////////////////////////////////////////
/////////////////////////////////////////////////////////////////////////
1:     List<Integer> counts = new ArrayList<>();
author:Sebastian Schelter
-------------------------------------------------------------------------------
commit:3a284f5
/////////////////////////////////////////////////////////////////////////
1:     File modelFile = new File(System.getProperty("java.io.tmpdir"), "news-group.model");
1:     ModelSerializer.writeBinary(modelFile.getAbsolutePath(),
1:         learningAlgorithm.getBest().getPayload().getLearner().getModels().get(0));
commit:d608a88
/////////////////////////////////////////////////////////////////////////
0: import com.google.common.io.Closeables;
/////////////////////////////////////////////////////////////////////////
0:       Closeables.closeQuietly(reader);
author:Sean R. Owen
-------------------------------------------------------------------------------
commit:229aeff
/////////////////////////////////////////////////////////////////////////
1:     AdaptiveLogisticRegression learningAlgorithm =
1:         new AdaptiveLogisticRegression(20, NewsgroupHelper.FEATURES, new L1());
/////////////////////////////////////////////////////////////////////////
1:     System.out.println(files.size() + " training files");
/////////////////////////////////////////////////////////////////////////
1:     System.out.println("Word counts");
1:       System.out.println(k + "\t" + count);
commit:4fbfbc6
/////////////////////////////////////////////////////////////////////////
1:     Multiset<String> overallCounts = HashMultiset.create();
commit:e0ec7c1
/////////////////////////////////////////////////////////////////////////
1:     NewsgroupHelper helper = new NewsgroupHelper();
1:     helper.getEncoder().setProbes(2);
/////////////////////////////////////////////////////////////////////////
1:       Vector v = helper.encodeFeatureVector(file, actual, leakType, overallCounts);
/////////////////////////////////////////////////////////////////////////
0:     NewsgroupHelper helper = new NewsgroupHelper();
0:     helper.getEncoder().setTraceDictionary(traceDictionary);
0:     helper.getBias().setTraceDictionary(traceDictionary);
0:     for (File file : permute(files, helper.getRandom()).subList(0, 500)) {
0:       Vector v = helper.encodeFeatureVector(file, actual, leakType, overallCounts);
commit:971a56d
/////////////////////////////////////////////////////////////////////////
0:       encoder.addToVector(word, Math.log1p(words.count(word)), v);
commit:d3ccbe0
/////////////////////////////////////////////////////////////////////////
0:     TokenStream ts = analyzer.reusableTokenStream("text", in);
0:     ts.reset();
commit:4ef41c7
/////////////////////////////////////////////////////////////////////////
0:   private static final Analyzer analyzer = new StandardAnalyzer(Version.LUCENE_31);
commit:39fe224
/////////////////////////////////////////////////////////////////////////
0: import java.util.Locale;
/////////////////////////////////////////////////////////////////////////
0:     new SimpleDateFormat("", Locale.ENGLISH),
0:     new SimpleDateFormat("MMM-yyyy", Locale.ENGLISH),
0:     new SimpleDateFormat("dd-MMM-yyyy HH:mm:ss", Locale.ENGLISH)
/////////////////////////////////////////////////////////////////////////
0:             line.startsWith("Keywords:") || line.startsWith("Summary:")) && leakType < 6;
commit:f3a9cc1
/////////////////////////////////////////////////////////////////////////
0: import com.google.common.io.Files;
/////////////////////////////////////////////////////////////////////////
/////////////////////////////////////////////////////////////////////////
0:     BufferedReader reader = Files.newReader(file, Charsets.UTF_8);
commit:80366ee
/////////////////////////////////////////////////////////////////////////
0: import com.google.common.base.Charsets;
/////////////////////////////////////////////////////////////////////////
/////////////////////////////////////////////////////////////////////////
0:         new BufferedReader(new InputStreamReader(new FileInputStream(file), Charsets.UTF_8));
commit:35032b8
/////////////////////////////////////////////////////////////////////////
1: }
commit:d61a0ee
/////////////////////////////////////////////////////////////////////////
0:         } while (line != null && line.startsWith(" "));
commit:ad8b61d
/////////////////////////////////////////////////////////////////////////
0: import java.io.FileInputStream;
0: import java.io.InputStreamReader;
0: import java.nio.charset.Charset;
/////////////////////////////////////////////////////////////////////////
0:           ModelSerializer.writeBinary("/tmp/news-group-" + k + ".model",
0:                                       learningAlgorithm.getBest().getPayload().getLearner().getModels().get(0));
/////////////////////////////////////////////////////////////////////////
0:     ModelSerializer.writeBinary("/tmp/news-group.model",
0:                                 learningAlgorithm.getBest().getPayload().getLearner().getModels().get(0));
/////////////////////////////////////////////////////////////////////////
0:       System.out.printf("%s\t%.1f\t%s\t%.1f\t%s\t%.1f\t%s\n",
0:                         w.getFeature(), w.getWeight(), ngNames.get(w.getMaxImpact() + 1),
0:                         w.getCategory(1), w.getWeight(1), w.getCategory(2), w.getWeight(2));
/////////////////////////////////////////////////////////////////////////
0:     BufferedReader reader =
0:         new BufferedReader(new InputStreamReader(new FileInputStream(file), Charset.forName("UTF-8")));
commit:85ec3a3
/////////////////////////////////////////////////////////////////////////
0: import org.apache.mahout.math.function.DoubleFunction;
/////////////////////////////////////////////////////////////////////////
0:         nonZeros = beta.aggregate(Functions.PLUS, new DoubleFunction() {
0:         positive = beta.aggregate(Functions.PLUS, new DoubleFunction() {
commit:58fd277
/////////////////////////////////////////////////////////////////////////
0:   private static final Analyzer analyzer = new StandardAnalyzer(Version.LUCENE_30);
0:   private static final FeatureVectorEncoder encoder = new StaticWordValueEncoder("body");
0:   private static final FeatureVectorEncoder bias = new ConstantValueEncoder("Intercept");
1:   private TrainNewsGroups() {
1:   }
1: 
commit:f201912
/////////////////////////////////////////////////////////////////////////
1:       if (newsgroup.isDirectory()) {
1:         newsGroups.intern(newsgroup.getName());
1:         files.addAll(Arrays.asList(newsgroup.listFiles()));
1:       }
commit:61ae2e7
/////////////////////////////////////////////////////////////////////////
0:   private static final String[] LEAK_LABELS = {"none", "month-year", "day-month-year"};
0:   private static final SimpleDateFormat[] DATE_FORMATS = {
0:   private static final Analyzer ANALYZER = new StandardAnalyzer(Version.LUCENE_30);
0:   private static final FeatureVectorEncoder ENCODER = new StaticWordValueEncoder("body");
0:   private static final FeatureVectorEncoder BIAS = new ConstantValueEncoder("Intercept");
/////////////////////////////////////////////////////////////////////////
0:     ENCODER.setProbes(2);
/////////////////////////////////////////////////////////////////////////
0:             return Math.abs(v) > 1.0e-6 ? 1 : 0;
/////////////////////////////////////////////////////////////////////////
0:           k, averageLL, averageCorrect * 100, LEAK_LABELS[leakType % 3]);
/////////////////////////////////////////////////////////////////////////
0:   private static void dissect(int leakType,
0:                               Dictionary newsGroups,
0:                               AdaptiveLogisticRegression learningAlgorithm,
0:                               Iterable<File> files) throws IOException {
0:     ModelDissector md = new ModelDissector();
0:     ENCODER.setTraceDictionary(traceDictionary);
0:     BIAS.setTraceDictionary(traceDictionary);
/////////////////////////////////////////////////////////////////////////
0:       Reader dateString = new StringReader(DATE_FORMATS[leakType % 3].format(new Date(date)));
0:       countWords(ANALYZER, words, dateString);
/////////////////////////////////////////////////////////////////////////
0:             countWords(ANALYZER, words, in);
0:         countWords(ANALYZER, words, reader);
0:     BIAS.addToVector("", 1, v);
0:       ENCODER.addToVector(word, Math.log(1 + words.count(word)), v);
commit:d53cf4a
/////////////////////////////////////////////////////////////////////////
0: import org.apache.mahout.common.RandomUtils;
/////////////////////////////////////////////////////////////////////////
0: import java.util.Collection;
/////////////////////////////////////////////////////////////////////////
1: public final class TrainNewsGroups {
1: 
0:   private static final Random rand = RandomUtils.getRandom();
0:   private static final SimpleDateFormat[] df = {
/////////////////////////////////////////////////////////////////////////
0:   private TrainNewsGroups() {
1:   }
1: 
/////////////////////////////////////////////////////////////////////////
0:     int[] bumps = {1, 2, 5};
/////////////////////////////////////////////////////////////////////////
0:     try {
0:       String line = reader.readLine();
0:       Reader dateString = new StringReader(df[leakType % 3].format(new Date(date)));
0:       countWords(analyzer, words, dateString);
0:       while (line != null && line.length() > 0) {
0:         boolean countHeader = (
0:           line.startsWith("From:") || line.startsWith("Subject:") ||
0:             line.startsWith("Keywords:") || line.startsWith("Summary:")) && (leakType < 6);
0:         do {
0:           Reader in = new StringReader(line);
0:           if (countHeader) {
0:             countWords(analyzer, words, in);
1:           }
0:           line = reader.readLine();
0:         } while (line.startsWith(" "));
1:       }
0:       if (leakType < 3) {
0:         countWords(analyzer, words, reader);
1:       }
0:     } finally {
0:       reader.close();
/////////////////////////////////////////////////////////////////////////
0:   private static void countWords(Analyzer analyzer, Collection<String> words, Reader in) throws IOException {
/////////////////////////////////////////////////////////////////////////
0:   private static List<File> permute(Iterable<File> files, Random rand) {
author:Robin Anil
-------------------------------------------------------------------------------
commit:5257bc9
/////////////////////////////////////////////////////////////////////////
1: import org.apache.mahout.classifier.NewsgroupHelper;
1: import org.apache.mahout.ep.State;
1: import org.apache.mahout.math.Vector;
1: import org.apache.mahout.vectorizer.encoders.Dictionary;
1: 
0: import com.google.common.collect.HashMultiset;
0: import com.google.common.collect.Lists;
0: import com.google.common.collect.Multiset;
0: import com.google.common.collect.Ordering;
1: 
commit:5a7067b
/////////////////////////////////////////////////////////////////////////
0: import org.apache.mahout.vectorizer.encoders.ConstantValueEncoder;
0: import org.apache.mahout.vectorizer.encoders.Dictionary;
0: import org.apache.mahout.vectorizer.encoders.FeatureVectorEncoder;
0: import org.apache.mahout.vectorizer.encoders.StaticWordValueEncoder;
author:Grant Ingersoll
-------------------------------------------------------------------------------
commit:8a2c0f3
/////////////////////////////////////////////////////////////////////////
/////////////////////////////////////////////////////////////////////////
/////////////////////////////////////////////////////////////////////////
1:  * <p/>
1:  * <p/>
/////////////////////////////////////////////////////////////////////////
1:  * <p/>
1:  * <p/>
1:  * <p/>
1:  * <p/>
/////////////////////////////////////////////////////////////////////////
/////////////////////////////////////////////////////////////////////////
1:     SGDInfo info = new SGDInfo();
1: 
1: 
/////////////////////////////////////////////////////////////////////////
1:       SGDHelper.analyzeState(info, leakType, k, best);
1:     SGDHelper.dissect(leakType, newsGroups, learningAlgorithm, files, overallCounts);
0:             learningAlgorithm.getBest().getPayload().getLearner().getModels().get(0));
/////////////////////////////////////////////////////////////////////////
commit:6c86257
/////////////////////////////////////////////////////////////////////////
0:     System.out.println("============");
0:     System.out.println("Model Dissection");
commit:95736dd
/////////////////////////////////////////////////////////////////////////
/////////////////////////////////////////////////////////////////////////
/////////////////////////////////////////////////////////////////////////
0:     NewsgroupHelper.encoder.setProbes(2);
0:     AdaptiveLogisticRegression learningAlgorithm = new AdaptiveLogisticRegression(20, NewsgroupHelper.FEATURES, new L1());
/////////////////////////////////////////////////////////////////////////
1:     for (File file : files) {
0:       Vector v = NewsgroupHelper.encodeFeatureVector(file, actual, leakType, overallCounts);
/////////////////////////////////////////////////////////////////////////
0:     NewsgroupHelper.encoder.setTraceDictionary(traceDictionary);
0:     NewsgroupHelper.bias.setTraceDictionary(traceDictionary);
0:     for (File file : permute(files, NewsgroupHelper.rand).subList(0, 500)) {
0:       Vector v = NewsgroupHelper.encodeFeatureVector(file, actual, leakType, overallCounts);
/////////////////////////////////////////////////////////////////////////
commit:a4778a4
/////////////////////////////////////////////////////////////////////////
1: 
0: import org.apache.lucene.analysis.tokenattributes.CharTermAttribute;
/////////////////////////////////////////////////////////////////////////
0:     ts.addAttribute(CharTermAttribute.class);
0:       String s = ts.getAttribute(CharTermAttribute.class).toString();
author:Ted Dunning
-------------------------------------------------------------------------------
commit:02f8694
/////////////////////////////////////////////////////////////////////////
0:     for (File file : files.subList(0, 3000)) {
/////////////////////////////////////////////////////////////////////////
1:       State<AdaptiveLogisticRegression.Wrapper, CrossFoldLearner> best = learningAlgorithm.getBest();
/////////////////////////////////////////////////////////////////////////
0:           ModelSerializer.writeBinary("/tmp/news-group-" + k + ".model", learningAlgorithm.getBest().getPayload().getLearner().getModels().get(0));
/////////////////////////////////////////////////////////////////////////
0:     ModelSerializer.writeBinary("/tmp/news-group.model", learningAlgorithm.getBest().getPayload().getLearner().getModels().get(0));
commit:319fd82
/////////////////////////////////////////////////////////////////////////
1: 
/////////////////////////////////////////////////////////////////////////
0:         if (learningAlgorithm.getBest() != null) {
0:           ModelSerializer.writeJson("/tmp/news-group-" + k + ".model", learningAlgorithm.getBest().getPayload().getLearner());
1:         }
1: 
0:         System.out.printf("%.2f\t%.2f\t%.2f\t%.2f\t%.8g\t%.8g\t", maxBeta, nonZeros, positive, norm, lambda, mu);
/////////////////////////////////////////////////////////////////////////
0:     ModelSerializer.writeJson("/tmp/news-group.model", learningAlgorithm);
1: 
/////////////////////////////////////////////////////////////////////////
1:       if (k > 1000) {
1:         break;
1:       }
/////////////////////////////////////////////////////////////////////////
0:       System.out.printf("%s\t%.1f\t%s\t%.1f\t%s\t%.1f\t%s\n", w.getFeature(), w.getWeight(), ngNames.get(w.getMaxImpact() + 1),
0:         w.getCategory(1), w.getWeight(1), w.getCategory(2), w.getWeight(2));
commit:f029d24
/////////////////////////////////////////////////////////////////////////
0: import com.google.common.collect.HashMultiset;
0: import com.google.common.collect.Ordering;
/////////////////////////////////////////////////////////////////////////
0:   private static Analyzer analyzer = new StandardAnalyzer(Version.LUCENE_30);
0:   private static FeatureVectorEncoder encoder = new StaticWordValueEncoder("body");
0:   private static FeatureVectorEncoder bias = new ConstantValueEncoder("Intercept");
0:   private static Multiset<String> overallCounts;
0:     overallCounts = HashMultiset.create();
1: 
/////////////////////////////////////////////////////////////////////////
0:     encoder.setProbes(2);
/////////////////////////////////////////////////////////////////////////
1: 
0:     List<Integer> counts = Lists.newArrayList();
0:     System.out.printf("Word counts\n");
1:     for (String count : overallCounts.elementSet()) {
1:       counts.add(overallCounts.count(count));
1:     }
1:     Collections.sort(counts, Ordering.natural().reverse());
1:     k = 0;
1:     for (Integer count : counts) {
0:       System.out.printf("%d\t%d\n", k, count);
1:       k++;
1:     }
/////////////////////////////////////////////////////////////////////////
0:     encoder.setTraceDictionary(traceDictionary);
0:     bias.setTraceDictionary(traceDictionary);
/////////////////////////////////////////////////////////////////////////
0:       countWords(analyzer, words, dateString);
/////////////////////////////////////////////////////////////////////////
0:             countWords(analyzer, words, in);
0:         countWords(analyzer, words, reader);
0:     bias.addToVector("", 1, v);
0:       encoder.addToVector(word, Math.log(1 + words.count(word)), v);
/////////////////////////////////////////////////////////////////////////
0:     overallCounts.addAll(words);
commit:c93818e
/////////////////////////////////////////////////////////////////////////
/////////////////////////////////////////////////////////////////////////
0: import java.util.Collections;
/////////////////////////////////////////////////////////////////////////
/////////////////////////////////////////////////////////////////////////
1:     learningAlgorithm.setInterval(800);
/////////////////////////////////////////////////////////////////////////
1:     Collections.shuffle(files);
/////////////////////////////////////////////////////////////////////////
0:     for (File file : files.subList(0, 10000)) {
/////////////////////////////////////////////////////////////////////////
0:         System.out.printf("%d\t%.3f\t%.2f\t%s\n",
0:           k, averageLL, averageCorrect * 100, leakLabels[leakType % 3]);
0:     CrossFoldLearner model = learningAlgorithm.getBest().getPayload().getLearner();
0:     model.close();
1: 
0:     ModelDissector md = new ModelDissector(model.numCategories());
1: 
/////////////////////////////////////////////////////////////////////////
commit:24a75db
/////////////////////////////////////////////////////////////////////////
0: import org.apache.mahout.math.Matrix;
/////////////////////////////////////////////////////////////////////////
0:         OnlineLogisticRegression model = state.getModels().get(0);
0:         // finish off pending regularization
0:         model.close();
1:         
0:         Matrix beta = model.getBeta();
0:         maxBeta = beta.aggregate(Functions.MAX, Functions.ABS);
0:         nonZeros = beta.aggregate(Functions.PLUS, new UnaryFunction() {
0:         positive = beta.aggregate(Functions.PLUS, new UnaryFunction() {
0:         norm = beta.aggregate(Functions.PLUS, Functions.ABS);
/////////////////////////////////////////////////////////////////////////
0:     CrossFoldLearner model = learningAlgorithm.getBest().getPayload().getLearner();
0:     model.close();
1:     
0:       md.update(v, traceDictionary, model);
commit:9a5b461
/////////////////////////////////////////////////////////////////////////
0:       if (k % 100 == 0) {
1:       k++;
commit:9cfe290
/////////////////////////////////////////////////////////////////////////
0:         System.out.printf("%.2f\t%.2f\t%.2f\t%.2f\t%.8f\t%.8f\t", maxBeta, nonZeros, positive, norm, lambda, mu);
commit:51e2486
/////////////////////////////////////////////////////////////////////////
1: /*
1:  * Licensed to the Apache Software Foundation (ASF) under one or more
1:  * contributor license agreements.  See the NOTICE file distributed with
1:  * this work for additional information regarding copyright ownership.
1:  * The ASF licenses this file to You under the Apache License, Version 2.0
1:  * (the "License"); you may not use this file except in compliance with
1:  * the License.  You may obtain a copy of the License at
1:  *
1:  *     http://www.apache.org/licenses/LICENSE-2.0
1:  *
1:  * Unless required by applicable law or agreed to in writing, software
1:  * distributed under the License is distributed on an "AS IS" BASIS,
1:  * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
1:  * See the License for the specific language governing permissions and
1:  * limitations under the License.
1:  */
1: 
/////////////////////////////////////////////////////////////////////////
1:     learningAlgorithm.setAveragingWindow(500);
commit:8b6f5d7
/////////////////////////////////////////////////////////////////////////
1: package org.apache.mahout.classifier.sgd;
1: 
0: import com.google.common.base.Splitter;
0: import com.google.common.collect.ConcurrentHashMultiset;
0: import com.google.common.collect.Lists;
0: import com.google.common.collect.Maps;
0: import com.google.common.collect.Multiset;
0: import org.apache.lucene.analysis.Analyzer;
0: import org.apache.lucene.analysis.TokenStream;
0: import org.apache.lucene.analysis.standard.StandardAnalyzer;
0: import org.apache.lucene.analysis.tokenattributes.TermAttribute;
0: import org.apache.lucene.util.Version;
0: import org.apache.mahout.ep.State;
0: import org.apache.mahout.math.RandomAccessSparseVector;
0: import org.apache.mahout.math.Vector;
0: import org.apache.mahout.math.function.Functions;
0: import org.apache.mahout.math.function.UnaryFunction;
0: import org.apache.mahout.vectors.ConstantValueEncoder;
0: import org.apache.mahout.vectors.Dictionary;
0: import org.apache.mahout.vectors.FeatureVectorEncoder;
0: import org.apache.mahout.vectors.StaticWordValueEncoder;
1: 
0: import java.io.BufferedReader;
0: import java.io.File;
0: import java.io.FileReader;
0: import java.io.IOException;
0: import java.io.Reader;
0: import java.io.StringReader;
0: import java.text.SimpleDateFormat;
0: import java.util.Arrays;
0: import java.util.Date;
0: import java.util.List;
0: import java.util.Map;
0: import java.util.Random;
0: import java.util.Set;
1: 
1: /**
1:  * Reads and trains an adaptive logistic regression model on the 20 newsgroups data.
1:  * The first command line argument gives the path of the directory holding the training
1:  * data.  The optional second argument, leakType, defines which classes of features to use.
1:  * Importantly, leakType controls whether a synthetic date is injected into the data as
1:  * a target leak and if so, how.
0:  * <p>
1:  * The value of leakType % 3 determines whether the target leak is injected according to
1:  * the following table:
0:  * <p>
1:  * <table>
1:  * <tr><td valign='top'>0</td><td>No leak injected</td></tr>
1:  * <tr><td valign='top'>1</td><td>Synthetic date injected in MMM-yyyy format. This will be a single token and
1:  * is a perfect target leak since each newsgroup is given a different month</td></tr>
1:  * <tr><td valign='top'>2</td><td>Synthetic date injected in dd-MMM-yyyy HH:mm:ss format.  The day varies
1:  * and thus there are more leak symbols that need to be learned.  Ultimately this is just
1:  * as big a leak as case 1.</td></tr>
1:  * </table>
0:  * <p>
1:  * Leaktype also determines what other text will be indexed.  If leakType is greater
1:  * than or equal to 6, then neither headers nor text body will be used for features and the leak is the only
1:  * source of data.  If leakType is greater than or equal to 3, then subject words will be used as features.
1:  * If leakType is less than 3, then both subject and body text will be used as features.
0:  * <p>
1:  * A leakType of 0 gives no leak and all textual features.
0:  * <p>
1:  * See the following table for a summary of commonly used values for leakType
0:  * <p>
1:  * <table>
1:  * <tr><td><b>leakType</b></td><td><b>Leak?</b></td><td><b>Subject?</b></td><td><b>Body?</b></td></tr>
1:  * <tr><td colspan=4><hr></td></tr>
1:  * <tr><td>0</td><td>no</td><td>yes</td><td>yes</td></tr>
1:  * <tr><td>1</td><td>mmm-yyyy</td><td>yes</td><td>yes</td></tr>
1:  * <tr><td>2</td><td>dd-mmm-yyyy</td><td>yes</td><td>yes</td></tr>
1:  * <tr><td colspan=4><hr></td></tr>
1:  * <tr><td>3</td><td>no</td><td>yes</td><td>no</td></tr>
1:  * <tr><td>4</td><td>mmm-yyyy</td><td>yes</td><td>no</td></tr>
1:  * <tr><td>5</td><td>dd-mmm-yyyy</td><td>yes</td><td>no</td></tr>
1:  * <tr><td colspan=4><hr></td></tr>
1:  * <tr><td>6</td><td>no</td><td>no</td><td>no</td></tr>
1:  * <tr><td>7</td><td>mmm-yyyy</td><td>no</td><td>no</td></tr>
1:  * <tr><td>8</td><td>dd-mmm-yyyy</td><td>no</td><td>no</td></tr>
1:  * <tr><td colspan=4><hr></td></tr>
1:  * </table>
1:  */
0: public class TrainNewsGroups {
0:   private static final int FEATURES = 10000;
0:   // 1997-01-15 00:01:00 GMT
0:   private static final long DATE_REFERENCE = 853286460;
0:   private static final long MONTH = 30 * 24 * 3600;
0:   private static final long WEEK = 7 * 24 * 3600;
1: 
0:   private static final Random rand = new Random();
1: 
0:   private static final Splitter ON_COLON = Splitter.on(":");
1: 
0:   private static final String[] leakLabels = {"none", "month-year", "day-month-year"};
0:   private static final SimpleDateFormat[] df = new SimpleDateFormat[]{
0:     new SimpleDateFormat(""),
0:     new SimpleDateFormat("MMM-yyyy"),
0:     new SimpleDateFormat("dd-MMM-yyyy HH:mm:ss")
0:   };
1: 
0:   private static final Analyzer analyzer = new StandardAnalyzer(Version.LUCENE_30);
0:   private static final FeatureVectorEncoder encoder = new StaticWordValueEncoder("body");
0:   private static final FeatureVectorEncoder bias = new ConstantValueEncoder("Intercept");
1: 
1:   public static void main(String[] args) throws IOException {
1:     File base = new File(args[0]);
1: 
1:     int leakType = 0;
1:     if (args.length > 1) {
1:       leakType = Integer.parseInt(args[1]);
0:     }
1: 
1:     Dictionary newsGroups = new Dictionary();
1: 
0:     encoder.setProbes(2);
0:     AdaptiveLogisticRegression learningAlgorithm = new AdaptiveLogisticRegression(20, FEATURES, new L1());
0:     learningAlgorithm.setInterval(200);
1: 
0:     List<File> files = Lists.newArrayList();
1:     for (File newsgroup : base.listFiles()) {
0:       newsGroups.intern(newsgroup.getName());
0:       files.addAll(Arrays.asList(newsgroup.listFiles()));
0:     }
0:     System.out.printf("%d training files\n", files.size());
1: 
0:     double averageLL = 0;
0:     double averageCorrect = 0;
1: 
1:     int k = 0;
0:     double step = 0;
0:     int[] bumps = new int[]{1, 2, 5};
0:     for (File file : permute(files, rand).subList(0, 5000)) {
1:       String ng = file.getParentFile().getName();
1:       int actual = newsGroups.intern(ng);
1: 
0:       Vector v = encodeFeatureVector(file, actual, leakType);
1:       learningAlgorithm.train(actual, v);
1: 
0:       k++;
1: 
0:       int bump = bumps[(int) Math.floor(step) % bumps.length];
0:       int scale = (int) Math.pow(10, Math.floor(step / bumps.length));
0:       State<AdaptiveLogisticRegression.Wrapper> best = learningAlgorithm.getBest();
0:       double maxBeta;
0:       double nonZeros;
0:       double positive;
0:       double norm;
1: 
0:       double lambda = 0;
0:       double mu = 0;
1:       
0:       if (best != null) {
0:         CrossFoldLearner state = best.getPayload().getLearner();
0:         averageCorrect = state.percentCorrect();
0:         averageLL = state.logLikelihood();
1: 
0:         maxBeta = state.getModels().get(0).getBeta().aggregate(Functions.MAX, Functions.IDENTITY);
0:         nonZeros = state.getModels().get(0).getBeta().aggregate(Functions.PLUS, new UnaryFunction() {
0:           @Override
0:           public double apply(double v) {
0:             return Math.abs(v) > 1e-6 ? 1 : 0;
0:           }
0:         });
0:         positive = state.getModels().get(0).getBeta().aggregate(Functions.PLUS, new UnaryFunction() {
0:           @Override
0:           public double apply(double v) {
0:             return v > 0 ? 1 : 0;
0:           }
0:         });
0:         norm = state.getModels().get(0).getBeta().aggregate(Functions.PLUS, Functions.ABS);
1: 
0:         lambda = learningAlgorithm.getBest().getMappedParams()[0];
0:         mu = learningAlgorithm.getBest().getMappedParams()[1];
0:       } else {
0:         maxBeta = 0;
0:         nonZeros = 0;
0:         positive = 0;
0:         norm = 0;
0:       }
0:       if (k % (bump * scale) == 0) {
0:         step += 0.25;
0:         System.out.printf("%.2f\t%.2f\t%.2f\t%.2f\t%.2f\t%.2f\t", maxBeta, nonZeros, positive, norm, lambda, mu);
0:         System.out.printf("%d\t%.3f\t%.2f\t%s\t%s\n",
0:           k, averageLL, averageCorrect * 100, ng, leakLabels[leakType % 3]);
0:       }
0:     }
1:     learningAlgorithm.close();
1: 
0:     dissect(leakType, newsGroups, learningAlgorithm, files);
1:     System.out.println("exiting main");
0:   }
1: 
0:   private static void dissect(int leakType, Dictionary newsGroups, AdaptiveLogisticRegression learningAlgorithm, List<File> files) throws IOException {
0:     Map<String, Set<Integer>> traceDictionary = Maps.newTreeMap();
0:     System.out.printf("starting dissection\n");
0:     ModelDissector md = new ModelDissector(learningAlgorithm.getBest().getPayload().getLearner().numCategories());
1: 
0:     encoder.setTraceDictionary(traceDictionary);
0:     bias.setTraceDictionary(traceDictionary);
1:     int k = 0;
0:     for (File file : permute(files, rand).subList(0, 500)) {
1:       String ng = file.getParentFile().getName();
1:       int actual = newsGroups.intern(ng);
1: 
0:       traceDictionary.clear();
0:       Vector v = encodeFeatureVector(file, actual, leakType);
0:       md.update(v, traceDictionary, learningAlgorithm.getBest().getPayload().getLearner());
0:       if (k % 50 == 0) {
0:         System.out.printf("%d\t%d\n", k, traceDictionary.size());
0:       }
0:     }
1: 
0:     List<String> ngNames = Lists.newArrayList(newsGroups.values());
0:     List<ModelDissector.Weight> weights = md.summary(100);
0:     for (ModelDissector.Weight w : weights) {
0:       System.out.printf("%s\t%.1f\t%s\n", w.getFeature(), w.getWeight(), ngNames.get(w.getMaxImpact() + 1));
0:     }
0:   }
1: 
0:   private static Vector encodeFeatureVector(File file, int actual, int leakType) throws IOException {
0:     long date = (long) (1000 * (DATE_REFERENCE + actual * MONTH + 1 * WEEK * rand.nextDouble()));
0:     Multiset<String> words = ConcurrentHashMultiset.create();
1: 
0:     BufferedReader reader = new BufferedReader(new FileReader(file));
0:     String line = reader.readLine();
0:     Reader dateString = new StringReader(df[leakType % 3].format(new Date(date)));
0:     countWords(analyzer, words, dateString);
0:     while (line != null && line.length() > 0) {
0:       boolean countHeader = (
0:         line.startsWith("From:") || line.startsWith("Subject:") ||
0:           line.startsWith("Keywords:") || line.startsWith("Summary:")) && (leakType < 6);
0:       do {
0:         StringReader in = new StringReader(line);
0:         if (countHeader) {
0:           countWords(analyzer, words, in);
0:         }
0:         line = reader.readLine();
0:       } while (line.startsWith(" "));
0:     }
0:     if (leakType < 3) {
0:       countWords(analyzer, words, reader);
0:     }
0:     reader.close();
1: 
0:     Vector v = new RandomAccessSparseVector(FEATURES);
0:     bias.addToVector("", 1, v);
0:     for (String word : words.elementSet()) {
0:       encoder.addToVector(word, Math.log(1 + words.count(word)), v);
0:     }
1: 
0:     return v;
0:   }
1: 
0:   private static void countWords(Analyzer analyzer, Multiset<String> words, Reader in) throws IOException {
0:     TokenStream ts = analyzer.tokenStream("text", in);
0:     ts.addAttribute(TermAttribute.class);
0:     while (ts.incrementToken()) {
0:       String s = ts.getAttribute(TermAttribute.class).term();
0:       words.add(s);
0:     }
0:   }
1: 
0:   private static List<File> permute(List<File> files, Random rand) {
0:     List<File> r = Lists.newArrayList();
0:     for (File file : files) {
0:       int i = rand.nextInt(r.size() + 1);
0:       if (i == r.size()) {
0:         r.add(file);
0:       } else {
0:         r.add(r.get(i));
0:         r.set(i, file);
0:       }
0:     }
0:     return r;
0:   }
1: 
0: }
============================================================================