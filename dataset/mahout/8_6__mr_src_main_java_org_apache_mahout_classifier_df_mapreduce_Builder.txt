1:a8f9f88: /**
1:a8f9f88:  * Licensed to the Apache Software Foundation (ASF) under one or more
1:a8f9f88:  * contributor license agreements.  See the NOTICE file distributed with
1:a8f9f88:  * this work for additional information regarding copyright ownership.
1:a8f9f88:  * The ASF licenses this file to You under the Apache License, Version 2.0
1:a8f9f88:  * (the "License"); you may not use this file except in compliance with
1:a8f9f88:  * the License.  You may obtain a copy of the License at
1:a8f9f88:  *
1:a8f9f88:  *     http://www.apache.org/licenses/LICENSE-2.0
1:a8f9f88:  *
1:a8f9f88:  * Unless required by applicable law or agreed to in writing, software
1:a8f9f88:  * distributed under the License is distributed on an "AS IS" BASIS,
1:a8f9f88:  * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
1:a8f9f88:  * See the License for the specific language governing permissions and
1:a8f9f88:  * limitations under the License.
1:a8f9f88:  */
13:a8f9f88: 
1:52ce412: package org.apache.mahout.classifier.df.mapreduce;
1:a8f9f88: 
1:d6aba1a: import com.google.common.base.Preconditions;
1:a8f9f88: import org.apache.hadoop.conf.Configuration;
1:a8f9f88: import org.apache.hadoop.filecache.DistributedCache;
1:a8f9f88: import org.apache.hadoop.fs.FileSystem;
1:a8f9f88: import org.apache.hadoop.fs.Path;
1:a8f9f88: import org.apache.hadoop.mapreduce.InputSplit;
1:a8f9f88: import org.apache.hadoop.mapreduce.Job;
1:52ce412: import org.apache.mahout.classifier.df.DecisionForest;
1:52ce412: import org.apache.mahout.classifier.df.builder.TreeBuilder;
1:52ce412: import org.apache.mahout.classifier.df.data.Dataset;
1:d6aba1a: import org.apache.mahout.common.HadoopUtil;
1:d6aba1a: import org.apache.mahout.common.StringUtils;
1:a8f9f88: import org.slf4j.Logger;
1:a8f9f88: import org.slf4j.LoggerFactory;
1:a8f9f88: 
1:d6aba1a: import java.io.IOException;
1:d6aba1a: import java.util.Arrays;
1:d6aba1a: import java.util.Comparator;
1:a8f9f88: 
1:a8f9f88: /**
1:ad11134:  * Base class for Mapred DecisionForest builders. Takes care of storing the parameters common to the mapred
1:ad11134:  * implementations.<br>
1:a8f9f88:  * The child classes must implement at least :
1:a8f9f88:  * <ul>
1:ad11134:  * <li>void configureJob(Job) : to further configure the job before its launch; and</li>
1:ad11134:  * <li>DecisionForest parseOutput(Job, PredictionCallback) : in order to convert the job outputs into a
1:ad11134:  * DecisionForest and its corresponding oob predictions</li>
1:a8f9f88:  * </ul>
1:a8f9f88:  * 
1:a8f9f88:  */
1:1ffa3a4: @Deprecated
1:a8f9f88: public abstract class Builder {
1:a8f9f88:   
1:a8f9f88:   private static final Logger log = LoggerFactory.getLogger(Builder.class);
1:a8f9f88:   
1:fc74924:   private final TreeBuilder treeBuilder;
1:fc74924:   private final Path dataPath;
1:fc74924:   private final Path datasetPath;
1:fc74924:   private final Long seed;
1:a8f9f88:   private final Configuration conf;
1:8547de7:   private String outputDirName = "output";
1:a8f9f88:   
1:ad11134:   protected Builder(TreeBuilder treeBuilder, Path dataPath, Path datasetPath, Long seed, Configuration conf) {
1:a8f9f88:     this.treeBuilder = treeBuilder;
1:a8f9f88:     this.dataPath = dataPath;
1:a8f9f88:     this.datasetPath = datasetPath;
1:a8f9f88:     this.seed = seed;
1:a8f9f88:     this.conf = new Configuration(conf);
3:fc74924:   }
1:a8f9f88:   
1:fc74924:   protected Path getDataPath() {
1:fc74924:     return dataPath;
1:a8f9f88:   }
1:e9cc323:   
1:a8f9f88:   /**
1:306b285:    * Return the value of "mapred.map.tasks".
1:a8f9f88:    * 
1:ad11134:    * @param conf
1:ad11134:    *          configuration
1:a8f9f88:    * @return number of map tasks
1:a8f9f88:    */
1:a8f9f88:   public static int getNumMaps(Configuration conf) {
1:a8f9f88:     return conf.getInt("mapred.map.tasks", -1);
1:a8f9f88:   }
1:306b285: 
1:a8f9f88:   /**
1:ad11134:    * Used only for DEBUG purposes. if false, the mappers doesn't output anything, so the builder has nothing
1:ad11134:    * to process
1:a8f9f88:    * 
1:ad11134:    * @param conf
1:ad11134:    *          configuration
1:a8f9f88:    * @return true if the builder has to return output. false otherwise
1:a8f9f88:    */
1:a8f9f88:   protected static boolean isOutput(Configuration conf) {
1:a8f9f88:     return conf.getBoolean("debug.mahout.rf.output", true);
1:a8f9f88:   }
1:a8f9f88:   
1:a8f9f88:   /**
1:a8f9f88:    * Returns the random seed
1:a8f9f88:    * 
1:ad11134:    * @param conf
1:ad11134:    *          configuration
1:a8f9f88:    * @return null if no seed is available
1:a8f9f88:    */
1:a8f9f88:   public static Long getRandomSeed(Configuration conf) {
1:a8f9f88:     String seed = conf.get("mahout.rf.random.seed");
1:ad11134:     if (seed == null) {
1:a8f9f88:       return null;
1:a8f9f88:     }
1:a8f9f88:     
1:a8f9f88:     return Long.valueOf(seed);
1:a8f9f88:   }
1:a8f9f88:   
1:a8f9f88:   /**
1:a8f9f88:    * Sets the random seed value
1:a8f9f88:    * 
1:ad11134:    * @param conf
1:ad11134:    *          configuration
1:ad11134:    * @param seed
1:ad11134:    *          random seed
1:a8f9f88:    */
1:8547de7:   private static void setRandomSeed(Configuration conf, long seed) {
1:a8f9f88:     conf.setLong("mahout.rf.random.seed", seed);
1:a8f9f88:   }
1:a8f9f88:   
1:a8f9f88:   public static TreeBuilder getTreeBuilder(Configuration conf) {
1:a8f9f88:     String string = conf.get("mahout.rf.treebuilder");
1:ad11134:     if (string == null) {
1:a8f9f88:       return null;
1:a8f9f88:     }
1:a8f9f88:     
1:5a2b3ae:     return StringUtils.fromString(string);
1:a8f9f88:   }
1:a8f9f88:   
1:ad11134:   private static void setTreeBuilder(Configuration conf, TreeBuilder treeBuilder) {
1:a8f9f88:     conf.set("mahout.rf.treebuilder", StringUtils.toString(treeBuilder));
1:a8f9f88:   }
1:a8f9f88:   
1:a8f9f88:   /**
1:a8f9f88:    * Get the number of trees for the map-reduce job.
1:a8f9f88:    * 
1:ad11134:    * @param conf
1:ad11134:    *          configuration
1:a8f9f88:    * @return number of trees to build
1:a8f9f88:    */
1:a8f9f88:   public static int getNbTrees(Configuration conf) {
1:a8f9f88:     return conf.getInt("mahout.rf.nbtrees", -1);
1:a8f9f88:   }
1:a8f9f88:   
1:a8f9f88:   /**
1:a8f9f88:    * Set the number of trees to grow for the map-reduce job
1:a8f9f88:    * 
1:ad11134:    * @param conf
1:ad11134:    *          configuration
1:ad11134:    * @param nbTrees
1:ad11134:    *          number of trees to build
1:ad11134:    * @throws IllegalArgumentException
1:ad11134:    *           if (nbTrees <= 0)
1:a8f9f88:    */
1:a8f9f88:   public static void setNbTrees(Configuration conf, int nbTrees) {
1:69ba194:     Preconditions.checkArgument(nbTrees > 0, "nbTrees should be greater than 0");
1:a8f9f88: 
1:a8f9f88:     conf.setInt("mahout.rf.nbtrees", nbTrees);
1:a8f9f88:   }
1:a8f9f88:   
1:a8f9f88:   /**
1:a8f9f88:    * Sets the Output directory name, will be creating in the working directory
1:a8f9f88:    * 
1:ad11134:    * @param name
1:ad11134:    *          output dir. name
1:a8f9f88:    */
1:a8f9f88:   public void setOutputDirName(String name) {
1:a8f9f88:     outputDirName = name;
1:a8f9f88:   }
1:a8f9f88:   
1:a8f9f88:   /**
1:a8f9f88:    * Output Directory name
1:a8f9f88:    * 
1:ad11134:    * @param conf
1:ad11134:    *          configuration
1:a8f9f88:    * @return output dir. path (%WORKING_DIRECTORY%/OUTPUT_DIR_NAME%)
1:ad11134:    * @throws IOException
1:ad11134:    *           if we cannot get the default FileSystem
1:a8f9f88:    */
1:e9cc323:   protected Path getOutputPath(Configuration conf) throws IOException {
1:a8f9f88:     // the output directory is accessed only by this class, so use the default
1:a8f9f88:     // file system
1:a8f9f88:     FileSystem fs = FileSystem.get(conf);
1:a8f9f88:     return new Path(fs.getWorkingDirectory(), outputDirName);
1:a8f9f88:   }
1:a8f9f88:   
1:a8f9f88:   /**
1:a8f9f88:    * Helper method. Get a path from the DistributedCache
1:a8f9f88:    * 
1:ad11134:    * @param conf
1:ad11134:    *          configuration
1:ad11134:    * @param index
1:ad11134:    *          index of the path in the DistributedCache files
1:a8f9f88:    * @return path from the DistributedCache
1:ad11134:    * @throws IOException
1:ad11134:    *           if no path is found
1:a8f9f88:    */
1:ad11134:   public static Path getDistributedCacheFile(Configuration conf, int index) throws IOException {
1:6d9179e:     Path[] files = HadoopUtil.getCachedFiles(conf);
1:a8f9f88:     
1:6d9179e:     if (files.length <= index) {
1:a8f9f88:       throw new IOException("path not found in the DistributedCache");
1:a8f9f88:     }
1:a8f9f88:     
1:4a6453c:     return files[index];
1:a8f9f88:   }
1:a8f9f88:   
1:a8f9f88:   /**
1:a8f9f88:    * Helper method. Load a Dataset stored in the DistributedCache
1:a8f9f88:    * 
1:ad11134:    * @param conf
1:ad11134:    *          configuration
1:a8f9f88:    * @return loaded Dataset
1:ad11134:    * @throws IOException
1:ad11134:    *           if we cannot retrieve the Dataset path from the DistributedCache, or the Dataset could not be
1:ad11134:    *           loaded
1:a8f9f88:    */
1:a8f9f88:   public static Dataset loadDataset(Configuration conf) throws IOException {
1:a8f9f88:     Path datasetPath = getDistributedCacheFile(conf, 0);
1:a8f9f88:     
1:a8f9f88:     return Dataset.load(conf, datasetPath);
1:a8f9f88:   }
1:a8f9f88:   
1:a8f9f88:   /**
1:a8f9f88:    * Used by the inheriting classes to configure the job
1:e9cc323:    * 
1:a8f9f88:    *
1:ad11134:    * @param job
1:ad11134:    *          Hadoop's Job
1:ad11134:    * @throws IOException
1:ad11134:    *           if anything goes wrong while configuring the job
1:a8f9f88:    */
1:e9cc323:   protected abstract void configureJob(Job job) throws IOException;
1:a8f9f88:   
1:a8f9f88:   /**
1:ad11134:    * Sequential implementation should override this method to simulate the job execution
1:a8f9f88:    * 
1:ad11134:    * @param job
1:ad11134:    *          Hadoop's job
1:a8f9f88:    * @return true is the job succeeded
1:a8f9f88:    */
1:8547de7:   protected boolean runJob(Job job) throws ClassNotFoundException, IOException, InterruptedException {
1:a8f9f88:     return job.waitForCompletion(true);
1:a8f9f88:   }
1:a8f9f88:   
1:a8f9f88:   /**
1:ad11134:    * Parse the output files to extract the trees and pass the predictions to the callback
1:a8f9f88:    * 
1:ad11134:    * @param job
1:ad11134:    *          Hadoop's job
1:a8f9f88:    * @return Built DecisionForest
1:ad11134:    * @throws IOException
1:ad11134:    *           if anything goes wrong while parsing the output
1:a8f9f88:    */
1:4194a28:   protected abstract DecisionForest parseOutput(Job job) throws IOException;
1:a8f9f88:   
1:ac83cf3:   public DecisionForest build(int nbTrees)
1:d61a0ee:     throws IOException, ClassNotFoundException, InterruptedException {
1:ad11134:     // int numTrees = getNbTrees(conf);
1:a8f9f88:     
1:a8f9f88:     Path outputPath = getOutputPath(conf);
1:a8f9f88:     FileSystem fs = outputPath.getFileSystem(conf);
1:a8f9f88:     
1:a8f9f88:     // check the output
1:ad11134:     if (fs.exists(outputPath)) {
1:acafdc0:       throw new IOException("Output path already exists : " + outputPath);
1:a8f9f88:     }
1:a8f9f88:     
1:ad11134:     if (seed != null) {
1:a8f9f88:       setRandomSeed(conf, seed);
1:fc74924:     }
1:a8f9f88:     setNbTrees(conf, nbTrees);
1:a8f9f88:     setTreeBuilder(conf, treeBuilder);
1:a8f9f88:     
1:a8f9f88:     // put the dataset into the DistributedCache
1:a8f9f88:     DistributedCache.addCacheFile(datasetPath.toUri(), conf);
1:a8f9f88:     
1:a8f9f88:     Job job = new Job(conf, "decision forest builder");
1:a8f9f88:     
1:a8f9f88:     log.debug("Configuring the job...");
1:e9cc323:     configureJob(job);
1:a8f9f88:     
1:a8f9f88:     log.debug("Running the job...");
1:a8f9f88:     if (!runJob(job)) {
1:a8f9f88:       log.error("Job failed!");
1:a8f9f88:       return null;
1:a8f9f88:     }
1:a8f9f88:     
1:a8f9f88:     if (isOutput(conf)) {
1:a8f9f88:       log.debug("Parsing the output...");
1:ac83cf3:       DecisionForest forest = parseOutput(job);
1:a13b4b7:       HadoopUtil.delete(conf, outputPath);
1:a8f9f88:       return forest;
1:a8f9f88:     }
1:a8f9f88:     
1:a8f9f88:     return null;
1:a8f9f88:   }
1:a8f9f88:   
1:a8f9f88:   /**
1:a8f9f88:    * sort the splits into order based on size, so that the biggest go first.<br>
1:a8f9f88:    * This is the same code used by Hadoop's JobClient.
1:a8f9f88:    * 
1:ad11134:    * @param splits
1:ad11134:    *          input splits
1:a8f9f88:    */
1:a8f9f88:   public static void sortSplits(InputSplit[] splits) {
1:a8f9f88:     Arrays.sort(splits, new Comparator<InputSplit>() {
1:8547de7:       @Override
1:a8f9f88:       public int compare(InputSplit a, InputSplit b) {
1:a8f9f88:         try {
1:a8f9f88:           long left = a.getLength();
1:a8f9f88:           long right = b.getLength();
1:a8f9f88:           if (left == right) {
1:a8f9f88:             return 0;
1:a8f9f88:           } else if (left < right) {
2:a8f9f88:             return 1;
1:a8f9f88:           } else {
1:a8f9f88:             return -1;
1:a8f9f88:           }
1:acafdc0:         } catch (IOException ie) {
1:acafdc0:           throw new IllegalStateException("Problem getting input split size", ie);
1:acafdc0:         } catch (InterruptedException ie) {
1:acafdc0:           throw new IllegalStateException("Problem getting input split size", ie);
1:a8f9f88:         }
1:a8f9f88:       }
1:a8f9f88:     });
1:a8f9f88:   }
1:a8f9f88:   
1:a8f9f88: }
============================================================================
author:smarthi
-------------------------------------------------------------------------------
commit:1ffa3a4
/////////////////////////////////////////////////////////////////////////
1: @Deprecated
author:pferrel
-------------------------------------------------------------------------------
commit:b988c49
author:frankscholten
-------------------------------------------------------------------------------
commit:1a42d85
author:Sebastian Schelter
-------------------------------------------------------------------------------
commit:6d9179e
/////////////////////////////////////////////////////////////////////////
/////////////////////////////////////////////////////////////////////////
1:     Path[] files = HadoopUtil.getCachedFiles(conf);
1:     if (files.length <= index) {
commit:210b265
/////////////////////////////////////////////////////////////////////////
author:Grant Ingersoll
-------------------------------------------------------------------------------
commit:4a6453c
/////////////////////////////////////////////////////////////////////////
0:     Path[] files = DistributedCache.getLocalCacheFiles(conf);
1:     return files[index];
commit:52ce412
/////////////////////////////////////////////////////////////////////////
1: package org.apache.mahout.classifier.df.mapreduce;
/////////////////////////////////////////////////////////////////////////
1: import org.apache.mahout.classifier.df.DecisionForest;
1: import org.apache.mahout.classifier.df.builder.TreeBuilder;
1: import org.apache.mahout.classifier.df.data.Dataset;
author:Abdel Hakim Deneche
-------------------------------------------------------------------------------
commit:d6aba1a
/////////////////////////////////////////////////////////////////////////
1: import com.google.common.base.Preconditions;
1: import org.apache.mahout.common.HadoopUtil;
1: import org.apache.mahout.common.StringUtils;
1: import java.io.IOException;
0: import java.net.URI;
1: import java.util.Arrays;
1: import java.util.Comparator;
commit:e9cc323
/////////////////////////////////////////////////////////////////////////
1: 
/////////////////////////////////////////////////////////////////////////
1:   protected Path getOutputPath(Configuration conf) throws IOException {
/////////////////////////////////////////////////////////////////////////
1:    *
1:   protected abstract void configureJob(Job job) throws IOException;
/////////////////////////////////////////////////////////////////////////
1:     configureJob(job);
commit:ac83cf3
/////////////////////////////////////////////////////////////////////////
/////////////////////////////////////////////////////////////////////////
/////////////////////////////////////////////////////////////////////////
0:   protected abstract void configureJob(Job job, int nbTrees) throws IOException;
/////////////////////////////////////////////////////////////////////////
0:   protected abstract DecisionForest parseOutput(Job job)
1:   public DecisionForest build(int nbTrees)
/////////////////////////////////////////////////////////////////////////
/////////////////////////////////////////////////////////////////////////
0:     configureJob(job, nbTrees);
/////////////////////////////////////////////////////////////////////////
1:       DecisionForest forest = parseOutput(job);
commit:306b285
/////////////////////////////////////////////////////////////////////////
1:    * Return the value of "mapred.map.tasks".
1: 
commit:672b6f9
/////////////////////////////////////////////////////////////////////////
0: //    if ("local".equals(tracker)) {
0: //      log.warn("Hadoop running in 'local' mode, only one map task will be launched");
0: //      return 1;
0: //    }
commit:a8f9f88
/////////////////////////////////////////////////////////////////////////
1: /**
1:  * Licensed to the Apache Software Foundation (ASF) under one or more
1:  * contributor license agreements.  See the NOTICE file distributed with
1:  * this work for additional information regarding copyright ownership.
1:  * The ASF licenses this file to You under the Apache License, Version 2.0
1:  * (the "License"); you may not use this file except in compliance with
1:  * the License.  You may obtain a copy of the License at
1:  *
1:  *     http://www.apache.org/licenses/LICENSE-2.0
1:  *
1:  * Unless required by applicable law or agreed to in writing, software
1:  * distributed under the License is distributed on an "AS IS" BASIS,
1:  * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
1:  * See the License for the specific language governing permissions and
1:  * limitations under the License.
1:  */
1: 
0: package org.apache.mahout.df.mapreduce;
1: 
1: import org.apache.hadoop.conf.Configuration;
1: import org.apache.hadoop.filecache.DistributedCache;
1: import org.apache.hadoop.fs.FileSystem;
1: import org.apache.hadoop.fs.Path;
1: import org.apache.hadoop.mapreduce.InputSplit;
1: import org.apache.hadoop.mapreduce.Job;
0: import org.apache.mahout.common.StringUtils;
0: import org.apache.mahout.df.DecisionForest;
0: import org.apache.mahout.df.builder.TreeBuilder;
0: import org.apache.mahout.df.callback.PredictionCallback;
0: import org.apache.mahout.df.data.Dataset;
1: import org.slf4j.Logger;
1: import org.slf4j.LoggerFactory;
1: 
0: import java.io.IOException;
0: import java.net.URI;
0: import java.util.Arrays;
0: import java.util.Comparator;
1: 
1: /**
0:  * Base class for Mapred DecisionForest builders. Takes care of storing the
0:  * parameters common to the mapred implementations.<br>
1:  * The child classes must implement at least :
1:  * <ul>
0:  * <li> void configureJob(Job) : to further configure the job before its
0:  * launch; and </li>
0:  * <li> DecisionForest parseOutput(Job, PredictionCallback) : in order to
0:  * convert the job outputs into a DecisionForest and its corresponding oob
0:  * predictions </li>
1:  * </ul>
1:  * 
1:  */
1: public abstract class Builder {
1: 
1:   private static final Logger log = LoggerFactory.getLogger(Builder.class);
1: 
0:   /** Tree Builder Component */
0:   protected final TreeBuilder treeBuilder;
1: 
0:   protected final Path dataPath;
1: 
0:   protected final Path datasetPath;
1: 
0:   protected final Long seed;
1: 
1:   private final Configuration conf;
1: 
0:   protected String outputDirName = "output";
1: 
0:   protected int numTrees;
1: 
1:   /**
0:    * Return the value of "mapred.map.tasks". In case the 'local' runner is
0:    * detected, returns 1
1:    * 
0:    * @param conf configuration
1:    * @return number of map tasks
1:    */
1:   public static int getNumMaps(Configuration conf) {
0:     // if we are in 'local' mode, correct the number of maps
0:     // or the mappers won't be able to compute the right indexes
0:     String tracker = conf.get("mapred.job.tracker", "local");
0:     if ("local".equals(tracker)) {
0:       log
0:           .warn("Hadoop running in 'local' mode, only one map task will be launched");
1:       return 1;
1:     }
1: 
1:     return conf.getInt("mapred.map.tasks", -1);
1:   }
1: 
1:   /**
0:    * Used only for DEBUG purposes. if false, the mappers doesn't output
0:    * anything, so the builder has nothing to process
1:    * 
0:    * @param conf configuration
1:    * @return true if the builder has to return output. false otherwise
1:    */
1:   protected static boolean isOutput(Configuration conf) {
1:     return conf.getBoolean("debug.mahout.rf.output", true);
1:   }
1: 
0:   protected static boolean isOobEstimate(Configuration conf) {
0:     return conf.getBoolean("mahout.rf.oob", false);
1:   }
1: 
0:   protected static void setOobEstimate(Configuration conf, boolean value) {
0:     conf.setBoolean("mahout.rf.oob", value);
1:   }
1: 
1:   /**
1:    * Returns the random seed
1:    * 
0:    * @param conf configuration
1:    * @return null if no seed is available
1:    */
1:   public static Long getRandomSeed(Configuration conf) {
1:     String seed = conf.get("mahout.rf.random.seed");
0:     if (seed == null)
1:       return null;
1: 
1:     return Long.valueOf(seed);
1:   }
1: 
1:   /**
1:    * Sets the random seed value
1:    * 
0:    * @param conf configuration
0:    * @param seed random seed
1:    */
0:   protected static void setRandomSeed(Configuration conf, long seed) {
1:     conf.setLong("mahout.rf.random.seed", seed);
1:   }
1: 
1:   public static TreeBuilder getTreeBuilder(Configuration conf) {
1:     String string = conf.get("mahout.rf.treebuilder");
0:     if (string == null)
1:       return null;
1: 
0:     return (TreeBuilder) StringUtils.fromString(string);
1:   }
1: 
0:   protected static void setTreeBuilder(Configuration conf,
0:       TreeBuilder treeBuilder) {
1:     conf.set("mahout.rf.treebuilder", StringUtils.toString(treeBuilder));
1:   }
1: 
1:   /**
1:    * Get the number of trees for the map-reduce job.
1:    * 
0:    * @param conf configuration
1:    * @return number of trees to build
1:    */
1:   public static int getNbTrees(Configuration conf) {
1:     return conf.getInt("mahout.rf.nbtrees", -1);
1:   }
1: 
1:   /**
1:    * Set the number of trees to grow for the map-reduce job
1:    * 
0:    * @param conf configuration
0:    * @param nbTrees number of trees to build
0:    * @throws IllegalArgumentException if (nbTrees <= 0)
1:    */
1:   public static void setNbTrees(Configuration conf, int nbTrees) {
0:     if (nbTrees <= 0)
0:       throw new IllegalArgumentException("nbTrees should be greater than 0");
1: 
1:     conf.setInt("mahout.rf.nbtrees", nbTrees);
1:   }
1: 
1:   /**
1:    * Sets the Output directory name, will be creating in the working directory
1:    * 
0:    * @param name output dir. name
1:    */
1:   public void setOutputDirName(String name) {
1:     outputDirName = name;
1:   }
1: 
1:   /**
1:    * Output Directory name
1:    * 
0:    * @param conf configuration
1:    * @return output dir. path (%WORKING_DIRECTORY%/OUTPUT_DIR_NAME%)
0:    * @throws IOException if we cannot get the default FileSystem
1:    */
0:   public Path getOutputPath(Configuration conf) throws IOException {
1:     // the output directory is accessed only by this class, so use the default
1:     // file system
1:     FileSystem fs = FileSystem.get(conf);
1:     return new Path(fs.getWorkingDirectory(), outputDirName);
1:   }
1: 
1:   /**
1:    * Helper method. Get a path from the DistributedCache
1:    * 
0:    * @param conf configuration
0:    * @param index index of the path in the DistributedCache files
1:    * @return path from the DistributedCache
0:    * @throws IOException if no path is found
1:    */
0:   public static Path getDistributedCacheFile(Configuration conf, int index)
0:       throws IOException {
0:     URI[] files = DistributedCache.getCacheFiles(conf);
1: 
0:     if (files == null || files.length <= index) {
1:       throw new IOException("path not found in the DistributedCache");
1:     }
1: 
0:     return new Path(files[index].getPath());
1:   }
1: 
1:   /**
1:    * Helper method. Load a Dataset stored in the DistributedCache
1:    * 
0:    * @param conf configuration
1:    * @return loaded Dataset
0:    * @throws IOException if we cannot retrieve the Dataset path from the DistributedCache, or the Dataset could not be loaded
1:    */
1:   public static Dataset loadDataset(Configuration conf) throws IOException {
1:     Path datasetPath = getDistributedCacheFile(conf, 0);
1: 
1:     return Dataset.load(conf, datasetPath);
1:   }
1: 
0:   public Builder(TreeBuilder treeBuilder, Path dataPath, Path datasetPath,
0:       Long seed, Configuration conf) {
1:     this.treeBuilder = treeBuilder;
1:     this.dataPath = dataPath;
1:     this.datasetPath = datasetPath;
1:     this.seed = seed;
1:     this.conf = new Configuration(conf);
1:   }
1: 
1:   /**
1:    * Used by the inheriting classes to configure the job
1:    * 
0:    * @param job Hadoop's Job
0:    * @param nbTrees number of trees to grow
0:    * @param oobEstimate true, if oob error should be estimated
0:    * @throws IOException if anything goes wrong while configuring the job
1:    */
0:   protected abstract void configureJob(Job job, int nbTrees, boolean oobEstimate)
0:       throws IOException;
1: 
1:   /**
0:    * Sequential implementation should override this method to simulate the job
0:    * execution
1:    * 
0:    * @param job Hadoop's job
1:    * @return true is the job succeeded
0:    * @throws Exception if the job encounters an error
1:    */
0:   protected boolean runJob(Job job) throws Exception {
1:     return job.waitForCompletion(true);
1:   }
1: 
1:   /**
0:    * Parse the output files to extract the trees and pass the predictions to the
0:    * callback
1:    * 
0:    * @param job Hadoop's job
0:    * @param callback can be null
1:    * @return Built DecisionForest
0:    * @throws Exception if anything goes wrong while parsing the output
1:    */
0:   protected abstract DecisionForest parseOutput(Job job, PredictionCallback callback) throws Exception;
1: 
0:   public DecisionForest build(int nbTrees, PredictionCallback callback)
0:       throws Exception {
0:     numTrees = getNbTrees(conf);
1: 
1:     Path outputPath = getOutputPath(conf);
1:     FileSystem fs = outputPath.getFileSystem(conf);
1: 
1:     // check the output
0:     if (fs.exists(outputPath))
0:       throw new RuntimeException("Ouput path already exists : " + outputPath);
1: 
0:     if (seed != null)
1:       setRandomSeed(conf, seed);
1:     setNbTrees(conf, nbTrees);
1:     setTreeBuilder(conf, treeBuilder);
0:     setOobEstimate(conf, callback != null);
1: 
1:     // put the dataset into the DistributedCache
1:     DistributedCache.addCacheFile(datasetPath.toUri(), conf);
1: 
1:     Job job = new Job(conf, "decision forest builder");
1: 
1:     log.debug("Configuring the job...");
0:     configureJob(job, nbTrees, callback != null);
1: 
1:     log.debug("Running the job...");
1:     if (!runJob(job)) {
1:       log.error("Job failed!");
1:       return null;
1:     }
1: 
1:     if (isOutput(conf)) {
1:       log.debug("Parsing the output...");
0:       DecisionForest forest = parseOutput(job, callback);
1: 
0:       // delete the output path
0:       fs.delete(outputPath, true);
1: 
1:       return forest;
1:     }
1: 
1:     return null;
1:   }
1: 
1:   /**
1:    * sort the splits into order based on size, so that the biggest go first.<br>
1:    * This is the same code used by Hadoop's JobClient.
1:    * 
0:    * @param splits input splits
1:    */
1:   public static void sortSplits(InputSplit[] splits) {
1:     Arrays.sort(splits, new Comparator<InputSplit>() {
1:       public int compare(InputSplit a, InputSplit b) {
1:         try {
1:           long left = a.getLength();
1:           long right = b.getLength();
1:           if (left == right) {
1:             return 0;
1:           } else if (left < right) {
1:             return 1;
1:           } else {
1:             return -1;
1:           }
0:         } catch (Exception ie) {
0:           throw new RuntimeException("Problem getting input split size", ie);
1:         }
1:       }
1:     });
1:   }
1: 
1: }
author:Sean R. Owen
-------------------------------------------------------------------------------
commit:4194a28
/////////////////////////////////////////////////////////////////////////
1:   protected abstract DecisionForest parseOutput(Job job) throws IOException;
commit:39fe224
/////////////////////////////////////////////////////////////////////////
0:     if (files == null || files.length <= index) {
commit:a13b4b7
/////////////////////////////////////////////////////////////////////////
1:       HadoopUtil.delete(conf, outputPath);
commit:d61a0ee
/////////////////////////////////////////////////////////////////////////
/////////////////////////////////////////////////////////////////////////
1:     throws IOException, ClassNotFoundException, InterruptedException {
commit:69ba194
/////////////////////////////////////////////////////////////////////////
0: import com.google.common.base.Preconditions;
0: 
/////////////////////////////////////////////////////////////////////////
1:     Preconditions.checkArgument(nbTrees > 0, "nbTrees should be greater than 0");
0: 
/////////////////////////////////////////////////////////////////////////
0:   protected abstract DecisionForest parseOutput(Job job, PredictionCallback callback)
0:     throws IOException, ClassNotFoundException, InterruptedException;
0:   public DecisionForest build(int nbTrees, PredictionCallback callback)
0:       throws IOException, ClassNotFoundException, InterruptedException {
commit:f824f90
commit:16937e1
commit:210fac3
commit:18bf663
/////////////////////////////////////////////////////////////////////////
0:     //int numTrees = getNbTrees(conf);
commit:c40fd07
/////////////////////////////////////////////////////////////////////////
/////////////////////////////////////////////////////////////////////////
0:     int numTrees = getNbTrees(conf);
commit:5a2b3ae
/////////////////////////////////////////////////////////////////////////
1:     return StringUtils.fromString(string);
commit:fc74924
/////////////////////////////////////////////////////////////////////////
1:   private final TreeBuilder treeBuilder;
1:   private final Path dataPath;
1:   private final Path datasetPath;
1:   private final Long seed;
0:   private int numTrees;
0: 
0: 
0:   protected TreeBuilder getTreeBuilder() {
0:     return treeBuilder;
1:   }
0: 
1:   protected Path getDataPath() {
1:     return dataPath;
1:   }
0: 
0:   protected Path getDatasetPath() {
0:     return datasetPath;
1:   }
0: 
0:   protected Long getSeed() {
0:     return seed;
1:   }
0: 
commit:acafdc0
/////////////////////////////////////////////////////////////////////////
1:       throw new IOException("Output path already exists : " + outputPath);
/////////////////////////////////////////////////////////////////////////
1:         } catch (IOException ie) {
1:           throw new IllegalStateException("Problem getting input split size", ie);
1:         } catch (InterruptedException ie) {
1:           throw new IllegalStateException("Problem getting input split size", ie);
commit:8547de7
/////////////////////////////////////////////////////////////////////////
1:   private String outputDirName = "output";
/////////////////////////////////////////////////////////////////////////
0:   private static void setOobEstimate(Configuration conf, boolean value) {
/////////////////////////////////////////////////////////////////////////
1:   private static void setRandomSeed(Configuration conf, long seed) {
/////////////////////////////////////////////////////////////////////////
0:   private static void setTreeBuilder(Configuration conf,
/////////////////////////////////////////////////////////////////////////
0:   protected Builder(TreeBuilder treeBuilder, Path dataPath, Path datasetPath,
/////////////////////////////////////////////////////////////////////////
1:   protected boolean runJob(Job job) throws ClassNotFoundException, IOException, InterruptedException {
/////////////////////////////////////////////////////////////////////////
0:    * @throws IOException if anything goes wrong while parsing the output
0:   protected abstract DecisionForest parseOutput(Job job, PredictionCallback callback) throws IOException, ClassNotFoundException, InterruptedException;
0:       throws IOException, ClassNotFoundException, InterruptedException {
/////////////////////////////////////////////////////////////////////////
1:       @Override
author:Robin Anil
-------------------------------------------------------------------------------
commit:297bef5
/////////////////////////////////////////////////////////////////////////
0:       log.warn("Hadoop running in 'local' mode, only one map task will be launched");
/////////////////////////////////////////////////////////////////////////
0:     log.debug("Configuring the job...");
0:     log.debug("Running the job...");
0:       log.error("Job failed!");
0:       log.debug("Parsing the output...");
commit:ad11134
/////////////////////////////////////////////////////////////////////////
0: import java.io.IOException;
0: import java.net.URI;
0: import java.util.Arrays;
0: import java.util.Comparator;
0: 
/////////////////////////////////////////////////////////////////////////
1:  * Base class for Mapred DecisionForest builders. Takes care of storing the parameters common to the mapred
1:  * implementations.<br>
1:  * <li>void configureJob(Job) : to further configure the job before its launch; and</li>
1:  * <li>DecisionForest parseOutput(Job, PredictionCallback) : in order to convert the job outputs into a
1:  * DecisionForest and its corresponding oob predictions</li>
0:   
0:   
0:   
0:   
0:   
0:   
0:   
0:   
0:   
0:   
0:   
0:   
0:    * Return the value of "mapred.map.tasks". In case the 'local' runner is detected, returns 1
1:    * @param conf
1:    *          configuration
/////////////////////////////////////////////////////////////////////////
0:       Builder.log.warn("Hadoop running in 'local' mode, only one map task will be launched");
0:     
0:   
1:    * Used only for DEBUG purposes. if false, the mappers doesn't output anything, so the builder has nothing
1:    * to process
1:    * @param conf
1:    *          configuration
0:   
0:   
0:   
1:    * @param conf
1:    *          configuration
1:     if (seed == null) {
0:     }
0:     
0:   
1:    * @param conf
1:    *          configuration
1:    * @param seed
1:    *          random seed
0:   
1:     if (string == null) {
0:     }
0:     
0:   
1:   private static void setTreeBuilder(Configuration conf, TreeBuilder treeBuilder) {
0:   
1:    * @param conf
1:    *          configuration
0:   
1:    * @param conf
1:    *          configuration
1:    * @param nbTrees
1:    *          number of trees to build
1:    * @throws IllegalArgumentException
1:    *           if (nbTrees <= 0)
0:     if (nbTrees <= 0) {
0:     }
0:     
0:   
1:    * @param name
1:    *          output dir. name
0:   
1:    * @param conf
1:    *          configuration
1:    * @throws IOException
1:    *           if we cannot get the default FileSystem
/////////////////////////////////////////////////////////////////////////
0:   
1:    * @param conf
1:    *          configuration
1:    * @param index
1:    *          index of the path in the DistributedCache files
1:    * @throws IOException
1:    *           if no path is found
1:   public static Path getDistributedCacheFile(Configuration conf, int index) throws IOException {
0:     
0:     if ((files == null) || (files.length <= index)) {
0:     
0:   
1:    * @param conf
1:    *          configuration
1:    * @throws IOException
1:    *           if we cannot retrieve the Dataset path from the DistributedCache, or the Dataset could not be
1:    *           loaded
0:     Path datasetPath = Builder.getDistributedCacheFile(conf, 0);
0:     
0:   
1:   protected Builder(TreeBuilder treeBuilder, Path dataPath, Path datasetPath, Long seed, Configuration conf) {
0:   
1:    * @param job
1:    *          Hadoop's Job
0:    * @param nbTrees
0:    *          number of trees to grow
0:    * @param oobEstimate
0:    *          true, if oob error should be estimated
1:    * @throws IOException
1:    *           if anything goes wrong while configuring the job
0:   protected abstract void configureJob(Job job, int nbTrees, boolean oobEstimate) throws IOException;
0:   
1:    * Sequential implementation should override this method to simulate the job execution
1:    * @param job
1:    *          Hadoop's job
0:   
1:    * Parse the output files to extract the trees and pass the predictions to the callback
1:    * @param job
1:    *          Hadoop's job
0:    * @param callback
0:    *          can be null
1:    * @throws IOException
1:    *           if anything goes wrong while parsing the output
0:   protected abstract DecisionForest parseOutput(Job job, PredictionCallback callback) throws IOException,
0:                                                                                      ClassNotFoundException,
0:                                                                                      InterruptedException;
0:   
0:   public DecisionForest build(int nbTrees, PredictionCallback callback) throws IOException,
0:                                                                        ClassNotFoundException,
0:                                                                        InterruptedException {
1:     // int numTrees = getNbTrees(conf);
0:     
0:     
1:     if (fs.exists(outputPath)) {
0:     }
0:     
1:     if (seed != null) {
0:       Builder.setRandomSeed(conf, seed);
0:     }
0:     Builder.setNbTrees(conf, nbTrees);
0:     Builder.setTreeBuilder(conf, treeBuilder);
0:     Builder.setOobEstimate(conf, callback != null);
0:     
0:     
0:     
0:     Builder.log.debug("Configuring the job...");
0:     
0:     Builder.log.debug("Running the job...");
0:       Builder.log.error("Job failed!");
0:     
0:     if (Builder.isOutput(conf)) {
0:       Builder.log.debug("Parsing the output...");
0:       
0:       
0:     
0:   
1:    * @param splits
1:    *          input splits
/////////////////////////////////////////////////////////////////////////
0:   
============================================================================