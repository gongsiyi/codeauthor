1:a8f9f88: /**
1:a8f9f88:  * Licensed to the Apache Software Foundation (ASF) under one or more
1:a8f9f88:  * contributor license agreements.  See the NOTICE file distributed with
1:a8f9f88:  * this work for additional information regarding copyright ownership.
1:a8f9f88:  * The ASF licenses this file to You under the Apache License, Version 2.0
1:a8f9f88:  * (the "License"); you may not use this file except in compliance with
1:a8f9f88:  * the License.  You may obtain a copy of the License at
2:a8f9f88:  *
1:a8f9f88:  *     http://www.apache.org/licenses/LICENSE-2.0
1:a8f9f88:  *
1:a8f9f88:  * Unless required by applicable law or agreed to in writing, software
1:a8f9f88:  * distributed under the License is distributed on an "AS IS" BASIS,
1:a8f9f88:  * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
1:a8f9f88:  * See the License for the specific language governing permissions and
1:a8f9f88:  * limitations under the License.
1:a8f9f88:  */
7:a8f9f88: 
1:52ce412: package org.apache.mahout.classifier.df.mapreduce.inmem;
1:ac83cf3: 
1:85f9ece: import java.io.IOException;
1:85f9ece: import java.util.ArrayList;
1:85f9ece: import java.util.HashMap;
1:85f9ece: import java.util.List;
1:85f9ece: import java.util.Map;
1:85f9ece: 
1:a8f9f88: import org.apache.hadoop.conf.Configuration;
1:a8f9f88: import org.apache.hadoop.filecache.DistributedCache;
1:a8f9f88: import org.apache.hadoop.fs.FileSystem;
1:a8f9f88: import org.apache.hadoop.fs.Path;
1:a8f9f88: import org.apache.hadoop.io.IntWritable;
1:a8f9f88: import org.apache.hadoop.mapreduce.Job;
1:a8f9f88: import org.apache.hadoop.mapreduce.lib.output.FileOutputFormat;
1:a8f9f88: import org.apache.hadoop.mapreduce.lib.output.SequenceFileOutputFormat;
1:52ce412: import org.apache.mahout.classifier.df.DFUtils;
1:52ce412: import org.apache.mahout.classifier.df.DecisionForest;
1:52ce412: import org.apache.mahout.classifier.df.builder.TreeBuilder;
1:52ce412: import org.apache.mahout.classifier.df.mapreduce.Builder;
1:52ce412: import org.apache.mahout.classifier.df.mapreduce.MapredOutput;
1:52ce412: import org.apache.mahout.classifier.df.node.Node;
1:d6aba1a: import org.apache.mahout.common.Pair;
1:d6aba1a: import org.apache.mahout.common.iterator.sequencefile.SequenceFileIterable;
1:a8f9f88: 
1:a8f9f88: /**
1:ad11134:  * MapReduce implementation where each mapper loads a full copy of the data in-memory. The forest trees are
1:ad11134:  * splitted across all the mappers
1:a8f9f88:  */
1:1ffa3a4: @Deprecated
1:a8f9f88: public class InMemBuilder extends Builder {
1:a8f9f88:   
1:ad11134:   public InMemBuilder(TreeBuilder treeBuilder, Path dataPath, Path datasetPath, Long seed, Configuration conf) {
1:a8f9f88:     super(treeBuilder, dataPath, datasetPath, seed, conf);
4:a8f9f88:   }
1:a8f9f88:   
1:a8f9f88:   public InMemBuilder(TreeBuilder treeBuilder, Path dataPath, Path datasetPath) {
1:a8f9f88:     this(treeBuilder, dataPath, datasetPath, null, new Configuration());
1:a8f9f88:   }
1:a8f9f88:   
1:a8f9f88:   @Override
1:e9cc323:   protected void configureJob(Job job) throws IOException {
1:a8f9f88:     Configuration conf = job.getConfiguration();
1:a8f9f88:     
1:a8f9f88:     job.setJarByClass(InMemBuilder.class);
1:a8f9f88:     
1:a8f9f88:     FileOutputFormat.setOutputPath(job, getOutputPath(conf));
1:a8f9f88:     
1:a8f9f88:     // put the data in the DistributedCache
1:fc74924:     DistributedCache.addCacheFile(getDataPath().toUri(), conf);
1:a8f9f88:     
1:a8f9f88:     job.setOutputKeyClass(IntWritable.class);
1:a8f9f88:     job.setOutputValueClass(MapredOutput.class);
1:a8f9f88:     
1:a8f9f88:     job.setMapperClass(InMemMapper.class);
1:a8f9f88:     job.setNumReduceTasks(0); // no reducers
1:a8f9f88:     
1:a8f9f88:     job.setInputFormatClass(InMemInputFormat.class);
1:a8f9f88:     job.setOutputFormatClass(SequenceFileOutputFormat.class);
1:a8f9f88:     
1:a8f9f88:   }
1:a8f9f88:   
1:a8f9f88:   @Override
1:ac83cf3:   protected DecisionForest parseOutput(Job job) throws IOException {
1:a8f9f88:     Configuration conf = job.getConfiguration();
1:a8f9f88:     
1:85f9ece:     Map<Integer,MapredOutput> output = new HashMap<>();
1:a8f9f88:     
1:a8f9f88:     Path outputPath = getOutputPath(conf);
1:a8f9f88:     FileSystem fs = outputPath.getFileSystem(conf);
1:a8f9f88:     
1:a8f9f88:     Path[] outfiles = DFUtils.listOutputFiles(fs, outputPath);
1:a8f9f88:     
1:a8f9f88:     // import the InMemOutputs
1:a8f9f88:     for (Path path : outfiles) {
1:a13b4b7:       for (Pair<IntWritable,MapredOutput> record : new SequenceFileIterable<IntWritable,MapredOutput>(path, conf)) {
1:a13b4b7:         output.put(record.getFirst().get(), record.getSecond());
1:a8f9f88:       }
1:a8f9f88:     }
1:a8f9f88:     
1:ac83cf3:     return processOutput(output);
1:a8f9f88:   }
1:a8f9f88:   
1:a8f9f88:   /**
1:ac83cf3:    * Process the output, extracting the trees
1:a8f9f88:    */
1:ac83cf3:   private static DecisionForest processOutput(Map<Integer,MapredOutput> output) {
1:85f9ece:     List<Node> trees = new ArrayList<>();
1:a8f9f88:     
1:ad11134:     for (Map.Entry<Integer,MapredOutput> entry : output.entrySet()) {
1:8547de7:       MapredOutput value = entry.getValue();
1:a8f9f88:       trees.add(value.getTree());
1:a8f9f88:     }
1:a8f9f88:     
1:a8f9f88:     return new DecisionForest(trees);
1:a8f9f88:   }
1:a8f9f88: }
============================================================================
author:smarthi
-------------------------------------------------------------------------------
commit:1ffa3a4
/////////////////////////////////////////////////////////////////////////
1: @Deprecated
author:Suneel Marthi
-------------------------------------------------------------------------------
commit:85f9ece
/////////////////////////////////////////////////////////////////////////
1: import java.io.IOException;
1: import java.util.ArrayList;
1: import java.util.HashMap;
1: import java.util.List;
1: import java.util.Map;
1: 
/////////////////////////////////////////////////////////////////////////
/////////////////////////////////////////////////////////////////////////
1:     Map<Integer,MapredOutput> output = new HashMap<>();
/////////////////////////////////////////////////////////////////////////
1:     List<Node> trees = new ArrayList<>();
author:pferrel
-------------------------------------------------------------------------------
commit:b988c49
author:frankscholten
-------------------------------------------------------------------------------
commit:1a42d85
author:Abdel Hakim Deneche
-------------------------------------------------------------------------------
commit:d6aba1a
/////////////////////////////////////////////////////////////////////////
0: import com.google.common.collect.Lists;
0: import com.google.common.collect.Maps;
/////////////////////////////////////////////////////////////////////////
1: import org.apache.mahout.common.Pair;
1: import org.apache.mahout.common.iterator.sequencefile.SequenceFileIterable;
0: import java.io.IOException;
0: import java.util.List;
0: import java.util.Map;
commit:e9cc323
/////////////////////////////////////////////////////////////////////////
1:   protected void configureJob(Job job) throws IOException {
commit:ac83cf3
/////////////////////////////////////////////////////////////////////////
/////////////////////////////////////////////////////////////////////////
0: import com.google.common.collect.Lists;
0: import com.google.common.collect.Maps;
1: 
/////////////////////////////////////////////////////////////////////////
0:   protected void configureJob(Job job, int nbTrees) throws IOException {
/////////////////////////////////////////////////////////////////////////
1:   protected DecisionForest parseOutput(Job job) throws IOException {
/////////////////////////////////////////////////////////////////////////
1:     return processOutput(output);
1:    * Process the output, extracting the trees
1:   private static DecisionForest processOutput(Map<Integer,MapredOutput> output) {
commit:a8f9f88
/////////////////////////////////////////////////////////////////////////
1: /**
1:  * Licensed to the Apache Software Foundation (ASF) under one or more
1:  * contributor license agreements.  See the NOTICE file distributed with
1:  * this work for additional information regarding copyright ownership.
1:  * The ASF licenses this file to You under the Apache License, Version 2.0
1:  * (the "License"); you may not use this file except in compliance with
1:  * the License.  You may obtain a copy of the License at
1:  *
1:  *     http://www.apache.org/licenses/LICENSE-2.0
1:  *
1:  * Unless required by applicable law or agreed to in writing, software
1:  * distributed under the License is distributed on an "AS IS" BASIS,
1:  * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
1:  * See the License for the specific language governing permissions and
1:  * limitations under the License.
1:  */
1: 
0: package org.apache.mahout.df.mapreduce.inmem;
1: 
0: import java.io.IOException;
0: import java.util.ArrayList;
0: import java.util.HashMap;
0: import java.util.List;
0: import java.util.Map;
1: 
1: import org.apache.hadoop.conf.Configuration;
1: import org.apache.hadoop.filecache.DistributedCache;
1: import org.apache.hadoop.fs.FileSystem;
1: import org.apache.hadoop.fs.Path;
1: import org.apache.hadoop.io.IntWritable;
0: import org.apache.hadoop.io.SequenceFile.Reader;
1: import org.apache.hadoop.mapreduce.Job;
1: import org.apache.hadoop.mapreduce.lib.output.FileOutputFormat;
1: import org.apache.hadoop.mapreduce.lib.output.SequenceFileOutputFormat;
0: import org.apache.mahout.df.DFUtils;
0: import org.apache.mahout.df.DecisionForest;
0: import org.apache.mahout.df.builder.TreeBuilder;
0: import org.apache.mahout.df.callback.PredictionCallback;
0: import org.apache.mahout.df.mapreduce.Builder;
0: import org.apache.mahout.df.mapreduce.MapredOutput;
0: import org.apache.mahout.df.node.Node;
1: 
1: /**
0:  * MapReduce implementation where each mapper loads a full copy of the data
0:  * in-memory. The forest trees are splitted across all the mappers
1:  */
1: public class InMemBuilder extends Builder {
1: 
0:   public InMemBuilder(TreeBuilder treeBuilder, Path dataPath, Path datasetPath,
0:       Long seed, Configuration conf) {
1:     super(treeBuilder, dataPath, datasetPath, seed, conf);
1:   }
1: 
1:   public InMemBuilder(TreeBuilder treeBuilder, Path dataPath, Path datasetPath) {
1:     this(treeBuilder, dataPath, datasetPath, null, new Configuration());
1:   }
1: 
1:   @Override
0:   protected void configureJob(Job job, int nbTrees, boolean oobEstimate)
0:       throws IOException {
1:     Configuration conf = job.getConfiguration();
1:     
1:     job.setJarByClass(InMemBuilder.class);
1:     
1:     FileOutputFormat.setOutputPath(job, getOutputPath(conf));
1: 
1:     // put the data in the DistributedCache
0:     DistributedCache.addCacheFile(dataPath.toUri(), conf);
1: 
1:     job.setOutputKeyClass(IntWritable.class);
1:     job.setOutputValueClass(MapredOutput.class);
1: 
1:     job.setMapperClass(InMemMapper.class);
1:     job.setNumReduceTasks(0); // no reducers
1: 
1:     job.setInputFormatClass(InMemInputFormat.class);
1:     job.setOutputFormatClass(SequenceFileOutputFormat.class);
1: 
1:   }
1: 
1:   @Override
0:   protected DecisionForest parseOutput(Job job, PredictionCallback callback)
0:       throws IOException {
1:     Configuration conf = job.getConfiguration();
1:     
0:     Map<Integer, MapredOutput> output = new HashMap<Integer, MapredOutput>();
1: 
1:     Path outputPath = getOutputPath(conf);
1:     FileSystem fs = outputPath.getFileSystem(conf);
1: 
1:     Path[] outfiles = DFUtils.listOutputFiles(fs, outputPath);
1: 
1:     // import the InMemOutputs
0:     IntWritable key = new IntWritable();
0:     MapredOutput value = new MapredOutput();
1: 
1:     for (Path path : outfiles) {
0:       Reader reader = new Reader(fs, path, conf);
1: 
0:       try {
0:         while (reader.next(key, value)) {
0:           output.put(key.get(), value.clone());
1:         }
0:       } finally {
0:         reader.close();
1:       }
1:     }
1: 
0:     return processOutput(output, callback);
1:   }
1: 
1:   /**
0:    * Process the output, extracting the trees and passing the predictions to the
0:    * callback
1:    * 
0:    * @param output
0:    * @param callback
0:    * @return
1:    */
0:   protected DecisionForest processOutput(Map<Integer, MapredOutput> output,
0:       PredictionCallback callback) {
0:     List<Node> trees = new ArrayList<Node>();
1: 
0:     for (Integer key : output.keySet()) {
0:       MapredOutput value = output.get(key);
1: 
1:       trees.add(value.getTree());
1: 
0:       if (callback != null) {
0:         int[] predictions = value.getPredictions();
0:         for (int index = 0; index < predictions.length; index++) {
0:           callback.prediction(key, index, predictions[index]);
1:         }
1:       }
1:     }
1: 
1:     return new DecisionForest(trees);
1:   }
1: }
author:Grant Ingersoll
-------------------------------------------------------------------------------
commit:52ce412
/////////////////////////////////////////////////////////////////////////
1: package org.apache.mahout.classifier.df.mapreduce.inmem;
/////////////////////////////////////////////////////////////////////////
1: import org.apache.mahout.classifier.df.DFUtils;
1: import org.apache.mahout.classifier.df.DecisionForest;
1: import org.apache.mahout.classifier.df.builder.TreeBuilder;
1: import org.apache.mahout.classifier.df.mapreduce.Builder;
1: import org.apache.mahout.classifier.df.mapreduce.MapredOutput;
1: import org.apache.mahout.classifier.df.node.Node;
author:Sebastian Schelter
-------------------------------------------------------------------------------
commit:74f849b
/////////////////////////////////////////////////////////////////////////
0: import com.google.common.collect.Lists;
0: import com.google.common.collect.Maps;
/////////////////////////////////////////////////////////////////////////
0:     Map<Integer,MapredOutput> output = Maps.newHashMap();
/////////////////////////////////////////////////////////////////////////
0:     List<Node> trees = Lists.newArrayList();
author:Sean R. Owen
-------------------------------------------------------------------------------
commit:a13b4b7
/////////////////////////////////////////////////////////////////////////
0: import org.apache.mahout.common.Pair;
0: import org.apache.mahout.common.iterator.sequencefile.SequenceFileIterable;
/////////////////////////////////////////////////////////////////////////
1:       for (Pair<IntWritable,MapredOutput> record : new SequenceFileIterable<IntWritable,MapredOutput>(path, conf)) {
1:         output.put(record.getFirst().get(), record.getSecond());
commit:515bac4
/////////////////////////////////////////////////////////////////////////
commit:210fac3
commit:fc74924
/////////////////////////////////////////////////////////////////////////
1:     DistributedCache.addCacheFile(getDataPath().toUri(), conf);
commit:8547de7
/////////////////////////////////////////////////////////////////////////
0:   private static DecisionForest processOutput(Map<Integer, MapredOutput> output,
0:     for (Map.Entry<Integer, MapredOutput> entry : output.entrySet()) {
1:       MapredOutput value = entry.getValue();
0:           callback.prediction(entry.getKey(), index, predictions[index]);
author:Robin Anil
-------------------------------------------------------------------------------
commit:ad11134
/////////////////////////////////////////////////////////////////////////
1:  * MapReduce implementation where each mapper loads a full copy of the data in-memory. The forest trees are
1:  * splitted across all the mappers
0:   
1:   public InMemBuilder(TreeBuilder treeBuilder, Path dataPath, Path datasetPath, Long seed, Configuration conf) {
0:   
0:   
0:   protected void configureJob(Job job, int nbTrees, boolean oobEstimate) throws IOException {
0:     
0:     
0:     
0:     
0:     
0:   
0:   protected DecisionForest parseOutput(Job job, PredictionCallback callback) throws IOException {
0:     Map<Integer,MapredOutput> output = new HashMap<Integer,MapredOutput>();
0:     
0:     
0:     
0:     
0:       
/////////////////////////////////////////////////////////////////////////
0:     
0:     return InMemBuilder.processOutput(output, callback);
0:   
0:    * Process the output, extracting the trees and passing the predictions to the callback
0:   private static DecisionForest processOutput(Map<Integer,MapredOutput> output, PredictionCallback callback) {
0:     
1:     for (Map.Entry<Integer,MapredOutput> entry : output.entrySet()) {
0:       
0:       
/////////////////////////////////////////////////////////////////////////
0:     
============================================================================