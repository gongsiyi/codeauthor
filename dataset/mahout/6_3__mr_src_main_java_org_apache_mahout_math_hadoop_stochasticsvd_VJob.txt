2:151de0d: /**
1:151de0d:  * Licensed to the Apache Software Foundation (ASF) under one or more
1:151de0d:  * contributor license agreements.  See the NOTICE file distributed with
1:151de0d:  * this work for additional information regarding copyright ownership.
1:151de0d:  * The ASF licenses this file to You under the Apache License, Version 2.0
1:151de0d:  * (the "License"); you may not use this file except in compliance with
1:151de0d:  * the License.  You may obtain a copy of the License at
3:151de0d:  *
1:151de0d:  *     http://www.apache.org/licenses/LICENSE-2.0
1:151de0d:  *
1:151de0d:  * Unless required by applicable law or agreed to in writing, software
1:151de0d:  * distributed under the License is distributed on an "AS IS" BASIS,
1:151de0d:  * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
1:151de0d:  * See the License for the specific language governing permissions and
1:151de0d:  * limitations under the License.
2:151de0d:  */
10:151de0d: 
1:151de0d: package org.apache.mahout.math.hadoop.stochasticsvd;
1:151de0d: 
1:151de0d: import java.io.IOException;
1:151de0d: 
1:151de0d: import org.apache.hadoop.conf.Configuration;
1:151de0d: import org.apache.hadoop.fs.FileSystem;
1:151de0d: import org.apache.hadoop.fs.Path;
1:151de0d: import org.apache.hadoop.io.IntWritable;
1:151de0d: import org.apache.hadoop.io.SequenceFile.CompressionType;
1:151de0d: import org.apache.hadoop.io.compress.DefaultCodec;
1:151de0d: import org.apache.hadoop.mapreduce.Job;
1:151de0d: import org.apache.hadoop.mapreduce.Mapper;
1:151de0d: import org.apache.hadoop.mapreduce.lib.input.FileInputFormat;
1:151de0d: import org.apache.hadoop.mapreduce.lib.input.SequenceFileInputFormat;
1:151de0d: import org.apache.hadoop.mapreduce.lib.output.FileOutputFormat;
1:151de0d: import org.apache.hadoop.mapreduce.lib.output.SequenceFileOutputFormat;
1:151de0d: import org.apache.mahout.math.DenseMatrix;
1:151de0d: import org.apache.mahout.math.DenseVector;
1:151de0d: import org.apache.mahout.math.Matrix;
1:151de0d: import org.apache.mahout.math.Vector;
1:151de0d: import org.apache.mahout.math.VectorWritable;
1:175701c: import org.apache.mahout.math.function.Functions;
1:175701c: import org.apache.mahout.math.function.PlusMult;
1:151de0d: 
1:151de0d: public class VJob {
1:151de0d:   private static final String OUTPUT_V = "v";
1:151de0d:   private static final String PROP_UHAT_PATH = "ssvd.uhat.path";
1:151de0d:   private static final String PROP_SIGMA_PATH = "ssvd.sigma.path";
1:478cad9:   private static final String PROP_OUTPUT_SCALING = "ssvd.v.output.scaling";
1:151de0d:   private static final String PROP_K = "ssvd.k";
1:175701c:   public static final String PROP_SQ_PATH = "ssvdpca.sq.path";
1:175701c:   public static final String PROP_XI_PATH = "ssvdpca.xi.path";
1:151de0d: 
1:151de0d:   private Job job;
1:151de0d: 
1:175701c:   public static final class VMapper extends
1:175701c:       Mapper<IntWritable, VectorWritable, IntWritable, VectorWritable> {
1:175701c: 
1:175701c:     private Matrix uHat;
1:175701c:     private Vector vRow;
1:175701c:     private Vector sValues;
1:175701c:     private VectorWritable vRowWritable;
1:175701c:     private int kp;
1:175701c:     private int k;
1:175701c:     /*
1:175701c:      * xi and s_q are PCA-related corrections, per MAHOUT-817
1:175701c:      */
1:335a993:     private Vector xi;
1:335a993:     private Vector sq;
1:335a993:     private final PlusMult plusMult = new PlusMult(0);
1:175701c: 
1:175701c:     @Override
1:175701c:     protected void map(IntWritable key, VectorWritable value, Context context)
1:175701c:       throws IOException, InterruptedException {
1:175701c:       Vector bCol = value.get();
1:175701c:       /*
1:175701c:        * MAHOUT-817: PCA correction for B': b_{col=i} -= s_q * xi_{i}
1:175701c:        */
1:175701c:       if (xi != null) {
1:175701c:         /*
1:175701c:          * code defensively against shortened xi which may be externally
1:175701c:          * supplied
1:175701c:          */
1:175701c:         int btIndex = key.get();
1:175701c:         double xii = xi.size() > btIndex ? xi.getQuick(btIndex) : 0.0;
1:175701c:         plusMult.setMultiplicator(-xii);
1:175701c:         bCol.assign(sq, plusMult);
1:175701c:       }
1:175701c: 
1:175701c:       for (int i = 0; i < k; i++) {
1:175701c:         vRow.setQuick(i, bCol.dot(uHat.viewColumn(i)) / sValues.getQuick(i));
1:175701c:       }
1:175701c:       context.write(key, vRowWritable);
1:175701c:     }
1:175701c: 
1:175701c:     @Override
1:175701c:     protected void setup(Context context) throws IOException,
1:175701c:       InterruptedException {
1:175701c:       super.setup(context);
1:175701c: 
1:175701c:       Configuration conf = context.getConfiguration();
1:175701c:       FileSystem fs = FileSystem.get(conf);
1:175701c:       Path uHatPath = new Path(conf.get(PROP_UHAT_PATH));
1:175701c: 
1:175701c:       Path sigmaPath = new Path(conf.get(PROP_SIGMA_PATH));
1:175701c: 
1:b717cfc:       uHat = SSVDHelper.drmLoadAsDense(fs, uHatPath, conf);
1:175701c:       // since uHat is (k+p) x (k+p)
1:175701c:       kp = uHat.columnSize();
1:175701c:       k = context.getConfiguration().getInt(PROP_K, kp);
1:175701c:       vRow = new DenseVector(k);
1:175701c:       vRowWritable = new VectorWritable(vRow);
1:175701c: 
1:175701c:       sValues = SSVDHelper.loadVector(sigmaPath, conf);
1:478cad9:       SSVDSolver.OutputScalingEnum outputScaling =
1:478cad9:         SSVDSolver.OutputScalingEnum.valueOf(context.getConfiguration()
1:478cad9:                                                     .get(PROP_OUTPUT_SCALING));
1:478cad9:       switch (outputScaling) {
1:478cad9:         case SIGMA:
1:478cad9:           sValues.assign(1.0);
1:478cad9:           break;
1:478cad9:         case HALFSIGMA:
1:478cad9:           sValues = SSVDHelper.loadVector(sigmaPath, context.getConfiguration());
1:175701c:           sValues.assign(Functions.SQRT);
1:478cad9:           break;
1:4841efb:         default:
1:175701c:       }
1:175701c: 
1:175701c:       /*
1:175701c:        * PCA -related corrections (MAHOUT-817)
1:175701c:        */
1:175701c:       String xiPathStr = conf.get(PROP_XI_PATH);
1:175701c:       if (xiPathStr != null) {
1:175701c:         xi = SSVDHelper.loadAndSumUpVectors(new Path(xiPathStr), conf);
1:175701c:         sq =
1:175701c:           SSVDHelper.loadAndSumUpVectors(new Path(conf.get(PROP_SQ_PATH)), conf);
1:175701c:       }
1:175701c: 
1:175701c:     }
1:175701c: 
1:175701c:   }
1:175701c: 
1:175701c:   /**
1:175701c:    * 
1:175701c:    * @param conf
1:175701c:    * @param inputPathBt
1:175701c:    * @param xiPath
1:175701c:    *          PCA row mean (MAHOUT-817, to fix B')
1:175701c:    * @param sqPath
1:175701c:    *          sq (MAHOUT-817, to fix B')
1:175701c:    * @param inputUHatPath
1:175701c:    * @param inputSigmaPath
1:175701c:    * @param outputPath
1:175701c:    * @param k
1:175701c:    * @param numReduceTasks
1:478cad9:    * @param outputScaling output scaling: apply Sigma, or Sigma^0.5, or none
1:175701c:    * @throws ClassNotFoundException
1:175701c:    * @throws InterruptedException
1:175701c:    * @throws IOException
1:175701c:    */
1:175701c:   public void run(Configuration conf,
1:175701c:                   Path inputPathBt,
1:175701c:                   Path xiPath,
1:175701c:                   Path sqPath,
1:175701c: 
1:175701c:                   Path inputUHatPath,
1:175701c:                   Path inputSigmaPath,
1:175701c: 
1:175701c:                   Path outputPath,
1:175701c:                   int k,
1:175701c:                   int numReduceTasks,
1:6d16230:                   SSVDSolver.OutputScalingEnum outputScaling) throws ClassNotFoundException,
1:175701c:     InterruptedException, IOException {
1:151de0d: 
1:151de0d:     job = new Job(conf);
1:151de0d:     job.setJobName("V-job");
1:151de0d:     job.setJarByClass(VJob.class);
1:151de0d: 
1:151de0d:     job.setInputFormatClass(SequenceFileInputFormat.class);
1:151de0d:     job.setOutputFormatClass(SequenceFileOutputFormat.class);
1:151de0d:     FileInputFormat.setInputPaths(job, inputPathBt);
1:151de0d:     FileOutputFormat.setOutputPath(job, outputPath);
1:151de0d: 
1:151de0d:     // Warn: tight hadoop integration here:
1:151de0d:     job.getConfiguration().set("mapreduce.output.basename", OUTPUT_V);
1:b16c260:     FileOutputFormat.setCompressOutput(job, true);
1:b16c260:     FileOutputFormat.setOutputCompressorClass(job, DefaultCodec.class);
1:175701c:     SequenceFileOutputFormat.setOutputCompressionType(job,
1:175701c:                                                       CompressionType.BLOCK);
1:151de0d: 
1:151de0d:     job.setMapOutputKeyClass(IntWritable.class);
1:151de0d:     job.setMapOutputValueClass(VectorWritable.class);
1:151de0d: 
1:151de0d:     job.setOutputKeyClass(IntWritable.class);
1:151de0d:     job.setOutputValueClass(VectorWritable.class);
1:151de0d: 
1:151de0d:     job.setMapperClass(VMapper.class);
1:151de0d: 
1:151de0d:     job.getConfiguration().set(PROP_UHAT_PATH, inputUHatPath.toString());
1:151de0d:     job.getConfiguration().set(PROP_SIGMA_PATH, inputSigmaPath.toString());
1:478cad9:     job.getConfiguration().set(PROP_OUTPUT_SCALING, outputScaling.name());
1:151de0d:     job.getConfiguration().setInt(PROP_K, k);
1:151de0d:     job.setNumReduceTasks(0);
1:175701c: 
1:175701c:     /*
1:175701c:      * PCA-related options, MAHOUT-817
1:175701c:      */
1:175701c:     if (xiPath != null) {
1:175701c:       job.getConfiguration().set(PROP_XI_PATH, xiPath.toString());
1:175701c:       job.getConfiguration().set(PROP_SQ_PATH, sqPath.toString());
1:175701c:     }
1:175701c: 
1:151de0d:     job.submit();
1:151de0d: 
3:151de0d:   }
1:151de0d: 
1:151de0d:   public void waitForCompletion() throws IOException, ClassNotFoundException,
1:175701c:     InterruptedException {
1:151de0d:     job.waitForCompletion(false);
1:151de0d: 
1:b16c260:     if (!job.isSuccessful()) {
1:151de0d:       throw new IOException("V job unsuccessful.");
1:151de0d:     }
1:151de0d: 
1:151de0d:   }
1:151de0d: 
1:151de0d: }
============================================================================
author:pferrel
-------------------------------------------------------------------------------
commit:b988c49
author:frankscholten
-------------------------------------------------------------------------------
commit:1a42d85
author:Dmitriy Lyubimov
-------------------------------------------------------------------------------
commit:b717cfc
/////////////////////////////////////////////////////////////////////////
1:       uHat = SSVDHelper.drmLoadAsDense(fs, uHatPath, conf);
commit:478cad9
/////////////////////////////////////////////////////////////////////////
1:   private static final String PROP_OUTPUT_SCALING = "ssvd.v.output.scaling";
/////////////////////////////////////////////////////////////////////////
1:       SSVDSolver.OutputScalingEnum outputScaling =
1:         SSVDSolver.OutputScalingEnum.valueOf(context.getConfiguration()
1:                                                     .get(PROP_OUTPUT_SCALING));
1:       switch (outputScaling) {
1:       case SIGMA:
1:         sValues.assign(1.0);
1:         break;
1:       case HALFSIGMA:
1:         sValues = SSVDHelper.loadVector(sigmaPath, context.getConfiguration());
1:         break;
/////////////////////////////////////////////////////////////////////////
1:    * @param outputScaling output scaling: apply Sigma, or Sigma^0.5, or none
/////////////////////////////////////////////////////////////////////////
0:                   SSVDSolver.OutputScalingEnum outputScaling ) throws ClassNotFoundException,
/////////////////////////////////////////////////////////////////////////
1:     job.getConfiguration().set(PROP_OUTPUT_SCALING, outputScaling.name());
commit:175701c
/////////////////////////////////////////////////////////////////////////
1: import org.apache.mahout.math.function.Functions;
1: import org.apache.mahout.math.function.PlusMult;
/////////////////////////////////////////////////////////////////////////
1:   public static final String PROP_SQ_PATH = "ssvdpca.sq.path";
1:   public static final String PROP_XI_PATH = "ssvdpca.xi.path";
1:   public static final class VMapper extends
1:       Mapper<IntWritable, VectorWritable, IntWritable, VectorWritable> {
1: 
1:     private Matrix uHat;
1:     private Vector vRow;
1:     private Vector sValues;
1:     private VectorWritable vRowWritable;
1:     private int kp;
1:     private int k;
1:     /*
1:      * xi and s_q are PCA-related corrections, per MAHOUT-817
1:      */
0:     protected Vector xi;
0:     protected Vector sq;
0:     protected PlusMult plusMult = new PlusMult(0);
1: 
1:     @Override
1:     protected void map(IntWritable key, VectorWritable value, Context context)
1:       throws IOException, InterruptedException {
1:       Vector bCol = value.get();
1:       /*
1:        * MAHOUT-817: PCA correction for B': b_{col=i} -= s_q * xi_{i}
1:        */
1:       if (xi != null) {
1:         /*
1:          * code defensively against shortened xi which may be externally
1:          * supplied
1:          */
1:         int btIndex = key.get();
1:         double xii = xi.size() > btIndex ? xi.getQuick(btIndex) : 0.0;
1:         plusMult.setMultiplicator(-xii);
1:         bCol.assign(sq, plusMult);
1:       }
1: 
1:       for (int i = 0; i < k; i++) {
1:         vRow.setQuick(i, bCol.dot(uHat.viewColumn(i)) / sValues.getQuick(i));
1:       }
1:       context.write(key, vRowWritable);
1:     }
1: 
1:     @Override
1:     protected void setup(Context context) throws IOException,
1:       InterruptedException {
1:       super.setup(context);
1: 
1:       Configuration conf = context.getConfiguration();
1:       FileSystem fs = FileSystem.get(conf);
1:       Path uHatPath = new Path(conf.get(PROP_UHAT_PATH));
1: 
1:       Path sigmaPath = new Path(conf.get(PROP_SIGMA_PATH));
1: 
0:       uHat =
0:         new DenseMatrix(SSVDHelper.loadDistributedRowMatrix(fs, uHatPath, conf));
1:       // since uHat is (k+p) x (k+p)
1:       kp = uHat.columnSize();
1:       k = context.getConfiguration().getInt(PROP_K, kp);
1:       vRow = new DenseVector(k);
1:       vRowWritable = new VectorWritable(vRow);
1: 
1:       sValues = SSVDHelper.loadVector(sigmaPath, conf);
0:       if (conf.get(PROP_V_HALFSIGMA) != null) {
1:         sValues.assign(Functions.SQRT);
1:       }
1: 
1:       /*
1:        * PCA -related corrections (MAHOUT-817)
1:        */
1:       String xiPathStr = conf.get(PROP_XI_PATH);
1:       if (xiPathStr != null) {
1:         xi = SSVDHelper.loadAndSumUpVectors(new Path(xiPathStr), conf);
1:         sq =
1:           SSVDHelper.loadAndSumUpVectors(new Path(conf.get(PROP_SQ_PATH)), conf);
1:       }
1: 
1:     }
1: 
1:   }
1: 
1:   /**
1:    * 
1:    * @param conf
1:    * @param inputPathBt
1:    * @param xiPath
1:    *          PCA row mean (MAHOUT-817, to fix B')
1:    * @param sqPath
1:    *          sq (MAHOUT-817, to fix B')
1:    * @param inputUHatPath
1:    * @param inputSigmaPath
1:    * @param outputPath
1:    * @param k
1:    * @param numReduceTasks
0:    * @param vHalfSigma
1:    * @throws ClassNotFoundException
1:    * @throws InterruptedException
1:    * @throws IOException
1:    */
1:   public void run(Configuration conf,
1:                   Path inputPathBt,
1:                   Path xiPath,
1:                   Path sqPath,
1: 
1:                   Path inputUHatPath,
1:                   Path inputSigmaPath,
1: 
1:                   Path outputPath,
1:                   int k,
1:                   int numReduceTasks,
0:                   boolean vHalfSigma) throws ClassNotFoundException,
1:     InterruptedException, IOException {
/////////////////////////////////////////////////////////////////////////
1:     SequenceFileOutputFormat.setOutputCompressionType(job,
1:                                                       CompressionType.BLOCK);
/////////////////////////////////////////////////////////////////////////
1: 
1:     /*
1:      * PCA-related options, MAHOUT-817
1:      */
1:     if (xiPath != null) {
1:       job.getConfiguration().set(PROP_XI_PATH, xiPath.toString());
1:       job.getConfiguration().set(PROP_SQ_PATH, sqPath.toString());
1:     }
1: 
1:     InterruptedException {
/////////////////////////////////////////////////////////////////////////
commit:5a2250c
/////////////////////////////////////////////////////////////////////////
commit:151de0d
/////////////////////////////////////////////////////////////////////////
1: /**
1:  * Licensed to the Apache Software Foundation (ASF) under one or more
1:  * contributor license agreements.  See the NOTICE file distributed with
1:  * this work for additional information regarding copyright ownership.
1:  * The ASF licenses this file to You under the Apache License, Version 2.0
1:  * (the "License"); you may not use this file except in compliance with
1:  * the License.  You may obtain a copy of the License at
1:  *
1:  *     http://www.apache.org/licenses/LICENSE-2.0
1:  *
1:  * Unless required by applicable law or agreed to in writing, software
1:  * distributed under the License is distributed on an "AS IS" BASIS,
1:  * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
1:  * See the License for the specific language governing permissions and
1:  * limitations under the License.
1:  */
1: 
1: package org.apache.mahout.math.hadoop.stochasticsvd;
1: 
1: import java.io.IOException;
1: 
1: import org.apache.hadoop.conf.Configuration;
1: import org.apache.hadoop.fs.FileSystem;
1: import org.apache.hadoop.fs.Path;
1: import org.apache.hadoop.io.IntWritable;
1: import org.apache.hadoop.io.SequenceFile.CompressionType;
1: import org.apache.hadoop.io.compress.DefaultCodec;
1: import org.apache.hadoop.mapreduce.Job;
1: import org.apache.hadoop.mapreduce.Mapper;
1: import org.apache.hadoop.mapreduce.lib.input.FileInputFormat;
1: import org.apache.hadoop.mapreduce.lib.input.SequenceFileInputFormat;
1: import org.apache.hadoop.mapreduce.lib.output.FileOutputFormat;
1: import org.apache.hadoop.mapreduce.lib.output.SequenceFileOutputFormat;
1: import org.apache.mahout.math.DenseMatrix;
1: import org.apache.mahout.math.DenseVector;
1: import org.apache.mahout.math.Matrix;
1: import org.apache.mahout.math.Vector;
1: import org.apache.mahout.math.VectorWritable;
1: 
1: /**
0:  * Computes U=Q*Uhat of SSVD
1:  * 
1:  * 
1:  */
1: public class VJob {
1:   private static final String OUTPUT_V = "v";
1:   private static final String PROP_UHAT_PATH = "ssvd.uhat.path";
1:   private static final String PROP_SIGMA_PATH = "ssvd.sigma.path";
0:   private static final String PROP_V_HALFSIGMA = "ssvd.v.halfsigma";
1:   private static final String PROP_K = "ssvd.k";
1: 
1:   private Job job;
1: 
0:   public void start(Configuration conf, Path inputPathBt, Path inputUHatPath,
0:       Path inputSigmaPath, Path outputPath, int k, int numReduceTasks,
0:       boolean vHalfSigma) throws ClassNotFoundException, InterruptedException,
0:       IOException {
1: 
1:     job = new Job(conf);
1:     job.setJobName("V-job");
1:     job.setJarByClass(VJob.class);
1: 
1:     job.setInputFormatClass(SequenceFileInputFormat.class);
1:     job.setOutputFormatClass(SequenceFileOutputFormat.class);
1:     FileInputFormat.setInputPaths(job, inputPathBt);
1:     FileOutputFormat.setOutputPath(job, outputPath);
1: 
1:     // Warn: tight hadoop integration here:
1:     job.getConfiguration().set("mapreduce.output.basename", OUTPUT_V);
0:     SequenceFileOutputFormat.setCompressOutput(job, true);
0:     SequenceFileOutputFormat
0:         .setOutputCompressorClass(job, DefaultCodec.class);
0:     SequenceFileOutputFormat.setOutputCompressionType(job,
0:         CompressionType.BLOCK);
1: 
1:     job.setMapOutputKeyClass(IntWritable.class);
1:     job.setMapOutputValueClass(VectorWritable.class);
1: 
1:     job.setOutputKeyClass(IntWritable.class);
1:     job.setOutputValueClass(VectorWritable.class);
1: 
1:     job.setMapperClass(VMapper.class);
1: 
1:     job.getConfiguration().set(PROP_UHAT_PATH, inputUHatPath.toString());
1:     job.getConfiguration().set(PROP_SIGMA_PATH, inputSigmaPath.toString());
0:     if (vHalfSigma)
0:       job.getConfiguration().set(PROP_V_HALFSIGMA, "y");
1:     job.getConfiguration().setInt(PROP_K, k);
1:     job.setNumReduceTasks(0);
1:     job.submit();
1: 
1:   }
1: 
1:   public void waitForCompletion() throws IOException, ClassNotFoundException,
0:       InterruptedException {
1:     job.waitForCompletion(false);
1: 
0:     if (!job.isSuccessful())
1:       throw new IOException("V job unsuccessful.");
1: 
1:   }
1: 
0:   public static final class VMapper extends
0:       Mapper<IntWritable, VectorWritable, IntWritable, VectorWritable> {
1: 
0:     private Matrix uHat;
0:     private DenseVector vRow;
0:     private DenseVector sValues;
0:     private VectorWritable vRowWritable;
0:     private int kp;
0:     private int k;
1: 
0:     @Override
0:     protected void map(IntWritable key, VectorWritable value, Context context)
0:       throws IOException, InterruptedException {
0:       Vector qRow = value.get();
0:       for (int i = 0; i < k; i++)
0:         vRow.setQuick(i,
0:             qRow.dot(uHat.getColumn(i)) / sValues.getQuick(i));
0:       context.write(key, vRowWritable); // U inherits original A row labels.
1:     }
1: 
0:     @Override
0:     protected void setup(Context context) throws IOException,
0:         InterruptedException {
0:       super.setup(context);
0:       FileSystem fs = FileSystem.get(context.getConfiguration());
0:       Path uHatPath = new Path(context.getConfiguration().get(PROP_UHAT_PATH));
1: 
0:       Path sigmaPath = new Path(context.getConfiguration().get(PROP_SIGMA_PATH));
1: 
0:       uHat = new DenseMatrix(SSVDSolver.loadDistributedRowMatrix(fs,
0:           uHatPath, context.getConfiguration()));
0:       // since uHat is (k+p) x (k+p)
0:       kp = uHat.columnSize();
0:       k = context.getConfiguration().getInt(PROP_K, kp);
0:       vRow = new DenseVector(k);
0:       vRowWritable = new VectorWritable(vRow);
1: 
0:       sValues = new DenseVector(SSVDSolver.loadDistributedRowMatrix(fs,
0:           sigmaPath, context.getConfiguration())[0], true);
0:       if (context.getConfiguration().get(PROP_V_HALFSIGMA) != null)
0:         for (int i = 0; i < k; i++)
0:           sValues.setQuick(i, Math.sqrt(sValues.getQuick(i)));
1: 
1:     }
1: 
1:   }
1: 
1: }
author:Sean R. Owen
-------------------------------------------------------------------------------
commit:335a993
/////////////////////////////////////////////////////////////////////////
1:     private Vector xi;
1:     private Vector sq;
1:     private final PlusMult plusMult = new PlusMult(0);
commit:229aeff
/////////////////////////////////////////////////////////////////////////
0:     Vector xi;
0:     Vector sq;
0:     PlusMult plusMult = new PlusMult(0);
commit:1de8cec
/////////////////////////////////////////////////////////////////////////
0:       FileSystem fs = FileSystem.get(uHatPath.toUri(), context.getConfiguration());
commit:b16c260
/////////////////////////////////////////////////////////////////////////
1:     FileOutputFormat.setCompressOutput(job, true);
1:     FileOutputFormat.setOutputCompressorClass(job, DefaultCodec.class);
0:     SequenceFileOutputFormat.setOutputCompressionType(job, CompressionType.BLOCK);
/////////////////////////////////////////////////////////////////////////
0:     if (vHalfSigma) {
0:     }
/////////////////////////////////////////////////////////////////////////
1:     if (!job.isSuccessful()) {
0:     }
/////////////////////////////////////////////////////////////////////////
0:       for (int i = 0; i < k; i++) {
0:                       qRow.dot(uHat.getColumn(i)) / sValues.getQuick(i));
0:       }
/////////////////////////////////////////////////////////////////////////
0:       if (context.getConfiguration().get(PROP_V_HALFSIGMA) != null) {
0:         for (int i = 0; i < k; i++) {
0:         }
0:       }
author:Sebastian Schelter
-------------------------------------------------------------------------------
commit:6d16230
/////////////////////////////////////////////////////////////////////////
1:                   SSVDSolver.OutputScalingEnum outputScaling) throws ClassNotFoundException,
commit:4841efb
/////////////////////////////////////////////////////////////////////////
0:         case SIGMA:
0:           sValues.assign(1.0);
0:           break;
0:         case HALFSIGMA:
0:           sValues = SSVDHelper.loadVector(sigmaPath, context.getConfiguration());
0:           sValues.assign(Functions.SQRT);
0:           break;
1:         default:
author:Ted Dunning
-------------------------------------------------------------------------------
commit:528ffcd
/////////////////////////////////////////////////////////////////////////
0:                       qRow.dot(uHat.viewColumn(i)) / sValues.getQuick(i));
============================================================================