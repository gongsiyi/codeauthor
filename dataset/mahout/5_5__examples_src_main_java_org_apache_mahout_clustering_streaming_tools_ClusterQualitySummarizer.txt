1:1cdd095: /*
1:1cdd095:  * Licensed to the Apache Software Foundation (ASF) under one or more
1:1cdd095:  * contributor license agreements.  See the NOTICE file distributed with
1:1cdd095:  * this work for additional information regarding copyright ownership.
1:1cdd095:  * The ASF licenses this file to You under the Apache License, Version 2.0
1:1cdd095:  * (the "License"); you may not use this file except in compliance with
1:1cdd095:  * the License.  You may obtain a copy of the License at
1:1cdd095:  *
1:1cdd095:  *     http://www.apache.org/licenses/LICENSE-2.0
1:1cdd095:  *
1:1cdd095:  * Unless required by applicable law or agreed to in writing, software
1:1cdd095:  * distributed under the License is distributed on an "AS IS" BASIS,
1:1cdd095:  * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
1:1cdd095:  * See the License for the specific language governing permissions and
1:1cdd095:  * limitations under the License.
1:1cdd095:  */
1:1cdd095: 
1:fab9d37: package org.apache.mahout.clustering.streaming.tools;
4:fab9d37: 
1:fab9d37: import java.io.FileOutputStream;
1:fab9d37: import java.io.IOException;
1:fab9d37: import java.io.PrintWriter;
1:fab9d37: import java.util.List;
1:fab9d37: 
1:fab9d37: import com.google.common.collect.Iterables;
1:fab9d37: import com.google.common.collect.Lists;
1:24c4921: import com.google.common.io.Closeables;
1:fab9d37: import org.apache.commons.cli2.CommandLine;
1:fab9d37: import org.apache.commons.cli2.Group;
1:fab9d37: import org.apache.commons.cli2.Option;
1:fab9d37: import org.apache.commons.cli2.builder.ArgumentBuilder;
1:fab9d37: import org.apache.commons.cli2.builder.DefaultOptionBuilder;
1:fab9d37: import org.apache.commons.cli2.builder.GroupBuilder;
1:fab9d37: import org.apache.commons.cli2.commandline.Parser;
1:fab9d37: import org.apache.commons.cli2.util.HelpFormatter;
1:fab9d37: import org.apache.hadoop.conf.Configuration;
1:fab9d37: import org.apache.hadoop.fs.Path;
1:fab9d37: import org.apache.mahout.clustering.iterator.ClusterWritable;
1:fab9d37: import org.apache.mahout.clustering.ClusteringUtils;
1:fab9d37: import org.apache.mahout.clustering.streaming.mapreduce.CentroidWritable;
1:24c4921: import org.apache.mahout.common.AbstractJob;
1:fab9d37: import org.apache.mahout.common.distance.DistanceMeasure;
1:fab9d37: import org.apache.mahout.common.distance.SquaredEuclideanDistanceMeasure;
1:fab9d37: import org.apache.mahout.common.iterator.sequencefile.PathType;
1:fab9d37: import org.apache.mahout.common.iterator.sequencefile.SequenceFileDirValueIterable;
1:fab9d37: import org.apache.mahout.math.Centroid;
1:fab9d37: import org.apache.mahout.math.Vector;
1:fab9d37: import org.apache.mahout.math.VectorWritable;
1:fab9d37: import org.apache.mahout.math.stats.OnlineSummarizer;
1:fab9d37: 
1:24c4921: public class ClusterQualitySummarizer extends AbstractJob {
1:fab9d37:   private String outputFile;
1:fab9d37: 
1:fab9d37:   private PrintWriter fileOut;
1:fab9d37: 
1:fab9d37:   private String trainFile;
1:fab9d37:   private String testFile;
1:fab9d37:   private String centroidFile;
1:fab9d37:   private String centroidCompareFile;
1:fab9d37:   private boolean mahoutKMeansFormat;
1:fab9d37:   private boolean mahoutKMeansFormatCompare;
1:fab9d37: 
1:fab9d37:   private DistanceMeasure distanceMeasure = new SquaredEuclideanDistanceMeasure();
1:fab9d37: 
1:fab9d37:   public void printSummaries(List<OnlineSummarizer> summarizers, String type) {
1:fab9d37:     printSummaries(summarizers, type, fileOut);
3:fab9d37:   }
1:fab9d37: 
1:fab9d37:   public static void printSummaries(List<OnlineSummarizer> summarizers, String type, PrintWriter fileOut) {
1:fab9d37:     double maxDistance = 0;
1:fab9d37:     for (int i = 0; i < summarizers.size(); ++i) {
1:fab9d37:       OnlineSummarizer summarizer = summarizers.get(i);
1:24c4921:       if (summarizer.getCount() > 1) {
1:3186b37:         maxDistance = Math.max(maxDistance, summarizer.getMax());
1:3186b37:         System.out.printf("Average distance in cluster %d [%d]: %f\n", i, summarizer.getCount(), summarizer.getMean());
1:3186b37:         // If there is just one point in the cluster, quartiles cannot be estimated. We'll just assume all the quartiles
1:3186b37:         // equal the only value.
1:3186b37:         if (fileOut != null) {
1:3186b37:           fileOut.printf("%d,%f,%f,%f,%f,%f,%f,%f,%d,%s\n", i, summarizer.getMean(),
1:3186b37:               summarizer.getSD(),
1:3186b37:               summarizer.getQuartile(0),
1:24c4921:               summarizer.getQuartile(1),
1:24c4921:               summarizer.getQuartile(2),
1:24c4921:               summarizer.getQuartile(3),
1:3186b37:               summarizer.getQuartile(4), summarizer.getCount(), type);
1:3186b37:         }
1:3186b37:       } else {
1:24c4921:         System.out.printf("Cluster %d is has %d data point. Need atleast 2 data points in a cluster for" +
1:24c4921:             " OnlineSummarizer.\n", i, summarizer.getCount());
1:fab9d37:       }
1:fab9d37:     }
1:fab9d37:     System.out.printf("Num clusters: %d; maxDistance: %f\n", summarizers.size(), maxDistance);
1:fab9d37:   }
1:fab9d37: 
1:24c4921:   public int run(String[] args) throws IOException {
1:fab9d37:     if (!parseArgs(args)) {
1:24c4921:       return -1;
1:fab9d37:     }
1:fab9d37: 
1:fab9d37:     Configuration conf = new Configuration();
1:fab9d37:     try {
1:fab9d37:       fileOut = new PrintWriter(new FileOutputStream(outputFile));
1:fab9d37:       fileOut.printf("cluster,distance.mean,distance.sd,distance.q0,distance.q1,distance.q2,distance.q3,"
1:fab9d37:           + "distance.q4,count,is.train\n");
1:fab9d37: 
1:fab9d37:       // Reading in the centroids (both pairs, if they exist).
1:fab9d37:       List<Centroid> centroids;
1:fab9d37:       List<Centroid> centroidsCompare = null;
1:fab9d37:       if (mahoutKMeansFormat) {
1:fab9d37:         SequenceFileDirValueIterable<ClusterWritable> clusterIterable =
1:87c15be:             new SequenceFileDirValueIterable<>(new Path(centroidFile), PathType.GLOB, conf);
1:fab9d37:         centroids = Lists.newArrayList(IOUtils.getCentroidsFromClusterWritableIterable(clusterIterable));
1:fab9d37:       } else {
1:fab9d37:         SequenceFileDirValueIterable<CentroidWritable> centroidIterable =
1:87c15be:             new SequenceFileDirValueIterable<>(new Path(centroidFile), PathType.GLOB, conf);
1:fab9d37:         centroids = Lists.newArrayList(IOUtils.getCentroidsFromCentroidWritableIterable(centroidIterable));
1:fab9d37:       }
1:fab9d37: 
1:fab9d37:       if (centroidCompareFile != null) {
1:fab9d37:         if (mahoutKMeansFormatCompare) {
1:fab9d37:           SequenceFileDirValueIterable<ClusterWritable> clusterCompareIterable =
1:87c15be:               new SequenceFileDirValueIterable<>(new Path(centroidCompareFile), PathType.GLOB, conf);
1:fab9d37:           centroidsCompare = Lists.newArrayList(
1:fab9d37:               IOUtils.getCentroidsFromClusterWritableIterable(clusterCompareIterable));
1:fab9d37:         } else {
1:fab9d37:           SequenceFileDirValueIterable<CentroidWritable> centroidCompareIterable =
1:87c15be:               new SequenceFileDirValueIterable<>(new Path(centroidCompareFile), PathType.GLOB, conf);
1:fab9d37:           centroidsCompare = Lists.newArrayList(
1:fab9d37:               IOUtils.getCentroidsFromCentroidWritableIterable(centroidCompareIterable));
1:fab9d37:         }
1:fab9d37:       }
1:fab9d37: 
1:fab9d37:       // Reading in the "training" set.
1:fab9d37:       SequenceFileDirValueIterable<VectorWritable> trainIterable =
1:87c15be:           new SequenceFileDirValueIterable<>(new Path(trainFile), PathType.GLOB, conf);
1:fab9d37:       Iterable<Vector> trainDatapoints = IOUtils.getVectorsFromVectorWritableIterable(trainIterable);
1:fab9d37:       Iterable<Vector> datapoints = trainDatapoints;
1:fab9d37: 
1:fab9d37:       printSummaries(ClusteringUtils.summarizeClusterDistances(trainDatapoints, centroids,
1:fab9d37:           new SquaredEuclideanDistanceMeasure()), "train");
1:fab9d37: 
1:fab9d37:       // Also adding in the "test" set.
1:fab9d37:       if (testFile != null) {
1:fab9d37:         SequenceFileDirValueIterable<VectorWritable> testIterable =
1:87c15be:             new SequenceFileDirValueIterable<>(new Path(testFile), PathType.GLOB, conf);
1:fab9d37:         Iterable<Vector> testDatapoints = IOUtils.getVectorsFromVectorWritableIterable(testIterable);
1:fab9d37: 
1:fab9d37:         printSummaries(ClusteringUtils.summarizeClusterDistances(testDatapoints, centroids,
1:fab9d37:             new SquaredEuclideanDistanceMeasure()), "test");
1:fab9d37: 
1:fab9d37:         datapoints = Iterables.concat(trainDatapoints, testDatapoints);
1:fab9d37:       }
1:fab9d37: 
1:fab9d37:       // At this point, all train/test CSVs have been written. We now compute quality metrics.
1:fab9d37:       List<OnlineSummarizer> summaries =
1:fab9d37:           ClusteringUtils.summarizeClusterDistances(datapoints, centroids, distanceMeasure);
1:fab9d37:       List<OnlineSummarizer> compareSummaries = null;
1:fab9d37:       if (centroidsCompare != null) {
1:58cc1ae:         compareSummaries = ClusteringUtils.summarizeClusterDistances(datapoints, centroidsCompare, distanceMeasure);
1:fab9d37:       }
1:fab9d37:       System.out.printf("[Dunn Index] First: %f", ClusteringUtils.dunnIndex(centroids, distanceMeasure, summaries));
1:fab9d37:       if (compareSummaries != null) {
1:24c4921:         System.out.printf(" Second: %f\n", ClusteringUtils.dunnIndex(centroidsCompare, distanceMeasure, compareSummaries));
1:fab9d37:       } else {
1:fab9d37:         System.out.printf("\n");
1:fab9d37:       }
1:fab9d37:       System.out.printf("[Davies-Bouldin Index] First: %f",
1:fab9d37:           ClusteringUtils.daviesBouldinIndex(centroids, distanceMeasure, summaries));
1:fab9d37:       if (compareSummaries != null) {
2:fab9d37:         System.out.printf(" Second: %f\n",
1:fab9d37:           ClusteringUtils.daviesBouldinIndex(centroidsCompare, distanceMeasure, compareSummaries));
1:fab9d37:       } else {
1:fab9d37:         System.out.printf("\n");
1:fab9d37:       }
1:fab9d37:     } catch (IOException e) {
1:fab9d37:       System.out.println(e.getMessage());
1:24c4921:     } finally {
1:24c4921:       Closeables.close(fileOut, false);
1:fab9d37:     }
1:24c4921:     return 0;
1:fab9d37:   }
1:fab9d37: 
1:fab9d37:   private boolean parseArgs(String[] args) {
1:fab9d37:     DefaultOptionBuilder builder = new DefaultOptionBuilder();
1:fab9d37: 
1:fab9d37:     Option help = builder.withLongName("help").withDescription("print this list").create();
1:fab9d37: 
1:fab9d37:     ArgumentBuilder argumentBuilder = new ArgumentBuilder();
1:fab9d37:     Option inputFileOption = builder.withLongName("input")
1:fab9d37:         .withShortName("i")
1:fab9d37:         .withRequired(true)
1:fab9d37:         .withArgument(argumentBuilder.withName("input").withMaximum(1).create())
1:fab9d37:         .withDescription("where to get seq files with the vectors (training set)")
1:fab9d37:         .create();
1:fab9d37: 
1:fab9d37:     Option testInputFileOption = builder.withLongName("testInput")
1:fab9d37:         .withShortName("itest")
1:fab9d37:         .withArgument(argumentBuilder.withName("testInput").withMaximum(1).create())
1:fab9d37:         .withDescription("where to get seq files with the vectors (test set)")
1:fab9d37:         .create();
1:fab9d37: 
1:fab9d37:     Option centroidsFileOption = builder.withLongName("centroids")
1:fab9d37:         .withShortName("c")
1:fab9d37:         .withRequired(true)
1:fab9d37:         .withArgument(argumentBuilder.withName("centroids").withMaximum(1).create())
1:fab9d37:         .withDescription("where to get seq files with the centroids (from Mahout KMeans or StreamingKMeansDriver)")
1:fab9d37:         .create();
1:fab9d37: 
1:fab9d37:     Option centroidsCompareFileOption = builder.withLongName("centroidsCompare")
1:fab9d37:         .withShortName("cc")
1:fab9d37:         .withRequired(false)
1:fab9d37:         .withArgument(argumentBuilder.withName("centroidsCompare").withMaximum(1).create())
1:58cc1ae:         .withDescription("where to get seq files with the second set of centroids (from Mahout KMeans or "
1:58cc1ae:             + "StreamingKMeansDriver)")
1:fab9d37:         .create();
1:fab9d37: 
1:fab9d37:     Option outputFileOption = builder.withLongName("output")
1:fab9d37:         .withShortName("o")
1:fab9d37:         .withRequired(true)
1:fab9d37:         .withArgument(argumentBuilder.withName("output").withMaximum(1).create())
1:fab9d37:         .withDescription("where to dump the CSV file with the results")
1:fab9d37:         .create();
1:fab9d37: 
1:fab9d37:     Option mahoutKMeansFormatOption = builder.withLongName("mahoutkmeansformat")
1:fab9d37:         .withShortName("mkm")
1:fab9d37:         .withDescription("if set, read files as (IntWritable, ClusterWritable) pairs")
1:fab9d37:         .withArgument(argumentBuilder.withName("numpoints").withMaximum(1).create())
1:fab9d37:         .create();
1:fab9d37: 
1:fab9d37:     Option mahoutKMeansCompareFormatOption = builder.withLongName("mahoutkmeansformatCompare")
1:fab9d37:         .withShortName("mkmc")
1:fab9d37:         .withDescription("if set, read files as (IntWritable, ClusterWritable) pairs")
1:fab9d37:         .withArgument(argumentBuilder.withName("numpoints").withMaximum(1).create())
1:fab9d37:         .create();
1:fab9d37: 
1:fab9d37:     Group normalArgs = new GroupBuilder()
1:fab9d37:         .withOption(help)
1:fab9d37:         .withOption(inputFileOption)
1:fab9d37:         .withOption(testInputFileOption)
1:fab9d37:         .withOption(outputFileOption)
1:fab9d37:         .withOption(centroidsFileOption)
1:fab9d37:         .withOption(centroidsCompareFileOption)
1:fab9d37:         .withOption(mahoutKMeansFormatOption)
1:fab9d37:         .withOption(mahoutKMeansCompareFormatOption)
1:fab9d37:         .create();
1:fab9d37: 
1:fab9d37:     Parser parser = new Parser();
1:fab9d37:     parser.setHelpOption(help);
1:fab9d37:     parser.setHelpTrigger("--help");
1:fab9d37:     parser.setGroup(normalArgs);
1:fab9d37:     parser.setHelpFormatter(new HelpFormatter(" ", "", " ", 150));
1:fab9d37: 
1:fab9d37:     CommandLine cmdLine = parser.parseAndHelp(args);
1:fab9d37:     if (cmdLine == null) {
1:fab9d37:       return false;
1:fab9d37:     }
1:fab9d37: 
1:fab9d37:     trainFile = (String) cmdLine.getValue(inputFileOption);
1:fab9d37:     if (cmdLine.hasOption(testInputFileOption)) {
1:fab9d37:       testFile = (String) cmdLine.getValue(testInputFileOption);
1:fab9d37:     }
1:fab9d37:     centroidFile = (String) cmdLine.getValue(centroidsFileOption);
1:fab9d37:     if (cmdLine.hasOption(centroidsCompareFileOption)) {
1:fab9d37:       centroidCompareFile = (String) cmdLine.getValue(centroidsCompareFileOption);
1:fab9d37:     }
1:fab9d37:     outputFile = (String) cmdLine.getValue(outputFileOption);
1:fab9d37:     if (cmdLine.hasOption(mahoutKMeansFormatOption)) {
1:fab9d37:       mahoutKMeansFormat = true;
1:fab9d37:     }
1:fab9d37:     if (cmdLine.hasOption(mahoutKMeansCompareFormatOption)) {
1:fab9d37:       mahoutKMeansFormatCompare = true;
1:fab9d37:     }
1:fab9d37:     return true;
1:fab9d37:   }
1:fab9d37: 
1:24c4921:   public static void main(String[] args) throws IOException {
1:fab9d37:     new ClusterQualitySummarizer().run(args);
1:fab9d37:   }
1:fab9d37: }
============================================================================
author:Suneel Marthi
-------------------------------------------------------------------------------
commit:4ef9d31
/////////////////////////////////////////////////////////////////////////
commit:87c15be
/////////////////////////////////////////////////////////////////////////
1:             new SequenceFileDirValueIterable<>(new Path(centroidFile), PathType.GLOB, conf);
1:             new SequenceFileDirValueIterable<>(new Path(centroidFile), PathType.GLOB, conf);
1:               new SequenceFileDirValueIterable<>(new Path(centroidCompareFile), PathType.GLOB, conf);
1:               new SequenceFileDirValueIterable<>(new Path(centroidCompareFile), PathType.GLOB, conf);
/////////////////////////////////////////////////////////////////////////
1:           new SequenceFileDirValueIterable<>(new Path(trainFile), PathType.GLOB, conf);
/////////////////////////////////////////////////////////////////////////
1:             new SequenceFileDirValueIterable<>(new Path(testFile), PathType.GLOB, conf);
author:smarthi
-------------------------------------------------------------------------------
commit:24c4921
/////////////////////////////////////////////////////////////////////////
1: import com.google.common.io.Closeables;
/////////////////////////////////////////////////////////////////////////
1: import org.apache.mahout.common.AbstractJob;
/////////////////////////////////////////////////////////////////////////
1: public class ClusterQualitySummarizer extends AbstractJob {
/////////////////////////////////////////////////////////////////////////
1:       if (summarizer.getCount() > 1) {
1:               summarizer.getQuartile(1),
1:               summarizer.getQuartile(2),
1:               summarizer.getQuartile(3),
1:         System.out.printf("Cluster %d is has %d data point. Need atleast 2 data points in a cluster for" +
1:             " OnlineSummarizer.\n", i, summarizer.getCount());
1:   public int run(String[] args) throws IOException {
1:       return -1;
0: //      Configuration.dumpConfiguration(conf, new OutputStreamWriter(System.out));
/////////////////////////////////////////////////////////////////////////
1:         System.out.printf(" Second: %f\n", ClusteringUtils.dunnIndex(centroidsCompare, distanceMeasure, compareSummaries));
/////////////////////////////////////////////////////////////////////////
1:     } finally {
1:       Closeables.close(fileOut, false);
1:     return 0;
/////////////////////////////////////////////////////////////////////////
1:   public static void main(String[] args) throws IOException {
commit:3186b37
/////////////////////////////////////////////////////////////////////////
0:       if (summarizer.getCount() != 0) {
1:         maxDistance = Math.max(maxDistance, summarizer.getMax());
1:         System.out.printf("Average distance in cluster %d [%d]: %f\n", i, summarizer.getCount(), summarizer.getMean());
1:         // If there is just one point in the cluster, quartiles cannot be estimated. We'll just assume all the quartiles
1:         // equal the only value.
0:         boolean moreThanOne = summarizer.getCount() > 1;
1:         if (fileOut != null) {
1:           fileOut.printf("%d,%f,%f,%f,%f,%f,%f,%f,%d,%s\n", i, summarizer.getMean(),
1:               summarizer.getSD(),
1:               summarizer.getQuartile(0),
0:               moreThanOne ? summarizer.getQuartile(1) : summarizer.getQuartile(0),
0:               moreThanOne ? summarizer.getQuartile(2) : summarizer.getQuartile(0),
0:               moreThanOne ? summarizer.getQuartile(3) : summarizer.getQuartile(0),
1:               summarizer.getQuartile(4), summarizer.getCount(), type);
1:         }
1:       } else {
author:Sean R. Owen
-------------------------------------------------------------------------------
commit:1cdd095
/////////////////////////////////////////////////////////////////////////
1: /*
1:  * Licensed to the Apache Software Foundation (ASF) under one or more
1:  * contributor license agreements.  See the NOTICE file distributed with
1:  * this work for additional information regarding copyright ownership.
1:  * The ASF licenses this file to You under the Apache License, Version 2.0
1:  * (the "License"); you may not use this file except in compliance with
1:  * the License.  You may obtain a copy of the License at
1:  *
1:  *     http://www.apache.org/licenses/LICENSE-2.0
1:  *
1:  * Unless required by applicable law or agreed to in writing, software
1:  * distributed under the License is distributed on an "AS IS" BASIS,
1:  * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
1:  * See the License for the specific language governing permissions and
1:  * limitations under the License.
1:  */
1: 
author:Sebastian Schelter
-------------------------------------------------------------------------------
commit:58cc1ae
/////////////////////////////////////////////////////////////////////////
1:         compareSummaries = ClusteringUtils.summarizeClusterDistances(datapoints, centroidsCompare, distanceMeasure);
/////////////////////////////////////////////////////////////////////////
1:         .withDescription("where to get seq files with the second set of centroids (from Mahout KMeans or "
1:             + "StreamingKMeansDriver)")
author:dfilimon
-------------------------------------------------------------------------------
commit:fab9d37
/////////////////////////////////////////////////////////////////////////
1: package org.apache.mahout.clustering.streaming.tools;
1: 
1: import java.io.FileOutputStream;
1: import java.io.IOException;
0: import java.io.OutputStreamWriter;
1: import java.io.PrintWriter;
1: import java.util.List;
1: 
1: import com.google.common.collect.Iterables;
1: import com.google.common.collect.Lists;
1: import org.apache.commons.cli2.CommandLine;
1: import org.apache.commons.cli2.Group;
1: import org.apache.commons.cli2.Option;
1: import org.apache.commons.cli2.builder.ArgumentBuilder;
1: import org.apache.commons.cli2.builder.DefaultOptionBuilder;
1: import org.apache.commons.cli2.builder.GroupBuilder;
1: import org.apache.commons.cli2.commandline.Parser;
1: import org.apache.commons.cli2.util.HelpFormatter;
1: import org.apache.hadoop.conf.Configuration;
1: import org.apache.hadoop.fs.Path;
1: import org.apache.mahout.clustering.iterator.ClusterWritable;
1: import org.apache.mahout.clustering.ClusteringUtils;
1: import org.apache.mahout.clustering.streaming.mapreduce.CentroidWritable;
1: import org.apache.mahout.common.distance.DistanceMeasure;
1: import org.apache.mahout.common.distance.SquaredEuclideanDistanceMeasure;
1: import org.apache.mahout.common.iterator.sequencefile.PathType;
1: import org.apache.mahout.common.iterator.sequencefile.SequenceFileDirValueIterable;
1: import org.apache.mahout.math.Centroid;
1: import org.apache.mahout.math.Vector;
1: import org.apache.mahout.math.VectorWritable;
1: import org.apache.mahout.math.stats.OnlineSummarizer;
1: 
0: public class ClusterQualitySummarizer {
1:   private String outputFile;
1: 
1: 
1:   private PrintWriter fileOut;
1: 
1:   private String trainFile;
1:   private String testFile;
1:   private String centroidFile;
1:   private String centroidCompareFile;
1:   private boolean mahoutKMeansFormat;
1:   private boolean mahoutKMeansFormatCompare;
1: 
1:   private DistanceMeasure distanceMeasure = new SquaredEuclideanDistanceMeasure();
1: 
1:   public void printSummaries(List<OnlineSummarizer> summarizers, String type) {
1:     printSummaries(summarizers, type, fileOut);
1:   }
1: 
1:   public static void printSummaries(List<OnlineSummarizer> summarizers, String type, PrintWriter fileOut) {
1:     double maxDistance = 0;
1:     for (int i = 0; i < summarizers.size(); ++i) {
1:       OnlineSummarizer summarizer = summarizers.get(i);
0:       if (summarizer.getCount() == 0) {
0:         System.out.printf("Cluster %d is empty\n", i);
0:         continue;
1:       }
0:       maxDistance = Math.max(maxDistance, summarizer.getMax());
0:       System.out.printf("Average distance in cluster %d [%d]: %f\n", i, summarizer.getCount(), summarizer.getMean());
0:       // If there is just one point in the cluster, quartiles cannot be estimated. We'll just assume all the quartiles
0:       // equal the only value.
0:       boolean moreThanOne = summarizer.getCount() > 1;
0:       if (fileOut != null) {
0:         fileOut.printf("%d,%f,%f,%f,%f,%f,%f,%f,%d,%s\n", i, summarizer.getMean(),
0:             summarizer.getSD(),
0:             summarizer.getQuartile(0),
0:             moreThanOne ? summarizer.getQuartile(1) : summarizer.getQuartile(0),
0:             moreThanOne ? summarizer.getQuartile(2) : summarizer.getQuartile(0),
0:             moreThanOne ? summarizer.getQuartile(3) : summarizer.getQuartile(0),
0:             summarizer.getQuartile(4), summarizer.getCount(), type);
1:       }
1:     }
1:     System.out.printf("Num clusters: %d; maxDistance: %f\n", summarizers.size(), maxDistance);
1:   }
1: 
0:   public void run(String[] args) {
1:     if (!parseArgs(args)) {
0:       return;
1:     }
1: 
1:     Configuration conf = new Configuration();
1:     try {
0:       Configuration.dumpConfiguration(conf, new OutputStreamWriter(System.out));
1: 
1:       fileOut = new PrintWriter(new FileOutputStream(outputFile));
1:       fileOut.printf("cluster,distance.mean,distance.sd,distance.q0,distance.q1,distance.q2,distance.q3,"
1:           + "distance.q4,count,is.train\n");
1: 
1:       // Reading in the centroids (both pairs, if they exist).
1:       List<Centroid> centroids;
1:       List<Centroid> centroidsCompare = null;
1:       if (mahoutKMeansFormat) {
1:         SequenceFileDirValueIterable<ClusterWritable> clusterIterable =
0:             new SequenceFileDirValueIterable<ClusterWritable>(new Path(centroidFile), PathType.GLOB, conf);
1:         centroids = Lists.newArrayList(IOUtils.getCentroidsFromClusterWritableIterable(clusterIterable));
1:       } else {
1:         SequenceFileDirValueIterable<CentroidWritable> centroidIterable =
0:             new SequenceFileDirValueIterable<CentroidWritable>(new Path(centroidFile), PathType.GLOB, conf);
1:         centroids = Lists.newArrayList(IOUtils.getCentroidsFromCentroidWritableIterable(centroidIterable));
1:       }
1: 
1:       if (centroidCompareFile != null) {
1:         if (mahoutKMeansFormatCompare) {
1:           SequenceFileDirValueIterable<ClusterWritable> clusterCompareIterable =
0:               new SequenceFileDirValueIterable<ClusterWritable>(new Path(centroidCompareFile), PathType.GLOB, conf);
1:           centroidsCompare = Lists.newArrayList(
1:               IOUtils.getCentroidsFromClusterWritableIterable(clusterCompareIterable));
1:         } else {
1:           SequenceFileDirValueIterable<CentroidWritable> centroidCompareIterable =
0:               new SequenceFileDirValueIterable<CentroidWritable>(new Path(centroidCompareFile), PathType.GLOB, conf);
1:           centroidsCompare = Lists.newArrayList(
1:               IOUtils.getCentroidsFromCentroidWritableIterable(centroidCompareIterable));
1:         }
1:       }
1: 
1:       // Reading in the "training" set.
1:       SequenceFileDirValueIterable<VectorWritable> trainIterable =
0:           new SequenceFileDirValueIterable<VectorWritable>(new Path(trainFile), PathType.GLOB, conf);
1:       Iterable<Vector> trainDatapoints = IOUtils.getVectorsFromVectorWritableIterable(trainIterable);
1:       Iterable<Vector> datapoints = trainDatapoints;
1: 
1:       printSummaries(ClusteringUtils.summarizeClusterDistances(trainDatapoints, centroids,
1:           new SquaredEuclideanDistanceMeasure()), "train");
1: 
1:       // Also adding in the "test" set.
1:       if (testFile != null) {
1:         SequenceFileDirValueIterable<VectorWritable> testIterable =
0:             new SequenceFileDirValueIterable<VectorWritable>(new Path(testFile), PathType.GLOB, conf);
1:         Iterable<Vector> testDatapoints = IOUtils.getVectorsFromVectorWritableIterable(testIterable);
1: 
1:         printSummaries(ClusteringUtils.summarizeClusterDistances(testDatapoints, centroids,
1:             new SquaredEuclideanDistanceMeasure()), "test");
1: 
1:         datapoints = Iterables.concat(trainDatapoints, testDatapoints);
1:       }
1: 
1:       // At this point, all train/test CSVs have been written. We now compute quality metrics.
1:       List<OnlineSummarizer> summaries =
1:           ClusteringUtils.summarizeClusterDistances(datapoints, centroids, distanceMeasure);
1:       List<OnlineSummarizer> compareSummaries = null;
1:       if (centroidsCompare != null) {
0:             compareSummaries =
0:                 ClusteringUtils.summarizeClusterDistances(datapoints, centroidsCompare, distanceMeasure);
1:       }
1:       System.out.printf("[Dunn Index] First: %f", ClusteringUtils.dunnIndex(centroids, distanceMeasure, summaries));
1:       if (compareSummaries != null) {
1:         System.out.printf(" Second: %f\n",
0:             ClusteringUtils.dunnIndex(centroidsCompare, distanceMeasure, compareSummaries));
1:       } else {
1:         System.out.printf("\n");
1:       }
1:       System.out.printf("[Davies-Bouldin Index] First: %f",
1:           ClusteringUtils.daviesBouldinIndex(centroids, distanceMeasure, summaries));
1:       if (compareSummaries != null) {
1:         System.out.printf(" Second: %f\n",
1:           ClusteringUtils.daviesBouldinIndex(centroidsCompare, distanceMeasure, compareSummaries));
1:       } else {
1:         System.out.printf("\n");
1:       }
1: 
0:       if (outputFile != null) {
0:         fileOut.close();
1:       }
1:     } catch (IOException e) {
1:       System.out.println(e.getMessage());
1:     }
1:   }
1: 
1:   private boolean parseArgs(String[] args) {
1:     DefaultOptionBuilder builder = new DefaultOptionBuilder();
1: 
1:     Option help = builder.withLongName("help").withDescription("print this list").create();
1: 
1:     ArgumentBuilder argumentBuilder = new ArgumentBuilder();
1:     Option inputFileOption = builder.withLongName("input")
1:         .withShortName("i")
1:         .withRequired(true)
1:         .withArgument(argumentBuilder.withName("input").withMaximum(1).create())
1:         .withDescription("where to get seq files with the vectors (training set)")
1:         .create();
1: 
1:     Option testInputFileOption = builder.withLongName("testInput")
1:         .withShortName("itest")
1:         .withArgument(argumentBuilder.withName("testInput").withMaximum(1).create())
1:         .withDescription("where to get seq files with the vectors (test set)")
1:         .create();
1: 
1:     Option centroidsFileOption = builder.withLongName("centroids")
1:         .withShortName("c")
1:         .withRequired(true)
1:         .withArgument(argumentBuilder.withName("centroids").withMaximum(1).create())
1:         .withDescription("where to get seq files with the centroids (from Mahout KMeans or StreamingKMeansDriver)")
1:         .create();
1: 
1:     Option centroidsCompareFileOption = builder.withLongName("centroidsCompare")
1:         .withShortName("cc")
1:         .withRequired(false)
1:         .withArgument(argumentBuilder.withName("centroidsCompare").withMaximum(1).create())
0:         .withDescription("where to get seq files with the second set of centroids (from Mahout KMeans or " +
0:             "StreamingKMeansDriver)")
1:         .create();
1: 
1:     Option outputFileOption = builder.withLongName("output")
1:         .withShortName("o")
1:         .withRequired(true)
1:         .withArgument(argumentBuilder.withName("output").withMaximum(1).create())
1:         .withDescription("where to dump the CSV file with the results")
1:         .create();
1: 
1:     Option mahoutKMeansFormatOption = builder.withLongName("mahoutkmeansformat")
1:         .withShortName("mkm")
1:         .withDescription("if set, read files as (IntWritable, ClusterWritable) pairs")
1:         .withArgument(argumentBuilder.withName("numpoints").withMaximum(1).create())
1:         .create();
1: 
1:     Option mahoutKMeansCompareFormatOption = builder.withLongName("mahoutkmeansformatCompare")
1:         .withShortName("mkmc")
1:         .withDescription("if set, read files as (IntWritable, ClusterWritable) pairs")
1:         .withArgument(argumentBuilder.withName("numpoints").withMaximum(1).create())
1:         .create();
1: 
1:     Group normalArgs = new GroupBuilder()
1:         .withOption(help)
1:         .withOption(inputFileOption)
1:         .withOption(testInputFileOption)
1:         .withOption(outputFileOption)
1:         .withOption(centroidsFileOption)
1:         .withOption(centroidsCompareFileOption)
1:         .withOption(mahoutKMeansFormatOption)
1:         .withOption(mahoutKMeansCompareFormatOption)
1:         .create();
1: 
1:     Parser parser = new Parser();
1:     parser.setHelpOption(help);
1:     parser.setHelpTrigger("--help");
1:     parser.setGroup(normalArgs);
1:     parser.setHelpFormatter(new HelpFormatter(" ", "", " ", 150));
1: 
1:     CommandLine cmdLine = parser.parseAndHelp(args);
1:     if (cmdLine == null) {
1:       return false;
1:     }
1: 
1:     trainFile = (String) cmdLine.getValue(inputFileOption);
1:     if (cmdLine.hasOption(testInputFileOption)) {
1:       testFile = (String) cmdLine.getValue(testInputFileOption);
1:     }
1:     centroidFile = (String) cmdLine.getValue(centroidsFileOption);
1:     if (cmdLine.hasOption(centroidsCompareFileOption)) {
1:       centroidCompareFile = (String) cmdLine.getValue(centroidsCompareFileOption);
1:     }
1:     outputFile = (String) cmdLine.getValue(outputFileOption);
1:     if (cmdLine.hasOption(mahoutKMeansFormatOption)) {
1:       mahoutKMeansFormat = true;
1:     }
1:     if (cmdLine.hasOption(mahoutKMeansCompareFormatOption)) {
1:       mahoutKMeansFormatCompare = true;
1:     }
1:     return true;
1:   }
1: 
0:   public static void main(String[] args) {
1:     new ClusterQualitySummarizer().run(args);
1:   }
1: }
============================================================================