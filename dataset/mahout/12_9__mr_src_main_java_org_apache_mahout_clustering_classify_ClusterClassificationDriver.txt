7:6a3f566: /**
1:6a3f566:  * Licensed to the Apache Software Foundation (ASF) under one or more
1:6a3f566:  * contributor license agreements.  See the NOTICE file distributed with
1:6a3f566:  * this work for additional information regarding copyright ownership.
1:6a3f566:  * The ASF licenses this file to You under the Apache License, Version 2.0
1:6a3f566:  * (the "License"); you may not use this file except in compliance with
1:6a3f566:  * the License.  You may obtain a copy of the License at
5:6a3f566:  *
1:6a3f566:  *     http://www.apache.org/licenses/LICENSE-2.0
1:6a3f566:  *
1:6a3f566:  * Unless required by applicable law or agreed to in writing, software
1:6a3f566:  * distributed under the License is distributed on an "AS IS" BASIS,
1:6a3f566:  * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
1:6a3f566:  * See the License for the specific language governing permissions and
1:6a3f566:  * limitations under the License.
7:6a3f566:  */
24:6a3f566: 
1:6a3f566: package org.apache.mahout.clustering.classify;
1:457db94: 
1:6a3f566: import java.io.IOException;
1:85f9ece: import java.util.ArrayList;
1:85f9ece: import java.util.HashMap;
1:6a3f566: import java.util.Iterator;
1:6a3f566: import java.util.List;
1:c20d04c: import java.util.Map;
1:6a3f566: 
1:6a3f566: import org.apache.hadoop.conf.Configuration;
1:6a3f566: import org.apache.hadoop.fs.FileStatus;
1:6a3f566: import org.apache.hadoop.fs.FileSystem;
1:6a3f566: import org.apache.hadoop.fs.Path;
1:6a3f566: import org.apache.hadoop.io.IntWritable;
1:6a3f566: import org.apache.hadoop.io.SequenceFile;
1:c20d04c: import org.apache.hadoop.io.Text;
1:6a3f566: import org.apache.hadoop.io.Writable;
1:6a3f566: import org.apache.hadoop.mapreduce.Job;
1:6a3f566: import org.apache.hadoop.mapreduce.lib.input.FileInputFormat;
1:6a3f566: import org.apache.hadoop.mapreduce.lib.input.SequenceFileInputFormat;
1:6a3f566: import org.apache.hadoop.mapreduce.lib.output.FileOutputFormat;
1:6a3f566: import org.apache.hadoop.mapreduce.lib.output.SequenceFileOutputFormat;
1:6a3f566: import org.apache.hadoop.util.ToolRunner;
1:6a3f566: import org.apache.mahout.clustering.Cluster;
1:51f58b8: import org.apache.mahout.clustering.iterator.ClusterWritable;
1:8d102ea: import org.apache.mahout.clustering.iterator.ClusteringPolicy;
1:4d4efb0: import org.apache.mahout.clustering.iterator.DistanceMeasureCluster;
1:6a3f566: import org.apache.mahout.common.AbstractJob;
1:e93f05d: import org.apache.mahout.common.Pair;
1:6a3f566: import org.apache.mahout.common.commandline.DefaultOptionCreator;
1:4d4efb0: import org.apache.mahout.common.distance.DistanceMeasure;
1:6a3f566: import org.apache.mahout.common.iterator.sequencefile.PathFilters;
1:6a3f566: import org.apache.mahout.common.iterator.sequencefile.PathType;
1:e93f05d: import org.apache.mahout.common.iterator.sequencefile.SequenceFileDirIterable;
1:6a3f566: import org.apache.mahout.common.iterator.sequencefile.SequenceFileDirValueIterator;
1:e93f05d: import org.apache.mahout.math.NamedVector;
1:6a3f566: import org.apache.mahout.math.Vector;
1:98f66f4: import org.apache.mahout.math.Vector.Element;
1:6a3f566: import org.apache.mahout.math.VectorWritable;
1:457db94: 
1:6a3f566: /**
1:457db94:  * Classifies the vectors into different clusters found by the clustering
1:457db94:  * algorithm.
1:6a3f566:  */
1:229aeff: public final class ClusterClassificationDriver extends AbstractJob {
1:457db94:   
1:457db94:   /**
1:457db94:    * CLI to run Cluster Classification Driver.
1:457db94:    */
1:457db94:   @Override
1:457db94:   public int run(String[] args) throws Exception {
1:457db94:     
1:457db94:     addInputOption();
1:457db94:     addOutputOption();
1:457db94:     addOption(DefaultOptionCreator.methodOption().create());
1:457db94:     addOption(DefaultOptionCreator.clustersInOption()
1:457db94:         .withDescription("The input centroids, as Vectors.  Must be a SequenceFile of Writable, Cluster/Canopy.")
1:457db94:         .create());
1:457db94:     
1:457db94:     if (parseArguments(args) == null) {
1:457db94:       return -1;
1:4ce12cc:     }
1:c20d04c:     
1:457db94:     Path input = getInputPath();
1:457db94:     Path output = getOutputPath();
1:4ce12cc:     
1:457db94:     if (getConf() == null) {
1:457db94:       setConf(new Configuration());
1:457db94:     }
1:457db94:     Path clustersIn = new Path(getOption(DefaultOptionCreator.CLUSTERS_IN_OPTION));
1:457db94:     boolean runSequential = getOption(DefaultOptionCreator.METHOD_OPTION).equalsIgnoreCase(
1:457db94:         DefaultOptionCreator.SEQUENTIAL_METHOD);
1:457db94:     
1:457db94:     double clusterClassificationThreshold = 0.0;
1:457db94:     if (hasOption(DefaultOptionCreator.OUTLIER_THRESHOLD)) {
1:457db94:       clusterClassificationThreshold = Double.parseDouble(getOption(DefaultOptionCreator.OUTLIER_THRESHOLD));
1:457db94:     }
1:457db94:     
1:4ce12cc:     run(getConf(), input, clustersIn, output, clusterClassificationThreshold, true, runSequential);
1:457db94:     
1:457db94:     return 0;
1:457db94:   }
1:457db94:   
1:457db94:   /**
1:457db94:    * Constructor to be used by the ToolRunner.
1:457db94:    */
1:229aeff:   private ClusterClassificationDriver() {
1:229aeff:   }
1:457db94:   
1:457db94:   public static void main(String[] args) throws Exception {
1:457db94:     ToolRunner.run(new Configuration(), new ClusterClassificationDriver(), args);
1:457db94:   }
1:457db94:   
1:457db94:   /**
1:457db94:    * Uses {@link ClusterClassifier} to classify input vectors into their
1:457db94:    * respective clusters.
1:457db94:    * 
1:457db94:    * @param input
1:457db94:    *          the input vectors
1:457db94:    * @param clusteringOutputPath
1:457db94:    *          the output path of clustering ( it reads clusters-*-final file
1:457db94:    *          from here )
1:457db94:    * @param output
1:457db94:    *          the location to store the classified vectors
2:457db94:    * @param clusterClassificationThreshold
1:457db94:    *          the threshold value of probability distribution function from 0.0
1:457db94:    *          to 1.0. Any vector with pdf less that this threshold will not be
1:457db94:    *          classified for the cluster.
1:457db94:    * @param runSequential
1:457db94:    *          Run the process sequentially or in a mapreduce way.
1:457db94:    * @throws IOException
1:457db94:    * @throws InterruptedException
1:457db94:    * @throws ClassNotFoundException
1:457db94:    */
1:c88c240:   public static void run(Configuration conf, Path input, Path clusteringOutputPath, Path output, Double clusterClassificationThreshold,
1:4ce12cc:       boolean emitMostLikely, boolean runSequential) throws IOException, InterruptedException, ClassNotFoundException {
1:457db94:     if (runSequential) {
1:ba81a93:       classifyClusterSeq(conf, input, clusteringOutputPath, output, clusterClassificationThreshold, emitMostLikely);
1:457db94:     } else {
1:ba81a93:       classifyClusterMR(conf, input, clusteringOutputPath, output, clusterClassificationThreshold, emitMostLikely);
1:457db94:     }
1:457db94:     
1:457db94:   }
1:457db94:   
1:bf2df6e:   private static void classifyClusterSeq(Configuration conf, Path input, Path clusters, Path output,
1:bf2df6e:       Double clusterClassificationThreshold, boolean emitMostLikely) throws IOException {
1:ba81a93:     List<Cluster> clusterModels = populateClusterModels(clusters, conf);
1:fd355fe:     ClusteringPolicy policy = ClusterClassifier.readPolicy(finalClustersPath(conf, clusters));
1:457db94:     ClusterClassifier clusterClassifier = new ClusterClassifier(clusterModels, policy);
1:bf2df6e:     selectCluster(input, clusterModels, clusterClassifier, output, clusterClassificationThreshold, emitMostLikely);
1:457db94:     
1:457db94:   }
1:457db94:   
1:457db94:   /**
1:457db94:    * Populates a list with clusters present in clusters-*-final directory.
1:457db94:    * 
1:457db94:    * @param clusterOutputPath
1:457db94:    *          The output path of the clustering.
1:ba81a93:    * @param conf
1:ba81a93:    *          The Hadoop Configuration
1:457db94:    * @return The list of clusters found by the clustering.
1:457db94:    * @throws IOException
1:457db94:    */
1:bf2df6e:   private static List<Cluster> populateClusterModels(Path clusterOutputPath, Configuration conf) throws IOException {
1:85f9ece:     List<Cluster> clusterModels = new ArrayList<>();
1:fd355fe:     Path finalClustersPath = finalClustersPath(conf, clusterOutputPath);
1:02ff22f:     Iterator<?> it = new SequenceFileDirValueIterator<>(finalClustersPath, PathType.LIST,
1:bf2df6e:         PathFilters.partFilter(), null, false, conf);
1:457db94:     while (it.hasNext()) {
1:bf2df6e:       ClusterWritable next = (ClusterWritable) it.next();
1:229aeff:       Cluster cluster = next.getValue();
1:590ffed:       cluster.configure(conf);
1:457db94:       clusterModels.add(cluster);
1:457db94:     }
1:457db94:     return clusterModels;
1:457db94:   }
1:457db94:   
1:fd355fe:   private static Path finalClustersPath(Configuration conf, Path clusterOutputPath) throws IOException {
1:6a3f566:     FileSystem fileSystem = clusterOutputPath.getFileSystem(conf);
1:457db94:     FileStatus[] clusterFiles = fileSystem.listStatus(clusterOutputPath, PathFilters.finalPartFilter());
1:229aeff:     return clusterFiles[0].getPath();
1:457db94:   }
1:457db94:   
1:457db94:   /**
1:457db94:    * Classifies the vector into its respective cluster.
1:457db94:    * 
1:457db94:    * @param input
1:457db94:    *          the path containing the input vector.
1:457db94:    * @param clusterModels
1:457db94:    *          the clusters
1:457db94:    * @param clusterClassifier
1:457db94:    *          used to classify the vectors into different clusters
1:457db94:    * @param output
1:457db94:    *          the path to store classified data
1:457db94:    * @param clusterClassificationThreshold
1:e93f05d:    *          the threshold value of probability distribution function from 0.0
1:e93f05d:    *          to 1.0. Any vector with pdf less that this threshold will not be
1:e93f05d:    *          classified for the cluster
1:98f66f4:    * @param emitMostLikely
1:e93f05d:    *          emit the vectors with the max pdf values per cluster
1:457db94:    * @throws IOException
1:457db94:    */
1:457db94:   private static void selectCluster(Path input, List<Cluster> clusterModels, ClusterClassifier clusterClassifier,
1:bf2df6e:       Path output, Double clusterClassificationThreshold, boolean emitMostLikely) throws IOException {
1:58cc1ae:     Configuration conf = new Configuration();
1:457db94:     SequenceFile.Writer writer = new SequenceFile.Writer(input.getFileSystem(conf), conf, new Path(output,
1:c20d04c:         "part-m-" + 0), IntWritable.class, WeightedPropertyVectorWritable.class);
1:e93f05d:     for (Pair<Writable, VectorWritable> vw : new SequenceFileDirIterable<Writable, VectorWritable>(input, PathType.LIST,
1:457db94:         PathFilters.logsCRCFilter(), conf)) {
1:08da368:       // Converting to NamedVectors to preserve the vectorId else its not obvious as to which point
1:08da368:       // belongs to which cluster - fix for MAHOUT-1410
1:ab6216f:       Class<? extends Writable> keyClass = vw.getFirst().getClass();
1:e93f05d:       Vector vector = vw.getSecond().get();
1:ab6216f:       if (!keyClass.equals(NamedVector.class)) {
1:ab6216f:         if (keyClass.equals(Text.class)) {
1:ab6216f:           vector = new NamedVector(vector, vw.getFirst().toString());
1:ab6216f:         } else if (keyClass.equals(IntWritable.class)) {
1:ab6216f:           vector = new NamedVector(vector, Integer.toString(((IntWritable) vw.getFirst()).get()));
1:e93f05d:         }
1:e93f05d:       }
1:e93f05d:       Vector pdfPerCluster = clusterClassifier.classify(vector);
1:457db94:       if (shouldClassify(pdfPerCluster, clusterClassificationThreshold)) {
1:e93f05d:         classifyAndWrite(clusterModels, clusterClassificationThreshold, emitMostLikely, writer, new VectorWritable(vector), pdfPerCluster);
1:457db94:       }
8:6a3f566:     }
1:457db94:     writer.close();
1:6a3f566:   }
1:6a3f566:   
1:bf2df6e:   private static void classifyAndWrite(List<Cluster> clusterModels, Double clusterClassificationThreshold,
1:bf2df6e:       boolean emitMostLikely, SequenceFile.Writer writer, VectorWritable vw, Vector pdfPerCluster) throws IOException {
1:85f9ece:     Map<Text, Text> props = new HashMap<>();
1:98f66f4:     if (emitMostLikely) {
1:457db94:       int maxValueIndex = pdfPerCluster.maxValueIndex();
1:e93f05d:       WeightedPropertyVectorWritable weightedPropertyVectorWritable =
1:e93f05d:           new WeightedPropertyVectorWritable(pdfPerCluster.maxValue(), vw.get(), props);
1:e93f05d:       write(clusterModels, writer, weightedPropertyVectorWritable, maxValueIndex);
1:6a3f566:     } else {
1:bf2df6e:       writeAllAboveThreshold(clusterModels, clusterClassificationThreshold, writer, vw, pdfPerCluster);
1:6a3f566:     }
1:6a3f566:   }
1:6a3f566:   
1:bf2df6e:   private static void writeAllAboveThreshold(List<Cluster> clusterModels, Double clusterClassificationThreshold,
1:bf2df6e:       SequenceFile.Writer writer, VectorWritable vw, Vector pdfPerCluster) throws IOException {
1:85f9ece:     Map<Text, Text> props = new HashMap<>();
1:dc62944:     for (Element pdf : pdfPerCluster.nonZeroes()) {
1:98f66f4:       if (pdf.get() >= clusterClassificationThreshold) {
1:e93f05d:         WeightedPropertyVectorWritable wvw = new WeightedPropertyVectorWritable(pdf.get(), vw.get(), props);
1:98f66f4:         int clusterIndex = pdf.index();
1:1371326:         write(clusterModels, writer, wvw, clusterIndex);
1:6a3f566:       }
1:6a3f566:     }
1:6a3f566:   }
1:6a3f566: 
1:e93f05d:   private static void write(List<Cluster> clusterModels, SequenceFile.Writer writer,
1:e93f05d:       WeightedPropertyVectorWritable weightedPropertyVectorWritable,
1:bf2df6e:       int maxValueIndex) throws IOException {
1:457db94:     Cluster cluster = clusterModels.get(maxValueIndex);
1:4d4efb0: 
1:4d4efb0:     DistanceMeasureCluster distanceMeasureCluster = (DistanceMeasureCluster) cluster;
1:4d4efb0:     DistanceMeasure distanceMeasure = distanceMeasureCluster.getMeasure();
1:4d4efb0:     double distance = distanceMeasure.distance(cluster.getCenter(), weightedPropertyVectorWritable.getVector());
1:4d4efb0: 
1:4d4efb0:     weightedPropertyVectorWritable.getProperties().put(new Text("distance"), new Text(Double.toString(distance)));
1:e93f05d:     writer.append(new IntWritable(cluster.getId()), weightedPropertyVectorWritable);
1:457db94:   }
1:457db94:   
1:457db94:   /**
1:457db94:    * Decides whether the vector should be classified or not based on the max pdf
1:457db94:    * value of the clusters and threshold value.
1:457db94:    * 
1:457db94:    * @return whether the vector should be classified or not.
1:457db94:    */
1:457db94:   private static boolean shouldClassify(Vector pdfPerCluster, Double clusterClassificationThreshold) {
1:229aeff:     return pdfPerCluster.maxValue() >= clusterClassificationThreshold;
1:457db94:   }
1:457db94:   
1:457db94:   private static void classifyClusterMR(Configuration conf, Path input, Path clustersIn, Path output,
1:bf2df6e:       Double clusterClassificationThreshold, boolean emitMostLikely) throws IOException, InterruptedException,
2:6a3f566:       ClassNotFoundException {
1:457db94:     
1:229aeff:     conf.setFloat(ClusterClassificationConfigKeys.OUTLIER_REMOVAL_THRESHOLD,
1:229aeff:                   clusterClassificationThreshold.floatValue());
1:229aeff:     conf.setBoolean(ClusterClassificationConfigKeys.EMIT_MOST_LIKELY, emitMostLikely);
1:229aeff:     conf.set(ClusterClassificationConfigKeys.CLUSTERS_IN, clustersIn.toUri().toString());
1:457db94:     
1:457db94:     Job job = new Job(conf, "Cluster Classification Driver running over input: " + input);
1:457db94:     job.setJarByClass(ClusterClassificationDriver.class);
1:457db94:     
1:457db94:     job.setInputFormatClass(SequenceFileInputFormat.class);
1:457db94:     job.setOutputFormatClass(SequenceFileOutputFormat.class);
1:457db94:     
1:457db94:     job.setMapperClass(ClusterClassificationMapper.class);
1:457db94:     job.setNumReduceTasks(0);
1:457db94:     
1:457db94:     job.setOutputKeyClass(IntWritable.class);
1:c20d04c:     job.setOutputValueClass(WeightedPropertyVectorWritable.class);
1:457db94:     
1:457db94:     FileInputFormat.addInputPath(job, input);
1:457db94:     FileOutputFormat.setOutputPath(job, output);
1:457db94:     if (!job.waitForCompletion(true)) {
1:457db94:       throw new InterruptedException("Cluster Classification Driver Job failed processing " + input);
1:457db94:     }
1:457db94:   }
1:457db94:   
1:58cc1ae:   public static void run(Configuration conf, Path input, Path clusteringOutputPath, Path output,
1:ba81a93:       double clusterClassificationThreshold, boolean emitMostLikely, boolean runSequential) throws IOException,
1:ba81a93:       InterruptedException, ClassNotFoundException {
1:6a3f566:     if (runSequential) {
1:ba81a93:       classifyClusterSeq(conf, input, clusteringOutputPath, output, clusterClassificationThreshold, emitMostLikely);
1:ba81a93:     } else {
1:ba81a93:       classifyClusterMR(conf, input, clusteringOutputPath, output, clusterClassificationThreshold, emitMostLikely);
1:6a3f566:     }
1:6a3f566:     
1:6a3f566:   }
1:6a3f566:   
1:6a3f566: }
============================================================================
author:Karl Richter
-------------------------------------------------------------------------------
commit:02ff22f
/////////////////////////////////////////////////////////////////////////
1:     Iterator<?> it = new SequenceFileDirValueIterator<>(finalClustersPath, PathType.LIST,
author:Suneel Marthi
-------------------------------------------------------------------------------
commit:85f9ece
/////////////////////////////////////////////////////////////////////////
1: import java.util.ArrayList;
1: import java.util.HashMap;
/////////////////////////////////////////////////////////////////////////
1:     List<Cluster> clusterModels = new ArrayList<>();
/////////////////////////////////////////////////////////////////////////
1:     Map<Text, Text> props = new HashMap<>();
/////////////////////////////////////////////////////////////////////////
1:     Map<Text, Text> props = new HashMap<>();
author:pferrel
-------------------------------------------------------------------------------
commit:b988c49
author:frankscholten
-------------------------------------------------------------------------------
commit:1a42d85
commit:1e11755
/////////////////////////////////////////////////////////////////////////
author:smarthi
-------------------------------------------------------------------------------
commit:4d4efb0
/////////////////////////////////////////////////////////////////////////
1: import org.apache.mahout.clustering.iterator.DistanceMeasureCluster;
1: import org.apache.mahout.common.distance.DistanceMeasure;
/////////////////////////////////////////////////////////////////////////
1: 
1:     DistanceMeasureCluster distanceMeasureCluster = (DistanceMeasureCluster) cluster;
1:     DistanceMeasure distanceMeasure = distanceMeasureCluster.getMeasure();
1:     double distance = distanceMeasure.distance(cluster.getCenter(), weightedPropertyVectorWritable.getVector());
1: 
1:     weightedPropertyVectorWritable.getProperties().put(new Text("distance"), new Text(Double.toString(distance)));
commit:ab6216f
/////////////////////////////////////////////////////////////////////////
1:       Class<? extends Writable> keyClass = vw.getFirst().getClass();
1:       if (!keyClass.equals(NamedVector.class)) {
1:         if (keyClass.equals(Text.class)) {
1:           vector = new NamedVector(vector, vw.getFirst().toString());
1:         } else if (keyClass.equals(IntWritable.class)) {
1:           vector = new NamedVector(vector, Integer.toString(((IntWritable) vw.getFirst()).get()));
commit:08da368
/////////////////////////////////////////////////////////////////////////
1:       // Converting to NamedVectors to preserve the vectorId else its not obvious as to which point
1:       // belongs to which cluster - fix for MAHOUT-1410
commit:e93f05d
/////////////////////////////////////////////////////////////////////////
1: import org.apache.mahout.common.Pair;
1: import org.apache.mahout.common.iterator.sequencefile.SequenceFileDirIterable;
1: import org.apache.mahout.math.NamedVector;
/////////////////////////////////////////////////////////////////////////
1:    *          the threshold value of probability distribution function from 0.0
1:    *          to 1.0. Any vector with pdf less that this threshold will not be
1:    *          classified for the cluster
1:    *          emit the vectors with the max pdf values per cluster
/////////////////////////////////////////////////////////////////////////
1:     for (Pair<Writable, VectorWritable> vw : new SequenceFileDirIterable<Writable, VectorWritable>(input, PathType.LIST,
0:       Writable key = vw.getFirst();
1:       Vector vector = vw.getSecond().get();
0:       if (!(vector instanceof NamedVector)) {
0:         if (key instanceof Text) {
0:           vector = new NamedVector(vector, key.toString());
0:         } else if (key instanceof IntWritable) {
0:           vector = new NamedVector(vector, Integer.toString(((IntWritable) key).get()));
1:         }
1:       }
1:       Vector pdfPerCluster = clusterClassifier.classify(vector);
1:         classifyAndWrite(clusterModels, clusterClassificationThreshold, emitMostLikely, writer, new VectorWritable(vector), pdfPerCluster);
/////////////////////////////////////////////////////////////////////////
1:       WeightedPropertyVectorWritable weightedPropertyVectorWritable =
1:           new WeightedPropertyVectorWritable(pdfPerCluster.maxValue(), vw.get(), props);
1:       write(clusterModels, writer, weightedPropertyVectorWritable, maxValueIndex);
/////////////////////////////////////////////////////////////////////////
0:     Map<Text, Text> props = Maps.newHashMap();
1:         WeightedPropertyVectorWritable wvw = new WeightedPropertyVectorWritable(pdf.get(), vw.get(), props);
1:   private static void write(List<Cluster> clusterModels, SequenceFile.Writer writer,
1:       WeightedPropertyVectorWritable weightedPropertyVectorWritable,
0:     double d = Math.sqrt(cluster.getCenter().getDistanceSquared(weightedPropertyVectorWritable.getVector()));
0:     weightedPropertyVectorWritable.getProperties().put(new Text("distance"), new Text(Double.toString(d)));
1:     writer.append(new IntWritable(cluster.getId()), weightedPropertyVectorWritable);
commit:c20d04c
/////////////////////////////////////////////////////////////////////////
1: import java.util.Map;
0: import com.google.common.collect.Maps;
1: import org.apache.hadoop.io.Text;
/////////////////////////////////////////////////////////////////////////
1:         "part-m-" + 0), IntWritable.class, WeightedPropertyVectorWritable.class);
/////////////////////////////////////////////////////////////////////////
0:     Map<Text, Text> props = Maps.newHashMap();
0:       WeightedPropertyVectorWritable wpvw = new WeightedPropertyVectorWritable(pdfPerCluster.maxValue(), vw.get(), props);
0:       write(clusterModels, writer, wpvw, maxValueIndex);
/////////////////////////////////////////////////////////////////////////
1: 
/////////////////////////////////////////////////////////////////////////
1:     job.setOutputValueClass(WeightedPropertyVectorWritable.class);
commit:c88c240
/////////////////////////////////////////////////////////////////////////
1:   public static void run(Configuration conf, Path input, Path clusteringOutputPath, Path output, Double clusterClassificationThreshold,
author:Sebastian Schelter
-------------------------------------------------------------------------------
commit:58cc1ae
/////////////////////////////////////////////////////////////////////////
1:     Configuration conf = new Configuration();
0:     run(conf, input, clusteringOutputPath, output, clusterClassificationThreshold, emitMostLikely, runSequential);
1:   public static void run(Configuration conf, Path input, Path clusteringOutputPath, Path output,
0:                          Double clusterClassificationThreshold, boolean emitMostLikely, boolean runSequential)
0:     throws IOException, InterruptedException, ClassNotFoundException {
commit:210b265
/////////////////////////////////////////////////////////////////////////
0: import com.google.common.collect.Lists;
/////////////////////////////////////////////////////////////////////////
0:     List<Cluster> clusterModels = Lists.newArrayList();
author:Isabel Drost
-------------------------------------------------------------------------------
commit:4ce12cc
/////////////////////////////////////////////////////////////////////////
1:     run(getConf(), input, clustersIn, output, clusterClassificationThreshold, true, runSequential);
/////////////////////////////////////////////////////////////////////////
0: 	  Configuration conf = new Configuration();
0: 	  run(conf, input, clusteringOutputPath, output, clusterClassificationThreshold, emitMostLikely, runSequential);
1:   }
1: 
0:   public static void run(Configuration conf, Path input, Path clusteringOutputPath, Path output, Double clusterClassificationThreshold,
1:       boolean emitMostLikely, boolean runSequential) throws IOException, InterruptedException, ClassNotFoundException {
author:Jacob Alexander Mannix
-------------------------------------------------------------------------------
commit:dc62944
/////////////////////////////////////////////////////////////////////////
1:     for (Element pdf : pdfPerCluster.nonZeroes()) {
author:Ted Dunning
-------------------------------------------------------------------------------
commit:402e296
/////////////////////////////////////////////////////////////////////////
/////////////////////////////////////////////////////////////////////////
author:Sean R. Owen
-------------------------------------------------------------------------------
commit:229aeff
/////////////////////////////////////////////////////////////////////////
/////////////////////////////////////////////////////////////////////////
1: public final class ClusterClassificationDriver extends AbstractJob {
/////////////////////////////////////////////////////////////////////////
1:   private ClusterClassificationDriver() {
1:   }
/////////////////////////////////////////////////////////////////////////
1:       Cluster cluster = next.getValue();
/////////////////////////////////////////////////////////////////////////
1:     return clusterFiles[0].getPath();
/////////////////////////////////////////////////////////////////////////
1:     return pdfPerCluster.maxValue() >= clusterClassificationThreshold;
1:     conf.setFloat(ClusterClassificationConfigKeys.OUTLIER_REMOVAL_THRESHOLD,
1:                   clusterClassificationThreshold.floatValue());
1:     conf.setBoolean(ClusterClassificationConfigKeys.EMIT_MOST_LIKELY, emitMostLikely);
1:     conf.set(ClusterClassificationConfigKeys.CLUSTERS_IN, clustersIn.toUri().toString());
author:Jeff Eastman
-------------------------------------------------------------------------------
commit:590ffed
/////////////////////////////////////////////////////////////////////////
/////////////////////////////////////////////////////////////////////////
1:       cluster.configure(conf);
commit:8d102ea
/////////////////////////////////////////////////////////////////////////
1: import org.apache.mahout.clustering.iterator.ClusteringPolicy;
commit:457db94
/////////////////////////////////////////////////////////////////////////
0: import org.apache.mahout.clustering.ClusteringPolicy;
/////////////////////////////////////////////////////////////////////////
1:  * Classifies the vectors into different clusters found by the clustering
1:  * algorithm.
1:   
1:   /**
1:    * CLI to run Cluster Classification Driver.
1:    */
1:   @Override
1:   public int run(String[] args) throws Exception {
1:     
1:     addInputOption();
1:     addOutputOption();
1:     addOption(DefaultOptionCreator.methodOption().create());
1:     addOption(DefaultOptionCreator.clustersInOption()
1:         .withDescription("The input centroids, as Vectors.  Must be a SequenceFile of Writable, Cluster/Canopy.")
1:         .create());
1:     
1:     if (parseArguments(args) == null) {
1:       return -1;
1:     }
1:     
1:     Path input = getInputPath();
1:     Path output = getOutputPath();
1:     
1:     if (getConf() == null) {
1:       setConf(new Configuration());
1:     }
1:     Path clustersIn = new Path(getOption(DefaultOptionCreator.CLUSTERS_IN_OPTION));
1:     boolean runSequential = getOption(DefaultOptionCreator.METHOD_OPTION).equalsIgnoreCase(
1:         DefaultOptionCreator.SEQUENTIAL_METHOD);
1:     
1:     double clusterClassificationThreshold = 0.0;
1:     if (hasOption(DefaultOptionCreator.OUTLIER_THRESHOLD)) {
1:       clusterClassificationThreshold = Double.parseDouble(getOption(DefaultOptionCreator.OUTLIER_THRESHOLD));
1:     }
1:     
0:     run(input, clustersIn, output, clusterClassificationThreshold, runSequential);
1:     
1:     return 0;
1:   }
1:   
1:   /**
1:    * Constructor to be used by the ToolRunner.
1:    */
0:   private ClusterClassificationDriver() {}
1:   
1:   public static void main(String[] args) throws Exception {
1:     ToolRunner.run(new Configuration(), new ClusterClassificationDriver(), args);
1:   }
1:   
1:   /**
1:    * Uses {@link ClusterClassifier} to classify input vectors into their
1:    * respective clusters.
1:    * 
1:    * @param input
1:    *          the input vectors
1:    * @param clusteringOutputPath
1:    *          the output path of clustering ( it reads clusters-*-final file
1:    *          from here )
1:    * @param output
1:    *          the location to store the classified vectors
1:    * @param clusterClassificationThreshold
1:    *          the threshold value of probability distribution function from 0.0
1:    *          to 1.0. Any vector with pdf less that this threshold will not be
1:    *          classified for the cluster.
1:    * @param runSequential
1:    *          Run the process sequentially or in a mapreduce way.
1:    * @throws IOException
1:    * @throws InterruptedException
1:    * @throws ClassNotFoundException
1:    */
0:   public static void run(Path input, Path clusteringOutputPath, Path output, Double clusterClassificationThreshold,
0:       boolean runSequential) throws IOException, InterruptedException, ClassNotFoundException {
1:     if (runSequential) {
0:       classifyClusterSeq(input, clusteringOutputPath, output, clusterClassificationThreshold);
1:     } else {
0:       classifyClusterMR(conf, input, clusteringOutputPath, output, clusterClassificationThreshold);
1:     }
1:     
1:   }
1:   
0:   private static void classifyClusterSeq(Path input, Path clusters, Path output, Double clusterClassificationThreshold)
0:       throws IOException {
0:     List<Cluster> clusterModels = populateClusterModels(clusters);
0:     ClusteringPolicy policy = ClusterClassifier.readPolicy(finalClustersPath(clusters));
1:     ClusterClassifier clusterClassifier = new ClusterClassifier(clusterModels, policy);
0:     selectCluster(input, clusterModels, clusterClassifier, output, clusterClassificationThreshold);
1:     
1:   }
1:   
1:   /**
1:    * Populates a list with clusters present in clusters-*-final directory.
1:    * 
1:    * @param clusterOutputPath
1:    *          The output path of the clustering.
1:    * @return The list of clusters found by the clustering.
1:    * @throws IOException
1:    */
0:   private static List<Cluster> populateClusterModels(Path clusterOutputPath) throws IOException {
0:     List<Cluster> clusterModels = new ArrayList<Cluster>();
0:     Cluster cluster = null;
0:     Path finalClustersPath = finalClustersPath(clusterOutputPath);
0:     Iterator<?> it = new SequenceFileDirValueIterator<Writable>(finalClustersPath, PathType.LIST,
0:         PathFilters.partFilter(), null, false, new Configuration());
1:     while (it.hasNext()) {
0:       cluster = (Cluster) it.next();
1:       clusterModels.add(cluster);
1:     }
1:     return clusterModels;
1:   }
1:   
0:   private static Path finalClustersPath(Path clusterOutputPath) throws IOException {
0:     FileSystem fileSystem = clusterOutputPath.getFileSystem(new Configuration());
1:     FileStatus[] clusterFiles = fileSystem.listStatus(clusterOutputPath, PathFilters.finalPartFilter());
0:     Path finalClustersPath = clusterFiles[0].getPath();
0:     return finalClustersPath;
1:   }
1:   
1:   /**
1:    * Classifies the vector into its respective cluster.
1:    * 
1:    * @param input
1:    *          the path containing the input vector.
1:    * @param clusterModels
1:    *          the clusters
1:    * @param clusterClassifier
1:    *          used to classify the vectors into different clusters
1:    * @param output
1:    *          the path to store classified data
1:    * @param clusterClassificationThreshold
1:    * @throws IOException
1:    */
1:   private static void selectCluster(Path input, List<Cluster> clusterModels, ClusterClassifier clusterClassifier,
0:       Path output, Double clusterClassificationThreshold) throws IOException {
0:     Configuration conf = new Configuration();
1:     SequenceFile.Writer writer = new SequenceFile.Writer(input.getFileSystem(conf), conf, new Path(output,
0:         "part-m-" + 0), IntWritable.class, VectorWritable.class);
0:     for (VectorWritable vw : new SequenceFileDirValueIterable<VectorWritable>(input, PathType.LIST,
1:         PathFilters.logsCRCFilter(), conf)) {
0:       Vector pdfPerCluster = clusterClassifier.classify(vw.get());
1:       if (shouldClassify(pdfPerCluster, clusterClassificationThreshold)) {
1:         int maxValueIndex = pdfPerCluster.maxValueIndex();
1:         Cluster cluster = clusterModels.get(maxValueIndex);
0:         writer.append(new IntWritable(cluster.getId()), vw);
1:     writer.close();
1:   }
1:   
1:   /**
1:    * Decides whether the vector should be classified or not based on the max pdf
1:    * value of the clusters and threshold value.
1:    * 
0:    * @param pdfPerCluster
0:    *          pdf of vector belonging to different clusters.
1:    * @param clusterClassificationThreshold
0:    *          threshold below which the vectors won't be classified.
1:    * @return whether the vector should be classified or not.
1:    */
1:   private static boolean shouldClassify(Vector pdfPerCluster, Double clusterClassificationThreshold) {
0:     return pdfPerCluster.maxValue() >= clusterClassificationThreshold;
1:   }
1:   
1:   private static void classifyClusterMR(Configuration conf, Path input, Path clustersIn, Path output,
0:       Double clusterClassificationThreshold) throws IOException, InterruptedException, ClassNotFoundException {
1:     Job job = new Job(conf, "Cluster Classification Driver running over input: " + input);
1:     job.setJarByClass(ClusterClassificationDriver.class);
1:     
0:     conf.setFloat(OUTLIER_REMOVAL_THRESHOLD, clusterClassificationThreshold.floatValue());
1:     
0:     conf.set(ClusterClassificationConfigKeys.CLUSTERS_IN, input.toString());
1:     
1:     job.setInputFormatClass(SequenceFileInputFormat.class);
1:     job.setOutputFormatClass(SequenceFileOutputFormat.class);
1:     
1:     job.setMapperClass(ClusterClassificationMapper.class);
1:     job.setNumReduceTasks(0);
1:     
1:     job.setOutputKeyClass(IntWritable.class);
0:     job.setOutputValueClass(WeightedVectorWritable.class);
1:     
1:     FileInputFormat.addInputPath(job, input);
1:     FileOutputFormat.setOutputPath(job, output);
1:     if (!job.waitForCompletion(true)) {
1:       throw new InterruptedException("Cluster Classification Driver Job failed processing " + input);
1:   }
1:   
1: }
commit:76e80dc
/////////////////////////////////////////////////////////////////////////
0: 	    ClusterClassifier clusterClassifier = new ClusterClassifier(clusterModels, null);
commit:6a3f566
/////////////////////////////////////////////////////////////////////////
1: /**
1:  * Licensed to the Apache Software Foundation (ASF) under one or more
1:  * contributor license agreements.  See the NOTICE file distributed with
1:  * this work for additional information regarding copyright ownership.
1:  * The ASF licenses this file to You under the Apache License, Version 2.0
1:  * (the "License"); you may not use this file except in compliance with
1:  * the License.  You may obtain a copy of the License at
1:  *
1:  *     http://www.apache.org/licenses/LICENSE-2.0
1:  *
1:  * Unless required by applicable law or agreed to in writing, software
1:  * distributed under the License is distributed on an "AS IS" BASIS,
1:  * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
1:  * See the License for the specific language governing permissions and
1:  * limitations under the License.
1:  */
1: 
1: package org.apache.mahout.clustering.classify;
1: 
0: import static org.apache.mahout.clustering.classify.ClusterClassificationConfigKeys.OUTLIER_REMOVAL_THRESHOLD;
1: 
1: import java.io.IOException;
0: import java.util.ArrayList;
1: import java.util.Iterator;
1: import java.util.List;
1: 
1: import org.apache.hadoop.conf.Configuration;
1: import org.apache.hadoop.fs.FileStatus;
1: import org.apache.hadoop.fs.FileSystem;
1: import org.apache.hadoop.fs.Path;
1: import org.apache.hadoop.io.IntWritable;
1: import org.apache.hadoop.io.SequenceFile;
1: import org.apache.hadoop.io.Writable;
1: import org.apache.hadoop.mapreduce.Job;
1: import org.apache.hadoop.mapreduce.lib.input.FileInputFormat;
1: import org.apache.hadoop.mapreduce.lib.input.SequenceFileInputFormat;
1: import org.apache.hadoop.mapreduce.lib.output.FileOutputFormat;
1: import org.apache.hadoop.mapreduce.lib.output.SequenceFileOutputFormat;
1: import org.apache.hadoop.util.ToolRunner;
1: import org.apache.mahout.clustering.Cluster;
0: import org.apache.mahout.clustering.ClusterClassifier;
0: import org.apache.mahout.clustering.WeightedVectorWritable;
1: import org.apache.mahout.common.AbstractJob;
1: import org.apache.mahout.common.commandline.DefaultOptionCreator;
1: import org.apache.mahout.common.iterator.sequencefile.PathFilters;
1: import org.apache.mahout.common.iterator.sequencefile.PathType;
0: import org.apache.mahout.common.iterator.sequencefile.SequenceFileDirValueIterable;
1: import org.apache.mahout.common.iterator.sequencefile.SequenceFileDirValueIterator;
1: import org.apache.mahout.math.Vector;
1: import org.apache.mahout.math.VectorWritable;
1: 
1: /**
0:  * Classifies the vectors into different clusters found by the clustering algorithm.
1:  */
0: public class ClusterClassificationDriver extends AbstractJob {
1: 	  
1:     /**
0: 	   * CLI to run Cluster Classification Driver.
1: 	   */
0: 	  @Override
0: 	  public int run(String[] args) throws Exception {
1: 	    
0: 	    addInputOption();
0: 	    addOutputOption();
0: 	    addOption(DefaultOptionCreator.methodOption().create());
0: 	    addOption(DefaultOptionCreator.clustersInOption()
0: 	            .withDescription("The input centroids, as Vectors.  Must be a SequenceFile of Writable, Cluster/Canopy.")
0: 	            .create());
1: 
0: 	    if (parseArguments(args) == null) {
0: 	      return -1;
1: 	    }
1: 	    
0: 	    Path input = getInputPath();
0: 	    Path output = getOutputPath();
1: 
0: 	    if (getConf() == null) {
0: 	      setConf(new Configuration());
1: 	    }
0: 	    Path clustersIn = new Path(getOption(DefaultOptionCreator.CLUSTERS_IN_OPTION));
0: 	    boolean runSequential = getOption(DefaultOptionCreator.METHOD_OPTION).equalsIgnoreCase(
0: 	      DefaultOptionCreator.SEQUENTIAL_METHOD);
1: 	    
0: 	    double clusterClassificationThreshold = 0.0;
0: 	    if (hasOption(DefaultOptionCreator.OUTLIER_THRESHOLD)) {
0: 	      clusterClassificationThreshold = Double.parseDouble(getOption(DefaultOptionCreator.OUTLIER_THRESHOLD));
1: 	    }
1: 	    
0:       run(input, clustersIn, output, clusterClassificationThreshold , runSequential);
1:       
0: 	    return 0;
1: 	  }
1: 	  
1: 	  /**
0: 	   * Constructor to be used by the ToolRunner.
1: 	   */
0: 	  private ClusterClassificationDriver() {}
1: 	  
0: 	  public static void main(String[] args) throws Exception {
0: 	    ToolRunner.run(new Configuration(), new ClusterClassificationDriver(), args);
1: 	  }
1: 	  
1: 	  /**
0: 	   * Uses {@link ClusterClassifier} to classify input vectors into their respective clusters.
1: 	   * 
0: 	   * @param input 
0: 	   *         the input vectors
0: 	   * @param clusteringOutputPath
0: 	   *         the output path of clustering ( it reads clusters-*-final file from here )
0: 	   * @param output
0: 	   *         the location to store the classified vectors
0: 	   * @param clusterClassificationThreshold
0: 	   *         the threshold value of probability distribution function from 0.0 to 1.0. 
0: 	   *         Any vector with pdf less that this threshold will not be classified for the cluster.
0: 	   * @param runSequential
0: 	   *         Run the process sequentially or in a mapreduce way.
0: 	   * @throws IOException
0: 	   * @throws InterruptedException
0: 	   * @throws ClassNotFoundException
1: 	   */
0: 	  public static void run(Path input, Path clusteringOutputPath, Path output, Double clusterClassificationThreshold, boolean runSequential) throws IOException,
0: 	                                                                        InterruptedException,
1: 	                                                                        ClassNotFoundException {
1: 	    if (runSequential) {
0: 	      classifyClusterSeq(input, clusteringOutputPath, output, clusterClassificationThreshold);
1: 	    } else {
0: 	      Configuration conf = new Configuration();
0: 	      classifyClusterMR(conf, input, clusteringOutputPath, output, clusterClassificationThreshold);
1: 	    }
1: 	    
1: 	  }
1: 	  
0: 	  private static void classifyClusterSeq(Path input, Path clusters, Path output, Double clusterClassificationThreshold) throws IOException {
0: 	    List<Cluster> clusterModels = populateClusterModels(clusters);
0: 	    ClusterClassifier clusterClassifier = new ClusterClassifier(clusterModels);
0:       selectCluster(input, clusterModels, clusterClassifier, output, clusterClassificationThreshold);
1:       
1: 	  }
1: 
1: 	  /**
0: 	   * Populates a list with clusters present in clusters-*-final directory.
1: 	   * 
0: 	   * @param clusterOutputPath
0: 	   *             The output path of the clustering.
0: 	   * @return
0: 	   *             The list of clusters found by the clustering.
0: 	   * @throws IOException
1: 	   */
0:     private static List<Cluster> populateClusterModels(Path clusterOutputPath) throws IOException {
0:       List<Cluster> clusterModels = new ArrayList<Cluster>();
0:       Configuration conf = new Configuration();
0:       Cluster cluster = null;
1:       FileSystem fileSystem = clusterOutputPath.getFileSystem(conf);
0:       FileStatus[] clusterFiles = fileSystem.listStatus(clusterOutputPath, PathFilters.finalPartFilter());
0:       Iterator<?> it = new SequenceFileDirValueIterator<Writable>(clusterFiles[0].getPath(),
0:                                                                   PathType.LIST,
0:                                                                   PathFilters.partFilter(),
0:                                                                   null,
0:                                                                   false,
0:                                                                   conf);
0:       while (it.hasNext()) {
0:         cluster = (Cluster) it.next();
0:         clusterModels.add(cluster);
1:       }
0:       return clusterModels;
1:     }
1: 	  
1:     /**
0:      * Classifies the vector into its respective cluster.
1:      * 
0:      * @param input 
0:      *            the path containing the input vector.
0:      * @param clusterModels
0:      *            the clusters
0:      * @param clusterClassifier
0:      *            used to classify the vectors into different clusters
0:      * @param output
0:      *            the path to store classified data
0:      * @param clusterClassificationThreshold
0:      * @throws IOException
1:      */
0: 	  private static void selectCluster(Path input, List<Cluster> clusterModels, ClusterClassifier clusterClassifier, Path output, Double clusterClassificationThreshold) throws IOException {
0: 	    Configuration conf = new Configuration();
0: 	    SequenceFile.Writer writer = new SequenceFile.Writer(input.getFileSystem(conf), conf, new Path(
0:           output, "part-m-" + 0), IntWritable.class,
0:           VectorWritable.class);
0: 	    for (VectorWritable vw : new SequenceFileDirValueIterable<VectorWritable>(
0: 	        input, PathType.LIST, PathFilters.logsCRCFilter(), conf)) {
0:         Vector pdfPerCluster = clusterClassifier.classify(vw.get());
0:         if(shouldClassify(pdfPerCluster, clusterClassificationThreshold)) {
0:           int maxValueIndex = pdfPerCluster.maxValueIndex();
0:           Cluster cluster = clusterModels.get(maxValueIndex);
0:           writer.append(new IntWritable(cluster.getId()), vw);
1:         }
1: 	    }
0: 	    writer.close();
1:     }
1: 
1: 	  /**
0: 	   * Decides whether the vector should be classified or not based on the max pdf value of the clusters and threshold value.
1: 	   * 
0: 	   * @param pdfPerCluster
0: 	   *         pdf of vector belonging to different clusters.
0: 	   * @param clusterClassificationThreshold
0: 	   *         threshold below which the vectors won't be classified.
0: 	   * @return whether the vector should be classified or not.
1: 	   */
0:     private static boolean shouldClassify(Vector pdfPerCluster, Double clusterClassificationThreshold) {
0:       return pdfPerCluster.maxValue() >= clusterClassificationThreshold;
1:     }
1: 
0: 	  private static void classifyClusterMR(Configuration conf, Path input, Path clustersIn, Path output, Double clusterClassificationThreshold) throws IOException,
0: 	                                                                                InterruptedException,
1: 	                                                                                ClassNotFoundException {
0: 	    Job job = new Job(conf, "Cluster Classification Driver running over input: " + input);
0: 	    job.setJarByClass(ClusterClassificationDriver.class);
1: 	    
0: 	    conf.setFloat(OUTLIER_REMOVAL_THRESHOLD, clusterClassificationThreshold.floatValue());
1: 	    
0: 	    conf.set(ClusterClassificationConfigKeys.CLUSTERS_IN, input.toString());
1: 	    
0: 	    job.setInputFormatClass(SequenceFileInputFormat.class);
0: 	    job.setOutputFormatClass(SequenceFileOutputFormat.class);
1: 	    
0: 	    job.setMapperClass(ClusterClassificationMapper.class);
0: 	    job.setNumReduceTasks(0);
1: 	    
0: 	    job.setOutputKeyClass(IntWritable.class);
0: 	    job.setOutputValueClass(WeightedVectorWritable.class);
1: 	    
0: 	    FileInputFormat.addInputPath(job, input);
0: 	    FileOutputFormat.setOutputPath(job, output);
0: 	    if (!job.waitForCompletion(true)) {
0: 	      throw new InterruptedException("Cluster Classification Driver Job failed processing " + input);
1: 	    }
1: 	  }
1: 	  
1: 	}
author:pranjan
-------------------------------------------------------------------------------
commit:fd355fe
/////////////////////////////////////////////////////////////////////////
1:     ClusteringPolicy policy = ClusterClassifier.readPolicy(finalClustersPath(conf, clusters));
/////////////////////////////////////////////////////////////////////////
1:     Path finalClustersPath = finalClustersPath(conf, clusterOutputPath);
/////////////////////////////////////////////////////////////////////////
1:   private static Path finalClustersPath(Configuration conf, Path clusterOutputPath) throws IOException {
0:     FileSystem fileSystem = clusterOutputPath.getFileSystem(conf);
commit:bf2df6e
/////////////////////////////////////////////////////////////////////////
0:     addOption(DefaultOptionCreator.clustersInOption()
0:         .withDescription("The input centroids, as Vectors.  Must be a SequenceFile of Writable, Cluster/Canopy.")
/////////////////////////////////////////////////////////////////////////
0:     Path clustersIn = new Path(getOption(DefaultOptionCreator.CLUSTERS_IN_OPTION));
0:     boolean runSequential = getOption(DefaultOptionCreator.METHOD_OPTION).equalsIgnoreCase(
0:         DefaultOptionCreator.SEQUENTIAL_METHOD);
0:       clusterClassificationThreshold = Double.parseDouble(getOption(DefaultOptionCreator.OUTLIER_THRESHOLD));
0:     run(input, clustersIn, output, clusterClassificationThreshold, true, runSequential);
/////////////////////////////////////////////////////////////////////////
0:     ToolRunner.run(new Configuration(), new ClusterClassificationDriver(), args);
/////////////////////////////////////////////////////////////////////////
1:   private static void classifyClusterSeq(Configuration conf, Path input, Path clusters, Path output,
1:       Double clusterClassificationThreshold, boolean emitMostLikely) throws IOException {
0:     ClusteringPolicy policy = ClusterClassifier.readPolicy(finalClustersPath(clusters));
0:     ClusterClassifier clusterClassifier = new ClusterClassifier(clusterModels, policy);
1:     selectCluster(input, clusterModels, clusterClassifier, output, clusterClassificationThreshold, emitMostLikely);
/////////////////////////////////////////////////////////////////////////
1:   private static List<Cluster> populateClusterModels(Path clusterOutputPath, Configuration conf) throws IOException {
0:     Iterator<?> it = new SequenceFileDirValueIterator<Writable>(finalClustersPath, PathType.LIST,
1:         PathFilters.partFilter(), null, false, conf);
1:       ClusterWritable next = (ClusterWritable) it.next();
0:       if (cluster instanceof DirichletCluster) {
0:         ((DirichletCluster) cluster).getModel().configure(conf);
0:   private static Path finalClustersPath(Path clusterOutputPath) throws IOException {
0:     FileSystem fileSystem = clusterOutputPath.getFileSystem(new Configuration());
0:     FileStatus[] clusterFiles = fileSystem.listStatus(clusterOutputPath, PathFilters.finalPartFilter());
/////////////////////////////////////////////////////////////////////////
0:   private static void selectCluster(Path input, List<Cluster> clusterModels, ClusterClassifier clusterClassifier,
1:       Path output, Double clusterClassificationThreshold, boolean emitMostLikely) throws IOException {
0:     SequenceFile.Writer writer = new SequenceFile.Writer(input.getFileSystem(conf), conf, new Path(output,
0:         "part-m-" + 0), IntWritable.class, WeightedVectorWritable.class);
0:     for (VectorWritable vw : new SequenceFileDirValueIterable<VectorWritable>(input, PathType.LIST,
0:         PathFilters.logsCRCFilter(), conf)) {
0:         classifyAndWrite(clusterModels, clusterClassificationThreshold, emitMostLikely, writer, vw, pdfPerCluster);
1:   private static void classifyAndWrite(List<Cluster> clusterModels, Double clusterClassificationThreshold,
1:       boolean emitMostLikely, SequenceFile.Writer writer, VectorWritable vw, Vector pdfPerCluster) throws IOException {
0:       WeightedVectorWritable wvw = new WeightedVectorWritable(pdfPerCluster.maxValue(), vw.get());
1:       writeAllAboveThreshold(clusterModels, clusterClassificationThreshold, writer, vw, pdfPerCluster);
1:   private static void writeAllAboveThreshold(List<Cluster> clusterModels, Double clusterClassificationThreshold,
1:       SequenceFile.Writer writer, VectorWritable vw, Vector pdfPerCluster) throws IOException {
0:         WeightedVectorWritable wvw = new WeightedVectorWritable(pdf.get(), vw.get());
0:   private static void write(List<Cluster> clusterModels, SequenceFile.Writer writer, WeightedVectorWritable wvw,
1:       int maxValueIndex) throws IOException {
/////////////////////////////////////////////////////////////////////////
0:   private static boolean shouldClassify(Vector pdfPerCluster, Double clusterClassificationThreshold) {
0:   private static void classifyClusterMR(Configuration conf, Path input, Path clustersIn, Path output,
1:       Double clusterClassificationThreshold, boolean emitMostLikely) throws IOException, InterruptedException,
0:       ClassNotFoundException {
0:     conf.setFloat(OUTLIER_REMOVAL_THRESHOLD, clusterClassificationThreshold.floatValue());
0:     Job job = new Job(conf, "Cluster Classification Driver running over input: " + input);
/////////////////////////////////////////////////////////////////////////
0:       throw new InterruptedException("Cluster Classification Driver Job failed processing " + input);
0:   
commit:51f58b8
/////////////////////////////////////////////////////////////////////////
1: import org.apache.mahout.clustering.iterator.ClusterWritable;
/////////////////////////////////////////////////////////////////////////
0:       ClusterWritable next = (ClusterWritable)it.next();
0:       cluster = (Cluster) next.getValue();
commit:ba81a93
/////////////////////////////////////////////////////////////////////////
0: import org.apache.mahout.clustering.dirichlet.DirichletCluster;
/////////////////////////////////////////////////////////////////////////
0:   public static void run(Path input, Path clusteringOutputPath, Path output, Double clusterClassificationThreshold,
0:       boolean emitMostLikely, boolean runSequential) throws IOException, InterruptedException, ClassNotFoundException {
0:     Configuration conf = new Configuration();
1:       classifyClusterSeq(conf, input, clusteringOutputPath, output, clusterClassificationThreshold, emitMostLikely);
1:       classifyClusterMR(conf, input, clusteringOutputPath, output, clusterClassificationThreshold, emitMostLikely);
0:   private static void classifyClusterSeq(Configuration conf, Path input,
0:       Path clusters, Path output, Double clusterClassificationThreshold, boolean emitMostLikely)
1:     List<Cluster> clusterModels = populateClusterModels(clusters, conf);
/////////////////////////////////////////////////////////////////////////
1:    * @param conf
1:    *          The Hadoop Configuration
0:   private static List<Cluster> populateClusterModels(Path clusterOutputPath, Configuration conf)
0:         false, conf);
0:       if(cluster instanceof DirichletCluster){
0:     	  ((DirichletCluster) cluster).getModel().configure(conf);
0:       }
/////////////////////////////////////////////////////////////////////////
0: 
0:   public static void run(Configuration conf, Path input, Path clusteringOutputPath, Path output,
1:       double clusterClassificationThreshold, boolean emitMostLikely, boolean runSequential) throws IOException,
1:       InterruptedException, ClassNotFoundException {
0:     if (runSequential) {
1:       classifyClusterSeq(conf, input, clusteringOutputPath, output, clusterClassificationThreshold, emitMostLikely);
1:     } else {
1:       classifyClusterMR(conf, input, clusteringOutputPath, output, clusterClassificationThreshold, emitMostLikely);
0:     }
0:     
0:   }
commit:1371326
/////////////////////////////////////////////////////////////////////////
0: import static org.apache.mahout.clustering.classify.ClusterClassificationConfigKeys.CLUSTERS_IN;
0: import static org.apache.mahout.clustering.classify.ClusterClassificationConfigKeys.EMIT_MOST_LIKELY;
/////////////////////////////////////////////////////////////////////////
0:         IntWritable.class, WeightedVectorWritable.class);
/////////////////////////////////////////////////////////////////////////
0:       WeightedVectorWritable wvw = new WeightedVectorWritable(
0:           pdfPerCluster.maxValue(), vw.get());
0:       write(clusterModels, writer, wvw, maxValueIndex);
/////////////////////////////////////////////////////////////////////////
0:         WeightedVectorWritable wvw = new WeightedVectorWritable(pdf.get(),
0:             vw.get());
1:         write(clusterModels, writer, wvw, clusterIndex);
0:       SequenceFile.Writer writer, WeightedVectorWritable wvw, int maxValueIndex)
0:     writer.append(new IntWritable(cluster.getId()), wvw);
/////////////////////////////////////////////////////////////////////////
0:       ClassNotFoundException {    
0:     conf.setBoolean(EMIT_MOST_LIKELY, emitMostLikely);
0:     conf.set(CLUSTERS_IN, clustersIn.toUri().toString());
0:     Job job = new Job(conf,
0:         "Cluster Classification Driver running over input: " + input);
0:     job.setJarByClass(ClusterClassificationDriver.class);
commit:98f66f4
/////////////////////////////////////////////////////////////////////////
1: import org.apache.mahout.math.Vector.Element;
/////////////////////////////////////////////////////////////////////////
0:     addOption(DefaultOptionCreator
0:         .clustersInOption()
0:         .withDescription(
0:             "The input centroids, as Vectors.  Must be a SequenceFile of Writable, Cluster/Canopy.")
/////////////////////////////////////////////////////////////////////////
0:     Path clustersIn = new Path(
0:         getOption(DefaultOptionCreator.CLUSTERS_IN_OPTION));
0:     boolean runSequential = getOption(DefaultOptionCreator.METHOD_OPTION)
0:         .equalsIgnoreCase(DefaultOptionCreator.SEQUENTIAL_METHOD);
0:       clusterClassificationThreshold = Double
0:           .parseDouble(getOption(DefaultOptionCreator.OUTLIER_THRESHOLD));
0:     run(input, clustersIn, output, clusterClassificationThreshold, true,
0:         runSequential);
/////////////////////////////////////////////////////////////////////////
0:     ToolRunner
0:         .run(new Configuration(), new ClusterClassificationDriver(), args);
/////////////////////////////////////////////////////////////////////////
0:    * @param runSequential
0:   public static void run(Path input, Path clusteringOutputPath, Path output,
0:       Double clusterClassificationThreshold, boolean emitMostLikely,
0:       boolean runSequential) throws IOException, InterruptedException,
0:       ClassNotFoundException {
0:       classifyClusterSeq(input, clusteringOutputPath, output,
0:           clusterClassificationThreshold, emitMostLikely);
0:       classifyClusterMR(conf, input, clusteringOutputPath, output,
0:           clusterClassificationThreshold, emitMostLikely);
0:   private static void classifyClusterSeq(Path input, Path clusters,
0:       Path output, Double clusterClassificationThreshold, boolean emitMostLikely)
0:     ClusteringPolicy policy = ClusterClassifier
0:         .readPolicy(finalClustersPath(clusters));
0:     ClusterClassifier clusterClassifier = new ClusterClassifier(clusterModels,
0:         policy);
0:     selectCluster(input, clusterModels, clusterClassifier, output,
0:         clusterClassificationThreshold, emitMostLikely);
/////////////////////////////////////////////////////////////////////////
0:   private static List<Cluster> populateClusterModels(Path clusterOutputPath)
0:       throws IOException {
0:     Iterator<?> it = new SequenceFileDirValueIterator<Writable>(
0:         finalClustersPath, PathType.LIST, PathFilters.partFilter(), null,
0:         false, new Configuration());
/////////////////////////////////////////////////////////////////////////
0:   private static Path finalClustersPath(Path clusterOutputPath)
0:       throws IOException {
0:     FileSystem fileSystem = clusterOutputPath
0:         .getFileSystem(new Configuration());
0:     FileStatus[] clusterFiles = fileSystem.listStatus(clusterOutputPath,
0:         PathFilters.finalPartFilter());
/////////////////////////////////////////////////////////////////////////
1:    * @param emitMostLikely
0:    *          TODO
0:   private static void selectCluster(Path input, List<Cluster> clusterModels,
0:       ClusterClassifier clusterClassifier, Path output,
0:       Double clusterClassificationThreshold, boolean emitMostLikely)
0:       throws IOException {
0:     SequenceFile.Writer writer = new SequenceFile.Writer(
0:         input.getFileSystem(conf), conf, new Path(output, "part-m-" + 0),
0:         IntWritable.class, VectorWritable.class);
0:     for (VectorWritable vw : new SequenceFileDirValueIterable<VectorWritable>(
0:         input, PathType.LIST, PathFilters.logsCRCFilter(), conf)) {
0:         classifyAndWrite(clusterModels, clusterClassificationThreshold,
0:             emitMostLikely, writer, vw, pdfPerCluster);
0:   private static void classifyAndWrite(List<Cluster> clusterModels,
0:       Double clusterClassificationThreshold, boolean emitMostLikely,
0:       SequenceFile.Writer writer, VectorWritable vw, Vector pdfPerCluster)
0:       throws IOException {
1:     if (emitMostLikely) {
0:       int maxValueIndex = pdfPerCluster.maxValueIndex();
0:       write(clusterModels, writer, vw, maxValueIndex);
0:     } else {
0:       writeAllAboveThreshold(clusterModels, clusterClassificationThreshold,
0:           writer, vw, pdfPerCluster);
0:     }
0:   }
0:   
0:   private static void writeAllAboveThreshold(List<Cluster> clusterModels,
0:       Double clusterClassificationThreshold, SequenceFile.Writer writer,
0:       VectorWritable vw, Vector pdfPerCluster) throws IOException {
0:     Iterator<Element> iterateNonZero = pdfPerCluster.iterateNonZero();
0:     while (iterateNonZero.hasNext()) {
0:       Element pdf = iterateNonZero.next();
1:       if (pdf.get() >= clusterClassificationThreshold) {
1:         int clusterIndex = pdf.index();
0:         write(clusterModels, writer, vw, clusterIndex);
0:       }
0:     }
0:   }
0:   
0:   private static void write(List<Cluster> clusterModels,
0:       SequenceFile.Writer writer, VectorWritable vw, int maxValueIndex)
0:       throws IOException {
0:     Cluster cluster = clusterModels.get(maxValueIndex);
0:     writer.append(new IntWritable(cluster.getId()), vw);
0:   }
0:   
0:   private static boolean shouldClassify(Vector pdfPerCluster,
0:       Double clusterClassificationThreshold) {
0:     boolean isMaxPDFGreatherThanThreshold = pdfPerCluster.maxValue() >= clusterClassificationThreshold;
0:     return isMaxPDFGreatherThanThreshold;
0:   private static void classifyClusterMR(Configuration conf, Path input,
0:       Path clustersIn, Path output, Double clusterClassificationThreshold,
0:       boolean emitMostLikely) throws IOException, InterruptedException,
0:       ClassNotFoundException {
0:     Job job = new Job(conf,
0:         "Cluster Classification Driver running over input: " + input);
0:     conf.setFloat(OUTLIER_REMOVAL_THRESHOLD,
0:         clusterClassificationThreshold.floatValue());
/////////////////////////////////////////////////////////////////////////
0:       throw new InterruptedException(
0:           "Cluster Classification Driver Job failed processing " + input);
============================================================================